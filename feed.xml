<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Wed, 03 Sep 2025 02:18:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[モノリポだっていけちゃうdenols/ts_ls共存術（Neovim >= 0.11）]]></title>
            <link>https://blog.atusy.net/2025/09/03/node-deno-decision-with-monorepo-support/</link>
            <guid isPermaLink="false">https://blog.atusy.net/2025/09/03/node-deno-decision-with-monorepo-support/</guid>
            <pubDate>Wed, 03 Sep 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Neovim >= 0.11では言語サーバーを簡単に自動起動する方法としてvim.lsp.enable({ ... })があるけど、FileTypeイベントに応じてvim.lsp.start(...)を呼べばもっと細かい制御ができるよ。応用すれば、denols/ts_lsの共存も細やか。モノリポだって怖くない。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AI Feature Store: オフラインストアを試してみた]]></title>
            <link>https://zenn.dev/akasan/articles/c542c7ec7a5cf2</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/c542c7ec7a5cf2</guid>
            <pubDate>Tue, 02 Sep 2025 11:48:40 GMT</pubDate>
            <content:encoded><![CDATA[今回はVertex AI上で提供されているFeature Storeについて、オフラインサービング機能を試してみたので共有します。 Vertex AI Feature Storeとは？まずはFeature Storeとは何かというところですが、簡単にいうとMLモデルを学習するための特徴量を中央集権的に管理してくれるレジストリです。MLモデルを開発する時はそれぞれの開発者がモデルに必要な特徴量を作るために前処理を実装しますが、生成された特徴量を他のエンジニアに共有するためにわざわざファイルに落として共有する必要があります。Feature Storeを導入すると、作成された特徴量はFe...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Amazon VPC CNIに学ぶCNI-LT版]]></title>
            <link>https://speakerdeck.com/bells17/amazon-vpc-cninixue-hucni-ltban</link>
            <guid isPermaLink="false">https://speakerdeck.com/bells17/amazon-vpc-cninixue-hucni-ltban</guid>
            <pubDate>Tue, 02 Sep 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[https://k8sjp.connpass.com/event/365262/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[lakeFSシリーズ: Qucikstart入門編]]></title>
            <link>https://zenn.dev/akasan/articles/f51ba2da49ec1a</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/f51ba2da49ec1a</guid>
            <pubDate>Mon, 01 Sep 2025 13:09:07 GMT</pubDate>
            <content:encoded><![CDATA[今回からlakeFS紹介シリーズも始めようと思います（一体いくつシリーズ始めるんだw）。lakeFSを利用することでデータのバージョンをGitで管理することができるようになります。今回は公式で提供されているQiuckstartを通して入門してみます。 lakeFSとは？lakeFSは、オブジェクトストレージをGitライクなリポジトリに変換するオープンソースのツールであり、コードを管理するようにデータレイクを管理できるもののようです。lakeFSを利用することで、複雑なETLジョブからデータサイエンスやアナリティクスまで、反復可能でアトミックかつバージョン管理されたデータレイクオペレ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[『禅とオートバイ修理技術』を読んだ。]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/09/01/145700</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/09/01/145700</guid>
            <pubDate>Mon, 01 Sep 2025 05:57:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめにプログラマーとして働き始めて数年が経った頃、私は壁にぶつかっていた。コードは書ける。バグも直せる。でも、何かが足りない。毎日キーボードを叩きながら、「これでいいのか」という疑問が頭をよぎる。そんな時期に、勉強会で出会った人が一冊の本を勧めてくれた。私は勧められた本を買うのが好きで、その場で積読として購入した。今となってはその人の顔も名前も思い出せないけれど、あの時の一言には本当に感謝しています。『禅とオートバイ修理技術』――タイトルを聞いた時は、正直なところピンと来なかった。禅？オートバイ？エンジニアである私とどう関係があるのか。禅とオートバイ修理技術 上 (ハヤカワ文庫NF)作者:ロバート Ｍ パーシグ早川書房Amazon禅とオートバイ修理技術 下 (ハヤカワ文庫NF)作者:ロバート Ｍ パーシグ早川書房Amazonでも読み始めてみると、これが不思議と心に響いた。技術と向き合うこと、品質を追求すること、理性と感性の葛藤。オートバイの修理を通じて語られる哲学は、まさに私がプログラミングで感じていた言語化できないモヤモヤそのものだった。以来、この本は私の座右の書となった。行き詰まるたびに読み返し、そのたびに新しい発見がある。最初は理解できなかった箇所が、経験を積むにつれて腑に落ちるようになる。まるで本自体が、読む人の成長に合わせて違う顔を見せてくれるかのようだ。実はこの文章も、5年前に書き始めて完成できずに下書きに眠っていたものだ。今回改めて書き直してみると、当時とはまったく違う視点でこの本を読んでいることに気づく。それだけ自分も変化したということなのだろう。特に若手のエンジニアには、ぜひ一度手に取ってもらいたい。技術書やビジネス書とは違う角度から、エンジニアリングの本質について考えさせてくれる。すぐには理解できなくても構わない。キャリアを重ねる中で、きっとこの本の言葉が響く瞬間が来るはずだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォロワーしてくれると嬉しいです。では、早速はじめていきます。古典的な思考とロマン的な思考本書の主人公は、物語の冒頭では古典的な（理性を重んじる）立場にいる。ロマン的な（情緒を重んじる）友人たちに対して、無理解で批判的な態度を取る。オートバイの構造を理解しようとしない友人を見下し、技術への無知を軽蔑する。メンテナンスを他人任せにする友人に苛立ち、「なぜ自分で理解しようとしないのか」と内心で批判する。読んでいて、胸が痛くなった。これは過去の私そのものだった。「なぜコードの仕組みを理解しようとしないんだ」と、フレームワークの内部実装に興味を示さない同僚を見下していた。「とりあえず動けばいい」という態度が理解できなかった。技術の背後にある原理を知ろうとしない人々を、内心で「浅い」と批判していた。私にとって、コードの構造を理解することこそが美しく、アルゴリズムの優雅さこそが感動的だった。でも、多くの人にとっては違う。彼らは技術を道具として使い、その先にある価値創造に集中していた。技術の詳細に囚われず、より大きな視点で物事を見ていた。古典的な視点からは、ロマン的な人々は「表面的」に見える。でもロマン的な視点からは、古典的な人々は「冷たく」「機械的」に見える。どちらも一面的な見方でしかない。以前、私は「正義のエンジニアという幻想」について考えたことがある。技術的に正しいことを追求し、それ以外を否定する。「媚びない」と言いながら、実際はただ無礼なだけ。技術的正しさを盾に、人間関係の機微を「非論理的」と切り捨てる。まさに、本書の主人公の初期の姿そのものだった。syu-m-5151.hatenablog.comしかし物語が進むにつれ、主人公の本当の目的が明らかになる。彼は実は中道を目指していた。古典的な立場とロマン的な立場を《クオリティ》という概念で統一しようとしていたのだ。分析と直感、構造と体験、理性と感性。対立ではなく、統合こそが答えだった。クオリティという統合点パーシグは「クオリティ」という概念を追求した。それは定義できない。定義した瞬間、別のものになってしまう。でも確実に存在する。誰もが「良いコード」と「悪いコード」の違いを感じることができる。しかし、その「良さ」を完全に言語化しようとすると、何か本質的なものが抜け落ちてしまう。改訂新版　良いコード／悪いコードで学ぶ設計入門 ―保守しやすい　成長し続けるコードの書き方作者:仙塲 大也技術評論社Amazon「可読性が高い」「保守しやすい」「パフォーマンスが良い」――これらは確かに重要な要素だが、それだけでは説明しきれない「何か」がある。syu-m-5151.hatenablog.comこの逆説的な性質は、グッドハートの法則やキャンベルの法則を思い起こさせる。「測定されるものは改善される。測定基準となったものは、良い測定基準ではなくなる」――クオリティを定量化しようとした瞬間、それは本来のクオリティから離れていく。コードカバレッジ100%を目指したら、意味のないテストが増えた。cyclomatic complexityを下げようとしたら、かえって読みにくいコードになった。メトリクスは重要だが、メトリクスがすべてではない。数値化された瞬間、クオリティは形骸化する。測りすぎ――なぜパフォーマンス評価は失敗するのか？作者:ジェリー・Z・ミュラーみすず書房Amazon優れたコードを見た瞬間の「これだ」という感覚。それは論理的分析より先に来る。でも、単なる感情でもない。理性と感性が融合した瞬間に現れる何か。ある日、オープンソースのコードを読んでいて息を呑んだことがある。複雑な問題を、驚くほどシンプルに解決していた。無駄が一切なく、それでいて拡張性も担保されている。「美しい」としか言いようがなかった。後から分析すれば、SOLID原則に従っているとか、デザインパターンが適切に使われているとか説明できる。でも、最初に感じたのは、理屈を超えた「美」だった。古代ギリシアでは、これを「アレテー」と呼んだ。「それそのものが持つポテンシャルを最大限発揮している状態」。馬には馬のアレテーがあり、ナイフにはナイフのアレテーがある。コードで言えば、その、コードやシステムが解決すべき問題に対して、最も自然で、最も美しく、最も効果的な形で存在している状態。過不足がない。シンプルだが単純ではない。複雑な問題を複雑に解くのではなく、本質を見抜いて エレガント に解く。それがコードのアレテー、つまりクオリティだ。理性だけでは到達できない。感性だけでも到達できない。両方が必要だ。論理的な正しさと、直感的な美しさ。分析と統合。部分と全体。これらが調和した時、初めてクオリティが現れる。A Philosophy of Software Design, 2nd Edition (English Edition)作者:Ousterhout, John K. ISSVWOAmazon物語の転換物語の終盤、主人公は古典的な立場への疑問を深めていく。科学的方法は使い続けるが、科学万能主義には批判的になる。むしろロマン的な立場に理解を示し始める。きっかけは、科学的方法の限界に直面したことだった。オートバイの不調の原因を論理的に分析し、仮説を立て、一つずつ検証していく。しかし、問題は解決しない。考えられる原因をすべて潰しても、バイクは不調のまま。そして気づく――仮説は無限に作れることに。「一定の現象を説明しうる合理的な仮説の数は無限にある」この気づきが、主人公を変えた。科学は仮説を検証する方法は教えてくれるが、どの仮説を選ぶべきかは教えてくれない。無限の可能性の中から、どうやって「これだ」という一つを選ぶのか。絶対的な真理など存在しない。だとしたら、何を基準に選択すればいいのか？答えは「クオリティ」だった。論理的な正しさだけでなく、その状況における「良さ」を感じ取る能力。理性と感性を統合した判断。優れた整備士は、エンジン音を聞いただけで不調の原因を言い当てる。それは論理的推論の結果ではない。経験と直感が導く「これしかない」という確信。主人公は理解する。友人たちがオートバイの仕組みを知ろうとしないのは、怠惰ではなく、別の関わり方を選んでいるからだ。彼らにとってバイクは、風を感じ、自由を味わう道具。内部構造など知らなくても、その本質的な価値は変わらない。古典的でもロマン的でもなく、その両方を包含する視点。それこそが、パーシグが追い求めていたものだった。無限の仮説とプログラミングプログラミングでも同じことが起きる。一つの問題を解決する方法は無数にある。私も経験がある。新規プロジェクトのアーキテクチャを決める時、本を読めば読むほど迷走した。『クリーンアーキテクチャ』は「ビジネスロジックを中心に」と説く。『マイクロサービスパターン』は「サービスの分割を」と勧める。『レガシーコード改善ガイド』は「まずテストから」と主張する。どれも正しい。でも、どれも部分的だ。ある時、気づいた。これらの本は地図のようなものだ。山頂への道は無数にあり、どの道も「正しい」。でも、今の自分たちのチームが、この天候で、この装備で登るべき道は一つ。その判断は、地図だけでは下せない。だから必要なのは、理論を超えた何か。コンテキストを読み取り、チームの状況を感じ取り、ユーザーの気持ちを想像する。スタートアップなら速度を、エンタープライズなら堅牢性を、でもそれも一概には言えない。チームの経験、プロダクトの成熟度、市場の要求、技術的負債の現状――すべてを総合的に「感じ取って」判断する。論理と感性を統合した判断。それは経験を積むことでしか身につかない。でも、それこそがシニアエンジニアの真の価値なのかもしれない。無限の選択肢の中から、「今、ここで、このチームが選ぶべき道」を見出す能力。それもまた、クオリティの一つの形だ。アーキテクトの教科書 価値を生むソフトウェアのアーキテクチャ構築作者:米久保 剛翔泳社Amazon主客の融合オートバイのメンテナンス中、固着したネジと格闘する場面がある。パーシグはこう語る。「修理工とオートバイは永遠に別個の存在ではない。二元的な考え方をすることで、修理工とオートバイとの間に存在する分離できない関係、つまり仕事に専心する職人気質といったものが失われてしまう」プログラミングも同じだ。私たちはコードを「書く」のではない。システムと対話し、問題空間と解決空間を行き来しながら、共に答えを見つけていく。フロー状態に入った時、キーボードは手の延長になり、思考は直接コードになる。変数名を考える必要もない。自然と適切な名前が浮かぶ。この時、プログラマーとコードの境界は消える。理性も感性も超えた、純粋な創造の瞬間。最近流行りのAIによるコード生成では、この感覚は得られない。プロンプトを書いて、生成されたコードをレビューして、修正を指示する。それは便利だし、効率的かもしれない。でも、そこには主客の分離がある。私とコード、指示する者と実行する者という二元的な関係。AIがどれだけ進化しても、この融合の瞬間は体験できないのかもしれない。それは効率や正確さとは別の次元の話だから。パーシグが固着したネジと格闘しながら得た洞察、その瞬間の一体感。それは自分の手でコードを書き、自分の頭で考え、自分の感覚で判断することでしか得られない。少なくとも今のところはその兆しすら感じない。熟達論―人はいつまでも学び、成長できる―作者:為末大新潮社Amazon心の静寂「バイクの修理に取り組むときに心がけるべきことは、自他の分離をしないような心の落ち着きを養うことである。心の落ち着きは正しい価値を生み、正しい価値は正しい思念を生む」デバッグで行き詰まった時、論理的分析だけでは見えないものがある。深呼吸して、システムの「気配」を感じる。ログを機械的に読むのではなく、パターンを「感じ取る」。正常時と異常時の「違和感」を察知する。これは非科学的なことではない。むしろ、科学と直感を統合した、より高次の認識方法だ。将棋の棋士が盤面を「読む」ように、経験豊富なエンジニアはシステムを「読む」。それは論理的分析と直感的理解が融合した、独特の認識方法だ。心が乱れていると、コードも乱れる。焦って書いたコードは、必ずどこかに歪みがある。逆に、落ち着いた心で書いたコードは、自然で無理がない。心の状態は、そのままコードの質に反映される。奪われた集中力: もう一度〝じっくり〟考えるための方法作者:ヨハン・ハリ作品社Amazonガンプション・トラップパーシグが作った「ガンプション・トラップ」という概念は、創造的な活動における意欲や熱意（ガンプション）を奪う罠のことだ。理性の側には、完璧な設計への固執という罠がある。「もっとエレガントな解法があるはずだ」という思いに囚われて、永遠にリファクタリングを続ける。より良い抽象化を求めるあまり、実装が進まない。分析に分析を重ね、結局は麻痺状態に陥る。一方、感性の側にも危険が潜んでいる。「なんとなくXXが好き」「とにかくYYに慣れている」という理由だけで技術選定をする。最初の直感に囚われて、他の可能性を検討しない。「このコードは美しい」という感覚に酔いしれて、実用性を忘れる。特に「価値観の硬直」の話が印象的だった。南インドの猿の罠――ココナッツの中の米を握った猿は、手を離せば自由になれるのに、米を手放せない。私たちも同じだ。「これがベストプラクティスだから」と言いながら、実は状況が変わっていることに気づかない。逆に、「自分のやり方」に固執して、明らかに優れた新しい手法を拒絶する。罠は至るところにある。それを避けるには、自分が今どの罠に陥りかけているかを認識し、一歩引いて見る必要がある。情報を正しく選択するための認知バイアス事典 行動経済学・統計学・情報学 編作者:情報文化研究所フォレスト出版Amazonテクノロジーとの関係性「真の醜さの原因は、テクノロジーを生み出す人々と、彼らが生み出す物との関係のなかに横たわっている」パーシグはこの言葉で、技術そのものが問題なのではなく、私たちと技術の関係が問題だと指摘する。オートバイを恐れる友人も、オートバイに依存する主人公も、どちらも不健全な関係だった。技術を理性的に分析するだけでも、感情的に拒絶するだけでもダメだ。技術と「共に在る」ことが大切。対話し、感じ取り、理解し、共に成長する。新しいフレームワークを学ぶ時、ドキュメントを読むだけでは不十分。実際に触って、感触を確かめ、「このフレームワークが望んでいること」を感じ取る。作者の思想、コミュニティの文化、設計の美学。技術の向こう側にある「人間」を理解する。技術は道具以上の存在になりうる。それは私たちの思考を拡張し、新しい可能性を開く。でも同時に、技術に振り回されることもある。流行に飛びつき、本質を見失い、手段が目的化する。パーシグが言うように、技術との健全な関係を築くには、クオリティを中心に据える必要がある。行き詰まりの価値プログラミングには様々な行き詰まりがある。どんな設計にすべきか何日も悩む。アーキテクチャの方向性で迷い続ける。技術選定で延々と議論する。実装方法が思いつかない。エラーの原因が分からない。これらはすべて、私たちが日常的に経験する行き詰まりだ。パーシグも、オートバイの不調だけでなく、人生の様々な場面で行き詰まりと向き合った。大学での哲学的探求、クオリティの定義、東洋と西洋の思想の統合。どれも簡単には答えが出ない問題だった。しかし、その行き詰まりこそが、彼を深い洞察へと導いた。行き詰まりは、今使っている思考法の限界を示すサインだ。論理だけで解決しようとしているなら、直感を使ってみる。感覚だけで進めているなら、分析的に考えてみる。視点を変え、アプローチを変え、時には問題そのものを問い直す必要がある。最高のブレイクスルーは、理性と感性が統合された瞬間に起きる。散歩中に突然解決策が浮かぶのは、論理的思考が一旦止まり、無意識の直感が働くからだ。しかし、その直感は、それまでの論理的分析があってこそ生まれる。苦闘は無駄ではない。それは答えを「熟成」させる時間なのだ。最近では、生成AIに問題を投げれば、すぐに答えが返ってくる。確かに便利だ。でも、そこには何かが欠けている。パーシグがオートバイと格闘しながら得た洞察、その苦闘の中で培われた理解の深さ。それは、答えを与えられることでは決して得られない。自分で考え、悩み、試行錯誤することで初めて、問題の本質が見えてくる。技術への理解が深まり、思考が鍛えられ、判断力が養われる。だから行き詰まりを恐れる必要はない。それは成長の前兆であり、ブレイクスルーの準備期間だ。大切なのは、行き詰まりと向き合う姿勢。焦らず、諦めず、クオリティを追求し続けること。その先に必ず何かが見えてくる。中道への道物語を通じて、主人公は変化していく。最初は理性の側に偏り、ロマン的なものを軽視していた。しかし、理性の限界を知り、感性の価値を認識し、最終的には両者を統合する道を見出す。この変化は緩やかで、時に後退しながら進む。主人公は何度も自分の過去（パイドロス）と向き合い、その度に少しずつ理解を深めていく。完全な統合ではなく、絶え間ない調整のプロセスとして。私も似た道を歩んでいる。最初は、論理と理性こそがすべてだと思っていた。設計パターンを暗記し、アルゴリズムを学び、ベストプラクティスを追求した。コードレビューでは「なぜこう書いたのか」を論理的に説明できることが最重要だと信じていた。感覚的な判断は「プロらしくない」と切り捨てていた。転機は、あるシニアエンジニアとのペアプログラミングだった。彼は設計を決める時、まず黙って考え、そして「これが気持ちいい」と言った。最初は戸惑った。でも、その設計は確かに優れていた。後から理由を分析すると論理的にも正しかったが、彼は直感が先行していた。今では分かる。優れたコードには、論理を超えた「何か」がある。それは説明できないけれど、確実に感じることができる。コードを読んだ瞬間の「あ、これは違う」という違和感。リファクタリング後の「これだ」という確信。これらは理性的分析の前に訪れる。でも、だからといって直感だけに頼るわけではない。感じた「何か」を論理的に検証し、言語化する努力も続ける。理性と感性は対立するものではなく、互いを補完し合うパートナーなのだ。中道とは、真ん中に立ち止まることではない。両極を知り、状況に応じて自在に行き来すること。時には徹底的に論理的に、時には大胆に直感的に。そして多くの場合は、その両方を同時に働かせながら。この「何か」を追求することこそが、本当のプログラミングなのかもしれない。技術は手段であり、目的は「良いもの」を作ること。その「良さ」は、理性と感性が調和した時に初めて生まれる。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazonおわりにパーシグは「クオリティ」を追求するあまり精神を病み、最終的には息子との旅を通じて、理性と感性を統合する道を見つけた。この本を読んで10年以上経つが、私のエンジニアリングへの向き合い方は確実に変わった。昔は「正しいコード」を書くことばかり考えていた。設計パターンに当てはめ、メトリクスを改善し、ベストプラクティスを守る。それが良いエンジニアだと思っていた。でも今は違う。チームの状況、プロダクトの段階、ユーザーのニーズ――すべてを考慮して「今ここで最適な選択」をすることが大切だと理解している。コードレビューの姿勢も変わった。以前は「なぜこう書いたのか」を論理的に説明することを求めていた。今は「これで良さそう」という直感的な判断も大切にしている。もちろん、その直感を後から論理的に検証することは忘れないが。『禅とオートバイ修理技術』は、エンジニアリングの教科書ではない。でも、技術と向き合う姿勢について、どんな技術書よりも深い示唆を与えてくれる。良いコードを書くには、論理的思考も直感も必要だ。設計の美しさを感じ取る感性と、それを実装する技術力。問題の本質を見抜く洞察力と、地道にデバッグする忍耐力。これらはどれも欠かせない。技術は進化し続ける。新しいフレームワーク、新しいパラダイム、新しいツール、AIなども忘れてはいけない。でも、「良いものを作りたい」という気持ちと、そのための試行錯誤は変わらない。もし若手エンジニアがこれを読んでいるなら、ぜひ『禅とオートバイ修理技術』を手に取ってみてほしい。すぐには理解できないかもしれない。でも、エンジニアとして経験を積むうちに、きっとこの本の言葉が響く瞬間が来る。その時、あなたのエンジニアリングは一段階上のレベルに達しているはずだ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[スリーシェイク、「Developers Summit 2025 KANSAI」に協賛・出展]]></title>
            <link>https://sreake.com/blog/developers-summit-2025-kansai/</link>
            <guid isPermaLink="false">https://sreake.com/blog/developers-summit-2025-kansai/</guid>
            <pubDate>Mon, 01 Sep 2025 01:30:00 GMT</pubDate>
            <content:encoded><![CDATA[株式会社スリーシェイク（本社：東京都新宿区、代表取締役社長：吉田 拓真、以下スリーシェイク）は、 2025年 9月17日（水）に開催される「Developers Summit 2025 KANSAI」に展示ブーススポンサーとして協賛します。The post スリーシェイク、「Developers Summit 2025 KANSAI」に協賛・出展 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AI Pipelinesに入門してみよう]]></title>
            <link>https://zenn.dev/akasan/articles/5697384428538b</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/5697384428538b</guid>
            <pubDate>Sun, 31 Aug 2025 07:18:14 GMT</pubDate>
            <content:encoded><![CDATA[今回はVertex AI Pipelinesに入門するための最もシンプルなところを紹介しようと思います。 Vertex AI Pipelinesとは？Vertex AI Pipelinesは、MLのパイプラインを記述することができるサービスになります。MLワークフローをおーけストレートすることにより、データ準備からモデルの学習、モニタリングまで全て自動で行うことができるようになります。MLOpsを実現するためにはMLのライフサイクルを可能な限り自動化するための取り組みが必要であり、そのための手段としてとても有益なツールとなります。パイプラインを記述するためにはKubeflow P...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PyPIで最もダウンロードされているライブラリ一覧みてみた]]></title>
            <link>https://zenn.dev/akasan/articles/b31578401de5e3</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/b31578401de5e3</guid>
            <pubDate>Sat, 30 Aug 2025 05:20:41 GMT</pubDate>
            <content:encoded><![CDATA[今回はPyPIで最もインストールされているライブラリが何か急に気になったので調べてみました。小方言としては以下のサイトを参照してみました。※ 今回参照したページの情報が必ずしも最新の情報とは限らないと思いますので、参考程度に見てもらえればと思いますhttps://pypistats.org/top 月間ダウンロード 1位 boto3boto3はAWSのSDKであり、AWSユーザかつPythonユーザであれば基本的に利用するのではないでしょうか？ダウンロード数が1,183,978,863ということで他のライブラリと一線を画すようなインストール数になっています。https:/...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[pyinstrumentを使ってPythonコードのプロファイリングをしてみた]]></title>
            <link>https://zenn.dev/akasan/articles/359a9d19a2c921</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/359a9d19a2c921</guid>
            <pubDate>Fri, 29 Aug 2025 13:38:31 GMT</pubDate>
            <content:encoded><![CDATA[今回はpyinstrumentを使ってPythonコードのプロファイリングをしてみました。自分が実装したコードのどこにボトルネックがあるかなどを分析するためのツールとしてどんな感じか調べてみました！ pyinstrumentとは？pyinstrumentはPython用のプロファイラで、コードの最適化を支援するツールです。プログラムの実行をするインフラ基盤の性能は向上しているものの、実装されているコードのパフォーマンスが悪ければ宝の持ち腐れになってしまいます。そこで、今回紹介するようなプロファイラが必要になってきます。プロファイラを利用することで、プログラムのどこに処理時間が多くか...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neovim 0.12（開発版）ではLSPクライアント経由でGitHub Copilotを使えるぞ（脱copilot.vim）]]></title>
            <link>https://blog.atusy.net/2025/08/29/copilot-via-nvim-lsp-client/</link>
            <guid isPermaLink="false">https://blog.atusy.net/2025/08/29/copilot-via-nvim-lsp-client/</guid>
            <pubDate>Fri, 29 Aug 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Neovim 0.12（開発版）にvim.lsp.inline_completionが実装されました。これと[copilot-language-server]を組み合わせると、GitHub Copilotによる補完を、LSPクライアント経由で利用できます。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私が情報収集するために利用している情報源公開]]></title>
            <link>https://zenn.dev/akasan/articles/c019ee769aadf1</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/c019ee769aadf1</guid>
            <pubDate>Thu, 28 Aug 2025 12:57:38 GMT</pubDate>
            <content:encoded><![CDATA[今回は、私が普段情報収集をする上で利用している情報源について共有しようと思います。特にMLエンジニアとしての色が濃いかなと思いますが、ぜひ参考にしてもらえればと思います。※ 紹介するいかなる媒体の回し者ではございません。単純に普段使ってて助かっているものの共有です ブログ媒体 MediumMediumは特に海外のエンジニアの情報発信を積極的にキャッチアップしたいということで利用しています。無料番だと全ての記事は読めないので、私はAnnual Medium membershipになっているので全ての記事を見れる状態です。特にプログラミング系の記事を多く読んでいますが、とても情報...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：claude codeに私のテックブログを評価させてみた]]></title>
            <link>https://zenn.dev/akasan/articles/af8b26620c9c98</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/af8b26620c9c98</guid>
            <pubDate>Wed, 27 Aug 2025 13:33:40 GMT</pubDate>
            <content:encoded><![CDATA[今回は私が今まで書いてきたZennの記事をclaude codeに読ませて、どんな記事を書いてきたかとかを分析させてみました。 claude codeへの指示今回claude codeへ指示を出すにあたり、まずはブログ一覧を取得させる必要があります。私はZennの記事をGitHubで管理していて、記事一覧はarticlesフォルダにおいています。なので指示の中でそのフォルダをみるように指定しています。今回claude codeに与えた指示は以下になっています。instructions.md## 分析articlesフォルダ以下に私のテックブログのマークダウンファイルがあり...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[paruzコマンドでArch Linux系のパッケージをあいまい検索]]></title>
            <link>https://blog.atusy.net/2025/08/27/fuzzy-find-pkgs-for-archlinux/</link>
            <guid isPermaLink="false">https://blog.atusy.net/2025/08/27/fuzzy-find-pkgs-for-archlinux/</guid>
            <pubDate>Wed, 27 Aug 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[paruzコマンドを使うとAURも含めてArch Linux系のパッケージをあいまい検索できて便利。PARU=pacman paruzでいけるよ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AIのColab Enterpriseのランタイムテンプレートを利用してみた]]></title>
            <link>https://zenn.dev/akasan/articles/454f57beb383e9</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/454f57beb383e9</guid>
            <pubDate>Tue, 26 Aug 2025 11:59:54 GMT</pubDate>
            <content:encoded><![CDATA[今回はVertex AI上で提供されているColab Enterpriseについて、ランタイムテンプレートの設定方法を調べてみました。 Colab Enterpriseとは？Colab EnterpriseはVertex AI上で提供されているマネージドなノートブック環境となります。ノートブックファイルを共有することで共同編集することができるほか、それぞれのユーザごとにランタイムを作成できます。また、Geminiによるコード補完機能があったり、BigQueryなどのGoogle Cloudのサービスと統合されているところも特徴です。詳細は以下のページに記載されているのでぜひ参照して...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BigQueryのMERGEステートメントについて]]></title>
            <link>https://zenn.dev/nedoko_dok0dko/articles/52a6a8e2412dcb</link>
            <guid isPermaLink="false">https://zenn.dev/nedoko_dok0dko/articles/52a6a8e2412dcb</guid>
            <pubDate>Tue, 26 Aug 2025 10:29:31 GMT</pubDate>
            <content:encoded><![CDATA[whatBigQueryのマージステートメントについて調べたことや知ったことを個人的にまとめたもの MERGEステートメントとはhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statementhttps://cloud.google.com/blog/ja/products/data-analytics/bigquery-explained-data-manipulation-dml別のテーブルと一致する値に基づいて以下のステートメントをまとめて実行できる機...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ミニマムかつ未来を見据えたGoogle Cloudアーキテクチャ]]></title>
            <link>https://zenn.dev/kamos/articles/poc_google_cloud</link>
            <guid isPermaLink="false">https://zenn.dev/kamos/articles/poc_google_cloud</guid>
            <pubDate>Tue, 26 Aug 2025 02:59:44 GMT</pubDate>
            <content:encoded><![CDATA[!この記事は人間が書き、AIにレビューしてもらいました はじめにAIによって開発が加速した現在、プロダクト開発においてアイデアを素早くプロダクトに落とし込み、実際に市場に展開することが重要になっています。しかしMVP(最小限の実用的製品)を立ち上げる際のクラウドインフラやアーキテクチャの選択は、その後のプロダクトの成長や運用に大きな影響を与えます。本格的な構成を最初期から採用することは立派ですが、MVPが成功するかわからないものに高コストなインフラを選択することはリスクが高いです。逆に、安易に無料枠や低コストなサービスを選択すると、将来的なスケーリングや機能追加が困難になりま...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ApacheおよびCNCFルーレットの実行をGitHub Actionsで実行できるようにした]]></title>
            <link>https://zenn.dev/akasan/articles/ef9e2919c312c1</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/ef9e2919c312c1</guid>
            <pubDate>Mon, 25 Aug 2025 10:51:07 GMT</pubDate>
            <content:encoded><![CDATA[私のテックブログでは、ApacheとCNCFについてブログネタを決めるためのルーレットというものを実装しています。今回はそのルーレットの実行環境をGitHub Actionsに移植したので共有します。 ApacheおよびCNCFルーレットとは？私は現在120日以上毎日何かしらをテックブログにしていますが、ネタ切れになったりすることもなくはないです。そのような状況に対応するため、一番初めに対応策としたのがApacheプロジェクトの中からランダムに一つプロジェクトw選択させるというものです。Apacheプロジェクトは300以上のプロジェクトがあるため、おかげさまでネタ切れにはまだ遭遇し...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CNCF様を引用してブログネタを決めるためのツールを作成しました]]></title>
            <link>https://zenn.dev/akasan/articles/42f5a1d2786ca5</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/42f5a1d2786ca5</guid>
            <pubDate>Sun, 24 Aug 2025 11:19:41 GMT</pubDate>
            <content:encoded><![CDATA[今回は、ついにApacheルーレットのCNCF版であるCNCFルーレットを作成しました。Apacheルーレットが全て完了するまで作る予定はなかったのですが、頑固にならずにやろうと思い作りました。なお、コードは基本的にApacheルーレットの時と一緒です。https://zenn.dev/akasan/articles/7e30ad266c02c4 データの用意CNCFのLandscapeページから全てのCNCFプロジェクトw取得するために、CNCFページをスクレイピングしてページを取得し、そこからCNCFプロジェクト名を取得するPythonコードを作成しました。なお、コードはcl...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Aimシリーズ：OptunaとPytorch Lightningを組み合わせたMNIST実験管理]]></title>
            <link>https://zenn.dev/akasan/articles/b965a302b7ea45</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/b965a302b7ea45</guid>
            <pubDate>Sat, 23 Aug 2025 14:39:10 GMT</pubDate>
            <content:encoded><![CDATA[今回はAimで実験管理を行いつつ、OptunaとPytorch Lightningを使ってMNISTの分類をしてみました。ぜひ過去の以下の記事を参考にしてください。https://zenn.dev/akasan/articles/6221f74bea622dhttps://zenn.dev/akasan/articles/a75361d039906f 早速実装 環境構築uvを使って以下で環境を構築します。uv init aim_optuna_lightning_mnist -p 3.12cd aim_optuna_lightning_mnistuv add aim l...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[claude commandのスラッシュコマンドでコーディングの癖を検知させてみた]]></title>
            <link>https://zenn.dev/akasan/articles/846cd9a610d5a4</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/846cd9a610d5a4</guid>
            <pubDate>Fri, 22 Aug 2025 14:38:31 GMT</pubDate>
            <content:encoded><![CDATA[今回はclaude codeのスラッシュコマンドを実装して、指定したファイルからコーディングの癖を見抜く仕組みを入れてみました。※ 出張中につき超短編です スラッシュコマンドの実装以下のようなスラッシュコマンドを作りました。ファイルは~/.claude/commands/kuse.mdです。内容は極めてシンプルで、指定されたファイルをよんでただただ癖を見抜かせるだけです。- 指定されたファイルから、私のコーディングの癖を見出してください- 癖の中でベストプラクティスに沿っていないコーディングがあれば指摘してください 使ってみた例えば以下のようなmain.pyを作ってみま...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ RustでLinuxのシグナル処理とプロセス間通信をしてみた]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/08/22/155856</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/08/22/155856</guid>
            <pubDate>Fri, 22 Aug 2025 06:58:56 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前回の記事「RustでLinuxプロセス管理をしてみた」の続編として、今回はシグナル処理とプロセス間通信（IPC）について解説します。これらの技術は、システムプログラミングの根幹をなす重要な概念です。doc.rust-lang.orgサンプルコードはこちらに配置しておきます。github.com2025年の最新動向2025年現在、Rustエコシステムは大きな転換期を迎えています。Linux 6.13が2025年1月にリリースされ、Rustサポートが「転換点」に到達しました。また、非同期ランタイムの世界では、async-stdが2025年3月に廃止されることが決まり、Tokioが事実上の標準となっています。さらに、Rust 1.85ではasync closuresが安定化され、より表現力豊かな非同期処理が可能になりました。1. 基礎知識書籍はこちらがめちゃくちゃに詳しいのでオススメです。ふつうのLinuxプログラミング 第2版　Linuxの仕組みから学べるgccプログラミングの王道作者:青木 峰郎SBクリエイティブAmazonプロセスとはプロセスは「実行中のプログラムのインスタンス」です。皆さんが日常的に使うWebブラウザのタブやターミナルのセッションは、すべてプロセスとして動作しています。各プロセスは独立したメモリ空間を持ち、他のプロセスから直接アクセスすることはできません。これがシステムの安定性と安全性を保証していますが、同時にプロセス間でデータをやり取りする特別な仕組みが必要になる理由でもあります。シグナルとはシグナルは、プロセス間の非同期通知メカニズムです。電話の着信音のように、プロセスに「何か重要なことが起きた」と割り込みで知らせる仕組みだと考えると分かりやすいでしょう。主要なシグナルと実際の用途： シグナル  番号  用途  実例  SIGTERM  15  正常終了要求  systemctl stopで送信される  SIGKILL  9  強制終了  kill -9、OOMキラー  SIGINT  2  割り込み  Ctrl+Cを押したとき  SIGHUP  1  設定再読み込み  nginxやsshdの設定リロード  SIGUSR1/2  10/12  カスタム用途  アプリ固有の動作トリガー シグナルには重要な特徴がいくつかあります。まず非同期性という性質があり、いつ届くか予測できません。また割り込みとして動作するため、実行中の処理を中断して処理されます。そしてシンプルな仕組みで、シグナル番号以外の追加情報を送ることはできません。rust-cli.github.ioプロセス間通信（IPC）とはIPCは、独立したプロセス同士がデータをやり取りするための仕組みです。それぞれの方式には特徴があり、用途に応じて使い分けます： 方式  特徴  実際の使用例  パイプ  単方向、親子プロセス間  ls | grepなどのシェルパイプ  名前付きパイプ  双方向、無関係なプロセス間も可  ログ収集デーモンへのデータ送信  Unix Domain Socket  双方向、高速、信頼性高  Docker、systemd、PostgreSQL  共有メモリ  最速、同期が複雑  データベースのバッファプール  メッセージキュー  非同期、順序保証  ジョブキューシステム 2. シンプルなシグナル処理Ctrl+Cを検知して安全に終了最もシンプルな例から始めてみましょう。Ctrl+Cを押したときに、きちんと後処理をしてから終了するプログラムです。use std::sync::atomic::{AtomicBool, Ordering};use std::sync::Arc;use std::thread;use std::time::Duration;fn main() {    println!("プログラム開始（Ctrl+Cで終了）");        // 実行中フラグ（スレッド間で安全に共有）    let running = Arc::new(AtomicBool::new(true));    let r = running.clone();        // Ctrl+Cハンドラーを設定    ctrlc::set_handler(move || {        println!("\n終了シグナルを受信しました");        r.store(false, Ordering::SeqCst);    }).expect("シグナルハンドラーの設定に失敗");        // メインループ    let mut counter = 0;    while running.load(Ordering::SeqCst) {        counter += 1;        println!("処理中... カウント: {}", counter);        thread::sleep(Duration::from_secs(1));    }        println!("プログラムを安全に終了しました");}このコードにはいくつかの重要なポイントがあります。まずAtomicBoolを使ってスレッド間で安全にフラグを共有しています。シグナルハンドラーはいつ呼ばれるか分からないため、アトミック操作が必要になります。そしてループを抜けてから終了処理を行うことで、データの整合性を保っています。docs.rsgithub.com複数のシグナルを処理実際のサーバーアプリケーションでは、複数のシグナルを適切に処理する必要があります。use signal_hook::{consts::signal::*, iterator::Signals};use std::{error::Error, thread, time::Duration};fn main() -> Result<(), Box<dyn Error>> {    let mut signals = Signals::new(&[SIGTERM, SIGINT, SIGHUP])?;        thread::spawn(move || {        for sig in signals.forever() {            match sig {                SIGTERM | SIGINT => {                    println!("終了シグナルを受信");                    std::process::exit(0);                }                SIGHUP => {                    println!("設定再読み込み");                }                _ => unreachable!(),            }        }    });        // メイン処理    loop {        println!("作業中...");        thread::sleep(Duration::from_secs(2));    }}docs.rsgithub.com3. プロセス間通信の基礎シンプルなパイプ通信親プロセスから子プロセスへメッセージを送る基本的な例です。use std::io::{Write, Read};use std::process::{Command, Stdio};fn main() -> std::io::Result<()> {    // catコマンドは標準入力をそのまま標準出力に出力    let mut child = Command::new("cat")        .stdin(Stdio::piped())        .stdout(Stdio::piped())        .spawn()?;        // 子プロセスに書き込み    if let Some(mut stdin) = child.stdin.take() {        stdin.write_all(b"Hello from Rust!\n")?;    }        // 結果を読み取り    let output = child.wait_with_output()?;    println!("受信: {}", String::from_utf8_lossy(&output.stdout));        Ok(())}パイプには特徴的な性質があります。まず単方向通信であり、データは一方向にのみ流れます。またバッファリング機能があり、OSが自動的にバッファを管理してくれます。そしてブロッキング動作をするため、読み込み側は書き込みを待つことになります。docs.rsUnix Domain Socketより本格的な双方向通信の例です。多くのシステムソフトウェアが採用している方式です。Unix Domain Socketには多くの利点があります。双方向通信が可能で、クライアント・サーバー間で自由にやり取りできます。また、ネットワークスタックを通らないため高速に動作します。そしてファイルシステム上のパスとして存在するため、アクセス制御が簡単に行えます。4. デバッグツールの活用詳解 システム・パフォーマンス 第2版作者:Brendan Greggオーム社Amazonシステムプログラミングにおいて、問題を解決するには、まず問題を観察できなければならないという原則があります。特にシグナル処理やIPCのような非同期的な動作は、従来のprint文デバッグでは限界があります。そこで重要になるのが可観測性（Observability）という概念です。効果的なデバッグには階層的なアプローチが必要です。まずアプリケーション層で何が起きているかを把握し、次にシステムコール層まで掘り下げ、必要に応じてカーネル層まで観察します。各層に適したツールを使い分けることで、最小のオーバーヘッドで最大の洞察を得ることができます。また、動的トレーシングと静的トレーシングを使い分けることも重要です。straceのような動的トレーシングツールは実行中のプロセスをリアルタイムで観察でき、rr-debuggerのような記録再生型ツールは時間を巻き戻して問題の根本原因を特定できます。これらを組み合わせることで、再現困難なバグも確実に捕捉できるようになります。strace - システムコールトレースシグナル処理やIPCのデバッグには、システムコールレベルでの動作確認が不可欠です。# シグナル関連のシステムコールのみ表示strace -e trace=signal,sigaction,kill,pause cargo run# 実際の出力例rt_sigaction(SIGINT, {sa_handler=0x5555555, ...}, NULL, 8) = 0--- SIGINT {si_signo=SIGINT, si_code=SI_KERNEL} ---rt_sigreturn({mask=[]}) = 0straceを使うと様々な情報が見えてきます。シグナルハンドラーの登録状況（sigaction）、シグナルの送受信タイミング、ブロックされたシグナル、そしてシステムコールの引数と戻り値などを確認できます。strace.iorr-debugger（最強のデバッグツール）rrは、GDBを拡張して作られたデバッガで、プログラムの実行を記録し、逆方向にステップ実行できます。# プログラムの実行を記録rr record ./target/debug/my_program# rust-gdbを使って再生rr replay -d rust-gdb# リバース実行のコマンド(rr) reverse-continue  # 逆方向にcontinue(rr) reverse-next      # 逆方向にnextrrが強力な理由はいくつかあります。まず100%再現性があり、非決定的な動作も完全に再現できます。また逆実行機能により、エラーの原因を遡って調査できます。そして低オーバーヘッドで動作するため、実用的な速度で記録が可能です。特にシステムプログラミングでは、「たまにしか起きないエラー」や「データ競合」のデバッグで威力を発揮します。rr-project.orgtokio-console - 非同期ランタイムデバッグ非同期Rustアプリケーションのデバッグには、tokio-consoleが非常に有用です。タスクの状態、実行時間、リソース使用状況をリアルタイムで監視できます。# tokio-consoleをインストールcargo install --locked tokio-console# アプリケーション起動（別ターミナル）RUSTFLAGS="--cfg tokio_unstable" cargo run# tokio-consoleで監視tokio-consolegithub.com5. グレイスフルシャットダウン実際のサービスで必要な、適切な終了処理の実装例を見てみましょう。グレイスフルシャットダウンが重要な理由は複数あります。まずデータの整合性を保つため、処理中のタスクを完了してから終了する必要があります。またリソースの解放として、ファイルやソケットを適切にクローズしなければなりません。そして状態の保存により、次回起動時に必要な情報を保存することも重要です。実装する際のポイントとしては、まず新規タスクの受付を停止し、新しい仕事を受け付けないようにします。次に既存タスクの完了を待機し、実行中の処理を最後まで実行させます。その後リソースのクリーンアップを行い、ファイルやネットワーク接続を閉じます。最後に統計情報の出力を行い、ログに実行結果を記録します。6. Tokioを使った非同期グレイスフルシャットダウンモダンなRustアプリケーションでは、Tokioを使った非同期処理が主流です。use tokio::signal;use tokio_util::sync::CancellationToken;#[tokio::main]async fn main() {    let token = CancellationToken::new();        // Ctrl+Cハンドラー    let shutdown_token = token.clone();    tokio::spawn(async move {        signal::ctrl_c().await.unwrap();        println!("シャットダウン開始");        shutdown_token.cancel();    });        // メインループ    loop {        tokio::select! {            _ = token.cancelled() => {                println!("終了処理中...");                break;            }            _ = do_work() => {                // 通常の処理            }        }    }}async fn do_work() {    // 非同期処理}CancellationTokenには多くの利点があります。階層的なキャンセルが可能で、親トークンをキャンセルすると子もキャンセルされます。また協調的な仕組みにより、各タスクが自分のタイミングで終了できます。そして非同期対応により、async/awaitと自然に統合されています。tokio.rsdocs.rsgithub.comdocs.rstokio.rs7. nixクレートでシステムコールを扱うRustでは、nixクレートを使って安全にUnixシステムコールを扱うことができます。libcクレートの生のAPIをラップし、Rust的な安全なインターフェースを提供しています。use nix::sys::signal::{self, Signal};use nix::unistd::{fork, ForkResult};match fork() {    Ok(ForkResult::Parent { child }) => {        println!("親プロセス、子PID: {}", child);    }    Ok(ForkResult::Child) => {        println!("子プロセス");    }    Err(_) => eprintln!("fork失敗"),}nixクレートを使うことで、エラーハンドリングが適切に行われ、メモリ安全性が保証されます。生のシステムコールを直接扱う必要がなくなり、より安全なコードが書けるようになります。docs.rsgithub.com8. 2025年の新機能：Async ClosuresRust 1.85.0で安定化されたasync closuresを使うと、より柔軟な非同期処理が書けます。async fn retry_with_backoff<F, Fut>(    mut f: F,     max_retries: u32,) -> Result<String>where    F: FnMut() -> Fut,    Fut: Future<Output = Result<String>>,{    for attempt in 1..=max_retries {        match f().await {            Ok(result) => return Ok(result),            Err(e) if attempt < max_retries => {                let backoff = Duration::from_secs(2_u64.pow(attempt - 1));                sleep(backoff).await;            }            Err(e) => return Err(e),        }    }    unreachable!()}async closuresを使うメリットは多岐にわたります。まず簡潔な記述が可能になり、非同期処理を関数引数として渡せるようになります。また型安全であるため、コンパイル時に型チェックが行われます。そして柔軟な制御フローにより、リトライやタイムアウトの実装が簡単になります。実装パターンの選び方シグナル処理の選択基準シグナル処理の実装方法を選ぶ際は、用途に応じて適切なツールを選択することが重要です。単純な終了処理であればctrlcクレートで十分です。複数のシグナルを扱う必要がある場合はsignal-hookを使用します。そして非同期処理と組み合わせる場合は、Tokioのsignalモジュールが最適です。IPC方式の選択基準IPC方式も同様に、用途に応じて選択します。親子プロセス間の単純な通信であればパイプが適しています。高速な双方向通信が必要な場合はUnix Domain Socketを選びます。大量データの共有には共有メモリが最適で、非同期メッセージングにはメッセージキューが向いています。まとめこの記事では、Rustでのシグナル処理とプロセス間通信について、基礎から実践まで段階的に解説しました。重要なポイント今回学んだ重要なポイントを振り返ってみましょう。まず、シグナルは非同期であり、いつ届くか分からないためアトミック操作が必要です。IPCは用途に応じて選ぶ必要があり、速度、双方向性、複雑さのトレードオフを考慮します。グレイスフルシャットダウンはデータの整合性を保つために必須です。straceやrr-debuggerなどのデバッグツールを活用することで、問題を効率的に解決できます。そして、async closuresやCancellationTokenなどの最新機能を活用することで、保守性を向上させることができます。各IPC方式の使い分け実際の開発では、各IPC方式を適切に使い分けることが重要です。パイプはシェルスクリプトとの連携や親子プロセス間の単純な通信に適しています。名前付きパイプはログ収集や順序保証が必要な場合に使います。Unix Domain Socketは高速な双方向通信やサービス間連携に最適です。共有メモリは大量データの高速処理やリアルタイム性が必要な場合に選択します。次のステップこの基礎を踏まえて、さらに高度な実装に挑戦することができます。分散システムへの拡張としてgRPCやメッセージキューの実装、コンテナ環境でのIPC最適化、リアルタイムシステムでの応用、そしてマイクロサービスアーキテクチャでの実装などが考えられます。完全なソースコードはGitHubリポジトリで公開しています。前回の記事「RustでLinuxプロセス管理をしてみた」と合わせて読むことで、Rustでのシステムプログラミングの基礎がしっかりと身につきます。Linuxカーネルプログラミング 第2版作者:Kaiwan N. Billimoria,武内 覚（翻訳）,大岩 尚宏（翻訳）オライリージャパンAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Aimシリーズ：入門してみた]]></title>
            <link>https://zenn.dev/akasan/articles/6221f74bea622d</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/6221f74bea622d</guid>
            <pubDate>Thu, 21 Aug 2025 12:28:55 GMT</pubDate>
            <content:encoded><![CDATA[今回から、Aimという実験管理ツールに入門してみます。※ 出張中につき、短編になります。 Aimとは？Aimとはオープンソースの実験管理ツールになります。Aimを利用すると実験を実行し、その結果発生する様々なメタデータを一元的に取り扱い、グラフィカルに解析することができます。Aimを利用することで以下のようなことが実現できます。MLパイプラインのロギングを可能にするUIを通してメタデータを比較分析できるML学習を効率的に実行可能実験管理のオーガナイズができるhttps://github.com/aimhubio/aim/ 早速使ってみる今回はGitHub上で提...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでLinuxプロセス管理をしてみた]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/08/21/161234</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/08/21/161234</guid>
            <pubDate>Thu, 21 Aug 2025 07:12:34 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこれまでPythonとGoでプロセス管理システムを実装してきましたが、今回Rustでも実装してみました。各言語にはそれぞれ得意不得意があり、プロジェクトの要件によって最適な選択は変わります。変なとこがあれば教えてください。この記事では、Rustでプロセス管理システムを実装した経験を共有します。標準ライブラリのstd::processだけでは不十分な要件があったため、より高度な制御が可能な実装を行いました。doc.rust-lang.orgサンプルコードはこちらに配置しておきます。github.comPython、Go、Rustでの実装経験から見えた違い3つの言語でプロセス管理を実装してきた経験から、それぞれの特徴をまとめます。Pythonでの実装subprocessモジュールは高レベルで使いやすいasyncioとの組み合わせで非同期処理も可能GILの影響で真の並行性には制限があるメモリ使用量が多く、長時間稼働で増加傾向Goでの実装os/execパッケージはシンプルで直感的goroutineによる並行処理が強力エラーハンドリングが冗長になりがちGCのオーバーヘッドが気になるケースがあるRustでの実装所有権システムによるリソース管理の確実性ゼロコスト抽象化による高パフォーマンス型システムによる実行前のバグ検出学習曲線は確かに急だが、長期的なメンテナンス性は高いRustの所有権システムとゼロコスト抽象化により、今回の要件を満たす堅牢なシステムを構築できました。特に、コンパイル時にリソースリークを防げる点、SendとSyncトレイトによる安全な並行処理、システムコールのオーバーヘッドが最小限である点が優れていました。1. まずはstd::processから始めよう最初の一歩：シンプルなコマンド実行Rustでプロセスを扱う最も簡単な方法は、標準ライブラリのstd::process::Commandを使うことです。use std::process::Command;fn main() {    // 最もシンプルな例    let output = Command::new("echo")        .arg("Hello, Rust!")        .output()        .expect("Failed to execute command");        println!("stdout: {}", String::from_utf8_lossy(&output.stdout));}パイプを使った入出力制御もう少し複雑な例として、子プロセスとパイプで通信してみましょう。use std::io::Write;use std::process::{Command, Stdio};fn main() -> std::io::Result<()> {    let mut child = Command::new("cat")        .stdin(Stdio::piped())        .stdout(Stdio::piped())        .spawn()?;        // 標準入力に書き込み    if let Some(mut stdin) = child.stdin.take() {        stdin.write_all(b"Hello from parent process!\n")?;    }        // 出力を取得    let output = child.wait_with_output()?;    println!("Child said: {}", String::from_utf8_lossy(&output.stdout));        Ok(())}std::processの限界しかし、実際のプロジェクトを進めていくと、std::processだけでは対応できない要件が出てきました。// ❌ std::processではできないこと// 1. 特定のシグナル（SIGTERM、SIGUSR1など）を送信できない// child.kill() はSIGKILLのみ// 2. プロセスグループの管理ができない// 複数の子プロセスをグループとして扱えない// 3. fork()が使えない// Unix系OSの基本的なプロセス生成方法が使えない// 4. 細かいリソース制限（CPU時間、メモリ量など）の設定ができない2. nixクレートの導入：なぜ必要なのかnixクレートとはnixクレートは、Unix系システムコールのRustラッパーです。std::processでは提供されていない低レベルな制御が可能になります。docs.rs[dependencies]nix = { version = "0.27", features = ["process", "signal"] }最初のnixプログラム：fork()の基本まずは最も基本的なfork()から始めましょう。fork()は現在のプロセスを複製し、親プロセスと子プロセスの2つに分岐します。use nix::unistd::{fork, ForkResult};fn main() -> Result<(), Box<dyn std::error::Error>> {    println!("親プロセス開始: PID={}", std::process::id());        // fork()は unsafe - プロセスの複製は危険を伴うため    match unsafe { fork() }? {        ForkResult::Parent { child } => {            // 親プロセスのコード            println!("親: 子プロセス {} を作成しました", child);        }        ForkResult::Child => {            // 子プロセスのコード            println!("子: 私は新しいプロセスです！PID={}", std::process::id());            std::process::exit(0); // 子プロセスは明示的に終了        }    }        Ok(())}なぜunsafeなのか？fork()がunsafeな理由を理解することは重要です。メモリの複製: fork時点のメモリ状態が複製されるマルチスレッドとの相性問題: スレッドがある状態でforkすると予期しない動作リソースの重複: ファイルディスクリプタなどが複製される3. 段階的に学ぶnixクレートの機能ステップ1: シグナル送信std::processではできなかったシグナル送信を実装してみます。use nix::sys::signal::{kill, Signal};use nix::unistd::Pid;use std::process::Command;use std::thread;use std::time::Duration;fn main() -> Result<(), Box<dyn std::error::Error>> {    // 子プロセスを起動    let mut child = Command::new("sleep")        .arg("30")        .spawn()?;        let pid = Pid::from_raw(child.id() as i32);    println!("子プロセス起動: PID={}", pid);        // 2秒待ってからSIGTERMを送信    thread::sleep(Duration::from_secs(2));    println!("SIGTERMを送信...");    kill(pid, Signal::SIGTERM)?;        // プロセスの終了を確認    let status = child.wait()?;    println!("子プロセス終了: {:?}", status);        Ok(())}ステップ2: プロセスの終了を待つ（ゾンビプロセスの防止）プロセスが終了しても、親がwait()しないとゾンビプロセスになります。nixを使った適切な処理方法を見てみましょう。use nix::sys::wait::waitpid;use nix::unistd::{fork, ForkResult};fn main() -> Result<(), Box<dyn std::error::Error>> {    match unsafe { fork() }? {        ForkResult::Parent { child } => {            println!("親: 子プロセス {} の終了を待機", child);                        // waitpid()で子プロセスの終了を待つ            // これによりゾンビプロセスを防ぐ            let status = waitpid(child, None)?;            println!("親: 子プロセスが終了 - {:?}", status);        }        ForkResult::Child => {            println!("子: 2秒間作業します...");            std::thread::sleep(std::time::Duration::from_secs(2));            println!("子: 作業完了！");            std::process::exit(0);        }    }        Ok(())}ステップ3: プロセスグループの管理複数のプロセスをグループとして管理し、まとめてシグナルを送信できます。use nix::sys::signal::{killpg, Signal};use nix::unistd::{fork, setpgid, ForkResult, Pid};fn main() -> Result<(), Box<dyn std::error::Error>> {    match unsafe { fork() }? {        ForkResult::Parent { child } => {            // 子プロセスを新しいプロセスグループのリーダーにする            setpgid(child, child)?;            println!("親: プロセスグループ {} を作成", child);                        // さらに子プロセスを同じグループに追加（省略）                        // グループ全体にシグナルを送信            std::thread::sleep(std::time::Duration::from_secs(2));            println!("親: グループ全体にSIGTERMを送信");            killpg(child, Signal::SIGTERM)?;        }        ForkResult::Child => {            // 新しいプロセスグループを作成            let my_pid = nix::unistd::getpid();            setpgid(my_pid, my_pid)?;                        // グループ内で作業            loop {                std::thread::sleep(std::time::Duration::from_secs(1));                println!("子: 作業中...");            }        }    }        Ok(())}4. 実用的な実装：ProcessGuardパターンRAIIを活用した安全なプロセス管理実際のプロジェクトでは、プロセスのライフサイクルを確実に管理する必要があります。こういうのは世の中に知見がたくさんあるのでちゃんと調べて行きましょう。今回はRustのRAII（Resource Acquisition Is Initialization）パターンを活用しましょう。use nix::sys::signal::{kill, Signal};use nix::unistd::Pid;use std::process::{Child, Command};/// プロセスの自動クリーンアップを保証する構造体pub struct ProcessGuard {    child: Option<Child>,    name: String,}impl ProcessGuard {    pub fn new(command: &str) -> std::io::Result<Self> {        let child = Command::new(command).spawn()?;        Ok(Self {            child: Some(child),            name: command.to_string(),        })    }        pub fn wait(&mut self) -> std::io::Result<std::process::ExitStatus> {        if let Some(mut child) = self.child.take() {            child.wait()        } else {            Err(std::io::Error::new(                std::io::ErrorKind::Other,                "Process already terminated"            ))        }    }}impl Drop for ProcessGuard {    fn drop(&mut self) {        if let Some(mut child) = self.child.take() {            // まだ実行中かチェック            if child.try_wait().ok().flatten().is_none() {                eprintln!("Terminating process: {}", self.name);                                // まずSIGTERMで優雅に終了を試みる                let pid = Pid::from_raw(child.id() as i32);                let _ = kill(pid, Signal::SIGTERM);                                // 少し待つ                std::thread::sleep(std::time::Duration::from_millis(500));                                // まだ生きていればSIGKILL                if child.try_wait().ok().flatten().is_none() {                    let _ = child.kill();                }                                // 必ずwait()してゾンビプロセスを防ぐ                let _ = child.wait();            }        }    }}// 使用例fn main() -> std::io::Result<()> {    {        let mut guard = ProcessGuard::new("sleep")?;        println!("プロセスを起動しました");                // スコープを抜けると自動的にクリーンアップ    } // ここでDropが呼ばれる        println!("プロセスは自動的に終了されました");    Ok(())}5. セキュリティ：入力検証とサニタイゼーションコマンドインジェクション対策ユーザー入力を含むコマンド実行は非常に危険です。悪意がなくても失敗する可能性があるものはいつか失敗します。ちなみに普通に入力は適切な検証が必要です。use thiserror::Error;#[derive(Error, Debug)]pub enum ProcessError {    #[error("Invalid input: {0}")]    InvalidInput(String),        #[error("Security violation: {0}")]    SecurityViolation(String),        #[error("IO error: {0}")]    Io(#[from] std::io::Error),}/// 安全な入力検証pub fn validate_input(input: &str) -> Result<&str, ProcessError> {    // 危険な文字をチェック    const DANGEROUS_CHARS: &[char] = &[        ';', '&', '|', '$', '`', '>', '<',         '(', ')', '{', '}', '\n', '\r', '\0'    ];        for &ch in DANGEROUS_CHARS {        if input.contains(ch) {            return Err(ProcessError::SecurityViolation(                format!("Dangerous character '{}' detected", ch)            ));        }    }        // パストラバーサル対策    if input.contains("..") || input.starts_with('~') {        return Err(ProcessError::SecurityViolation(            "Path traversal detected".into()        ));    }        // コマンド置換パターンをチェック    let dangerous_patterns = ["$(", "${", "&&", "||"];    for pattern in dangerous_patterns {        if input.contains(pattern) {            return Err(ProcessError::SecurityViolation(                format!("Dangerous pattern '{}' detected", pattern)            ));        }    }        Ok(input)}// 使用例fn safe_execute(user_input: &str) -> Result<(), ProcessError> {    let safe_input = validate_input(user_input)?;        let output = std::process::Command::new("echo")        .arg(safe_input)        .output()?;        println!("Safe output: {}", String::from_utf8_lossy(&output.stdout));    Ok(())}リソース制限の設定www.linkedin.comプロセスが使用できるリソースを制限することで、システム全体への影響を防げます。#[cfg(target_os = "linux")]use nix::sys::resource::{setrlimit, Resource};#[cfg(target_os = "linux")]fn set_resource_limits() -> nix::Result<()> {    // CPU時間を10秒に制限    setrlimit(Resource::RLIMIT_CPU, 10, 10)?;        // メモリを100MBに制限    let memory_limit = 100 * 1024 * 1024; // 100MB in bytes    setrlimit(Resource::RLIMIT_AS, memory_limit, memory_limit)?;        // プロセス数を50に制限    setrlimit(Resource::RLIMIT_NPROC, 50, 50)?;        Ok(())}6. 高度な実装例：プロセスプール複数のワーカープロセスを管理実際のシステムでは、複数のワーカープロセスを効率的に管理する必要があります。use std::sync::{Arc, Mutex};use std::collections::HashMap;use nix::unistd::Pid;pub struct ProcessPool {    workers: Arc<Mutex<HashMap<Pid, ProcessGuard>>>,    max_workers: usize,}impl ProcessPool {    pub fn new(max_workers: usize) -> Self {        Self {            workers: Arc::new(Mutex::new(HashMap::new())),            max_workers,        }    }        pub fn spawn_worker(&self, command: &str) -> Result<Pid, ProcessError> {        let mut workers = self.workers.lock().unwrap();                if workers.len() >= self.max_workers {            return Err(ProcessError::InvalidInput(                "Maximum workers reached".into()            ));        }                let child = std::process::Command::new(command)            .spawn()            .map_err(|e| ProcessError::Io(e))?;                let pid = Pid::from_raw(child.id() as i32);        let guard = ProcessGuard {            child: Some(child),            name: command.to_string(),        };                workers.insert(pid, guard);        Ok(pid)    }        pub fn terminate_worker(&self, pid: Pid) -> Result<(), ProcessError> {        let mut workers = self.workers.lock().unwrap();                if let Some(mut guard) = workers.remove(&pid) {            guard.wait()?;            Ok(())        } else {            Err(ProcessError::InvalidInput(                "Worker not found".into()            ))        }    }        pub fn active_workers(&self) -> usize {        self.workers.lock().unwrap().len()    }}// 使用例fn main() -> Result<(), Box<dyn std::error::Error>> {    let pool = ProcessPool::new(5);        // ワーカーを起動    for i in 0..3 {        let pid = pool.spawn_worker("sleep")?;        println!("Started worker {}: PID={}", i, pid);    }        println!("Active workers: {}", pool.active_workers());        // プールがスコープを抜けると全ワーカーが自動終了    Ok(())}7. 非同期処理との統合（Tokio）Tokioを使った非同期プロセス管理docs.rs大規模なシステムでは、非同期処理と組み合わせることが重要です。use tokio::process::Command;use tokio::time::{timeout, Duration};#[tokio::main]async fn main() -> Result<(), Box<dyn std::error::Error>> {    // 非同期でコマンド実行    let output = Command::new("echo")        .arg("Hello, async!")        .output()        .await?;        println!("Output: {}", String::from_utf8_lossy(&output.stdout));        // タイムアウト付き実行    let result = timeout(        Duration::from_secs(2),        Command::new("sleep").arg("10").output()    ).await;        match result {        Ok(Ok(_)) => println!("Command completed"),        Ok(Err(e)) => println!("Command failed: {}", e),        Err(_) => println!("Command timed out"),    }        Ok(())}8. デバッグとテスト単体テストの実装プロセス管理のコードは、適切にテストすることが重要です。#[cfg(test)]mod tests {    use super::*;    use std::time::Instant;        #[test]    fn test_input_validation() {        // 安全な入力        assert!(validate_input("hello.txt").is_ok());                // 危険な入力        assert!(validate_input("; rm -rf /").is_err());        assert!(validate_input("$(whoami)").is_err());        assert!(validate_input("../../../etc/passwd").is_err());    }        #[test]    fn test_process_timeout() {        let start = Instant::now();                let mut guard = ProcessGuard::new("sleep").unwrap();                // 1秒でタイムアウト        std::thread::sleep(std::time::Duration::from_secs(1));        drop(guard); // 強制的にDropを呼ぶ                // 2秒以内に終了していることを確認        assert!(start.elapsed() < std::time::Duration::from_secs(2));    }        #[test]    fn test_process_pool() {        let pool = ProcessPool::new(2);                // 最大数まで起動できることを確認        assert!(pool.spawn_worker("true").is_ok());        assert!(pool.spawn_worker("true").is_ok());                // 最大数を超えるとエラー        assert!(pool.spawn_worker("true").is_err());    }}統合テスト実際のプロセスを起動して動作を確認します。// tests/integration_test.rsuse std::process::Command;use std::time::Duration;#[test]fn test_zombie_prevention() {    // 子プロセスを起動    let mut child = Command::new("sh")        .arg("-c")        .arg("sleep 0.1")        .spawn()        .expect("Failed to spawn");        // プロセスの終了を待つ    let status = child.wait().expect("Failed to wait");    assert!(status.success());        // psコマンドでゾンビプロセスがないことを確認    let output = Command::new("ps")        .arg("aux")        .output()        .expect("Failed to run ps");        let ps_output = String::from_utf8_lossy(&output.stdout);    assert!(!ps_output.contains("<defunct>"));}まとめRustでプロセス管理システムを実装する際のポイントをまとめます。std::processから始める簡単な用途には標準ライブラリで十分パイプや環境変数の設定も可能多くの場合、これだけで要件を満たせるnixクレートが必要な場面シグナルの細かい制御が必要プロセスグループの管理fork()やexec()の直接的な使用リソース制限の設定実装のベストプラクティスRAIIパターンの活用: ProcessGuardでリソースの自動解放入力検証の徹底: コマンドインジェクション対策エラーハンドリング: thiserrorで構造化されたエラーテストの充実: 単体テストと統合テストの両方Rustの優位性メモリ安全性: 所有権システムによる確実なリソース管理ゼロコスト抽象化: 高レベルAPIでも性能劣化なし型システム: コンパイル時のバグ検出並行性: Send/Syncトレイトによる安全な並行処理長期運用するシステムでは、これらの特性が大きなメリットとなります。特に、ゾンビプロセスの防止やリソースリークの回避が、コンパイル時に保証される点は、運用の安定性に大きく貢献します。The Linux Programming Interface: A Linux and UNIX System Programming Handbook作者:Kerrisk, MichaelNo Starch PressAmazonLinuxプログラミングインタフェース作者:Michael KerriskオライリージャパンAmazon今後は、分散システムでのプロセス管理や、より高度なモニタリング機能の実装を予定しています。Rustのエコシステムは急速に発展しており、プロセス管理の分野でも新しい可能性が広がっています。github.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DockerからECSへ 〜 AWSの海に出る前に知っておきたいこと 〜]]></title>
            <link>https://speakerdeck.com/ota1022/dockerkaraecshe-awsnohai-nichu-ruqian-nizhi-tuteokitaikoto</link>
            <guid isPermaLink="false">https://speakerdeck.com/ota1022/dockerkaraecshe-awsnohai-nichu-ruqian-nizhi-tuteokitaikoto</guid>
            <pubDate>Thu, 21 Aug 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[JAWS-UGコンテナ支部 入門編 #8 初心者大歓迎LT大会のLT登壇資料です。https://jawsug-container.connpass.com/event/361918/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dataprepを使ってデータ編集レシピを作ってみた]]></title>
            <link>https://zenn.dev/akasan/articles/ebd4be0540c5f2</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/ebd4be0540c5f2</guid>
            <pubDate>Wed, 20 Aug 2025 11:51:38 GMT</pubDate>
            <content:encoded><![CDATA[今回はDataprepを利用してTitanicデータセットを加工してみようと思います。 Dataprepとは？Dataprepとは、Alteryxと共同で構築されたGoogleのセルフサービスデータ準備ツールです。BigQueryなどのデータソースからデータを取得してレシピを通して加工し、再度BigQueryやCloud Storageなどに格納するといったデータの処理を実行することができます。Dataprepを利用することでデータのクリーニングや変換などを一つのプラットフォームで可視化しながら実行できるため、データエンジニアリングに大きく貢献します。なお、今回の検証ではトライア...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BigQueryMLを用いてtitanic生存者予測タスクをやってみた]]></title>
            <link>https://zenn.dev/akasan/articles/4ac96d153149c7</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/4ac96d153149c7</guid>
            <pubDate>Tue, 19 Aug 2025 11:28:44 GMT</pubDate>
            <content:encoded><![CDATA[今回はタイトルにあるように、BigQueryMLを用いてtitanicの生存者予測タスクに取り組めるモデルを開発してみました。なお、今回BigQueryMLを利用するにあたり、クエリの生成にはGPT-5を存分に利用しました。 実際にやってみる！ BigQueryにデータセットをアップロードするまずはtitanicのデータセットをBigQueryにアップロードします。以下の手順でデータを準備します。titanicデータセットをダウンロードするBigQueryにてtitanicデータセットを作成するtitanicデータセットに対してtrainとtestというテーブルを登録...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AIのModel Garden調査　〜基盤モデル〜]]></title>
            <link>https://zenn.dev/akasan/articles/adca86f2802773</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/adca86f2802773</guid>
            <pubDate>Mon, 18 Aug 2025 10:42:05 GMT</pubDate>
            <content:encoded><![CDATA[今回はVertex AIのModel Gardenで提供されているText Generation用の基盤モデルについて何が提供されているかまとめてみます。全ては網羅せず、ざっくり眺めてみます。※ Model Gardenで説明に利用されているサムネイルのテキストから要約しています。モデルの詳細などは省略します。 Gemini系 Gemini 2.5 Proコーディングおよび複雑なプロンプトに特に強く対応 Gemini 2.5 Flashリーズニングとレスポンススピードの両方がバランスよく使える Imagen系列 Imagen Ultra 4 for im...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Gemma3 270M がでたらしいのでスペックを見てみる]]></title>
            <link>https://zenn.dev/satohjohn/articles/0866bbd4b2cefa</link>
            <guid isPermaLink="false">https://zenn.dev/satohjohn/articles/0866bbd4b2cefa</guid>
            <pubDate>Sun, 17 Aug 2025 15:17:18 GMT</pubDate>
            <content:encoded><![CDATA[概要説明を見ている限り LLM にしてはめちゃくちゃ軽いなという印象があります（桁が違う）がそれがどういうことなのかを見てみます。 3行まとめローカル(M3 MacBook Pro のメモリ 16GB)で動かす分に関しては全く問題なく動かせる。普通のアプリケーション動かすのと大差なく周りに影響もないシンプルなユースケースに限られる。（後に検証Cloud Run GPU NVIDIA L4 1台で十分スピード感出せる (簡単な文章 200ms程度で返却できるイメージ) docker で動かすモデルが https://hub.docker.com/r/ai/gemm...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Eventarcを用いてCloud Storageへのファイルアップロードイベントを検知してみた]]></title>
            <link>https://zenn.dev/akasan/articles/ca5a602c2482f5</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/ca5a602c2482f5</guid>
            <pubDate>Sun, 17 Aug 2025 12:30:42 GMT</pubDate>
            <content:encoded><![CDATA[今回はEventarcを利用して、Cloud Storageにファイルがアップロードされたことを検知してCloud Runのエンドポイントを呼び出してみました。以前Pub/Subを使ってBigQueryと連携することはしてみましたが、今回はEventarcを使ってみました。https://zenn.dev/akasan/articles/e17a1867408c53 早速試してみる今回は以下の公式ドキュメントを参考に進めました。https://cloud.google.com/run/docs/tutorials/eventarc?hl=ja#before-you-begin...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud RunのDocker Compose対応]]></title>
            <link>https://speakerdeck.com/aminevg/google-cloud-runnodocker-composedui-ying</link>
            <guid isPermaLink="false">https://speakerdeck.com/aminevg/google-cloud-runnodocker-composedui-ying</guid>
            <pubDate>Sun, 17 Aug 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Google Cloud RunがDocker Composeに対応しました！。これで、既存のDockerfileを活用してCloud Runにデプロイできるようになりました。複数のコンテナをまとめて単一サービスとしてデプロイのも便利です。一方で、モノレポへの不向きだったり、複数のサービスをデプロイできなかったり、デメリットもあります。今後はTerraform対応やCompose機能の拡充を期待しています。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年8月版読んでいて良かった本紹介]]></title>
            <link>https://zenn.dev/akasan/articles/e8c40a51231ade</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/e8c40a51231ade</guid>
            <pubDate>Sat, 16 Aug 2025 13:06:36 GMT</pubDate>
            <content:encoded><![CDATA[8月も気づけば中盤になってきましたね。先月に続いて、今月読んでいた本を紹介しようと思います。7月版はこちらになりますので、ご興味があればぜひ参照してください！https://zenn.dev/akasan/articles/05fc81a6ab77e5 データ分析 Python実践データ分析100本ノック最近改めてデータ分析をやり直そうと思ってこちらの書籍をやってます。データ分析入門書としては結構有名な気がします！Pythonでpandasとかを用いてデータ分析を始めたい人はぜひ参考にしてください。https://www.amazon.co.jp/Python実践データ分析...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OptunaとPyTorch Lightningを組み合わせてモデルの最適化をしてみた]]></title>
            <link>https://zenn.dev/akasan/articles/a75361d039906f</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/a75361d039906f</guid>
            <pubDate>Fri, 15 Aug 2025 11:10:23 GMT</pubDate>
            <content:encoded><![CDATA[今回はOptunaとPyTorch Lightningを組み合わせてモデルの最適化をしてみようと思います。なお、Optunaは昨日紹介した記事を、Pytorch Lightningについてはこちらの記事をベースに進めようと思います。https://zenn.dev/akasan/articles/ca4ab7c5f5da15https://zenn.dev/akasan/articles/2b625606090524 それでは早速やってみる！ 環境構築uvを使って環境構築していきます。uv init optuna_pytorch_lightning -p 3.12cd ...]]></content:encoded>
        </item>
    </channel>
</rss>