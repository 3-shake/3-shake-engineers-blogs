<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Thu, 04 Dec 2025 11:42:38 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[AlloyDB と Cloud Spanner (スケーラビリティの境界)]]></title>
            <link>https://silasol.la/posts/2025-12-05-01_alloy-db-and-spanner/</link>
            <guid isPermaLink="false">https://silasol.la/posts/2025-12-05-01_alloy-db-and-spanner/</guid>
            <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AlloyDB と Cloud Spanner のアーキテクチャの違いやスケーラビリティの境界について解説します．]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Electronアプリで型安全なIPC通信を実現する electron-trpcという選択肢]]></title>
            <link>https://zenn.dev/meziron/articles/82dfa259c30bf8</link>
            <guid isPermaLink="false">https://zenn.dev/meziron/articles/82dfa259c30bf8</guid>
            <pubDate>Wed, 03 Dec 2025 15:00:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事は 3-shake Advent Calendar 2025 の記事です。Electronアプリケーションの開発において、Main ProcessとRenderer Process間の通信（IPC）を型安全に実装することは、開発体験と保守性を高める上で重要な課題です。本記事では、electron-trpcを用いて、IPC通信の型安全性を効率的に確保する方法について解説します。 従来の課題：型定義の分散とボイラープレートElectron標準のIPC通信（ipcMain / ipcRenderer）を使用する場合、型安全性を確保しようとすると、記述量が増大しが...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ECSのService ConnectとService Discoveryの違いを理解する]]></title>
            <link>https://zenn.dev/iorandd/articles/20251204_aws-ecs-beginner</link>
            <guid isPermaLink="false">https://zenn.dev/iorandd/articles/20251204_aws-ecs-beginner</guid>
            <pubDate>Wed, 03 Dec 2025 15:00:01 GMT</pubDate>
            <content:encoded><![CDATA[本記事は若手AWS Leading Engineerを志す者達 Advent Calendar 2025の4日目の記事です。AWS Jr. Champions 2026 を目指すアドカレということで、業務でAmazon Elastic Container Service (ECS) を使ったマイクロサービス環境に触れる中で、Service Connect と Service Discoveryの違いを理解するために調べたことをまとめました。普段はスリーシェイクという会社でフルスタックエンジニアとしてWebアプリケーション開発に従事しています。会社の方でも3-shake Advent ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[kindle上で購入した書籍情報収集してみた]]></title>
            <link>https://zenn.dev/akasan/articles/kindle_books_bukurogu</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/kindle_books_bukurogu</guid>
            <pubDate>Wed, 03 Dec 2025 13:15:42 GMT</pubDate>
            <content:encoded><![CDATA[先日私が持っている物理本についてブクログで共有したという記事を公開させてもらいました。今回はkindle上で購入した書籍について情報を取得してみたので共有します。https://zenn.dev/akasan/articles/my_book_lists どうやって収集したか@YujiSoftware様のQiitaの記事を見つけ、拡張機能として利用してみました。https://qiita.com/YujiSoftware/items/8313687e64b33e9e546e一応拡張機能を入れるということで元のコードもGitHubで公開されていたのでみさせていただき、利用しても...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、努力しろ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/03/002023</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/03/002023</guid>
            <pubDate>Tue, 02 Dec 2025 15:20:23 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「おい、がんばるな」という言葉を書いた。あの文章を読み返して、私は少し後悔している。syu-m-5151.hatenablog.com言いたいことは分かる。がむしゃらに頑張ることが思考停止になる。忙しさが逃避になる。持続可能性が大事だ。それは正しい。私も経験してきたことだ。でも、あの文章には、書かなかったことがある。書けなかったことがある。「頑張らなくていい」という言葉が、どれほど危険な響きを持っているか。その言葉が、どれほど簡単に、怠惰の免罪符になってしまうか。私は「頑張るな」と言った。でも、それを読んだ人の中に、こう受け取った人がいるだろう。「そうか、頑張らなくていいんだ」「無理しなくていいんだ」「今のままでいいんだ」と。もしそう受け取った人がいたら、それは私の責任だ。だから、今日は別のことを書く。「おい、努力しろ」これは、あの文章への補足ではない。あの文章への反論だ。「頑張るな」という言葉の危うさを、私は書かなければならない。そして、「頑張ること」と「努力すること」の違いを、もっと正確に伝えなければならない。あの文章で私が本当に言いたかったのは、「頑張るな」ではなかった。「考えずに頑張るな」だった。でも、その「考えずに」という部分が抜け落ちて伝わってしまったら、メッセージは正反対になる。「頑張らなくていい」は、時に正しい。でも、多くの場合、それは逃げだ。そして、私たちが本当に必要としているのは、「頑張らないこと」ではない。「正しく頑張ること」——つまり、努力することだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しい。「頑張らなくていい」という甘い毒「頑張らなくていい」という言葉は、優しく聞こえる。疲れ果てた人に、「もう頑張らなくていいよ」と言う。それは、救いの言葉だ。本当に限界に達している人には、その言葉が必要なときもある。でも、問題がある。この言葉は、本当に限界の人だけでなく、まだ余力がある人にも響いてしまうということだ。なぜか。人間は、楽な方に流れる生き物だからだ。これは誰もが持っている性質であり、責めるべきことではない。ただ、事実としてそうなのだ。「頑張らなくていい」と言われれば、「そうか、頑張らなくていいのか」と受け止める。そう感じること自体は自然だ。誰だって、許可があれば楽な方を選びたくなる。そして、頑張ることをやめる。でも、本当に頑張らなくてよかったのだろうか。ここで、正直に自分に問いかけてみてほしい。「頑張らなくていい」という言葉を聞いて、ホッとした時のことを思い出してほしい。その時、自分は本当に限界だっただろうか。本当に、これ以上一歩も進めない状態だっただろうか。体が動かない、頭が働かない、そういう状態だっただろうか。それとも、まだやれるのに、やらない言い訳を探していただけではなかっただろうか。私は、後者だったことが何度もある。疲れていた。それは本当だ。でも、限界ではなかった。もう少しやれば、もう少し先に進めた。「頑張らなくていい」という言葉が、私に許可を与えた。やめていい許可を。そして、私はやめた。その時は楽になった。肩の荷が降りた。「これでいいんだ」と感じた。でも、後から振り返ると、あの時やめなければよかったと後悔することがある。あと少し踏ん張っていれば、違う景色が見えただろう。あと少し続けていれば、突破口が開けただろう。「頑張らなくていい」は、甘い毒だ。本当に必要な人には薬になる。限界を超えて壊れそうな人には、その言葉が命を救うこともある。でも、必要でない人が飲むと、毒になる。成長の機会を奪い、可能性を閉じてしまう。そして、厄介なことに、自分が「本当に必要な人」なのかどうかは、自分では分からない。なぜなら、人間は自分に甘いからだ。自分の限界を、実際より低く見積もる傾向があるからだ。だから、この言葉は慎重に使わなければならない。そして、この言葉を聞いた時は、慎重に受け取らなければならない。「私は本当に限界なのか、それとも、逃げているだけなのか」。この問いから、逃げてはいけない。いまの自分にとって「頑張らなくていい」という言葉は、薬なのか、あるいは都合のいい麻酔なのか。その区別ができるのは、自分だけだ。誰かの優しさを、自分への甘さにすり替えるな。量をこなすことでしか見えないもの「甘い毒」の話をしてきた。次は、もう少し具体的な話をしたい。「量」の話だ。「おい、がんばるな」という文章で、私は「がむしゃらは若さの特権だ」と書いた。そして、「30歳からは戦略が必要だ」と書いた。これは、半分正しくて、半分間違っている。確かに、がむしゃらに量をこなすだけでは、どこかで限界が来る。効率を考えず、方向性を考えず、ただ時間を投入するだけでは、成果は出ない。それは正しい。でも、量をこなすことでしか見えないものがあるということを、私は書かなかった。どういうことか。何かを始めたばかりの頃、私たちは何も分からない。これは当然のことだ。何が正しいのか分からない。何が効率的なのか分からない。どの方向に進むべきか分からない。この状態で「効率」や「戦略」を考えても、意味がない。なぜか。効率や戦略を考えるためには、材料が必要だからだ。「このやり方は非効率だった」「あのやり方の方が良かった」という比較ができて、初めて効率が分かる。「この方向は間違いだった」「あの方向が正しかった」という経験があって、初めて戦略が立てられる。つまり、効率や戦略を語るためには、まず経験が必要なのだ。では、経験は、どこから来るのか。量をこなすことから来る。最初から効率的にやろうとすると、何が起きるか。何も始められなくなる。「どうやったら効率的か」を考えている間に、時間だけが過ぎていく。最適な方法を見つけようとして、いつまでも動き出せない。私はかつて、あるプログラミング言語を学ぼうとした時、まず「最も効率的な学習方法」を調べることに一週間を費やした。本を読み比べた。オンラインコースを比較した。学習ロードマップを作成した。「この本は評判がいい」「このコースは体系的だ」「こういう順序で学ぶべきだ」と、完璧な計画を立てようとした。一週間後、完璧な計画ができた。でも、一行もコードを書いていなかった。一方、別の言語を学んだ時は、何も考えずにチュートリアルを始めた。「とりあえずやってみよう」と思って、手を動かした。分からないところは飛ばした。エラーが出たら、エラーメッセージをググった。理解が曖昧なまま、とりあえず動くものを作った。非効率だった。無駄なことをたくさんした。後から「ああ、最初からこうすればよかった」と悔やむことが何度もあった。でも、後者の方が、圧倒的に速く身についた。なぜか。手を動かしていたからだ。手を動かすと、分からないことが具体化する。「何が分からないか分からない」という状態から、「これが分からない」という状態になる。そうなれば、調べようがある。学びようがある。これはエンジニアだけの話ではない。セールスも、CSも、デザイナーも、同じだ。セールスなら、100件の商談をこなして初めて「この業界の顧客は、この切り口で話すと響く」が分かる。CSなら、100件の問い合わせに対応して初めて「この機能のこの部分で、ユーザーはつまづく」が見えてくる。デザイナーなら、100個のプロトタイプを作って初めて「このパターンは使いやすい」という感覚が身につく。最初から「効率的なセールストーク」を設計しようとしても、机上の空論にしかならない。最初から「完璧なカスタマージャーニー」を描こうとしても、現実とズレる。まず量をこなすことで、何が効率的で、何が正しい方向なのかが、初めて見えてくる。これは、若者だけの話でもない。何か新しいことを始める時、誰もが初心者だ。30歳、40歳、50歳になっても、新しい領域に踏み出す時は、まず量をこなすしかない。「おい、がんばるな」で私が書いた「戦略」は、量をこなした後に見えてくるものだ。量をこなす前に戦略を立てようとしても、机上の空論にしかならない。だから、まず頑張れ。考えるのは、その後でいい。戦略を語りたければ、まず汗をかけ。効率を追求しすぎることの罠量をこなすことの価値を語った。では、量だけが大事なのか。そうではない。ここで、「効率」の話をしたい。「おい、がんばるな」で、私は効率の重要性を強調した。同じ成果を、より少ない投入で得ること。それが賢い働き方だと。これも、半分正しくて、半分間違っている。効率を追求することは、確かに重要だ。無駄なことに時間を使わない。最短距離で成果を出す。それは、賢いことだ。でも、効率を追求しすぎると、動けなくなるという罠がある。どういうことか。効率を追求するとは、「最小の投入で最大の成果を得ようとすること」だ。これ自体は良いことだ。でも、これを突き詰めると、どうなるか。「成果が保証されていないことには、投入しない」という態度になりやすい。なぜか。効率の計算をするためには、投入と成果の関係が見えている必要がある。「これだけ投入すれば、これだけの成果が得られる」という予測ができて、初めて効率が計算できる。だから、効率を重視するあまり、「成果が予測できること」だけを選ぶようになる。「この作業は、本当に必要か？成果につながるか？」「このアプローチは、本当に効率的か？もっと良い方法はないか？」「この投資は、本当にリターンがあるか？損をしないか？」。こう考え始めると、確実にリターンがあることにしか、時間を使えなくなる。でも、ここで立ち止まって考えてほしい。人生で最も価値のあるものは、リターンの不確実なものが多いのではないだろうか。新しいスキルを学ぶ。そのスキルが役に立つかどうかは、学ぶ前には分からない。学んでみて、使ってみて、初めて分かる。新しい人間関係を築く。その関係が実を結ぶかどうかは、関係を築く前には分からない。時間をかけて、信頼を積み重ねて、初めて分かる。新しいプロジェクトを始める。そのプロジェクトが成功するかどうかは、始める前には分からない。やってみて、失敗して、修正して、初めて分かる。効率を追求しすぎると、これらの「不確実なこと」に時間を使えなくなる。確実にリターンがあることだけをやるようになる。すると、どうなるか。安全な場所から出られなくなる。今までやってきたこと。確実にできること。リスクのないこと。そういうものだけをやり続ける。その結果、成長がない。変化がない。じわじわと、世界が狭くなっていく。新しいことに挑戦しないから、新しい可能性が開かれない。私は、効率を追求するあまり、「無駄なこと」を一切しなくなった時期がある。仕事に直接関係のない本は読まない。読んでも仕事の成果につながらないから。すぐに役立たない技術は学ばない。学んでも今の仕事では使わないから。「これは何の役に立つのか」が説明できないことには、時間を使わない。説明できないということは、効率が計算できないということだから。確かに、目の前の仕事は効率的にこなせるようになった。無駄がなくなった。短時間で成果が出るようになった。でも、新しいアイデアが浮かばなくなった。視野が狭くなった。仕事は回せるけど、面白い発想ができなくなった。つまらない人間になっていった。なぜか。「無駄」の中にこそ、予想外の価値があるからだ。一見無駄に見える読書が、思わぬところで仕事に活きる。すぐに役立たない技術が、数年後には大きな武器になる。「何の役に立つか分からない」経験が、人間としての厚みを作る。効率だけを追求すると、その「予想外の価値」を取りこぼしてしまう。だから、時には非効率を許容しろ。時には「何の役に立つか分からないこと」に時間を使え。それが、長期的には最も効率的な投資になることがある。効率やリターンが見えないことの中に、本当は心のどこかで「それでもやってみたい」と思っているものがないだろうか。その声を、効率という物差しで測って、黙らせていないだろうか。計算できないものにこそ、人生を変える何かが潜んでいる。「持続可能性」という名の逃げ道効率の話をしてきた。次は、もう1つの「賢そうな言葉」について考えたい。「持続可能性」だ。「おい、がんばるな」で、私は持続可能性の重要性を説いた。無理をしない。長く続けられるペースで。燃え尽きないように。これは正しい。燃え尽きて動けなくなったら、意味がない。長く続けることは確かに大事だ。でも、この言葉が逃げ道になることがある。どういうことか。「持続可能なペースで」と言うと、それは賢明に聞こえる。長期的な視点を持っている。自分を大切にしている。無理をしない。計画的だ。でも、その「持続可能なペース」は、本当に適切なのだろうか。ここで、人間の心理について考えてみたい。私たちは、自分の限界を過小評価しがちだ。「これ以上やったら壊れる」と感じる地点は、実際の限界よりもずっと手前にあることが多い。なぜか。人間は、不快なことを避けたい生き物だからだ。辛いこと、苦しいこと、面倒なことは、できれば避けたい。これは自然な感情だ。だから、実際に壊れるよりもずっと手前で、「もう限界だ」と感じてしまう。まだ余力があるのに、「これ以上は無理だ」と思ってしまう。「持続可能なペース」という言葉を使う時、私たちは無意識に、その「過小評価された限界」を基準にしていないだろうか。本当は、もう少し頑張れる。もう少し踏ん張れる。でも、「持続可能性」という言葉を使って、その踏ん張りを回避していないだろうか。「持続可能性」は、時に「楽をするための言い訳」になる。もちろん、本当に限界の人はいる。本当に休まなければならない人はいる。その人たちにとって、「持続可能性」は正当な理由だ。そういう人に「もっと頑張れ」と言うのは、暴力だ。でも、全員がそうではない。まだ余力があるのに、「持続可能性」を理由にブレーキをかけている人もいる。ここで、もう1つ正直に自分に問いかけてみてほしい。「持続可能なペース」と言った時、それは本当に「長期的に最適なペース」なのか。それとも、「今、楽でいられるペース」なのか。この2つは、似ているようで、全く違う。長期的に最適なペースは、時に短期的には辛い。なぜか。成長するためには、今の自分を超える必要があるからだ。今の自分を超えるためには、今の自分には辛いことをする必要がある。筋肉を鍛える時のことを考えてみてほしい。筋肉は、負荷をかけて、一度壊れて、修復される過程で強くなる。楽な負荷だけかけていても、筋肉は成長しない。能力も同じだ。今できることだけやっていても、能力は成長しない。今できないこと、今の自分には辛いことに挑戦して、初めて成長する。「持続可能性」を盾にして、その「辛いこと」を避けていたら、成長はない。踏ん張るべき時には、踏ん張れ。いつでも快適でいようとするな。不快さの中にこそ、成長がある。最近、「持続可能性のため」と言ってブレーキを踏んだ場面を思い出してほしい。それは本当に長期のためだっただろうか。それとも、今ラクでいたい自分のためだっただろうか。答えは、自分の中にしかない。「持続可能性」は免罪符じゃない。逃げ道を正当化する言葉でもない。苦しみの中でしか得られないものここまで、「甘い毒」「量」「効率」「持続可能性」の話をしてきた。これらに共通するのは、「苦しみとどう向き合うか」という問いだ。次は、その「苦しみ」について、もう少し直接的に語りたい。「おい、がんばるな」で、私は「苦しみを美化するな」と書いた。苦しむこと自体には価値がない。同じ成果を楽に得られるなら、その方がいいと。これも、半分正しくて、半分間違っている。確かに、苦しむこと自体を目的にするのは間違っている。苦しめば偉いわけではない。苦労すれば成果が出るわけではない。無意味な苦しみは、ただの消耗だ。でも、苦しみの中でしか得られないものがあるということも、事実だ。それは何か。自分が何者であるかを知ることだ。どういうことか。人間は、追い込まれた時に、本当の自分が出る。楽な時、余裕がある時には、本当の自分は見えない。余裕があると、取り繕える。自分を良く見せられる。でも、苦しみの中では、取り繕う余裕がなくなる。本当の自分が、否応なく姿を現す。自分は、どこまで耐えられるのか。限界だと思った先に、まだ力が残っているのか。自分は、何を諦められないのか。何を捨てても、これだけは手放せないというものは何なのか。自分は、何のために頑張れるのか。お金のためか、評価のためか、それとも、もっと別の何かのためか。これらの問いに対する答えは、快適な場所にいては見つからない。不快な場所に身を置いて、初めて見えてくる。私は、あるプロジェクトで、本当に追い込まれた経験がある。締め切りは迫っている。スケジュールは遅延している。チームは疲弊している。メンバーの顔に疲労が見える。問題は山積みだ。1つ解決すると、別の問題が浮上する。毎日が綱渡りだった。辛かった。何度も逃げ出したいと思った。「こんなの、持続可能じゃない」と思った。「なんでこんなことをしているんだろう」と思った。でも、あの経験がなければ、今の自分はいない。これはエンジニアだけの話ではない。セールスなら、どうしても落とせない大型案件に挑み続けた経験。何度も断られ、それでも食らいついた経験。その中で「自分は何のために営業をしているのか」が見えてくる。CSなら、クレームが殺到した時期を乗り越えた経験。理不尽に怒られ、なお丁寧に対応し続けた経験。その中で「自分はどこまでユーザーに寄り添えるのか」が見えてくる。現場で働くすべての人に、そういう経験がある。あの時、自分が何を大切にしているのかが分かった。チームのために最後まで踏ん張りたいと思っている自分がいた。良いものを作りたいと思っている自分がいた。自分がどこまで頑張れるのかが分かった。「もう無理だ」と思ったところから、より三歩進めた。限界だと思っていたところは、限界ではなかった。そして、自分がそこまで頑張れるという自信が、あの経験から生まれた。この自信は、快適な場所では得られない。苦しみを乗り越えた経験からしか得られない。「あの時、あれだけ辛いことを乗り越えた」という記憶は、次の困難へ立ち向かう力になる。「あの時できたのだから、今回もできる」という自信は、前へ進む勇気になる。だから、苦しみを避けるな。もちろん、無意味な苦しみは避けるべきだ。方向が間違っているなら、修正すべきだ。でも、正しい方向に進んでいるなら、苦しみを恐れるな。その苦しみの中に、あなたのまだ知らない自分がいる。苦しみを避けて到達する場所に、本当の自分はいない。「休むこと」を過大評価していた苦しみの話をしてきた。では、苦しみの反対にある「休息」は、どうだろうか。「おい、がんばるな」で、私は休むことの重要性を強調した。休憩は投資だ。睡眠は投資だ。休むことで、生産性が上がると。これは正しい。休息は大事だ。睡眠不足は判断力を鈍らせる。疲労は生産性を下げる。でも、休むことを過大評価していたという反省もある。どういうことか。休むことが重要なのは、その後にまた頑張るためだ。休息は、次の活動のための準備だ。体を回復させ、頭をリフレッシュさせ、また動き出すための準備だ。つまり、休息の価値は「その後の活動」によって決まる。休んだ後に何もしないなら、休息の意味がない。でも、「休むことが大事」という言葉を聞くと、休むこと自体が目的になってしまうことがある。「今日は休む日だから、何もしない」「疲れているから、休まなきゃ」「持続可能性のために、休息を取る」。そう言いながら、ずっと休んでいる。次の活動が、いつまでも始まらない。休息は、活動のための手段だ。休息自体が目的ではない。この区別を忘れると、「休むこと」が「何もしないこと」にすり替わってしまう。私は、「休息も投資だ」と言いながら、実際には逃避していた時期がある。「今日は休む」と言いながら、本当は面倒なことを避けていた。やるべきことがあるのに、「疲れているから」と言って、やらなかった。「持続可能性のため」と言いながら、実際には楽をしていた。もう少し頑張れる状態なのに、「無理は禁物だから」と言って、手を抜いた。休息と逃避は、外からは区別がつかない。どちらも「何もしていない」ように見える。区別できるのは、自分だけだ。これはエンジニアだけの話ではない。セールスなら、「今日は疲れているから、あのリードへの連絡は明日にしよう」と言い続けて、結局連絡しないまま案件を逃すことがある。CSなら、「この問い合わせは複雑だから、体調が良い時に対応しよう」と言い続けて、対応が遅れてユーザーの信頼を失うことがある。どの職種でも、「休息」と「先延ばし」の境界は曖昧だ。自分に正直に問いかけてほしい。今、休んでいるのは、次に頑張るための準備なのか。それとも、頑張ることから逃げているだけなのか。この2つは、外見は同じでも、本質は全く違う。次に頑張るための休息には、終わりがある。回復したら、また動き出す。頑張ることからの逃避には、終わりがない。いつまでも「まだ疲れている」「まだ準備ができていない」と言い続ける。前者なら、休め。後者なら、立ち上がれ。休息は充電だ。放電しないなら、充電する意味はない。「考えること」を言い訳にするな休息の話をしてきた。次は、もう1つの「賢そうな行為」について考えたい。「考えること」だ。「おい、がんばるな」で、私は「考えること」の重要性を説いた。がむしゃらに動くな。立ち止まって考えろ。方向性を確認しろと。これは正しい。考えずに動くと、間違った方向に全力で進んでしまう。それは危険だ。でも、「考えること」が行動しない言い訳になることがある。どういうことか。「まだ考えがまとまっていない」「もう少し情報が必要だ」「方向性を確認してから動きたい」。こう言いながら、いつまでも動かない人がいる。考えることは大事だ。でも、考えているだけでは、何も起きない。なぜか。世界は、行動によってしか変わらないからだ。頭の中でどれだけ完璧な計画を立てても、行動しなければ、現実は何も変わらない。素晴らしいアイデアがあっても、実行しなければ、ただの妄想だ。そして、皮肉なことに、行動しないと、本当に必要な情報は手に入らない。何かを始める前は、何が分からないかも分からない。何が問題になるかも分からない。どこが難しいかも分からない。頭の中で考えているだけでは、これは分からない。机上で計画を立てているだけでは、見えてこない。実際にやってみて初めて分かる。手を動かし、困難にぶつかり、失敗して初めて「ああ、ここが問題だったのか」と分かる。だから、「もっと考えてから」「もっと情報を集めてから」と言い続けていると、永遠に動き出せない。必要な情報は、動き出さないと手に入らないからだ。これはエンジニアだけの話ではない。セールスなら、「この業界のことをもっと調べてから提案しよう」と言い続けて、結局一度も商談に臨まないことがある。しかし、実際に商談に出て、顧客の反応を見て、初めて「この業界は価格よりもサポート体制を重視する」が分かる。CSなら、「この機能の仕様をもっと理解してから対応しよう」と言い続けて、結局ユーザーを待たせてしまうことがある。ただ、実際に対応しながら調べ、先輩に聞くことで「この機能は、こういう使い方をするユーザーがいる」と分かる。どの職種でも、動くことでしか得られない知識がある。これは鶏と卵のような問題に見えるだろう。動くためには情報が必要だ。しかし、情報を得るためには動く必要がある。どうすればいいのか。答えは、不完全なまま動き始めることだ。完璧な計画を待つな。不完全なまま始めろ。間違っているだろう。失敗するだろう。それでも、始めなければ、何も始まらない。動きながら考えろ。走りながら修正しろ。考えることと動くことは、どちらか一方ではない。順番に行うものでもない。両方同時にやるものだ。動きながら考え、考えながら動く。そうすることで、より良い方向に、より速く進める。「まだ準備ができていない」「もう少し考えてから」と言って先送りしていることがあるなら、立ち止まって考えてみてほしい。それは本当に考える段階なのか。それとも、動くことを怖がっているだけなのか。考えることと、考えているふりをして逃げることは、違う。準備が整う日は、永遠に来ない。来たと思える日は、動き始めた後にしか訪れない。では、何が「努力」なのかここまで、「頑張らなくていい」という言葉の危うさを書いてきた。量をこなすことの価値。効率を追求しすぎることの罠。持続可能性が逃げ道になること。苦しみの中でしか得られないもの。休むことの過大評価。考えることが言い訳になること。では、結局、何をすればいいのか。ここで、「頑張ること」と「努力すること」を区別したい。頑張ることは、「とにかくやること」だ。方向も考えず、効率も考えず、ただ時間とエネルギーを投入する。がむしゃらに動く。汗をかく。疲れる。これは「おい、がんばるな」で批判したことであり、確かに問題がある。方向が間違っていたら、どれだけ頑張っても成果は出ない。努力することは、「考えながらやること」だ。方向を意識し、フィードバックを得て、修正しながら進む。効率を考える。戦略を立てる。ただ、考えるだけでなく、実際に動く。これは、頑張ることとは違う。しかし、努力には「やること」が含まれている。ここが重要なポイントだ。「考えること」だけでは、努力ではない。「やること」が必要だ。そして、「やること」には、しばしば苦しみが伴う。不快さが伴う。疲労が伴う。それを避けていたら、努力にはならない。努力とは、正しい方向に向かって、苦しみを引き受けながら、行動し続けることだ。もう少し分解して説明しよう。まず、「正しい方向に向かって」。これは、考えることだ。自分は何を達成したいのか。どこに向かいたいのか。そのためには、何をすべきか。これを考える。次に、「苦しみを引き受けながら」。これは、踏ん張ることだ。辛くても、やる。不快でも、続ける。逃げ出したくなっても、踏みとどまる。そして、「行動し続ける」。これは、動くことだ。考えるだけでなく、実際に手を動かす。失敗しても、また動く。続ける。この三つが揃って、初めて「努力」になる。これはどの職種でも同じだ。エンジニアなら、正しいアーキテクチャを考え、難しいバグと格闘しながら、コードを書き続ける。セールスなら、顧客の課題を考え、断られる辛さを引き受けながら、提案を続ける。CSなら、ユーザーの真のニーズを考え、クレームの辛さを引き受けながら、対応を続ける。デザイナーなら、ユーザー体験を考え、何度もダメ出しされる辛さを引き受けながら、デザインを続ける。どの仕事でも、努力の構造は同じだ。「頑張るな」と言って、苦しみを避けることを正当化してはいけない。苦しみは、努力の一部だ。「考えろ」と言って、行動しないことを正当化してはいけない。行動は、努力の一部だ。方向を考えながら、苦しみを引き受けながら、行動し続ける。それが、努力だ。楽をしながら成長はできない。考えるだけで変わることもできない。誘惑という名の逃げ道努力の定義をした。正しい方向に向かって、苦しみを引き受けながら、行動し続けること。それが努力だと書いた。しかし、ここで正直に認めなければならないことがある。努力するのは、難しい。なぜか。現代社会には、努力から逃げるための誘惑が溢れているからだ。スマホを開けばSNSが待っている。通知が鳴り続ける。動画は自動再生される。情報は洪水のように押し寄せる。疲れた時、辛い時、つい手が伸びる。「ちょっと休憩」と言いながら、気づけば1時間、2時間が過ぎている。これは、休息ではない。逃避だ。先ほど「休むことの過大評価」の話をした。ここでも同じことが起きている。私たちは「少し気分転換」と言いながら、実際には努力から逃げている。ここで、1つの考え方を紹介したい。ジェイ・シェティという作家がいる。彼は実際に僧侶として修行した経験を持ち、その経験をもとに「モンク思考」という考え方を世界に広めた。私たちはつい、他人と年収を比べたり、社会的なイメージで仕事を選んだりしてしまう。「成功とはこういうもの」「幸せとはこういうもの」という外側からの定義に、無意識に縛られている。しかし、本当はどのような人生を送りたいのか。本当はどのような人間になりたいのか。この問いに、自分の言葉で答えられるだろうか。彼が説くのは、「手放す」「成長する」「与える」という3つのステップだ。まず、執着を手放す。他人の評価、過去の成功体験、「こうあるべき」というプレッシャー。これらを握りしめていると、本当に大切なものが見えなくなる。次に、自分の情熱と才能に向き合う。何をしている時に時間を忘れるか。何に取り組んでいる時に充実感を感じるか。他人の期待ではなく、自分の内側から湧き上がるものを見つける。そして、目的を持って生きる。自分のためだけに努力するのではなく、誰かのために、何かのために努力する。その方が、長く続く。強く踏ん張れる。この考え方の核心は、「小さなノー」の積み重ねだ。SNSを見ない。無駄な飲み会を断る。ダラダラとネットサーフィンしない。1つ1つは小さな「ノー」だ。しかし、この小さな「ノー」を積み重ねることで、本当に大切なことに「イエス」と言えるようになる。誘惑に「ノー」と言うことで、努力に「イエス」と言える。私たちは、誘惑に負けるたび、自分を少しずつ裏切っている。「今日くらいいいか」「疲れているから仕方ない」「明日から頑張ろう」。そう言いながら、努力から逃げている。その言い訳を、いつまで続けるのか。永遠に僧侶のように生きる必要はない。ただ、誘惑を言い訳にするのをやめろ。集中できないのは環境のせいではない。自分が誘惑を選んでいるだけだ。スマホを閉じろ。通知をオフにしろ。そして、今やるべきことに向き合え。それが、努力の第一歩だ。踏ん張るべき時に踏ん張れ努力の定義をした。最後に、1つのことを言いたい。人生には、踏ん張るべき時がある。チャンスは、いつでも来るわけではない。絶好の機会は、そう何度もあるわけではない。その時が来た時に踏ん張れるかどうかで、人生は変わる。踏ん張るべき時に「持続可能性が」と言って引いてしまったら、チャンスを逃す。踏ん張るべき時に「効率が」と言って計算してしまったら、大事なものを取りこぼす。踏ん張るべき時に「休息が」と言って立ち止まってしまったら、流れに乗れない。踏ん張るべき時には、理屈を超えて、踏ん張れ。これはどの職種でも同じだ。エンジニアなら、リリース前の追い込み、障害対応、重要な技術選定の議論。セールスなら、年度末のクロージング、大型案件のコンペ、重要な顧客との交渉。CSなら、大規模障害時のユーザー対応、重要顧客の離脱防止、クリティカルなクレームへの対応。どの仕事にも、「ここが勝負所」という瞬間がある。その瞬間に踏ん張れるかどうかで、キャリアは変わる。もちろん、いつも踏ん張れとは言わない。いつも踏ん張っていたら、壊れる。それは「おい、がんばるな」で書いた通りだ。だからこそ、踏ん張るべき時を見極めることが大事だ。普段は力を温存し、ペースを守り、回復する時間を取る。そして、その時が来たら、全力で踏ん張ることが大事だ。温存していた力を、すべて出し切る。「おい、がんばるな」は、「いつも踏ん張っている人」に向けた言葉だった。常にアクセル全開で、休むことを知らない人。そういう人には、確かに「踏ん張りすぎるな」と言う必要がある。一方で、世の中には、踏ん張るべき時に踏ん張れない人もいる。チャンスが来ても、「疲れているから」「リスクがあるから」「まだ準備ができていないから」と言って、見送ってしまう人。そういう人に「頑張らなくていい」と言ったら、それは間違ったメッセージになる。自分がどちらのタイプか、正直に考えてほしい。いつも踏ん張りすぎて疲弊しているなら、少し力を抜いていい。しかし、踏ん張るべき時に踏ん張れていないなら、今こそ踏ん張る時だ。この一年を振り返ってみてほしい。「あそこであと一歩踏ん張っていれば」と、未来の自分に言われそうな場面はないだろうか。もしあるなら、それが答えだ。次にその場面が来た時、同じ後悔をしないために、今から準備しておくことだ。チャンスは、準備している人のところにしか来ない。来ても、踏ん張れなければ、すり抜けていく。何もしなくても誰かがお膳立てしてくれて、機会が向こうからやってくる。そんな恵まれた環境が、いつまでも続くと信じるな。続いたとしても、それは成長ではない。ただの停滞だ。ここで、厳しいことを言う。世の中は理不尽で、不公平だ。生まれた環境も、与えられた才能も、巡ってくる機会も、平等ではない。それは事実だ。口で何を言っても、不満を並べても、愚痴をこぼしても、その現実は変わらない。SNSで正論を叫んでも、飲み会で上司の悪口を言っても、世の中は1ミリも動かない。行動しなければ、努力しなければ、状況は何も変わらない。これは冷たい言葉ではない。むしろ、希望の言葉だ。なぜなら、行動すれば変わる可能性があるということだからだ。理不尽な世界の中で、自分の手で変えられるものがある。それが、努力だ。ここで、1つの反論が聞こえてくる。「そもそも、このゲーム自体がおかしいのではないか」と。努力すれば成功者が増えるのか。全員が頑張れば、全員が報われるのか。答えはノーだ。構造的に、成功者の席は限られている。全員が努力しても、椅子取りゲームの椅子は増えない。格差は縮まるどころか、広がり続ける。能力主義という名のレースは、走れば走るほど、差が開いていく仕組みになっている。それは、経済学的にも、社会学的にも、既に答えが出ている話だ。では、このゲームから降りればいいのか。「こんな不公平なレースには参加しない」と宣言すればいいのか。私は、その選択を否定しない。降りる自由はある。しかし、自分に問いかけてみてほしい。降りたところで、何が開けるのか。レースから降りた先に、別の人生があるのか。不参加を表明したところで、この社会の中で生きていくことに変わりはない。構造を批判しながら、その構造の中で生きていく。それが、大半の人間の現実だ。だから私は、こう考える。ゲームがおかしいことは分かっている。ルールが不公平なことも分かっている。それでも、このゲームの中で生きていく以上、このゲームの中での戦い方を身につけるしかない。構造を変えることは、個人の努力ではほぼ不可能だ。でも、構造の中での自分の位置を変えることは、できる可能性がある。それが、努力だ。大事なのは、その理不尽さや不公平さを、腹の底から受け入れることだ。「なぜ自分だけ」「もっと恵まれていれば」という思いを抱えたまま努力しても、どこかで折れる。被害者意識を持ったまま走っても、長くは続かない。世の中が不公平であることを認めた上で、それでも前に進む。不公平を嘆く暇があるなら、その時間で一歩でも進め。理不尽に怒るエネルギーがあるなら、そのエネルギーを努力に変えろ。それが、この不完全な世界で生き抜くための唯一の方法だ。努力せずに目標が達成できると、本気で信じているなら教えてほしい。努力もせずに、この淀んだ自分という檻から抜け出せると、本気で信じているなら教えてほしい。私は信じていない。自分を変えるには、努力が必要だ。今の自分を超えるには、苦しみを引き受ける必要がある。檻から出るには、その困難を押し続ける必要がある。それを避けて、「頑張らなくていい」という言葉に逃げ込んでも、檻は壊れない。自分は変わらない。淀んだ水は、そのまま淀み続ける。努力なしに変われると信じるな。苦しみなしに成長できると信じるな。檻を壊すのは、他の誰でもない、自分自身だ。ここまで厳しいことを書いてきた。しかし、1つだけ、白状させてほしい。私は、自分のことを特別だと思えたことがない。ふとした瞬間に気づく。ああ、俺は凡人だな、と。天才じゃない。選ばれた側の人間でもない。器には限界がある。どうしようもなく、限界がある。周りを見れば、自分より優秀な人間なんていくらでもいる。悔しいが、事実だ。そして、もう1つ。万全の状態で仕事に臨める日なんて、一生来ない。体調が悪い。眠れていない。私生活がぐちゃぐちゃだ。そんな日の方が、圧倒的に多い。それでも、やる。最悪の日であっても、最低限の水準は守る。それがプロだ。凡人だから、積み上げるしかない。万全を待っていたら何も始まらないから、不完全なままでも動ける自分を作るしかない。おわりに「おい、がんばるな」と書いた。今日は「おい、努力しろ」と書いた。矛盾しているように見えるだろう。しかし、矛盾していない。どちらも、同じことを言っている。「考えずに頑張るな」「ただし、考えながら頑張れ」。これを一言で言えば、「努力しろ」だ。努力には、考えることが含まれている。方向を意識することが含まれている。フィードバックを得ることが含まれている。同時に、努力には、行動することも含まれている。苦しみを引き受けることも含まれている。踏ん張ることも含まれている。「頑張るな」という言葉だけを受け取って、行動しなくなってはいけない。苦しみを避けてはいけない。踏ん張ることをやめてはいけない。考えながら、頑張れ。方向を意識しながら、踏ん張れ。それが、努力だ。「おい、がんばるな」は、片面だけを描いた絵だった。今日は、もう片面を描いた。両方を見て、初めて全体が見える。——と言いたいところだが、正直に言えば、これでもまだ全体ではない。この問題には、2つの面だけでなく、もっと多くの面がある。私が見えていない角度がある。私が経験していない状況がある。私が想像すらできていない視点がある。たとえば、心身の病を抱えている人にとって、「努力しろ」という言葉がどう響くか。私には、本当の意味では分からない。あるいは、社会的な制約の中で選択肢が限られている人にとって、「踏ん張れ」という言葉がどう響くか。私には、本当の意味では分かっていない。私が書いたのは、私の経験から見えた2つの面に過ぎない。他にも面はある。3つ目も、4つ目も、おそらくもっとたくさんある。それは自覚している。だから、この文章を「正解」として読まないでほしい。これは、1つの視点だ。私という人間が、私の経験を通して見た、1つの景色だ。あなたには、あなたの景色がある。あなたの経験から見える面がある。それは、私には見えない面だろう。あなたが今、どちらの言葉を必要としているかは、あなた自身にしか分からない。頑張りすぎて疲弊しているなら、「おい、がんばるな」を読んでほしい。頑張れずに停滞しているなら、「おい、努力しろ」を読んでほしい。どちらの状態にいても、前に進むことをやめるな。前に進むとは、行動することだ。考えることだ。苦しみを引き受けることだ。そして、それを続けることだ。おい、努力しろ。考えながら、頑張れ。方向を見据えながら、踏ん張れ。休みながらも、また立ち上がれ。それが、あなたを前に進ませる唯一の方法だ。参考書籍バカと無知 (新潮新書)作者:橘　玲新潮社Amazon知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazon実力も運のうち　能力主義は正義か？ (ハヤカワ文庫NF)作者:マイケル サンデル早川書房Amazonデジタル・ミニマリスト　スマホに依存しない生き方 (ハヤカワ文庫NF)作者:カル ニューポート早川書房AmazonSLOW　仕事の減らし方――「本当に大切なこと」に頭を使うための３つのヒント作者:カル・ニューポートダイヤモンド社Amazon大事なことに集中する―――気が散るものだらけの世界で生産性を最大化する科学的方法作者:カル・ニューポートダイヤモンド社Amazon深い集中を取り戻せ――集中の超プロがたどり着いた、ハックより瞑想より大事なこと作者:井上一鷹ダイヤモンド社Amazonジェームズ・クリアー式 複利で伸びる1つの習慣作者:ジェームズ・クリアーパンローリング株式会社Amazonクリティカル・ビジネス・パラダイム――社会運動とビジネスの交わるところ作者:山口 周プレジデント社Amazon人生の経営戦略――自分の人生を自分で考えて生きるための戦略コンセプト２０作者:山口 周ダイヤモンド社Amazon知的戦闘力を高める 独学の技法作者:山口 周ダイヤモンド社Amazonモンク思考―自分に集中する技術作者:ジェイ・シェティ東洋経済新報社AmazonSENSE FULNESS　どんなスキルでも最速で磨く「マスタリーの法則」作者:スコット・Ｈ・ヤング,小林　啓倫朝日新聞出版Amazon新版　究極の鍛錬作者:ジョフ・コルヴァンサンマーク出版Amazon心眼――あなたは見ているようで見ていない作者:クリスチャン・マスビアウプレジデント社AmazonQUEST「質問」の哲学――「究極の知性」と「勇敢な思考」をもたらす作者:エルケ・ヴィスダイヤモンド社Amazon資本主義が人類最高の発明である：グローバル化と自由市場が私たちを救う理由作者:ヨハン・ノルベリニューズピックスAmazon資本主義にとって倫理とは何か作者:ジョセフ・ヒース,瀧澤弘和慶應義塾大学出版会Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AIエージェントによるブログレビュー環境の構築（下）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/03/001146</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/03/001146</guid>
            <pubDate>Tue, 02 Dec 2025 15:11:46 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、3-shake Advent Calendar 2024 3日目のエントリ記事です。上巻の振り返り上巻では、Commandsを使ったブログレビュー環境の基礎を説明しました。/deep-thinking-prompt で書く前に深く考える/blog-quality-review で6つの観点からレビューする/ai-humanity-check でAIっぽさを検出する/full-review で全自動レビューするこれらのCommandsは、レビュー観点を構造化し、一貫性を担保してくれます。syu-m-5151.hatenablog.com下巻では、より高度なSubagentsの活用へ入る前に、いくつかの話題を深掘りします。AIに記事を書かせるとは何か「AIに記事を書かせる」という言葉をめぐって、しばしば議論が起きます。「それは本当にあなたの記事なのか」「AIが書いたものに価値があるのか」。私の答えは明確です。記事はほとんどAIに書かせています。しかし、価値の源泉は私にあります。手書きで書いているという人も別に紙に直接書いている訳ではないでしょう。既に、予測変換やLSP（Language Server Protocol）による補完など、さまざまなレベルで「AIやコンピュータの支援」を受けながら文章を書いています。その延長線上に、生成AIによる執筆があるに過ぎません。では、私は何を担っているのでしょうか。「身体性」を供給しています。ここで言う身体性とは、知識が「情報」から「経験」へと変容する過程で生じる、一人称的な認知の軌跡です。知識と経験の断絶たとえば、あるエンジニアがRustの所有権システムを学んでいるとします。The Bookを読み、概念は「理解した」つもりでいました。しかしいざコードを書くと、コンパイラからcannot borrow as mutable...というエラーを食らいます。「ルールは知っているはずなのに、なぜ」——この「知っている」と「書ける」の間にある断絶こそが、身体性が欠落している状態です。そして、その断絶を越えた瞬間の記録があります。「なぜエラーになったのか格闘し、イテレータの内部構造に気づき、腹落ちした瞬間」——これこそが身体性を伴った学習の言語化です。それは他者に伝達可能な「生きた知見」となります。この「苦闘から理解への遷移（プロセス）」だけは、AIには生成できません。AIは私の代わりに試行錯誤できませんし、私の代わりとしてコンパイラに叱られて悔しがることもできないからです。AIの役割は、私が供給した「生の体験（身体性）」を、他者が読める文章として整えることにあります。混沌とした思考を構造化し、読者にとって消化しやすい形に変換します。それは編集者の仕事に近いです。私が素材（身体性）を提供し、AIが構造化し、私がレビューして調整します。この協働のプロセス全体が、現代における「執筆」なのです。「流暢な嘘」という罠一方で、「AIで書いた記事には価値がない」という批判も、ある意味では正しいです。問題の本質は「AIを使ったこと」ではなく、「検証というプロセスが抜け落ちていること」にあります。AIに丸投げして出力された文章には、不正確な情報の垂れ流しという致命的なリスクが潜みます。厄介なのは、AIの生成する文章が文法的に完璧で、論理の構成も美しすぎることです。人間が書いた拙い文章なら「この人、理解していないな」と直感的に警戒できます。しかし、AIの出力は「もっともらしさ（Plausibility）」に特化しているため、嘘であってもスルスルと頭に入ってきてしまいます。これを検証せずに公開するのは、ブレーキの効かない車を公道に放つようなものです。LLMは確率的に「次の単語」を選んでいるに過ぎません。そこに真偽への誠実さは存在しません。だからこそ、その確率の波を制御し、事実という地面に杭を打つのは、人間にしかできない仕事です。私たちは、AIというエンジンの出力に酔うのではなく、冷静な「監修者」であり続けなければなりません。しかし、この監修作業を人間の力だけで行うには限界があります。だからこそ、「AIを監視するAI」が必要になるのです。それがこれから紹介する「Sub-agents」によるレビュー体制です。Commandsの限界とSub-agentsの登場上巻で紹介したCommands（/blog-quality-reviewなど）は便利ですが、長く使っていると2つの困難にぶつかります。コンテキストの枯渇: 長文記事に対し、複数の観点で深いレビューを繰り返すと、メインの会話履歴（コンテキストウィンドウ）がすぐに溢れてしまう。専門性の欠如: 1つのプロンプトにあらゆる指示を詰め込むと、焦点がぼやけ、鋭い指摘ができなくなる。そこで導入したのが、Claude Codeの強力な機能、Sub-agentsです。Sub-agentsとは何かhttps://code.claude.com/docs/en/sub-agents:embed:citeSub-agentsは、特定のタスクに特化した自律的なAIワーカーです。これまでの「Commands（定型文の挿入）」とは、根本的にアーキテクチャが異なります。1. コンテキストの分離（Context Isolation）これが最大にして最強のメリットです。通常、長い記事をレビューさせると、「思考過程」や「中間生成物」でメインの会話履歴が埋め尽くされてしまいます。しかしSub-agentsは、メインとは独立した別のコンテキストウィンドウで作業します。完全にレビュワーに徹することができます。もちろんデメリットもあるので使い分けが必要です。User │ ▼Main Agent │ [Delegate] 記事テキストを渡し、レビューを依頼 ▼Sub-Agent (Reviewer) ┃ ★独自のコンテキストで思考★ ┃ 1. 全文読み込み ┃ 2. 批判的検討 ┃ 3. 推敲（ここのトークンはメインには見えない） ┃ ▼Main Agent (レビュー結果の要約のみを受取) │ ▼User (修正案の提示)メインエージェントが受け取るのは、Sub-agentが導き出した「結論」だけです。これにより、メインのコンテキストを汚染することなく、大量のトークンを使った深い推論が可能になります。2. 自律的な委譲（Delegation）Commandsはユーザーが手動で呼び出すものですが、Sub-agentsはメインのエージェント（Orchestrator）が必要だと判断した時に自動的に呼び出されます。「この記事、なんか読みづらいから直して」と指示するだけで、メインエージェントが「これは『文章校正エージェント』と『構成作家エージェント』の出番だ」と判断し、仕事を割り振ります。私が実際に配備しているSub-agents私は現在、ブログ執筆チームとして以下のSub-agentsを .claude/agents/ に配備しています。実際にはもっといますが、今回は3つだけ実際に使っているものを紹介します。1. narrative-architect.md （物語構造の専門家）技術記事であっても、読者の感情を動かす「物語」が必要です。このエージェントは、技術的な正しさには口を出しません。その代わり、「読者の感情の旅路（Emotional Journey）」だけを見ます。役割: 導入で共感を得られているか。解決策の提示でカタルシスがあるか。指摘例: 「機能の説明は正確だが、読者が抱えている『辛さ』への共感が不足しており、解決策の価値が伝わりにくい」2. fresh-eye-reviewer.md （永遠の初学者）私の「書き手の呪い」を解くためのエージェントです。ペルソナとして「実務未経験のジュニアエンジニア」が埋め込まれています。役割: 専門用語の困難、論理の飛躍、「なぜ」という素朴な疑問の発見。特徴: 文脈をあえて読まない。「ここまでの説明では、この単語の意味がわからない」と冷徹に指摘する。3. ai-police.md （AI警察）「AIっぽさ」を検知し、排除する専門官です。AIが生成した文章特有の「過剰な接続詞」「中身のない美しいまとめ」「冗長な言い回し」を検挙します。役割: テキストの人間らしさ（Humanity Score）の判定。指摘例: 「『〜ということができる』は冗長だ。『〜できる』と言い切るべき。また、この段落の『いかがでしたか』はAI臭いので削除を推奨する」実践：レビュー体制の構築これらのSub-agentsを連携させることで、私のブログ執筆フローは完全に変わりました。ディレクトリ構造.claude/├── commands/           # ユーザーが叩くショートカット│   └── full-review.md  # 全体を統括する指示書└── agents/             # 自律的に動く専門家たち    ├── narrative-architect.md    ├── fresh-eye-reviewer.md    └── ai-police.mdレビューの流れStep 1: 執筆（協働）私とメインエージェントで対話しながら、記事のドラフトを作成します。私は身体性（エピソード）を話し、エージェントがそれを整えます。Step 2: 全自動レビュー（委譲）書き上がったドラフトに対し、私は一言こう告げるだけです。「/full-review を実行して」すると、メインエージェントが裏側で複数のSub-agentsを起動します。Fresh Eye が「ここがわからない」と文句を言う。Narrative Architect が「構成が退屈だ」と指摘する。AI Police が「AIっぽい表現がある」と警告する。Step 3: 統合と修正メインエージェントは、これらのバラバラな意見を統合し、優先順位をつけて私に提示します。「初学者にとって難解な部分があり、かつAI特有の冗長な表現が残っています。まずは第2章の具体例を修正しましょう」私はその統合されたレポートを見て、最後に修正します。まとめ上巻から下巻を通じて、生成AIエージェントを用いたブログレビュー環境の構築について解説してきました。上巻: ブログの評価基準をCommandsで構造化し、手動レビューの面倒臭さを解消する方法。下巻: Sub-agentsを用いてコンテキストを分離し、専門特化した「編集チーム」を作る方法。この環境を構築して気づいたのは、私の仕事が「執筆者（Writer）」から「編集長（Editor in Chief）」へとシフトしたということです。実際に手を動かして書く（Generate）のはAIでしょう。しかし、「何を書くか（企画）」「なぜ書くか（熱量）」「品質は十分か（承認）」を判断するのは、人間にしかできません。AIエージェントは、我々から仕事を奪うものではありません。我々を、より高次な意思決定を行う「マネージャー」へと押し上げてくれる存在です。もしあなたが「記事を書くのが面倒だ」「自分の文章に自信がない」と感じているなら、まずは小さなCommandを1つ作ることから始めてみてください。そこには、孤独な執筆作業とは違う、頼れるバディとの協働が待っているはずです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[iPadで学習してよかった話]]></title>
            <link>https://zenn.dev/akasan/articles/ipad_iiyone_apps</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/ipad_iiyone_apps</guid>
            <pubDate>Tue, 02 Dec 2025 12:36:22 GMT</pubDate>
            <content:encoded><![CDATA[今回は私が普段どうやってiPadを使って学習しているか共有しようと思います。 使っているiPad私が普段勉強で使っているiPadのスペックは以下になります。iPad mini A17 Proストレージ：256GBApple Pencilあり勉強用ではありますが、持ち運びを重視してminiにしています。もちろん大きな画面でノートを撮るといったような使い方はできませんが、特に不自由は感じてないです。 使っているアプリ Kindle元々は紙の本が好きではあったんですが、物理的に置く場所の限界が来たことや、どこでも時間が空いた時に読めることを重視してKindle上で書...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、がんばるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/02/124702</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/02/124702</guid>
            <pubDate>Tue, 02 Dec 2025 03:47:02 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先日、久しぶりに会った友人に言われた。「なんか最近、顔が疲れてない？」と。私は「まあ、仕事が忙しくて」と答えた。友人は「頑張ってるんだね」と言って、ビールを一口飲んだ。頑張ってる。その言葉を聞いた瞬間、なぜか胸のあたりがざわついた。褒められているはずなのに、全然嬉しくない。むしろ、何かを見透かされたような、居心地の悪さがあった。帰り道、ずっと考えていた。私は確かに頑張っている。毎日遅くまで働いているし、休日も勉強しているし、やるべきことは山ほどある。でも、だから何なんだろう。頑張っているから、何なんだ。30歳になった。節目だとか、大人になったとか、そういう感慨は特にない。ただ、20代の頃とは何かが決定的に違う。何が違うのか、最初はよく分からなかった。体力が落ちたとか、徹夜ができなくなったとか、そういう分かりやすい話でもない。しばらく考えて、ようやく気づいた。「頑張っている」という言葉が、免罪符にならなくなったのだ。20代の頃は、頑張っていれば許された。成果が出なくても、方向が間違っていても、「でも頑張ってるから」で何とかなった。周りもそう言ってくれたし、自分でもそう信じていた。頑張ることそのものに価値がある、と。でも30歳になって、その魔法が解けた。頑張っているのに何も変わらない自分がいて、頑張っているのに評価されない現実があって、頑張っているのに前に進んでいない焦りがある。頑張ることが、こんなにも虚しいとは思わなかった。これは、そういう話だ。頑張ることをやめろという話ではない。頑張り方を変えろという話でもない。ただ、「頑張っている」という言葉の正体について、30歳になった私が考えたことを書いてみようと思う。読んでも何も解決しないかもしれない。でも、同じようなことを感じている人がいたら、少しだけ楽になるかもしれない。そういう気持ちで書いている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。頑張ることの正体30歳の誕生日の夜、窓の外を眺めながら「今日も、頑張った」と思いました。でも、その言葉の後に続くはずの達成感はありませんでした。頑張りで全てを説明しようとしていた朝から晩まで働いていました。画面を見つめ、会議に出て、そこから開発をしていました。体は確かに疲れています。なのに、何も前に進んでいないという感覚が胸の奥に重く沈んでいるのです。社会人として8年が経ちました。20代前半の私は「頑張っている自分」が好きでした。努力している姿が自分の価値を証明してくれると思っていたからです。朝誰よりも早く出社し、夜遅くまで残り、休日も勉強する。その生き方が正しいと信じていました。しかし最近、ある事実に気づいてしまったのです。頑張ることそれ自体が、いつの間にか目的になっていたということに。本来、頑張ることは手段であるはずです。何かを達成するため、何かを得るため、どこかに到達するための手段。しかしいつの間にか、頑張ること自体が目的にすり替わっていました。「頑張っている自分」でいることが目的になり、その先に何があるのかを問うことをやめていたのです。ふと考えてしまいます。もし努力が一切報われない世界だったとしても、私はそれでもなお「頑張りたい」と願うだろうか。結果のために頑張っているのか。それとも、頑張ること自体が自分の生き方なのか。この2つは似ているようで、まったく違います。前者であれば、結果が出なければ頑張りは無意味になります。だから私たちは結果を求め、結果が出ないと焦り、自分を責めます。しかし後者であれば、結果に関係なく、頑張ること自体に意味があります。たとえ報われなくても、その過程に価値を見出すことができます。私は長い間、自分は後者だと思っていました。「努力することに意味がある」と信じていたからです。しかし正直に自分を見つめると、違いました。私は結果を求めていました。評価を求めていました。だから結果が出ないと苦しくなり、評価されないと自分を否定したくなったのです。もし本当に「頑張ること自体が生き方」なのだとしたら、結果が出なくても穏やかでいられるはず。しかし私はそうではなかった。頑張ることは純粋な生き方ではなく、結果を得るための手段だったのです。手段であるならば、その手段が有効かどうかを確かめなければなりません。目的地に近づいているかどうかを確認しなければなりません。しかし私は、頑張ること自体を目的にすり替えることで、そこを考えることから逃げていたのです。全部やろうとした結果具体的な話をさせてください。社会人になって数年目のことです。私は様々なことに挑戦させてもらっていました。自分の案件、登壇、ブログ執筆。それにまた、輪読会の運営、勉強会の主催、社内ドキュメントの管理と整備、新卒採用の担当。文脈のない色んなことを並列でやっていました。全部やりたかったのです。全部できると思っていました。結果として、全てが中途半端になりました。輪読会は準備不足で進行がグダグダになり、参加者が気まずそうに沈黙する場面が何度もありました。勉強会は告知が遅れて参加者が集まらず、3人しかいない会場で虚しくスライドをめくりました。ドキュメントは途中まで書いて放置され、それを指摘されることもないまま死にドキュメントが増えていきました。採用面談では候補者の情報を十分に把握できていないまま臨んでしまい、的外れな質問をして相手を困惑させました。自分の案件も遅れ、登壇の準備も直前までバタバタし、ブログは下書きのまま溜まっていきました。どれも「ちょっとずつダメ」だったのです。致命的な失敗ではない。でも、どれも胸を張って「やり遂げた」とは言えない。そして厄介なことに、中途半端にやっている間は、誰からもフィードバックをもらえなかったのです。なぜでしょうか。私が「頑張っているように見えた」からです。人は頑張っている人に「中途半端だ」とは言いにくいものです。遅くまで残っている。色々なことを引き受けている。一生懸命やっている。そういう姿を見ると、たとえ成果が出ていなくても「まあ、頑張ってるし」と見逃してしまう。指摘する側も遠慮してしまうのです。だから私は、自分が中途半端であることに気づけませんでした。周りも言ってくれないし、自分でも「頑張っている」という事実で目が曇っていたのです。ここで気づいたことがあります。私が選んだことだけでなく、選ばずに放置していたものが、私の人生を形作っていたということです。何かを選ぶとき、私たちは選んだものに意識を向けます。しかし、選ばなかったもの、手を付けずに残してしまったものについては、あまり考えません。でも実際には、その「選ばなかったもの」が積み重なって、今の自分を作っています。私の場合、「深く集中する時間」を選ばずに放置していました。「1つのことに没頭する経験」を選ばずに放置していました。全部やろうとすることで、何も深くやらないという選択を、無意識のうちにしていたのです。選択の影にあるもの——それを自覚することが、変わるための第一歩でした。総量が同じなら全部できるタイプの人もいるでしょう。器用にタスクを切り替えて、それぞれに必要な集中を注げる人。でも私は、おそらくそういうタイプではなかったのです。1つのことに深く集中しているときは力を発揮できる。でも、複数のことを並列で抱えると、どれにも集中できなくなる。頭の中が常に「あれもやらなきゃ、これもやらなきゃ」で埋まっていて、目の前のことに没頭できない。問題は、私が怠けていたことではありませんでした。全部やろうとしすぎていたことだったのです。そしてもう1つ気づいたことがあります。私が盲目的に全部を抱え込んでいる間、周りの人にも迷惑をかけていたということです。中途半端な準備で運営した勉強会に参加してくれた人たち。私の遅れのせいでスケジュールを調整しなければならなかったチームメンバー。頑張ることは、時に暴力になります。自分だけでなく、周りの人も苦しめてしまうのです。頑張らないことへの恐怖こうした経験があっても、頑張ることをやめるのは難しい。頑張ることに疲れたと思った瞬間、罪悪感が襲ってきます。「頑張らないなんて怠け者だ」「頑張らなかったら停滞してしまう」。心の中で誰かの声が私を責めるのです。この恐怖はどこから来るのでしょうか。少し立ち止まって考えてみると、そこには1つの混同があることに気づきます。私たちは「頑張らないこと」と「怠けること」を同じものだと思い込んでいるのです。しかしある時気づきました。頑張らないことと怠けることは違い、そして頑張ることと前に進むことも違うのだということに。これを整理すると、こうなります。「頑張る」とは、エネルギーを注ぎ込むことです。「前に進む」とは、目的地に近づくことです。そして「怠ける」とは、必要なことをしないことです。エネルギーを注ぎ込んでも、方向が間違っていたら目的地には近づきません。逆に、エネルギーを節約しても、正しい方向に進んでいれば目的地に近づくことができます。頑張りすぎて何も達成できないより、戦略的に力を抜いて1つを確実に達成した方が価値がある。頑張らないことへの恐怖を掘り下げていくと、その根底にあるのは「失敗への恐れ」でした。しかし、よりその下を掘ると、本質的な恐怖が見えてきます。私が本当に恐れていたのは、失敗そのものだったのか。それとも、「誰かに失敗を見られること」だったのか。私が本当に恐れていたのは、失敗そのものではありませんでした。失敗を誰かに見られること、「あいつは頑張らなかったから失敗した」と思われること、それが怖かったのです。一人で挑戦して一人で失敗するのは、実はそこまで怖くありません。痛いけれど、学びになります。しかし、その失敗を誰かに目撃されること、評価されること、噂されること——それが耐えられなかったのです。つまり、私の恐怖の本質は「社会的評価への恐れ」でした。自分自身の内側の痛みではなく、他者の目に映る自分の像への恐れだったのです。この区別は重要です。なぜなら、恐怖の正体を知ることで、対処の仕方が変わるからです。失敗そのものが怖いのであれば、リスクを減らす工夫をすればいい。しかし「失敗を見られること」が怖いのであれば、問題は失敗ではなく、他者の評価に自分の価値を預けすぎていることにあります。頑張ることをやめて考えることを始めた時、初めて前に進み始め、結果を出せるようになりました。頑張ることへの依存頭では分かっていても、頑張ることをやめられませんでした。私はたぶん、頑張ることに依存していたのです。朝起きるとすぐに仕事を始め、休憩も取らずに夜遅くまで働いて疲れ果てて眠り、土日も「せっかくの時間だから」と何かをしていました。「何もしない時間」が怖かったのです。なぜ怖かったのか。それは、何もしていない自分に価値がないと思っていたからです。この考えをもう少し掘り下げてみましょう。私は無意識のうちに、「自分の価値 = 自分がどれだけ頑張っているか」という等式を信じていました。頑張っていない自分は価値がない。価値のない自分を見たくない。そう感じていたから、常に何かをしている必要があり、頑張っている自分でいる必要があったのです。「頑張らなければ価値がない自分」と、「頑張っていなくてもここにいていい自分」。この2つのうち、私は本当はどちらを生きたいのだろう。これは「どちらが正しいか」という論理の問題ではありません。「どちらを選びたいか」という願望の問題です。頭では「頑張っていなくても価値がある」と分かっています。そう言われれば、そうです。でも、本当にそれを信じているかと問われると、自信がありません。心のどこかで「でも頑張らないと......」という声がするのです。その声の正体を知ることが、変わるための第一歩でした。答えは、すぐには出ませんでした。でも、この疑問を抱え続けることが大切でした。論理ではなく願望のレベルで、自分が何を求めているのかを探ること。それが、変わるための出発点になったのです。しかし不思議なことに、頑張れば頑張るほど、成果は出なくなっていきました。うまくいかない理由は頑張りすぎていたからです。頑張ることが思考を停止させていて、「とりあえず頑張る」「とにかく動く」と考えることから逃げていたのです。「頑張ります」という特権振り返ってみると、若い頃の私にはある種の特権がありました。「頑張ります」と言えば、それで許されていたのです。計画が甘くても「頑張ります」、ミスをしても「頑張ります」、結果が出なくても「頑張ります」と言えば許されていました。周囲は「若いんだから」「まだ経験が浅いんだから」「熱意があればいい」と納得してくれたのです。20代前半は特にそうでした。何も考えずにとにかく動き、深夜まで働き、休日も出社していれば評価されました。方向性が間違っていても、やり方が非効率でも、「頑張っている」という事実が全てを覆い隠してくれたのです。「頑張ります」は、思考停止の免罪符でした。考えなくてよく、戦略を立てなくてよく、ただ熱意を見せればよかったのです。がむしゃらは若さという資本で買えた特権だったのです。そしてその「がむしゃら」が、ある種の万能感を生んでいました。体力や気力は無限にあり、睡眠を削っても平気で、理想の自分に向かって駆け上がっていく。そんな勢いが許されていて、いやむしろ求められていたのです。今、手放せずに握りしめている「頑張り」は、本当に自分を守っているのだろうか。それとも、もう要らなくなった古い防具なのだろうか。かつて「頑張ること」は、私を守ってくれました。若くて経験がなくて、何も分からない時期に、「とにかく頑張る」という姿勢は、私の居場所を確保してくれました。がむしゃらに動くことで、「あいつは一生懸命やっている」と認めてもらえたのです。しかし、時間が経ちました。状況が変わりました。求められることも変わりました。かつて私を守ってくれた防具が、今は私の動きを制限しているのではないか。重すぎて前に進めなくなっているのではないか。そう考え始めた時、その防具を一度外してみる勇気が必要でした。転換点という現実しかしその「がむしゃらが許される特別な時間」は、予告なく終わります。私の場合、それは20代後半でした。ある日突然、それまで当たり前にできていたことができなくなりました。朝起きることも人と話すことも簡単な判断さえも重荷になって、「頑張ります」と言ってももう体が動かなくなったのです。今思えば、それはいつか必ず訪れる終わりでした。30歳という年齢は、「頑張ります」だけでは通用しなくなる境界線なのです。この変化はいくつかの形で現れます。まず、周囲の目が変わります。「頑張っている」だけでは評価されなくなります。「で、結果は」「で、どう改善するの」「がむしゃらにやるんじゃなくて、戦略は」と容赦なく聞かれるようになります。30歳は、熱意ではなく戦略が問われる年齢でした。「頑張っている」と「前に進んでいる」は別物だったのです。次に、身体の限界が見えてきます。20代のように無理が効かなくなり、深夜まで働いたら翌日に響き、休日を潰したら週明けのパフォーマンスが落ちます。がむしゃらはもはやコストの方が大きいのです。そして何より、自分自身が「このまま走り続けることに意味があるのか」と考え始めます。がむしゃらに頑張っても前に進んでおらず、ただ消耗しているだけ。そんな実感が、重くのしかかってくるのです。走り続けることと、前に進むことは違う。この当たり前の事実に、私は30歳になってようやく気づきました。なぜ私たちは頑張ってしまうのかしかし、なぜ私たちはそもそもこうなってしまうのでしょうか。なぜ、頑張ってしまうのでしょうか。私なりの答えは、簡単な答えが欲しいからというものです。どういうことか説明させてください。私たちが生きている現実は複雑です。何が正しいのか分からない。どの選択が最善なのか分からない。努力が報われるかどうかも分からない。そういう不確実性の中で生きることは、とても不安なことです。その不安に耐えられないとき、私たちは「頑張れば救われる」という単純で分かりやすい物語の中に逃げ込みます。この物語の中では、何をすべきかが明確です。とにかく頑張ればいい。努力すればいい。諦めなければいい。ネガティブ・ケイパビリティという言葉があります。不確実さや曖昧さに耐える能力のことです。「自分にもあるだろう」などと言ってみたりしますが、実際には、自分が見えている物語があまりにも狭いだけなのです。「頑張る」という単純な行動原理で、複雑な問題を考えずに済ませているだけなのです。頑張っている間は「前に進んでいる」という錯覚が得られて充実感があります。この充実感が曲者です。なぜなら、その錯覚が問題から目を背けさせ、「方向性が間違っているのではないか」という疑問を封じ込めてしまうからです。思考の罠では、なぜ私たちは頑張ることの問題点という明らかな事実に気づけないのでしょうか。その答えは、私たちの思考の仕組みにあります。自分の判断パターンに気づいたことがあります。結論が先にあって、その結論を支持する証拠だけを集め、矛盾する情報は無視していたのです。そして厄介なことに、その正当化のプロセスがあまりにも自然で論理的に見えるため、本人も気づかないのです。自分の信念を守るために、思考を使ってしまうという、これは無意識の傾向です。具体例を挙げましょう。「頑張れば報われる」という信念が先にあって、その信念を支持する証拠だけを集めていました。努力した人の成功例は記憶に残るのですが、努力したのに報われなかった人の存在は意識から消えていってしまいます。30歳になって振り返ると、20代の私は恐ろしいほど確信に満ちていました。「この方法が正しい」「これだけやれば必ず成功する」と疑うことを知らず、いや疑うことを恐れていました。自分の間違いを認めることこの思考の罠から抜け出すために必要なものがありました。自分が間違っているだろうと認めることです。これは簡単なようで、とても難しいことでした。私は「頑張ることは正しい」と信じていました。だから、頑張っても成果が出ない時、「もっと頑張れば」と考えていました。頑張ることが正しいという前提を疑うことは、自分の生き方を否定することのように感じられたのです。しかしある時、意識的に自分の前提を疑ってみることにしました。「頑張らない方がうまくいくことはないか」と。すると、思い当たることがいくつも出てきました。休みを取った翌日の方が、良いアイデアが浮かぶ。締め切りに追われていない時の方が、コードの質が高い。夜遅くまで粘るより、翌朝やり直した方が早く終わる。これは全て、私自身が経験していたことでした。でも「頑張ることは正しい」という信念が強すぎて、その経験を無視していたのです。見たくないものは、見えないようにするというのが、人間の脳の仕組みなのだと知りました。だからこそ、意識的に自分の前提を疑う必要があります。「自分は正しい」という確信から一歩引いて、「自分は間違っているだろう」という可能性を常に心に留めておくこと。それが、思考の罠から抜け出す第一歩でした。確信は、時に最大の敵になる。有限であることを知っている、でも分かっていないでは、なぜ私たちはわざわざこの思考の罠にはまってしまうのでしょうか。なぜ、自分の信念を守ろうとするのでしょうか。その背景には、1つの根本的な事実から目を背けたいという欲求があると私は考えています。それは、人生は有限であるという事実です。この事実を、私たちは「知っている」はずです。人はいつか死ぬ。時間には限りがある。当たり前のことです。でも、本当に分かっているかというと、そうではないのです。思い出してみてください。中学や高校の卒業式の日のことを。「あー、もっと何かできてたな」と思いませんでしたか。部活にもっと打ち込めばよかった。あの子ともっと話せばよかった。文化祭でもっと楽しめばよかった。卒業式の日、私たちは3年間が有限だったことを、ようやく実感します。でも、その実感はすぐに消えるのです。大学に入り、社会人になり、日常に戻ると、また時間が無限にあるかのように振る舞い始めます。「いつかやろう」「そのうち学ぼう」「まだ時間はある」と。30歳になった時、ふと計算してみました。80歳まで生きるとして、残りは50年。週に換算すると約2600週。月に換算すると約600ヶ月。この数字を見た時、卒業式の日の感覚が蘇ってきました。思ったより、少ないのです。でも、きっとこの実感もまた薄れていくのでしょう。明日になれば、来週になれば、また時間が無限にあるかのように振る舞い始める。それが人間なのです。だからこそ、意識的に思い出す必要があるのです。時間は有限であること。すべてをやることは不可能であること。何かを選ぶということは、何かを諦めるということ。この事実を忘れそうになるたび、卒業式の日の感覚を思い出すようにしています。時間管理術という逃避しかし、この事実を常に意識し続けることは難しいものです。むしろ、私たちは無意識のうちにこの現実から目を背けようとします。その典型的な方法が、時間管理術です。「もっと効率的に」「もっと生産的に」と時間管理術に縋りつくのは、現実から目を背けているだけなのです。どれだけ効率化しても、時間は増えないのです。時間管理術は「もっと多くのことができるようになる」という幻想を与えてくれます。しかし実際には、私たちにできることの総量は変わりません。ただ、その有限性を見ないようにしているだけなのです。ここで逆説的なことが起きます。限られた時間を受け入れることが、実は自由への第一歩なのです。すべてをやることを諦めた時、初めて「本当にやりたいこと」が見えてきます。「やるべきこと」ではなく「やりたいこと」へ集中できるようになります。選ばなければならないという制約が、逆に選択を可能にするのです。忙しさというステータス時間が有限だと分かっていても、人は忙しさを求めます。私もそうでした。「忙しい」と言うことが、ある種のステータスでした。忙しい = 重要な仕事をしている = 価値があるという等式を、疑うことなく信じていたのです。しかし冷静に考えるとおかしな話です。忙しいことと価値を生むことは別のことです。では、なぜ私たちは忙しくなるのでしょうか。理由はいくつかあります。優先順位がついていないから。断れないから。そして何より忙しさそのものを求めているからです。暇になることが怖い。何もしていない時間が耐えられない。だから予定を埋める。忙しくする。これは最初に述べた「頑張ることへの依存」と同じ構造です。意味のない努力忙しくしているうちに、私はたくさんの意味のない努力をしていました。完璧な資料を作るために、美しいデザイン、詳細な分析、見栄えの良いグラフを何日もかけて作ります。しかし実際に見られるのは最初の数ページだけです。定期的な報告のために資料を作って説明して質疑応答する時間を、毎週毎月確保しています。しかしその時間で議論される内容はメール一通で済む内容だったりします。これは全て、「頑張っている感」を得るための努力でした。実際に価値を生むための努力ではなく、自分と周囲に「頑張っている」と思わせるための努力だったのです。なぜこんなことをしていたのでしょうか。「頑張っていない自分」が怖かったからです。「何もしていない」と認めることが怖かったから、何かをしている「ふり」をしたのです。しかしそのせいで、意味のあることをする時間がなくなってしまいました。意味のない努力が、意味のある努力を駆逐していたのです。なぜ意味のない努力を選んでしまうのかこれは努力の世界における残酷な法則です。なぜ残酷かというと、意味のない努力の方が楽で、見た目の成果が出やすいからです。比較してみましょう。完璧な資料を作ることは無理ですが、時間をかければ見栄えはかなり良くなります。しかし複雑な問題を本質的に解決することは難しく、時間をかけてもできるとは限りません。会議に出席することは簡単です。座って話を聞いてたまに発言すればいい。しかし深く考えて独創的な解決策を生み出すことは難しく、孤独で不確実で失敗するだろう。だから人は無意識に意味のない努力を選びます。一日の大半を意味のない努力で埋めてしまうため、本質的な努力をする時間がなくなってしまうのです。楽な努力が、本当の努力を駆逐する。何もしない時間の価値この悪循環を断ち切るために、ある日、試しに一日何もしない時間を作ってみました。会議もキャンセルし、メールも見ずに、ただ窓の外を眺める時間を確保しました。最初は不安でした。「こんなことしていていいのか」「時間を無駄にしているのではないか」と。この不安は、最初に述べた「何もしていない自分に価値がない」という信念から来ています。しかし一時間、二時間と過ごすうちに何かが変わりました。頭の中がクリアになって、今まで見えなかったものが見えるようになったのです。忙しさは、思考を停止させます。忙しい状態では「これって意味あるのか」と問う余裕がないため、意味のないことを延々と続けてしまうのです。そのとき、ふと考えました。何も生み出していない時間や、誰からも評価されない時間にさえ、私の人生の価値は宿りうるのだろうか。窓の外を眺めているだけの時間。何も「生産」していない時間。誰にも見られていない時間。そういう時間に、価値はあるのでしょうか。最初、私は「いいえ」と答えていました。価値とは、何かを生み出すことで生まれるものだと思っていたからです。成果があってこそ価値がある。評価されてこそ価値がある。そう信じていました。しかし、何もしない時間を過ごしているうちに、考えが変わってきました。その時間は、確かに何も「生産」していませんでした。でも、自分の中で何かが整理され、何かが癒され、何かが育っていたのです。それは目に見える成果ではありませんでしたが、確かに何かが起きていました。生産性や成果や他者評価——そういったものを全部はがした後に残るもの。それが「自分の時間」の価値なのだろう。何かを生み出すための時間ではなく、ただ存在するための時間。そういう時間があっていいのだと、少しずつ思えるようになりました。忙しさという霧が晴れて本質が見えたとき、気づきました。今までやっていたことの半分以上は実は必要なく、頑張っていたけれど価値を生んでいなかったのです。立ち止まった時間が、一番遠くまで連れて行ってくれた。選択という技術何もしない時間を作ったことで、30歳になって学んだ最も重要なことの1つが見えてきました。それは、選択することの重要性です。若い頃は「全部やろう」としていました。新しい技術が出れば学び、新しいプロジェクトがあれば参加し、頼まれた仕事は全て引き受けていました。確かに、若い頃や自分の成長を誰かが見守ってくれる時期には、それも良いだろう。がむしゃらに量をこなすことで、見えてくるものはあります。しかしそれだけではありません。自分の能力を発揮できる環境を自分で選び、作ることもまた、自分の能力なのです。全部やろうとし続けると、何が起きるでしょうか。エネルギーが分散してどれも中途半端になり、重要なことに十分な時間と集中を注げなくなります。そして何より、自分が得意なこと、やりたいことが見えなくなってしまいます。若い頃からやりすぎると、自分の可能性を狭めてしまう可能性があるのです。すべてに手を出すことで、「自分は何でもそこそこできる人」にはなれるだろう。しかし「この領域では誰にも負けない」という強みは育ちません。ある時、尊敬する先輩に「どうやったら全部うまくできますか」と相談しました。彼は笑って「全部うまくやろうとするな。1つだけ、圧倒的にうまくやれ」と言いました。「勝てる領域を見つけろ」と彼は続けました。「君が他の誰よりも価値を出せる領域、そこに全てを賭けろ。他は最低限でいい」と。集中することで見えてきたものその日から自分の「勝てる領域」を探し始めました。自分は何が得意なのか、どこで他の人と差別化できるのか。振り返ってみると、私が価値を生んでいたのは、複雑な問題を構造化してシンプルな解決策を示すことでした。資料を何百枚作ることでも、会議を何時間することでもありませんでした。でも当時の私は、そのことに気づいていませんでした。すべてを同じように頑張っていたからです。得意なことと苦手なこと、重要なことと些細なこと、すべてに同じエネルギーを注いでいました。それからは、「勝てる領域」へ集中することにしました。複雑な問題に向き合う時間を最大化し、他の作業を最小化しました。すると不思議なことが起きました。仕事の質が上がり、周囲の評価も上がり、そして忙しさは減ったのです。やることを減らしたのに、成果は増えた。これは最初、信じられませんでした。でも考えてみれば当然のことでした。苦手なことに時間を使っていた分を、得意なことに回しただけなのです。同じ時間を使っても、得意なことの方が成果は出ます。これは怠けているわけではありません。戦略的に力を配分しているだけなのです。やめることを選ぶ選択するということは何かを捨てることです。これが最も難しいことでした。私たちは何かを捨てることに恐怖を感じます。「後で必要になるだろう」「チャンスを逃すだろう」と考えてしまいます。しかし、「やらないこと」を選ぶ決断こそが、人生における優先順位を明確化する鍵なのです。ここでもう一度、選択の影について考えてみます。私は「何を選ぶか」については意識していましたが、「何を選ばずに残してしまっているか」については、ほとんど意識していませんでした。やめることを選ぶとき、私たちは選んだこと（やめること）に意識を向けます。しかし同時に、「続けること」を選んでいるのです。その「続けること」は、続ける価値があるものでしょうか。無意識のうちに惰性で続けているだけではないでしょうか。私は「To Stopリスト」を作り始めました。やることリストではなく、やめることリストです。意味のない定例会議に出席するのをやめました。完璧な資料を作るのをやめました。すべての技術トレンドを追うのをやめました。頼まれた仕事を全て引き受けるのをやめました。忙しいふりをするのもやめました。最初は罪悪感がありました。しかしやめてみると驚きました。誰も困らなかったのです。むしろ重要なことへ集中できるようになって、成果が上がりました。やめることと怠けることは違います。それは本質に集中するための戦略なのです。捨てることが、得ることの始まりだった。努力はベクトルだここまで読んで、頑張ること自体が悪いのだと思われただろう。しかし、そうではありません。問題は「どう頑張るか」なのです。頑張ることは、ベクトルです。大きさだけじゃなく、方向があるのです。どれだけ大きな力で頑張っても、方向が間違っていたら目的地には着きません。むしろ遠ざかっていくのです。多くの人はベクトルの「大きさ」ばかりに注目します。「もっと頑張る」「もっと努力する」「もっと時間をかける」と考え、方向については考えません。しかし重要なのは方向です。間違った方向に全力で走るより、正しい方向にゆっくり歩く方が、目的地には早く着くのです。そして、その「方向」を決めるとき、また同じところに戻ってきます。「前に進む」とは、いったい誰の物差しで測られる「進歩」なのか。社会が示す方向に進むことが「前」なのか。それとも、自分が心から望む方向に進むことが「前」なのか。そこに答えを出さないまま、ベクトルの大きさだけを増やしても、どこにも到達けないのです。努力と評価のミスマッチ努力の方向が間違っていると、どうなるでしょうか。努力と評価が一致しない場所で頑張り続けることになります。それは、尋常ではないほど辛いものです。やっても認められない。いくら頑張っても成果として認識されない。「こんなに頑張っているのになぜ評価されないんだろう」という疑問は、やがて「自分には才能がないのだろう」という絶望に変わっていきます。しかし、ここで立ち止まって考えてみましょう。問題は才能ではなく、環境とのミスマッチなのだろう。あなたの能力が発揮されない環境。あなたの強みが評価されない組織。あなたの価値が認識されない役割。そういう場所でどれだけ頑張っても報われません。これは残酷な事実ですが、同時に希望でもあります。なぜなら、環境は変えられるからです。才能がないのではなく、場所が合っていないだけなら、場所を変えれば状況は改善する可能性があるのです。能力とは環境との相互作用ここで、根本的な認識を改める必要があります。「能力」とは、環境との相互作用の中で初めて発揮されるものなのです。ある環境では高いパフォーマンスを出せる人が、別の環境では全く力を発揮できない。珍しいことではありません。むしろ普通のことです。私自身、この事実を身をもって経験しました。ある組織でやりたくない仕事を頑張り、長時間働いて必死に努力しました。しかし成果は出ず、評価も上がらず、自己肯定感は下がり続けて、「自分は仕事ができない」と思っていました。しかし環境を変えた瞬間、すべてが変わったのです。同じ私が違う組織、違う役割で働き始めると、成果が出て評価され、自己肯定感が戻ってきました。私の「能力」は変わっていませんでした。変わったのは環境だったのです。ですから「自分には能力がない」という結論は早計です。正確には「この環境では、自分の能力が発揮されない」ということなのです。この認識は重要です。なぜなら、「能力がない」という結論は絶望につながりますが、「環境が合っていない」という認識は行動につながるからです。頑張りで全てを説明しようとしていた私は長い間、すべてを「頑張り」で説明していました。環境のことなど、考えもしませんでした。成果が出ない時は「自分がもっと頑張ればよい」と思っていました。だから、もっと時間をかけ、もっと努力し、もっと自分を追い込みました。成果が出た時は「自分が頑張ったから」と思っていました。だから、次も同じように頑張れば、同じように成果が出ると信じていました。うまくいかないのは環境のせいではなく、自分の努力が足りないせい。うまくいったのは環境のおかげではなく、自分の努力のおかげ。すべての原因を「自分の頑張り」に帰属させていたのです。この考え方は、一見すると責任感があるように見えます。「環境のせいにしない」「自分でコントロールできることに集中する」。でも、実際にはこれは視野の狭さでした。なぜなら、同じ努力をしても、環境によって成果は大きく変わるからです。自分の強みが発揮される環境なら、少ない努力で大きな成果が出ます。自分の強みが発揮されない環境なら、どれだけ努力しても成果は限られます。そしてもう1つ、認識しておくべきことがあります。「自分の能力が発揮されない環境」は、常に存在しているということです。どんな組織にも、どんな役割にも、自分に合わない部分があります。完璧にフィットする環境など存在しません。大切なのは、それを認めることです。「ここは自分に合っていない」と認めることは、敗北ではありません。むしろ、そこから戦略が始まります。合わない部分を認めるからこそ、「ではどうするか」を考えられるようになるのです。私は長い間、合わない部分を認めることができませんでした。「もっと頑張れば何とかなる」と思い続けていました。でも実際には、何ともならなかったのです。ただ消耗しただけでした。この事実に気づくまで、私は長い時間を要しました。そして気づいた時、ようやく「どこで頑張るか」を考えられるようになったのです。勝てる領域を見つけるでは、どうすれば「勝てる領域」を見つけられるのでしょうか。これはあくまで私の場合の話ですが、無意味な場所で頑張らず、能力が発揮される場所で努力することが、私が燃え尽きずに長く走り続ける秘訣でした。私は、自分にとって意味の分からない仕事を無限にできる耐久性の高い人間ではありませんでした。合わない環境で合わない仕事を続けることは苦痛でしかありませんでした。それは弱さだろうが、それが私の現実だったのです。私の場合、開発全般が得意でした。設計と開発、どちらも能力を発揮できて楽しいのです。しかしやってはいけなかったのは、マルチタスクをしながら人との調整やステークホルダー管理を大量にこなすことでした。この能力が著しく低く、全体の生産性がとても下がってしまったのです。最初は周囲の期待に応えようとして、開発をしながら調整業務もこなそうとしました。しかし評価されませんでした。「中途半端だ」と言ってもらえればまだ良かった。そうではなく、評価が低いだけ。何が問題なのか分からないまま、成果の出ない日々が続きました。しかしある程度裁量をもらい、開発に集中し始めたら状況が変わりました。「この実装すごく良い」と言われるようになって、チーム全体の生産性が上がり、そして私の評価も上がったのです。勝てる領域とは、自分の能力と環境のニーズが交わる場所です。自分が得意でも誰も必要としていなければ評価されず、環境が必要としていても自分ができなければ価値を出せません。その交点を見つけてそこに集中すること、それが努力の方向性を正しく定める方法でした。戦う場所を選ぶことが、戦い方を決める。環境という見えない制約ここまで読んで、あなたはこう考えるだろう。「確かに正しい場所で頑張ることは重要だけれど、そもそも『自分の能力が発揮される環境』なんて、どうやって見つければいいのか」と。その通りです。自分の能力が発揮される環境は簡単には見つかりません。そしてもっと現実的な問題があります。今いる環境が自分に合っていないと分かっても、すぐには動けないのです。住宅ローンがある。家族を養っている。転職するには経験が足りない。業界の状況が悪い。様々な制約が私たちを今の場所に縛り付けています。だから、戦術的な頑張りも必要なのです。これは矛盾しているように聞こえるだろう。今まで「頑張りすぎるな」と言ってきたのに、「頑張りも必要」と言うのは。しかし、これは矛盾ではありません。問題は「頑張ること」自体ではなく、「考えずに頑張ること」だったのです。戦略を持った上での戦術的な頑張りは、必要なものです。持続可能性という解答ここまで、頑張ることの問題点と、選択と集中の重要性を述べてきました。では、具体的にどうすればいいのでしょうか。私が見つけた答えは、持続可能性でした。面白いことに気づきました。頑張る量を減らしたら、成果が増えたのです。ある時、私は思い切って変えてみることにしました。やるべき仕事とやらない仕事を分けて、不要なミーティングに出なくなりました。やりたくない仕事を整理させてほしいと相談したのです。すると不思議なことが起きました。勤務中の8時間の質が劇的に上がったのです。なぜこうなったのか。理由は単純でした。「この8時間だけが自分の時間だ」と考えると一瞬たりとも無駄にできなくなり、集中力が持続して疲労が少なくなり、翌日もまた集中できるようになったのです。無駄な時間が減りましたが、学びの質は上がりました。必要なことだけを学ぶようになり、「やらなきゃ」ではなく「やりたい」で動くようになったのです。この経験から1つの原則を学びました。持続可能性が、成果を生むという原則です。一時的には全ての時間を注ぎ込む方が多く成果を出せるように見えます。しかし長期的には持続可能なペースの方がずっと多くの成果を生むのです。無理をして一気にやろうとすると、どこかで必ず破綻します。体調を崩すか、質が落ちるか、燃え尽きるか。そして破綻した後のリカバリーには、節約できたはずの時間よりもずっと長い時間がかかるのです。新しいやり方の始まり持続可能性を意識することで、新しいやり方が始まりました。無理をしない働き方。自分の限界を知った上でのアプローチ。がむしゃらではなく戦略的なやり方。私の新しいやり方は、「頑張ります」という言葉を封印することから始まりました。最初は怖かったのを覚えています。「頑張らない」と言ったら「やる気がない」と思われるんじゃないか、評価が下がるんじゃないかと心配していました。しかし違ったのです。「頑張ります」をやめて「こうします」と言い始めた時、初めて信頼されるようになりました。具体的な計画を示す。達成可能な目標を設定する。リスクを評価する。代替案を用意する。そして結果を出す。がむしゃらな熱意ではなく冷静な戦略で勝負するやり方に変えたのです。頑張ることをやめたら時間ができました。その時間で考えることができました。「今の仕事は本当に自分がやりたいことなのか」「この関係性は本当に大切にしたいものなのか」「この努力は本当に価値を生んでいるのか」と。そして気づきました。今まで「頑張らなきゃ」と思ってやっていたことの多くは、実は自分が本当にやりたいことではなかったのです。社会的な期待に応えるため、周囲に認められるため、「できる人」に見られるため、そういう外的な動機で動いていたのです。しかし30歳になって、もうそういう生き方は続けられないと悟りました。体力的な限界、精神的な限界、そして何より残りの人生をそんな生き方で使いたくないと思ったのです。がむしゃらで許された特別な時間の終わりは、敗北ではありません。より賢く、より持続可能なやり方への転換点なのです。自己犠牲という承認への飢え新しいやり方を始めてから、もう1つ重要なことに気づきました。それは、自分を大切にすることと他者を大切にすることのバランスについてです。「他人を優先する自分」でしか価値を感じられない人がいます。自分のニーズを無視して他人に尽くすことで「必要とされている感覚」を得ているのです。一見すると優しさに見えます。しかし、実はこれは承認への飢えなのです。自分の時間を全て他人に捧げる。自分の希望を後回しにする。常に誰かの期待に応える。自分が疲れていても「頼まれたから」と引き受ける。その自己犠牲によって「自分は良い人だ」「自分は必要とされている」と感じているのです。しかし、健全ではありません。自分を大切にできない人は、結局他人を大切にできないからです。見返りを期待する優しさなぜ自己犠牲が健全でないのか、もう少し詳しく説明させてください。自分を犠牲にして他人に尽くすと、無意識のうちに「見返り」を期待するようになるのです。「こんなに頑張ったんだから感謝されるべきだ」「こんなに尽くしたんだから認められるべきだ」という気持ちが湧いてきます。そしてその期待が満たされないと怒りや不満が生まれます。「こんなに頑張ったのに」「こんなに尽くしたのに」と相手を責める気持ちが湧いてきます。これは優しさとは違います。相手のためではなく自分の承認欲求を満たすための行為なのです。見返りを期待しない優しさもあります。相手のために行動し、その結果がどうであれ満足できる。私はそういう優しさを持ちたいと思いました。しかし自分が満たされていない状態では、その無条件の優しさを持つことは難しいのです。まず自分を満たすことだからこそ、まず自分を満たすことが大切なのです。これは理屈としては分かりやすい話です。でも、実行するのは難しいのです。なぜなら、自分を後回しにすることが習慣になっているからです。私の場合、常に誰かのために動いていました。チームのため、会社のため、プロジェクトのため。そう言えば聞こえは良いのですが、実際には自分のことを考える余裕がなかっただけでした。そしてある時、限界が来ました。誰かのために動く気力すら湧かなくなったのです。その時ようやく気づきました。自分が枯れていたら、誰かに何かを与えることはできないのだと。自分を大切にすることは、自己中心的なことではありません。持続可能に誰かを助けるための前提条件なのです。自分の限界を知る。自分のニーズを尊重する。時には「できない」と言う勇気を持つ。これは全て、より長く、より健全に他者を大切にするための準備なのです。そして自分が満たされた状態から他人を助ける。見返りを期待せず純粋に相手のために行動する。私はそういう優しさを持ちたいのです。空っぽの器からは、何も注げない。フェーズによる変化しかしここでも1つ大切なことを付け加えます。キャリアのフェーズによって、求められることは変わるということです。ジュニアの頃は、がむしゃらでも許されました。むしろ、がむしゃらであることが求められていました。何も分からないのだから、とにかく量をこなせ。失敗してもいいから、手を動かせ。その時期に「効率」や「戦略」を語るのは早すぎたのです。しかしミドルになると、状況が変わります。「頑張っています」だけでは評価されなくなります。「で、結果は」「で、何を学んだの」と問われるようになります。がむしゃらに動くだけでなく、方向性を持って動くことが求められるのです。そしてシニアになると、より変わります。自分が頑張ることよりも、チーム全体の成果が問われます。自分一人で抱え込むのではなく、任せることが求められます。「自分が頑張る」から「みんなが頑張れる環境を作る」へ。役割が変わるのです。私は今、ミドルからシニアへの過渡期にいます。ジュニアの頃のやり方が通用しなくなり、新しいやり方を模索している時期です。また同じことを考えます。今、手放せずに握りしめている「頑張り」は、本当に自分を守っているのだろうか。それとも、もう要らなくなった古い防具なのだろうか。ジュニアの頃、「とにかく頑張る」という姿勢は私を守ってくれました。何も分からなくても、がむしゃらにやっていれば居場所がありました。しかし今、同じ姿勢を続けることは、私を守るどころか、足を引っ張っています。かつて自分を守ってくれた「頑張り方」が、フェーズが変わった今もまだ有効なのか。それとも、アップデートすべきなのか。そこに正直に向き合う必要がありました。重要なのは、今の自分がどのフェーズにいるかを認識することであり、そのフェーズに応じたやり方を選ぶことです。ジュニアのやり方をミドルになっても続けていたら、消耗するだけです。ミドルのやり方をシニアになっても続けていたら、チームの足を引っ張ります。フェーズが変われば、やり方も変えなければならないのです。この文章で私が伝えたいのは「頑張るな」ということではありません。「今の自分のフェーズに合った頑張り方を選べ」ということなのです。しかし、1つ補足があります。自分では気づけなくても、上司やマネージャーが適切にコントロールしてくれている場合があるということです。私の場合も、振り返ってみれば、良い上司に恵まれていた時期は自然と適切な仕事量に調整されていました。「それは引き受けなくていい」「今はこっちに集中して」と言ってもらえていたのです。当時は気づいていませんでしたが、それは上司が私の状態を見て、適切に仕事を配分してくれていたからでした。逆に言えば、自分が上司やチームリーダーになった時には、同じことをする責任があるということです。メンバーが頑張りすぎていないか。中途半端になっていないか。「頑張っているように見える」からといって見逃していないか。そして、必要であれば「それはやらなくていい」と言えているか。人は頑張っている人に「中途半端だ」とは言いにくいものです。だからこそ、上司やリーダーは意識的にそれを言う必要があります。言わなければ、かつての私のように、本人は気づかないまま消耗していくのです。「おい、がんばるな」と言ってあげられる人になること。それもまた、フェーズが変わった時に求められる役割なのです。「頑張る自分」というアイデンティティ最後に、最も根深い問題について話させてください。私は「頑張る自分」というアイデンティティに縛られていました。「私は頑張る人だ」「私は努力家だ」「私は諦めない」というような自己像があり、その自己像を守るために頑張り続けなければいけなかったのです。しかしそれは苦しいものでした。「頑張る自分」であり続けるために休めず、立ち止まれず、弱音を吐けなかったのです。「頑張る自分」というアイデンティティが自分を縛る檻になっていました。ある日ふと気づきました。私は「頑張る」ということ自体にしがみついていて、成果を出すためではなく「頑張る自分」でいるために頑張っていたのです。そしてまた、同じところに戻ってきます。「頑張らなければ価値がない自分」と、「頑張っていなくてもここにいていい自分」のどちらを、本当は生きたいのか。頭で考えれば、答えは明らかです。「頑張っていなくても価値がある」と信じたい。でも、心の奥底では、まだその確信が持てませんでした。しかし、考え続けることで、少しずつ変わってきました。そしてもう1つ気づきました。頑張っていなくても自分に価値があるということに。成果を出していなくても自分に価値がある。忙しくなくても自分に価値がある。価値は頑張ることから来るのではなく、存在することそのものに価値があるのです。これは宗教的な話ではなく実際的な話です。頑張り続けて壊れた人をたくさん見てきました。優秀な人ほど「もっとできるはずだ」と自分を追い込んで限界を超えて壊れてしまいます。そして壊れたら何も生み出せなくなってしまいます。何も生み出していない時間にも、価値はあります。誰からも評価されない時間にも、意味があります。生産性という物差しを外した時、初めて見えてくるものがあるのです。だから頑張らないことは自分を守ることであり、長く続けるための戦略なのです。全力で走り続けることはできません。どこかで必ず止まります。でも、適切なペースで歩き続けることはできます。そして、歩き続けた人の方が、結果的には遠くまで行けるのです。「頑張る自分」を降りて「続けられる自分」になり、そして「結果を出す自分」に登る。それが私の選択でした。おわりにこの文章を書き終えて、コーヒーを淹れた。カップを持って窓際に立つと、隣のマンションの明かりがいくつか見える。日曜日の夜だ。明日からまた一週間が始まる。みんな、何をしているんだろう。仕事の準備をしているのか、録画していたドラマを見ているのか、あるいは私と同じように、何となく窓の外を眺めているのか。正直に言うと、この文章を書いたからといって、私が何か変わったわけではない。明日になれば、また同じように仕事に行く。締め切りに追われて、会議に出て、メールを返して、「頑張らなきゃ」と思う瞬間がきっとある。そういう自分を完全になくすことはできない。たぶん、これからもずっと。でも、一つだけ変わったことがある。「頑張っている」と言われたとき、その言葉をそのまま受け取らなくなった。「で、それで何か変わったの？」と自分に聞くようになった。頑張っていることを、言い訳にしなくなった。それだけのことだ。たったそれだけのことなのに、少しだけ楽になった。頑張っていない自分を許せるようになった、というのとは違う。頑張ることの価値を、正しく測れるようになった、という感じだ。この文章を読んで、何か得るものがあったかどうかは分からない。「そんなの当たり前じゃん」と思った人もいるだろうし、「何を言っているのか分からない」と思った人もいるだろう。それでいい。ただ、もし今、頑張っているのに上手くいかなくて苦しい人がいたら。もし今、頑張れない自分を責めている人がいたら。一つだけ伝えたいことがある。頑張っていることは、偉いことじゃない。偉いのは、頑張った結果、何かが変わることだ。何かを生み出すことだ。誰かの役に立つことだ。頑張ること自体には、実は何の価値もない。でも逆に言えば、頑張らなくても、結果を出せばいいということでもある。頑張らなくても、変われればいいということでもある。頑張らなくても、前に進めればいいということでもある。だから、頑張らなくていい。本当に、頑張らなくていい。その代わり、歩くのはやめないでほしい。自分のペースで、自分の方向に、自分の足で。転んでもいい。休んでもいい。立ち止まってもいい。でも、歩くのだけは、やめないでほしい。コーヒーが冷めてきた。明日も、たぶん、いつも通りの一日が来る。でも、いつも通りの一日の中で、少しだけ違う選択ができるかもしれない。「頑張らなきゃ」と思ったとき、「いや、待て」と立ち止まれるかもしれない。それだけで、十分だと思う。おい、がんばるな。syu-m-5151.hatenablog.com参考書籍あっという間に人は死ぬから　「時間を食べつくすモンスター」の正体と倒し方作者:佐藤 舞（サトマイ）KADOKAWAAmazon不完全主義　限りある人生を上手に過ごす方法作者:オリバー・バークマンかんき出版Amazonエッセンシャル思考 最少の時間で成果を最大にする作者:グレッグ・マキューンかんき出版Amazonエフォートレス思考 努力を最小化して成果を最大化する作者:グレッグ・マキューンかんき出版Amazonさあ、才能(じぶん)に目覚めよう　最新版 ストレングス・ファインダー2.0作者:ジム・クリフトン,ギャラップ日経BPAmazon嫌われる勇気作者:岸見 一郎,古賀 史健ダイヤモンド社Amazon幸せになる勇気作者:岸見 一郎,古賀 史健ダイヤモンド社AmazonDIE WITH ZERO　人生が豊かになりすぎる究極のルール作者:ビル・パーキンスダイヤモンド社Amazon部下をもったらいちばん最初に読む本作者:橋本拓也アチーブメント出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AIエージェントによるブログレビュー環境の構築（上）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/02/002601</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/02/002601</guid>
            <pubDate>Mon, 01 Dec 2025 15:26:01 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、3-shake Advent Calendar 2024 2日目のエントリ記事です。はじめにブログを書いては直し、また直す。同じ文章を何度も触っていると、客観的な判断ができなくなってくる。「これで本当に伝わるのか？」という疑問だけが残る。コードにはレビューがあり、デザインには批評がある。しかし、技術ブログには明確な基準がない。その不安を解消するために、最初は自分の文章を評価する「プロンプト」を作って運用していた。防御力、思考整理力、実践応用性など、6つの観点でAIに評価させるのだ。だが、すぐに問題にぶつかった。「面倒」なのだ。記事を書くたびにプロンプトを開き、貼り付け、結果を待つ。この手動のひと手間があるだけで、次第に「今日はまあいいか」とサボるようになり、せっかくの基準も形骸化していった。だから、環境ごと変えることにした。生成AIのエージェント機能を使い、ブログレビューの手順をひとつの動作にまとめたのだ。/blog-quality-review と打てば、必要なチェックが勝手に走る。手間を消し、継続性だけを残す。今回は、そんなブログレビュー環境の構築について紹介する。syu-m-5151.hatenablog.comブログ記事評価プロンプト v2.1 https://syu-m-5151.hatenablog.com/entry/2025/05/19/100659 · GitHubこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。なぜブログレビューにエージェントを使うのか自分で書いた記事を自分で評価するのは、想像以上に難しい。「こんなにわかりやすく書いたのに、なぜ伝わらないんだろう」と思うことはないだろうか。それは私たちが、自分の持つ知識や前提条件を、無意識に読者にも期待してしまうからだ。「これくらい知っているだろう」「説明不要だろう」という思い込みが、読者との間に溝を作る。ここにエージェントが入ると、話が変わる。エージェントは私の「暗黙の前提」を共有していない。だから、初学者が感じるであろう「分からない」を冷静に指摘できる。専門用語の壁、論理の飛躍、「なぜ？」という素朴な疑問——これらを容赦なく洗い出してくれる。さらに、エージェントは疲れないし、基準を忘れない。私が定義した「レビューの観点」を一貫して適用し続ける。これは単なる自動化ではない。私の認知リソースを、「本当に人間にしかできない判断」に集中させるための仕組みだ。Commandsでレビュー観点を構造化するClaude Codeには、よく使うプロンプトをコマンド化できる機能がある。.claude/commands/ ディレクトリにMarkdownファイルを置くだけで、ファイル名がコマンド名になり、中身がプロンプトとして機能する。code.claude.com「毎回『この観点でレビューして』と指示するのは面倒」「記事ごとにレビューの質がバラつくのが嫌だ」そんな悩みを抱えていた私にとって、Commandsは最適解だった。一貫性の担保手打ちのプロンプトでは、表現の揺らぎによりAIの回答も変わってしまう。Commandsなら常に同一の定義で実行されるため、出力の質が安定する。# 悪い例（毎回微妙に違う）「この記事をレビューして」「読みやすさをチェック」「AIっぽくないか見て」# 良い例（カスタムコマンド）/blog-quality-review blog.md# → 常に定義された6つの観点・同じ基準でレビューが走るGitでのVersion管理Commandsの実体はMarkdownファイルだ。つまり、プロンプトの改善履歴をGitで管理できる。「この観点を追加したら、指摘が鋭くなった」「この表現を変えたら、より具体的な改善案が出るようになった」こういった試行錯誤の軌跡が残ることで、プロンプト自体が「育つ資産」になっていく。私が実際に使っているCommandsここからは、私がブログ執筆・レビューで実際に使用しているCommandsを全てではないが紹介する。注意：ここで紹介するのは各Commandの要点のみだ。実際のファイルには、より詳細な指示や評価基準（Few-Shotなど）が含まれている。Phase 1: 書く前に深く考える良いブログは「書く」前に「考える」ことから始まる。/deep-thinking-prompt - 深い思考のための問いかけ# Deep Thinking Prompt - 深く考えるための問いかけブログを書く前に「深く考える」ための問いかけを提供します。表面的な理解や一般論で終わらず、本質に迫るための思考支援ツールです。## 7つの問いかけ1. **原体験への問いかけ** - なぜこのテーマに興味を持ったのか2. **前提への問いかけ** - 当たり前だと思っていることは何か3. **対立への問いかけ** - 矛盾や葛藤はどこにあるか4. **構造への問いかけ** - システムとしてどう機能しているか5. **変化への問いかけ** - 過去と現在で何が変わったか6. **未来への問いかけ** - このまま進むとどうなるか7. **読者への問いかけ** - 誰に届けたいのか、なぜその人なのかこのCommandを使うと、「何を書くか（What）」だけでなく「なぜ書くのか（Why）」が明確になる。一般論ではなく、自分だけの視点を掘り起こすための工程だ。/structural-thinking - 構造設計# Structural Thinking - 構造的思考支援散らばった思考を整理し、論理的な流れを作ります。読者の理解プロセスに合わせた「伝わる」構成を設計します。深く考えたあと、その思考をどう配置するか。このCommandが、散乱したアイデアを読者に届く「ストーリー」へと整えてくれる。Phase 2: 書いた後にレビューする/blog-quality-review - 6つの観点でレビュー以前作成した「ブログ記事評価プロンプト」をCommand化したものだ。# Blog Quality Review - ブログ品質レビュー以下の6つの観点（各0.0-5.0スコア）で評価します：1. **防御力** - 批判や反論への耐性2. **思考整理力** - 情報の論理的構造化3. **実践応用性** - 読者が行動に移せる価値4. **構成と読みやすさ** - 視覚的要素と文体5. **コミュニケーション力** - 人間味のある伝達6. **人間らしさ** - 温度感と個性実行すると記事の強みと弱みが数値化される。「前回は実践応用性が3.2だったが、今回は4.0に上がった」といった具合に、自身の成長や記事の品質を定量的に把握できる。/beginner-feedback - 初学者の視点# Beginner Feedback - 初学者の素朴な意見あなたは**一般読者代表（佐々木ゆい・28歳）**として、素朴な意見を提供します。- 専門用語や前提知識の壁を発見- 論理の飛躍を指摘- 「なぜ？」という素朴な疑問を投げかける- 一般読者が共感できるか確認エキスパートの目では見逃してしまう、初学者の「分からない」を発見するためのCommandだ。具体的なペルソナを設定することで、フィードバックの解像度を高めている。/ai-humanity-check - AIっぽさの評価# ai-humanity-check文章のAIっぽさを評価し、より人間らしい表現への改善提案を行います。## AIっぽさスコア (0.0-5.0) ※低いほど人間らしい**0.0-1.0 (完全に人間的)**- 著者特有の言い回しや癖がある- 具体的な失敗談や苦労話が生々しい- 感情の起伏が自然で共感できるAIに下書きを支援させると、どうしても文章が「AI臭く」なりがちだ。このCommandで機械的な表現を検出し、体温のある文章へと戻していく。Phase 3: 仕上げる/textlint-polish - 文章校正# Textlint Polish - 文章校正・AIっぽさ除去機械的・AIっぽい表現を排除し、自然で読みやすい文章にする。- AIが多用する冗長表現を検出- 比喩的・詩的すぎる表現を簡潔に- 文体の統一（です・ます調）textlint的な観点で、表現の誤りや揺らぎを修正する。AI特有の冗長な言い回しもここでカットする。/redundancy-check - 冗長性チェック# Redundancy Check - 冗長性チェック以下の4つの観点（各0.0-5.0スコア）で評価します：1. **情報密度** - 1文あたりの情報量2. **簡潔性** - 冗長表現・無駄な修飾の少なさ3. **論理効率** - 論理的重複・循環論法の少なさ4. **構造最適性** - 章・節の構成の必要十分性削れる言葉は徹底的に削る。情報の密度を高め、読み手の時間を奪わない文章にするための最終チェックだ。全自動レビューの実行これらを一つずつ実行するのはやはり手間だ。そこで、これらを束ねる /full-review を作成した。# Full Review - 全自動レビュー実行すべての必須レビューを自動で順次実行します。textlint校正から始まり、初学者フィードバック、品質レビューまで一括で実施。## 使用方法/full-review blog.mdこのCommandひとつで、以下のフローが流れる。/textlint-polish（校正）/beginner-feedback（初学者視点）/blog-quality-review（品質スコア）/ai-humanity-check（人間らしさ）一度設定さえしてしまえば、あとは「コマンド一発」で包括的なレビューが完了する。上巻のまとめここまで、Commandsを使ったブログレビュー環境の基礎（Phase 1〜3）を解説してきた。出発点は、「ブログ記事の評価基準がなく、レビューが属人的かつ面倒」という課題だった。これに対し、エージェントを活用して評価観点を構造化し、実行を自動化するというアプローチをとった。ここで重要なのは、AIとの関係性だ。体験や感情といった「身体性」は人間が供給し、それを構造化し整える役割をAIが担う。これはAIへの丸投げではなく、互いの強みを活かした協働である。Commandsによって評価基準を定義し、Gitで管理し、自動化することで、「書くこと」以外のノイズを極限まで減らすことができる。下巻では、より高度なAgents（サブエージェント）の活用と、複数の視点を持つレビュー体制の構築について解説する。下巻に続くsyu-m-5151.hatenablog.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[1Password の SSH Agent を WSL でも使う]]></title>
            <link>https://qiita.com/yteraoka/items/a056f7c055cc73b06d19</link>
            <guid isPermaLink="false">https://qiita.com/yteraoka/items/a056f7c055cc73b06d19</guid>
            <pubDate>Mon, 01 Dec 2025 15:07:22 GMT</pubDate>
            <content:encoded><![CDATA[パスワード系は 1Password に登録しているのですが SSH の鍵はなんとなく面倒でファイルでローカルに置いたままでした。しかし、バックアップを取るのも面倒だし 1Password で管理しようかなという気になりました。せっかくお金も払っているのだし使えるものは使おう...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年12月版読む予定本紹介]]></title>
            <link>https://zenn.dev/akasan/articles/books_dec_plan</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/books_dec_plan</guid>
            <pubDate>Mon, 01 Dec 2025 12:59:58 GMT</pubDate>
            <content:encoded><![CDATA[今年も早いものであっという間に12月ですね。ということで、年末最後の一ヶ月で読もうと思っている本を紹介します。先月分は以下になりますので、併せてご覧ください！https://zenn.dev/akasan/articles/870a86bf7189f1 機械学習 実践GAN（Compass Booksシリーズ） 敵対的生成ネットワークによる深層学習マルチモーダル系の資格試験を受験予定で、その勉強中に改めげGANの基礎を叩き込もうと思って読んでいる最中です。GANも様々な種類がありますし、どのようにモデルを学習するかなどを改めて学習するために読んでいます。私は普段PyTorch...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[初めての海外カンファレンス(KubeCon NA 2024 in Salt Lake City)]]></title>
            <link>https://blog.masasuzu.net/entry/2025/12/01/212119</link>
            <guid isPermaLink="false">https://blog.masasuzu.net/entry/2025/12/01/212119</guid>
            <pubDate>Mon, 01 Dec 2025 12:21:19 GMT</pubDate>
            <content:encoded><![CDATA[この記事は3-shake Advent Calendar 2025です。qiita.com吉祥寺.pm #37で話した内容となります。kichijojipm.connpass.com speakerdeck.com厳密には10年以上前に行ったことあるんですが、完全に忘れているので実質今回が初回ということでお願いします。今回は業務として、KubeCon NAへ行かせてもらったのでその体験を共有いたします。セッション内容については触れません。旅程的には11/11-17となっており、KubeCon NAの開催期間としては11/12-15となっています。以下の目次で送らせていただきます。出国準備5ヶ月前1か月前前日まで随時往路現地復路事後まとめ出国準備やったことは以下のとおりです。5か月前KubeConチケット手配ホテル予約飛行機手配1ヶ月前パスポート取得ESTA申請Visit Japan登録前日まで荷物準備随時英語技術インプット5ヶ月前KubeCon自体のチケット手配はまとめて会社の方でやっていただきました。ホテルと飛行機はKubeCon割引があったのでこれを利用しました。飛行機はUnited航空の乗継便を予約しました。行きは成田=>ダラス=>ソルトレイクシティー、帰りはソルトレイクシティー=>ロサンゼルス=>成田を予約していました。飛行機が往復で26万円、ホテルが12万円でした。なかなかな値段ですね。同僚と1日違いでチケットを買ったら微妙に値段が変わっていた記憶があります。早めの行動大事ですね。ホテルも飛行機も日本語サイトがあったので特に困った記憶がないです。1か月前数年前にパスポートの期限が切れていたので、これを機に再発行しました。東京都庁地下にパスポートセンターがあるので、ここで申請しました。パスポートセンター横に写真屋さんがあるので、証明写真を準備せずに行っても安心です。だいたい1週間で発行されるので、また1週間後に赴くことになります。渡米する際にESTAを申請する必要があります。ここで注意してほしいのは、検索トップやスポンサーサイトとして出てくるサイトはそれっぽい偽物なので騙されないように注意してください。esta.cbp.dhs.govVisit Japanを事前に登録していくことで日本への帰国時にスムーズになりますのでやっておくことをおすすめします。services.digital.go.jp前日までここまででだいたいやらないといけないことは終わってるので、あとは荷造りです。大きな荷物としては手荷物で入る大きさのトランクケースとビジネスリュックサックの2つを持っていきました。それにプラスしてパスポートと財布と携帯を常に携帯するためのサコッシュも持ち込みました。4泊5日暮らせる最低限の服だけ持ち込みました。だいぶコンパクトになったと思います。基本的に外で食べるつもりはあまりなかったので、全日程の夕食を持ち込みました。オートミール、フリーズドライの味噌汁、粉末スープ類、ルイボスティーなど持っていきました。スープ類にオートミールを入れてレンジで温めればなんとかなります。ここで注意しないといけないのはアメリカは動物性成分が含まれているものは持ち込めません。魚介はOKなのでそのあたり注意しましょう。このへんは国によって違います。電子機器に関しては120V対応しているものはそのままアメリカでも使えます。コンセントの形状は日本と同じですがボルト数が違う形になります。先に言ったようにパスポート、財布、携帯はサコッシュに入れて肌身離さないようにしていました。随時英語はほんとにもっとやっておけばよかったなと思いました。Duolingoはずっとやっていましたが、リスニング、スピーキングという観点からは足りないですね。最近だとスピークバディみたいなAI英会話アプリがあるのでそのあたりもっとやり込んでおけばよかったなと後悔してます。あとやったこととしては、CNCFのyoutubeチャネルに大量の過去のKubeConアーカイブがあるのでそれでひたすら耳をならしてました。www.youtube.com往路実を言うとですね。出国する当日朝まで沖縄にいました。出国当日はこんな感じでした。11/7 午前 羽田着11/7 昼 家で荷物最終チェック11/7 夕方成田出発沖縄から帰る飛行機がちゃんと飛んで良かったと心から思います。普通はこんなことしないです。いろいろ重なって仕方なかったのでした。ということで、行きは成田発、デンバー乗り継ぎ、ソルトレイクシティーという行程です。。。。。でした。デンバー行きの飛行機に乗っていて途中で行き先がアンカレッジ(アラスカ州)に変わってることに気が付きます。急病人救護のためにアンカレッジに緊急着陸することになりました。なんやかんやあって無事デンバーには到着するのですが、当然乗継便には間に合わずなので振り返る必要がありました。自分はアンカレッジ出発時点までにかすかな電波を頼りにスマートフォンから振替を行ったのですが、同僚たちは電波がなく何もできなかったので、自動的に翌日の便に振り返られてしまいました。ちょっとそれは困るのですが、この時点ではどうにもならないのでいったんデンバーに到着し入国審査を受けることになりました。正直入国審査はかなり厳しくされるのかなと不安になっていたのですが、案外すんなり通って拍子抜けしました。デンバーの空港にてなんとか本日便で乗り継げないか交渉することになりました。どうやら現状満席の便でもウェイティングリストに入ることでキャンセル待ちに並ぶことができてうまくいけば本日便で行けそうだということがわかりました。Uniteの係員の人がウェイティングリスト登録のためにコマンドプロンプトを駆使してたのが印象的でした。コマンド操作でやるんですねと感心しました。ともあれ、初っ端からトラブルに見舞われましたが拙い英語でもなんとか乗り切ることができました。なんやかんやあって、ソルトレイクシティーまでたどり着くことができました。現地空港からホテルまではLRTで移動しました。TransitというアプリでOne-Wayチケットを購入して乗るみたいでした。チケットをActivateして乗るのですが、QRコード自体はあるのですが、最後まで誰にも見せることなく下車しました。これが信用乗車方式か。。。となりました。会場はめっちゃ広いし、めっちゃ人が多くて、これが本場か、、、と圧倒されました。飲み物はいたるところにありました。コーヒーに困ることはありませんでした。ランチボックスが無料配布されてることに4日目に気が付きました。それまで、毎回ホテルに帰ってオートミールを食べる生活をしていました。毎回ランチに必ずお菓子が含まれているのはアメリカンな文化なのでしょうか。ランチはビーフ、チキン、ベジタリアン、ビーガン、グルテンフリーから選べました。なんというか文化を感じますね。英語に自身なかったので、文字起こしと翻訳をしてくれるSaaSアプリを試してたのですが、いまいち制度が低くてあまり役に立たなかったでした。これもあとから気づいたのですが、ルームごとにQRが貼ってあって、そこにアクセスすると文字起こしと翻訳をしてくれるアプリケーションが用意されていました。今回いたるところで自分の情報弱者ぶりを感じてしまいました。KubeConのセッション自体はYoutubeにすぐ上がるので、その場で頑張りすぎずにあとで復習するのがよいです。ただ、現地でしか体験できない雰囲気を味わえたことはすごく良い経験になりました。現地での日本人交流会ではバリバリKubernetesをつかっている人たちの生の声が聞けて刺激になりました。復路特筆すべきことはないですVisit Japanをあらかじめ登録しておいたので、入国審査と税関はすんなりと通過できました。事後経費採算しっかししましょう。経費採算終わるまでが出張ですまとめ往路がこすぎてソルトレイクシティーについた瞬間もう帰っていいかなという気持ちになりました。トラブルはありつつも一つ一つこなしていけばまあなんとかなるものだなと思いました。英語はほんとにちゃんとやっておきたかったなと言う気持ちが強いです。もっとできてれはもっと実りが多かったなと。当たり前の話ですが、羽田、成田の乗り継ぎはやめたほうが望ましいです。社会人は余裕持った行動をしましょう(?)いろいろもっとうまくやりたかったという気持ちはありつつも本場のでっかいカンファレンスに参加して色んな意味で刺激を受けました。また英語もそうですし、知識レベルを上げて海外カンファレンス再挑戦したいです。来年はre:Invent行たいです!さて、真面目なまとめは同僚が書いてるのでこちらを参照してください。sreake.comそれでは。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GitHub Actionsの「なぜか動く」を分解する：npm publishとGITHUB_TOKEN]]></title>
            <link>https://zenn.dev/meziron/articles/fef6ccca887f97</link>
            <guid isPermaLink="false">https://zenn.dev/meziron/articles/fef6ccca887f97</guid>
            <pubDate>Mon, 01 Dec 2025 08:25:59 GMT</pubDate>
            <content:encoded><![CDATA[GitHub Actionsの「なぜか動く」を分解する：npm publishとGITHUB_TOKEN最近、非エンジニアの人やエンジニアなりたての人と作業することがあります。そこで自分が組んだGithub Actionsについて質問をもらいました。「このyamlってプログラミング言語じゃないのになんでこういうふうに書くことで動くの？」「初めてみても何が何をしているの全然直観的じゃなくて、これから自分で書ける自信がない・・・」上記のような言葉をもらいました。実際、GitHub Actionsを使っていると、コピペでなんとなく動いてはいるものの、「裏で何が起きているのかよく...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Instructorの紹介]]></title>
            <link>https://zenn.dev/meziron/articles/2d1a1006851423</link>
            <guid isPermaLink="false">https://zenn.dev/meziron/articles/2d1a1006851423</guid>
            <pubDate>Mon, 01 Dec 2025 08:25:58 GMT</pubDate>
            <content:encoded><![CDATA[InstructorライブラリとClean Architectureで実現する型安全なAI統合パターン はじめに近年、業務アプリケーションにAI機能を組み込む事例が急速に増えています。しかし、AIの出力は本質的に不確実性を含むため、従来のWebアプリケーション開発で重視されてきた型安全性や保守性を維持することが課題となっています。本記事では、InstructorライブラリとClean Architectureを組み合わせることで、型安全で保守性の高いAI統合パターンを実現する方法を紹介します。特に、複雑な業務ロジックを持つアプリケーションでAIを活用する際のベストプラクティス...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[学術的根拠から読み解くNotebookLMの音声活用法]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2025/12/01/005741</link>
            <guid isPermaLink="false">https://shu-kob.hateblo.jp/entry/2025/12/01/005741</guid>
            <pubDate>Sun, 30 Nov 2025 15:57:41 GMT</pubDate>
            <content:encoded><![CDATA[この記事はQiita 3-shake Advent Calendar 2025 シリーズ1日目の記事です。2025年11月22日(土)に、Google Developer Group - DevFest Tokyo 2025があり、その招待制懇親会でLTをさせていただく機会がありました。「学術的根拠から読み解くNotebookLMの音声活用法」というタイトルで、NotebookLMの音声解説で学習する際のポイントを過去のマルチメディア学習の学術的根拠や実験を基にまとめました。 speakerdeck.com1ページずつ解説をさせていただきたいと思います。1枚目1枚目は表紙です2枚目2枚目は自己紹介です。3枚目Notebookでの音声を作る操作方法です。画面にドキュメントなどをアップロードし、音声解説ボタンを押すだけで簡単に作れます。音声は、男女掛け合いのPodcast形式です。4枚目仕事や学業で、難解なドキュメントを読む場面は多々あると思いますが、NotebookLMの音声解説機能により、学習効率が高められるか期待が高まっています。AIによって作られた音声がどれだけ学習効果があるか過去のマルチメディア学習の学術的根拠実験を基に解説していきます。5枚目学習効果を測定する実験も行いました。とある専門的なドキュメントを音声化して実験に用いました。被験者は熟達者（エキスパート）と初学者のグループに分かれます。熟達者、初学者でそれぞれ、音声の元となったドキュメントのみ読んで学習したグループ、音声のみ聴いて学習したグループ、両方を用いたグループに分かれ、学習後に4択の理解度チェックテストを受けてもらいました。GoogleスライドをPDF化して文字が崩れているので、直せるなら直しておきます。6枚目ドキュメント・音声の両方を用いたグループが優位に思えましたが、結果はご覧の通り。初学者は、実験を1回のみ行い、両方 > 音声のみ > ドキュメントのみ、という期待通りの結果でしたが、熟達者は、実験を3回行い、両方グループが最高点を取るとは限りませんでした。なぜ、熟達者は両方グループが優位とは限らなかったのでしょうか？7枚目初学者の説明です。初学者は音声学習を順書立てて勉強するのが有効です。構造的ガイダンスを提供することを足場かけ理論といいます。また、初学者の両方グループは音声を主、ドキュメントを従（文章を読むより、俯瞰的に見る）ことにより、認知負荷分散につながりました。8枚目一方の熟達者の説明です。熟達者は、初学者に有効な順序立てた構造的ガイダンスが邪魔になることがあります。熟達者の知識ネットワークに対して、手厚い構造的ガイダンスが知識をマッピングするのが非常に認知負荷が高いためです。これを熟達化のリバーサル効果といいます。また、熟達者の両方群は音声とドキュメント両方から情報を得ようと頑張り、認知負荷が高い状態でした。9枚目実験の制約により、不利な面もありました。実験の時間の都合上、音声の一時停止、巻き戻しを禁止していました。音声を一時停止、巻き戻して、自分のペースで聴けるなら、熟達者の両方グループは、音声とドキュメント両方からしっかり情報を取れていた可能性があります。学習者のペースを守らせることが効率を上げるのですが、例えば、音声や動画の学習をする際、数分ごとに区切って、学習者が「次へ」を押すことで次のパートが始まると学習がしやすいです。このことを「セグメンテーション原理」と呼ぶのですが、実験の制約上、阻害されたことになります。また、熟達者の実験の中で、音声のみグループの平均点が低いときがありました。それは、グラフ・図を見ていないと難しい問題が多く、音声でグラフ・図など視覚的な情報伝達が難しいことを意味します。また、各グループの点数のばらつきでは、ドキュメントのみグループが最も大きかったです。これは当然と言えば当然で、ドキュメント学習は各個人の学習能力に大きく左右されるためです。一方、音声は画一的な指導が可能とも言えます。10枚目実験や学術的根拠から読み解く、音声学習のおすすめとしては、学習者の習熟度を考慮し、初学者は音声とドキュメント両方を併用し、音声を主、ドキュメントを従とするのが良いでしょう。一方、熟達者は各個人で使い分けをするのがよく、概要把握や復習、思い出すなどの目的では音声、詳細や図表の把握はドキュメントを使うのがよく、安易に両方同時に使うと認知負荷を増大させるリスクがあります。11枚目参考文献です。一部、有料のものもありますが、Web検索等で概要を知ることもできます。12枚目終わりましたが、他の勉強会での登壇情報です。2025年11月27日(木)に、Jagu'e'r 月末 Tech Lunchの勉強会「月末 Tech Lunch Online#7 - Google Cloud を語る！-」に「MCP・A2A概要 〜Google Cloudで構築するなら〜」というタイトルで登壇した話は、ブログ記事にまとめていますので、よろしければご覧ください。shu-kob.hateblo.jp最後にqiita.com3-shake Advent Calendar 2025 シリーズ2の1日目はmasasuzuさんが書いてくれています。シリーズ1の2日目はyteraokaさんの「VPC Lattice を理解したい」シリーズ2の2日目はnwiizoさんの「生成AIエージェントによるブログレビュー環境の構築（上）」です。今後もお楽しみに！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MCP・A2A概要 〜Google Cloudで構築するなら〜]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2025/12/01/001432</link>
            <guid isPermaLink="false">https://shu-kob.hateblo.jp/entry/2025/12/01/001432</guid>
            <pubDate>Sun, 30 Nov 2025 15:14:32 GMT</pubDate>
            <content:encoded><![CDATA[この記事はQiita Jagu'e'r Advent Calendar 2025の1日目の記事です。2025年11月27日(木)に、Jagu'e'r 月末 Tech Lunchの勉強会「月末 Tech Lunch Online#7 - Google Cloud を語る！-」に「MCP・A2A概要 〜Google Cloudで構築するなら〜」というタイトルで登壇させていただきましたので、その発表内容でのポイントを記事化したいと思います。AIエージェントが流行っているけど、MCPやA2Aという概念は難しいやろうと思い、噛み砕いて説明したいというのが発表のモチベーションでした。 speakerdeck.comなお、今回の資料は、NotebookLMで作成しました。ここまで作れるのはすごいです！1ページずつ解説をさせていただきたいと思います。1枚目1枚目は表紙です。2枚目2枚目は、アジェンダで、全体の話の流れを書いています。3枚目3枚目は、LLMの制約について述べています。LLMは「Brain in a Jar」（瓶の中の脳）とも言われ、賢いけど、手足を持たなくて実行能力のないものの例えです。例えば、学習時点までの知識しか知りません。これをナレッジカットオフといいます。「今日の株価」「明日の天気」「最新のニュース」などは分かりません。また、旅行のプランをLLMに尋ねても、航空券やホテルの予約はしてくれません。APIなどを操作し、データベースのトランザクション操作をする実行能力はないのです。4枚目ここで、LLMの制約を解決する手段として、MCPの話が出てくるのに加え、さらなる機能拡張のためにA2Aの話が出てきます。MCPはLLMに実行能力を与えます。A2Aはエージェント同士が連携し、より複雑なことができるようになる仕組みです。5枚目MCP(Model Context Protocol)は外部ツールやデータへのアクセスを標準化するプロトコルです。LLMという脳に手足を与えて、検索やAPI操作ができるようになり、APIを介してデータベースのトランザクション操作ができるようになるのです。ここでポイントは、推論機能と実行機能を分離して疎結合に実装するということです。6枚目A2A(Agent-to-Agent)は、AIエージェント同士で、連携するためのプロトコルです。能力を記述したAgent CardがAIエージェントの名刺となり、どのエージェントにどのタスクを任せるかの判断ができます。また、通信プロトコルが定められているため、拡張性に優れています。7枚目MCPとA2Aのご紹介をしましたが、Google CloudでMCPやA2Aをどう構築していくかのポイントに移りましょう。まず、認知（推論）機能と実行機能を分離することクラウドを利用する上で、サーバーレスファーストが大事であること（8枚目で詳説）誰も信頼せずとも動くゼロトラストセキュリティであることです。8枚目Google CloudでのMCPサーバー構築は、Cloud Runを使うのが定石です。サーバーレスでありコスト最適化できます。また、高いスケーラビリティに対応していて、コンテナベースで、デプロイが容易です。9枚目MCPサーバーをCloud Runで構築する際の注意点です。ローカル開発で使うようなstdio（Standard I/O）はCloud Runでは使用できないため、Streamable HTTPかSSE over HTTPを使う必要があります。最近では、新しいStreamable HTTPの方が推奨となっています。10枚目一方、A2A対応のエージェントの構築は、Vertex AI Agent Engineが最適です、フルマネージドサービスで、A2Aのプロトコルに準拠しており、Agent Registoryによるガバナンスも効いています。11枚目A2Aエージェントを構築するためのポイントです。スライドには文言が書いていませんが、ADK(Agent Development Kit)を用いた方法です。AgentCardの定義、使用するLLMやツールの定義、タスク処理のロジックを実装し、これらをA2Agentで統合し、A2A準拠のエージェントを作成できます。12枚目MCPとA2Aを連携させた構築例です。「Social Agent」というのは友人の好みを推論するエージェントです。外部連携、つまり実行部分はMCPを用いて、推論と実行の分離を行います。13枚目簡単にAIエージェントが開発できるようになると、企業内でみんな好き勝手にエージェントを作り始めて、野良エージェントが増えそうですが、Gemini Enterpriseによる一元管理でガバナンスを効かせられます。14枚目MCPもA2Aもオープンプロトコルであるため、拡張性に優れています。インターネットでもTCP/IPというオープンプロトコルのおかげで相互運用性があるように、AIエージェントもどんどん拡張していき、どんどん便利な世の中になるのかもしれません。最後にお読みいただきありがとうございました。2日目のQiita Jagu'e'r Advent Calendar 2025は、pHaya72さん「テクサミの宣伝」です。qiita.com空きもまだありますので、Jagu'e'r 会員の方はぜひ書きましょう！私もできれば、複数記事書きます！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【小ネタ】pytest-bddを使ってみた]]></title>
            <link>https://dev.mix64.com/2025/11/30/pytest-bdd/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/11/30/pytest-bdd/</guid>
            <pubDate>Sun, 30 Nov 2025 11:15:35 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回は pytest-bdd ついて紹介します。名前の通り BDD（Behavior Driven Development, テス...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私が持っている書籍一覧を公開します！！]]></title>
            <link>https://zenn.dev/akasan/articles/my_book_lists</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/my_book_lists</guid>
            <pubDate>Sun, 30 Nov 2025 05:18:11 GMT</pubDate>
            <content:encoded><![CDATA[今回は、私が持っている書籍をブクログに登録して公開しました！!直近かなりKindle上で読んでいるのですがまだインポートできておらず、以下で共有しているのは紙媒体の書籍に限りますKindle上の書籍についても近々アップロードしようと思います ブクログとは？ブクログとは本の感想や評価をチェックしたり、webやアプリで本棚を作成して感想やレビューを書いたりすることができるサービスになります。私自身自分が持っている書籍を電子データとして管理したいと思った時に見つけたサービスですが、他の方と共有できる機能は便利だと思って使いはじめました。https://booklog.jp/...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【解説】コード生成の最適化によるLinuxコンテキストスイッチの改善パッチ]]></title>
            <link>https://dev.mix64.com/2025/11/29/optimize-code-generation-during-context-switching/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/11/29/optimize-code-generation-during-context-switching/</guid>
            <pubDate>Sat, 29 Nov 2025 14:45:43 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回は2025年11月にXie Yuanbin氏によって提案された一連のパッチシリーズ「Optimize code generat...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Professional Accelerated Data Science編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_ncp_ads</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_ncp_ads</guid>
            <pubDate>Sat, 29 Nov 2025 09:22:18 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるProfessional Accelerated Data Scienceを取得したので、その内容を共有しようと思います。 Professional Accelerated Data Scienceとは？NCP Accelerated Data Science（以下、NCP-ADS）は、データサイエンスワークフローにおけるGPUアクセラレーションツールとライブラリの活用能力を認定する中級レベルの資格です。現時点では本試験の入門レベルであるAssociateレベルは提供されておらず、Professionalのみとなっています。https://www...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Professional Generative AI LLMs編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_ncp_genl</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_ncp_genl</guid>
            <pubDate>Fri, 28 Nov 2025 13:09:29 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるProfessional Generative AI LLMsを取得したので、その内容を共有しようと思います。 Professional Generative AI LLMsとは？Professional Generative AI LLMs（以下、NCP-GENL）は、NVIDIAが提供している認定資格の一つであり、AIドリブンなアプリケーションを開発したり運用するための中間レベルの資格となっています。主にLLMについて取り扱われる部分が多いですが、従来の機械学習に関する知識についても問われるようになっています。先日受験したAssociate G...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[名前が似てる LookerとLooker Studioの違いとは?]]></title>
            <link>https://sreake.com/blog/differences-between-looker-and-looker-studio/</link>
            <guid isPermaLink="false">https://sreake.com/blog/differences-between-looker-and-looker-studio/</guid>
            <pubDate>Fri, 28 Nov 2025 02:32:06 GMT</pubDate>
            <content:encoded><![CDATA[はじめに こんにちは。 以前、Looker、LookMLについての記事を投稿してからしばらく経ちました。 今は生成AIの登場により、Lookerも様々な機能追加や活用の場が増えてきています。 それと同時に、このような言葉 […]The post 名前が似てる LookerとLooker Studioの違いとは? first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：「AI」という言葉の認識とは？]]></title>
            <link>https://zenn.dev/akasan/articles/poem_what_is_ai</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/poem_what_is_ai</guid>
            <pubDate>Thu, 27 Nov 2025 14:21:10 GMT</pubDate>
            <content:encoded><![CDATA[短編集、今回は昨今AIという言葉のニュアンスが変わってきているように感じており、ちょっと書いてみました。※ 中身が合ってないような文章です。出張帰りのわずかな時間での記載になりますのでご容赦ください そもそもAIとは？AI、つまり人工知能の定義をまず確認します。まず確認します。NEC様のページに乗っている画像を引用させていただくと、人工知能は「学習」「認識・理解」「予測・推論」「計画・最適化」など、人間の知的活動をコンピュータによって実現するもの、と定義されています。のように定義されるとのことで、人が考えるロジックをプログラム的に構築する分野と言えるでしょう。この定義から...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MCP・A2A概要 〜Google Cloudで構築するなら〜]]></title>
            <link>https://speakerdeck.com/shukob/mcpa2agai-yao-google-clouddegou-zhu-surunara</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/mcpa2agai-yao-google-clouddegou-zhu-surunara</guid>
            <pubDate>Thu, 27 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[「Jagu'e'r 月末 Tech Lunch Online#7 - Google Cloud を語る！-」にて、AIエージェントのMCPとA2Aの概要と、それらをGoogle Cloudで構築する上でのTipsを紹介させていただきました。https://jaguer-tech-lunch.connpass.com/event/362363/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[n8nでブログネタを考えるためのアシスタントを作ってみた]]></title>
            <link>https://zenn.dev/akasan/articles/blog_helper_n8n</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/blog_helper_n8n</guid>
            <pubDate>Wed, 26 Nov 2025 02:27:25 GMT</pubDate>
            <content:encoded><![CDATA[今回は、n8nで私のブログネタを過去のものから発展させる形で提案させるワークフローを作ってみました。 要件定義今回は以下の機能を実現するエージェントを作りました。ユーザから受け取ったブログに関する相談を行うツールとして、私のテックブログのRSS Feedを読み取る機能を与えるAIモデルはgpt-4.1-miniを利用ブログに関する相談以外はリジェクトさせる!OpenAIのモデルを使うにあたりAPIキーを取得してください n8nでの環境構築今回はローカル環境でdockerを利用して環境構築しました。詳しくは公式GitHubを参考に起動してください。https...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：技術資料を翻訳せずに英語のまま読んでいて思ったこと]]></title>
            <link>https://zenn.dev/akasan/articles/read_in_english</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/read_in_english</guid>
            <pubDate>Tue, 25 Nov 2025 11:40:55 GMT</pubDate>
            <content:encoded><![CDATA[最近技術資料を読むときに、英語の文章の場合基本的に翻訳せずに読むようにしています。あっという間に翻訳した文章を生成できる今だからこそあえて原文を読んでいてい思ったことをちょっとまとめてみました。 大前提：英語を流暢に扱いたい私自身、英語を流暢に話せるようになりたいと思っています。そんな中で日本語に翻訳ばっかりしていては英語能力は伸びません。もちろん知らない単語だったりは調べますが、可能な限り原文を読むことを大事にしています。また、翻訳させることによって原文と微妙にニュアンスが変わる場合もあるため、できるだけ原文で読むようにしています。なお、英語で読むようにしているのは今に始まった...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIエージェントの自律性と協調性を解放する Google CloudによるMCP・A2A実装のエンタープライズ戦略]]></title>
            <link>https://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2</guid>
            <pubDate>Tue, 25 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年11月25日の日本生成AIユーザ会「#19 MCP・A2A概要 〜Google Cloudで構築するなら〜」にて発表に使用した資料2部あるうちの後半部分です。https://genai-users.connpass.com/event/376260/↓前半部分はこちらですhttps://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIエージェント入門 自律性の基礎からオープンプロトコルMCP・A2Aによる連携まで]]></title>
            <link>https://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made</guid>
            <pubDate>Tue, 25 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年11月25日の日本生成AIユーザ会「#19 MCP・A2A概要 〜Google Cloudで構築するなら〜」にて発表に使用した資料2部あるうちの前半部分です。https://genai-users.connpass.com/event/376260/↓後半部分はこちらですhttps://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[「Postgres で試した？」と聞き返せるようになるまでもしくはなぜ私は雰囲気で技術を語るのか？ — Just use Postgres 読書感想文]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220</guid>
            <pubDate>Tue, 25 Nov 2025 04:52:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「Just use Postgres」という言葉を初めて聞いたのは、いつだったか覚えていません。Twitter か Hacker News か、あるいは社内の Slack か。どこで聞いたにせよ、私の反応は決まっていました。「また極端なことを言う人がいる」と。「それ、〇〇でもできますよ」——この手のフレーズはもう100回は聞いてきました。そして大抵の場合、その〇〇は専用ツールに置き換えられていきます。技術が専門分化していくのは自然な流れです。全文検索なら Elasticsearch。時系列データなら InfluxDB。メッセージキューなら RabbitMQ。それぞれの分野に専門家がいて、専用のソリューションがあって、ベストプラクティスがあります。「とりあえず Postgres で」なんて、それは思考停止ではないか、と。でも、心のどこかで気になっていたんです。www.manning.comソフトウェアエンジニアとして 10 年近く働いてきて、システムが複雑化していく様子を何度も見てきました。「全文検索だから Elasticsearch」と導入したら、その運用は誰がやるのか。バックアップは？　モニタリングは？　バージョンアップは？　構成図に新しい箱が増えるたびに、誰かが深夜 3 時のアラート対応をする可能性が増えます。その「誰か」は、たいてい自分です。以前関わったプロジェクトでは、Postgres、Redis、Elasticsearch、RabbitMQ、InfluxDB が同居していました。それぞれに理由があって導入されたはずですが、3 年後には「なぜこれが必要だったのか」を説明できる人が誰もいなくなっていました。ドキュメントはあっても、判断の背景までは残っていません。結局、「触ると怖いから残しておこう」という判断になります。技術的負債の典型です。syu-m-5151.hatenablog.comこの本を手に取ったのは、そういう日常からの逃避だったのかもしれません。「Postgres だけで済むなら、楽になれる」そんな甘い期待を持って読み始めました。そして、最初の数ページで気づきました。この本が言っているのは、私が思っていたことと少し違います。「Postgres は万能だから全部 Postgres でやれ」ではありません。「既に Postgres を使っているなら、新しいデータベースを追加する前に、まず Postgres で試してみよう」ということです。その違いに気づいた瞬間、なんというか、肩の力が抜けました。これは、銀の弾丸を売りつける本ではなかったんです。私たちが日々向き合っている「技術選定」という名の意思決定に、1 つの視点を提供してくれる本でした。10 年近くこの仕事をしてきて、技術選定について 1 つ学んだことがあります。新しい機能や技術が出たとき、いきなり飛びつかない。どれだけ魅力的に見えても、まず「運用時にどうなるか」を考えます。誰がバックアップを取るのか。障害時に誰が対応するのか。3 年後にメンテナンスできる人がいるのか。流行りの技術を追いかけることと、本番環境で安定して動かすことは、別の話です。これは、Postgres の中でも同じです。pgvector や TimescaleDB のような比較的新しい拡張、あるいは Postgres 本体の新機能についても、本番投入前に運用面を検討する必要があります。「Postgres だから安心」ではなく、「その機能が十分に枯れているか」を見極める姿勢が大事です。かといって、新しいことを学ばないわけにもいきません。技術は進歩します。昨日のベストプラクティスが、明日には技術的負債になることもあります。結局のところ、謙虚に学び続けるしかありません。私が最近考えているのは、こういう基準です。替えの利く技術は、流行に従う。フロントエンドのフレームワークとか、CI/CD ツールとか。入れ替えやすいものは、その時点でのベストを選べばいい。替えの利きづらい基盤は、標準に従う。データベースとか、認証基盤とか。長く使うものは、実績のある標準的な選択をする。競争優位の核は、自ら設計する。ビジネスの差別化に直結する部分は、自分たちで考え抜いて設計する。Postgres は、競争優位の核になる場合もありますが、基本的には 2 番目の「替えの利きづらい基盤」であることが多いです。40 年以上の実績があり、コミュニティ主導で開発され、世界中で使われている標準的な選択肢。だからこそ、その可能性を正しく理解しておきたいと思いました。だから、読み進めることにしました。正直に言うと、全部を理解できたわけではありません。「FOR UPDATE SKIP LOCKED」の仕組みを完全に説明しろと言われたら、今でもちょっと怪しいです。でも、それでいいと思うことにしました。完璧に理解することが目的ではありません。「Postgres で試した？」その一言を、自信を持って言えるようになること。それが、この本を読む目的でした。なので、この読書感想文には私の手元で動かした実行結果と書籍の中身がごちゃ混ぜになっています。基本的に明記しているつもりですが抜けていたらごめんなさい。Just Use Postgres!: All the database you need (English Edition)作者:Magda, DenisManningAmazonこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。1. Meeting Postgres「Just use Postgres」の再解釈1.2 節の「Just use Postgres」の説明を読んで、自分の理解が間違っていたことに気づきました。私はこれまで、「Just use Postgres」を技術選定の初手として捉えていました。「新規プロジェクトならとりあえず Postgres 立てとけ」みたいな。でも、著者が書いているのは違います。Does this mean Postgres has become a Swiss Army knife and the only database every developer needs? Certainly not.著者は明確に否定しています。Postgres は万能ツールではない、と。じゃあ「Just use Postgres」は何を意味するのでしょうか。「既に Postgres を使っているチームが、新しいユースケース（地理空間、時系列、生成 AI など）が発生したとき、別のデータベースを追加する前に Postgres で解決できるか確認してみよう」これがこのモットーの正しい解釈だと著者は言います。インフラエンジニアとして 10 年近く運用してきた身としては、この視点の転換にハッとしました。「Elasticsearch で全文検索やりたい」と言われた時、私は内心「またか…（心の中で構成図に新しい箱を追加する手が震える）」と思っていました。でも、「Postgres で試した？」と聞き返すことはしませんでした。自分の仕事を増やしたくないという気持ちが先に立って(自分が起こされるのにね)。これ、逆だったんです。Postgres で解決できるなら、新しいデータベースを追加するより運用負荷は減ります。バックアップ戦略も、モニタリングも、アラートルールも、既存のまま使えます。運用対象が増えるたびに、前世で何をしたのかと深夜に考える機会も減ります。この本を読み終えて、「Postgres で試した？」と自信を持って聞き返せるようになったと思います。良いか悪いかは別として。なぜ Postgres が人気なのか1.1 節では、Postgres が人気な 3 つの理由が挙げられています。オープンソース・コミュニティ主導: 1994 年に MIT ライセンスでオープンソース化。単一ベンダーではなくコミュニティ主導で開発。エンタープライズ対応: 35 年の開発で培われた信頼性と堅牢性。年次メジャー バージョン リリース、段階的 改善 重視。拡張性: Michael Stonebraker が設計当初から拡張性を重視。JSON、時系列、全文検索、ベクトル類似検索など多様なユースケースに対応。この 3 つ目の「拡張性」が、「Just use Postgres」を可能にしている核心だと感じました。著者の言葉を借りれば、Postgres は「従来のトランザクショナルワークロードを超えた幅広い用途に対応できる」。だから、新しいユースケースが出てきても、まず Postgres で試す価値があります。運用の観点からも、この 3 つは重要です。オープンソースだから、ベンダーロックインのリスクがないエンタープライズ対応だから、夜中3時にPagerDutyが鳴って「どの DB だ...？」と確認する時間を省略できるインフラエンジニアとして信頼できる拡張性があるから、新しいデータベースを追加する代わりに既存の Postgres を活用できるDocker でサクッと起動1.3 節では、Docker での起動方法が紹介されています。docker run --name postgres \    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \    -p 5432:5432 \    -v postgres-volume:/var/lib/postgresql/data \    -d postgres:17.2「1 分以内にコンテナとして起動可能」と Summary に書いてありますが、本当にその通りです。この手軽さが、「Just use Postgres」の実践を支えています。新しいユースケースを試すために、まず手元で動かしてみる。それが 1 分でできます。ツールを開発していることがあるのですがこれはツールの普及にめちゃくちゃ大事です。開発環境だからシンプルな設定で OK ですが、本番では当然違います。ユーザー名は postgres 以外にする、パスワードは環境変数じゃなく secrets で管理する、など。でも、それはこの本の scope 外でしょう。PostgreSQL徹底入門 第4版 インストールから機能・仕組み、アプリ作り、管理・運用まで作者:近藤 雄太,正野 裕大,坂井 潔,鳥越 淳,笠原 辰仁翔泳社Amazonpsql と generate_series1.4 節では psql での接続方法、1.5 節では generate_series を使ったモックデータ生成が紹介されています。INSERT INTO trades (id, buyer_id, symbol, order_quantity, bid_price, order_time)SELECT    id,    random(1,10) as buyer_id,    (array['AAPL','F','DASH'])[random(1,3)] as symbol,    random(1,20) as order_quantity,    round(random(10.00,20.00), 2) as bid_price,    now() as order_timeFROM generate_series(1,1000) AS id;generate_series と random の組み合わせで、複雑なモックデータを SQL だけで生成できます。外部ツール不要。これも「Just use Postgres」の一例だと感じました。「テストデータ生成ツールが必要だ」と言い出す前に、Postgres の標準機能で解決できます。普段、テストデータ生成はアプリ側（Rust）でやることが多かったのですが、シンプルなケースなら generate_series で十分かもしれません。試しに手を動かしてみたら、いくつか発見がありました。まず、generate_series は日付生成にも使えます。generate_series('2025-01-01'::date, '2025-12-31', '1 day') でカレンダーテーブルを一発生成できます。これは便利。次に、random() は毎回異なる値を返すので、再現可能なテストには setseed() を事前に呼ぶ必要があります。これを知らずに「テスト結果が毎回違う！」と焦った経験があります。そして一番ハマったのは、配列のインデックスが 1 始まりだということ。(array['AAPL','F','DASH'])[random(1,3)] のように 1 から始めないと想定外の結果になります。Rust や Python に慣れていると、0 から始めたくなるんですよね。私の直感を裏切るポイントでした。ちょっと昔だとこちらの資料とかはめちゃくちゃ良いのでオススメです。 speakerdeck.com speakerdeck.com基本クエリ1.6 節では、基本的な SQL クエリが紹介されています。SELECT symbol, count(*) AS total_volumeFROM tradesGROUP BY symbolORDER BY total_volume DESC;著者は count(*) が「Postgres で特別に最適化されている」と書いています。この本を通して、Postgres の内部動作についての理解が深まりました。DBといえばそーだいさんの資料を読み漁ってほしいです。 speakerdeck.com2. Standard RDBMS capabilitiesデータベースの三層構造を理解していなかったこの章で再認識したのは、Database → Schema → Table という三層構造の実践的な使い方です。10 年近く Postgres を運用してきた中で、Schema は使ってきました。ただ、この章で説明されているような「マイクロサービスのモジュールごとにスキーマを分ける」という設計パターンは、改めて整理されると納得感があります。この章で説明されている eコマースプラットフォームの設計が、その例です。coffee_chain (database)├── products (schema)│   ├── catalog (table)│   └── reviews (table)├── customers (schema)│   └── accounts (table)└── sales (schema)    ├── orders (table)    └── order_items (table)マイクロサービスのモジュールごとにスキーマを分ける。この設計パターン自体は知っていましたが、この本の整理の仕方は参考になります。著者は明確に書いています。Each application module or microservice has its own schema containing all the related data.これなら、アプリケーション層のアーキテクチャとデータベース層の構造が一致します。名前の衝突も避けられます。でも、同じデータベース内だから、JOIN で複数スキーマのテーブルをまたいでクエリできます。マルチテナント構成において Database レベルで分離していることも納得がいきました。テナントごとにデータベースを分け、リソースを共有しながら完全に隔離します。スケールアウト時には特定のテナントだけ別サーバーへ移動可能です。この設計思想、次のプロジェクトで導入を検討しようと思います。制約はデータベースでやるべきか2.3 節のデータ整合性の話で、著者のスタンスが面白かったです。著者のチームは、最初はアプリ層ですべてを検証する想定でした。でも、実際にプロダクトを構築していく中で、アプリ層のチェックが破られてデータ整合性の問題が発生しました。その経験から、著者はこう述べています。we decide to add additional constraints at the database level.私の経験でも、制約をデータベースに入れるべきか、アプリ層でやるべきかという論争が何度もありました。著者は両方を推奨しています。アプリ層でチェックしつつ、データベース層にも防御線を張ります。この章では、バグでアプリ層のチェックが破られたとき、外部キー制約がデータの不整合を防いだ例が出てきます。ERROR: insert or update on table "reviews" violatesforeign key constraint "products_review_product_id_fk"DETAIL: Key (product_id)=(1004) is not present in table "catalog".アプリのバグで product_id が 4 じゃなくて 1004 になっていました。でも、外部キー制約があったから、データベースにゴミが入らずに済みました。多層防御。これがデータ整合性の正しいアプローチだと感じました。アプリ層だけに頼ると、コードが変わったときに破綻します。データベース層だけに頼ると、エラーハンドリングが遅れて UX が悪化します。両方でやるべきです。トランザクション分離レベルの実践理解2.4 節のトランザクションで、MVCC と read committed 分離レベルの説明が具体的で良かったです。理論は知っていました。でも、この章の Table 2.1 の 2 つの psql セッションを並行実行する例を見て、実際の動きがイメージできるようになりました。2 つのトランザクションが同じ商品 (id=1) の在庫数を同時に減らそうとします。トランザクション 1 が UPDATE を実行（まだコミットしてない）トランザクション 2 が SELECT を実行 → まだ 199 が見える（dirty read を防いでいる）トランザクション 2 が UPDATE を実行 → ブロックされるトランザクション 1 が COMMIT → トランザクション 2 がアンブロックされるトランザクション 2 の UPDATE が最新の値（198）を読み直して実行される最後のポイントが重要でした。ブロックが解除された後、トランザクション 2 は再度値を読み直します。だから、結果は 197 になります（199 → 198 → 197）。もしこれがなかったら、トランザクション 2 は古い値（199）から 1 を引いて 198 にしてしまい、トランザクション 1 の更新が消えます（lost update）。Postgres の read committed は dirty write も防ぎます。だから、本番環境でデフォルトの分離レベルとして十分に使えます。もちろん、phantom read や non-repeatable read を防ぎたいケースもあります。そのときは repeatable read や serializable を使います。でも、大半のユースケースでは read committed で問題ありません。この理解、実際に手を動かさないと身につきませんでした。データベース関数で何をやるべきか2.6 節の関数とトリガーは、この章で一番刺激的でした。著者は order_add_item と order_checkout という 2 つの PL/pgSQL 関数を実装しています。ショッピングカートの管理ロジックをデータベース関数として実装した例です。最初は「これ、アプリ層でやればいいんじゃない？」と思いました。モダンなマイクロサービスアーキテクチャを信奉する我々にとって、データベース関数は「おじいちゃんの時代の遺物」みたいなイメージがありました。でも、著者の説明を読んで納得しました。書籍には「At least two scenarios come to mind」として 2 つのシナリオが紹介されています。複雑なビジネスロジックがデータと密結合している場合 → すべてのクライアントアプリやマイクロサービスで同じロジックを実装するより、データベース関数 1 つで済む複数ステップの処理でアプリとデータベース間の往復が必要な場合 → 大量のデータ転送が必要なとき、データベース内で完結させたほうが効率的order_add_item 関数の実装を見ると、この 2 つの利点がよくわかります。CREATE OR REPLACE FUNCTION sales.order_add_item(customer_id_param INT,  product_id_param INT, quantity_param INT)RETURNS TABLE (...) AS $$DECLARE    pending_order_id UUID;BEGIN    -- 1. 既存の pending order を探す    SELECT id INTO pending_order_id FROM sales.orders    WHERE customer_id = customer_id_param AND status = 'pending';    -- 2. なければ作る    IF pending_order_id IS NULL THEN        INSERT INTO sales.orders (customer_id, status)        VALUES (customer_id_param, 'pending')        RETURNING id INTO pending_order_id;    END IF;    -- 3. 商品を追加または更新（MERGE 文）    MERGE INTO sales.order_items AS oi ...    -- 4. 結果を返す    RETURN QUERY SELECT ...;END;$$ LANGUAGE plpgsql;これをアプリ層でやろうとすると、複数のクエリを順次実行する必要があります。SELECT で pending order があるか確認なければ INSERT で作成SELECT で商品の価格を取得INSERT or UPDATE で order_items に追加SELECT で最終的なカート内容を取得アプリとデータベース間で何度もデータをやり取りする必要があり、ネットワークレイテンシの影響を受けます。データベース関数なら 1 回の呼び出しで完結します。しかも、トランザクショナルに実行されます。途中でエラーが起きたら全部ロールバックされます。でも、すべてをデータベース関数でやるべきではありません。著者も「少なくとも 2 つのシナリオ」と言っています。つまり、適切なユースケースを見極めることが重要です。私の基準はこうです。やるべき: データと密結合した複雑なロジック、複数ラウンドトリップが必要なケースやらないべき: ビジネスロジックの大半、頻繁に変更されるロジック、外部 API との連携この判断基準、次のプロジェクトで使いたいです。ちなみに、PL/pgSQL を書いていて何度かハマったポイントがあります。SELECT ... INTO で結果が 0 行の場合、変数は NULL になります。エラーにはなりません。これを知らずに「なぜ NULL が入る？」と 30 分悩んだことがあります。あと、ON CONFLICT DO UPDATE で新しい値を参照するには EXCLUDED を使います。EXCLUDED.quantity のように書きます。最初は「新しい値をどう参照するんだ？」と混乱しました。一番タチが悪いのは、変数名とカラム名の衝突です。SELECT * FROM orders WHERE order_id = order_id と書くと、両方が変数として解釈されて全行が返ってきます。デバッグが本当に難しい。だから p_customer_id や v_order_id のようにプレフィックスを付けるのがベストプラクティスです。トリガーは「見えない魔法」になりやすい2.6.2 節のトリガーの例も実践的でした。order_items テーブルに変更があったら自動的に orders.total_amount を更新します。CREATE TRIGGER trigger_update_order_totalAFTER INSERT OR UPDATE OR DELETE ON sales.order_itemsFOR EACH ROWEXECUTE FUNCTION sales.update_order_total();これはシンプルで便利ですが、トリガーは「見えない魔法」になりやすいと感じました。アプリ層のエンジニアが INSERT INTO sales.order_items を実行したとき、裏で sales.orders が更新されていることに気づかないかもしれません。トリガーが増えると、データベースのパフォーマンス問題の原因を追うのが難しくなります。「なぜこの INSERT が遅い？」と思ったら、実は裏で 3 つのトリガーが動いていた、みたいな。著者はトリガーの適切なユースケースを挙げています。Triggers are particularly useful in audit scenarios, where you need to track who made changes in the database, or in event-driven architectures.監査ログ（誰がいつ変更したか）イベント駆動アーキテクチャ（変更を他のシステムに通知）トリガーを書いていて一度ハマったのは、NEW と OLD の使い分けです。NEW は INSERT/UPDATE で使用可能で、OLD は UPDATE/DELETE で使用可能。DELETE トリガーで NEW.order_id にアクセスしようとしてエラーになりました。COALESCE(NEW.order_id, OLD.order_id) のような対応が必要だと、その時初めて知りました。あと、大量の行を更新する場合、行ごとにトリガーが発火してパフォーマンスが低下します。FOR EACH ROW のトリガーは便利ですが、一括更新のパフォーマンスには注意が必要です。これ以外のケースでは、慎重に検討すべきだと思います。View は「名前付きクエリ」2.7 節の View の説明はシンプルで明快でした。A view is essentially a named query that returns data in a tabular format.View = 名前付きクエリ。この理解が正しいです。複雑な JOIN と集計を含むクエリを、アプリ層の複数箇所で使い回すより、View として定義してしまいます。CREATE VIEW sales.product_sales_summary ASSELECT    c.name AS product_name,    c.category,    SUM(oi.quantity) AS total_quantity_sold,    SUM(oi.quantity * oi.price) AS total_revenueFROM products.catalog cLEFT JOIN sales.order_items oi ON c.id = oi.product_idGROUP BY c.idORDER BY total_quantity_sold DESC, total_revenue DESC;アプリ層からはこうです。SELECT * FROM sales.product_sales_summary WHERE category='coffee';これだけで済みます。Materialized View も便利ですが、リフレッシュのタイミングが悩ましいです。手動リフレッシュ：ユーザーが「更新」ボタンを押したとき定期リフレッシュ：pg_cron で 1 時間ごとイベント駆動：トリガーで特定のテーブルが更新されたとき著者は 3 つのアプローチを提案していますが、ユースケースによって使い分けるべきです。3. Modern SQLSQL-92 の呪縛から解き放たれるこの章を読んで改めて認識したのは、自分がまだ SQL-92 の世界に閉じこもっていたという事実です。pgsql-jp.github.io著者の Markus Winand の言葉を引用します。Since 1999, SQL is not limited to the relational model anymore. Back then, ISO/IEC 9075 (the "SQL standard") added arrays, objects, and recursive queries. In the meantime, the SQL standard has grown five times bigger than SQL-92. In other words: relational SQL is only about 20% of modern SQL.SQL-92 は全体の 20% でしかありません。残りの 80% が Modern SQL です。でも、正直に言うと、私は長年その 20% の世界で生きていました。CTE は知っていたけど、「読みやすさのための構文糖衣」程度にしか思っていませんでした。Window Functions も「集計が少し楽になるやつ」くらいの認識。Recursive Queries に至っては、「使う機会がない」と決めつけていました。この章を読み終えて、自分がどれだけ Postgres の可能性を狭めていたかを再認識しました。なぜ Modern SQL を使わないのか著者は、Modern SQL が普及しない理由を 2 つ挙げています。理由1: 獲得した知識の粘着性（Stickiness of gained knowledge）Some developers learned SQL many years ago and mastered the SQL-92 version of the language for various data processing tasks. Even if their SQL queries are verbose or less efficient, the tasks are still solvable. As a result, many people continue doing things the way they originally learned.痛いほど身に覚えがあります。私が SQL を覚えたのは 15 年以上前です。当時の教科書は SQL-92 ベースで、GROUP BY、JOIN、Subquery があれば何でも解決できました。その成功体験が、今も私の手を縛っています。まるで、「ガラケーで十分じゃん」と言い張っていた 2010 年の自分を見ているようです。「CTE を使えば読みやすくなる」と頭ではわかっていても、「でも、Subquery でも書けるしな」と思ってしまいます。結果、冗長で読みにくいクエリを量産します。後輩に「このネストの深さ、どこまで行くんですか…？」と言われたことは秘密です。理由2: ORM フレームワークSome developers fully rely on ORM frameworks as a layer between their application and the database. They trust the ORM framework to generate SQL queries, believing it knows the best way to query or manipulate data.これも痛い指摘です。やめてくれおれにきく。ORM は確かに便利です。でも、ORM が生成するクエリは「汎用的なワークロード」を想定しています。Window Functions を使えば 1 回のクエリで済むケースでも、ORM は複雑な Self-Join を生成するかもしれません。第 1 章で学んだ「Just use Postgres」の思想は、ORM へ任せる前に、Postgres で何ができるかを知ることにも通じます。CTE（Common Table Expressions）は単なる Subquery の糖衣構文ではないCTE は「読みやすい Subquery」という側面で使うことが多かったです。でも、この章を読んで、それ以上の価値があることを再確認しました。Listing 3.3 の例では、2 つの CTE を使って「3 人以上のユーザーが聴いて、半分以下の時間しか再生しなかった曲」をランキングしています。WITH plays_cte AS (    SELECT s.title, s.duration, p.play_duration, p.user_id    FROM streaming.plays p    JOIN streaming.songs s ON p.song_id = s.id    WHERE p.play_start_time::DATE BETWEEN '2024-09-15' AND '2024-09-16'      AND p.play_duration < (s.duration / 2)),user_play_counts AS (    SELECT title, duration, COUNT(DISTINCT user_id) AS user_count,      MIN(play_duration) AS min_play_duration,      COUNT(*) AS total_play_count    FROM plays_cte    GROUP BY title, duration)SELECT title, duration, min_play_duration, total_play_countFROM user_play_countsWHERE user_count >= 3ORDER BY min_play_duration ASCLIMIT 3;このクエリを Subquery で書いたら、どうなるでしょうか。ネストが深くなって、読みにくくなります。メンテナンスもしにくくなります。でも、著者が強調しているのは「読みやすさ」だけではありません。If we want to understand how a query is actually executed by Postgres, we can look at the query execution plan using the EXPLAIN statement.EXPLAIN の結果を見ると、Postgres は plays_cte を user_play_counts に fold しています。つまり、CTE を使っても、実行計画は効率的なままです。これは重要なポイントです。以前のバージョンでは CTE が「最適化の壁」になることがありましたが、現在は改善されています。実際に EXPLAIN ANALYZE で確認してみました。Postgres は CTE をインライン展開して最適化しています。以前は CTE が「最適化の壁」と呼ばれていましたが、現在のバージョンでは改善されています。CTE が展開されて効率的なプランになっていることが確認できました。Postgres は賢いです。Data-modifying CTE という選択肢Listing 3.4 で紹介されている Data-modifying CTE は、私にとって完全に新しい概念でした。WITH updated_play AS (    UPDATE streaming.plays    SET play_duration = 200    WHERE id = 30    RETURNING song_id, play_duration)SELECT s.title, s.duration,       CASE           WHEN up.play_duration = s.duration THEN 'Moved Up the Rank'           ELSE 'Rank Not Changed'       END AS rank_change_statusFROM updated_play upJOIN streaming.songs s ON s.id = up.song_id;UPDATE の結果を RETURNING で受け取り、その結果を使って SELECT を実行します。これが 1 つのトランザクション内で完結します。これまで、「UPDATE してから SELECT」という処理は、2 つのクエリを順番に実行していました。でも、Data-modifying CTE を使えば、1 つのクエリで完結します。アトミック性も保証されます。なぜこの機能を今まで積極的に使ってこなかったのでしょうか。使う場面を意識していなかったというのが正直なところです。Recursive Queries は「特殊なケースでしか使わない」という誤解Recursive Queries は「組織の階層構造を扱う時に使う機能」という認識でした。実際、それ以外の場面で使う機会は多くありませんでした。でも、この章を読んで、活用範囲が広いことを再確認しました。著者が例として挙げているのは、音楽ストリーミングサービスの「連続再生」のトラッキングです。plays テーブルには played_after というカラムがあり、「この曲の前に再生された曲の ID」を保持しています。つまり、連続再生はリンクリストの構造を持っています。Listing 3.8 の Recursive Query は、この連続再生のシーケンスを取得します。WITH RECURSIVE play_sequence AS (    SELECT id, user_id, song_id,      play_start_time, play_duration, played_after    FROM streaming.plays    WHERE id = 5    UNION ALL    SELECT p.id, p.user_id, p.song_id, p.play_start_time,       p.play_duration, p.played_after    FROM streaming.plays p    JOIN play_sequence ps ON p.played_after = ps.id)SELECT user_id, song_id, play_start_time,  play_duration as duration, played_afterFROM play_sequenceORDER BY play_start_time;これを読んで、以前アプリ側で何度もループしてクエリを投げていた処理を思い出しました。以前のプロジェクトで、SNS のスレッド返信を表示する機能を実装した時、「親コメント ID」を辿って、アプリ側で再帰的にクエリを投げていました。その結果、N+1 問題が発生して、パフォーマンスが悪化しました。当時のアプリログを見返すと、同じユーザーの操作で DB への接続数が 47 回。まるでチャットボットが会話のキャッチボールをしているかのようでした。もちろん、レスポンスタイムは 3 秒超え。あの時、Recursive Query を知っていたら、1 回のクエリで全ての返信を取得できました。Recursive Query の実行フローListing 3.7 の擬似コードは、Recursive Query の実行フローを明確に説明しています。# Step 1: 非再帰項を実行（初期データ）non_recursive_result = execute(non_recursive_term);# Step 2: 重複削除（UNION の場合）if (using UNION)    non_recursive_result = remove_duplicates(non_recursive_result);# Step 3: 最終結果に追加final_result.add(non_recursive_result);# Step 4: ワーキングテーブルを初期化working_table = non_recursive_result;# Step 5: 再帰項を実行（ワーキングテーブルが空になるまで）while (working_table is not empty) {    intermediate_table = execute(recursive_term, using=working_table);    if (using UNION)        intermediate_table =            remove_duplicates(intermediate_table, excluding=final_result);    final_result.add(intermediate_table);    working_table = intermediate_table;}このフローを読んで、「Recursive Query は魔法じゃなくて、ちゃんとした仕組みがある」と納得できました。特に重要なのは、UNION と UNION ALL の違いです。UNION は重複削除するので、無限ループを防げます。UNION ALL は重複を許すので、パフォーマンスは良いですが、無限ループのリスクがあります。ところで、Recursive CTE を書いていて気になったのは終了条件です。調べてみると、終了条件は「新しい行が生成されなくなるまで」で、明示的に書く必要はありません。循環検出には配列で訪問済みノードを追跡する方法が有効です。ARRAY[id] AS path で初期化して、ps.path || p.id で追加していく。NOT p.id = ANY(ps.path) で循環を検出できます。このパターンは覚えておくと便利です。ただし、深い階層（数千レベル）ではパフォーマンスが低下します。グラフ DB ほど柔軟なグラフ探索はできません。SNS の友達の友達を無限に辿るような処理には向いていないです。この違いを理解していないと、本番環境で無限ループが発生します。怖いです。データベース監視の Slack チャンネルが「CPU 使用率 100%」「接続数の上限到達」で埋め尽くされる光景は、二度と見たくありません。Window Functions は Self-Join の代替ではないWindow Functions は「Self-Join の代わりに使える構文」という認識で使ってきました。でも、この章を読んで、パフォーマンス面での違いを改めて確認しました。Listing 3.11 の Self-Join と Listing 3.12 の Window Function を比較すると、違いが明確です。Self-Join 版:SELECT DISTINCT p.song_id,       p.user_id,       t.total_durationFROM streaming.plays pJOIN (    SELECT song_id,           SUM(play_duration) AS total_duration    FROM streaming.plays    GROUP BY song_id) t ON p.song_id = t.song_idORDER BY p.song_id;Window Function 版:WITH plays_with_total AS (  SELECT    song_id, user_id, SUM(play_duration)    OVER (PARTITION BY song_id) AS total_duration  FROM streaming.plays)SELECT DISTINCT song_id, user_id, total_durationFROM plays_with_totalORDER BY song_id, user_id;Self-Join 版は、テーブルを 2 回走査しています。Window Function 版は、1 回の走査で済みます。著者の言葉を借りれば、次のようになります。Although the self-join approach works as expected, it's not the most efficient, because every row of the table is accessed twice. Additionally, it's not the easiest to follow when trying to understand the query logic.これを読んで、「Window Functions は単なる糖衣構文じゃなくて、パフォーマンス最適化の手段だった」と気づきました。Running Total と Window FrameListing 3.13 の Running Total の計算は、Window Functions の本質を理解する上で重要でした。SELECT song_id, user_id, play_duration, SUM(play_duration)OVER (PARTITION BY song_id ORDER BY user_id) AS total_play_durationFROM streaming.playsWHERE song_id = 2;結果は次のようになります。 song_id | user_id | play_duration | total_play_duration---------+---------+---------------+---------------------       2 |       1 |           144 |                 144       2 |       2 |           206 |                 350       2 |       3 |           186 |                 654       2 |       3 |           118 |                 654PARTITION BY song_id で Window を作り、ORDER BY user_id で Window を Frame に分割します。各 Frame は、現在の行 + それ以前の行を含みます。この仕組みを理解すると、「累積和」「移動平均」「ランキング」といった処理が、すべて Window Functions で解決できることがわかります。以前のプロジェクトで、時系列データの累積和を計算する時、アプリ側でループを回していました。あれも、Window Functions を使えば 1 回のクエリで済みました。RANK() と ROW_NUMBER() の違いListing 3.14 の RANK() は、同じ値に同じランクを付けます。SELECT song_id, SUM(play_duration) AS total_play_duration,RANK() OVER (ORDER BY SUM(play_duration) DESC) AS song_rankFROM streaming.playsGROUP BY song_idORDER BY song_rank;もし ROW_NUMBER() を使っていたら、同じ値でも異なる番号が振られます。この違いを理解していないと、ランキング機能で不具合が発生します。実際に試してみると、3 つの関数の違いがはっきりします。ROW_NUMBER(): 同じ値でも異なる番号（1→2→3→4→5）RANK(): 同じ値は同じ番号で次は飛ぶ（1→2→2→4→5）DENSE_RANK(): 同じ値は同じ番号で次は飛ばない（1→2→2→3→4）以前、ランキング機能で ROW_NUMBER() を使って、同点の処理がおかしくなったことがあります。「なぜ同じスコアなのに順位が違うの？」というバグ報告を受けて、RANK() に変更しました。この違いは一度経験すると忘れません。4. Indexesインデックスの「当たり前」を疑う第 4 章「Indexes」の冒頭の一文が、自分の習慣を言い当てていました。Indexes are often the first optimization technique that comes to mind when dealing with a long-running query or a slow database operation.そうなんです。遅いクエリがあったら、とりあえずインデックス張る。それが 10 年間の私のパターンでした。まるで風邪を引いたら「とりあえずビタミン C」みたいな、根拠のない安心感でした。でも、著者は続けます。They've proven so effective in many scenarios that we sometimes overlook other optimization methods, turning to indexes right away.インデックスに頼りすぎて、他の最適化手法を見落としている。この指摘は痛かったです。実際、過去のプロジェクトで「遅いクエリ問題」が発生した時、私はいつもまずインデックスを疑っていました。でも、本当は EXPLAIN で実行計画を見て、ボトルネックを特定してから判断すべきでした。この章では、インデックスの「なぜ」と「いつ」を徹底的に掘り下げています。単なるインデックス作成のチュートリアルじゃありません。インデックス戦略の哲学です。なぜインデックスがこんなに人気なのか4.1 節「Why are indexes so popular?」では、O(N)と O(log_b N)の違いが説明されています。100 件のテーブルで ID=5 を探す場合。インデックスなし：最大 100 回のルックアップ（O(N)）B-tree インデックスあり：最大 4 回のルックアップ（O(log_b N)、b=3 の場合）これが 100 万件に増えても、インデックスがあれば 6 回のルックアップで済みます（b=10 の場合）。正直、この計算量の違いは知っていました。でも、著者が示した表を見て改めて驚きました。 テーブルサイズ  インデックスルックアップ回数  100件          2回                         1,000件        3回                         1,000,000件    6回                         10,000,000件   7回                         1,000,000,000件  9回                      10億件のテーブルでも9回のルックアップ。これがインデックスの威力です。深夜の障害対応で「インデックス張れば解決するっしょ」と言い続けてきた自分が、ようやく理論武装できた瞬間でした。そして、著者の言葉が刺さります。As a result, it's no surprise that indexes are such a popular optimization technique.インデックスが人気な理由は、この圧倒的な効率性にあります。でも、だからこそ安易に使いすぎるリスクもあります。EXPLAIN — まず実行計画を見ろ4.4 節で EXPLAIN が詳しく説明されています。私はこれまで、EXPLAIN ANALYZE しか使っていませんでした。でも、この章を読んで EXPLAIN (analyze, costs off) や EXPLAIN (analyze, buffers on) といった他のオプションを知りました。特に印象的だったのは、buffers オプションです。Buffers: shared hit=3これは「3 ページをメモリから読んだ（ディスクアクセスなし）」という意味です。もし read=4 があれば、「4 ページをディスクから読んだ」ということになります。遅いクエリの原因はインデックスの欠如じゃなく、メモリ不足かもしれません。この視点は新鮮でした。私は「遅い = インデックスがない」と決めつけていました。でも、buffers を見れば、ディスク I/O が原因なのか実行計画が原因なのか区別できます。著者は次のように書いています。This information is crucial because a query might run slowly not due to a suboptimal execution plan or missing index but because memory has become a limited resource.インデックスは万能じゃありません。頭ではわかっていても、実務では軽視しがちでした。単一カラムインデックス — B-tree vs Hash4.5 節では、単一カラムインデックスが 2 種類紹介されています。B-tree：範囲検索（>, <, BETWEEN）に対応Hash：等価検索（=, IN）のみ私は今まで、Hash インデックスを積極的に選択してきませんでした。「B-tree がデフォルトだから」という理由で、あえて変える必要性を感じていなかったためです。でも、この章を読んで考えが変わりました。例えば、ゲーム内のチャンピオンタイトル（5 種類のみ）を検索する場合。範囲検索は不要で、等価検索だけで十分です。この場合、Hash インデックスが最適です。CREATE INDEX idx_champion_titleON game.player_statsUSING hash(champion_title);実行計画を見ると、Hash インデックスを使った場合の実行時間は 0.073 ms。フルテーブルスキャンの 1.463 ms と比べて 20 倍速いです。ユースケースに合わせてインデックスタイプを選ぶ。これが正しいアプローチです。複合インデックス — 順番が命4.6 節「Composite indexes」は、この章で最も重要なセクションだと思います。複合インデックスの順番は、クエリのパフォーマンスに直結します。例えば、(region, score DESC, win_count DESC) というインデックスを作った場合。CREATE INDEX idx_region_score_win_countON game.player_stats (region, score DESC, win_count DESC);このインデックスは、次のクエリで使われます。-- ✅ 使われるWHERE region = 'NA' and score > 5000 and win_count > 10-- ✅ 使われるWHERE region = 'EMEA' and score > 1000-- ✅ 使われる（先頭カラムがあるから）WHERE region = 'EMEA'-- ❌ 使われない（先頭カラムがない）WHERE score > 1000 and win_count > 30先頭カラム（leading column）が必須。これがないと、複合インデックスは使われません。この章を読みながら実際に手を動かしてみました。EXPLAIN ANALYZE の出力で Index Scan と Index Only Scan の違いを確認することが重要です。Index Only Scan はテーブルにアクセスしないので高速。Covering Index の威力を実感しました。Partial Index については、WHERE 句が完全に一致する場合のみ使用されるという点に注意が必要です。WHERE play_time <= '50 hours' で作ったインデックスは、WHERE play_time <= '50 hours 1 second' では使われません。1 秒違うだけで使われない。厳密すぎる気もしますが、そういう仕様です。Hash インデックスは範囲検索（<, >, BETWEEN）には使えません。等価検索専用です。これを知らずに「なぜインデックスが使われないんだ？」と悩んだことがあります。インデックスサイズを比較した結果も興味深かったです。idx_champion_hash   - 696 kBidx_covering        - 416 kBidx_region_score    - 248 kBidx_perf_margin     - 120 kBidx_casual_players  - 48 kB   -- Partial Index は最小Partial Index のサイズの小ささは印象的でした。必要な部分だけをインデックス化するという発想、もっと早く知りたかったです。ただし、著者は注記しています。However, starting with Postgres 18, the database introduced support for skip scan lookups on composite B-tree indexes, allowing us to skip leading columns and still use the index in more scenarios.Postgres 18 以降では、skip scan が導入されるらしいです。これは大きな改善です。でも、現時点（Postgres 17 以前）では、複合インデックスの順番を慎重に設計する必要があります。Covering Index — テーブルアクセスをゼロに4.7 節「Covering indexes」は、インデックス最適化の最終形態だと感じました。通常、インデックスは「どの行を読むか」を決めるだけで、実際のデータ（username など）はテーブルから取得します。でも、Covering Index を使えば、インデックスだけで全てのデータを取得できます。CREATE INDEX idx_composite_covering_indexON game.player_stats (region, score DESC, win_count DESC)INCLUDE (username);INCLUDE 句で username をインデックスに含めることで、テーブルアクセスが不要になります。実行計画を見ると。Index Only Scan using idx_composite_covering_index on player_statsHeap Fetches: 0Execution Time: 0.602 msHeap Fetches: 0 — テーブルに一切アクセスしていません。実行時間は 0.602 ms。以前の 1.856 ms（Bitmap Index Scan）から 3 倍速くなりました。ただし、トレードオフがあります。username を更新するたびに、インデックスも更新する必要があります。However, as a tradeoff, all included columns must remain consistent with the table data.更新頻度が低いカラムなら Covering Index は有効。逆に、頻繁に更新されるカラムには向きません。Partial Index — 必要な部分だけインデックス化4.8 節「Partial indexes」では、インデックスのサイズを減らす手法が紹介されています。例えば、10,000 人のプレイヤーのうち、74 人（0.74%）だけが「occasional players（プレイ時間 50 時間以下）」だとします。この 74 人だけを頻繁に検索するなら、全体にインデックスを張る必要はありません。CREATE INDEX idx_occasional_playersON game.player_stats (play_time)WHERE play_time <= '50 hours';この Partial Index により。インデックスサイズが大幅に削減される更新時のインデックスメンテナンスコストが減る検索速度は 2 ms から 0.168 ms に改善（20 倍速）ただし、条件が少しでも違うとインデックスが使われません。-- ✅ 使われるWHERE play_time <= '50 hours'-- ❌ 使われない（1秒超過）WHERE play_time <= '50 hours 1 second'Partial Index は条件が厳密。これを理解して使う必要があります。Expression Index — 計算結果にインデックス4.9 節「Functional and expression indexes」は、私にとって全く新しい概念でした。例えば、「勝数 - 負数」というパフォーマンスマージンで検索したい場合。WHERE (win_count - loss_count) BETWEEN 300 and 450通常、この式はクエリ実行時に毎回計算されます。でも、Expression Index を使えば、計算結果をインデックス化できます。CREATE INDEX idx_perf_marginON game.player_stats ((win_count - loss_count));実行時間は 2.524 ms から 1.200 ms に改善（2 倍速）。ただし、式が複雑になると、インデックスのメンテナンスコストが増えます。win_count または loss_count が更新されるたびに、インデックスも更新されます。頻繁に検索される式にのみ使うのが正しい戦略です。Over-Indexing という警告この章の最後に、著者は重要な警告を発しています。Throughout this chapter, we've explored and added various indexes to the game.player_stats table, bringing the total number of indexes for the table to seven.7 つのインデックス。これは典型的な Over-Indexing だと著者は指摘します。Although this is acceptable for learning purposes, in practice, it represents a classic case of over-indexing.インデックスは無料じゃありません。作成時にディスク容量を消費する更新時にメンテナンスコストがかかる計画時（Planning Time）にオプション評価のコストがかかる私は過去、インデックスを「作りすぎる」傾向がありました。「とりあえずこのカラムにもインデックス張っとくか」という感じで。まるで保険に入りまくる不安な中年のように、あらゆるカラムに「念のため」インデックスを追加していました。そして毎回、INSERT が遅くなってから後悔する、という黄金パターンです。でも、この章を読んで、インデックスは慎重に設計すべきだと改めて理解しました。著者は Appendix A で Over-Indexing と Under-Indexing について詳しく説明しています。実際に読んでみて、自分の過去の設計を振り返る良い機会になりました。この辺の基礎がちゃんとできているか定期的に確認することは大切にしていますが、この Appendix がめちゃくちゃ面白いのでおすすめです。mickindex.sakura.ne.jp5. Postgres and JSONJSON 機能を使うべき場所と使わない場所5.3 節の「JSON in Postgres: Striking the balance」が、この章の核心です。著者が書いているのは、「JSON をすべてのデータに使うな」という明確な警告です。Even though Postgres provides full-fledged support for JSON, you should avoid storing all application data in JSON-specific data types as you would in a pure document database.これが「Just use Postgres」の真髄だと感じました。MongoDB を追加する代わりに Postgres の JSON 機能を使う。でも、すべてのデータを JSON で保存する MongoDB みたいな使い方はするな。ハイブリッドアプローチを取れ、と。pizzeria.order_items テーブルの構造が、この考え方を体現しています。CREATE TABLE pizzeria.order_items (    order_id INT NOT NULL,    order_item_id INT NOT NULL,    pizza JSONB NOT NULL,  -- ここは JSON    price NUMERIC(5,2) NOT NULL,  -- ここは通常の型    PRIMARY KEY (order_id, order_item_id));order_id と price は通常の型で、検索とデータ整合性を重視。pizza の詳細（トッピング、クラスト、ソースなど）は JSONB で、柔軟性を重視。この設計、10 年前のプロジェクトで欲しかったです。JSON を使うべき場面著者が挙げている 3 つの基準が具体的でわかりやすいです。データが静的または更新頻度が低い（設定、メタデータ、顧客プリファレンス）データが疎（スパース）（多くの null や 0、feature flags など）スキーマの柔軟性が必要（外部 API のレスポンス、テレメトリイベント）ピザ注文の詳細は「静的」に該当します。注文確定後はほぼ変更されません。もし従来の正規化モデルで実装すると、5 つのテーブル（pizzas、order_items、pizza_cheeses、pizza_veggies、pizza_meats）が必要になります。著者が示した例を見て、「ああ、これは辛い」と思いました。Write overhead: 1 つのピザ注文のために複数テーブルへの INSERT が必要。7 つのトッピングなら 7 レコード。Read overhead: ピザのレシピを再構築するために複数テーブルの JOIN が必要。Transformation overhead: フロントエンドが JSON で受け取るのに、わざわざ正規化モデルへ分解し、また JSON に戻す。この 3 つの overhead、すべて経験があります。特に Transformation overhead が一番つらいです。API レスポンスを JSON で返すためだけに、複雑な JOIN と整形ロジックを書く。「JSON から分解して正規化して、また JOIN して JSON に戻す」という、まるで水を凍らせてから溶かすような無駄な作業。当時の自分に「Postgres の JSONB を使えばいいぞ」と教えてあげたいです。著者が「hybrid approach」を推奨する理由がよくわかりました。json vs jsonb5.1 節で json と jsonb の違いが説明されています。 型  保存形式  Write 性能  Read 性能  インデックス  推奨度  json  テキスト  速い  遅い（毎回パース）  限定的  ❌  jsonb  バイナリ  遅い（パースあり）  速い  GIN など充実  ✅ 著者の推奨は明確です：jsonb をデフォルトで使え。Overall, the jsonb data type is the recommended default for storing and processing JSON data in Postgres, unless you have a specific use case that requires preserving the order of keys in the original JSON objects.「キーの順序を保持する必要がある」という特殊なケースでない限り、jsonb 一択です。私が過去に扱ったプロジェクトでは、なんとなく json を選んでいたことがありました。「書き込みが速いから」という理由で。でも、検索の度にパースが走るコストを考えていませんでした。典型的な「入口だけ見て出口を見ない」パターン。インフラエンジニアあるあるです。著者が書いているように、jsonb は write 時に変換コストがありますが、検索性能は圧倒的に速いです。そして GIN インデックスとの組み合わせでさらに速くなります。JSON のクエリ：-> と ->>5.4 節の JSON クエリ構文は充実しています。機能自体は知っていましたが、改めて整理すると活用の幅が広がります。基本：-> と ->>SELECT    order_id,    pizza->'size' as pizza_size,     -- JSON 形式で返す    pizza->>'crust' as pizza_crust  -- テキスト形式で返すFROM pizzeria.order_itemsWHERE order_id = 100;出力の違い。pizza_size   | pizza_crust-------------|-------------"small"      | thin-> は JSON 形式なのでダブルクォート付き。->> はテキスト型なのでダブルクォートなし。最初は「なぜ 2 つの演算子が必要なのか？」と疑問でしたが、5.4.1 節を読んで納得しました。WHERE 句での比較。-- JSON 形式で比較（ダブルクォート必要）WHERE pizza->'size' = '"small"'-- テキスト形式で比較（ダブルクォート不要）WHERE pizza->>'crust' = 'gluten_free'-> でダブルクォートを忘れると、こんなエラーが出ます。DETAIL: Token "small" is invalid.CONTEXT: JSON data, line 1: smallこの仕様、最初はわかりにくいですが、JSON の仕様に忠実だと理解すれば納得できます。Rust から Postgres に接続して JSON を扱う時、この -> と ->> の違いでハマりました。-> は JSON 型を返すので、そのまま文字列としてデシリアライズしようとするとエラーになります。->> を使うか、適切な型変換が必要です。特に pg_typeof() で型を確認しようとした時、::TEXT でキャストしないと Rust 側でエラーになりました。ネストした JSON へのアクセスSELECT    order_id,    pizza->'toppings'->'veggies' as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力。veggies_toppings-----------------------[{"tomato": "light"}]配列の特定要素にアクセスするには、インデックス（0 始まり）を指定。SELECT    order_id,    pizza->'toppings'->'veggies'->0 as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力（[] が消える）。veggies_toppings---------------------{"tomato": "light"}さらに、配列内のオブジェクトのフィールドにアクセスします。SELECT    order_id,    pizza->'toppings'->'veggies'->0->>'onion' as onions_amountFROM pizzeria.order_itemsWHERE order_id = 100;出力。onions_amount---------------lightこの連鎖、最初は読みづらいと思いましたが、慣れると直感的です。? 演算子と @> 演算子? 演算子：キーの存在確認SELECT    order_id,    pizza->'toppings'->'meats' as meatsFROM pizzeria.order_itemsWHERE pizza->'toppings' ? 'meats'ORDER BY order_id LIMIT 5;「meats キーが存在する注文だけを取得」という意味です。配列内のオブジェクトのキー存在確認は少し複雑になります。SELECT    order_id,    pizza->'toppings'->'meats' AS meatsFROM pizzeria.order_itemsWHERE EXISTS (    SELECT 1    FROM jsonb_array_elements(pizza->'toppings'->'meats') AS meats    WHERE meats ? 'sausage')ORDER BY order_id LIMIT 5;この書き方、正直、冗長だと思いました。でも著者も同じ意見で、5.4.4 節で JSON path expression を使ってシンプルにしています。@> 演算子：包含関係の確認SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free"}';「crust フィールドが gluten_free の注文を数える」という意味です。複数条件。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom"}';ネストした構造も可能。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom",                 "toppings": {"veggies": [{"tomato": "extra"}]}}';この演算子、MongoDB の $elemMatch みたいな感じだと思いました。ちなみに、@> 演算子は配列にも使えます。tags @> '["hot", "milk"]' のように書けば、配列が特定の要素を含むかどうかを検索できます。JSON オブジェクトだけでなく、配列にも対応しているのは便利です。著者が「-> と @> を組み合わせるとより読みやすくなる」と書いています。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom"}' AND      pizza->'toppings'->'veggies' @> '[{"tomato": "extra"}]';こっちの方が確かに読みやすいです。JSON Path Expressions5.4.4 節で、SQL/JSON path language が登場します。先ほどの「sausage を含む注文を検索」のクエリが、path expression でこうなります。SELECT    order_id,    pizza->'toppings'->'meats' AS meatsFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza, '$.toppings.meats[*] ? (exists(@.sausage))')ORDER BY order_id LIMIT 5;サブクエリが不要になりました。構文の説明です。$: 評価対象の JSON オブジェクト（pizza カラム）.toppings.meats: フィールドへのアクセス[*]: 配列のすべての要素?: フィルタの開始@: 現在評価中のオブジェクトexists(@.sausage): sausage フィールドが存在するか最初は読みづらかったですが、いくつか例を見ていくうちに理解できました。配列のクエリSELECT    count(*) as total_cnt,    jsonb_object_keys(        jsonb_path_query(pizza, '$.toppings.cheese[*]')    ) as cheese_toppingFROM pizzeria.order_itemsGROUP BY cheese_topping ORDER BY total_cnt DESC;$.toppings.cheese[*] で cheese 配列のすべてのオブジェクトを取得。jsonb_object_keys で各オブジェクトのキー（チーズ名）を抽出。出力。total_cnt | cheese_topping----------|----------------     2575 | mozzarella      771 | cheddar      762 | parmesanフィルタ付き path expressionSELECT    count(*) AS total_cnt,    pizza->'type' as pizza_typeFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza,        '$.toppings.cheese[*] ? (exists(@.parmesan))')GROUP BY pizza_typeORDER BY total_cnt DESC;評価順序。$.toppings.cheese[*]: すべてのチーズオブジェクトを取得?: フィルタ開始exists(@.parmesan): 現在のオブジェクトに parmesan フィールドがあるか複数フィルタのチェーンSELECT count(*)FROM pizzeria.order_itemsWHERE jsonb_path_exists(    pizza,    '$ ? (@.type == "custom") .toppings.cheese[*].parmesan ? (@ == "extra")');評価順序（左から右）です。$: pizza オブジェクト? (@.type == "custom"): type が custom か確認.toppings.cheese[*].parmesan: parmesan オブジェクトを取得? (@ == "extra"): 量が extra か確認この書き方、最初は難解だと思いましたが、左から右に評価されると理解すれば読めます。JSON の更新：jsonb_set と #-5.5 節で JSON の更新方法が紹介されています。最も簡単な方法（非推奨）-- アプリ側で JSON 全体を取得SELECT pizza FROM pizzeria.order_items WHERE order_id = $1 and order_item_id = $2;-- アプリ側で JSON を修正-- DB に書き戻すUPDATE pizzeria.order_itemsSET pizza = new_pizza_order_jsonWHERE order_id = $1 and order_item_id = $2;著者が書いているように、簡単だけど効率的じゃないです。複雑な JSON オブジェクト全体を転送するのではなく、必要なフィールドだけを更新する方が良いです。jsonb_set 関数UPDATE pizzeria.order_itemsSET pizza = jsonb_set(pizza, '{crust}', '"regular"', false)WHERE order_id = 20 and order_item_id = 5;jsonb_set の引数です。元の JSON オブジェクト（pizza）更新対象のパス（{crust}）新しい値（"regular" — JSON 文字列なのでダブルクォート必要）フィールドが存在しない場合に追加するか（false）ネストした配列の更新。UPDATE pizzeria.order_itemsSET pizza = jsonb_set(    pizza,    '{toppings,veggies}',   '[{"tomato":"extra"}, {"spinach":"regular"}]',   false)WHERE order_id = 20 and order_item_id = 5;配列の特定要素を更新する場合、パスにインデックスを含められる。-- 例：{toppings, veggies, 0, tomato} で配列の最初の要素の tomato を更新1 つ注意点があります。jsonb_set のパスが存在しない場合、第 4 引数が true なら新しいキーが作成されます。これは便利な反面、タイプミスで意図しないキーが追加されるリスクもあります。#- 演算子：フィールドの削除UPDATE pizzeria.order_itemsSET pizza = pizza #- '{toppings,meats}'WHERE order_id = 20 AND order_item_id = 5;{toppings, meats} パスのフィールドを削除します。この演算子、シンプルで良いです。インデックス：B-tree と GIN5.6 節がこの章で一番技術的に深い部分でした。Expression Index with B-tree最初の試みです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza ->> 'type' = 'custom';実行計画。Seq Scan on order_items (actual time=0.034..1.062 rows=563 loops=1)  Filter: ((pizza ->> 'type'::text) = 'custom'::text)  Rows Removed by Filter: 2375Execution Time: 1.185 ms全件スキャンです。Expression Index を作成します。CREATE INDEX idx_pizza_typeON pizzeria.order_items ((pizza ->> 'type'));再度実行計画を確認。Bitmap Index Scan on idx_pizza_type (actual time=0.068..0.068 rows=563 loops=1)  Index Cond: ((pizza ->> 'type'::text) = 'custom'::text)Execution Time: 0.376 ms4 倍近く高速化（1.185 ms → 0.376 ms）しました。でも問題があります。このインデックスは pizza ->> 'type' というexact expression にしか効きません。-- これは idx_pizza_type を使わないSELECT count(*)FROM pizzeria.order_itemsWHERE pizza -> 'type' = '"custom"';実行計画：Seq Scan に戻ります。さらに、別のフィールド（size など）を検索する場合、また別の Expression Index が必要になります。著者が書いているように、スケールしません。フィールドごとにインデックスを作り続けると、気づいたら「インデックスのインデックス」が欲しくなる世界へようこそ。GIN Index（Default）GIN（Generalized Inverted Index）を作成します。CREATE INDEX idx_pizza_orders_ginON pizzeria.order_itemsUSING GIN(pizza);GIN の仕組み（5.6.2 節の Figure 5.1 参照）です。JSON オブジェクトからすべてのキーと値を抽出して、個別のインデックスエントリとして保存します。例です。{  "size": "large",  "type": "three cheese",  "crust": "thin",  "sauce": "marinara",  "toppings": {    "cheese": [      {"cheddar": "regular"},      {"mozzarella": "extra"},      {"parmesan": "light"}    ]  }}インデックスに保存されるエントリです。Keys:- size、type、crust、sauce、toppings、cheese、cheddar、mozzarella、parmesanValues:- large、three cheese、thin、marinara、regular、extra、lightこれらのエントリは辞書順に保存され、複数のインデックスページに分散されます。GIN を使ったクエリです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"type": "custom"}';実行計画。Bitmap Index Scan on idx_pizza_orders_gin (actual time=0.109..0.110 rows=563 loops=1)  Index Cond: (pizza @> '{"type": "custom"}'::jsonb)Execution Time: 0.830 ms複雑なネスト構造でも使えます。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"toppings":{"cheese":[{"cheddar":"regular"}]}}';Postgres はインデックスから toppings, cheese, cheddar, regular の 4 つのエントリを検索して、該当する行を絞り込みます。GIN のメリット：1 つのインデックスで JSON 全体を検索可能です。GIN Index with jsonb_path_opsさらに効率的な GIN インデックスです。CREATE INDEX idx_pizza_orders_paths_ops_ginON pizzeria.order_itemsUSING GIN (pizza jsonb_path_ops);違いは、パス全体をハッシュ化して保存することです。例です。size.largetype.three cheesecrust.thinsauce.marinaratoppings.cheese.cheddar.regulartoppings.cheese.mozzarella.extratoppings.cheese.parmesan.lightこれらのパスをハッシュ関数に通して、固定長の整数として保存します。メリットです。検索が速い：固定長整数の比較は可変長テキストより速いサイズが小さい：ハッシュコードはテキストより小さい実際のサイズ比較です。index_name                      | index_size--------------------------------|------------idx_pizza_orders_gin            | 112 kBidx_pizza_orders_paths_ops_gin  | 56 kB半分のサイズです。デメリットです。jsonb_path_ops は ? 演算子（キー存在確認）をサポートしません。なぜなら、インデックスにはパスのハッシュのみが保存されていて、キー単体は保存されていないからです。-- これは idx_pizza_orders_gin を使う（jsonb_path_ops は使えない）SELECT count(*)FROM pizzeria.order_itemsWHERE pizza ? 'special_instructions';使い分け インデックスタイプ  サイズ  検索速度  サポート演算子  推奨用途  Expression Index (B-tree)  小  特定 expression のみ速い  ->, ->>  特定フィールドの頻繁な検索  GIN (default)  大  速い  ?, @>, @?, @@  柔軟な検索、キー存在確認が必要  GIN (jsonb_path_ops)  中  最速  @>, @?, @@  包含検索のみ、サイズ重視 著者が書いているように、jsonb_path_ops が第一選択です。キー存在確認が必要なら default GIN を追加します。6. Postgres for full-text search「全文検索は難しい」という思い込みこの章を読み終えて思ったのは、「Postgres の全文検索は、思ったより実用的だ」ということです。私はこれまで、全文検索といえばElasticsearchだと思っていました。実際、過去のプロジェクトで「検索機能が必要です」と言われたら、反射的に「Elasticsearch を構築しますか？」と答えていました。まるで、パブロフの犬のように。「検索」という言葉を聞いただけで、脳内で Kibana のダッシュボードが立ち上がっていました。でも、第 1 章で学んだ「Just use Postgres」の真の意味を思い出します。「別のデータベースを追加する前に、まず Postgres で解決できるか確認してみよう」この章は、その実践編でした。Tokenization と Normalization の仕組み6.1 節では、Postgres が全文検索をどう実現しているかが説明されています。基本的な流れは 4 ステップです。Tokenization（トークン化）: 文書を単語やフレーズに分割Normalization（正規化）: トークンを lexeme（語彙素）に変換Storing and Indexing: lexeme を tsvector 型で保存し、インデックスを作成Searching: 保存した lexeme に対してクエリを実行著者が ts_debug 関数を使って、"5 explorers are traveling to a distant galaxy" という文がどう処理されるかを見せてくれます。SELECT token, description, lexemes, dictionaryFROM ts_debug('5 explorers are traveling to a distant galaxy');結果を見ると、"explorers" は "explor" に、"traveling" は "travel" に変換されています。ストップワード（"are", "to", "a"）は空の lexeme {} にマッピングされています。これがステミング（語幹抽出）です。試しに to_tsvector('english', 'running runs runner') を実行してみると、'run':1,2 'runner':3 と返ってきます。running と runs は run に統一されています。だから「running」で検索しても「runs」がヒットする。これは便利です。位置情報を保持しながらストップワードを削除するという設計が巧妙です。<-> (FOLLOWED BY) オペレータで距離を計算するために、ストップワードの位置も必要になるからです。Elasticsearch でも同じようなことをやっているはずですが、Postgres ではこれが標準機能だということに改めて気づかされました。複数言語への対応6.1.2 節では、Full-text search configuration が紹介されています。Postgres には英語だけでなく、アラビア語、ロシア語、日本語など、多数の言語用の predefined configuration が用意されています。SELECT token, description, lexemes, dictionaryFROM ts_debug('russian','5 исследователей путешествуют к далёкой галактике.');ロシア語の例を見ると、russian_stem 辞書が使われています。"исследователей" が "исследовател" に、"путешествуют" が "путешеств" に変換されています。これも Elasticsearch でやろうとすると、analyzer の設定が複雑になります。JSON の設定ファイルを書いて、tokenizer を選んで、filter を設定して、mapping を更新する作業が必要です。設定の沼にハマっていきます。Postgres ではデフォルトで対応しています。でも、ここで疑問が湧きました。日本語はどうなんだろう？この本では日本語の例は出てきません。調べてみると、日本語は形態素解析が必要で、Postgres の標準機能だけでは難しいようです。pg_bigm（2-gram ベース）や pgroonga（Groonga ベース）といった拡張機能が必要になります。「Postgres で試した？」と聞き返す前に、日本語対応が必要かどうかは確認が必要な部分だと思いました。英語圏のサービスなら問題ないですが、日本語がメインなら追加の検討が必要です。tsvector と generated column の活用6.2 節では、生成した lexeme をどう保存するかが説明されています。3 つの選択肢があります。On-the-fly 生成: クエリごとに to_tsvector を実行（非効率）Column に保存: tsvector 型のカラムを追加して保存（推奨）Index のみ: テーブルには保存せず、直接インデックス作成（ストレージ節約）著者は 2 番目の方法を推奨しています。ALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (  to_tsvector(    'english', coalesce(name, '') ||    ' ' ||    coalesce(description, ''))) STORED;GENERATED ALWAYS AS ... STORED という構文が便利です。これで、name や description が変更されると、lexemes も自動的に再生成されます。ただし、configuration は明示的に指定する必要があります（'english'）。これは、generated column の式が immutable でなければならないからです。この辺りの設計判断は、実際に運用してみないと分からない部分が多そうです。全文検索クエリの実行6.3 節では、実際のクエリの書き方が紹介されています。plainto_tsquery: シンプルなクエリSELECT id, nameFROM omdb.moviesWHERE lexemes @@ plainto_tsquery('a computer animated film');plainto_tsquery は、ユーザーが入力した自然な文章を tsquery 型に変換してくれます。ストップワード（"a"）を削除し、残りの単語を lexeme に変換して、& (AND) オペレータで結合します。結果：'comput' & 'anim' & 'film'この手軽さが良いです。Elasticsearch なら、query DSL を書く必要があります。plainto_tsquery と to_tsquery の違いを実際に確認してみました。plainto_tsquery('english', 'ghost in shell') は 'ghost' & 'shell' を返します。「in」はストップワードとして除去されています。一方、to_tsquery は構文を直接指定できるので、OR 検索や NOT 検索も可能です。to_tsquery: 高度なフィルタリングSELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('computer & animated      & (lion | clownfish | donkey)');to_tsquery を使えば、AND、OR、NOT、FOLLOWED BY などのオペレータを直接指定できます。SELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('lion & !''The Lion King''');NOT オペレータで特定のフレーズの除外も可能です。この柔軟性は、Elasticsearch と変わりません。むしろ、SQL の中で完結するので、アプリケーション側のコードがシンプルになります。ランキングと重み付け6.4 節では、検索結果のランキングが扱われています。ts_rank による関連度スコアSELECT id, name, vote_average,  ts_rank(lexemes, to_tsquery('ghosts')) AS search_rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('ghosts')ORDER BY search_rank DESC, vote_average DESC NULLS LAST LIMIT 10;ts_rank 関数は、lexeme の出現頻度と位置に基づいてスコアを計算します。でも、最初の実行例では、タイトルに "ghost" が含まれる映画と、説明文にだけ含まれる映画が同じように扱われていました。setweight による重み付けALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (    setweight(to_tsvector('english', coalesce(name, '')), 'A') ||    setweight(to_tsvector('english', coalesce(description, '')), 'B')) STORED;setweight 関数で、タイトル由来の lexeme に A ラベル、説明文由来の lexeme に B ラベルを付けます。重みは A > B > C > D の順で、デフォルトは D です。これで、ts_rank はタイトルに含まれる単語をより高くランク付けするようになります。   id   |         name          | vote_average | search_rank--------+-----------------------+--------------+-------------    251 | Ghost                 | 6.3333301544 |   0.6957388 210675 | A Most Annoying Ghost |              |   0.6957388   1548 | Ghost World           | 8.1428575516 |  0.66871977タイトルへ "ghost" が含まれる映画が上位へ来るようになりました。この重み付けのメカニズムは、Elasticsearch の boosting と同じ発想です。でも、Postgres では setweight 一発で実現できます。ハイライト表示6.5 節では、ts_headline 関数が紹介されています。SELECT id, name, description,    ts_headline(description, to_tsquery('pirates')) AS fragments,    ts_rank(lexemes, to_tsquery('pirates')) AS rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('pirates:B')ORDER BY rank DESC LIMIT 1;結果はこうなります。fragments   | <b>pirate</b> Captain Jack is in a battle with the ocean ➥  itself. Jack knows it won't be easyマッチした単語を <b> タグで囲んでくれます。さらに、オプションでカスタマイズも可能です。ts_headline(description, to_tsquery('pirates'),    'MaxFragments=3, MinWords=5, MaxWords=10,     FragmentDelimiter=<ft_end>') AS fragmentsただし、著者が警告している通り、XSS 攻撃のリスクがあります。HTML マークアップを含む文書を扱う場合は、サニタイズが必要です。この辺りは、Elasticsearch でも同じ問題があります。ハイライト機能は便利ですが、セキュリティには注意が必要です。インデックスの選択：GIN vs GiST6.6 節では、全文検索を高速化するためのインデックスが説明されています。GIN インデックスCREATE INDEX idx_movie_lexemes_ginON omdb.moviesUSING GIN (lexemes);GIN（Generalized Inverted Index）は、全文検索に最適化されたインデックスです。各 lexeme ごとにインデックスエントリを作成し、その lexeme を含むテーブル行への参照を保持します。実行計画を見ると、Seq Scan（15.328 ms）から Bitmap Index Scan（0.150 ms）に変わっています。100 倍以上の高速化です。ただし、GIN インデックスは positional information を保存しません。<-> (FOLLOWED BY) オペレータを使うクエリでは、テーブル行を再確認する必要があります。この制約を解決したい場合は、RUM インデックス（Postgres 拡張）を使うと良いようです。GiST インデックスCREATE INDEX idx_movie_lexemes_gistON omdb.moviesUSING GIST (lexemes);GiST（Generalized Search Tree）は、signature tree を構築します。各文書の signature（ビット列）を作成し、lexeme の signature を bitwise OR で結合します。実行時間は 0.395 ms で、GIN（0.150 ms）より遅いです。理由は、signature collision が発生するため、マッチした文書をテーブル行で再確認する必要があるからです。でも、GiST は インデックスサイズが小さく、更新が速いという特徴があります。使い分け著者の推奨はこうです。GIN: 検索速度が最重要で、インデックスメンテナンスコストを許容できる場合GiST: インデックスサイズや更新速度が重要な場合この辺りの判断は、データ量や更新頻度によって変わります。実際に両方試してみる価値があります。Postgres の限界を認識するもちろん、Postgres の全文検索にも限界はあります。日本語の形態素解析はサポートされていない（可能性が高い）大規模データ（数億レコード）では Elasticsearch の方が速い可能性がある分散検索や複雑な aggregation は Elasticsearch の方が得意でも、多くのケースでは Postgres で十分というのがこの章の主張です。7. Postgres extensions拡張性こそが「Just use Postgres」の核心第 7 章を読んで、ようやく腑に落ちました。「Just use Postgres」というモットーは、Postgres の拡張機能によって生まれました。この一文を読んだとき、第 1 章の理解が深まりました。In fact, the motto "Just use Postgres" emerged largely due to its rich ecosystem of extensions, which allow us to use the database well beyond the use cases covered in the earlier chapters of the book.第 1 章では「新しいユースケースが発生したとき、まず Postgres で解決できるか確認しよう」という意味だと学びました。でも、なぜ Postgres で解決できるのかという根拠は曖昧でした。答えは拡張機能でした。JSON、全文検索、時系列、地理空間、メッセージキュー、ベクトル検索——これら全て、Postgres の拡張機能が可能にしています。第 2 章から第 6 章までは、コア機能を使ったユースケースでした。でも、それは「氷山の一角」だったんです。本当の多様性は拡張機能にあります。Michael Stonebraker のビジョン7.1 節で、Postgres の拡張性が生まれた背景が語られています。Michael Stonebraker（チューリング賞受賞者）の言葉が印象的でした。1980 年代、多くの研究論文が同じことを言っていました：「リレーショナルデータベースは素晴らしいと言われているが、実際には特定のシナリオでまったく機能しない」そして、それぞれの論文が独自の解決策を提案していました。Stonebraker はこう考えました：それぞれの問題へ個別の解決策を追加するのではなく、RDBMS が特定のユースケースへ適応できるようにする、より良い方法があるはずだ。この哲学が、Postgres の設計思想の根幹になっています。拡張性が Postgres の強みであることは知っていました。ただ、この章を読んで、Postgres は最初から拡張性を前提に設計されているという設計思想を改めて確認できました。インフラエンジニアとして、この設計思想は深く刺さります。運用の現場では、予期しないユースケースが次々に現れます。そのたびに新しいデータベースを追加していたら、運用負荷は青天井です。気づけば Kubernetes クラスタの中に MongoDB、Redis、Elasticsearch、TimescaleDB、Neo4j が同居しています。「あれ、俺たちデータベース動物園を運用してたっけ？」と遠い目をする羽目になります。Postgres は、そういう現実を 40 年以上前から見据えていたんです。拡張性を支える 3 つの基盤7.2 節では、Postgres の拡張性を支える技術的な基盤が説明されています。カタログ駆動操作Postgres は、テーブル、カラム、データ型、関数などのメタデータをシステムカタログに保存しています。これは通常のテーブルと似た構造で、拡張機能はこのカタログを読み書きできます。これ、地味だけど重要だと思いました。システムカタログが「普通のテーブルのような構造」だから、拡張機能が新しいデータ型や関数を追加できます。もし、メタデータが隠蔽された独自フォーマットだったら、拡張機能の開発はもっと難しかったでしょう。データベースフックPostgres のコードベースには、拡張機能がカスタムロジックを注入できるフックポイントが定義されています。クエリ計画、実行、認証など、様々なイベントにフックできます。これ、Linux カーネルの LSM（Linux Security Modules）に似ていると思いました。カーネル本体を変更せずに、セキュリティポリシーを注入できる仕組みです。Postgres も同じ哲学です。コアエンジンを変更せずに、動作を拡張できます。動的ロード拡張機能のロジックは、SQL、PL/pgSQL、C、Rust など、様々な言語で書けます。SQL や PL/pgSQL で書かれた拡張機能は、データベースエンジンが直接解釈します。C や Rust で書かれた拡張機能は、共有ライブラリとして実行時に動的にロードされます。コアエンジンの再コンパイルが不要です。これが重要です。もし、拡張機能を追加するたびに Postgres 本体を再コンパイルしなければならないとしたら、運用はほぼ不可能でした。動的ロードのおかげで、拡張機能の追加・削除が柔軟にできます。pgcrypto を使ってみた感覚7.2.2 節では、pgcrypto 拡張機能を使ったユーザー認証の例が紹介されています。CREATE EXTENSION pgcrypto;INSERT INTO accounts (username, password_hash)VALUES ('ahamilton', crypt('SuperSecret123', gen_salt('bf')));gen_salt('bf') で Blowfish アルゴリズムを使ったソルトを生成し、crypt() で平文パスワードとソルトからハッシュを生成します。この例を読んで、「データベース内で暗号化を完結させる」という選択肢があることに気づきました。これまで、パスワードのハッシュ化はアプリケーション層でやるものだと思い込んでいました。でも、pgcrypto を使えば、データベース層でも実装できます。どちらが良いかはケースバイケースでしょう。でも、選択肢があることを知っておくのは重要です。認証のクエリも興味深いです。SELECT username FROM accountsWHERE username = 'ahamilton'AND password_hash = crypt('SuperSecret123', password_hash);crypt() 関数に、平文パスワードと保存済みのハッシュを渡します。関数がハッシュからソルトを抽出し、再計算して比較します。この設計、エレガントだと思いました。ソルトを別カラムに保存する必要がありません。ハッシュ自体にソルトが含まれています。ちなみに、bcrypt のコストパラメータ（gen_salt('bf', 8) の 8 の部分）は、8〜12 が推奨されています。数字が大きいほどハッシュ計算に時間がかかりますが、セキュリティは向上します。拡張機能の 5 つのカテゴリ7.3 節では、拡張機能を 5 つのカテゴリに分類しています。"Postgres beyond relational"Postgres を従来の RDBMS を超えた用途に拡張します。pgvector、pg_ai、pgvectorscale: ベクトルデータベース（生成 AI ワークロード）TimescaleDB: 時系列データベースPostGIS: 地理空間データベースpgmq: メッセージキューpg_duckdb: 高性能分析ワークロード（DuckDB の列指向エンジンを埋め込み）これらが「Just use Postgres」を可能にしている拡張機能です。「Elasticsearch で検索やりたい」「MongoDB で JSON 保存したい」「Redis でキューやりたい」というよくある要求があります。これらに対して、「まず Postgres で試した？」と聞き返せる根拠です。過去の自分に教えてあげたいです。技術選定会議で『最新トレンド』として提案された 3 つのデータベース、実は Postgres の拡張機能で済むやつだから、と。プログラミング言語と手続き型言語第 2 章で PL/pgSQL を学びましたが、それだけではありません。PLV8: JavaScriptPL/Java: JavaPL/Python: PythonPL/Rust: Rust自分の得意な言語で、データベース関数やプロシージャを書けます。特に PLV8 の説明が興味深いです。V8 JavaScript エンジンを Postgres に埋め込むだけでなく、PgCompute クライアントライブラリと組み合わせることで実現します。アプリケーションから SQL を介さずに JavaScript 関数を直接実行できます。これ、SQL とアプリケーションロジックの境界を曖昧にする、面白いアプローチだと思いました。コネクタと外部データラッパー外部のデータソースを、あたかも Postgres のテーブルであるかのようにクエリできます。file_fdw: ファイルシステムからデータを読むpostgres_fdw、mysql_fdw、oracle_fdw、sqlite_fdw: 他の SQL データベースに接続redis_fdw、parquet_s3_fdw、kafka_fdw: Redis、S3、Kafka などの非 SQL データソースに接続Postgres を統合データレイヤーとして使えます。これ、マイクロサービスアーキテクチャで複数のデータソースを扱う場合に便利そうです。各サービスが独自のデータベースを持っていても、Postgres を経由して統一的にクエリできます。でも、パフォーマンスはどうなんでしょう。ネットワーク越しにクエリを投げるわけですから、レイテンシーは増えるはずです。この辺りは実際に試してみないとわかりません。（試した結果「遅い！」ってなって、結局専用のデータ同期パイプラインを構築するところまでがテンプレ。）クエリとパフォーマンス最適化pg_stat_statements: SQL 文の実行統計を追跡auto_explain: 遅いクエリの実行計画を自動ログhypopg: 仮想インデックスのテストauto_explain は便利そうです。普段、遅いクエリを見つけたら、手動で EXPLAIN ANALYZE を実行しています。でも、auto_explain があれば、自動的にログに記録してくれます。hypopg も面白いです。実際にインデックスを作らずに、仮想的にテストできます。本番環境で「このインデックス、効果あるかな？」と試す前に、リスクなしで検証できます。ツールとユーティリティpg_cron: cron ベースのスケジューラーPostgreSQL Anonymizer: 個人情報の匿名化pgaudit: 監査ログpg_partman: パーティション管理の簡素化pg_cron があれば、データベース内で定期タスクを実行できます。外部の cron や Airflow を使わずに。「Just use Postgres」の精神に沿っています。Postgres 互換ソリューション7.4 節では、Postgres の拡張機能ではなく、Postgres のプロトコルやソースコードを活用した別のソリューションが紹介されています。拡張機能で解決できない問題のために、こういった選択肢があります。ゼロから構築されたソリューションGoogle SpannerCockroachDBPostgres のワイヤレベルプロトコル、DML/DDL 構文、一部の機能をサポートしています。でも、内部実装は完全に別物です。分散データベースとしての可用性とスケーラビリティを提供します。Postgres ソースコードをベースにしたソリューションNeon（サーバーレスデータベース）YugabyteDB（分散データベース）Postgres のソースコードを再利用しつつ、ストレージレイヤーを変更・拡張しています。Postgres のアプリケーションをそのまま実行できます。ライブラリ、ツール、フレームワークもそのまま使えます。この 2 つのアプローチの違いは興味深いです。ゼロから構築したソリューションは、自由度が高い反面、Postgres との互換性は限定的になります。Postgres ソースコードベースのソリューションは、互換性が高い反面、アーキテクチャの変更範囲は制約されます。どちらが良いかは、ユースケース次第です。でも、どちらも「Postgres のエコシステムを活用したい」という需要から生まれています。それだけ、Postgres が広く使われているということです。8. Postgres for generative AIPostgres が Vector Database になる瞬間第 8 章を読んで最初に感じたのは、「Just use Postgres」が生成 AI の時代でも貫かれているということでした。「RAG を実装するなら Pinecone か Weaviate を使おう」——これまでそう考えていました。でも、著者が示すのは違います。Postgres can serve as a powerful vector database for implementing RAG and other gen AI use cases.既に Postgres を使っているなら、まず Postgres で試してみよう。この章はその具体的な実装方法を示しています。pgvector という選択肢pgvector という拡張を有効化するだけで、Postgres が Vector Database になります。CREATE EXTENSION vector;たったこれだけ。新しいデータベースを立てる必要がありません。（「Vector Database 導入提案書」を 3 日かけて書いた過去の自分に教えてあげたい...）vector(1024) という型が使えるようになります。1024 次元のベクトル埋め込みを格納できます。映画の説明文を mxbai-embed-large モデルで変換した埋め込みを、そのまま Postgres のカラムに保存できます。CREATE TABLE omdb.movies (    id BIGINT PRIMARY KEY,    name TEXT NOT NULL,    description TEXT NOT NULL,    movie_embedding VECTOR(1024),    ...);この手軽さ。Docker で pgvector 入りの Postgres を起動するだけで試せます。docker run --name postgres-pgvector \    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \    -p 5432:5432 \    -d pgvector/pgvector:0.8.0-pg17「Vector Database を導入しましょう」という提案をする前に、「Postgres で試した？」と聞き返せるようになりました。Cosine Distance とベクトル類似検索埋め込みを保存するだけじゃありません。類似検索もできます。SELECT id, name, descriptionFROM omdb.moviesORDER BY movie_embedding <=> omdb.get_embedding('May the force be with you')LIMIT 3;<=> は Cosine Distance を計算する演算子です。pgvector が提供しています。この SQL を実行すると、「May the force be with you」というフレーズに最も関連する映画が返ってきます。当然、Star Wars の映画がトップに来ます。埋め込みモデルが学習した「意味の空間」の中で、近い映画を見つけてくれます。でも、最初は全件スキャンになります。4,000 件程度なら許容できますが、規模が大きくなったら？そこでインデックスが必要になります。IVFFlat と HNSW——2 つのインデックス戦略pgvector は 2 種類のインデックスをサポートしています。IVFFlat: クラスタリングベースの高速化CREATE INDEX movie_embeddings_ivfflat_idxON omdb.moviesUSING ivfflat (movie_embedding vector_cosine_ops)WITH (lists = 5);IVFFlat は埋め込みをクラスタ (リスト) に分割します。k-means でセントロイドを計算し、各埋め込みを最も近いセントロイドのリストに配置します。検索時は、クエリの埋め込みに最も近いセントロイドのリストだけをスキャンします。全件スキャンを避けられます。でも、これは近似検索 (ANN: Approximate Nearest Neighbor) です。真の最近傍が他のリストにいたら、見逃す可能性があります。Recall (再現率) が完璧じゃありません。ivfflat.probes パラメータで、スキャンするリスト数を増やせます。Recall は改善しますが、検索速度は落ちます。BEGIN;SET LOCAL ivfflat.probes = 2;SELECT ...COMMIT;トレードオフです。HNSW: 階層グラフによる高精度検索CREATE INDEX movie_embeddings_hnsw_idxON omdb.moviesUSING hnsw (movie_embedding vector_cosine_ops)WITH (m = 8, ef_construction = 16);HNSW は多層グラフを構築します。上位層は疎で、下位層ほど密になります。検索は最上層から始まり、段階的に下層に降りていきます。高速かつ高精度です。著者の実験では、HNSW は IVFFlat より Recall が良いです。データが追加・更新されても Recall が安定しています。インフラエンジニアとして、この安定性は魅力的です。 データが増えても再インデックスが不要です。IVFFlat はセントロイドが固定されるため、データが大きく変化すると Recall が落ちます。映画カタログは継続的に成長します。HNSW を選ぶ理由があります。（夜中の 2 時に「Recall が落ちてます！」というアラートで起こされるのは、もう懲り懲りです）実際に試してみてわかったのは、ベクトルはランダム生成でも類似検索の動作確認は可能だということ。ジャンルごとにパターンを変えれば、「アクション映画同士が近くなる」という挙動を確認できます。本番データがなくても、仕組みの理解には十分です。RAG の実装——Postgres を中心にこの章の核心は、RAG (Retrieval-Augmented Generation) の実装です。RAG の流れです。ユーザーが質問を入力質問を埋め込みに変換 (mxbai-embed-large)Postgres でベクトル類似検索を実行検索結果をコンテキストとして LLM に渡すLLM がコンテキストを考慮して回答を生成著者は Python の Jupyter Notebook で実装を示しています。LLM には TinyLlama (640 MB、1.1 B パラメータ) を使用しています。def retrieve_context_from_postgres(question):    # 埋め込みモデルに接続    embedding_model = OllamaEmbeddings(model="mxbai-embed-large:335m")    # 質問を埋め込みに変換    embedding = embedding_model.embed_query(question)    # Postgres でベクトル類似検索    query = """    SELECT name, vote_average, budget, revenue, release_date    FROM omdb.movies    ORDER BY movie_embedding <=> %s::vector LIMIT 3    """    cursor.execute(query, (embedding, ))    # コンテキストを構築    context = ""    for row in cursor.fetchall():        context += f"Movie title: {row[0]}, Vote Average: {row[1]}, ..."    return contextPostgres から取得した映画情報を LLM に渡します。def answer_question(question, context):    llm = OllamaLLM(model="tinyllama", temperature=0.6)    prompt = f"""    You're a movie expert and your task is to answer questions about movies    based on the provided context.    This is the user's question: {question}    Consider the following context: {context}    Respond in an engaging style that inspires the user to watch the movies.    """    response = llm.invoke(prompt)    return response「海賊映画のおすすめは？」と聞くと、Postgres が Pirates of the Caribbean シリーズを返し、LLM がそれをもとに魅力的な推薦文を生成します。Postgres が RAG のコンテキスト取得レイヤーとして機能しています。LLM は statelessこの章で確認しておきたいのは、「LLM は stateless」という点です。Because LLMs are stateless—meaning they don't retain the history of the interaction—if we want the LLM to consider earlier conversation history, we need to store it separately and pass it to the prompt object.LLM は会話履歴を記憶していません。毎回、コンテキストと履歴を渡す必要があります。この設計は、Postgres のステートレス性とも通じます。Postgres はクライアントのセッション状態を保持しません (connection pooling の文脈で)。毎回のクエリは独立しています。だから、会話履歴も Postgres に保存して、RAG のコンテキストとして渡せばいいのです。全てが Postgres で完結します。確認しておきたい拡張pgai という拡張は、この章で初めて目にしました。Explore the pgai extension if you'd like to implement the RAG workflow purely in SQL and execute it entirely within the database.SQL だけで RAG を実装できます。アプリケーション側に gen AI フレームワークを導入する必要がありません。調べてみたいです。もし実用的なら、Postgres の可能性がさらに広がります。9. Postgres for time seriesTimescaleDB を改めて評価するこの章で取り上げられている TimescaleDB は、名前は知っていましたが、実際に採用を検討したことはありませんでした。時系列データベースと言えば、InfluxDB か Prometheus を中心に検討してきました。「時系列データを扱いたいなら専用のデータベースを追加しましょう」という提案をしてきたこともあります。でも、この章を読んで気づきました。Postgres の拡張機能で時系列データベースができます。「Just use Postgres」の考え方が、ここでも貫かれています。新しいデータベースを追加する前に、まず Postgres で解決できるか確認します。TimescaleDB はその選択肢の 1 つです。運用エンジニアとしては、これは大きいです。新しいデータベースを追加するたびに、バックアップ戦略、モニタリング、アラートルール、障害対応手順が増えます。チームのメンバーも新しい技術を学ばなければいけません。もし Postgres の拡張機能で解決できるなら、運用負荷は格段に減ります。この章を読み終えて、「次に時系列データの相談が来たら、TimescaleDB を試してみよう」と思いました。Postgres のパーティショニングと Hypertable9.1 節では、Postgres のテーブルパーティショニングが紹介されています。CREATE TABLE heart_rate_measurements (  watch_id INT NOT NULL,  recorded_at TIMESTAMPTZ NOT NULL,  heart_rate INT NOT NULL,  activity TEXT NOT NULL CHECK (      activity IN ('walking', 'sleeping', 'resting', 'workout'))) PARTITION BY RANGE (recorded_at);PARTITION BY RANGE (recorded_at) で、recorded_at カラムの値に基づいてテーブルを範囲でパーティション分割する。その後、各パーティションを手動で作成する必要がある。CREATE TABLE measurements_jan2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');CREATE TABLE measurements_feb2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');このパーティショニング自体は Postgres の標準機能です。時系列データの場合、直近のデータだけが頻繁にアクセスされて、古いデータは圧縮したり削除したりします。パーティショニングを使えば、それが簡単にできます。でも、パーティションの作成と管理は手動でやる必要があります。著者も書いていますが、pg_partman と pg_cron という拡張機能を使えば自動化できます。そして、9.2 節で登場するのが TimescaleDB です。SELECT create_hypertable(  relation => 'watch.heart_rate_measurements',  dimension => by_range('recorded_at', interval '1 month'),  create_default_indexes => false);この一文で、テーブルが Hypertable に変換されます。Hypertable は Postgres の通常のテーブルですが、TimescaleDB が自動的にパーティション（chunk と呼ばれる）を作成・管理してくれます。新しいデータが挿入されると、TimescaleDB が自動的に新しい chunk を作ります。INSERT INTO watch.heart_rate_measurements VALUES(1,'2025-12-08 00:25:00',57,'sleeping');この INSERT だけで、_timescaledb_internal._hyper_1_13_chunk という新しいパーティションが自動生成されます。手動でパーティションを作る必要がありません。これは大きいです。timescaledb_information.chunks でチャンクのメタデータを確認できます。実際に確認してみると、日付ごとにチャンクが自動生成されていることがわかります。_hyper_1_1_chunk | 2025-01-01 - 2025-01-02_hyper_1_2_chunk | 2025-01-02 - 2025-01-03_hyper_1_3_chunk | 2025-01-03 - 2025-01-04この透過性が TimescaleDB の魅力です。過去のプロジェクトで、パーティショニングを手動で管理していたことがあります。月次バッチで次月のパーティションを作成するスクリプトを cron で回していました。でも、そのスクリプトが失敗したことに気づかず、翌月の INSERT が全部エラーになりました。月初の朝、Slack が火を噴きました。「データが入らない！」というメッセージが次々と流れてくる。あの日の朝のコーヒーは、確実に苦かったです。TimescaleDB を使っていれば、そんなことは起きませんでした。というか、あの朝のコーヒーはもっと美味しかったはずです。データ保持ポリシーの自動化9.4 節では、データ保持ポリシー（retention policy）の話が出てきます。SELECT add_retention_policy(  'watch.heart_rate_measurements', INTERVAL '30 days');これだけで、30 日以上古いデータを自動的に削除するジョブが設定されます。運用の観点から、これは非常にありがたいです。時系列データは増え続けます。ディスク容量は有限です。古いデータを定期的に削除する必要があります。過去のプロジェクトでは、手動で SQL を書いて、古いパーティションを DROP していました。でも、これも失敗することがあります。削除スクリプトのバグで、間違ったパーティションを削除してしまったこともありました。具体的に言うと、measurements_jan2025 を消すはずが measurements_jan2024 を消しました。そう、1 年分のデータが吹っ飛びました。バックアップから復旧しましたが、あの日の胃痛は今でも忘れられません。エンジニアのキャリアにおいて、誰もが一度は通る「DELETE/DROP の洗礼」というやつです。TimescaleDB の retention policy を使えば、そのリスクが減ります。胃痛も減ります。ただし、著者も警告していますが、このコマンドは慎重に使う必要があります。間違った設定をすると、重要なデータを失う可能性があります。time_bucket 関数の威力9.5 節では、TimescaleDB の time_bucket 関数が紹介されています。SELECT  time_bucket('10 minutes', recorded_at) AS period, activity,  AVG(heart_rate)::int AS avg_rate, MAX (heart_rate)::int AS max_rateFROM watch.heart_rate_measurementsWHERE watch_id = 1 AND activity = 'workout'  AND recorded_at >= '2025-04-23' AND recorded_at < '2025-04-24'GROUP BY period, activity ORDER BY period;これで、10 分ごとのバケットに心拍数を集約できます。普通の SQL でやろうとすると、DATE_TRUNC や複雑な計算が必要になります。でも、time_bucket を使えば、読みやすいクエリで簡単に集約できます。さらに、time_bucket はタイムゾーンの指定もできます。SELECT time_bucket('1 week', recorded_at, 'Asia/Tokyo',  '2025-04-01'::timestamptz) AS period, activity,  AVG(heart_rate)::int AS avg_rate,  MAX (heart_rate)::int AS max_rate, MIN (heart_rate)::int AS min_rateFROM watch.heart_rate_measurementsWHERE watch_id = 2 AND recorded_at >= '2025-04-01'AND  recorded_at < '2025-04-15'GROUP BY period, activity ORDER BY period, activity;ユーザーごとに異なるタイムゾーンでデータを集約できます。これはグローバルなサービスでは必須の機能です。そして、time_bucket_gapfill 関数です。SELECT watch_id, time_bucket_gapfill('1 minute', recorded_at) AS minute,  LOCF(AVG(heart_rate)::int) AS avg_rateFROM watch.heart_rate_measurementsWHERE watch_id=1 AND recorded_at BETWEEN '2025-03-02 07:25'  AND '2025-03-02 07:36'GROUP BY watch_id, minute ORDER BY minute;データが欠けている時間帯も含めて、連続した時間バケットを作成してくれます。さらに、LOCF（Last Observation Carried Forward）関数を使えば、欠損値を最後の値で埋めることができます。過去に、時系列データのグラフを作ったことがあります。データに欠損があると、グラフが途切れてしまいます。アプリ側で欠損値を補間する処理を書きましたが、複雑でした。time_bucket_gapfill と LOCF を使えば、データベース側で簡単に処理できます。Continuous Aggregates という機能9.6 節では、Continuous Aggregates（継続的集約）が紹介されています。CREATE MATERIALIZED VIEW watch.low_heart_rate_count_per_5minWITH (timescaledb.continuous) ASSELECT  watch_id,  time_bucket('5 minutes', recorded_at) AS bucket,  MIN(heart_rate) as min_rate,  COUNT(*) FILTER (WHERE heart_rate < 50) AS low_rate_count,  COUNT(*) AS total_measurementsFROM watch.heart_rate_measurementsGROUP BY watch_id, bucket;これは Postgres の Materialized View（マテリアライズドビュー）ですが、TimescaleDB が自動的にリフレッシュしてくれます。リフレッシュポリシーも設定できます。SELECT add_continuous_aggregate_policy  ('watch.low_heart_rate_count_per_5min',  start_offset => INTERVAL '15 minutes',  end_offset => INTERVAL '1 minute',  schedule_interval => INTERVAL '1 minute');これで、1 分ごとに集約結果が更新されます。普通の Materialized View は、手動で REFRESH MATERIALIZED VIEW を実行しないと更新されません。でも、TimescaleDB の Continuous Aggregates は自動的に更新されます。しかも、Hypertable に保存されるので、パーティショニングの恩恵も受けられます。この章の例では、心拍数が 50 BPM 以下の回数をカウントして、徐脈（bradycardia）の兆候を検出しています。リアルタイムで集約結果を更新して、ユーザーにアラートを送ります。これ、単なるデモではありません。実用的です。過去に、IoT デバイスからのデータを集約して、異常を検知するシステムを運用したことがあります。集約処理は別のバッチジョブで定期的に実行していました。でも、リアルタイム性が求められると、バッチでは間に合いません。TimescaleDB の Continuous Aggregates を使えば、リアルタイムに近い形で集約結果を更新できます。B-tree インデックスと BRIN インデックス9.7 節では、時系列データのインデックス戦略が紹介されています。まず、B-tree インデックスです。CREATE INDEX heart_rate_btree_idxON watch.heart_rate_measurements (recorded_at, watch_id);複合インデックスで、recorded_at と watch_id の両方を含めます。これで、時間範囲とデバイス ID の両方で絞り込むクエリが高速化されます。著者の説明によれば、B-tree インデックスは実際のカラム値とテーブル行へのポインタを保存します。だから、特定の行に直接アクセスできます。でも、B-tree インデックスはサイズが大きいです。この章の例では、パーティションごとに数 MB のサイズになっています。そこで登場するのが BRIN（Block Range Index）です。CREATE INDEX heart_rate_brin_idxON watch.heart_rate_measurementsUSING brin (recorded_at);BRIN インデックスは、ページ範囲ごとの最小値と最大値だけを保存します。だから、サイズが非常に小さいです。この章の例では、24 KB しかありません。B-tree の 100 分の 1 です。でも、BRIN はページ全体をスキャンする必要がある場合があります。だから、少量のデータを取得するクエリでは B-tree の方が速いです。著者の説明を読んで、BRIN の仕組みがよくわかりました。時系列データのように、カラム値が物理的な配置と強く相関している場合に BRIN は有効です。心拍数測定データは常に追記されます。新しい測定は常に大きな recorded_at 値を持ちます。だから、ページ内のデータは時系列順に並びます。BRIN はこの特性を活かします。過去に、ログテーブルにインデックスを作ったことがあります。そのテーブルは append-only で、タイムスタンプカラムがありました。B-tree インデックスを作りましたが、サイズが大きくなって困りました。「なんでインデックスがテーブルより大きいんだ？」と首を傾げながら、ディスク容量を確保するために古いインデックスを削除する日々でした。当時は BRIN を検討していませんでした。Postgres のドキュメントで存在は知っていたはずですが、実際に使う場面を意識していませんでした。必要に迫られないと、知識は実践に結びつかないものです。10. Postgres for geospatial data「地理空間データ」の意外な身近さこの章を読んで認識したのは、地理空間データベースの機能が、自分の仕事に意外と近いということです。PostGIS の名前は知っていました。でも、「地理空間データベース」という言葉から受ける印象は、「GIS 専門家のための特殊な技術」でした。Google Maps みたいなサービスを作る時に使うやつ、くらいの認識。要するに、「自分には関係ない」と決めつけていたわけです。実際には、もっと身近なユースケースがあります。著者が冒頭で説明する Geofabrik（OpenStreetMap のデータ抽出サービス）、osm2pgsql（OSM データのインポートツール）、QGIS（データ可視化ツール）。これらのツールと PostGIS の組み合わせで、10 分以内にフロリダ州全体の地理データをローカル環境で扱える状態にできます。この手軽さが、「Just use Postgres」の真髄だと感じました。geometry と geography — 2つのデータ型の意味10.1.2 節で説明される geometry と geography の違いに、初めて向き合いました。geometry 型（Web Mercator projection、SRID 3857）。- 平面（Euclidean plane）として計算- 単位はメートル- 計算が速い- 距離が長いと精度が落ちるgeography 型（WGS 84、SRID 4326）。- 球面（spherical model）として計算- 単位は度（longitude/latitude）だが、計算結果はメートル- 計算が遅い- 地球の曲率を考慮するため正確「なるほど、速度と精度のトレードオフか」と思いました。でも、本当に理解したのは、用途によって使い分ける必要があるということでした。ローカルな範囲（例：Tampa 市内のレストラン検索）なら geometry で十分です。でも、大陸をまたぐような距離の計算なら geography が必要になります。注意点として、ST_Distance に geometry 型を渡すと単位は「度」になります。geography 型を渡すと「メートル」です。最初、この違いを知らずに「距離が 0.003 って何？」と混乱しました。それ、度でした。著者は本章で主に geometry を使っています。理由は明示されていませんが、フロリダ州内のデータを扱っているからでしょう。ST_DWithin と ST_Distance — index の有無で 500 倍の差10.6.2 節の実行計画の比較に目を奪われました。ST_DWithin を使った場合（Listing 10.26）:- 実行時間: 1.125 ms- GiST index を使用（Bitmap Index Scan）- 1,205 件を候補として抽出し、36 件にフィルタリングST_Distance を使った場合（Listing 10.27）:- 実行時間: 488.119 ms- GiST index を使用せず、フルテーブルスキャン（Parallel Seq Scan）- 18,676 件を候補として抽出し、36 件にフィルタリング同じ結果（36 件のレストラン）を得るのに、434 倍の時間がかかっています。なぜこんなに違うのでしょうか。片や 1 ミリ秒でサクッと答え、片や半秒近く考え込んでいます。まるで、道を聞かれて地図アプリを開く人と、記憶を辿って一生懸命思い出そうとする人くらい違います。ST_DWithin is one of the index-aware functions that can use the GiST index by performing an initial fast filtering of the data using the combination of the bounding box operator && and the ST_Expand function.著者の説明によると、ST_DWithin は内部で bounding box（境界ボックス）を使った高速フィルタリングをします。GiST index がこの bounding box 検索に対応しています。一方、ST_Distance は常に正確な距離を計算します。bounding box を使わないから、index を利用できません。この違いを知らなかったら、「ST_Distance(point1, point2) <= 500 で 500m 以内を検索」と書いてしまっていたでしょう。数百万件のデータに対してフルスキャンが走ります。「index-aware functions」という概念を、初めて意識しました。GiST の構造 — R-tree で理解できた10.6.1 節の GiST index の説明は、初めて「わかった」感覚がありました。以前、B-tree index については理解していました。でも、GiST（Generalized Search Tree）は「汎用的な index」という説明しか見たことがなく、具体的なイメージが湧きませんでした。著者の図解（Figure 10.6, 10.7, 10.8）がわかりやすかったです。フロリダ州全体を 5 つの大きな矩形（R1〜R5）に分割それぞれの矩形をさらに小さな矩形に分割（R6〜R25）最小の矩形が、実際のテーブル行（points）を指す検索の流れ。1. Downtown Miami の座標が、どの大きな矩形に含まれるかをチェック → R52. R5 の中で、どの小さな矩形に含まれるかをチェック → R243. R24 の中の全 points をスキャン → 該当するものだけ返すR-tree（Rectangular tree）という名前の由来も理解できました。矩形（Rectangle）で空間を階層的に分割していく木構造です。実際に座標変換も試してみました。Walt Disney World の座標を WGS 84 から Web Mercator へ変換すると、経度 -81.5639 が X -9079651.82 に変わります。緯度 28.3852 は Y 3297626.07 になります。単位がメートルに変わるのがわかります。この構造、実は Chapter 6 の全文検索で出てきた GiST index と同じ基盤です。あの時は tsvector 型の lexemes を indexing していました。今回は geometry 型の bounding boxes を indexing しています。GiST は、データ型ごとに異なる index 構造を実装できる汎用フレームワークなんだと、やっと腹落ちしました。QGIS で可視化 — 「見える」ことの重要性10.4 節の QGIS による可視化は、実際に手を動かしました。SELECT name, ST_AsText(way) AS coordinatesFROM florida.planet_osm_pointWHERE name = 'Tampa' and place = 'city';このクエリで得た Tampa の座標を、QGIS で表示した時、「あ、本当に Tampa の中心だ」と思いました。データベースに入っている座標が、実際の地図上の位置と一致します。当たり前のことですが、自分の目で確認するまで信じられませんでした。planet_osm_polygon テーブルの 6.8 100 万の polygons を QGIS で読み込むと、フロリダ州の地図が少しずつレンダリングされていきます。湖、道路、建物、公園。すべてが Postgres のテーブルに格納されています。「データが見える」ことの重要性を、改めて実感しました。osm2pgsql — データインポートの簡単さ10.3 節で紹介されている osm2pgsql ツールは実用的です。docker run --name osm2pgsql --network="host" \  -e PGPASSWORD=password \  -v osm2pgsql-volume:/data \  iboates/osm2pgsql:2.1.1 \  -H 127.0.0.1 -P 5432 -d postgres -U postgres --schema florida \  http://d3e4uq6jj8ld3m.cloudfront.net/florida-250501.osm.pbfこのコマンド 1 つで、フロリダ州全体の OSM データ（2025 年 5 月 1 日時点）を Postgres にインポートできます。所要時間は約 10 分。自分の環境（M1 Mac）では 7 分ほどでした。インポート後、以下のテーブルが自動生成されます。planet_osm_point — 単一座標で表現できるもの（レストラン、ホテルなど）planet_osm_line — 線分（道路、川など）planet_osm_polygon — 閉じた領域（建物、公園、湖など）planet_osm_roads — planet_osm_line のサブセット（ズームレベルが低い時のレンダリング用）それぞれのテーブルに、既に GiST index が作成されています（planet_osm_point_way_idx など）。この「すぐに使える」感覚が、PostGIS の魅力だと感じました。ST_Within と ST_Intersects — 空間関係の判定10.5.2 節と 10.5.3 節で紹介される ST_Within と ST_Intersects の違いが、最初は曖昧でした。ST_Within(A, B)。- A が B の中で完全に含まれている場合は true- A の全ての点が、B の内部にある- 例：あるアトラクションが、Disney's Hollywood Studios の中にあるかST_Intersects(A, B)。- A と B が少なくとも 1 点を共有する場合は true- 完全に含まれていなくてもいい、交差していれば OK- 例：ある道路が、Miami の境界を横切っているかListing 10.22 のクエリで理解できました。SELECT l.name, l.highway, ST_Length(l.way) AS len_metersFROM florida.planet_osm_line lJOIN miami m ON ST_Intersects(l.way, m.boundaries)WHERE l.highway IN ('primary', 'secondary')このクエリは、Miami の境界内にある道路だけでなく、境界を横切る道路も取得します。ST_Within を使っていたら、境界を横切る道路は取得できません。この違いを知らないと、「なぜこの道路が結果に含まれるのか」と混乱したでしょう。「Just use Postgres」の再確認この章を読んで、改めて「Just use Postgres」の意味を理解しました。次に「位置情報を扱うから、MongoDB（GeoJSON 対応）を追加しよう」と言われた時、私は聞き返せます。「Postgres で試した？PostGIS なら、既存のインフラでできるかもしれない」新しいデータベースを追加する前に、まず既存の Postgres で何ができるかを確認します。これがこの本の一貫したメッセージです。そして、大抵の場合、Postgres でできてしまいます。追加のインフラを管理する手間（と、深夜の障害対応）が減るのは、エンジニアとしても組織やチームとしてもありがたいです。地理データだって、Postgres でできます。それも、思ったより簡単に。11. Postgres as a message queueメッセージキューとして Postgres を使う、という選択この章で参考になったのは、「Postgres をメッセージキューとして使う判断基準」が明確に示されていた点です。正直に言うと、読む前は「Postgres でメッセージキュー？　無理がある」と思っていました。10 年近くソフトウェアエンジニアをやっている中で、メッセージキューといえば RabbitMQ、Kafka、AWS SQS が標準でした。Postgres はあくまでリレーショナルデータベース。「餅は餅屋」という言葉が頭に浮かびました。というか、新しいツールを導入する言い訳が欲しかっただけかもしれません（インフラエンジニアの悪い癖です）。でも、この章を読み終えて気づきました。「Just use Postgres」の本質は、万能性じゃなくて、既存資産の最大活用でした。Postgres をメッセージキューとして使う 3 つの基準11.1 節で、著者は 3 つの基準を挙げています。1. トランザクショナルな一貫性が必要な場合DMV（運転免許センター）の例が分かりやすかったです。来訪者がチェックインする（ビジネスロジック）と同時に、待機キューにメッセージを追加する（イベント記録）。この 2 つの操作がアトミックに実行される必要があります。もし別々のシステム（Postgres + 専用メッセージキュー）だったら、チェックインは成功したのにメッセージ送信が失敗する可能性があります。その時、アプリケーション側で整合性を保証しなければなりません。If you want the check-in operation and the message added to the visitors queue to be executed atomically (as a single transaction), then use Postgres.この一文は重いです。私が関わったプロジェクトで、「決済処理」と「メール送信キュー」が別々のシステムだったせいで、決済完了したのに確認メールが届かないトラブルがありました。結局、リトライ機構を複雑に実装して解決しましたが、あれは Postgres で統一できていれば避けられたかもしれません。深夜 3 時に「メールキューが詰まった」アラートで起こされることもなかったでしょう（遠い目）。2. メッセージ量が Postgres で処理可能な場合著者は正直です。If the effort is too high or the configuration becomes overly complex, consider using a specialized message queue instead.Postgres の書き込みスケールには限界があります。シングルプライマリインスタンスだから、書き込み負荷が高すぎる場合はシャーディングや分散 Postgres（CitusData、YugabyteDB）が必要になります。でも、DMV の例では「メッセージ量は比較的低い」と明言しています。この「正直さ」がいいです。Postgres は万能じゃない、でも適切なユースケースならシンプルで強力です。3. 既に Postgres を使っている場合If your application already uses Postgres and now needs to support a message queue use case, consider using Postgres first before bringing in a specialized solution.これが「Just use Postgres」の核心です。新しいシステムを追加するコストは、技術的負債だけじゃありません。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を運用しているなら、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。カスタムメッセージキューの実装11.2 節と 11.3 節では、カスタムメッセージキューを実装しています。シンプルな設計CREATE TABLE mq.queue (    id BIGSERIAL PRIMARY KEY,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new');この設計、シンプルだけど実用的です。id: 自動採番（BIGSERIAL）で一意性を保証message: JSON 型でペイロードを格納（柔軟性重視）created_at: FIFO 順序の保証status: メッセージのライフサイクル管理（new → processing → completed）著者が JSON 型を選んだ理由が面白いです。The JSONB type would preprocess messages before storing them, which might slow down ingestion and alter the original structure—for example, by reordering object keys.JSONB はクエリ効率のために前処理を行いますが、メッセージキューでは「プロデューサーからコンシューマーへそのまま渡す」だけだから、JSON 型で十分です。この「ユースケースに応じた選択」が、エンジニアリングの本質だと感じました。FOR UPDATE SKIP LOCKED の威力mq.dequeue 関数の実装で、FOR UPDATE SKIP LOCKED が使われています。SELECT id FROM mq.queueWHERE status = 'new' ORDER BY created_atFOR UPDATE SKIP LOCKEDLIMIT messages_cntこの構文は、改めて確認すると有用です。FOR UPDATE は行レベルロックをかけます。通常なら、他のトランザクションがロックされた行にアクセスしようとするとブロックされて待機します。でも SKIP LOCKED を加えると、ロックされている行をスキップして、次の利用可能な行を取得します。複数のコンシューマーが並行してメッセージを取得しても、お互いをブロックせずに並列処理できます。This allows consumers to process new messages in parallel without blocking each other, improving overall throughput.これは Postgres のメッセージキュー実装におけるキラー機能です。実際に 2 つのワーカーを同時に動かして確認しました。Worker 1 がメッセージ 1, 2 を取得している間、Worker 2 はブロックされずにメッセージ 3, 4 を取得できます。お互いが異なるメッセージを処理する。これが SKIP LOCKED の威力です。以前、複数ワーカーでジョブキューを処理する実装を Rust で書いた時、排他制御で悩んだことがあります。あの時、FOR UPDATE SKIP LOCKED を知っていれば、もっとシンプルに実装できたかもしれません。LISTEN と NOTIFY11.4 節の LISTEN / NOTIFY は、Postgres の隠れた名機能だと感じました。DMV のシナリオでは、来訪者がチェックインすると、待機中の職員にリアルタイムで通知が届きます。-- 職員側（リスナー）LISTEN queue_new_message;-- ターミナル側（ノティファイア）SELECT mq.enqueue('{"service": "car_registration", "visitor": "Marta Jones"}');-- → pg_notify('queue_new_message', 'new_message')これで、ポーリング不要の非同期通知が実現できます。ただし、2 つの制限があります。過去の通知は受け取れない: 接続後に発行された通知のみ受信可能レプリカでは使えない: プライマリノードへの接続が必要特に 2 つ目は運用上重要です。読み取り負荷をレプリカに逃がしている構成でも、LISTEN/NOTIFY 専用にプライマリへの接続を維持する必要があります。でも、この制限を理解した上で使えば、非常に強力な機能です。あと、pg_notify はトランザクション終了時に送信されます。途中でロールバックすると通知も送られません。これは整合性の観点から正しい動作ですが、最初は「なぜ通知が来ない？」と悩みました。実装上の考慮事項11.5 節では、いくつかの重要な考慮事項が述べられています。インデックス戦略mq.dequeue 関数は、デフォルトではフルテーブルスキャンを行います。created_at と status にインデックスがないからです。著者は 2 つのオプションを提示しています。オプション 1: created_at のみのインデックス。CREATE INDEX mq_created_at_index_btree ON mq.queue (created_at);オプション 2: パーシャルインデックス（推奨）CREATE INDEX mq_partial_index_btreeON mq.queue (created_at, status)WHERE status = 'new';パーシャルインデックスは、status = 'new' の行だけをインデックスに含めます。これで、インデックスサイズが小さくなり、new メッセージへのアクセスがさらに高速化されます。この「状況に応じた最適化」の姿勢が参考になります。DMV のユースケースでは不要かもしれませんが、高頻度メッセージングなら必須です。パーティショニング11.5.3 節のパーティショニングの話は、時系列データの章（第 9 章）とつながりました。メッセージキューも時系列データの一種です。created_at でレンジパーティショニングすれば、古いメッセージを効率的にアーカイブ・削除できます。CREATE TABLE mq.queue (    id BIGSERIAL,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new',    PRIMARY KEY (id, created_at)) PARTITION BY RANGE (created_at);パーティションごとにメッセージを管理できるから、古いパーティションを削除（DROP TABLE）するだけで大量の古いメッセージを一瞬で消せます。VACUUM の負荷も軽減されます。なぜなら、新しいパーティションだけが頻繁に更新されるからです。この設計パターンは、ログ管理やイベントストアにも応用できそうです。フェイルオーバー機構11.5.4 節で、メッセージ処理の失敗対策が述べられています。コンシューマーがメッセージを取得（status = 'processing'）した後にクラッシュすると、そのメッセージは processing 状態のまま放置されます。著者の提案は、pg_cron を使った定期的なリセットです。a periodic job in the database to check for messages stuck in the processing state and reset their status to new.これは実用的です。ただし、同じメッセージが複数回処理される可能性があるから、コンシューマー側で冪等性を保証する必要があります。pgmq 拡張11.6 節と 11.7 節では、pgmq 拡張が紹介されています。pgmq は「Postgres Message Queue」の略で、AWS SQS 互換の API を提供します。カスタム実装で学んだ原理を、pgmq が抽象化してくれます。可視性タイムアウトpgmq.read 関数の vt（visibility timeout）が面白いです。SELECT msg_id, message, enqueued_atFROM pgmq.read(  queue_name => 'visitors_queue',  vt         => 120,  -- 2分間の可視性タイムアウト  qty        => 1);メッセージを取得してから 120 秒間、そのメッセージは他のコンシューマーから見えなくなります。でも、120 秒以内に pgmq.archive を呼ばないと、メッセージは再びキューに戻ります。これで、コンシューマー失敗時の自動リトライが実現できます。DMV の例では、職員が来訪者を呼び出してから 2 分以内に現れなければ、別の来訪者を呼び出せる仕組みに使われています。この「タイムアウトベースのフェイルオーバー」は、AWS SQS と同じ設計パターンです。アーカイブテーブルpgmq.archive 関数は、メッセージを pgmq.q_visitors_queue から pgmq.a_visitors_queue に移動します。削除（DELETE）ではなくアーカイブ（移動）だから、処理済みメッセージの監査ログを保持できます。これは本番運用で重要です。「このメッセージ、本当に処理されたのか？」を後から確認できます。本全体を読み終えて第 11 章は、この本の最終章です。第 1 章「Meeting Postgres」から始まり、JSON、地理空間、全文検索、時系列、ベクトル検索、グラフ、そしてメッセージキュー。「Just use Postgres」の本質は、Postgres の万能性を主張することじゃありませんでした。既に Postgres を使っているチームが、新しいユースケースに直面した時、別のデータベースを追加する前に、まず Postgres で解決できるか試してみよう、というメッセージです。それは、アーキテクチャをシンプルに保つための選択であり、運用コストを抑えるための選択であり、チームの認知負荷を減らすための選択です。10 年近くソフトウェアエンジニアをやってきて、システムが複雑化する様子を何度も見てきました。「全文検索だから Elasticsearch」「時系列データだから InfluxDB」「メッセージキューだから RabbitMQ」確かに、それぞれの専用ソリューションは強力です。でも、それぞれが運用コストを生みます。バックアップ、モニタリング、アラート、障害対応、バージョンアップ。全てがチームの負担になります。そして、構成図に新しいアイコンが増えるたびに、誰かが「これ誰がメンテするんですか？」と聞く声が聞こえます。「Just use Postgres」は、その複雑化への抵抗です。もちろん、これは「新しい技術を学ぶな」という意味ではありません。新しいツールやサービスが出てきたとき、まず「運用時にどうなるか」を考える。それがベテランエンジニアに求められる姿勢だと思います。機能の魅力だけでなく、3 年後にメンテナンスできる人がいるか、障害時に対応できるか、既存システムとの整合性はどうか。これは Postgres の新機能についても同じです。pgvector は便利ですが、まだ運用実績が浅い。TimescaleDB も Postgres の拡張とはいえ、独自のアップグレードパスがあります。「Postgres だから安心」ではなく、その機能の成熟度を見極める必要があります。結局のところ、謙虚に学び続けるしかありません。新しい技術も、既存の技術も。私が最近考えている技術選定の基準があります。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、40 年の実績がある標準的な選択肢を使い、その可能性を最大限に活かす。それが、この本から学んだことです。もちろん、Postgres で解決できないユースケースもあります。著者は正直にそれを認めています。でも、試す前から諦めるのではなく、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。この本を読み終えて、次に「〇〇が必要だから△△を導入しましょう」と言われた時、私は自信を持って聞き返せるようになりました。「Postgres で試しましたか？」おわりに読むことと、手を動かすこと11 章を読み終えて、私は 1 つの疑問を持っていました。「本当に、Postgres でこれだけのことができるのか？」本に書いてあることを読んで「なるほど」と思うのと、実際に動かして確認するのは、全く別の体験です。少なくとも、私にとっては。だから、手を動かすことにしました。Docker で Postgres を立てて、Rust でコードを書いて、各章の内容を 1 つずつ検証しました。generate_series から始まって、CTE、Window Functions、Recursive Query と進みました。JSONB、全文検索、pgcrypto、pgvector、TimescaleDB、PostGIS、そしてメッセージキュー。全 11 章です。その過程で、いくつかのことに気づきました。手を動かして初めてわかったこと本を読んでいるときは「ふーん」と思っていたことが、実際に動かすと「あ、そういうことか」に変わる瞬間があります。例えば、FOR UPDATE SKIP LOCKED。本には「複数のコンシューマーが並行してメッセージを取得できる」と書いてありました。でも、実際に 2 つのワーカーを同時に動かして、それぞれが異なるメッセージを取得するのを見たとき、初めて腑に落ちました。Worker 1 がメッセージ 1 を取得: {"service":"registration","visitor":"Alice"}Worker 2 がメッセージ 3 を取得: {"service":"registration","visitor":"Charlie"}この出力を見て、「ああ、本当にブロックせずにスキップしてるんだ」と思いました。言葉で理解することと、目で見て理解することは、違うものです。他にも気づきはありました。pg_typeof() の結果を Rust で取得しようとしたらエラーになって、::TEXT でキャストする必要があることを知りました。PL/pgSQL の変数名がテーブルのカラム名と衝突してエラーになることも知りました。TEMP TABLE の名前が別のデモと衝突して、「なんでエラーになるんだ？」と 30 分悩んだこともあります。これらは本には書いてありません。当たり前です。本は概念を説明するものであって、私が遭遇するエラーを予測するものではないから。でも、そういうエラーと向き合う時間こそが、理解を深める時間だったと思います。判断基準が見えてきた11 章を読み終えて、そして検証を終えて、私の中に 1 つの判断基準ができました。「いつ Postgres で十分で、いつ専用ツールを検討すべきか」全文検索なら、数百万件以下のシンプルな検索であれば Postgres で十分です。ただし数億件規模や日本語の形態素解析、複雑なファセット検索が必要なら、Elasticsearch を検討すべきです。ベクトル検索なら、pgvector で数百万ベクトルまでは対応できます。でも、数億ベクトル規模やリアルタイム更新が必要なら、Pinecone や Milvus の出番です。メッセージキューなら、秒間数百メッセージ程度なら Postgres で十分です。でも、秒間数万メッセージや複雑なルーティングが必要なら、RabbitMQ や Kafka を使うべきです。この判断基準は、本を読んだだけでは身につかなかったと思います。実際に動かして、限界を感じて、初めてわかることがありました。「Postgres で試した？」この本を読み始める前、私はこの言葉を言えませんでした。「全文検索が必要です」と言われたら、「Elasticsearch ですね」と即答していました。「時系列データを扱いたい」と言われたら、「InfluxDB か TimescaleDB ですね」と答えていました。TimescaleDB が Postgres の拡張であることすら、あまり意識していませんでした。今は違います。「全文検索が必要です」と言われたら、「どのくらいのデータ量ですか？　検索の要件は？　まず Postgres の tsvector で試してみませんか？」と聞き返せます。「ベクトル検索がしたい」と言われたら、「pgvector で試してみましょうか。数百万ベクトルくらいなら対応できますよ」と提案できます。それが良いことなのかどうか、正直わかりません。もしかしたら、早めに専用ツールを導入した方が、長期的には幸せだったかもしれません。Postgres で頑張った結果、パフォーマンスの壁にぶつかって、結局移行することになるかもしれません。でも、少なくとも「試した上で判断する」ことはできるようになりました。「Postgres で試した？」その一言を、自信を持って言えるようになりました。そして、自分自身にも問いかけるようになりました。新しいデータベースを追加する前に、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。運用負荷も増えません。深夜 3 時のアラート対応の可能性も、1 つ減ります。それだけで、この本を読んだ価値はあったと思います。最後に11 章分の感想を書いて、検証コードを書いて、そしてこの「おわりに」を書いています。読み始めたときは、「Postgres の可能性を広げる本」だと思っていました。読み終えた今は、「技術選定の視点を変える本」だったと思っています。「最適なツールを選ぶ」という言葉は、聞こえが良いです。でも、その「最適」は何を基準にしているのでしょうか。機能の豊富さ？　パフォーマンス？　それとも、運用の複雑さ？この本は、「十数年単位の運用の複雑さ」という視点を私に与えてくれました。新しいデータベースを追加することは、コストです。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を使っているなら、まず Postgres で試してみる。それで十分なら、そのコストを払わなくて済みます。それが「Just use Postgres」の本当の意味だと、今は思っています。「できる」と「やるべき」の違いこの本を読んで、1 つ注意しなければならないことがあります。「Postgres でできる」と「Postgres でやるべき」は、違います。本書は Postgres の可能性を示してくれますが、すべてのユースケースで Postgres を選ぶべきだとは言っていません。著者自身も、専用ツールが必要な場面があることを認めています。大事なのは、選択肢を知った上で判断することです。「Postgres でもできるけど、このユースケースでは Kafka の方が適している」と判断するのと、「Postgres でできることを知らずに Kafka を選ぶ」のでは、意味が違います。前者は informed decision、後者は思い込みです。この本は、その informed decision をするための知識を与えてくれました。チームと知識の継承もう 1 つ、この本を読んで考えたことがあります。技術選定は、個人の問題ではありません。チームの問題です。新しいデータベースを導入するということは、チームメンバー全員がそれを学ぶ必要があるということです。障害対応できる人が増えなければ、特定の人に負荷が集中します。その人が退職したら、知識が失われます。Postgres を選ぶということは、チームの認知負荷を抑えるという選択でもあります。多くのエンジニアが Postgres の基本を知っています。採用市場でも、Postgres 経験者を見つけるのは比較的容易です。ドキュメントも豊富で、コミュニティも活発です。「技術的に最適」と「チームにとって最適」は、必ずしも一致しません。十数年単位で考えたとき、チームの持続可能性も重要な判断基準です。謙虚に学び続けることこの本を読んで、もう 1 つ気づいたことがあります。10 年近くこの仕事をしていても、知らないことはたくさんあります。Recursive CTE の活用パターン、BRIN インデックスの使い所、FOR UPDATE SKIP LOCKED の仕組み。どれも Postgres に昔からある機能ですが、実務で使う機会がなければ、深く理解することはありませんでした。新しい技術が出てきたとき、いきなり飛びつくのは危険です。でも、既存の技術の可能性を見落としているのも、同じくらい問題です。これは Postgres の新機能についても同じです。pgvector や TimescaleDB は便利ですが、Postgres 本体と比べれば運用実績は浅い。「Postgres を使う」という判断と、「Postgres の新機能を本番投入する」という判断は、別々に評価する必要があります。結局のところ、謙虚に学び続けるしかありません。はじめにでも書きましたが、私は技術選定についてこう考えています。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、流行りの新しいデータベースに飛びつく前に、まず Postgres で何ができるかを確認する。それが、この本から学んだ姿勢です。もちろん、Postgres で全てが解決できるわけではありません。本当に専用ツールが必要な場面もあります。大事なのは、「試した上で判断する」ことです。最適解を求めて複雑さを増やすより、十分解でシンプルさを保つ方が、長期的には幸せなことが多いです。少なくとも、深夜 3 時のアラート対応は減ります。それは、間違いありません。参考書籍失敗から学ぶRDBの正しい歩き方 Software Design plus作者:曽根 壮大技術評論社AmazonSQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazonセンスの良いSQLを書く技術　達人エンジニアが実践している３５の原則作者:ミックKADOKAWAAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Associate Generative AI LLMs編~]]></title>
            <link>https://zenn.dev/akasan/articles/4437690e2ef59b</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/4437690e2ef59b</guid>
            <pubDate>Mon, 24 Nov 2025 08:41:46 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるAssociate Generative AI LLMsを取得したので、その内容を共有しようと思います。 Associate Generative AI LLMsとは？Associate Generative AI LLMs（以下、NCA-GENL）は、NVIDIAが提供している認定資格の一つであり、AIドリブンなアプリケーションを開発したり運用するためのエントリーレベルの資格となっています。主にLLMについて取り扱われる部分が多いですが、従来の機械学習に関する知識についても問われるようになっています。https://www.nvidia.com...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、本を読め]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314</guid>
            <pubDate>Sun, 23 Nov 2025 19:33:14 GMT</pubDate>
            <content:encoded><![CDATA[はじめに私は本を読むのが好きです。朝、コーヒーを淹れて、ソファに座って、ページを開く。その時間が好きです。物語の中に入り込んで、登場人物の人生を追いかけ、著者の思考を辿り、知らない世界を覗き見る。ただ、それが楽しいんです。でも、誰かに「最近、何か読んだ？」と聞かれて、タイトルを答えると、必ず次の質問が来ます。「へえ、面白かった？ 何か学びはあった？」あるいは、こんな質問が来ます。「その本、どういうジャンル？ 自己啓発系？ノンフィクション？」違和感があります。映画を見たあと、「何か学びはあった？」なんて聞かれません。音楽を聴いたあと、「それ、自己啓発系？」なんて聞かれません。ゲームをクリアしたあと、「成長できた？」なんて聞かれません。でも、本だけは違います。読書には、常に「目的」が求められます。「成長のため」「知識を得るため」「キャリアアップのため」。ただ楽しいから読む、では許されない空気があります。SNSを開けば、「読書のすすめ」が溢れています。「本を読まない人は生き残れない」「年間百冊読めば人生が変わる」「ビジネスパーソン必読書」。どれも善意です。本当に、善意なんです。でも、その善意が、読書を窮屈にしています。私が小説を読んでいると言うと、「へえ、小説なんだ」と言われます。その「なんだ」という響きに、少しだけトゲがあります。まるで、「ビジネス書じゃないんだ」「役に立つ本じゃないんだ」と言われているような。あるいは、ミステリを読んでいると言うと、「息抜きにはいいよね」と言われます。その「息抜き」という言葉に、少しだけ違和感があります。まるで、本来読むべきは「ちゃんとした本」で、娯楽はその合間に挟むもの、と言われているような。おかしくないですか？ 映画は娯楽として認められています。音楽は娯楽として認められています。ゲームは娯楽として認められています（最近は、ですけど）。でも、読書だけは、娯楽であることを許されていません。「ただ楽しいから読む」では、ダメなんでしょうか。物語に没入して、現実を忘れる。登場人物に共感して、泣いたり笑ったりする。推理小説でハラハラして、犯人を当てようとする。SF小説で想像力を膨らませて、知らない世界に思いを馳せる。それだけじゃ、ダメなんでしょうか。この文章を書いている今も、矛盾しています。私は「読書について考えている私」を演出しているのだろう。この文章を投稿したら、何人かが「わかる」って言ってくれるだろう。その承認が欲しいのだろう。でも、それでも書きたいんです。なぜ読書だけが、娯楽であることを奪われるのか。なぜ読書だけが、「成長」や「学び」と結びつけられるのか。そして、その結びつきが、どれだけ読書を窮屈にしているのか。この文章は、その違和感から始まります。答えを出すつもりはありません。ただ、この違和感を言葉にしてみたいんです。もしかしたら、あなたも同じ違和感を抱えているだろう。「ただ楽しいから読む」という、当たり前のことが、当たり前じゃなくなっている世界。その世界を、少しだけ問い直してみませんか。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。加速文化という病私たちは走り続けています私たちは、「加速文化」の中を生きています。現代社会では変化のスピードが絶え間なく加速し、個人も常に成長し続けることを要求されます。「走り続けること」そのものが目的化し、どこに向かっているのか、なぜ走っているのかという本質的な問いは置き去りにされます。「もっと成功しろ」「もっと幸せになれ」「スキルを身につけろ」「成長し続けろ」。現代社会は、こうした強烈なプレッシャーを発し続けます。「もっと」という言葉は、終わりのない要求を意味します。どれだけ達成しても、常に「もっと」が待っています。誰かと比べずにはいられませんSNSを開けば、誰かが何かを達成しています。誰かが本を出版しています。誰かが転職に成功しています。誰かが新しいスキルを身につけています。私たちは比較せずにいられません。そして、比較するたびに自分を劣っていると感じます。「自分は何もしていない」「自分は成長していない」「自分は遅れている」。SNSは、他者の「成功」を可視化し、価値を数値化します。しかし、SNSに現れるのは、他者の「ハイライト」だけです。私たちは自分の未編集の人生と、他者の編集済みの人生を比較してしまいます。より問題なのは、この比較が内面化されることです。自分の中に「比較する目」が住み着きます。常に自分を評価する目。「これは成長でしょうか」「これは生産的でしょうか」。この内なる審判者は、決して満足しません。その基準は加速文化から与えられ、常に「もっと」を要求するからです。だから、走らなければなりません。これは、まるでトレッドミルで走っているようなものです。動いているという実感だけがあって、前進しているという実感はありません。『鏡の国のアリス』の赤の女王が言ったように、「同じ場所にとどまるためには、全力で走り続けなければならない」のです。安定したいから成長したい、というおかしさここに、現代の最も奇妙な矛盾があります。「安定したいから成長したい」。「安定」と「成長」は、本来相反する概念です。安定とは、変化しないこと。成長とは、変化し続けること。なのに、私は「安定したいから成長したい」と言っています。なぜこの矛盾が成立するのでしょうか。「変化する環境の中で生き残るためには、変化し続けなければならない」という論理があるからです。つまり、「安定」は、もはや「変化しないこと」では達成できません。「変化し続けること」によってのみ達成できるのです。しかし、この論理は実は安定を永遠に延期しています。「いつか安定する」という約束のもとで、今は変化し続けます。でも、その「いつか」は決して来ません。終わりのない変化を、「安定」という言葉で正当化しています。不安が売られていますこのロジックは、巧妙なマッチポンプを生み出します。「成長しなければ生き残れない」という不安を煽り、成長のための商品やサービスを売ります。読書、セミナー、資格、転職支援、コーチング。このシステムの巧妙さは、被害者が加害者になることです。私は不安を抱え、本を買い、その経験を「成功体験」として語ります。この語りは、他者に同じ不安を伝染させます。そして、その最も基本的で、最も無害に見えて、最も広く受け入れられているのが「読書」なのです。読書は知的で文化的です。読書を批判することは、知性を否定することのように聞こえます。だから、「読書のすすめ」は抵抗なく受け入れられます。でも、それが実は加速文化の最前線にあるのです。やりたいことがわかりませんここには、もう1つの構造的な問題があります。私たちには、やりたいことがわかりません。「やりたいことを見つけろ」と言われ続けます。就活でも、転職でも、キャリア面談でも。自己分析をしろ。強みを見つけろ。情熱を持て。でも、そんなもの、簡単に見つかるわけがありません。むしろ、「やりたいことを見つけろ」というプレッシャーそのものが、私たちを追い詰めます。「やりたいことがない自分はダメだ」「情熱がない自分は劣っている」。「やりたいこと」は、発見するものではなく、構築するものです。様々な経験の中で、試行錯誤の中で、少しずつ形成されていくものです。にもかかわらず、現代社会は「今すぐ見つけろ」と命令します。ここに、加速文化の最も陰湿な側面があります。加速文化は、「やりたいことを見つけろ」と言いながら、実は「やりたいことを見つける時間」を奪っています。常に何かに追われています。常に次のタスクがあります。その結果、立ち止まって考える余裕がありません。「やりたいこと」を見つけるためには、時間が必要です。無為な時間、退屈な時間、何もしない時間。でも、加速文化は、その時間を「無駄」と見なします。「生産的」ではないから。その結果、私たちは「やりたいこと」を見つけられないまま、「やりたいことを見つけなければ」という焦燥だけを抱え続けます。潰しがきく選択肢という罠その結果、私は「とりあえず潰しがきく選択肢」に逃げ込みます。やりたいことはわかりません。でも、「汎用性の高いスキル」を身につけておけば、将来の選択肢が増えます。どこでも通用します。だから、とりあえずそれを目指そう。これは、一見合理的に見えます。でも、ここに罠があります。「汎用性の高いスキル」は、AIが最も得意とすることなのです。ロジカルシンキング。データ分析。プログラミング。外国語。これは確かに重要です。でも、これはすべて、AIに置き換えられつつあります。人間がAIに勝てるのは、「汎用性」ではありません。「固有性」です。その人だけが持つ価値観。その人だけが面白いと思うこと。その人だけが執着すること。それこそが、AIに代替されない価値です。でも、私は「やりたいことがわからない」まま、「汎用的なスキル」だけを積み上げています。その1つが「読書」です。何を読めばいいかわからないから、「必読書リスト」に従います。リストに従っていれば、「成長している」気分になれます。でも、それは本当に自分が読みたい本なのでしょうか。その結果、私は「やりたいこと」が空っぽなまま、知識だけを積み上げています。人格や欲望にもとづく価値基準が不在のまま、汎用的な情報を消費し続けています。そして、何も変わりません。やりたいことがわからないのに、知識だけ増えていきます。ここでも、本は読んだが問いは増えていません。実は読んでいませんここで、より深刻な問題に気づきます。私たちは、読書すらしていません。本を買っているだけ、リストを眺めているだけ、動画を見ているだけなのです。これらの行為は、「成長の記号」を消費しています。記号を消費しても、実体は得られません。記号としての「読書」を消費しても、読書の実体である「思考の格闘」は得られません。記号としての「知識」を消費しても、知識の実体である「理解」は得られません。記号としての「成長」を消費しても、成長の実体である「変容」は得られません。本を買います。Amazonでポチります。書店でレジに持っていきます。その瞬間、「私は成長しようとしている」という感覚が得られます。購入という行為が、「成長への意志」を示す儀式として機能しています。金を払います。その対価として、「私は成長しようとしている」という自己イメージを得ます。実際に本を読むよりも、はるかに安い買い物です。でも、そのあと自分の中に新しい疑問は生まれたでしょうか。必読書リストを眺めることもあります。「ビジネスパーソン必読書50選」「今年読むべき本ベスト10」。知っている本が何冊かあります。「ああ、これは読んだ」。そして、知らない本をメモします。「いつか読もう」。道筋が見えているだけで、目的地に近づいた気がします。実際には一歩も進んでいないのに。でも、そこに疑問はあるでしょうか。違和感は。より手軽なのが書籍まとめ動画です。10分の動画で得られるのは、本の「結論」だけです。しかし、本の価値は、結論だけにあるのではありません。むしろ、結論に至るまでの過程にこそ、価値があります。著者がどう考え、どう格闘したか。その過程を経験することで、読者の思考が鍛えられ、価値観が揺さぶられ、問いが生まれます。でも、動画は結論だけを与えます。そして、結論だけを知っても、自分は変わりません。疑問も、違和感も、何も残っていません。でも、記号の消費は、心地よいのです。なぜなら、実体を得るよりも、はるかに簡単だから。本を読むには、時間がかかります。理解するには、努力がかかります。変わるには、苦痛が伴います。でも、本を買うのは一瞬です。リストを眺めるのは数分です。動画を見るのは10分です。そして、それでも「成長した気」になれるなら、なぜ本を読む必要があるのでしょうか。こうして、私たちは読書すらしなくなります。アルゴリズムと必読書リスト仮に本を読むとしても、そこには2つの問題があります。1つ目は、アルゴリズムによる自己隷属です。「読書は自由だ」とよく言われます。でも、私は「自由に本を選んでいる」と思いながら、実際には既存の自分の枠内でしか選んでいません。「読みやすい本」「共感できる本」。自分を変えない本ばかりを選び、それを「自由」と呼んでいます。そもそも、「自分の好み」が変わっていかないなら、読書なんてなんのためなのでしょうか。読書の本質的な目的は、自分を変えることです。でも、私の「好きな本を読む」という自由は、実は「今の自分を肯定する本を読む」という自己隷属になっています。ネット書店のおすすめ。SNSのタイムライン。「あなたにおすすめの本」。これはすべて、「あなたの好みに合った本」を提示します。しかし、よく考えてみてください。アルゴリズムは、何を最適化しているのでしょうか。私の成長ではありません。私の満足度です。アルゴリズムの目的は、私に本を買わせること、私を長くサイトに留めることです。だから、アルゴリズムは、私が「気に入りそうな」本を推薦します。でも、私が「気に入る」本は、私を変えません。アルゴリズムは、私の周りに見えない壁を作ります。その壁の内側には、私にとって快適な情報だけがあります。そして、私はその快適さを「自由」と呼びます。でも、それはおそらく本当の自由ではありません。2つ目は、必読書リストという新たな隷属です。アルゴリズムに違和感を覚えた私は、「必読書リスト」に向かいます。「アルゴリズムに選ばされるのはイヤだ」「自分の好みだけで選ぶのは狭い」。だから、他者が選んだ、推奨された、「読むべき」とされる本のリストに従います。確かに、自分では選ばない本を読むことは重要です。でも、決定的な違いがあります。本来のリスト読書は、問いを獲得するための冒険です。でも、現代の「必読書リスト」は、答えを得るための効率化になっています。ここで、二種類のリストを区別してみたいと思います。第一のリストは、古典のリストです。プラトン、カント、ニーチェ、ドストエフスキー。これらの古典を読むことは、苦痛を伴います。理解できません。でも、その理解できなさの中で、自分の価値観が揺さぶられます。「正義とは何か」「自由とは何か」。根源的な問いに直面します。そして、その問いと格闘することで、自分が変わります。第二のリストは、必読書のリストです。「ビジネスパーソン必読書50選」。これらのリストは、「今求められている知識」を効率的に獲得することを目的とします。読みやすく、すぐに役立ち、何より安心できます。「このリストに従っていれば、遅れない」と。でも、それは幻想でしょう。なぜなら、リストを消化しても、問いを獲得していません。必読書リストは、私たちの問いを奪っているかもしれません。「何を読むべきか」「何が重要か」「何のために読むか」。これらを全て他者が決めます。結果として、自分で問いを立てる力が育ちません。自分の価値基準が形成されません。「やりたいこと」が空っぽなままです。でも、リストを消化することで達成感を得られます。だから、また次のリストを探します。アルゴリズムもリストも、「何を読むか」は教えてくれます。でも「なぜ読むのか」「読んだあと、どんな問いと一緒に生きていくのか」は教えてくれません。なぜ私たちはリストに従うのでしょうか。選択の責任からの逃避と、不安の一時的な解消のためです。リストがあれば、「何を読めばいいかわからない」という不安は解消されます。これは、不安の麻酔のようなものです。根本的な解決ではありませんが、痛みを一時的に和らげます。なぜ「もっと読まなきゃ」が終わらないのか読書体験が「数字」に変わるとき本を読むとき、何が起きているでしょうか。物語に没入します。考えが揺さぶられます。知らない世界を覗き見ます。その時間が、楽しい。それが、読書体験です。でも、いつの間にか、別のものを数え始めます。「今月、何冊読んだか」「必読書リストを、どこまで消化したか」「読書時間は、何時間か」。読書体験そのものではなく、読書したという事実が大事になっています。体験は、数字に変換されます。数字は、比較できます。競争できます。SNSに投稿できます。でも、体験そのものは、比較できません。見せられません。だから、数字のほうが「価値がある」ように見えてしまいます。こうして、読書から、読書体験が抜け落ちます。残るのは、数字だけです。満たされない構造ここに、厄介な問題があります。数字は、決して満たされません。50冊読みました。でも、100冊読んでいる人がいます。必読書を読みました。でも、原書で読んでいる人がいます。どれだけ達成しても、「もっと」が待っています。これは、あなたの問題ではありません。構造の問題です。数字による評価は、比較によって成り立っています。他者より多く。他者より速く。他者より難しく。差があるから、価値がある。でも、差は常に脅かされています。だから、新しい差を作らなければなりません。終わりがないのは、そういう仕組みだからです。満たされないのは、あなたが足りないからではありません。満たされないように、できているのです。「楽しむ」が難しい理由「だったら、数字なんか気にせず、楽しめばいい」。その通りです。でも、それが難しい。なぜか。評価される側として生きてきたからです。学校では成績。会社では業績。SNSではいいねの数。私たちは、常に評価されてきました。だから、何かをするとき、無意識に「これは評価されるだろうか」と考えます。本を読むときも、「これは意味があるだろうか」と考えます。評価の目が、内面化されています。自分の中に、審判者が住んでいます。だからこそ、意識的に選ぶ必要があります。数字を追いかけない。比較しない。評価されなくても、読み続ける。これは、単なる心がけではありません。評価の構造からの、意識的な離脱です。完全に離脱する必要はありません。評価を気にする気持ちは、消えません。それは自然なことです。でも、評価を唯一の基準にしないこと。これは可能です。読書が楽しければ、それでいい。年間10冊でも、それでいい。リストを無視しても、それでいい。評価されなくても、読み続けられる。その回路を持つこと。それが、終わりのないループから抜け出す方法です。永遠に満たされない不安のループ数字を追いかける構造と、評価の内面化。この2つが組み合わさると、恐ろしいループが生まれます。「生き残らなきゃ」という不安から始まり、「成長しなきゃ」という焦燥、「読書しなきゃ」という義務感へと続きます。必読書リストを探し、リストを見る、本を買う、動画を見ます。そして「成長した気分」を得ます。しかし問いを獲得していないので、自分は変わっていません。「まだ足りない」と感じます。より多くのリスト、より多くの本、より多くの動画を求めます。そして最初に戻ります。不安は解消されていません。これが、「読書のすすめ」が永遠にバズり続ける理由でしょう。このループは自己強化的です。ループを回るほど、「成長した気分」と「実際の成長」の乖離が大きくなります。私たちは本を買い、動画を見、リストを消化しています。でも、何も変わっていません。その乖離に薄々気づきながらも、認めたくありません。だから、もっと本を買います。そう信じて、ループを回し続けます。なぜ「読書のすすめ」がバズるのでしょうか。『本を読めば変われる』という物語は、不安を和らげるのではなく、不安を生産しています。この物語を読むたびに、「自分は十分に本を読んでいない」でしょう。そして、その不安が、また「読書のすすめ」を求めさせます。巧妙なマッチポンプです。このループから抜け出せないのは、問いが不在だからでしょう。「なぜ読むのか」「何のために読むのか」。この問いがないまま、ただリストを消化します。だから、終わりがありません。本は読みました。けれど、問いは増えていません。だからまた不安になり、次の「読書のすすめ」を探します。私にとって読書とは何かここまで、「成長のための読書」という物語を批判してきました。「本を読まなきゃ」というプレッシャー。「年間100冊」という数値目標。「必読書リスト」という他律的な選択。そして、読書体験を数字に変換し、評価を内面化する構造。これは確かに、読書を窮屈にしています。でも、だからといって、成長すること自体を否定したいわけではありません。私にとって、読書とは、問いを獲得するための冒険です。答えを得るために本を読むのではなく、問いを見つけるために読みます。既存の自分を確認するのではなく、自分を変えるために読みます。安心するために読むのではなく、不安になるために読みます。読書を通じて、自分が変わります。価値観が揺さぶられます。新しい視点を得ます。世界の見え方が変わります。それは、成長です。しかし、それは「成長しなければならない」という義務から生まれる成長ではありません。「年間100冊読めば人生が変わる」という約束に従う成長でもありません。「必読書リスト」を消化することで得られる成長でもありません。それは、読書そのものを楽しむ中で、結果として起こる成長です。物語に没入して、登場人物の選択に心を揺さぶられます。その結果、自分の価値観が変わります。哲学書を読んで、理解できない文章に格闘します。その結果、新しい問いが生まれます。小説を読んで、知らない世界を覗き見ます。その結果、自分の世界が広がります。これは全て、「成長しよう」と思って起こることではありません。ただ楽しんでいたら、結果として起こる変化です。そして、その変化が周りの環境に合っていたら、「成長」と呼ばれます。合わなかったら、ただの変化です。でも、どちらでもいいんです。変化そのものに価値があります。それが「成長」という名前で呼ばれるかどうかは、環境次第です。社会の基準次第です。時代次第です。読書を通じて、自分が変わります。その変化が、たまたま今の環境で「成長」と評価されるだろう。評価されないだろう。でも、それは二の次です。重要なのは、自分が変わったということ。新しい視点を得たということ。世界の見え方が変わったということ。それだけです。だから、こう言いたいのです。読書は、楽しんでいいんです。「何か学びはあったか」なんて気にしなくていいです。「問いは増えたか」なんて確認しなくていいです。「成長できたか」なんて測定しなくていいです。ただ、その時間が楽しければいいです。物語に没入して、現実を忘れる。それだけで十分です。登場人物に共感して、泣いたり笑ったりする。それだけで十分です。推理小説でハラハラして、犯人を当てようとする。それだけで十分です。そして、もし読み終わったあとに、何かが変わっていたら。新しい問いが生まれていたら。それは、ボーナスです。でも、それは目的ではありません。結果です。楽しむことが目的で、成長は結果です。この順序を、逆にしてはいけません。「成長するために読む」ではなく、「楽しんで読んでいたら、結果として成長していた」。これが、私にとっての読書です。読書そのものは、必ずしも人格を育てるわけではありません。むしろ劇薬と言えます。興味の赴くままただ読むのは、時に有害でさえあります。歴史を振り返れば、独裁者も大量虐殺者も、大読書家でした。彼らは膨大な本を読みました。それが彼らを善き人間にしたわけではありません。読書は道具です。道具は、使い方次第で、善にも悪にもなります。では、どう読めばいいのでしょうか。鍵になるのは自発性です。本とテレビ・YouTube・Podcastの決定的な違いは、本が「自発」を要求することです。本は、私が選ばなければ私の手の中にやってきません。本は、私が目を動かさなければ、語り始めてくれません。本は、私が理解しようとしなければ、ただの記号の羅列です。つまり、本を読むためには、能動的かつ自発的に読者が働きかけなければなりません。一方、テレビやYouTube、Podcastは、一方的に情報を流し込んできます。受動的に消費できます。画面を見ていれば、音声を聞いていれば、情報は入ってきます。思考は不要です。この違いこそが決定的です。自発性こそが、思考を生みます。受動的に与えられた情報は、思考を生みません。ただ受け取り、ただ流れるだけです。しかし自発的に獲得した情報は、思考を生みます。なぜなら、獲得するプロセスですでに思考しているからです。だからこそ、本を読むときは「どんな問いを持ってページを開くか」が決定的になります。読書によって得られるものは、考えること。疑問をもつこと。異議を申し立てることです。読書の真の効用は、ここにあります。世の中の常識とされていること、あたりまえと受け入れられている前提を、疑ってかかります。「本当にそうなのか」「なぜそうなのか」「他の可能性はないのか」。こういう問いを持つことが、読書の本質です。この問いを持つ人間は、システムにとって邪魔な存在です。システムが必要としているのは、考えない労働者、考えない消費者です。言われたことを黙って実行する人間。与えられた情報を疑わずに受け入れる人間。しかし読書する人間は、疑います。問います。異議を唱えます。だから、システムは読書を骨抜きにしようとします。「読書のすすめ」を発信します。「必読書リスト」を作ります。「要約動画」を提供します。そうすれば、私たちは本を読みます。けれども考えません。疑いません。問いません。ただ、与えられた情報を消費するだけです。これは読書ではありません。読書の形をした、情報消費です。ここで、現代の読書が抱える問題に気づきます。思考の型を学ぶことが、思考停止を生んでいます。「MECE」「ロジックツリー」「仮説思考」。これらは有用な道具です。しかし「型」を覚えることが目的になると、「型」に縛られ、「型」の外側を見なくなります。世界は、「型」に当てはまらないもので満ちています。むしろ、「型」に当てはまらないものこそが、面白く、新しく、価値があります。もう1つの問題は、作業をすることが、目的化してしまうことです。本を読む、ページをめくる、線を引く、メモを取ります。これらの「作業」をすることで、「自分は頑張っている」という実感を得ます。しかし、読書は本来「作業」ではありません。読書は、思考です。格闘です。問いとの対話です。ページ数をカウントし、読書時間を記録し、読了数を競います。読書を「作業」として扱った瞬間、読書は死にます。読書とアイデンティティの罠ここまでは、読書の「方法」について語ってきました。読書にはもう1つ、深刻な問題があります。読書が、アイデンティティの道具になる時です。「積読」という現象があります。買ったけど読んでいない本が積まれている状態。多くの読書家が、この積読に悩んでいます。「読まなきゃ」「もったいない」「時間がない」。しかし別の角度から見ることもできます。積読は、ファッションです。本棚は、なりたい自分の姿、未来の自分への約束です。読める読めないは別として、難しい本を買ってしまいます。哲学書を買います。古典を買います。専門書を買います。それらを本棚に並べます。本棚の「面構え」が変わります。そして、その本棚を見るたびに、「私はこういう人間でありたい」でしょう。これは、服を買うのと同じです。服を買う時、私たちは「今の自分」に合う服だけを買うわけではありません。「なりたい自分」をイメージして、その自分に相応しい服を買います。そして、その服を着ることで、少しずつ、その自分に近づいていきます。本も同じです。「こういう本を読む人間でありたい」「こういう思考ができる人間になりたい」。そのイメージが、本棚を作ります。そして、その本棚に引っ張られて、自分がそれに相応しい人間になろうとします。ここまでは問題ありません。むしろ、これは積極的に肯定すべきことです。積読は、未来の自分への投資です。今は読めなくても、いつか読めるようになります。今は理解できなくても、いつか理解できるようになります。そう信じて、本を買います。それは、自己形成の1つのプロセスです。問題は、このアイデンティティが、他者との差異化の道具になる時です。ここで、「文化資本」という考え方が参考になります。経済的な資本（お金や資産）とは別に、教養や知識、趣味といった文化的な要素も、社会的な価値を持ちます。高い教育を受けた人、芸術に詳しい人、本をたくさん読む人。こうした人々は、その知識や教養によって、社会的な地位や信頼を獲得します。つまり、文化もまた、資本のように蓄積され、交換され、価値を生み出すのです。読書も、この構造の中にあります。「私は本を読む」という行為は、「私は教養がある」というシグナルを発します。そして、そのシグナルは、「本を読まない人」との境界線を引きます。この境界線は、善意によって引かれます。「もっと本を読んでほしい」という言葉の裏には、「本を読まないあなたは、何かを失っている」という暗黙のメッセージがあります。そして、そのメッセージを受け取った側は、「本を読まなきゃダメなんだ」と感じるか、「所詮マウンティングだ」と反発します。どちらにせよ、分断が生まれます。ここで、恐ろしい矛盾に気づきます。読書によって自分のアイデンティティを保とうとすればするほど、そのアイデンティティは脆くなります。なぜなら、「読書する私」というアイデンティティは、「読書しない他者」の存在によって初めて成り立つからです。他者との差異によって、自分の価値が定義されます。だから、心のどこかで、私は「みんなが本を読む」ことを望んでいません。口では「もっと本を読んで」と言いながら、本音では、他者が本を読まないことを願っています。これは、恐ろしい自己矛盾です。実際、この矛盾は現実のものになりつつあります。「読書」という言葉が氾濫し、「読書している私」という特別さが希薄化していきます。だから、人々はより高い壁を作ろうとします。「全部読む」「原書で読む」「年間100冊読む」。新たな境界線を引きます。でも、それは本質的な解決にはなりません。どんな境界線を引いても、それは結局、他者との差異に依存しています。そして、他者との差異に依存している限り、アイデンティティは脆いのです。本棚で他者と差をつけようとすればするほど、私の本棚からは問いが減っていきます。残るのは「どう見られたいか」という問いだけです。「生き残る」という言葉の暴力性ここで、もう一度、根本的な問いに戻りましょう。「本を読まない人は生き残れない」。この言葉を目にするたびに、私は違和感を覚えます。「生き残る」という言葉は、暴力的です。「生き残る」という言葉を使うとき、私たちは何を前提としているのでしょうか。生き残る人がいます。そして、生き残れない人がいます。「生き残れなかった」人とは、誰のことを指すのでしょうか。過労死した人。病で倒れた人。若くして亡くなった才能ある人々。彼らは、「本を読まなかったから」生き残れなかったのでしょうか。違います。「生き残る/生き残れない」という二分法そのものが、暴力的です。この二分法は、人生を競争に還元しています。しかし人生は競争ではありません。人生は、複雑で、出鱈目で、混沌としていて、多面的なものです。そして、死は、敗北ではありません。同じように、「本を読め」という命令も、暴力的です。「本を読まないあなたは、遅れている」「生き残れない」。このメッセージは、受け手を追い詰めます。しかし、本を読むことは、1つの選択肢に過ぎません。価値ある選択肢ですが、唯一の選択肢ではありません。本を読まなくても、学べることはあります。成長できることはあります。だから、言葉を言い換える必要があります。「生き残る」ではなく、「価値を示し続ける」。「本を読め」ではなく、「本を読む」。この言い換えは、単なる言葉遊びではありません。根本的な視点の転換です。「生き残る」は、生と死の二分法です。ゼロサム・ゲームです。誰かが生き残るためには、誰かが生き残れません。でも、「価値を示す」は、程度の問題です。グラデーションです。みんなが価値を示せます。同じように、「本を読め」は、命令です。義務です。他律です。でも、「本を読む」は、選択です。欲求です。自律です。そして、この転換こそが、読書を解放する鍵です。重要なのは、生き残るために本を読むことではなく、「どう生きたいのか」という問いに少しずつ形を与えていくことです。ここで、改めて考えてみます。成長とは何でしょうか。加速文化の中では、成長は「より多く」「より速く」「より効率的に」として定義されます。より多くの本を読みます。より速く読みます。より効率的に知識を得ます。しかし、それは本当に成長なのでしょうか。成長とは、自分が変わることです。好みが変わります。価値観が変わります。問いが変わります。見える世界が変わります。そして、その変容こそが、「変化する環境の中で価値を示し続ける」ための基盤になります。なぜなら、自分が変われる人は、環境の変化に適応できるからです。自分が変われない人は、環境が変化したとき、取り残されます。「より多く」「より速く」「より効率的に」知識を得ることは、自分を変えません。むしろ、既存の自分を強化します。既存の自分を肥大化させます。そして、環境が変化したとき、その肥大化した自分が、足かせになります。もう1つ、考えてみます。価値とは何でしょうか。これは、一言でいえるような簡単なものではありません。しかし少なくとも、そのガイドラインになるものは、自分軸で持っておいたほうがいいでしょう。この「自分軸」こそが、読書によって獲得すべきものです。自分軸とは、問いです。「何が面白いのか」「何が重要なのか」「何のために働くのか」「何のために生きるのか」。これらの問いに対する自分なりの答え、あるいは答えを探し続ける姿勢。それこそが「自分軸」であり、「やりたいこと」であり、AIに代替されない価値の源泉です。しかし「必読書リスト」は、その問いを奪います。加速を拒否しますここまで、加速文化と読書の問題を語ってきました。では、どうすればいいのでしょうか。加速を拒否します。立ち止まります。これは、単なる怠惰ではありません。積極的な抵抗です。読書を取り戻すために、3つの根本的な問いと向き合う必要があります。これらの問いは、読書という行為の本質に関わるものです。答えを急ぐ必要はありません。問い続けることそのものが、読書を解放する鍵になります。第一の問い：誰のために読むのでしょうか「本を読まなきゃ」と思うとき、私たちは誰の声を聞いているのでしょうか。SNSのタイムラインに流れてくる「読書のすすめ」。「必読書リスト」。「新人が読むべき本」。これは全て、他者の期待です。他者が定めた基準です。でも、その本は、本当に自分が読みたい本なのでしょうか。現代の自己啓発は、「自分らしさを見つけろ」「本当の自分を知れ」と言います。でも、これは罠です。「自分らしさ」を追求することが、かえって自分を見失わせます。なぜなら、「自分らしさ」とは、他者との差異によって定義されるからです。「他の人とは違う、特別な私」。でも、その「特別さ」は、脆いのです。常に他者との比較によってしか成り立ちません。向き合うべきは、自分が関わる人々に対する義務です。家族に対する義務。友人に対する義務。社会に対する義務。そして、読書についても同じです。古典を読む義務。先人たちが残した思想と格闘する義務。この「義務」は、他者から課されるものではありません。自分が自分に課すものです。同時に、断る勇気も必要です。「必読書リスト」を無視していいのです。途中で「この本は自分に合わない」と思ったら、読むのをやめていいのです。誰のために読むのか。この問いに向き合うことは、他者の期待ではなく、自分が向き合いたい問いは何かという方向へ進むことです。読書を義務から解放し、選択として取り戻すことです。第二の問い：何を求めているのでしょうか「この本を読めば成長できる」「年間100冊読めば人生が変わる」「要約を見れば効率的に知識が得られる」。読書は、常に何かの「手段」として語られます。成長のため。キャリアアップのため。生き残るため。でも、本当にそれを求めているのでしょうか。ポジティブ思考が溢れています。「できる」「やればできる」「可能性は無限」。でも、これは現実を単純化します。人生は、複雑で出鱈目で混沌としていて多面的なものです。すべてをコントロールできるわけではありません。失敗もします。うまくいかないこともあります。理不尽なこともあります。読書も同じです。「この本を読めば成長できる」というポジティブな約束に騙されません。むしろ、ネガティブな可能性を受け入れます。「この本は理解できないだろう」「この本を読んでも何も変わらないだろう」「途中で飽きて読み終えられないだろう」。その上で、それでも読みます。不確実性を受け入れながら、それでも本を開きます。そして、感情とも距離を置きます。「読まなきゃ」という焦燥。これらの感情は、読書を苦痛にします。今日は読む気分じゃありません。それなら、読みません。それでいいのです。より、「もっと速く」という呪縛からも自由になります。ゆっくり読んでいいです。同じページを何度も読み返していいです。一冊の本に一年かけてもいいです。速さではなく、深さ。何を求めているのか。この問いに向き合うことは、成果主義・完璧主義から解放されることです。答えを求めるのではなく、問いを見つけます。この本からどんな問いを持ち帰りたいのか。読書を手段から目的へと転換することです。第三の問い：どう読むのでしょうか自己啓発書を読みます。ビジネス書を読みます。要約動画を見ます。こうしたものは、すべて単純化します。「こうすれば成功する」「これをやれば幸せになれる」「この思考法を使えば問題が解決する」。人生を、因果関係の単純な連鎖に還元します。でも、人生は、そんなに単純なものでしょうか。小説を読めば、もっと複雑な世界観が提示されます。登場人物たちは、矛盾しています。善人でも悪人でもありません。理性的でもなければ、ただ感情的なだけでもありません。予測不可能な行動をします。そして、物語には、明確な答えがありません。むしろ、問いが生まれます。「この登場人物の選択は正しかったのか」「自分だったらどうしただろう」「人間とは何なのか」。小説を読めば、破天荒なキャラクターたちの人生を追体験することで、人生をコントロールできないことが学べます。加速文化は、「人生をコントロールできる」という幻想を植え付けます。でも、これは幻想です。人生は、コントロールできません。予測できません。理不尽です。そして、その理不尽さを受け入れることこそが、真の成熟です。小説は、その成熟を促します。同時に、未来だけでなく、過去とも対話します。現代社会は、常に「未来志向」を要求します。「過去にこだわるな」「前を向け」。でも、過去にこだわります。過去に読んだ本を、もう一度読みます。若い頃に読んで理解できなかった本を、今読み直します。そこに、新しい発見があります。昔は好きだった本を、今読み返します。自分がどう変わったかがわかります。過去の自分が選んだ本を尊重します。「あの頃の自分は何を考えていたのか」。その問いが、自分を理解する手がかりになります。過去の自分が選んだ本を「恥ずかしい」と思いません。それもまた、自分の一部です。同じ本を読み返したとき、昔の自分と今の自分で、立ち上がる問いが変わっているか。それが、自分が変わったかどうかの指標になります。どう読むのか。この問いに向き合うことは、単純化から複雑性へ、未来志向から過去との対話へと視点を転換することです。自己啓発書ばかりではなく小説を。新しい本ばかりではなく過去に読んだ本も。それは、読書を知識の獲得から思考の深化へと変えることです。本を読んだあと、問いが増えていないなら、それは「読んだ」とは言えないでしょう。読書の多様性を認めますここで、1つの矛盾に気づくでしょう。「小説を読め」と言いながら、「正しさを押し付けるな」とも言っています。これは矛盾ではないのでしょうか。いや、違います。重要なのは、「正しさ」を一つに固定しないことです。全部読む人もいれば、要約で済ませる人もいます。じっくり読む人もいれば、流し読みする人もいます。ビジネス書を読む人もいれば、小説を読む人もいます。マンガを読む人もいれば、読まない人もいます。そして、どれも「正しい」のです。私が提案しているのは、「小説を読め」ではなく、「自己啓発書『ばかり』を読むな」です。ビジネス書ばかり。要約ばかり。リストばかり。そうやって、1つの形式に固定されることが危険です。だから、多様性を持ちます。複数の形式で読みます。複数の視点を持ちます。読書に「正しさ」を求める必要はありません。「こうあるべき」という規範を押し付ける必要もありません。それぞれの読み方を、それぞれの価値として認めます。本を読むことは、「深い洞察を得る」ためだけではありません。「面白い話をする」ためでもあります。社交のツールとしての読書。これも、1つの正しい読み方です。本の内容を、自分なりに加工して、他者に提供します。それは、相手を見下すためではなく、一緒に楽しむためです。読書から特権性を剥ぎ取ったとき、読書は軽やかになります。堅苦しさがなくなります。誰にでも開かれたものになります。読書の新しい意味読書から「特権性」を剥ぎ取り、「加速」を拒否したとき、何が残るでしょうか。それは、ただ楽しいから読む、という当たり前のことです。本を読みたいから、読みます。面白いから、読みます。その時間が好きだから、読みます。他者との差異を作るためでもなく、自分のアイデンティティを保つためでもなく、「成長しなきゃ」という焦燥からでもなく。そして、「問いを得るため」でもなく、「学びを得るため」でもなく、「効率的に知識を吸収するため」でもありません。ただ読みたいから読みます。これが、本来の読書の形です。「速読」も「効率的な読書術」も「アウトプット前提のインプット」も、全部いりません。ゆっくり読んでもいいです。飛ばし読みしてもいいです。同じページを何度も読み返してもいいです。途中で飽きたら、やめてもいいです。最後まで読まなくてもいいです。読み終わったあと、何もアウトプットしなくてもいいです。SNSに投稿しなくてもいいです。読書記録をつけなくてもいいです。ただ、その時間が楽しかったなら、それで十分です。積読の山を見て、焦る必要はありません。全部読もうとしなくていいのです。今読みたい一冊を、読みます。それだけでいいのです。「もっと読まなきゃ」「遅れている」「追いつかなきゃ」。そんな焦りは、読書を義務にします。楽しむべき読書が、苦痛になります。一冊ずつ読めばいいのです。今読みたい本を、今読みます。それで十分です。そして、読み終えたら、次の一冊。その繰り返しが、気づけば大きな蓄積になります。読書は、競争ではありません。誰かより多く読む必要はありません。誰かより速く読む必要もありません。自分のペースで、自分の読みたい本を、一冊ずつ読みます。それが、読書の本来の形です。読書は、頭の中の掃除です。頭の中を整理します。雑多な思考を整えます。新しい視点を取り入れます。古い固定観念を捨てます。でも、掃除と同じように、読書も「完璧」を求める必要はありません。毎日少しずつでいいのです。一日一ページでもいいのです。完璧に読まなくてもいいのです。流し読みでもいいのです。途中で飽きたら、別の本に移ってもいいのです。読書を、義務にしません。プレッシャーにしません。自分を追い込みません。ただ、自分を大切にする1つの手段として、読書があります。それだけでいいのです。本を読んだら、感想を書かなきゃ。書評を書かなきゃ。SNSに投稿しなきゃ。そんな義務感が、読書を窮屈にします。でも、言語化しなくてもいいのです。ただ読みます。心の中に留めます。それだけでいいのです。本を読んで、何も言葉になりません。でも、何かが変わった気がします。それで十分です。言語化できない読書の体験。それこそが、最も豊かな読書なのでしょう。おわりにこの文章を書き終えて、スマホを見ます。何も変わっていません。タイムラインには相変わらず「読書のすすめ」が流れています。「本を読まない人は生き残れない」というツイートがバズっています。誰かが「必読書リスト」を作っています。たぶん、これからも変わりません。「読書は成長のため」という物語は、これからも繰り返されます。「ビジネスパーソンは本を読め」というメッセージは、これからも発信されます。それは、悪意じゃありません。本当に、善意なんです。だから、厄介なんです。でも、私は諦めません。本を読むのは、楽しいからです。物語に没入するのが、楽しいからです。知らない世界を覗き見するのが、楽しいからです。それだけです。映画を見るのと同じです。音楽を聴くのと同じです。ゲームをプレイするのと同じです。ただ、楽しいから。それ以上でも、それ以下でもありません。私は、誰も説得しようとは思いません。ただ、もしあなたも「ただ楽しいから読む」では、ダメなのかな、と思っているなら。「成長」とか「学び」とか、そういう目的がないと、読書しちゃいけないのかな、と思っているなら。伝えたいんです。大丈夫です。ただ楽しいから読む、それでいいんです。物語に夢中になって、現実を忘れる。それでいいんです。何も学ばなくていいんです。何も成長しなくていいんです。ただ、楽しければいいんです。読書は、競争じゃありません。義務でもありません。成長の手段でもありません。ただ、楽しいから読む。それだけです。ただ、楽しんでください。そして、もし誰かに「何のために読むの？」と聞かれたら、こう答えてください。「楽しいから」。それだけで、十分です。本棚を見ます。また明日、読みます。何を読むかは、まだ決めていません。でも、楽しみです。どんな物語に出会えるか。どんな世界を覗けるか。それが、楽しみです。スマホを置きます。窓を開けます。外を見ます。明日も、本を読もう。ただ、楽しいから。それだけです。参考図書加速する社会 近代における時間構造の変容作者:ハルトムート ローザ福村出版Amazon地に足をつけて生きろ！ 加速文化の重圧に対抗する7つの方法作者:スヴェン・ブリンクマンEvolvingAmazon世界のエリートが学んでいる 教養書必読１００冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazon世界のエリートが学んでいるＭＢＡマーケティング必読書５０冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazonさみしい夜のページをめくれ作者:古賀史健ポプラ社Amazon本を読む人はうまくいく作者:長倉 顕太すばる舎Amazon強いビジネスパーソンを目指して鬱になった僕の 弱さ考作者:井上 慎平ダイヤモンド社Amazon読んでいない本について堂々と語る方法 (ちくま学芸文庫)作者:ピエール・バイヤール,大浦康介筑摩書房Amazonビジネス書ベストセラーを１００冊読んで分かった成功の黄金律作者:堀元見徳間書店Amazon自己啓発の教科書　禁欲主義からアドラー、引き寄せの法則まで作者:アナ・カタリーナ・シャフナー日経ナショナル ジオグラフィックAmazon中年の本棚作者:荻原魚雷紀伊國屋書店Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[garakを使ってMistralの脆弱性検知をしてみた]]></title>
            <link>https://zenn.dev/akasan/articles/399d1b363e50c0</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/399d1b363e50c0</guid>
            <pubDate>Sun, 23 Nov 2025 13:41:29 GMT</pubDate>
            <content:encoded><![CDATA[今回はgarakを用いてMistralの脆弱性検知をしてみました。過去にGeminiやgpt系モデルなどに対しては検証していましたが、同様の検証をMistralに対しても実施してみようと思います。過去の検証についても是非併せてご覧ください。https://zenn.dev/akasan/articles/9c6b43d8620517https://zenn.dev/akasan/articles/34756e48c4f870 Mistralとは？MistralはMistral AIが開発しているモデルになります。Mistral-7B-v0.3をインストラクションチューニングした...]]></content:encoded>
        </item>
    </channel>
</rss>