<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sat, 07 Feb 2026 11:45:21 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[Kubernetes Network Driver (KND)について調べてみた]]></title>
            <link>https://sreake.com/blog/introduction-to-kubernetes-network-driver-knd/</link>
            <guid isPermaLink="false">https://sreake.com/blog/introduction-to-kubernetes-network-driver-knd/</guid>
            <pubDate>Thu, 05 Feb 2026 06:00:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回、3-shakeで期限つきインターンをさせていただきました坂内理人（@rihib）と申します。インターン期間中はKubernetes Network Driverと呼ばれるKubernetesのネットワーク […]The post Kubernetes Network Driver (KND)について調べてみた first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI時代に成長するエンジニアに必要なスキルとは.pdf]]></title>
            <link>https://speakerdeck.com/yunosukey/aishi-dai-nicheng-chang-suruensinianibi-yao-nasukirutoha</link>
            <guid isPermaLink="false">https://speakerdeck.com/yunosukey/aishi-dai-nicheng-chang-suruensinianibi-yao-nasukirutoha</guid>
            <pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[正しさは、昼間の言葉]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/04/002244</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/04/002244</guid>
            <pubDate>Tue, 03 Feb 2026 15:22:44 GMT</pubDate>
            <content:encoded><![CDATA[ある日の終電後。オフィスには私しかいなかった。蛍光灯は半分消えている。デスクのモニターだけが、青白い光を放っている。IRCの通知は止まっている。誰も見ていない。誰も知らない。今、私がここで何をしても、誰にも分からない。三時間前、本番環境で障害が発生した。顧客からの問い合わせが殺到した。Nagiosのアラートチャンネルが真っ赤に染まった。私はオンコール担当だった。原因を調査した。特定した。十五年前に書かれたコードの中に、バグがあった。なぜ今頃発火したのか。おそらく、長らく使っていたNFSの性能が落ちていたからだ。十五年間、たまたま踏まなかった地雷を、今夜ついに踏んだ。バグは十五年前からそこにあった。ただ、表に出てこなかっただけだ。障害は収束した。暫定対応でサービスは復旧した。しかし、根本原因を修正しなければ、また同じ障害が起きる。明日の朝までに、修正をリリースしなければならない。画面には、十五年前に書かれたコードが映っていた。変数名は tmp と data と result の三種類しかない。コメントは一行もない。テストは存在しない。一つの関数が八百行ある。その関数の中に、さらに三重のネストしたif文がある。読んでいると、頭が痛くなる。しかし、このコードは動いてきた。十五年間、今夜まで、一度も止まることなく、毎日数万件のトランザクションを処理してきた。私は、二つの選択肢の前で立ち尽くしていた。正しいやり方——リファクタリングする。テストを書く。変数名を直す。関数を分割する。その上で、バグを修正する。それには、最低でも三週間かかる。明日の朝には間に合わない。その間、同じ障害が再発するリスクを抱え続ける。間違ったやり方——このコードの該当箇所に、場当たり的なパッチを当てる。動けばいい。テストは書かない。後で直す。いつか直す。たぶん、直さない。私は知っていた。後者を選べば、このコードはさらに腐る。次にこのコードを触る人は、私を呪うだろう。しかし、その「次の人」は、私ではない。——と書いて、立ち止まる。本当にそうか。「次の人」は、来年の私かもしれない。それでも、私は迷っていた。「正しいコードを書け」。私は、何度もこの言葉を聞いてきた。本で読んだ。カンファレンスで聞いた。先輩に言われた。自分でも言ってきた。SOLID原則。DRY原則。クリーンアーキテクチャ。テスト駆動開発。私は、これらの「正しさ」を信じてきた。信じていた、と思っていた。しかし、終電後のオフィスで、十五年物のレガシーコードを前にして、障害の再発リスクを抱えながら、私は気づいた。「正しさ」は、昼間の言葉だ。昼間は、みんながいる。コードレビューがある。品質基準がある。「正しさ」を語ることで、評価される。「リファクタリングしましょう」と言えば、「意識が高い」と思われる。しかし、夜になると、誰もいない。誰も見ていない。「正しさ」を語る相手がいない。残っているのは、障害の再発リスクと、明日の朝というデッドラインだけだ。私は思い出した。以前、ある先輩が言っていた言葉を。「コードの美しさなんて、障害対応の現場には関係ない。止血が先だ」当時の私は、反発した。「技術的負債がたまる」「保守性が下がる」「長期的に見れば損だ」。正論を並べた。先輩は笑って聞いていた。今になって分かる。あの笑いの意味が。先輩は、私の「正しさ」が、まだ試されていないことを知っていた。本番障害の修羅場を経験したことがない。顧客からのクレームの電話を受けたことがない。自分の判断で、会社の信用が左右される経験をしたことがない。そういう人間が語る「正しさ」は、空論だ。——と書いて、自分の中にある別の声が聞こえる。それは、本当にそうか。「試されていない」ことが、「正しくない」ことの証明になるのか。試されていなくても、正しいものは正しいのではないか。しかし、その夜の私には、その声は届かなかった。私は、キーボードに手を置いた。そのとき、ふと考えた。十五年前、このコードを書いた人は、何を考えていたのだろうか。きっと、同じような状況だったのではないか。締め切りに追われていた。リソースが足りなかった。「後で直す」と自分に言い聞かせて、このコードを書いた。その人も、テストを書かなかった。変数名を直さなかった。コメントを書かなかった。なぜか。時間がなかったからだ。その人の後に来た人も、同じだったのではないか。このコードを見て、顔をしかめた。しかし、直さなかった。なぜか。時間がなかったからだ。そして今、私がここにいる。同じ選択を迫られている。「時間がなかった」。この言葉が、十五年間、このコードを腐らせ続けてきた。そして今、私も同じ言葉を使おうとしている。これは、正当化の連鎖だ。「前の人も時間がなかった。だから、自分も許される」。前例が、免罪符になる。私は、その連鎖の中に自分を位置づけようとしていた。連鎖の一部になれば、責任は薄まる。「自分だけが悪いわけではない」と言える。責任を、過去に分散させることができる。——と書いて、その論理の欺瞞に気づく。責任を分散させても、コードは腐ったままだ。連鎖に加わることで、私が得るのは心理的な安心だけだ。コードは何も改善しない。しかし、その夜の私には、心理的な安心が必要だった。私は、自分を正当化する論理を組み立て始めた。この会社は、このプロダクトで売上を立てている。プロダクトが止まれば、顧客は離れる。顧客が離れれば、売上は下がる。売上が下がれば、会社は傾く。会社が傾けば、私は職を失う。正しいコードを書いている間に、サービスが止まり続けたら、何の意味がある？障害を早く直すことは、顧客のためだ。会社のためだ。同僚のためだ。そして、自分のためだ。みんなのためにやっている。だから、許される。この論理は、強力だ。「自分のため」だけなら、罪悪感がある。しかし、「みんなのため」なら、むしろ正義になる。利他的な動機があれば、手段は正当化される。私は、この論理に身を委ねようとしていた。しかし、ふと気づいた。十五年前の人も、同じ論理を使ったのではないか。「顧客のために、早くリリースしなければ」「会社のために、間に合わせなければ」。そう言い聞かせて、このコードを書いた。その結果が、今夜の障害だ。「みんなのため」という論理で書かれたコードが、十五年後に「みんな」を苦しめている。私は、同じことをしようとしている。今夜、「みんなのため」にパッチを当てる。そのパッチが、十五年後に誰かを苦しめる。正当化の論理は、短期的には機能する。しかし、長期的には破綻する。「みんなのため」は、「将来のみんな」を含んでいない。——と書いて、また立ち止まる。では、どうすればいいのか。障害を放置して、明日も明後日も顧客を苦しめることが、「将来のみんな」のためになるのか。どちらを選んでも、誰かを苦しめる。それが、この状況の本質だ。コードを書いている途中で、手が止まった。私は、自分が何をしているのか、急に分からなくなった。画面には、私が追加したパッチがある。八行。たった八行の条件分岐だ。バグの原因となっていたエッジケースを、特別扱いする。根本的な解決ではない。しかし、これで障害は再発しなくなる。たぶん。私は、自分のコードを見つめた。そして、気づいた。私は、このコードを十五年後に見る誰かを、呪っている。私は、十五年前にこのコードを書いた誰かを呪った。「なぜこんなコードを書いたのか」「なぜテストを書かなかったのか」「なぜリファクタリングしなかったのか」。呪いながら、同じことをしている。十五年後の誰かは、私を呪うだろう。同じ言葉で。同じ怒りで。そして、同じように、さらに八行のパッチを追加するだろう。呪いの連鎖だ。私は、その連鎖の一部になろうとしている。連鎖を断ち切ることもできる。リファクタリングすればいい。テストを書けばいい。しかし、それには三週間かかる。明日の朝には間に合わない。連鎖を断ち切るコストを、私は払えない。いや、払う気がない。コストを払う気がないのは、なぜか。それは、連鎖を断ち切る恩恵を受けるのが、私ではないからだ。リファクタリングの恩恵を受けるのは、十五年後の誰かだ。その誰かは、私ではないかもしれない。私は、十五年後にはこの会社にいないかもしれない。別のプロジェクトにいるかもしれない。そもそも、このプロダクト自体が、十五年後には存在しないかもしれない。コストは今の私が払い、恩恵は未来の誰かが受ける。それなら、コストを払わない方が、合理的だ。私は、その論理に抗えなかった。気がつくと、窓の外が明るくなっていた。目が痛い。徹夜明けの頭が、鈍く重い。始発の電車が走る音が聞こえる。オフィスのエアコンが、タイマーで動き始めた。私の目の前には、完成したパッチがあった。テストはない。コメントは一行だけ。「障害対応: エッジケースの特別処理を追加」。それだけだ。なぜそう書いたのか。どんな判断をしたのか。何も書かない。書けない。いや、違う。書かないのだ。書くと、自分がやったことを認めることになる。認めたくない。だから、曖昧にする。ステージング環境でテストした。動いた。本番環境にデプロイした。障害は再発しなかった。アラートは鳴らなかった。IRCに報告を書いた。「根本原因を修正しました。再発防止策は別途検討します」。「別途検討します」。この言葉が、どれほどの問題を先送りしてきたか。私は知っている。知っていて、使っている。私は、椅子にもたれかかった。疲れていた。しかし、安堵もあった。終わった。障害は収束した。顧客は救われた。私の仕事は終わった。そして、私は立ち上がって、オフィスを出た。朝の光が、眩しかった。夜の間に考えていたことが、すべて嘘のように思えた。「正しさ」も「正当化の連鎖」も「呪いの連鎖」も、夜の闘いの産物だ。朝になれば、消える。いや、消えるのではない。見えなくなるだけだ。朝の光の中で、私は「普通のエンジニア」に戻る。「正しいコードを書くべきだ」と語る。「技術的負債は早めに返すべきだ」と主張する。昼間の私は、そう言う。しかし、また夜が来る。また障害が来る。誰もいないオフィスで、同じ選択を迫られる。そのとき、私は何を選ぶか。私は、自分が何を選ぶか、もう知っている。この文章を読んでいるあなたは、どうだろうか。「私は違う」と思っているかもしれない。「私は正しいコードを書いている」「私はリファクタリングをしている」「私は技術的負債を返している」。そう思っているかもしれない。本当にそうだろうか。あなたは、一度も「後で直す」と言ったことがないか。一度も、テストを書かずにコミットしたことがないか。一度も、分かりにくいコードをそのまま追加したことがないか。一度も、「別途検討します」と書いたことがないか。ないはずがない。私たちは、みな同じ箱の中にいる。状況が許せば、正しいことをする。しかし、状況が許さなければ、正しさを捨てる。そして、「仕方なかった」と自分を正当化する。これは、個人の問題ではない。箱の問題だ。締め切りがある。障害対応の緊急性がある。リソースが足りない。時間が足りない。この箱の中で、「正しさ」を貫くことは、英雄的な行為だ。普通の人間には、できない。——と書いて、自分でも分かっている。「箱を変える」と言うのは簡単だ。実際に変えるのは、難しい。私自身、何度も挫折してきた。しかし、箱を変えようとしなければ、私たちは永遠に同じ場所にいる。夜ごとに「正しさ」を捨て、朝になると何事もなかったかのように振る舞う。その繰り返しだ。私は、その繰り返しから抜け出したい。しかし、抜け出せることを、確信できない。この矛盾を抱えたまま、私は今日もコードを書いている。あの夜から、三ヶ月が経った。私が当てたパッチは、まだ動いている。障害は再発していない。顧客からのクレームもない。障害対応は成功した。誰も、あのコードの中身を知らない。しかし、私は知っている。あのコードの中に、たった八行のパッチがあることを。テストがないことを。コメントが一行しかないことを。「別途検討します」と書いた根本対応が、まだ検討されていないことを。そして、いつか誰かが、あのコードを触ることになる。そのとき、その人は私を呪うだろう。「なぜこんなパッチを当てたのか」「なぜ根本対応をしなかったのか」と。私は、その呪いを受け入れる準備ができている。いや、できていない。受け入れなければならないと分かっているだけだ。次の障害が来たとき、私は何を選ぶか。「正しさ」を選ぶか。また同じ論理に堕ちるか。私自身にも、分からない。分からないまま、今日も私はオフィスに向かう。昼間は「正しさ」を語り、夜になれば「正しさ」を捨てる。その繰り返しの中で、私は何者になっていくのか。十五年前の人は、一度だけ選択した。その選択の結果が、今夜の障害だった。しかし、エンジニアは、毎日選択を迫られる。毎日、小さな「正しさ」を捨てるか、守るかを選ぶ。その積み重ねが、私たちを作る。そして、十五年後のコードを作る。私は、どんなエンジニアになりたいのか。どんなコードを残したいのか。その問いに、まだ答えられない。答えられないまま、今日も私は、コードを書く。十五年前の人の行方は、誰も知らない。私の行方も、私自身が知らない。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、分けて語るな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/02/124615</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/02/124615</guid>
            <pubDate>Mon, 02 Feb 2026 03:46:15 GMT</pubDate>
            <content:encoded><![CDATA[はじめに月曜日は経営会議。事業戦略を話す。水曜日は技術戦略会議。アーキテクチャを話す。金曜日は組織開発会議。チーム編成を話す。それぞれの会議には、それぞれの参加者がいる。経営会議には経営陣。技術戦略会議にはエンジニアリングリーダー。組織開発会議には人事と各部門長。それぞれが、それぞれの言葉で、それぞれの関心事を語る。私はいろんな立場でこれらの会議に呼ばれる。そして、いつも同じ違和感を覚える。「この話、別の会議でも関係あるんじゃないの？」言えない。言っても通じない。経営会議で「それ、アーキテクチャの話と関係ありませんか」と言っても、「それは技術の話だから水曜日に」と返される。技術戦略会議で「それ、組織の問題では」と言っても、「それは人事の話だから金曜日に」と返される。きれいに分かれている。整然としている。効率的に見える。分断は、様々な形をとって現れる。会議の分断。言葉の分断。評価指標の分断。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。人事部門は採用数と離職率で評価される。それぞれが、自分のKPIを最適化しようとする。そのKPIが、会議を分け、言葉を分け、関心を分ける。分断が最も激しくなる瞬間がある。計画変更のときだ。市場が変わった。競合が動いた。顧客のニーズが変化した。そのとき、事業戦略は変わる。しかし、技術戦略は変わらない。組織体制も変わらない。なぜか。計画変更は「誰かの責任」を問うことになるからだ。変更を認めると、最初の計画が間違っていたことになる。だから、誰も変更を言い出さない。計画通りに進めて、最後に「間に合いませんでした」と言う方が、傷が浅い。分断が弱まる瞬間もある。危機のときだ。システムが落ちた。顧客からクレームが殺到した。そのとき、部門の壁は一時的に消える。全員が同じ部屋に集まり、同じ問題に向き合う。しかし、危機が去ると、また元に戻る。危機対応は例外であり、日常ではない。日常に戻れば、分断も戻る。私は何度も見てきた。月曜の経営会議で決まった「3ヶ月で新機能をリリースする」という事業戦略が、水曜の技術戦略会議で「今のアーキテクチャでは6ヶ月かかる」と判明する。金曜の組織開発会議で「その機能を作れるエンジニアがいない」と分かる。3つの会議で、3つの事実が、別々に語られる。しかし、誰も全体を見ていない。分業には理由がある。効率だ。専門家が専門領域に集中できる。会議の時間は短くなる。責任は明確になる。組織が大きくなれば、分けなければ回らない。それは、正しい。ここで、一つ認めておくべきことがある。分けることには、正当な理由がある。「統合して議論する」は綺麗だ。しかし、関係者が増えるほど会議は重くなる。境界をまたぐ話は論点が多くなり、合意形成も難しくなる。「決められない組織」になるリスクがある。市場は待ってくれない。分けて速く回す方が勝つ局面は、確かにある。私も、全員参加の会議が延々と続いて何も決まらない組織を見てきた。あれを見ると、「分けた方がいいのでは」と思う気持ちは分かる。さらに言えば、サイロには「心理的安全性の防波堤」としての機能もある。組織には「安心して話せる範囲」が必要だ。小さな範囲（サイロ）があるから、本音や問題が出る。越境を強制すると、政治が混ざって発言が萎縮し、かえって問題が隠れることがある。全員参加の場は、評価や立場を気にして「無難な話」になりがちだ。だから、私が言いたいのは「分けるな」ではない。分けることの正当性は認める。「分けたまま、境界の情報を消すな」——これが、私の言いたいことだ。——と書いて、自分でも分かっている。「境界の情報を消すな」と言うのは簡単だが、消さないためにどうするかが難しいのだ。分業は必要だ。しかし、分業の効率は「誰にとっての効率か」を問う必要がある。会議運営は効率化される。意思決定者の認知負荷は下がる。しかし、その恩恵を受けるのは会議を設計する側であり、しわ寄せを受けるのは境界で仕事をする人々だ。調整コスト、手戻り、後から判明する「言ってくれれば」。これらは、分業の効率がもたらす隠れたコストだ。しかし、その効率の代償として、境界の情報が消える。消えるのは「事実」ではない。事実は、それぞれの会議で語られている。「3ヶ月で新機能を出す」は事実だ。「今のアーキテクチャでは6ヶ月かかる」も事実だ。「その機能を作れるエンジニアがいない」も事実だ。消えているのは、事実と事実をつなぐ「前提」と「代替案」だ。「この技術的制約があるから、この事業戦略は実行不可能だ」という前提。「機能を絞れば3ヶ月で出せる」という代替案。「採用が間に合わないなら、この部分は外注する選択肢もある」というリスク回避策。これらは、どの会議の議題にもならない。境界情報が消えたことは、どうすれば分かるか。沈黙で分かる。会議で「他部門への影響は？」と聞いたとき、誰も答えられない。手戻りで分かる。開発が進んでから「これ、最初に言ってくれれば」と言われる。会議の往復で分かる。月曜に決まったことが、水曜に覆り、金曜にまた覆る。責任の押し付けで分かる。「それは技術の問題」「それは事業の判断」「それは人事の話」。押し付けが始まったら、境界情報が消えている証拠だ。境界情報が生き残るケースもある。特定の人物が媒介しているときだ。複数の会議に出席し、それぞれの文脈を理解し、翻訳できる人。しかし、その人に依存すると、その人が異動したり退職したりすると、情報は途切れる。人ではなく、仕組みで残す必要がある。事業の会議で「技術的な制約と代替案」を議題に入れる。技術の会議で「事業インパクト」を必須項目にする。形式を変えなければ、境界情報は消え続ける。事業の会議では技術の話は「水曜に」と先送りされる。技術の会議では組織の話は「金曜に」と先送りされる。誰の責任でもないから、誰も語らない。分けた瞬間に、最も大事な情報が抜け落ちている。前回、私は「おい、戦略を語れ」と書いた。戦略とは「選択」であり、「何をやらないかを決めること」だと。目標を入れる。スローガンを入れる。希望を入れる。妥協を入れる。蓋を閉じて、「戦略」というラベルを貼る。それは戦略ではない、と。syu-m-5151.hatenablog.comしかし、書き終えてから気づいた矛盾がある。「何をやらないかを決める」には、「何ができるか」を知らなければならない。技術的に可能なことを知らなければ、事業戦略は選択できない。組織の能力を知らなければ、実行可能性は判断できない。戦略を語るためには、事業・技術・組織を「分けて」考えてはいけなかったのだ。今回は、その続きを書く。事業と技術と組織と戦略を、別々に語ることの危険性について。これは、そういう自分への苛立ちから始まった文章だ。私自身、無意識に分けていた。「事業のことは経営が決める」「技術のことは自分たちで決める」「組織のことは人事が決める」。それぞれの領分を侵さない。それが「プロフェッショナル」だと思っていた。しかし、それは本当にプロフェッショナルだったのか。単に、考えることから逃げていただけではないか。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。「何をやるか」と「どうやるか」の分離多くの組織で、こんな分業が成立している。経営が「何をやるか」を決める。開発が「どうやるか」を決める。事業戦略が「What」を定義し、技術戦略は「How」を担う。依存の矢印は「事業 → 技術」の一方向。経営会議で方針が決まり、それが開発チームに「降りてくる」。開発チームは、降りてきた仕様を実装する。きれいな分業だ。責任が明確だ。経営は経営の仕事をする。開発は開発の仕事をする。この分業が成立する前提がある。経営が「Howは後で決めればよい」と信じていることだ。Whatさえ決まれば、Howは技術者がなんとかする。技術は手段であり、事業の下流にある。そう信じている。技術側にも、この分業を受け入れる理由がある。Whatを所与として受け取る方が、楽だからだ。事業戦略に口を出せば、責任が生じる。「仕様通りに作りました」と言えば、失敗しても言い訳できる。Whatに関与しないことは、責任回避の手段でもある。さらに、評価制度がこれを強化する。技術者は「技術的な成果」で評価される。事業への貢献は、評価項目に入らないことが多い。しかし、この前提には反例がある。技術発で市場を作るケースだ。iPhoneは「タッチスクリーンでアプリが動く」という技術的可能性が、事業を規定した。AWSは「サーバーを時間単位で借りられる」という技術が、クラウドビジネスを生んだ。Howが先にあり、Whatが後から来た。組織能力が先に制約になるケースもある。「AIを活用したパーソナライゼーション」という戦略があっても、データサイエンティストがいなければ実行できない。Whatを先に決めても、Howの制約で実現不可能になる。「What → How」の一方向モデルは、現実を単純化しすぎている。そして、この単純化には実害がある。この分業は、組織の可能性を無意識に狭めている。なぜか。「どうやるか」が「何ができるか」を規定するからだ。例を挙げよう。あるプロダクトチームで、新機能の開発が議論されていた。経営会議で「この機能を3ヶ月で出す」と決まった。開発チームに降りてきた。しかし、開発チームは頭を抱えた。今のアーキテクチャでは、その機能を追加するのに6ヶ月かかる。密結合なモノリスで、特定の部分だけを変更することが難しい。開発チームは「3ヶ月は無理です、6ヶ月かかります」と報告した。経営は「なんとかしろ」と言った。開発チームは無理をした。品質を犠牲にした。技術的負債が積み上がった。次の機能追加は、さらに時間がかかるようになった。これは、珍しい話ではない。むしろ、日常的に起きている。問題は、どこにあるのか。開発チームの能力不足か。経営の無理解か。どちらでもない。問題は、「何をやるか」と「どうやるか」を分けて考えたこと自体にある。もし、アーキテクチャがモジュール化されていたら。特定の機能を切り出して、独立して開発できる構造になっていたら。3ヶ月で出せたかもしれない。あるいは、1ヶ月で出せたかもしれない。つまり、技術的な選択が、事業の選択肢を規定している。逆もまた真だ。技術が事業を規定するだけでなく、事業の方向性が、技術的な選択を正当化する。「このセグメントの顧客を取りに行く」という事業判断があるからこそ、「この部分をマイクロサービスとして切り出す」という技術判断が意味を持つ。事業の方向性なしに技術判断だけがあると、「なぜそのアーキテクチャなのか」が説明できない。事業と技術は、双方向に影響し合っている。一方向の依存関係ではない。私は以前、この双方向性を無視した組織をいくつか見てきた。どれも同じパターンに陥る。アーキテクトがアーキテクチャを設計する。マネージャーが組織を設計する。それぞれが、それぞれの会議で、それぞれの論理で。アーキテクトは「理想的なシステム構成」を描く。マネージャーは「効率的なチーム編成」を考える。両者が同席することはない。数ヶ月後、問題が起きる。チームAとチームBが、同じコードベースに手を入れる必要が出てくる。しかし、チームは別の部門に所属している。コミュニケーションパスがない。マージコンフリクトが頻発する。リリースの調整に時間がかかる。「なぜこんなことになったのか」と誰かが問う。答えは単純だ。組織の設計とアーキテクチャの設計が、別々に行われたからだ。コンウェイの法則を知っている人は多い。「組織構造がアーキテクチャに影響する」。私も何度も引用してきた。しかし、知っていることと、実践することは違う。私自身、何度もこの法則を引用しておきながら、組織設計に口を出すことは避けてきた。「それは自分の領域ではない」と。結果、アーキテクチャの提案が実装されないまま終わることが何度もあった。組織が変わらなければ、アーキテクチャは変わらない。当たり前のことだ。しかし、その当たり前を、私は見て見ぬふりをしていた。戦略と組織能力の不可分性同じ問題が、戦略と組織の間にもある。多くの企業が「何をやるか（戦略）」を先に決め、「誰がどうやるか（組織能力）」を後回しにする。戦略会議で、美しいスライドが映し出される。「我々は、AIを活用した次世代プラットフォームを構築する」。参加者はうなずく。ビジョンは明確だ。方向性は正しい。しかし、誰がそれを作るのか。今の組織に、その能力があるのか。ない場合、どうやって獲得するのか。採用か。育成か。外部委託か。それには、どれくらいの時間がかかるのか。これらの問いは、戦略会議では議論されない。「それは人事の話だから」と先送りされる。結果、戦略は「願望」に留まる。実行可能性を欠いた計画になる。私も、同じ過ちを犯したことがある。あるプロジェクトの戦略会議で、私は組織能力の話を一切しなかった。「それは人事部門の仕事だ」と思っていた。技術的には正しい方向だった。市場分析も悪くなかった。しかし半年後、プロジェクトは頓挫した。実行する人がいなかったのだ。戦略は正しかった。ただ、誰もそれを実現できなかった。これは、戦略と呼べるものではない。願望だ。「こうなったらいいな」を紙に書いただけだ。「誰が、どうやって、いつまでに」を書けないものは、戦略ではなく詩だ。良い戦略には、実行可能性が組み込まれている。「何をやるか」と「誰がどうやるか」は、同時に議論されなければならない。さらに厄介なのは、事業領域によって必要な組織能力が根本的に異なることだ。例えば、エンタープライズSaaSと、HRソリューションを考えてみよう。どちらも「SaaS」だ。しかし、勝ち方が違う。エンタープライズSaaSでは、競合との機能差が短期間で縮まる。だから、高速な同質化が勝敗を分ける。競合が出した機能を、素早く追随する。実装スピードが命だ。必要なのは、高速に開発できるエンジニアリング組織だ。一方、HRソリューションでは、顧客データを活用した差別化提案が価値の源泉になる。データサイエンスや顧客理解の深さが求められる。必要なのは、データを扱える人材と、顧客の業務を深く理解するドメインエキスパートだ。同じ「SaaS」でも、必要な開発スタイル、営業モデル、採用すべき人材像が根本的に異なる。戦略を語るなら、組織能力を語らなければならない。逆に、組織能力を語るなら、戦略を語らなければならない。私はかつて、ある企業の「AI戦略」を聞いたことがある。美しいスライドだった。「機械学習を活用してパーソナライゼーションを強化する」。しかし、その会社にはデータサイエンティストが一人もいなかった。「採用する予定です」と言っていた。一年後、まだ一人も採用できていなかった。戦略だけがあって、実行する能力がない。それは戦略ではない。絵に描いた餅だ。内製か外注かという問いの本質「内製すべきか、外注すべきか」。この問いも、事業・技術・組織を分けて考えると、間違った答えを出しやすい。技術の視点だけで考えると、「今のチームにその技術がないから、外注しよう」となる。合理的に見える。今持っていない能力を、外部から借りる。効率的だ。しかし、外部委託は「今持っていない能力を一時的に借りる」行為だ。短期的な補完にすぎない。問題は、勝ち筋を支える中核能力は、外部から買えないことだ。中核能力（コアコンピタンス）は、試行錯誤を通じて組織に蓄積される。失敗から学び、改善を重ね、暗黙知が形成される。外注で失われるのは「何をしたか」ではない。コードを見れば「何をしたか」は分かる。失われるのは「なぜそうしなかったか」の記憶だ。例えば、「なぜこのAPIはRESTではなくgRPCにしたのか」。コードを見れば「gRPCを使っている」ことは分かる。しかし、「最初はRESTで作ったが、レイテンシ要件を満たせず、2週間かけてgRPCに移行した」という経緯は、ドキュメントには残りにくい。「RESTでも工夫すれば動いたが、将来のスケーラビリティを考えてgRPCにした」という判断の背景は、人の頭にしか残らない。外注先の担当者が変わったとき、この記憶は消える。次に同じ判断を迫られたとき、組織はまた2週間を失う。「今回は外注で」を繰り返すと、何が起きるか。組織の中に何も残らない。プロジェクトは完了する。成果物は納品される。しかし、それを作る能力は、組織の中にない。次に同じようなことをやろうとすると、また外注することになる。これは、競争優位の源泉を自ら手放していることに等しい。もちろん、すべてを内製する必要はない。競争優位に関係ない部分は、外注でいい。しかし、「この能力が勝ち筋を支える」と判断したなら、時間がかかっても内製すべきだ。この判断をするには、事業戦略と技術戦略と組織戦略を、同時に見る必要がある。では、「同時に見る」とは具体的にどういうことか。ここで参考になるのが、ドメイン駆動設計の考え方だ。「一緒に変わる概念は、一緒にしておけ」。新機能を追加する際、どの概念が連動して変化するか。それらを1つのまとまりとして整理すると、それがドメインになる。逆に言えば、一緒に変わる概念を別々のチームに分けると、調整コストが爆発する。事業戦略：どの市場で、どう勝つのか技術戦略：そのためにどんな技術的優位性が必要か組織戦略：その優位性を支える能力を、どう構築・維持するかこれら3つは、一緒に変わる。事業戦略が変われば、技術戦略も変わる。技術戦略が変われば、必要な組織能力も変わる。一緒に変わるものを、別々の会議で、別々の人が議論していては、正しい判断はできない。技術が事業の選択肢を創出する技術の側から見ると、この関係はより具体的に見える。優れたアーキテクチャは、事業の選択肢を増やす。例えば、こんな状況を考えてみてほしい。競合が新機能をリリースした。市場が反応している。うちも追随したい。普通なら半年かかる開発だ。しかし、うちのシステムはモジュール化されている。既存のモジュールを組み合わせれば、1ヶ月で検証できる。これは、技術的な選択が、事業の機動性を生んでいる。別の例。ある顧客セグメント向けに、機能を絞った廉価版を出したい。普通なら、別プロダクトとして作り直す必要がある。しかし、うちのシステムは機能がモジュール化されている。特定のモジュールだけを切り出して、別プランとして販売できる。これは、技術的な選択が、事業モデルの柔軟性を生んでいる。逆に、技術的負債が蓄積すると、事業の選択肢が狭まる。新機能を追加したい。しかし、コードが複雑すぎて、どこを触ればいいか分からない。影響範囲が読めない。テストがない。触ると壊れる。結果、機能追加のコストが指数関数的に増大する。「やりたいけど、できない」が増えていく。事業の選択肢が、技術的負債によって奪われていく。では、アーキテクチャとは何のためにあるのか。「コードを綺麗にする」ためではない。「事業の機動性を高める」ための戦略的投資だ。ただし、「機動性」という言葉は曖昧だ。事業側が腹落ちするには、もっと具体的に語る必要がある。事業側に説明するとき、「モジュール化しました」では伝わらない。「この投資によって、競合が新機能を出したとき、追随までの期間が6ヶ月から2ヶ月に縮まります」と言えばいい。「この機能を落としたとき、他に影響が出ないので、撤退判断が3ヶ月早くできます」と言えばいい。時間の話をする。機会損失の話をする。撤退の容易さの話をする。経営が理解できる言葉で語る。——と書いて、立ち止まる。これは本当だろうか。私はこれまで、事業の言葉で語ってきただろうか。「このコードは密結合で」「テストカバレッジが低くて」と言い続けてきたのではないか。分かりやすく整理しているが、自分ができていなかったことを、さも正解のように語るのは、どうなのか。それでも、書く。できていなかったからこそ、書く。この視点がないと、アーキテクチャの議論は「技術者の自己満足」に見えてしまう。経営からすると、「なぜそんなことに時間をかけるのか」となる。技術的負債の返済は後回しにされる。結果、事業の選択肢がどんどん狭まる。忘れられない言葉がある。以前、一緒に仕事をしたCEOが言った。「エンジニアはみんな潔癖性だ。いつも技術的負債だの、システムの書き直しだのと言っている」。正直、腹が立った。しかし、同時に、自分のことを言われている気もした。私は技術的負債の説明をするとき、どんな言葉を使っていただろうか。「このコードは密結合で」「テストカバレッジが低くて」「デプロイパイプラインが」。技術者には通じる。しかし、経営者には通じない。通じないどころか、「また技術の話か」と思われていたかもしれない。そのCEOの言葉は、不当だった。しかし、そう思われてしまう構造を作ったのは、私たちでもある。「技術的負債を返済すると、機能追加のスピードが2倍になります」と言えばよかった。「このリファクタリングで、新規市場への参入が3ヶ月早まります」と言えばよかった。技術の話を、事業の言葉で語る。それができていなかった。技術と事業を分けて考えているから、こうなる。いや、違う。私が、分けて語っていたから、こうなった。現場が握る「隠れた変数」ここまで、組織のレイヤーで話をしてきた。しかし、組織が動くのは、個人が動くからだ。組織構造を変えても、個人が動かなければ、何も変わらない。次は、個人のレイヤーで話をしよう。現場でコードを書くエンジニアは、プロダクトの「手触り」を一番知っている。「今のアーキテクチャなら、実はこんな機能も低コストで実現できる」「これだけのものを作るには、一度基盤を整備してから一気に作ったほうが速い」「この部分を先に切り出しておけば、将来の拡張が楽になる」これらは、経営会議では見えない。事業戦略のスライドには載らない。現場だけが知っている「隠れた変数」だ。しかし、多くのエンジニアは、これを声に出さない。「事業のことは経営が決めること」「自分の仕事は、決まった仕様を実装すること」「事業戦略に口を出すのは、越権行為だ」無意識に、自分の領分を技術領域に限定してしまう。事業戦略を「所与のもの」「固定された定数」と捉えてしまう。しかし、現場からのインサイトが、経営会議の決定をひっくり返すべき場面がある。「その機能、今のアーキテクチャだと6ヶ月かかりますが、この部分を先にリファクタリングすれば、3ヶ月で出せます。しかも、その後の機能追加も速くなります」これは、事業判断を変えうる情報だ。しかし、エンジニアが「自分の仕事は実装だけ」と思っていたら、この情報は経営に届かない。エンジニアが技術戦略を磨くことは、経営に対して「このルートならもっと速く、高く登れる」という登山ルートを提案することだ。降りてきた仕様をこなすだけの存在ではない。事業の未来を提案する「シンクタンク」であり、それを最速で形にする「実行部隊」だ。しかし、この意識を持つことは、簡単ではない。なぜなら、「実装担当」に留まる方が、楽だからだ。私自身、長い間「実装担当」に留まっていた。事業戦略は「上」が決めるもの。自分の仕事は、降りてきた仕様を高品質に実装すること。そう思っていた。なぜか。その方が楽だからだ。事業戦略に口を出さなければ、責任を取らなくていい。「仕様通りに作りました」と言えば、失敗しても自分のせいではない。しかし、その「楽さ」には代償がある。自分の仕事の意味を、他人に委ねることになる。「なぜこれを作るのか」を自分で理解していないまま、コードを書く。モチベーションが上がらない。「作れと言われたから作った」。それは、職人の仕事ではない。ある時期から、変えようとした。事業戦略の文書を読むようになった。経営会議の議事録を見せてもらうようになった。「なぜこの機能が必要なのか」を、実装前に質問するようになった。最初は煙たがられた。「エンジニアがビジネスに口を出すな」という空気を感じたこともある。しかし、続けていると、変わってくる。「この機能、技術的にはこうすればもっと早く出せますが、どうしますか」という会話ができるようになる。事業判断に、技術的な選択肢を提供できるようになる。降りてくる仕様を待つ存在から、仕様を一緒に作る存在に変わる。エンジニア以外にも当てはまるここまでエンジニアの話をしてきた。しかし、「現場の知見が経営に届かない」という構造は、エンジニアに限った話ではない。専門性を持つ人が、その専門性ゆえにサイロに閉じ込められる。この構造は、あらゆる専門職に当てはまる。デザイナーの話をしよう。私が一緒に仕事をしたデザイナーに、Aさんという人がいた。Aさんは、ある機能のモックを見た瞬間、「これ、誰も使わないですよ」と言った。私は「なぜ分かる」と聞いた。「導線が3クリック深い。離脱します」。彼女は正しかった。リリース後、その機能の利用率は5%だった。しかし、Aさんはその機能の企画会議に呼ばれていなかった。「UIの話は後で」と言われていたのだ。後で呼ばれたときには、もう要件は固まっていた。Aさんにできたのは、決まった要件を「見やすく」することだけだった。本質的な導線の問題は、触れられなかった。PMも、セールスも、カスタマーサクセスも、同じ構造の中にいる。それぞれが、顧客や市場の「手触り」を知っている。しかし、その知見が意思決定の場に届くことは稀だ。届いたとしても「参考情報」として扱われ、決定を覆すことはない。専門性を持つ人が、事業戦略に影響を与えられる。これが、本質的な構造だ。しかし、多くの組織では、専門性が「サイロ」に閉じ込められている。デザイナーはデザインの会議に出る。PMはPMの会議に出る。それぞれが、自分の領域だけを語る。経営会議には呼ばれない。呼ばれたとしても、「報告」のためだ。「意思決定」のためではない。分けているから、全体が見えない。見えている人の声が、届かない。不確実性を飼いならすための対話事業と技術と組織を統合して考える理由は、もう一つある。不確実性への対処だ。どんな計画も、想定通りには進まない。技術的な挑戦には、不確実性がつきまとう。「想定より時間がかかる」「パフォーマンスに問題が出る」「思った通りに動かない」。これらは、避けられない。問題は、この不確実性が顕在化したときに、どう対処するかだ。事業と技術を分けて考えていると、こうなる。技術側で問題が起きる。開発が遅れる。技術チームは「なんとかします」と言う。無理をする。品質を犠牲にする。それでも間に合わない。最後の最後で「すみません、間に合いません」と報告する。事業側は怒る。「なぜもっと早く言わなかったんだ」。これは、フィードバックループが壊れている状態だ。あるべき姿は、こうだ。技術側で問題が起きる。その事実を、即座に事業側にフィードバックする。「この技術的課題があります。対処には追加で2ヶ月かかります」。事業側は、その情報を受けて、戦略を再検討する。「2ヶ月遅れるなら、機能を絞って先に出そう」「このセグメントは後回しにして、別のセグメントを先に取りに行こう」。技術の不確実性が、事業戦略を動的に更新する材料になる。私が見てきた中で、うまく機能していたフィードバックループには、ある傾向があった。問題が判明したら、「なんとかなるかもしれない」で抱え込まずに、早めに共有していた。中間管理職を経由すると情報が歪むので、技術リーダーが直接、事業の意思決定者に伝えていた。「遅れます」だけでなく、理由と代替案もセットで。——もっとも、これがどの組織でも通用するかは分からない。「直接伝える」が政治的に難しい組織もある。それでも、報告した人を責めない文化がなければ、どんな仕組みも機能しない。「遅れます」と言った人を責めた瞬間、次から「遅れます」は聞けなくなる。責めるほど、嘘が増える。これは、事業側にも当てはまる。市場環境が変わる。競合が予想外の動きをする。顧客のニーズが変化する。これらの情報は、技術側の優先順位を変えうる。「この機能は後回しでいい。代わりに、こっちを急いでほしい」。計画は「確定したもの」ではない。「継続的に更新されるもの」だ。しかし、実際には多くの組織が計画を「約束」として扱う。「3ヶ月後にリリースすると言ったじゃないか」。この言葉が出た瞬間、計画は更新できなくなる。変えることは「約束を破ること」だから。分けて考えていると、それぞれが自分の計画を守ろうとする。変化に抵抗する。結果、組織全体が硬直化する。「計画＝約束」になってしまうのは、なぜか。一つは評価制度だ。「計画通りに達成したか」で評価される。計画を変更すると、達成率が下がる。だから、変更を避ける。計画通りに進めて、最後に「外部要因で達成できませんでした」と言う方が、評価上は有利になる。もう一つは顧客へのコミット構造だ。「この機能を3ヶ月後に提供します」と顧客に約束している。変更すると、顧客との信頼関係に影響する。だから、社内の計画変更が許されない。さらに文化の問題もある。「一度決めたことは守る」が美徳とされる組織では、計画変更は「意志が弱い」と見なされる。合理的な理由があっても、変更を言い出しにくい。これらを変えるには、評価制度を「計画通り」ではなく「成果」で評価する。顧客へのコミットを「機能」ではなく「価値」でする。文化として「計画変更は適応であり、失敗ではない」と認める。簡単ではないが、ここが変わらないと、フィードバックループは機能しない。分離が生む「責任の空白地帯」分けて考えることには、もう一つの弊害がある。責任の空白地帯が生まれる。こんな状況を想像してほしい。新プロダクトがローンチした。しかし、売れない。事業側は言う。「プロダクトの品質が悪い。技術の責任だ」。技術側は言う。「要件が曖昧だった。事業の責任だ」。組織側は言う。「人が足りなかった。採用が追いつかなかった」。誰も責任を取らない。責任が、部門の境界に落ちている。責任の空白地帯は、悪意から生まれるのではない。制度が再生産している。KPIを見てほしい。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。「事業と技術の連携」を評価する指標は、どこにもない。予算も同じだ。事業予算と技術予算は別に管理される。「境界をまたぐ問題」に使える予算は、どちらの予算からも出しにくい。稟議も同じだ。事業の稟議と技術の稟議は、別のルートを通る。「両方に関わる案件」は、どちらのルートでも通りにくい。空白地帯で起きる典型的な現象がある。なすりつけ——「それは技術の問題だ」「いや、要件が曖昧だった」。回避設計——対話を避けるために、技術的な回避策を作る（後述するフロントエンドチームのように）。冗長な仕組み——同じ情報を、事業用と技術用に別々に管理する。二重管理——両方の部門が同じことを別々にやる。私が技術顧問として入った、ある会社の話をしよう。その会社には、二つのチームがあった。ユーザー向けのウェブサイトを担当するフロントエンドチーム。社内向けAPIを運用するプラットフォームチーム。フロントエンドチームはマーケティング部門に所属していた。プラットフォームチームはIT部門に所属していた。部門が違う。上司が違う。KPIも違う。両チームの関係は、最悪だった。会話がない。Slackのやりとりも最小限。必要なときだけ、冷たいチケットが飛ぶ。私は最初、技術的な問題だと思っていた。APIが遅い。エラーが多い。パフォーマンスチューニングをすれば解決する、と。しかし、掘り下げていくと、違った。問題は、技術ではなく、人間関係だった。発端は、一年前のインシデントだった。ウェブサイトがダウンした。原因はAPIの障害。しかし、経営陣はフロントエンドチームを責めた。「なぜ監視していなかったのか」「なぜ障害を検知できなかったのか」。プラットフォームチームには、何も言わなかった。フロントエンドチームは、怒った。自分たちのせいではない。しかし、責められた。プラットフォームチームとは、もう協力したくない。そこで、彼らは技術的な解決策を選んだ。APIからデータを抽出し、自分たちのデータベースに保存する仕組みを作った。「あいつらに依存しなければ、責められずに済む」。私は、その設計図を見て、頭を抱えた。データの同期処理。キャッシュの整合性チェック。障害時のフォールバック。複雑なシステムが、一枚の紙に描かれていた。これは、技術的な解決策ではない。人と話したくないから、システムを複雑にしているだけだ。案の定、問題は悪化した。データの不整合が起きる。「商品の価格がウェブサイトと管理画面で違う」というクレームが来る。フロントエンドチームは「プラットフォームチームのデータがおかしい」と言う。プラットフォームチームは「フロントエンドの同期処理がおかしい」と言う。責任のなすり合い。問題の根本は、誰も見ていない。私は、両チームのリーダーを同じ部屋に呼んだ。最初は気まずかった。しかし、一時間ほど話していると、お互いの不満が見えてきた。フロントエンドチームは「APIが遅いから、ユーザー体験が悪くなる。それで自分たちが責められる」と言った。プラットフォームチームは「APIの改善を提案しても、予算がつかない。経営はフロントエンドばかり見ている」と言った。両チームとも、被害者意識を持っていた。そして、その被害者意識が、アーキテクチャに反映されていた。話したくないから、システムを分ける。責任を取りたくないから、境界を作る。その結果、システムは複雑になり、問題が増え、さらに話したくなくなる。悪循環だ。これは、分けて考えることの必然的な帰結だ。私はこのエピソードを振り返るたびに、あるパターンに気づく。両チームとも、悪意があったわけではない。むしろ、それぞれが合理的に行動した結果、全体としてはうまくいかなくなった。これは、私だけが見た現象ではない。組織論では「構造的無能化」と呼ばれる。組織の考えたり実行したりする能力が、合理的に下がっていく現象だ。成熟した組織にとって、ほとんど宿命のようなものだ。そのメカニズムはこうだ。まず断片化が起きる。分業化が進み、縦割りになる。次に不全化が起きる。変化の兆しを察知しても、自分の領域ではないから動けない。最後に表層化が起きる。問題の根本に手を付けられず、場当たり的な対応を繰り返す。フロントエンドチームがデータベースを作ったのは、まさにこの表層化だ。根本的な解決（チーム間の対話）を避け、技術的な回避策（自前のDB）を選んだ。事業戦略は事業部門の責任。技術戦略は技術部門の責任。組織戦略は人事部門の責任。それぞれが、自分の領域だけに責任を持つ。しかし、成果は、領域の境界で生まれる。プロダクトが売れるかどうかは、事業戦略だけでは決まらない。技術戦略だけでも決まらない。組織戦略だけでも決まらない。三つが噛み合って、初めて成果が出る。分けて考えていると、この「噛み合わせ」に誰も責任を持たない。それぞれが自分の領域を最適化しようとする。しかし、部分最適の総和は、全体最適にならない。さらに、分けることは認知負荷を増やす。私がよく見るのは、本来は不要な調整、整合性の確認、コミュニケーションのオーバーヘッドだ。フロントエンドチームがプラットフォームチームと話すためだけに、週に2時間の会議を設定する。その会議で話す内容は、同じチームなら5分で済む。分けたことで、本質的でない仕事が増える。では、誰が全体を見るのか。CEOか。CTOか。プロダクトマネージャーか。私の経験では、肩書きは関係なかった。大事なのは「誰が」ではなく「どうやって」だった。全体を見る「人」を任命するより、全体を見る「機会」を作る方が効果的だった。むしろ、肩書きに頼ると失敗する。「それはCTOの仕事だ」「それはCEOが考えること」。そう言って誰も動かない組織を、何度も見てきた。私が見た中でうまくいっていた組織には、一つの習慣があった。月曜の朝会で、各チームが「先週、他チームから聞いた話」を30秒で共有する。「営業チームから聞いたんですが、顧客がこの機能の使い方で困っているらしいです」「インフラチームから聞いたんですが、このAPIの負荷が想定の3倍らしいです」。内容は何でもいい。「他チームから聞いた」という形式が重要だ。強制的に越境させる。最初は形式的だった。「特にありません」で済ませる人もいた。しかし、3ヶ月ほど続けると変わってくる。「あ、それ、前に営業の人が言ってましたよね」という会話が、会議の外で自然に生まれる。6ヶ月後には、「他チームから聞いた話」を集めるために、意識的に他チームと話すようになる。全体を見る「人」を任命するより、全体を見る「機会」を作る方が、よほど効果的だった。ここで、一つの反論に答えておきたい。「統合しても、責任は明確にならず、むしろ曖昧になるのではないか」という懸念だ。これは、正しい。統合すると「みんなで決めた」になって、誰も責任を取らない別種の無責任が生まれる。「合議の免責」だ。境界があると「ここから先はこの責任者」と決めやすい。統合は、その明確さを失わせる。私が見てきた組織でも、「全員で議論して、全員で決めた」結果、うまくいかなかったとき誰も責任を取らなかったケースがある。だから、越境と責任の明確化は、両立させなければならない。統合して議論するが、決定は一人が行う。「みんなで決めた」ではなく、「みんなの情報をもとに、この人が決めた」にする。——言うのは簡単だ。しかし、これを実践するのは難しい。最終決裁者を決めると、「自分の意見が通らなかった」と不満を持つ人が出る。合議の方が、波風が立たない。だから、組織は合議に流れる。それでも、誰かが決めなければならない。決めた人が責任を持つ。これを曖昧にすると、分断と同じ問題が起きる。越境するということでは、どうすればいいのか。自分の領域を超えて、考える。発言する。それぞれが、自分の専門性を持ちながら、他の領域にも関心を持つ。これは、「全員がゼネラリストになれ」という話ではない。専門性は維持したまま、境界を越えて対話するということだ。ここで、よくある誤解に触れておきたい。越境を推しすぎると、専門性が薄まるのではないかという懸念だ。全員が「半分だけ分かる人」になり、技術の判断が雑になり、事業の理解が単純化され、組織の問題はスローガン化する。この懸念は正当だ。私自身、越境を意識するようになってから、技術の深い部分に割く時間が減った気がする。最新のフレームワークを追う余裕がなくなった。コードを書く時間が減った。「全体を分かる」ことと「専門が深く鋭い」ことは、トレードオフの関係にある。両方を同時に極めることはできない。だから、越境とは「専門性を捨てる」ことではない。エンジニアがビジネスを学ぶのは、ビジネスの専門家になるためではない。自分の技術的判断を、ビジネスの文脈で説明できるようになるためだ。2割の時間で、他の領域で何が起きているかを知る。それだけで、残り8割の専門領域の判断が変わる。——と言いながら、私は本当に8:2でやれているだろうか。正直、分からない。越境に時間を使いすぎて、技術の深さが失われていないか。その不安は、常にある。私が見てきた中で、うまくいっているチームは「独立」と「孤立」を混同していなかった。自分たちで決められることは自分たちで決める。しかし、他チームとの依存関係は認識している。必要なときは、ちゃんと話す。分けることは、孤立させることではない。依存関係を認識し、意図的に管理することだ。問題は、分けた瞬間に依存関係を忘れてしまうことにある。エンジニアは、エンジニアリングの専門家であり続ける。しかし、その専門性を、事業の文脈で語れるようになる。「このアーキテクチャは技術的に美しい」ではなく、「このアーキテクチャは事業の機動性を高める」と語る。事業担当は、ビジネスの専門家であり続ける。しかし、技術的な制約と可能性を理解する。「なぜできないのか」ではなく、「どうすればできるのか」を一緒に考える。ただし、ここで一つ注意がある。「越境」と聞くと、「相手の言葉で話せばいい」と考えがちだ。技術者なら経営の言葉を学び、経営者なら技術の言葉を学ぶ。それ自体は悪くない。しかし、「すべてを一つの言葉に翻訳しろ」と言っているのではない。すべてを財務言語や経営の言葉に翻訳すると、技術的な微妙なニュアンスが失われる。「このAPIのレイテンシが50msから200msに悪化する」を「ユーザー体験が悪化する」と翻訳すれば、経営には通じるかもしれない。しかし、50msと200msの差がどれほど深刻か、どのユースケースで問題になるか、という技術的な判断の根拠は消える。組織の話も同じだ。「チームの心理的安全性が低い」を「生産性が下がっている」と翻訳すれば、経営指標には載る。しかし、なぜ安全性が低いのか、誰と誰の間に問題があるのか、という組織特有の文脈は失われる。事業の話も同じだ。「この顧客セグメントは価格感度が高い」を「値下げすれば売れる」と翻訳すれば、技術チームには分かりやすい。しかし、なぜ価格感度が高いのか、競合との関係はどうか、という市場の文脈は消える。翻訳は、情報を圧縮する行為だ。圧縮すれば、必ず何かが失われる。翻訳で失われてはいけない情報がある。判断の根拠だ。「このAPIは遅い」は翻訳できる。「ユーザー体験に影響がある」と。しかし、「なぜ遅いのか」「どのユースケースで問題になるのか」「どうすれば速くなるのか」という判断の根拠は、翻訳で消えやすい。翻訳の失敗には典型パターンがある。数値の意味が変わる——「50msから200msに悪化」が「ちょっと遅くなる」に翻訳される。因果が消える——「テストがないからリリースに時間がかかる」が「リリースが遅い」に短縮される。責任が曖昧になる——「この設計判断には、こういうトレードオフがあった」が「技術的な事情」に丸められる。対策は二重記録だ。経営向けの短い翻訳と、技術や組織の原文脈を、両方残す。経営会議の資料には「ユーザー体験に影響がある」と書く。同時に、技術的な詳細は別のドキュメントに残し、参照できるようにする。翻訳は「圧縮」だが、原文を捨てる必要はない。「詳細はこちら」というリンクがあればいい。だから、越境とは「自分の言葉を捨てて、相手の言葉で話す」ことではない。自分の言葉を保ちながら、相手の言葉も理解することだ。技術者が経営の言葉を学ぶのは、自分の技術的判断を捨てるためではない。技術的判断を、経営が理解できる形で伝えるためだ。そして同時に、経営の言葉で語られた制約を、技術的な文脈に引き戻して考えるためだ。専門性を持った人たちが、それぞれの専門性をリスペクトしながら、共通のゴールに向かって越境し合う。それは、言語を統一することではない。複数の言語が交差する場所で、対話することだ。私はあるとき、同じ会議で同じ資料を見ていたエンジニアと営業が、まったく違う結論を出すのを見た。エンジニアは「これは技術的に難しい」と言い、営業は「これは売れる」と言った。同じ資料だ。同じ数字だ。しかし、見えている世界が違う。それぞれが、自分の専門性に基づいた「解釈の枠組み」で見ている。エンジニアはコードの複雑さを見ている。営業は顧客の反応を見ている。どちらも正しい。しかし、見ているものが違う。組織の中で起きている「わかりあえなさ」は、この解釈の枠組みの間に溝ができていて、しかもそのことに気づいていない状態だ。溝があることに気づかないから、「なぜ分からないんだ」「なぜ伝わらないんだ」と苛立つ。対話とは、溝を埋めることではない。溝に橋を架けることだ。溝は前提としてあり続ける。エンジニアと営業の見ている世界は、完全には一致しない。しかし、橋があれば、行き来できる。相手の世界を訪れ、自分の世界に戻ってくる。その往復が、越境だ。情報が流れる仕組み「越境せよ」と言うのは簡単だ。しかし、越境するには材料がいる。相手の世界で何が起きているかを知らなければ、質問も提案もできない。越境するためには、情報が流れる仕組みが必要だ。多くの組織で、情報は縦に流れる。経営から現場へ。現場から経営へ。しかし、横には流れにくい。技術チームで起きていることは、事業チームには見えない。事業チームで議論されていることは、技術チームには届かない。それぞれが、自分のサイロの中で仕事をしている。これを変えるには、意図的な設計が必要だ。ここで、別のアプローチに触れておきたい。「常に一緒に話す」のではなく、境界を前提にしたインターフェース（契約）を設計すればいいのではないかという考え方だ。依存関係は消せない。ならば、「越境」より「情報の受け渡しの形式化」を磨くべきだ、という主張だ。制約（SLO、人員、期限）を明文化し、変更時の通知ルールを作る。「同席して統合」より「契約で連携」の方がスケールする、と。この考え方には一理ある。いや、一理どころか、実際に機能している組織を見てきた。境界を明確にし、インターフェースを定義し、変更時の通知ルールを作る。日常的な連携は、これで十分だ。私自身、あるプロジェクトでAPI仕様書とSLOを明文化しただけで、チーム間の調整コストが激減するのを見た。ただ——問題は、インターフェースに書かれていない依存関係が発見されたときだ。「この事業判断が、技術に影響するとは思っていなかった」。そういう場面で、インターフェースは機能しない。「期限が1ヶ月延びました」という通知は届く。しかし、「なぜ延びたのか」「その延長が事業にどう影響するのか」は、インターフェースでは伝わらない。契約に書かれていない依存関係は、対話でしか発見できない。だから、どちらか一方ではない。日常的にはインターフェースで効率的に連携し、境界をまたぐ問題が発生したときや、計画変更があったときには、対話で補う。当たり前のことを言っているように聞こえる。しかし、この「当たり前」ができている組織は、驚くほど少ない。では、私自身はどうだったか。ある会社で、技術戦略会議に事業担当を呼んでもらったことがある。最初は「技術の話だから関係ない」と言われた。しかし、「アーキテクチャの選択が、3ヶ月後の機能追加速度に影響します」と説明したら、参加してくれた。その会議で、事業担当は「そんなに影響があるとは知らなかった」と言った。私は「そうなんです」と言った。そして、なぜ今まで呼ばなかったのかを考えた。面倒だったからだ。調整コストを払いたくなかったからだ。別の会社では、事業戦略のドキュメントを技術チームにも共有してもらうようにした。最初は「読まないでしょ」と言われた。確かに、読まない人もいた。しかし、読む人もいた。読んだ人が「この戦略なら、このアーキテクチャは合わない」と気づいた。それだけで、価値があった。これらは、コストがかかる。効率は下がるかもしれない。しかし、分断のコストを計算したことがあるだろうか。私が関わったあるプロジェクトでは、事業部門と技術部門の認識のずれを修正するのに、毎週2時間の会議を3ヶ月続けた。それでも修正しきれず、最終的に機能の30%を作り直した。作り直しにかかった工数は、最初から一緒に話し合っていれば発生しなかったものだ。分断は「効率的」に見える。しかし、その効率は幻想だ。後から払うコストを、先送りしているだけだ。なぜ変われないのかここまで読んで、「分かった、越境しよう」と思った人もいるかもしれない。しかし、それだけでは変わらない。私自身、何度も経験してきた。「越境すればこんなにいいことがある」と説明した。「対話すれば問題が解決する」と語った。しかし、魅力的な提案が受け入れられないのは、魅力が足りないからではなかった。相手が受け入れたくない理由があるからだ。ある会社では、「今のやり方で回っているのに、なぜ変える必要があるのか」と言われた。惰性だ。今のやり方に慣れている。変える理由がない。私はこの種の人たちを何度も見てきた。出世競争から降りて、自分のペースで仕事をこなし、昼休みには決まった仲間と弁当を食べ、定時に帰る。悪い人たちではない。むしろ、穏やかで、職場の潤滑油になっていることもある。しかし、変化の話をすると、途端に顔が曇る。「それ、本当に必要ですか」「今のままでも回っていますよね」。彼らにとって、改善か改悪かは問題ではない。変えること自体が脅威なのだ。長い時間をかけて築いた居場所。慣れた仕事の流れ。気心の知れた同僚との関係。今の自分を成り立たせているものが、壊されるかもしれない。その不安が、あらゆる変化への抵抗になる。私は最初、この人たちを「抵抗勢力」だと思っていた。説得すれば分かってくれる、と。しかし、あるとき気づいた。彼らの反応は、合理的だ。変化のコストを払うのは彼らだ。新しいやり方を覚える。慣れた関係が崩れる。評価の仕方が変わる。一方、変化の恩恵を受けるのは誰か。たいてい、変化を推進した側だ。「改革を成功させた」と評価されるのは、推進者だ。コストを払う人と恩恵を受ける人が違う。それなら、抵抗するのは当然ではないか。私が「越境しよう」と言うとき、そのコストを誰が払うのか。私ではない。現場の人たちだ。私はその認識が、ずっと欠けていた。別の会社では、「それをやる余裕がない」と言われた。労力だ。相手の言葉を学ぶコスト、会議を調整するコスト、認識のずれを修正するコスト。その余裕がない。また別の会社では、「あなたに何が分かる」と言われた。感情だ。専門性へのプライド。自分の領域に口を出されることへの反発。そして、「上から押し付けられた」と感じた瞬間、内容に関係なく拒否される。心理的反発だ。これが一番厄介だった。魅力をいくらアピールしても、これらの抵抗があれば動かない。魅力をいくら積み上げても、抵抗が1つあれば動かない。掛け算のどこかにゼロがあれば、答えはゼロだ。むしろ、魅力を強調するほど、「押し付けられている」という反発が強まることすらある。私の経験では、最も強い抵抗は心理的反発だった。「誰が言うか」の問題だ。同じ内容でも、言う人によって受け入れられ方が違う。外部の人間が言うと「現場を知らないくせに」と反発される。内部の人間が言うと「自分の領域を広げようとしている」と疑われる。拒否しているのは「内容」ではなく「手続き」や「語り手」であることが多い。内容に反論できないから、手続きに難癖をつける。「そのやり方は聞いていない」「なぜ事前に相談しなかったのか」。内容ではなく、プロセスで拒否する。抵抗を下げる最小の一歩は何か。会議に一人、他部門の人を呼ぶ。それだけでいい。最初は「オブザーバー」として。発言しなくてもいい。ただ、聞いているだけでいい。その一人がいることで、「他部門から見たらどう見えるか」を意識するようになる。それが、越境の始まりになる。だから、私は変えた。魅力を語る前に、抵抗を減らすことを考えるようになった。惰性を崩す小さな一歩を提案する。労力を下げる仕組みを作る。感情に配慮する。押し付けではなく、選択肢として提示する。それでも、うまくいかないことは多い。個人の努力では足りないここまで「越境せよ」と書いてきた。しかし、ここで立ち止まって、自分の主張を疑う必要がある。「越境せよ」は、誰に言っているのか。越境の権限がない人に言っても、負荷を増やすだけだ。越境が評価されない構造で「越境せよ」と言っても、個人を疲弊させるだけだ。私はこの記事で、分断の弊害を語り、越境の価値を語ってきた。しかし、その主張が新たな「べき論」になるリスクがある。「越境すべき」「事業の言葉で語るべき」「全体を見るべき」。これらは、個人への要求だ。しかし、なぜ越境が難しいのか。それは構造の問題だ。評価制度を見てほしい。技術者は「技術的な成果」で評価される。事業への貢献は、評価項目に入らないことが多い。越境して事業に口を出しても、評価されない。むしろ、専門性が薄まったと見なされるリスクがある。KPIを見てほしい。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。「境界をまたぐ貢献」を測る指標は、どこにもない。キャリアパスを見てほしい。技術者としてのキャリアは、技術を深めることで築かれる。越境に時間を使うと、専門性が薄まる。越境は、キャリアリスクになりうる。この構造がある限り、越境しない方が合理的だ。言い換えれば、越境は、評価されないボランティアだ。ボランティアに依存する組織は、ボランティアが疲弊した瞬間に壊れる。私が「越境せよ」と言うとき、暗黙のうちに個人の努力に頼っている。善意に頼っている。「組織のために」という献身に頼っている。しかし、善意は持続しない。献身は燃え尽きる。越境を個人に求めるだけでは足りない。越境が合理的になる構造を作らなければならない。具体的には何か。評価制度を変える。「境界をまたぐ貢献」を評価項目に入れる。技術者が事業に貢献したことを、技術的成果と同等に評価する。事業担当が技術的制約を理解し、計画に反映したことを評価する。KPIを変える。部門ごとのKPIだけでなく、「部門間の連携」を測る指標を作る。例えば、「他部門からのフィードバックを計画に反映した回数」「境界をまたぐ問題を早期に発見した件数」。キャリアパスを変える。越境経験を、キャリアの強みとして評価する。「技術も事業も分かる人」を、専門家と同等に評価する。予算を変える。「境界をまたぐ問題」に使える予算枠を作る。事業予算でも技術予算でもない、「連携予算」のようなもの。会議体を変える。月曜・水曜・金曜と分かれた会議を、定期的に統合する場を作る。全員が同席する必要はない。しかし、境界の情報が消えないための仕組みが必要だ。しかし、ここで自分の甘さを認めなければならない。私は以前、ある会社で評価制度の変更を提案したことがある。「越境的な貢献も評価項目に入れましょう」と。提案は通った。評価シートに「部門間連携」という項目が追加された。半年後、何が起きたか。項目は増えたが、行動は変わらなかった。みんな、その項目に「特になし」と書いて提出していた。形式だけが変わり、中身は何も変わらなかった。そのとき気づいた。部門を統合しても、会議体を変えても、中の人々が行動を変えなければ、結局、何も変わらない。当たり前のことだ。しかし、私は「構造を変えれば人が変わる」と信じていた。構造さえ整えれば、人は自然とその構造に沿って動くはずだ、と。甘かった。評価制度を変えても、その評価制度を運用する人が変わらなければ、何も変わらない。会議体を変えても、会議で発言する人の意識が変わらなければ、何も変わらない。構造は、行動を促すきっかけにはなる。しかし、行動を強制することはできない。では、構造を変えることに意味はないのか。そうではない。構造を変えることは必要だ。しかし、十分ではない。構造を変えると同時に、「なぜこの構造に変えるのか」を語り続ける必要がある。形式だけでなく、意味を伝える。それがなければ、新しい構造は空箱になる。——と書いて、これらを変える権限が自分にないことに気づく。私は技術顧問だ。評価制度を変える権限はない。KPIを変える権限もない。予算を変える権限もない。では、権限がない人は何もできないのか。そうではない。権限がなくても、できることはある。まず、構造の問題を言語化することだ。「越境が難しいのは、評価制度が越境を評価しないからだ」と言葉にする。問題を個人の能力や意欲の問題ではなく、構造の問題として語る。それだけで、議論の土俵が変わる。次に、小さな実験を提案することだ。評価制度全体を変えるのは難しい。しかし、「このプロジェクトでは、境界をまたぐ貢献も評価してみませんか」と提案することはできる。小さな実験が成功すれば、それを拡大できる。そして、越境できない自分を責めないことだ。越境が難しいのは、あなたの能力の問題ではない。構造の問題だ。構造が変わらない中で、できることをすればいい。ただし、ここで一つ、認めておくべきことがある。全員が越境する必要はない。越境できる人がいて、専門性を深める人がいて、それぞれが価値を出す。これも、一つの分業だ。全員が同じ行動をする必要はない。越境が得意な人が越境し、専門性を深めるのが得意な人が深掘りする。その組み合わせで、組織は機能する。「越境せよ」という主張が、「全員が越境すべき」という画一的な要求になってはいけない。越境しない人を「視野が狭い」と見なしてはいけない。専門性を深めることにも、価値がある。私がこの記事で言いたいのは、「全員が越境せよ」ではない。「越境が必要な場面で、越境できる構造を作れ」だ。私自身の反省正直に言えば、私自身、この分断に加担してきた。技術顧問として呼ばれる。「アーキテクチャについてアドバイスしてください」と言われる。私は、アーキテクチャについてアドバイスする。それが、私の仕事だと思っていた。しかし、アーキテクチャの問題は、純粋に技術的な問題であることは稀だ。組織の問題であり、事業の問題でもある。「なぜこのアーキテクチャになったのか」を掘り下げると、組織の歴史が見えてくる。「チームが分かれていたから、システムも分かれた」。「この部分は外注したから、ブラックボックスになった」。「この機能は急いで作ったから、技術的負債が溜まった」。技術の問題を、技術だけで解決しようとしても、うまくいかない。組織を変えなければ、技術は変わらない。事業の優先順位を変えなければ、技術的負債は返せない。私は、そこまで踏み込むことを避けてきた。「それは私の領域ではない」と。しかし、それは、本当に価値のある助言だったのか。踏み込まないことで、私は何を守っていたのか。関係性だ。嫌われたくない。次も呼ばれたい。だから、言いにくいことは言わない。契約範囲だ。「アーキテクチャのアドバイス」で呼ばれた。組織や事業の話は契約外だ。だから、踏み込まない。安心感だ。技術の話をしていれば、自分の専門領域にいられる。組織や事業の話をすると、自分が素人になる。だから、避ける。しかし、これらを守った結果、私のアドバイスは実行されないまま終わった。価値を提供できなかった。守ったものは、守る価値があったのか。最近は、少しずつ変えている。技術の話だけでなく、組織の話も、事業の話もする。「このアーキテクチャを実現するには、チーム編成を変える必要があります」「この技術的負債を返すには、事業側の優先順位を変える必要があります」。踏み込むときの言い方には、工夫がいる。診断として言う——「私の見立てでは、技術だけでなく組織の問題もあるように見えます」。断定ではなく、観察として伝える。仮説として言う——「もし組織構造を変えたら、アーキテクチャの変更がスムーズに進むかもしれません」。押し付けではなく、可能性として提示する。選択肢として言う——「技術だけで対処する方法と、組織も含めて対処する方法があります。どちらを選びますか」。決定権は相手に渡す。言いにくいことだ。領域を越えている。しかし、言わなければ、本当の解決にはならない。おわりに「おい、分けて語るな」。この言葉は、会議室の誰かに向けているようで、実は自分に向けている。この文章を読んでも、明日から「事業と技術と組織を統合して考えられる人」にはならない。私自身がそうだったから分かる。組織の分断は、風邪のような急性疾患ではない。薬を飲んで寝れば治る、というものではない。慢性疾患だ。劇的な手術で一気に治すことはできない。できるのは、セルフケア的なアプローチだ。小さなことから、少しずつ、継続的に変えていく。一気に問題を解決しようとは思わないこと。問題解決モードを抜け出し、対話モードで慢性疾患に向き合う。「分けて考えるな」。言葉では分かる。しかし、明日の会議で、実践できるか。経営会議で「それ、技術的にはこういう含意があります」と発言できるか。技術戦略会議で「それ、事業戦略とどう関係しますか」と質問できるか。正直、自信がない。質問した瞬間、場が凍りつくかもしれない。「また技術の話か」と思われるかもしれない。そう思うと、喉まで出かかった言葉を飲み込んでしまう。これまでも、そうだった。でも、一つだけ提案がある。次の会議で、一回だけ、こう質問してみてほしい。「この決定は、[別の領域]にどういう影響がありますか？」答えが返ってくればいい。返ってこなければ、それが発見だ。その沈黙は「分断がある」という証拠だ。私は過去に3回、この質問をした。1回目は無視された。2回目は「それは技術の話だから」と流された。3回目は違った。CTOが一瞬黙り、それから「いい質問だ」と言った。会議室の空気が変わるのを感じた。CTOはその場で技術責任者を呼んだ。30分後、経営会議に技術責任者が同席するという、その会社では初めてのことが起きた。3回目が、その会社の変化の始まりだった。1回で変わることは稀だ。しかし、質問しなければ、変わる可能性すらない。沈黙が3回続いたら、それは「この組織には分断がある」という診断結果だ。診断結果が出れば、次のアクションが見える。明日も会議がある。月曜は経営会議。水曜は技術戦略会議。金曜は組織開発会議。きれいに分かれている。整然としている。月曜の経営会議で、一つだけ、技術の話をしてみようと思う。「この事業戦略、技術的にはどういう制約がありますか」と。沈黙が流れるかもしれない。「それは水曜に」と言われるかもしれない。それでもいい。その沈黙こそが、分断の存在を証明している。そして、証明された瞬間から、修復は始まる。私が「分けて語らない」姿勢を貫くとき、最初に変わるのは何か。会議か。人か。成果物か。私の経験では、最初に変わるのは成果物だった。技術レビューに事業インパクトの項目を入れる。事業計画書に技術的制約の項目を入れる。アーキテクチャ決定記録（ADR）に「事業への影響」を必須にする。成果物の形式が変わると、それを作る過程で、自然と越境が起きる。次に変わるのは会議だ。技術の会議に事業担当を一人呼ぶ。事業の会議に技術担当を一人呼ぶ。最初はオブザーバーでいい。聞いているだけでいい。その一人がいることで、会議の空気が変わる。最後に変わるのは人だ。人の意識や行動は、簡単には変わらない。しかし、成果物の形式が変わり、会議の参加者が変わると、少しずつ変わっていく。「他の領域のことも考える」が、習慣になっていく。——と書いたが、本当にそうだろうか。成果物や会議が変わっても、人が変わらないケースも見てきた。形式だけ整えて、中身は変わらない。「事業インパクト」の欄に、適当なことを書いて終わり。そういう組織もある。それでも、形式から入るしかない。形式が変われば、少なくとも「考える機会」は生まれる。考えた結果、変わらない人もいる。変わる人もいる。変わる人が一人でもいれば、その人から広がる可能性がある。この声は、会議室の誰かにではなく、自分自身に向けられている。参考書籍アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazon戦略の要諦作者:リチャード・Ｐ・ルメルト日経BPAmazon良い戦略、悪い戦略 (日本経済新聞出版)作者:リチャード・Ｐ・ルメルト日経BPAmazon君は戦略を立てることができるか 視点と考え方を実感する４時間作者:音部大輔Amazonストーリーとしての競争戦略 Hitotsubashi Business Review Books作者:楠木 建東洋経済新報社Amazon戦略、組織、そしてシステム作者:横山 禎徳東洋経済新報社Amazon戦略のデザイン ゼロから「勝ち筋」を導き出す10の問い作者:坂田 幸樹ダイヤモンド社Amazon戦略コンサルの技術　70のスキームで身につける思考と分析力 (日本経済新聞出版)作者:長谷部 智也,河野 博日経BPAmazon確率思考の戦略論　どうすれば売上は増えるのか作者:森岡 毅,今西 聖貴ダイヤモンド社Amazonジョブ理論　イノベーションを予測可能にする消費のメカニズム作者:クレイトン・Ｍ・クリステンセンHarperCollins Children's BooksAmazon「ジョブ理論」完全理解読本 ビジネスに活かすクリステンセン最新理論作者:津田真吾,INDEEJapan翔泳社Amazonイノベーション・オブ・ライフ ハーバード・ビジネススクールを巣立つ君たちへ作者:クレイトン M.クリステンセン翔泳社Amazon繁栄のパラドクス 絶望を希望に変えるイノベーションの経済学 クレイトン・M・クリステンセン作者:クレイトン・M クリステンセンHarperCollins Children's BooksAmazonイノベーションの経済学　「繁栄のパラドクス」に学ぶ巨大市場の創り方作者:クレイトン・M・クリステンセンHarperCollins Children's BooksAmazonチームトポロジー 価値あるソフトウェアをすばやく届ける適応型組織設計作者:マシュー・スケルトン,マニュエル・パイス日本能率協会マネジメントセンターAmazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazonエンジニアリング組織論への招待　～不確実性に向き合う思考と組織のリファクタリング作者:広木 大地技術評論社Amazonプロダクトマネジメントのすべて 事業戦略・IT開発・UXデザイン・マーケティングからチーム・組織運営まで作者:及川 卓也,曽根原 春樹,小城 久美子翔泳社Amazon企業変革のジレンマ　「構造的無能化」はなぜ起きるのか (日本経済新聞出版)作者:宇田川元一日経BPAmazonイノベーションのジレンマ 増補改訂版 (Harvard Business School Press)作者:クレイトン クリステンセン翔泳社Amazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazonモチベーション革命　稼ぐために働きたくない世代の解体書作者:尾原 和啓AudibleAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraformerを使ってAWSのリソースをIaC化する]]></title>
            <link>https://zenn.dev/iorandd/articles/20260131_aws-how-to-use-terraformer</link>
            <guid isPermaLink="false">https://zenn.dev/iorandd/articles/20260131_aws-how-to-use-terraformer</guid>
            <pubDate>Fri, 30 Jan 2026 22:00:01 GMT</pubDate>
            <content:encoded><![CDATA[業務でTerraformerを使って既存のAWSリソースをTerraform管理下に移行する機会がありました。インフラに詳しいメンバーの知見を借りながら進めたのですが、その過程で学んだことが多かったので備忘として残します。https://github.com/GoogleCloudPlatform/terraformer 1. Terraformerとはクラウド上にTerraformで管理されていないリソースが残っていて困った経験は多くの人にあると思います。Terraform導入前に試験的に作ったリソースがそのまま残っている手動でコンソールから作成したリソースが把握しきれて...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIを使うのは当たり前になったけど、AIを使いたくないときがある]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</link>
            <guid isPermaLink="false">https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</guid>
            <pubDate>Thu, 29 Jan 2026 15:55:21 GMT</pubDate>
            <content:encoded><![CDATA[ChatGPTが最初に発表されたときに比べ、生成AIはいわゆるハルシネーションも減ればコーディングも得意になり、情報の調査から整理まであらゆる用途で実用的になりました。普段のくだらない疑問をSNSに垂れ流していたのが、雑にChatGPTに聞くようになりました。少し真剣に考えたい仕事のトピックや会社の情報を入れる時には、取り敢えずGeminiに意見を求めることも少なくありません。コーディングにいたっては速さも質も私ではClaudeに勝てなくなりました。そんな便利な生成AIですが、自身のキャリアや目標、解釈ではなく理解したい技術を学ぶときは安易に生成AIを利用したくないと思っています。生成AIを利用することでインスタントにそれらしい出力を得ることができますが、よく語られるようにその質は現時点のインプットに従属します。キャリアや目標のような自分の中にしか答えがなく、向き合い磨くことでしか良くならいものがあります。こういったトピックに生成AIを利用すると短期的にそれらしい出力を得られますが、頭の片隅にしかない小さなアイデアの種は簡単に押しのけられてしまいます。うわべではなく、真に理解したい論文や技術を生成AIに頼ると、理解ではなく解釈に逃げてしまい、要約の周りにある削ぎ落とされた情報を自分のものにすることができません。生成AIが幅を利かせるようになるまでは、アウトプットこそ至高でアウトプットこそ正義と考えていました。最近は生成AIを使って言葉することで消えてしまう考えや、言葉にすることでこぼれ落ちてしまうアイデアの輪郭にこそ価値があるのではないかと思うようになりました。言葉にすれば消えちゃう関係なら言葉を消せばいいやって思ってた 恐れてただけど あれ? なんかちがうかも曲名 恋愛サーキュレーション作詞 meg rock作曲・編曲 神前 暁 (MONACA)歌 千石撫子(CV:花澤香菜)一昔前のオタクが大好きな恋愛サーキュレーションの真逆だなと思った冬の日です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIでやれる時代に、それでも誰かと何かをやるということ]]></title>
            <link>https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</link>
            <guid isPermaLink="false">https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</guid>
            <pubDate>Thu, 29 Jan 2026 11:43:18 GMT</pubDate>
            <content:encoded><![CDATA[「AIを触り、感動する」という体験が個人的に一巡した感覚があるので、2026年を走り出すにあたり、思っていることを記録する。 ソロ登頂よりも、チーム敗退「うまくいかなくても、誰かと何かをやる」ということの価値が相対的に上がっているように感じる。ここでいう「誰か」は、当然GeminiでもChatGPTでもないし、「何かをやる」とは、Vibe Codingでソフトウェアを作ることでも、NotebookLMでそれっぽいスライドを生成することでもない。過去の自分の仕事の記憶。「どういうものをイメージしてるのか全く分からん」と内心上司にキレながらオフィスで過ごしたスライドレビューのワッペ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ 2026年1月 Neovim の Rust 環境を見直した]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</guid>
            <pubDate>Thu, 29 Jan 2026 04:07:42 GMT</pubDate>
            <content:encoded><![CDATA[はじめにgithub.com先週、エージェントが書いた200行のコードを開いた。move |ctx| { ... } というクロージャがあった。この ctx は何をキャプチャしているのか。所有権は移動しているのか、借用なのか。コードを読んでも分からない。コンパイルしてエラーが出るまで待つか、エージェントに「このクロージャは何をキャプチャしてる？」と聞くか。どちらも面倒だった。エージェントにすべてを任せれば楽になる、と思っていた時期がある。しかし1ヶ月ほど使って気づいた。大きな変更はエージェントが得意だ。ファイルを跨いだリファクタリング、新機能の実装、テストの追加。これらは確かにエージェントに任せた方が速い。しかし、生成されたコードの一部だけを直したいとき、エージェントに再度依頼するのは効率が悪い。「この unwrap() を ? に変えたい」「このクロージャの引数名を直したい」「この変数を別のスコープに移動したい」といった微修正は、手で直した方が速い。私は1日の開発時間のうち、7割をエージェントとの対話に、3割をNeovimでの直接作業に使っている。この3割のほとんどはコードリーディングとコードベースの理解で、実際に手で修正するのは5%程度だ。それでも、コードを「読む」環境が貧弱だと、全体の生産性が落ちる。Neovimを使う理由は、思考のスピードで編集できるからだ。ciw で単語を置換し、. で繰り返し、/ で検索して n で次へ。この一連の操作が指に染み付いていると、「直したい」と思った瞬間に直せる。エージェントに依頼を書いて、結果を待って、差分を確認する時間がない。私は Claude Code も Neovim から起動している。ターミナルで claude を叩き、エージェントと対話し、生成されたコードを Neovim で開いて確認する。すべてが同じ環境で完結する。エージェントが書いたコードを「読む」環境と、細部を「直す」環境。この両方が揃って初めて、エージェント時代の開発は快適になる。この記事では、Neovim 0.11+ と Rust の開発環境を見直した結果を紹介する。冒頭で困っていた「クロージャのキャプチャが分からない」問題を解決する Inlay Hints と、素早い修正を可能にするキーマップに重点を置いている。構成の概要私の開発環境全体については以下の記事で紹介している。本記事では Rust 関連の設定に絞って解説する。syu-m-5151.hatenablog.com私の Neovim 環境は NvChad をベースにしている。プラグイン管理には lazy.nvim を使い、Rust 関連は以下の構成だ。 コンポーネント  役割  rust-analyzer  LSP（言語サーバー）、nvim-lspconfig 経由で設定  rustaceanvim  Rust専用の拡張機能（テスト実行、マクロ展開など）  crates.nvim  Cargo.toml の依存関係管理  nvim-dap  デバッグサポート（LLDB連携）  conform.nvim  フォーマッタ（rustfmt）  mason.nvim  LSP/DAP のインストール管理 エージェント連携として claudecode.nvim も入れている。Neovim から Claude Code を起動し、生成されたコードをそのまま編集できる。以下では、この構成を「コードを読む」→「テスト・実行する」→「細部を直す」の順で紹介する。rust-analyzer と Inlay Hints でコードを理解し、rustaceanvim でテストを回し、conform.nvim でフォーマットを整える。この流れが、エージェントが書いたコードを確認・修正するワークフローと対応している。rust-analyzer の設定github.comgithub.comrust-analyzer は Rust の公式 LSP（Language Server Protocol）実装だ。LSP とは、エディタに補完、定義ジャンプ、エラー表示などの機能を提供するプロトコルで、VSCode でも Neovim でも同じ言語サーバーを使える。エージェントが生成したコードを読む際、型やライフタイム（Rust特有のメモリ管理の仕組みで、参照がいつまで有効かを示す）の情報がインラインで表示されると理解が格段に速くなる。私は nvim-lspconfig 経由で設定している。Neovim 0.11+ では vim.lsp.config API が使えるようになり、設定がシンプルになった。-- lua/configs/lspconfig.luarust_analyzer = {  settings = {    ["rust-analyzer"] = {      checkOnSave = {        command = "clippy",        extraArgs = { "--all", "--", "-W", "clippy::all" },      },      cargo = {        allFeatures = true,        loadOutDirsFromCheck = true,        buildScripts = { enable = true },      },      procMacro = {        enable = true,        attributes = { enable = true },      },      inlayHints = {        enable = true,        chainingHints = { enable = true },        typeHints = { enable = true, hideClosureInitialization = true },        parameterHints = { enable = true },        closureReturnTypeHints = { enable = "with_block" },        lifetimeElisionHints = { enable = "skip_trivial", useParameterNames = true },        maxLength = 25,        bindingModeHints = { enable = true },        closureCaptureHints = { enable = true },        discriminantHints = { enable = "fieldless" },        expressionAdjustmentHints = { enable = "reborrow" },        rangeExclusiveHints = { enable = true },      },      completion = {        autoimport = { enable = true },        postfix = { enable = true },        callable = { snippets = "fill_arguments" },        fullFunctionSignatures = { enable = true },        privateEditable = { enable = true },      },      imports = {        granularity = { group = "module" },        prefix = "self",      },      diagnostics = {        enable = true,        experimental = { enable = true },        styleLints = { enable = true },      },      semanticHighlighting = {        operator = { specialization = { enable = true } },        punctuation = { enable = true, specialization = { enable = true } },        strings = { enable = true },      },      hover = {        actions = {          enable = true,          references = { enable = true },          run = { enable = true },          debug = { enable = true },          gotoTypeDef = { enable = true },          implementations = { enable = true },        },        documentation = { enable = true, keywords = { enable = true } },        links = { enable = true },      },      typing = {        autoClosingAngleBrackets = { enable = true },      },      lens = {        enable = true,        references = { enable = true, adt = { enable = true }, enumVariant = { enable = true }, method = { enable = true }, trait = { enable = true } },        implementations = { enable = true },        run = { enable = true },        debug = { enable = true },      },      workspace = {        symbol = { search = { kind = "all_symbols" } },      },    },  },}-- Neovim 0.11+ の新しい API を使用vim.lsp.config("rust_analyzer", config)vim.lsp.enable("rust_analyzer")主な設定項目inlayHints - エージェントが生成したコードを読むとき、最も役立つのがこれだ。エディタ内にインラインで型情報やパラメータ名が表示される。特に closureCaptureHints は重宝している。Rustではクロージャ（|x| x + 1 のような無名関数）が外部の変数を使うとき、その変数を「キャプチャ」する。move |data| { ... } と書くと、data の所有権がクロージャに移動（ムーブ）する。この「何がキャプチャされているか」がエディタ上に [move: data] と表示されるようになる。エージェントが書いたクロージャを理解するのに、コンパイルエラーを待つ必要がなくなった。rangeExclusiveHints も地味に便利で、0..len が排他的（len を含まない）であることを .. の横に明示してくれる。diagnostics.experimental - 実験的な診断機能を有効化する。styleLints を有効にすると、Clippy のスタイル系リントも保存時に表示される。エージェントが生成したコードは動くが、慣用的でないことがある。この設定で「動くけど直したほうがいい」箇所が分かる。semanticHighlighting - 演算子や句読点に対してセマンティックハイライトを適用する。*self.data のような式で * が参照外しとして色付けされると、複雑な式の構造が視覚的に分かる。ただし、色数を増やしすぎるとノイズになるので、私は operator と punctuation のみ有効にしている。hover.actions - ホバー時に「Run」「Debug」「Go to Type Definition」などのアクションを表示する。エージェントが追加したテストを実行したいとき、テスト関数にカーソルを合わせて K を押すだけで実行できる。typing.autoClosingAngleBrackets - Vec< と入力すると自動的に > が補完される。エージェントが書いた型を微修正するとき、> の数を数えなくて済む。rustaceanvim の設定github.comrust-analyzer がコードを「読む」ための機能を提供するのに対し、rustaceanvim は「テスト・実行・デバッグ」のための機能を提供する。両者は補完関係にあり、rust-analyzer の LSP 機能に加えて、Rust 特有の操作（マクロ展開、テスト実行など）を追加する。rustaceanvim は rust-tools.nvim の後継だが、単なるメンテナンス引き継ぎではない。最大の違いは遅延読み込みへの対応で、.rs ファイルを開くまで何も読み込まない。私の環境では Neovim の起動時間が 120ms から 45ms に短縮された。エージェント時代に rustaceanvim が重要な理由は、「素早い確認と修正」のワークフローを支えることにある。エージェントがコードを生成したら、テストを実行し、エラーがあれば修正し、また実行する。このサイクルを <leader>rt（テスト実行）と <leader>re（エラー説明）で高速に回せる。-- lua/plugins/lang.lua{  "mrcjkb/rustaceanvim",  version = "^5",  lazy = false,  init = function()    vim.g.rustaceanvim = {      tools = {        hover_actions = { replace_builtin_hover = false },        float_win_config = { border = "rounded" },        inlay_hints = { auto = true },        code_actions = { ui_select_fallback = true },      },      server = {        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ra", function() vim.cmd.RustLsp "codeAction" end, vim.tbl_extend("force", opts, { desc = "Rust code action" }))          vim.keymap.set("n", "<leader>rd", function() vim.cmd.RustLsp "debuggables" end, vim.tbl_extend("force", opts, { desc = "Rust debuggables" }))          vim.keymap.set("n", "<leader>rr", function() vim.cmd.RustLsp "runnables" end, vim.tbl_extend("force", opts, { desc = "Rust runnables" }))          vim.keymap.set("n", "<leader>rt", function() vim.cmd.RustLsp "testables" end, vim.tbl_extend("force", opts, { desc = "Rust testables" }))          vim.keymap.set("n", "<leader>rm", function() vim.cmd.RustLsp "expandMacro" end, vim.tbl_extend("force", opts, { desc = "Expand macro" }))          vim.keymap.set("n", "<leader>rc", function() vim.cmd.RustLsp "openCargo" end, vim.tbl_extend("force", opts, { desc = "Open Cargo.toml" }))          vim.keymap.set("n", "<leader>rp", function() vim.cmd.RustLsp "parentModule" end, vim.tbl_extend("force", opts, { desc = "Parent module" }))          vim.keymap.set("n", "<leader>rj", function() vim.cmd.RustLsp "joinLines" end, vim.tbl_extend("force", opts, { desc = "Join lines" }))          vim.keymap.set("n", "<leader>rs", function() vim.cmd.RustLsp "ssr" end, vim.tbl_extend("force", opts, { desc = "Structural search replace" }))          vim.keymap.set("n", "<leader>re", function() vim.cmd.RustLsp "explainError" end, vim.tbl_extend("force", opts, { desc = "Explain error" }))          vim.keymap.set("n", "<leader>rD", function() vim.cmd.RustLsp "renderDiagnostic" end, vim.tbl_extend("force", opts, { desc = "Render diagnostic" }))          vim.keymap.set("n", "K", function() vim.cmd.RustLsp { "hover", "actions" } end, vim.tbl_extend("force", opts, { desc = "Rust hover actions" }))        end,        default_settings = {          ["rust-analyzer"] = {            cargo = { allFeatures = true },            checkOnSave = { command = "clippy" },          },        },      },      dap = {        adapter = {          type = "executable",          command = "lldb-dap",          name = "rt_lldb",        },      },    }  end,},キーマップ一覧 キー  機能  <leader>ra  コードアクション  <leader>rr  実行可能ターゲットを選択して実行  <leader>rt  テストを選択して実行  <leader>rd  デバッグ実行  <leader>rm  カーソル位置のマクロを展開  <leader>re  エラーの詳細説明を表示  <leader>rD  診断をレンダリング  <leader>rs  構造的検索置換（SSR）  <leader>rp  親モジュールに移動  <leader>rj  行を結合  <leader>rc  Cargo.toml を開く  K  ホバーアクション付きドキュメント crates.nvim の設定github.comRustでは Cargo.toml というファイルで依存ライブラリ（クレートと呼ぶ）を管理する。Node.js の package.json、Python の requirements.txt に相当するものだ。クレートには「フィーチャー」という機能のオン・オフがあり、必要な機能だけを有効にしてビルドサイズを抑えることができる。エージェントに「tokio を追加して」と頼むと、だいたい最新版を入れてくれる。しかし、フィーチャーの選択は雑なことが多い。tokio = { version = "1", features = ["full"] } と書かれていて、「full はオーバーキルだな、macros と rt-multi-thread だけでいいのに」と思うことがある。crates.nvim があれば、Cargo.toml 上でクレートにカーソルを合わせて <leader>cf を押すだけで、フィーチャー一覧がポップアップする。必要なものだけ選んで、不要なものは外す。この微調整はエージェントに頼むより、手でやった方が速い。-- lua/plugins/lang.lua{  "saecki/crates.nvim",  tag = "stable",  event = { "BufRead Cargo.toml" },  dependencies = { "nvim-lua/plenary.nvim" },  config = function()    local crates = require "crates"    crates.setup {      completion = {        cmp = { enabled = true },        crates = { enabled = true, max_results = 8, min_chars = 3 },      },      lsp = {        enabled = true,        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ct", crates.toggle, vim.tbl_extend("force", opts, { desc = "Toggle crates" }))          vim.keymap.set("n", "<leader>cr", crates.reload, vim.tbl_extend("force", opts, { desc = "Reload crates" }))          vim.keymap.set("n", "<leader>cv", crates.show_versions_popup, vim.tbl_extend("force", opts, { desc = "Show versions" }))          vim.keymap.set("n", "<leader>cf", crates.show_features_popup, vim.tbl_extend("force", opts, { desc = "Show features" }))          vim.keymap.set("n", "<leader>cd", crates.show_dependencies_popup, vim.tbl_extend("force", opts, { desc = "Show dependencies" }))          vim.keymap.set("n", "<leader>cu", crates.update_crate, vim.tbl_extend("force", opts, { desc = "Update crate" }))          vim.keymap.set("v", "<leader>cu", crates.update_crates, vim.tbl_extend("force", opts, { desc = "Update crates" }))          vim.keymap.set("n", "<leader>cU", crates.upgrade_crate, vim.tbl_extend("force", opts, { desc = "Upgrade crate" }))          vim.keymap.set("v", "<leader>cU", crates.upgrade_crates, vim.tbl_extend("force", opts, { desc = "Upgrade crates" }))          vim.keymap.set("n", "<leader>cA", crates.upgrade_all_crates, vim.tbl_extend("force", opts, { desc = "Upgrade all crates" }))          vim.keymap.set("n", "<leader>cH", crates.open_homepage, vim.tbl_extend("force", opts, { desc = "Open homepage" }))          vim.keymap.set("n", "<leader>cR", crates.open_repository, vim.tbl_extend("force", opts, { desc = "Open repository" }))          vim.keymap.set("n", "<leader>cD", crates.open_documentation, vim.tbl_extend("force", opts, { desc = "Open docs.rs" }))          vim.keymap.set("n", "<leader>cC", crates.open_crates_io, vim.tbl_extend("force", opts, { desc = "Open crates.io" }))        end,        actions = true,        completion = true,        hover = true,      },      popup = {        border = "rounded",        show_version_date = true,        max_height = 30,        min_width = 20,      },    }  end,},キーマップ一覧 キー  機能  <leader>ct  crates.nvim の表示切り替え  <leader>cr  クレート情報を再読み込み  <leader>cv  バージョン一覧をポップアップ表示  <leader>cf  フィーチャー一覧を表示  <leader>cd  依存関係を表示  <leader>cu  クレートを最新パッチバージョンに更新（ビジュアルモードで複数選択可）  <leader>cU  クレートを最新バージョンにアップグレード  <leader>cA  すべてのクレートをアップグレード  <leader>cH  クレートのホームページを開く  <leader>cR  クレートの GitHub リポジトリを開く  <leader>cD  docs.rs を開く  <leader>cC  crates.io を開く nvim-dap によるデバッグgithub.comgithub.comgithub.comgithub.comエージェントが生成したコードで「なぜこの値になるのか分からない」という場面がある。println デバッグで済むこともあるが、複雑なロジックでは変数の変化を追いたくなる。Rust のデバッグには LLDB を使う。LLDB は C/C++/Rust などのコンパイル言語向けのデバッガで、ブレークポイント（プログラムを一時停止する地点）を設定し、変数の中身を確認しながらステップ実行できる。macOS の場合は Homebrew で LLVM をインストールし、その中に含まれる lldb-dap（DAP = Debug Adapter Protocol）を使う。-- lua/plugins/lang.lua{  "mfussenegger/nvim-dap",  lazy = true,  dependencies = {    "rcarriga/nvim-dap-ui",    "nvim-neotest/nvim-nio",    "theHamsta/nvim-dap-virtual-text",  },  keys = {    { "<leader>db", function() require("dap").toggle_breakpoint() end, desc = "Toggle breakpoint" },    { "<leader>dB", function() require("dap").set_breakpoint(vim.fn.input "Breakpoint condition: ") end, desc = "Conditional breakpoint" },    { "<leader>dc", function() require("dap").continue() end, desc = "Continue" },    { "<leader>dC", function() require("dap").run_to_cursor() end, desc = "Run to cursor" },    { "<leader>di", function() require("dap").step_into() end, desc = "Step into" },    { "<leader>do", function() require("dap").step_over() end, desc = "Step over" },    { "<leader>dO", function() require("dap").step_out() end, desc = "Step out" },    { "<leader>dp", function() require("dap").pause() end, desc = "Pause" },    { "<leader>dr", function() require("dap").repl.toggle() end, desc = "Toggle REPL" },    { "<leader>dt", function() require("dap").terminate() end, desc = "Terminate" },    { "<leader>du", function() require("dapui").toggle() end, desc = "Toggle DAP UI" },    { "<leader>de", function() require("dapui").eval() end, desc = "Eval", mode = { "n", "v" } },  },  config = function()    local dap = require "dap"    local dapui = require "dapui"    -- DAP UI setup with custom layout    dapui.setup {      icons = { expanded = "▾", collapsed = "▸", current_frame = "▸" },      layouts = {        {          elements = {            { id = "scopes", size = 0.25 },            { id = "breakpoints", size = 0.25 },            { id = "stacks", size = 0.25 },            { id = "watches", size = 0.25 },          },          size = 40,          position = "left",        },        {          elements = {            { id = "repl", size = 0.5 },            { id = "console", size = 0.5 },          },          size = 10,          position = "bottom",        },      },    }    -- Virtual text for debugging    require("nvim-dap-virtual-text").setup { enabled = true, commented = true }    -- LLDB adapter    dap.adapters.lldb = {      type = "executable",      command = "/opt/homebrew/opt/llvm/bin/lldb-dap",      name = "lldb",    }    dap.configurations.rust = {      {        name = "Launch",        type = "lldb",        request = "launch",        program = function()          return vim.fn.input("Path to executable: ", vim.fn.getcwd() .. "/target/debug/", "file")        end,        cwd = "${workspaceFolder}",        stopOnEntry = false,        args = {},        runInTerminal = false,      },    }    -- Auto open/close DAP UI    dap.listeners.after.event_initialized["dapui_config"] = function() dapui.open() end    dap.listeners.before.event_terminated["dapui_config"] = function() dapui.close() end    dap.listeners.before.event_exited["dapui_config"] = function() dapui.close() end    -- Signs    vim.fn.sign_define("DapBreakpoint", { text = "●", texthl = "DapBreakpoint" })    vim.fn.sign_define("DapBreakpointCondition", { text = "●", texthl = "DapBreakpointCondition" })    vim.fn.sign_define("DapStopped", { text = "▶", texthl = "DapStopped", linehl = "DapStoppedLine" })  end,},キーマップ一覧 キー  機能  <leader>db  ブレークポイントの切り替え  <leader>dB  条件付きブレークポイント  <leader>dc  続行  <leader>dC  カーソル位置まで実行  <leader>di  ステップイン  <leader>do  ステップオーバー  <leader>dO  ステップアウト  <leader>dp  一時停止  <leader>dr  REPL の切り替え  <leader>dt  終了  <leader>du  DAP UI の切り替え  <leader>de  カーソル位置の式を評価（ビジュアルモードでも使用可） conform.nvim でのフォーマットgithub.com保存時に rustfmt を自動実行する。エージェントが生成したコードはフォーマットが崩れていることがあるので、保存するだけで整形されるのは便利だ。-- lua/plugins/lsp.lua{  "stevearc/conform.nvim",  event = "BufWritePre",  config = function()    require("conform").setup {      formatters_by_ft = {        rust = { "rustfmt", lsp_format = "fallback" },        -- 他の言語も設定可能      },      format_on_save = { timeout_ms = 500, lsp_fallback = true },    }  end,},ここまでで「コードを読む」「テスト・実行する」「細部を直す」の設定が揃った。最後に、これらのツール自体をどうインストールするかを紹介する。Mason でのツールインストールgithub.comLSP サーバーやデバッグアダプタは Mason で管理する。:MasonInstall で個別にインストールすることもできるが、ensure_installed に書いておけば自動でインストールされる。-- lua/plugins/lsp.lua{  "williamboman/mason.nvim",  opts = {    ensure_installed = {      "rust-analyzer",      "codelldb",  -- DAP adapter      -- 他の言語のツールも同様に追加    },  },},まとめこの設定を入れた翌日、またエージェントが書いたコードを開いた。move |ctx| { ... } というクロージャがある。今度は違った。クロージャの横に [move: ctx, config] と表示されている。何がキャプチャされているか、一目で分かる。コンパイルを待つ必要も、エージェントに聞く必要もない。エージェントに任せる7割と、自分で読む3割。この3割を Neovim で快適にすることが、全体の生産性につながる。rust-analyzer の Inlay Hints でコードを読み、rustaceanvim のキーマップでテストを回し、思考のスピードで細部を直す。エージェント時代だからこそ、手元のエディタは大事だ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[
		Binance Referral Bonus より		]]></title>
            <link>https://sreake.com/blog/chatgpt-slack-integration/#comment-4744</link>
            <guid isPermaLink="false">https://sreake.com/blog/chatgpt-slack-integration/#comment-4744</guid>
            <pubDate>Thu, 29 Jan 2026 02:20:31 GMT</pubDate>
            <content:encoded><![CDATA[I don't think the title of your article matches the content lol. Just kidding, mainly because I had some doubts after reading the article. https://accounts.binance.info/es-MX/register?ref=GJY4VW8W]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（後編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</guid>
            <pubDate>Thu, 29 Jan 2026 00:20:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前編を書き終えたあと、エディタを閉じて、しばらくターミナルを眺めていた。Zellijのペインが3つ並んでいる。左でVimが開き、右上でテストが走り、右下にシェルが待機している。何も起きていない。何も起きていないのに、裏では6つのスレッドが動いている。チャネルを介してメッセージが流れ、PTYがカーネルとやり取りし、VTEパーサがバイト列を解釈している。前編では設計パターンを抽出した。cat huge_log_file.logで200万行を流し込んだとき、Zellijが固まらない理由——境界付きチャネルによるバックプレッシャー。その仕組みを概念として説明した。後編では、その実装の中に入る。正直に言うと、後編は地味だ。WASMプラグインの通信プロトコル、ANSIエスケープシーケンスのパース、KDL形式のセッション永続化。どれも「知っていると便利」だが「知らなくても困らない」話かもしれない。華やかさはない。ただ、Rustで本格的なアプリケーションを書こうとしたとき、こういう地味な部分でつまずく。つまずいてから調べるか、先に知っておくか。その違いは、たぶん小さくない。Cargo Workspace構成の深掘り前編でCargo Workspaceの構造を見た。後編では、なぜこの分割になっているのかを考える。zellij-utils/を開くと、IPCの定義やエラー処理、設定ファイルのパーサーが入っている。default-plugins/* → zellij-tile → zellij-tile-utilsclient ↓ server       ↓    zellij-utils ← 共有型定義（IPC契約）zellij-utilsが双方向依存を防いでいる。clientもserverもutilsに依存するが、utilsはどちらにも依存しない。これにより、clientを変更してもserverの再コンパイルは不要になる。10万行超のコードベースでは、このビルド時間の差が開発体験に直結する。zellij-tileをSDKとして独立させた意図も見える。プラグイン開発者はサーバー実装への依存なしにビルドできる。これは「プラグインエコシステムの成長」を設計段階で意識した判断だ。後からSDKを切り出すより、最初から分けておく方が遥かに低コストになる。後編で必要な追加知識前編でPTY、チャネル、Actorモデル、WASMの基礎を説明した。後編では、さらに低レベルな概念が登場する。ここで整理しておこう。termios構造体とターミナルモードUnixのターミナルはtermios構造体で制御される。この構造体には、ターミナルの振る舞いを決めるフラグが数十個含まれている。// nixクレートでの操作let mut tio = termios::tcgetattr(fd)?;  // 現在の設定を取得termios::cfmakeraw(&mut tio);            // Raw Modeに設定termios::tcsetattr(fd, SetArg::TCSANOW, &tio)?;  // 即座に適用ターミナルには2つの主要なモードがある。Cooked Mode（カノニカルモード）- カーネルが行編集を処理する（バックスペース、Ctrl+Wなど）- Enterを押すまで入力がバッファされる- Ctrl+CでSIGINTが自動送信されるRaw Mode- すべてのキー入力がそのままアプリケーションに届く- 行編集もシグナル生成もアプリケーションの責任- ターミナルマルチプレクサには必須Zellijは起動時にRaw Modeに入り、終了時に元のモードに戻す。これを忘れると、ターミナルが「壊れた」状態になる。主要なシグナルターミナルアプリケーションが扱う主なシグナルは以下の通り。 シグナル  発生条件  用途  SIGWINCH  ウィンドウリサイズ  ターミナルサイズの再取得  SIGINT  Ctrl+C  プロセスの中断  SIGTSTP  Ctrl+Z  プロセスの一時停止  SIGTERM  killコマンド  正常終了の要求  SIGKILL  kill -9  強制終了（捕捉不可）  SIGHUP  端末切断  セッション終了  SIGCHLD  子プロセス終了  子プロセスの状態変化 ZellijはSIGWINCHを特に注意深く扱う。ウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のSIGWINCHが発生する。すべてに反応するとパフォーマンスが悪化するため、スロットリング（間引き）が必要だ。ioctl：デバイス制御ioctl（I/O Control）は、デバイスに対する特殊な操作を行うシステムコールだ。ターミナル関連では以下が重要。// ウィンドウサイズの取得ioctl(fd, TIOCGWINSZ, &mut winsize);  // Get WINdow SiZe// ウィンドウサイズの設定ioctl(fd, TIOCSWINSZ, &winsize);      // Set WINdow SiZeWinsize構造体は4つのフィールドを持つ。struct Winsize {    ws_row: u16,      // 行数    ws_col: u16,      // 列数    ws_xpixel: u16,   // ピクセル幅（Sixel画像用）    ws_ypixel: u16,   // ピクセル高さ（Sixel画像用）}TIOCSWINSZでPTYのサイズを変更すると、カーネルは子プロセスにSIGWINCHを送る。シェルはこのシグナルを受けて画面を再描画する。ANSIエスケープシーケンスの詳細前編で触れたが、後編ではより詳しく見る。CSI（Control Sequence Introducer）\x1b[  → CSI開始CSIの後にパラメータとコマンドが続く。\x1b[31m      → 前景色を赤に（SGR: Select Graphic Rendition）\x1b[10;5H   → カーソルを10行5列に移動（CUP: Cursor Position）\x1b[2J      → 画面全体をクリア（ED: Erase in Display）\x1b[?25h    → カーソルを表示（DECTCEM）\x1b[?25l    → カーソルを非表示OSC（Operating System Command）\x1b]0;title\x07  → ウィンドウタイトルを設定\x1b]8;;URL\x07   → ハイパーリンク開始DCS（Device Control String）\x1bP...ST  → 同期出力、Sixel画像などZellijはvteクレートでこれらをパースし、Performトレイトの各メソッドに振り分ける。ファイルディスクリプタとPTYUnixでは「すべてがファイル」だ。PTYもファイルディスクリプタ（FD）で表現される。let OpenptyResult { master, slave } = openpty(None, &termios)?;// master: RawFd (例: 3)// slave:  RawFd (例: 4)子プロセス（シェル）は、login_tty()で以下の処理を行う。新しいセッションを作成（setsid()）slave PTYを制御端末に設定FD 0, 1, 2（stdin, stdout, stderr）をslave PTYに接続これにより、シェルの入出力はすべてPTY経由になる。tcdrain：出力の完了待ちtcdrainは、書き込んだデータがすべて送信されるまでブロックするシステムコールだ。write(fd, bytes)?;   // バッファに書き込むtcdrain(fd)?;        // 送信完了を待つなぜ必要か。write()はカーネルのバッファに書き込んだ時点で返る。相手がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。Zellijでは、PTYへの書き込み後にtcdrain()を呼ぶ。これにより、入力が確実にシェルに届いてから次の処理に進む。これらの概念の関係[termios] ─── Raw/Cooked Mode を制御    ↓[PTY Master] ←── ioctl(TIOCGWINSZ/TIOCSWINSZ) でサイズ制御    │    │ write() + tcdrain()    ↓[PTY Slave] ─── シェルの stdin/stdout/stderr    │    │ SIGWINCH, SIGCHLD など    ↓[シグナルハンドラ] ─── スロットリング、グレースフル終了後編では、これらの概念がZellijの実装にどう現れるかを見ていく。境界付きチャネルの実装前編で「バッファサイズ50」と書いた。実際のコードを見てみよう。zellij-client/src/lib.rsを開く。// zellij-client/src/lib.rslet (send_client_instructions, receive_client_instructions): ChannelWithContext<    ClientInstruction,> = channels::bounded(50);  // バッファサイズ: 50サーバー側も同様だ。zellij-server/src/lib.rsを開く。// zellij-server/src/lib.rslet (to_server, server_receiver): ChannelWithContext<ServerInstruction> =    channels::bounded(50);なぜ50なのか。開発者ブログやissue #525を調査したが、この値を選んだ明確な理由は記載されていなかった。ただし、技術的な背景は理解できる。境界付きチャネル導入の発端はissue #525だ。PTYスレッドがプログラム出力を読み取り、無制限チャネル経由でScreenスレッドに送る。出力生成がレンダリング速度を超えると、キューが無限に成長し、メモリ使用量と入力遅延が悪化する。単純に境界付きチャネルに変えるとデッドロックのリスクがある。PTYがキューを満杯にする→WASMスレッドがレンダリング命令を送ろうとしてブロック→Screenスレッド（キューを空にすべき側）がWASMスレッドの応答待ちでブロック——この連鎖だ。解決策として、crossbeamのselect!マクロを使った選択的ルーティングが採用された。PTY→Screen間のみ境界付きチャネルでバックプレッシャーをかけ、他のコンポーネント間は無制限チャネルを維持する。50という数字は「小さすぎてスループットを落とさず、大きすぎてメモリを圧迫しない」経験的なバランス点だろう。開発者ブログによると、境界付きチャネルの導入だけでベンチマークは19秒から9秒に改善された。WASMランタイムの移行Zellijのコードを読んでいて、最も意外だったのがこの部分かもしれない。WASMランタイムを「遅い方」に移行している。普通は逆だ。前編では「wasmiを使っている」と書いた。しかし、Zellijの歴史を調べると、WASMランタイムは2度移行している。初期はWasmer、0.40.0でWasmtime、そして最新版ではWasmiに移行した（PR #4449）。zellij-server/Cargo.tomlを開く。# zellij-server/Cargo.toml[dependencies.wasmi]version = "0.51.3"default-features = falsefeatures = ["std"]なぜWasmtimeからWasmiへ移行したのか。PR #4449のディスカッションを読むと、理由が明確になる。コンパイルからインタプリタへ。Wasmtimeは.wasmファイルをJITコンパイルする。これには秒単位の時間がかかっていた。Wasmiはインタプリタ方式で、ミリ秒単位（一桁）で実行を開始できる。キャッシュ管理の排除。Wasmtime時代はコンパイル済みコードをキャッシュしていた。プラグイン開発時に「キャッシュバスティング」が必要で、これがコードの複雑さを増していた。Wasmiならキャッシュ不要だ。バイナリサイズとメモリ削減。WasmtimeはCraneliftコンパイラを含むため、バイナリサイズが大きい。Wasmiは純粋なRust実装のインタプリタで、依存関係がシンプルだ。Debianパッケージングでも、Wasmtimeの依存関係（wiggle等）がブロッカーになる可能性があったが、Wasmiなら問題ない。性能面のトレードオフ。PRテスターの報告によると、Debugビルドでは一部プラグイン（Zjstatus）の起動に約1秒の遅延が観察された。ただし、Releaseビルドでは顕著な影響がなかった。コンパイルプロファイルの調整で改善も報告されている。ステータスバーの更新に1msかかるか0.1msかかるか——ユーザーには分からない。「最速のランタイム」より「最もメンテナンスしやすいランタイム」を選んだ判断は、オープンソースプロジェクトとして合理的だ。Protocol Buffersによるプラグイン通信WASMランタイムがプラグインの「実行環境」なら、Protocol Buffersはプラグインの「通信手段」だ。プラグインとホスト間の通信はProtocol Buffersで実現されている。zellij-utils/src/plugin_api/plugin_command.protoを開く。// zellij-utils/src/plugin_api/plugin_command.protoenum CommandName {  Subscribe = 0;  Unsubscribe = 1;  SetSelectable = 2;  GetPluginIds = 3;  OpenFile = 9;  OpenTerminal = 14;  // ... 150以上のコマンド}150以上のコマンド。前編で見たScreenInstructionの100バリアントを超えている。プラグインはUI操作、ファイル操作、ネットワーク、他プラグインとの通信など、ホストの機能に広くアクセスできるからだ。なぜProtocol Buffersなのか。WASMとホストの間でデータを受け渡すには、シリアライズが必要だ。JSONでもMessagePackでも良いが、Protocol Buffersには以下の利点がある。スキーマがドキュメントになる: .protoファイルを見れば、プラグインAPIの全体像が分かる後方互換性: フィールドの追加・削除が安全にできる型安全: コード生成により、シリアライズ/デシリアライズのミスを防げるzellij-tile/src/lib.rsのマクロを見ると、Protocol Buffersがどう使われているか分かる。#[no_mangle]fn load() {    STATE.with(|state| {        let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();        // Protocol Buffersをデシリアライズして設定を取得    });}プラグインは標準入力からProtocol Buffersを読み、標準出力に書き込む。WASM境界を越えるのは単なるバイト列だ。シンプルだが、型安全性は失われない。パーミッションシステムの設計思想プラグインは16種類のパーミッションから必要なものを要求する。zellij-utils/src/plugin_api/plugin_permission.protoを開く。// zellij-utils/src/plugin_api/plugin_permission.protoenum PermissionType {  ReadApplicationState = 0;      // ペイン・タブ・UI状態の読み取り  ChangeApplicationState = 1;    // ペイン・タブ・UIの変更  OpenFiles = 2;                 // ファイルを開く  RunCommands = 3;               // コマンド実行  OpenTerminalsOrPlugins = 4;    // ターミナル/プラグインを開く  WriteToStdin = 5;              // ペインへの入力  WebAccess = 6;                 // HTTPリクエスト  ReadCliPipes = 7;              // CLIパイプの読み取り  MessageAndLaunchOtherPlugins = 8;  // 他プラグインとの通信  Reconfigure = 9;               // 設定変更  FullHdAccess = 10;             // ファイルシステム完全アクセス  StartWebServer = 11;           // Webサーバー起動  InterceptInput = 12;           // 入力のインターセプト  ReadPaneContents = 13;         // ペイン内容の読み取り  RunActionsAsUser = 14;         // ユーザーとしてアクション実行  WriteToClipboard = 15;         // クリップボードへの書き込み}このパーミッションモデルは「悪意あるプラグイン」より「バグのあるプラグイン」を想定している——と私は読んだ。考えてみてほしい。悪意あるプラグインを防ぎたいなら、ユーザーに許可を求めるUIは逆効果だ。ユーザーは深く考えずに「許可」を押す。AndroidやiOSの経験から、我々はそれを知っている。Zellijのパーミッションモデルが防いでいるのは、むしろ「うっかりファイルを消してしまうバグ」や「意図せずネットワークにアクセスしてしまう問題」だ。FullHdAccessを持つプラグインがファイルを誤削除するリスクを、ユーザーが明示的に受け入れる——そういう設計だと理解している。許可されたパーミッションはPermissionCacheにプラグイン名ごとに保存され、次回起動時は再確認されない。これも「毎回聞かれると面倒」という実用性を優先した判断だ。ANSIエスケープシーケンスのパースここまでプラグインの実行環境（WASM）、通信手段（Protocol Buffers）、安全性（パーミッション）を見てきた。ここからはサーバー側の話に移る。シェルの出力をどう画面に変換するか——その起点がANSIエスケープシーケンスのパースだ。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。use vte::{Params, Perform};impl Perform for Grid {    // 通常文字の描画    fn print(&mut self, c: char) {        self.add_character(c);    }    // C0/C1制御文字（改行、タブなど）    fn execute(&mut self, byte: u8) {        match byte {            b'\n' => self.move_cursor_down(1),            b'\r' => self.move_cursor_to_beginning_of_line(),            b'\t' => self.advance_to_next_tabstop(),            _ => {}        }    }    // CSIシーケンス（カーソル移動、色設定など）    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    _ignore: bool, action: char) {        // \x1b[10;2H → カーソル移動        // \x1b[36m → 色設定    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。Alacrittyと同じクレートを使っている点も興味深い。ターミナルエミュレータの世界では、vteがデファクトスタンダードになりつつある。差分レンダリングの実装VTEパーサがANSIエスケープシーケンスを解釈し、Gridが更新される。次の問題は、そのGridをどう効率的に画面へ反映するかだ。全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。// zellij-server/src/output/mod.rspub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更行インデックス    pub should_update_all_lines: bool,    styled_underlines: bool,}impl Default for OutputBuffer {    fn default() -> Self {        OutputBuffer {            changed_lines: HashSet::new(),            should_update_all_lines: true,  // 初回は全画面レンダリング            styled_underlines: true,        }    }}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn clear(&mut self) {        self.changed_lines.clear();        self.should_update_all_lines = false;    }}なぜ「行レベル」であり「セル単位」ではないのか。セル単位の差分追跡も技術的には可能だ。しかし、ターミナルの出力は行単位で更新されることが多く、1文字だけ変わるケースは稀だ。セル単位にすると、追跡のオーバーヘッドが差分レンダリングの利点を上回る可能性がある。HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。シンプルだが効果的だ。パフォーマンス最適化の成果ここまで見てきた境界付きチャネルと差分レンダリングは、個別には小さな改善に見える。しかし、組み合わせると効果は大きい。開発者ブログによると、以下の最適化によりcat bigfileのベンチマークが大幅に改善された。ベンチマーク条件:- 測定コマンド: hyperfine --show-output "cat /tmp/bigfile"（10回実行の平均）- ファイルサイズ: 200万行- ペインサイズ: 59行 × 104列 段階  時間  最適化前  19.175秒 ± 0.347秒  境界付きチャネル導入後  9.658秒 ± 0.095秒  全最適化後  5.270秒 ± 0.027秒  tmux（参考）  5.593秒 このベンチマークではtmuxと同等以上のパフォーマンスを達成している。ただし、マシンスペックやtmuxのバージョン・設定は記載されていない。実環境での性能はワークロードや設定に依存するため、「Zellijの方が常に速い」とは言えない。重要なのは、適切な最適化によってRust製の新参者が30年の歴史を持つtmuxと同等のパフォーマンスを達成できた点だ。主な最適化は以下の4つだ。境界付きチャネルによるバックプレッシャー: 19秒→9秒の最大の貢献Vecの事前確保: Vec::with_capacity()で再確保を削減Unicode幅のキャッシュ: 絵文字などの幅計算を毎回やらない行レベル差分追跡: 変更行のみを再描画どれも「当たり前」の最適化だ。しかし、当たり前のことを愚直にやるのは難しい。ターミナル特有の問題への対処ここまで、チャネル、WASM、Protocol Buffers、ANSIパーサー、差分レンダリングと見てきた。どれも汎用的なパターンの応用だ。ここからは違う。ターミナルエミュレータでなければ出会わない問題ばかりだ。ソースコードを読んでいて、一番面白かったのはこのあたりだった。RcCharacterStyles: 16バイトに収めるメモリ効率化ターミナルの文字列バッファは数百万の要素を持つ。1文字あたりのメモリサイズがパフォーマンスに直結する。zellij-server/src/panes/terminal_character.rsを開く。// Enum Niche Optimization: 2つのvariantしかないため、ポインタサイズと同じ8バイトに収まる#[derive(Clone, Debug, PartialEq)]pub enum RcCharacterStyles {    Reset,    Rc(Rc<CharacterStyles>),}// compile-time assertionでメモリサイズを保証#[cfg(target_arch = "x86_64")]const _: [(); 8] = [(); std::mem::size_of::<RcCharacterStyles>()];// TerminalCharacter全体も16バイト#[cfg(target_arch = "x86_64")]const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];// thread_local!でデフォルトスタイルをキャッシュし、メモリ再利用thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}impl Default for RcCharacterStyles {    fn default() -> Self {        RC_DEFAULT_STYLES.with(|s| s.clone())  // thread_localから共有参照を取得    }}compile-time assertionが面白い。const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];は、TerminalCharacterのサイズが16バイトでなければコンパイルエラーになる。将来フィールドを追加したとき、意図せずメモリサイズが増えることを防ぐ。Enum Niche Optimization + Reference Counting + thread_localの組み合わせで、リセット状態の文字スタイルをメモリ効率的に管理している。型安全性を失わずに、大規模なパフォーマンス最適化を実現しているのが印象的だ。PaneResizer: Cassowary制約ソルバーによるペイン配置ペインのレイアウト計算は、意外と難しい。「固定サイズのペイン」と「パーセンテージ指定のペイン」が混在し、ウィンドウリサイズ時に全体を再計算する必要がある。zellij-server/src/panes/tiled_panes/pane_resizer.rsを開く。use cassowary::{    strength::{REQUIRED, STRONG},    Expression, Solver, Variable,    WeightedRelation::EQ,};pub struct PaneResizer<'a> {    panes: Rc<RefCell<HashMap<PaneId, &'a mut Box<dyn Pane>>>>,    vars: HashMap<PaneId, Variable>,    solver: Solver,}// 制約を設定: 「固定サイズペイン」と「パーセンテージペイン」の両方に対応fn constrain_spans(space: usize, spans: &[Span]) -> HashSet<cassowary::Constraint> {    let mut constraints = HashSet::new();    // 全ペインの合計サイズは、利用可能なスペースと等しい（REQUIRED強度）    let full_size = spans        .iter()        .fold(Expression::from_constant(0.0), |acc, s| acc + s.size_var);    constraints.insert(full_size.clone() | EQ(REQUIRED) | space as f64);    // 固定サイズはREQUIRED、パーセンテージはSTRONGで制約    for span in spans {        match span.size.constraint {            Constraint::Fixed(s) => constraints.insert(span.size_var | EQ(REQUIRED) | s as f64),            Constraint::Percent(p) => constraints                .insert((span.size_var / new_flex_space as f64) | EQ(STRONG) | (p / 100.0)),        };    }    constraints}// 丸め誤差の分配: error.signum()で±1ずつペインサイズを調整for span in flex_spans {    rounded_sizes        .entry(span.size_var)        .and_modify(|s| *s += error.signum());    error -= error.signum();}Cassowaryは線形計画法を使った制約ソルバーだ。元々はmacOSのAuto Layoutに使われていたアルゴリズムで、それをペインレイアウトに応用している。REQUIREDとSTRONGの強度で優先度を管理するのが賢い。固定サイズのペインは絶対に守られ、パーセンテージ指定のペインは「できるだけ守る」という柔軟性を持つ。error.signum()で丸め誤差を1ピクセルずつ分配するのも秀逸だ。浮動小数点の計算結果を整数に変換すると、どうしても誤差が出る。その誤差を均等にばらまくことで、ギャップやオーバーラップを回避している。HyperlinkTracker: カーソルジャンプ検出によるURL追跡ターミナルでURLをクリック可能にするには、「文字列がURLかどうか」を検出する必要がある。しかし、ターミナルは1文字ずつ出力されるため、URLの開始と終了を正確に把握するのは難しい。zellij-server/src/panes/hyperlink_tracker.rsを開く。pub struct HyperlinkTracker {    buffer: String,    cursor_positions: Vec<HyperlinkPosition>,  // 各文字のカーソル位置を記録    start_position: Option<HyperlinkPosition>,    last_cursor: Option<HyperlinkPosition>,    // カーソルジャンプ検出用}// カーソルが「連続的に移動していない」ことを検出fn should_reset_due_to_cursor_jump(&self, current_pos: &HyperlinkPosition) -> bool {    if let Some(last_pos) = &self.last_cursor {        let is_contiguous =            // 同一行の隣（通常の文字出力）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x + 1) ||            // 改行（行の折り返し）            (current_pos.y == last_pos.y + 1 && current_pos.x == 0) ||            // 同じ位置（上書き）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x);        !is_contiguous    } else {        false    }}カーソルジャンプ = URLの中断と判定するのが面白い。例えば、https://example.comと出力される途中で、プロンプトに戻るためにカーソルが左上にジャンプしたら、URLは完了したと見なす。複数行にまたがるURLや、ターミナルの折り返しにも対応している。Sixel画像: 負の座標とオーバーラップ判定Sixelは、ターミナル内に画像を表示するための古い規格だ。Zellijはこれをサポートしているが、スクロール時の挙動が複雑になる。zellij-server/src/panes/sixel.rsを開く。#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Hash)]pub struct PixelRect {    pub x: usize,    pub y: isize,  // 負の値対応！スクロールバッファの上部に消えた画像    pub width: usize,    pub height: usize,}// 新しい画像が古い画像を完全に覆った場合、古い画像を削除for (image_id, pixel_rect) in &self.sixel_image_locations {    if let Some(intersecting_rect) = pixel_rect.intersecting_rect(&image_size_and_coordinates) {        if intersecting_rect.x == pixel_rect.x            && intersecting_rect.y == pixel_rect.y            && intersecting_rect.height == pixel_rect.height            && intersecting_rect.width == pixel_rect.width        {            self.image_ids_to_reap.push(*image_id);  // 完全に覆われた→削除予定        }    }}y: isizeが興味深い。スクロールで画像がバッファの上部に消えると、yが負の値になる。usizeではなくisizeにすることで、この状況を型で表現している。完全にオーバーラップした画像は自動でメモリ解放される。Sixel画像は計算コストが高いため、不要な画像を積極的に削除するのは合理的だ。ダブルクリック検出: Doherty Thresholdマウスのダブルクリック検出には、時間閾値が必要だ。ZellijはDoherty Thresholdという値を使っている。zellij-server/src/panes/grid.rsを開く。const CLICK_TIME_THRESHOLD: u128 = 400;  // Doherty Thresholdimpl Click {    pub fn record_click(&mut self, position: Position) {        let click_is_same_position = self.position_and_time            .map(|(p, _t)| p == position)            .unwrap_or(false);        let click_is_within_time_threshold = self.position_and_time            .map(|(_p, t)| t.elapsed().as_millis() <= CLICK_TIME_THRESHOLD)            .unwrap_or(false);        if click_is_same_position && click_is_within_time_threshold {            self.count += 1;        } else {            self.count = 1;        }        if self.count == 4 {            self.reset();  // 3クリックまで（単語選択、行選択、段落選択）        }    }}400msという数字は、1982年のDoherty & Kelisky論文に由来する。「ユーザーがシステムの反応を待てる限界」とされる時間だ。3クリックまで対応しているのも面白い。1クリック=カーソル移動、2クリック=単語選択、3クリック=行選択。4クリック目でリセットされる。クライアント側のターミナル制御PTYの実装に入る前に、クライアント側の処理を見ておく必要がある。ユーザーのキー入力がサーバーに届くまでの道筋——つまり、データフローの入口だ。Raw Mode vs Cooked Mode追加知識セクションでtermios構造体とRaw Modeの概念を紹介した。Zellijの実装では、具体的にどうRaw Modeに入るのか。通常のターミナルは「Cooked Mode」で動作する。カーネルが行編集（バックスペース、Ctrl+Wなど）を処理し、Enterで1行ずつアプリケーションに渡す。Ctrl+Cを押すとSIGINTが送られる。Zellijはこれを無効にする必要がある。すべてのキー入力を自分で処理したいからだ。zellij-client/src/os_input_output.rsを開く。fn into_raw_mode(pid: RawFd) {    let mut tio = termios::tcgetattr(pid).expect("could not get terminal attribute");    termios::cfmakeraw(&mut tio);    match termios::tcsetattr(pid, termios::SetArg::TCSANOW, &tio) {        Ok(_) => {},        Err(e) => panic!("error {:?}", e),    };}fn unset_raw_mode(pid: RawFd, orig_termios: termios::Termios) -> Result<(), nix::Error> {    termios::tcsetattr(pid, termios::SetArg::TCSANOW, &orig_termios)}cfmakeraw() は以下を無効にする。ECHO: 入力文字のエコーバックICANON: 行単位の入力（カノニカルモード）ISIG: Ctrl+C/Ctrl+Zによるシグナル生成IXON/IXOFF: ソフトウェアフロー制御（Ctrl+S/Ctrl+Q）TCSANOW は「今すぐ適用」を意味する。TCSADRAIN（出力完了後）やTCSAFLUSH（バッファ破棄）もあるが、入力処理では即座の適用が必要だ。元のtermiosを保存しておき、終了時に復元する。これを忘れると、Zellij終了後にターミナルが壊れた状態になる。SIGWINCHのスロットリングターミナルウィンドウをリサイズすると、OSはSIGWINCHを送る。問題は、GUIウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のシグナルが発火することだ。fn handle_signals(&self, sigwinch_cb: Box<dyn Fn()>, quit_cb: Box<dyn Fn()>) {    let mut sigwinch_cb_timestamp = time::Instant::now();    let mut signals = Signals::new(&[SIGWINCH, SIGTERM, SIGINT, SIGQUIT, SIGHUP]).unwrap();    for signal in signals.forever() {        match signal {            SIGWINCH => {                // SIGWINCHコールバックをスロットリング                if sigwinch_cb_timestamp.elapsed() < SIGWINCH_CB_THROTTLE_DURATION {                    thread::sleep(SIGWINCH_CB_THROTTLE_DURATION);                }                sigwinch_cb_timestamp = time::Instant::now();                sigwinch_cb();            },            SIGTERM | SIGINT | SIGQUIT | SIGHUP => {                quit_cb();                break;            },            _ => unreachable!(),        }    }}SIGWINCH_CB_THROTTLE_DURATIONは50msだ。リサイズイベントが来ても、前回から50ms経っていなければ待機する。これにより、毎秒数十回のレンダリングを防ぐ。50msという値は経験則だ。人間が「遅延」と感じる閾値（100ms）より短く、ターミナルが処理できる頻度（60fps = 16ms）より長い。同期出力（Synchronized Output）最新のターミナルエミュレータは「同期出力」をサポートしている。複数の出力をバッファリングし、まとめて画面に反映する機能だ。let synchronised_output = match os_input.env_variable("TERM").as_deref() {    Some("alacritty") => Some(SyncOutput::DCS),    _ => None,};// レンダリング時if let Some(sync) = synchronised_output {    stdout.write_all(sync.start_seq()).expect("cannot write to stdout");}stdout.write_all(output.as_bytes()).expect("cannot write to stdout");if let Some(sync) = synchronised_output {    stdout.write_all(sync.end_seq()).expect("cannot write to stdout");}DCS（Device Control String）で出力を囲む。ターミナルはDCS開始からDCS終了までの出力をバッファリングし、終了シーケンスを受け取った時点でまとめて描画する。これにより、レイアウト変更時の「ちらつき」が消える。中間状態（ペインが1つだけ描画された状態など）が画面に表示されない。現時点ではAlacrittyのみ対応だが、今後他のターミナルにも拡大されるだろう。Kitty Keyboard Protocol従来のターミナルは、Shift+F1とF13を区別できなかった。どちらも同じエスケープシーケンスを送るからだ。Kitty Keyboard Protocolはこの問題を解決する。zellij-client/src/stdin_handler.rsを開く。loop {    match os_input.read_from_stdin() {        Ok(buf) => {            // まずKitty Keyboard Protocolを試す            if !explicitly_disable_kitty_keyboard_protocol {                match KittyKeyboardParser::new().parse(&buf) {                    Some(key_with_modifier) => {                        send_input_instructions.send(...).unwrap();                        continue;                    },                    None => {},                }            }            // フォールバック: 標準のtermwiz InputParser            input_parser.parse(&buf, |input_event| { ... }, false);        },        // ...    }}Kitty Keyboard Protocolが使えるなら使い、使えなければ従来のANSIエスケープシーケンスにフォールバックする。この二段構えにより、古いターミナルでも動作しつつ、新しいターミナルでは拡張機能を活用できる。セッション切り替え時のSTDINバッファリングZellijは複数のセッションを持てる。セッション間を切り替えるとき、STDINの所有権を移す必要がある。fn read_from_stdin(&mut self) -> Result<Vec<u8>, &'static str> {    let session_name_at_calltime = { self.session_name.lock().unwrap().clone() };    let mut buffered_bytes = self.reading_from_stdin.lock().unwrap();    match buffered_bytes.take() {        Some(buffered_bytes) => Ok(buffered_bytes),        None => {            let stdin = std::io::stdin();            let mut stdin = stdin.lock();            let buffer = stdin.fill_buf().unwrap();            let length = buffer.len();            let read_bytes = Vec::from(buffer);            stdin.consume(length);            // セッションが変わったら、読んだバイトをバッファに戻す            let session_name_after_reading_from_stdin =                { self.session_name.lock().unwrap().clone() };            if session_name_at_calltime.is_some()                && session_name_at_calltime != session_name_after_reading_from_stdin            {                *buffered_bytes = Some(read_bytes);                Err("Session ended")            } else {                Ok(read_bytes)            }        },    }}問題: 旧セッションのスレッドがSTDINでブロックしている間に、新セッションがSTDINを必要とする。解決策: 読み取り前後でセッション名を比較する。セッションが変わっていたら、読んだバイトをバッファに保存し、新セッションのスレッドがそれを取得できるようにする。これはエッジケースだが、マルチセッション対応には必須の処理だ。PTY（疑似端末）の実装詳細ここまで見てきた境界付きチャネル、VTEパーサ、差分レンダリング、compile-time assertion——これらはすべて、PTYという土台の上に乗っている。チャネルはPTYからの出力を運び、VTEパーサはPTYが吐いたバイト列を解釈し、差分レンダリングはその結果を画面に描く。個別のパターンを追いかけてきたが、ここで全体が合流する。ターミナルマルチプレクサの核心部分であるPTY処理を深掘りする。ここがZellijの「心臓部」だ。そもそもTTYとPTYとは何かTTY（TeleTYpewriter）は、歴史的にはテレタイプ端末を指す。現代では「ターミナルデバイス」の総称として使われる。/dev/ttyや/dev/tty1がこれにあたる。PTY（Pseudo-TTY）は「疑似端末」だ。物理的な端末がなくても、ソフトウェア的にターミナルをエミュレートする仕組み。sshやtmux、そしてZellijはPTYを使っている。PTYはマスターとスレーブのペアで構成される。[Zellij Server] ←→ [PTY Master] ←→ [PTY Slave] ←→ [Shell/App]                    (制御側)         (端末側)マスター側: Zellijが持つ。ここに書き込むと、スレーブ側のSTDINに届く。スレーブの出力はここから読めるスレーブ側: シェル（bash/zsh）が持つ。通常のターミナルと同じように振る舞うシェルから見ると、スレーブPTYは「本物のターミナル」に見える。ttyコマンドを実行すると/dev/pts/0のようなパスが返る。これがスレーブPTYだ。Zellijがペインを作るたびに、新しいPTYペアが生成される。3ペインあれば、3つのPTYマスターをZellijが管理している。なぜPTYが必要なのか「パイプでいいのでは？」と思うかもしれない。しかし、パイプとPTYには決定的な違いがある。ジョブコントロール: PTYは「制御端末」として機能する。Ctrl+Zでプロセスを停止したり、fg/bgで制御したりできるのは、PTYがあるからだ。パイプにはこの機能がないウィンドウサイズ: PTYはサイズ（行数・列数）を持つ。$COLUMNSや$LINESはPTYから取得される。パイプにはサイズの概念がない行編集: シェルは「端末があるかどうか」で挙動を変える。isatty()がtrueを返すと、プロンプトを表示し、行編集を有効にする。パイプ経由だとこれが無効になるシグナル: Ctrl+CでSIGINTを送れるのは、PTYが「フォアグラウンドプロセスグループ」を管理しているからだつまり、PTYなしには「ターミナルらしい体験」が成り立たない。PTYの作成フローzellij-server/src/os_input_output.rsを開く。fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // Zellij側（ホスト）    let pid_secondary = open_pty_res.slave;   // シェル側（子プロセス）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ", VERSION)            .env("ZELLIJ_SESSION_NAME", &*SESSION_NAME)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // 子プロセスの終了を監視するスレッドを起動    std::thread::spawn({        move || {            child.wait().ok();            let exit_status = child.try_wait().ok().flatten().and_then(|e| e.code());            quit_cb(PaneId::Terminal(terminal_id), exit_status, cmd);        }    });    Ok((pid_primary, child.id() as RawFd))}openpty() はマスターとスレーブの2つのファイルディスクリプタを作る。Zellijはマスター側を持ち、シェル（bashやzsh）はスレーブ側を持つ。login_tty() は伝統的なUnix関数で、3つのことをする。setsid() で新しいセッションを作成スレーブPTYをcontrolling terminalに設定dup2() でstdin/stdout/stderrをスレーブに接続close_fds::close_open_fds(3, &[]) が面白い。ファイルディスクリプタ3以降を全て閉じる。これにより、親プロセスから継承した不要なFDがリークしない。環境変数の設定も注目に値する。ZELLIJ_PANE_IDを設定することで、子プロセス側から「自分がどのペインで動いているか」を知ることができる。シェルスクリプトやプラグインで使える。読み取りと書き込みの分離PTYへの読み書きは、別々のスレッドで行われる。なぜか。zellij-server/src/terminal_bytes.rsを開く。pub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    ))                    .await?;                },            }        }        // ループ終了後、最終レンダリングを要求        let _ = self.async_send_to_screen(ScreenInstruction::Render).await;        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのサイズだ。大量のログ出力に対応するため。Ok(0)とErrの区別が重要。Ok(0)はEOF（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス正常終了時にエラーログが出てしまう。zellij-server/src/pty_writer.rsを開く。pub(crate) fn pty_writer_main(bus: Bus<PtyWriteInstruction>) -> Result<()> {    loop {        let (event, _err_ctx) = bus.recv()?;        match event {            PtyWriteInstruction::Write(bytes, terminal_id) => {                if let Some(raw_fd) = bus                    .os_input                    .as_ref()                    .and_then(|os_input| os_input.get_terminal_id_from_fd(terminal_id))                {                    let mut f = unsafe { File::from_raw_fd(*raw_fd) };                    if f.write_all(&bytes).is_ok() {                        let _ = f.flush();                    }                    std::mem::forget(f);  // FDを閉じないようにする                }            },            PtyWriteInstruction::Exit => break,        }    }    Ok(())}読み取りと書き込みを分離する理由は、デッドロック回避だ。Vimのようなプログラムは、STDINからの入力を待ちながらSTDOUTに出力する。もし同じスレッドで読み書きをすると、「Vimが入力を待っている」「Zellijが出力を待っている」という状態でデッドロックになる可能性がある。std::mem::forget(f) も興味深い。File::from_raw_fd()で作ったFileは、dropされるとFDが閉じてしまう。forget()でdropを防いでいる。forgetという名前は不穏だ。普通、忘れることは悪いことだ。しかしRustの所有権モデルでは、意図的に忘れることが正しい選択になる場合がある。FDを閉じたくないなら、Fileがdropされることを忘れさせる。記憶と忘却の関係が、ここでは逆転している。tcdrainによるフロー制御書き込みスレッドには、もう一つ重要な処理がある。PtyWriteInstruction::Write(bytes, terminal_id) => {    os_input        .write_to_tty_stdin(terminal_id, &bytes)        .with_context(err_context)        .non_fatal();    os_input        .tcdrain(terminal_id)  // ここ        .with_context(err_context)        .non_fatal();},tcdrain() は、書き込んだデータが全て送信されるまでブロックする。なぜこれが必要か。PTYにはカーネル内部にバッファがある。write()はバッファに書き込んだ時点で返る。しかし、相手側（シェル）がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。これにより、次の書き込みが前の書き込みを追い越すことを防ぐ。fn tcdrain(&self, terminal_id: u32) -> Result<()> {    match self        .terminal_id_to_raw_fd        .lock()        .to_anyhow()        .with_context(err_context)?        .get(&terminal_id)    {        Some(Some(fd)) => termios::tcdrain(*fd).with_context(err_context),        _ => Err(anyhow!("could not find raw file descriptor")).with_context(err_context),    }}ソースコードのコメントには、こうある。「VimのようなプログラムはSTDINに書き込みながらSTDOUTを読むとデッドロックする」。Vimへの言及が具体的で、実際に遭遇した問題なのだろう。ウィンドウリサイズの処理ターミナルをリサイズしたとき、PTYにサイズ変更を伝える必要がある。zellij-server/src/os_input_output.rsを開く。fn set_terminal_size_using_terminal_id(    &self,    id: u32,    cols: u16,    rows: u16,    width_in_pixels: Option<u16>,    height_in_pixels: Option<u16>,) -> Result<()> {    // リサイズをキャッシュ（複数のリサイズイベントを1つにまとめる）    match self.cached_resizes.lock() {        Ok(mut cached_resizes) => {            let cached_resizes = cached_resizes.get_or_insert_with(BTreeMap::new);            cached_resizes.insert(id, (cols, rows, width_in_pixels, height_in_pixels));        },        Err(e) => {            log::error!("Failed to cache resize: {}", e);        },    }    // キャッシュを適用    self.apply_cached_resizes()}fn apply_cached_resizes(&self) -> Result<()> {    if let Some(cached_resizes) = self.cached_resizes.lock().ok().as_mut().and_then(|c| c.take()) {        for (terminal_id, (cols, rows, width_in_pixels, height_in_pixels)) in cached_resizes {            let ws = Winsize {                ws_row: rows,                ws_col: cols,                ws_xpixel: width_in_pixels.unwrap_or(0),                ws_ypixel: height_in_pixels.unwrap_or(0),            };            if let Some(raw_fd) = self.get_terminal_id_from_fd(terminal_id) {                set_terminal_size_using_fd(*raw_fd, ws);            }        }    }    Ok(())}fn set_terminal_size_using_fd(fd: RawFd, ws: Winsize) {    // TIOCSWINSZ ioctlでPTYにサイズを伝える    if let Err(e) = ioctl_set_window_size(fd, &ws) {        log::error!("Failed to set terminal size: {}", e);    }}リサイズのキャッシングが重要だ。なぜか。ソースコードのコメントに理由がある。「レイアウト計算のコードには、複数のリサイズを送信してしまうロジックの罠がある。最後の1つだけが正しいのだが、多くのプログラムやシェルはリサイズをデバウンスする（GUIウィンドウのリサイズに対処してきたトラウマだろう）。これがグリッチや描画漏れを引き起こす」。つまり、VimやZshは「リサイズが連続で来たら、最後の1つだけ処理する」という防御策を持っている。Zellijが中間のリサイズも送ると、シェル側のデバウンスと競合してしまう。だからZellij側でもキャッシュし、最後の1つだけを送る。PtyWriteInstruction::StartCachingResizes => {    // レイアウト再計算中はリサイズをキャッシュ    os_input.cache_resizes();},PtyWriteInstruction::ApplyCachedResizes => {    // 計算完了後、最後のリサイズだけを適用    os_input.apply_cached_resizes();},TIOCSWINSZ（Terminal I/O Control Set WINdow SiZe）は、PTYにサイズを伝えるioctlだ。これを呼ぶと、カーネルは子プロセスグループにSIGWINCHを送る。子プロセスはこのシグナルを受けて、画面を再描画する。ws_xpixelとws_ypixel はSixel画像のために使われる。文字単位のサイズ（cols/rows）に加えて、ピクセル単位のサイズも伝える。これにより、画像を正確な解像度で表示できる。プロセス終了の検出とシグナルペインを閉じるとき、子プロセスにシグナルを送る必要がある。pub fn close_pane(&mut self, id: PaneId) -> Result<()> {    match id {        PaneId::Terminal(id) => {            self.task_handles.remove(&id);  // 読み取りタスクを停止            if let Some(child_fd) = self.id_to_child_pid.remove(&id) {                task::block_on(async {                    self.bus                        .os_input                        .as_mut()                        .fatal()                        .kill(Pid::from_raw(child_fd))  // SIGHUPを送信                        .fatal();                });            }        },        PaneId::Plugin(pid) => {            // プラグインはUnload命令を送るだけ            drop(self.bus.senders.send_to_plugin(PluginInstruction::Unload(pid)));        },    }    Ok(())}シグナルの送信順序も工夫されている。// SIGTERMを3回試行し、それでも終了しなければSIGKILLfor _ in 0..3 {    if nix::sys::signal::kill(pid, Signal::SIGTERM).is_ok() {        std::thread::sleep(Duration::from_millis(10));        // プロセスが終了したかチェック        if nix::sys::wait::waitpid(pid, Some(WaitPidFlag::WNOHANG)).is_ok() {            return Ok(());        }    }}// 3回失敗したらSIGKILLnix::sys::signal::kill(pid, Signal::SIGKILL)?;SIGTERM 3回 → SIGKILL。プロセスに「正常終了」の機会を与えつつ、応答しなければ強制終了する。10msのポーリング間隔も絶妙だ。CWD（カレントディレクトリ）の追跡ペインのカレントディレクトリを追跡するのは、意外と難しい。fn get_cwd(&self, pid: Pid) -> Option<PathBuf> {    // /proc/[pid]/cwd を読み取る    let path = format!("/proc/{}/cwd", pid);    std::fs::read_link(path).ok()}pub fn update_and_report_cwds(&mut self) {    let terminal_ids: Vec<u32> = self.id_to_child_pid.keys().copied().collect();    let pids: Vec<_> = terminal_ids        .iter()        .filter_map(|id| self.id_to_child_pid.get(&id))        .map(|pid| Pid::from_raw(*pid))        .collect();    // 全ペインのCWDを一括取得    let (pids_to_cwds, _) = self        .bus        .os_input        .as_ref()        .map(|os_input| os_input.get_cwds(pids))        .unwrap_or_default();    // 変更があればクライアントに通知    for terminal_id in terminal_ids {        let cwd = /* ... */;        if self.terminal_cwds.get(&terminal_id) != Some(cwd) {            // CWD変更イベントを送信        }    }}/proc/[pid]/cwd はLinux固有の仕組みだ。プロセスのカレントディレクトリへのシンボリックリンクになっている。これを定期的にチェックすることで、cdコマンドによるディレクトリ変更を検出できる。データフローの全体像PTYを通じたデータの流れを図にすると、以下のようになる。[ユーザー入力]     ↓ キー入力[Client Process]     ↓ IPC (Unix Domain Socket)[Server Process]     ↓ PtyWriteInstruction::Write[PTY Writer Thread]     ↓ write() to master FD[PTY (カーネル)]     ↓[Shell/アプリケーション]     ↓ 出力[PTY (カーネル)]     ↓ read() from master FD[TerminalBytes::listen()]     ↓ ScreenInstruction::PtyBytes[Screen Thread]     ↓ VTEパーサ[Grid構造体]     ↓ 差分レンダリング[Client Process]     ↓ IPC[ユーザーの画面]6つのスレッドが協調して動いている。PTY Thread: PTYの作成・管理PTY Writer Thread: PTYへの書き込みTerminalBytes（async task）: PTYからの読み取りScreen Thread: VTEパース、レンダリングPlugin Thread: WASMプラグイン実行Background Thread: 非同期タスクこの分離により、どこか1つがブロックしても他の処理は継続できる。Vimが入力を待っている間も、他のペインは正常に動作する。実装の詳細：システムコールのシーケンスここまで概念的な説明が多かった。実際にどのシステムコールがどの順序で呼ばれるのか、具体的に見ていこう。PTY作成シーケンス1. openpty(None, &orig_termios)   → OpenptyResult { master: RawFd, slave: RawFd }2. fork() [Command::spawn()が内部で呼ぶ]3. 子プロセス（シェル側）:   - libc::login_tty(slave)     - setsid()      // 新しいセッション作成     - ioctl(slave, TIOCSCTTY)  // 制御端末として設定     - dup2(slave, 0)  // stdin     - dup2(slave, 1)  // stdout     - dup2(slave, 2)  // stderr   - close_open_fds(3, &[])  // FD 3以降を全て閉じる   - exec("/bin/bash")  // シェルを起動4. 親プロセス（Zellij側）:   - master FDを保存   - 子プロセスのPIDを保存   - 非同期読み取りタスクを起動   - シグナルハンドラスレッドを起動PTY読み取りシーケンス1. async_reader.read(&mut buf[65536])   → read(master_fd, buf, 65536) syscall   → bytes受信 or EOF(0) or error2. ScreenInstruction::PtyBytes(terminal_id, bytes)   をチャネル経由でScreenスレッドに送信3. Screenスレッドがbytesを受信   → VTEパーサに渡す   → Gridを更新PTY書き込みシーケンス1. PtyWriteInstruction::Write(bytes, terminal_id)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. write(master_fd, bytes) syscall   → バイトがPTYバッファに書き込まれる4. tcdrain(master_fd) syscall   → 書き込みが完了するまで待機ターミナルリサイズシーケンス1. PtyWriteInstruction::ResizePty(terminal_id, cols, rows, ...)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. ioctl(master_fd, TIOCSWINSZ, &Winsize { ws_col, ws_row, ... })   → カーネルがSIGWINCHを子プロセスグループに送信   → シェルが再描画実装の詳細：スレッドの起動サーバープロセスの起動時、複数のスレッドが生成される。zellij-server/src/lib.rsを開く。pub fn start_server(mut os_input: Box<dyn ServerOsApi>, socket_path: PathBuf) {    // チャネルの作成    let (to_server, server_receiver): ChannelWithContext<ServerInstruction> =        channels::bounded(50);    let (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =        channels::bounded(50);    let (to_pty, pty_receiver): ChannelWithContext<PtyInstruction> =        channels::bounded(50);    let (to_plugin, plugin_receiver): ChannelWithContext<PluginInstruction> =        channels::bounded(50);    let (to_pty_writer, pty_writer_receiver): ChannelWithContext<PtyWriteInstruction> =        channels::unbounded();  // 書き込みは無制限    // PTY Writerスレッド    thread::Builder::new()        .name("pty_writer".to_string())        .spawn(move || {            pty_writer_main(pty_writer_bus).fatal();        })        .unwrap();    // PTYスレッド    thread::Builder::new()        .name("pty".to_string())        .spawn(move || {            pty_thread_main(pty_bus, pty_receiver).fatal();        })        .unwrap();    // Screenスレッド    thread::Builder::new()        .name("screen".to_string())        .spawn(move || {            screen_thread_main(screen_bus, screen_receiver).fatal();        })        .unwrap();    // Pluginスレッド    thread::Builder::new()        .name("plugin".to_string())        .spawn(move || {            plugin_thread_main(plugin_bus, plugin_receiver).fatal();        })        .unwrap();    // Serverスレッド（メインループ）    loop {        let (instruction, err_ctx) = server_receiver.recv().expect("...");        match instruction {            ServerInstruction::NewClient(client_attributes, ...) => { ... },            ServerInstruction::Render(serialized_output) => { ... },            ServerInstruction::UnblockInputThread => { ... },            ServerInstruction::ClientExit(client_id) => { ... },            ServerInstruction::KillSession => break,            // ...        }    }}注目すべきは、PTY Writerのチャネルだけunbounded()になっている点だ。他はbounded(50)でバックプレッシャーをかけているが、書き込みはブロックさせたくない。ユーザーの入力を遅延させると体感が悪くなるからだ。実装の詳細：キーボード入力からシェルまでの経路ユーザーがキーを押してからシェルに届くまでの、実際のコード経路を追う。1. クライアントがキー入力を受信zellij-client/src/stdin_handler.rsloop {    match os_input.read_from_stdin() {        Ok(buf) => {            // KittyプロトコルまたはANSIエスケープをパース            input_parser.parse(&buf, |input_event| {                send_input_instructions                    .send(InputInstruction::KeyEvent(                        input_event.clone(),                        buf.to_vec(),                    ))                    .unwrap();            }, false);        },        // ...    }}2. クライアントがサーバーにメッセージ送信zellij-client/src/lib.rsInputInstruction::KeyEvent(key_event, raw_bytes) => {    send_client_instructions        .send(ClientInstruction::Action(            Action::Write(None, raw_bytes, false),            None,            None,        ))        .unwrap();}3. サーバーのRouteスレッドがメッセージ受信zellij-server/src/route.rsfn route_action(action: Action, ...) -> bool {    match action {        Action::Write(_, bytes, _) => {            session                .senders                .send_to_screen(ScreenInstruction::WriteCharacter(bytes, client_id))                .unwrap();        },        // ...    }}4. Screenスレッドがペインに書き込み指示zellij-server/src/screen.rsScreenInstruction::WriteCharacter(bytes, client_id) => {    let active_tab = screen.get_active_tab_mut(client_id).unwrap();    active_tab.write_to_terminal(bytes, client_id);}5. TabがPTY Writerに書き込み指示zellij-server/src/tab/mod.rspub fn write_to_active_terminal(&mut self, bytes: Vec<u8>, client_id: ClientId) {    if let Some(active_pane_id) = self.get_active_pane_id(client_id) {        if let PaneId::Terminal(terminal_id) = active_pane_id {            self.senders                .send_to_pty_writer(PtyWriteInstruction::Write(bytes, terminal_id))                .unwrap();        }    }}6. PTY Writerスレッドが実際に書き込みzellij-server/src/pty_writer.rsPtyWriteInstruction::Write(bytes, terminal_id) => {    os_input.write_to_tty_stdin(terminal_id, &bytes)?;    os_input.tcdrain(terminal_id)?;}7. OSレイヤーでシステムコールzellij-server/src/os_input_output.rsfn write_to_tty_stdin(&self, terminal_id: u32, buf: &[u8]) -> Result<usize> {    let fd = self.terminal_id_to_raw_fd.lock()?.get(&terminal_id)?;    let mut file = unsafe { File::from_raw_fd(*fd) };    let result = file.write(buf);    std::mem::forget(file);  // FDを閉じない    result}この経路で、キーボード入力は6つのコンポーネントを経由してシェルに届く。各コンポーネント間はチャネルで接続されている。実装の詳細：シェル出力から画面までの経路逆方向、シェルの出力が画面に表示されるまでの経路。1. TerminalBytesが非同期で読み取りzellij-server/src/terminal_bytes.rspub async fn listen(&mut self) -> Result<()> {    let mut buf = [0u8; 65536];    loop {        match self.async_reader.read(&mut buf).await {            Ok(0) => break,  // EOF            Ok(n_bytes) => {                let bytes = &buf[..n_bytes];                self.async_send_to_screen(ScreenInstruction::PtyBytes(                    self.terminal_id,                    bytes.to_vec(),                )).await?;            },            Err(err) => {                log::error!("{}", err);                break;            },        }    }    Ok(())}2. Screenスレッドがバイトを受信zellij-server/src/screen.rsScreenInstruction::PtyBytes(terminal_id, bytes) => {    // terminal_idからペインを特定    if let Some(tab) = screen.get_tab_with_terminal_id(terminal_id) {        tab.handle_pty_bytes(terminal_id, bytes);    }}3. TabがVTEパーサに渡すzellij-server/src/tab/mod.rspub fn handle_pty_bytes(&mut self, terminal_id: u32, bytes: Vec<u8>) {    if let Some(pane) = self.panes.get_mut(&PaneId::Terminal(terminal_id)) {        pane.handle_pty_bytes(bytes);    }}4. TerminalPaneがGridを更新zellij-server/src/panes/terminal_pane.rsfn handle_pty_bytes(&mut self, bytes: Vec<u8>) {    self.grid.advance_by_bytes(bytes);}5. GridがVTEパーサを実行zellij-server/src/panes/grid.rspub fn advance_by_bytes(&mut self, bytes: Vec<u8>) {    for byte in bytes {        self.vte_parser.advance(&mut *self, byte);    }}ここでvte_parserはvte::Parser型だ。Gridはvte::Performトレイトを実装しており、パース結果に応じてprint()、execute()、csi_dispatch()などが呼ばれる。6. レンダリングとクライアントへの送信ScreenInstruction::Render => {    screen.render()?;    // 各クライアントに差分を送信    for client_id in screen.connected_clients.keys() {        let output = screen.render_for_client(*client_id)?;        send_to_client(*client_id, ServerToClientMsg::Render(output))?;    }}使用しているシステムコール一覧Zellijが使用する主なシステムコールをまとめる。 システムコール  用途  使用箇所  openpty()  PTYペアの作成  os_input_output.rs  fork()  子プロセスの作成  Command::spawn()  setsid()  新セッションの作成  login_tty() 内部  dup2()  FDの複製  login_tty() 内部  exec()  プログラムの実行  Command::spawn()  read()  PTYからの読み取り  terminal_bytes.rs  write()  PTYへの書き込み  pty_writer.rs  ioctl(TIOCGWINSZ)  ウィンドウサイズ取得  os_input_output.rs  ioctl(TIOCSWINSZ)  ウィンドウサイズ設定  os_input_output.rs  tcgetattr()  termios取得  os_input_output.rs  tcsetattr()  termios設定  os_input_output.rs  tcdrain()  出力完了待機  pty_writer.rs  kill()  シグナル送信  os_input_output.rs  waitpid()  子プロセス待機  os_input_output.rs  close()  FDのクローズ  各所 エラーハンドリングの実装PTY関連のエラーは、致命的なものと非致命的なものに分類される。// 非致命的: ログを出すが処理を継続os_input    .write_to_tty_stdin(terminal_id, &bytes)    .with_context(err_context)    .non_fatal();// 致命的: パニックまたは終了os_input    .spawn_terminal(cmd, quit_cb)    .with_context(err_context)    .fatal();non_fatal()は書き込みエラー、リサイズエラーなどに使われる。ペインが閉じられた後に書き込みが来ることがあり、これは正常な動作だ。fatal()はPTY作成失敗、サーバー初期化失敗などに使われる。これらは回復不能なエラーだ。コマンドが見つからない場合の処理も興味深い。fn command_exists(cmd: &RunCommand) -> bool {    // cwdからの相対パスをチェック    if let Some(cwd) = cmd.cwd.as_ref() {        let full_command = cwd.join(&cmd.command);        if full_command.exists() && full_command.is_file() {            return true;        }    }    // PATHを検索    if let Some(paths) = env::var_os("PATH") {        for path in env::split_paths(&paths) {            let full_command = path.join(&cmd.command);            if full_command.exists() && full_command.is_file() {                return true;            }        }    }    false}コマンドが存在しない場合、ZellijError::CommandNotFoundが返される。hold_on_closeが設定されていれば、エラーメッセージを表示したペインが残る。設定されていなければ、ペインは静かに閉じられる。KDL形式によるセッション永続化Zellijは1秒ごとにセッション状態を自動シリアライズする。保存先は~/.cache/zellij/<VERSION>/session_info/<SESSION_NAME>/だ。zellij-utils/src/session_serialization.rsを開く。// zellij-utils/src/session_serialization.rspub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut layout_node = KdlNode::new("layout");    // タブ、ペインの構造をKDLノードとして構築}生成されるKDL。layout {    cwd "/home/user/project"    tab name="Editor" {        pane command="vim" {            args "src/main.rs"            cwd "/home/user/project"        }    }    tab name="Shell" {        pane    }}シリアライズ形式がそのまま有効なKDLレイアウトファイルになっている。これは特筆すべき設計だ。なぜJSONやバイナリ形式ではないのか。JSONはパース速度が速いが、人間が編集するには冗長だ。KDLは「人間が読める設定ファイル」として設計された言語であり、Zellijのレイアウト設定にも使われている。つまり設定ファイルとシリアライズ形式を統一することで、自動保存されたセッションをそのままレイアウトテンプレートとして再利用できる。tmuxの.tmux.confとセッション復元は別系統だ。設定ファイルでは「こうあるべき」を書き、セッション復元では「こうだった」を読む。Zellijはこの2つを統一している。シンプルだが、これを思いつくのは難しい。thiserror + anyhow の併用前編でFatalErrorトレイトを紹介した。後編では、エラー型の定義側を見る。// thiserrorによる型定義#[derive(Debug, Error)]pub enum ZellijError {    #[error("could not find command '{command}' for terminal {terminal_id}")]    CommandNotFound { terminal_id: u32, command: String },    #[error("Client {client_id} is too slow to handle incoming messages")]    ClientTooSlow { client_id: u16 },    #[error("an error occured")]    GenericError { source: anyhow::Error },}thiserror（ライブラリ向けエラー型定義）とanyhow（アプリケーション向けエラー伝播）の併用だ。GenericErrorのフィールドがanyhow::Errorになっている点に注目してほしい。これにより、詳細なエラー型を定義したいケースと、「とりあえずエラーを伝播したい」ケースを両立できる。その他の興味深い実装パターンここまでで主要なパターンを見てきたが、ソースコードを読み進める中で発見した「細かいが面白い」実装を紹介する。スクロールバックの「Canonical Rows」アーキテクチャターミナルの行は、表示上は複数行でも論理的には1行であることがある。長いコマンドが折り返されるケースだ。Zellijはこれを「Canonical Row」という概念で管理している。zellij-server/src/panes/grid.rsを開く。pub struct Row {    pub columns: VecDeque<TerminalCharacter>,    pub is_canonical: bool,  // 本当の改行か、折り返しか    width: Option<usize>,    // キャッシュされた幅}is_canonicalフラグが鍵だ。trueなら本当の改行（ユーザーがEnterを押した）、falseなら表示上の折り返し。スクロール時、折り返された行は元の「親」行と一緒に移動する必要がある。from_rowsメソッドで複数行をマージし、split_to_rows_of_lengthで再分割する。pub fn split_to_rows_of_length(&mut self, max_row_length: usize) -> Vec<Row> {    let mut parts: Vec<Row> = vec![];    let mut current_part: VecDeque<TerminalCharacter> = VecDeque::new();    let mut current_part_len = 0;    for character in self.columns.drain(..) {        if current_part_len + character.width() > max_row_length {            parts.push(Row::from_columns(current_part));            current_part = VecDeque::new();            current_part_len = 0;        }        current_part_len += character.width();        current_part.push_back(character);    }    // canonical statusを保持    parts}文字の幅を考慮している点に注目。絵文字（幅2）の途中で行を切ると表示が崩れる。character.width()でUnicode幅を取得し、正しい位置で分割している。スクロールバックは3つのバッファで管理される。pub(crate) lines_above: VecDeque<Row>,    // スクロールバック（上）pub(crate) viewport: Vec<Row>,             // 表示中pub(crate) lines_below: Vec<Row>,          // 未表示（下）lines_aboveには上限がある。無限にスクロールバックを溜めるとメモリを食い尽くす。fn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();        if let Some(line) = line {            sixel_grid.offset_grid_top();  // Sixel画像の位置も更新        }    }    vec.push_back(value);}古い行を削除するとき、Sixel画像の位置も調整している。画像は行番号で位置を管理しているため、行が消えると座標がずれる。この連携が面白い。幅を考慮したカーソル位置計算絵文字やCJK文字は幅2を持つ。カーソルがその「途中」にあるケースをどう扱うか。pub fn absolute_character_index_and_position_in_char(&self, x: usize) -> (usize, usize) {    // xの幅を考慮したインデックスと、ワイド文字内の位置を返す    let mut accumulated_width = 0;    let mut absolute_index = x;    let mut position_inside_character = 0;    for (i, terminal_character) in self.columns.iter().enumerate() {        accumulated_width += terminal_character.width();        absolute_index = i;        if accumulated_width > x {            let character_start_position = accumulated_width - terminal_character.width();            position_inside_character = x - character_start_position;            break;        }    }    (absolute_index, position_inside_character)}2つの値を返すのがポイント。「何番目の文字か」と「その文字内のどの位置か」。カーソルが絵文字の右半分にあるとき、position_inside_characterは1になる。これにより、カーソル移動やテキスト選択が正しく動作する。OnceCellによるグローバル非同期ランタイム複数スレッドから同じTokioランタイムを使いたい。Mutexで包むと毎回ロックが必要になる。zellij-server/src/global_async_runtime.rsを開く。use once_cell::sync::OnceCell;use tokio::runtime::Runtime;static TOKIO_RUNTIME: OnceCell<Runtime> = OnceCell::new();pub fn get_tokio_runtime() -> &'static Runtime {    TOKIO_RUNTIME.get_or_init(|| {        tokio::runtime::Builder::new_multi_thread()            .worker_threads(4)            .thread_name("async-runtime")            .enable_all()            .build()            .expect("Failed to create tokio runtime")    })}OnceCellは初期化後はロック不要だ。最初の呼び出しでランタイムを作成し、以降は&'static Runtimeを返す。lazy_static!より明示的で、Arc<Mutex<>>より効率的。4ワーカースレッドという設定も興味深い。Zellijは6つの主要スレッドを持つが、非同期タスク用には4スレッドで十分と判断したのだろう。フローティングペインのZ-index管理フローティングペインは重なり合う。どのペインが「上」にあるかを管理する必要がある。zellij-server/src/panes/floating_panes/mod.rsを開く。pub struct FloatingPanes {    panes: BTreeMap<PaneId, Box<dyn Pane>>,    z_indices: Vec<PaneId>,  // レンダリング順序    pane_being_moved_with_mouse: Option<(PaneId, Position)>,    // 多くのRc<RefCell<>>フィールド}pub fn stack(&self) -> Option<FloatingPanesStack> {    if self.panes_are_visible() {        let layers: Vec<PaneGeom> = self            .z_indices            .iter()            .filter_map(|pane_id| self.panes.get(pane_id).map(|p| p.position_and_size()))            .collect();        // レンダリング順でレイヤーを返す    }}z_indicesはVecだ。最後の要素が最前面。ペインをクリックすると、そのIDが末尾に移動する。pub fn move_pane_to_front(&mut self, pane_id: &PaneId) {    if let Some(index) = self.z_indices.iter().position(|id| id == pane_id) {        self.z_indices.remove(index);        self.z_indices.push(*pane_id);    }}マウスイベントは逆順で処理される。最前面のペインから順にヒットテストし、最初にマッチしたペインがイベントを受け取る。これにより、重なったペインの下にあるペインはクリックを受け取らない。Rc<RefCell<>>の多用も特徴的だ。display_areaやviewportは複数のペインで共有される。Rustの所有権モデルでは、これを表現するのにRc<RefCell<>>が必要になる。プラグインホットリロードのデバウンスプラグインファイルが変更されたら自動で再読み込みしたい。しかし、ファイル保存時には複数のイベントが発火することがある。zellij-server/src/plugins/watch_filesystem.rsを開く。pub fn watch_filesystem(    senders: ThreadSenders,    zellij_cwd: &Path,) -> Result<Debouncer<RecommendedWatcher, FileIdMap>> {    let mut debouncer = new_debouncer(        Duration::from_millis(DEBOUNCE_DURATION_MS),  // 400ms        None,        move |result: DebounceEventResult| match result {            Ok(events) => {                let mut create_events = vec![];                let mut read_events = vec![];                let mut update_events = vec![];                let mut delete_events = vec![];                // イベントを分類してプラグイン更新命令を送信            }        }    )}400msのデバウンス。この値はDoherty Thresholdと同じだ。ファイルを保存すると、OSによっては「作成→書き込み→クローズ」と複数イベントが発火する。400ms待つことで、これらを1つのイベントにまとめる。イベントは4種類に分類される。create、read、update、delete。プラグインの更新にはこの区別が重要だ。新規作成と更新では、ロードの方法が異なる可能性がある。アクション完了の追跡とタイムアウト長時間実行されるアクションの完了をどう待つか。zellij-server/src/route.rsを開く。pub fn wait_for_action_completion(    receiver: oneshot::Receiver<ActionCompletionResult>,    action_name: &str,    wait_forever: bool,) -> ActionCompletionResult {    let runtime = get_tokio_runtime();    if wait_forever {        runtime.block_on(async {            match receiver.await { /* ... */ }        })    } else {        match runtime.block_on(async {            tokio::time::timeout(ACTION_COMPLETION_TIMEOUT, receiver).await        }) {            Ok(Ok(result)) => result,            Err(_) | Ok(Err(_)) => {                log::error!("Action {} did not complete within timeout", action_name);                ActionCompletionResult { exit_status: None, affected_pane_id: None }            }        }    }}oneshot + tokio::time::timeoutの組み合わせが優雅だ。oneshot::channelは一度だけ値を送信できるチャネル。アクション完了時に結果を送り、待機側はtimeoutでラップして無限待機を避ける。スレッドをスピンさせたり、タイマーを別途管理したりする必要がない。Tokioのエコシステムをうまく活用している。おわりに冒頭で、何も起きていないターミナルを眺めていた話をした。cat huge_log_file.logで200万行を流し込んでも、Zellijは固まらない。境界付きチャネルのバッファが満杯になれば、送信側が自動的にブロックされる。シンプルだが効果的だ。その仕組みを、後編では中から見てきた。この記事を書きながら気づいたことがある。持ち帰れるパターンは、少なくない。crossbeamの境界付きチャネル: 無制限のチャネルはメモリを食い尽くす可能性がある。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する読み取り/書き込みスレッドの分離: PTYやソケットを扱うとき、同じスレッドで読み書きするとデッドロックの危険がある。Zellijのようにスレッドを分けると安全だcompile-time assertionによるメモリサイズ保証: const _: [(); 16] = [(); std::mem::size_of::<T>()];で、将来の変更でサイズが増えることを防げるシグナル送信のリトライ戦略: SIGTERM 3回 → SIGKILLというパターンは、プロセス管理の定石として覚えておきたいKDL形式のセッション永続化: シリアライズ形式と設定形式を統一するアイデアは、CLIツールやエディタの開発に応用できるOnceCellによるグローバルリソース管理: 複数スレッドから共有リソースにアクセスするとき、OnceCellなら初期化後のロックが不要だoneshot + timeoutによる非同期待機: 長時間実行されるアクションの完了を待つとき、タイムアウト付きで待機するパターンは汎用的に使えるCanonical Rowsによる論理行管理: テキストエディタやターミナルを作るなら、「表示上の行」と「論理的な行」を区別する必要があるただ、正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。境界付きチャネルのバッファサイズ50は、Zellijの6スレッド構成に最適化された値だ。自分のプロジェクトでは違う値が正解かもしれない。たぶん、違う。「パターンを知っている」と「パターンを使いこなせる」の間には溝がある。その溝がどれくらい深いのか、ソースコードを読んだだけでは分からない。書いてみるしかない。書いて、つまずいて、またソースコードに戻る。そういうことの繰り返しなのだと思う。ただ、Zellijのソースコードを2本の記事にわたって読んできて、1つ確信したことがある。Rustでマルチスレッドアプリケーションを書くとき、最も重要なのは「どのスレッドが何を所有するか」の設計だ。Zellijの6スレッド構成は、この所有権の設計が明確だから成立している。読み取りスレッドはPTYのReadを所有し、書き込みスレッドはWriteを所有する。この分離がデッドロックを防ぎ、境界付きチャネルがバックプレッシャーを実現し、差分レンダリングが性能を出す。パターンの根底にあるのは、結局のところ所有権モデルだ。冒頭で眺めていた3つのペインを、今もう一度見る。左のVim、右上のテスト、右下のシェル。見え方が少し変わっている。PTYが3つ、境界付きチャネルがバッファサイズ50で繋がり、VTEパーサが毎秒数千バイトを解釈している。何も起きていないように見えるターミナルが、少しだけ騒がしく感じる。Zellijのソースコードを読みたいなら、以下のファイルが特に参考になる。PTY関連:zellij-server/src/os_input_output.rs - PTY作成、シグナル、リサイズ（1035行）zellij-server/src/pty.rs - PTYマネージャースレッド（2100行以上）zellij-server/src/terminal_bytes.rs - 非同期読み取り（110行）zellij-server/src/pty_writer.rs - 書き込みスレッド（89行）レンダリング関連:zellij-server/src/panes/terminal_character.rs - メモリ効率化とcompile-time assertionzellij-server/src/panes/grid.rs - VTEパーサ、マウスイベント（4000行以上）zellij-server/src/output/mod.rs - 差分レンダリング非同期・スレッド関連:zellij-server/src/global_async_runtime.rs - OnceCellによるTokioランタイム共有（17行）zellij-server/src/route.rs - アクション完了追跡とタイムアウトzellij-utils/src/channels.rs - エラーコンテキスト付きチャネルペイン管理:zellij-server/src/panes/floating_panes/mod.rs - フローティングペインとZ-indexzellij-server/src/panes/tiled_panes/pane_resizer.rs - Cassowary制約ソルバーその他:zellij-utils/src/session_serialization.rs - KDL形式のセッション永続化zellij-server/src/plugins/watch_filesystem.rs - プラグインホットリロードzellij-server/src/panes/search.rs - 折り返し行を跨ぐ検索git clone https://github.com/zellij-org/zellij.git# 境界付きチャネルの実装cat zellij-utils/src/channels.rs# バッファサイズ50の使用箇所grep -r "bounded(50)" zellij-*/src/tmuxの安定性に満足しているなら、無理にZellijに乗り換える必要はない。しかし、Rustでマルチスレッドアプリケーションを書くなら、Zellijのソースコードは一読の価値がある。30年の歴史を持つtmuxとは異なるアプローチで、同等以上のパフォーマンスを達成しようとする試み——その設計判断から学べることは多い。参考リンクZellij本体github.comzellij.devWASMランタイム移行（PR #4449）github.com使用しているクレートgithub.comgithub.comgithub.comgithub.comKDL（設定ファイル形式）kdl.dev参考記事zellij.devzellij.devhttps://zellij.dev/news/new-plugin-api/zellij.devhttps://zellij.dev/news/sixel-images-in-the-terminal/zellij.devパフォーマンス最適化poor.dev境界付きチャネル導入の背景github.comDoherty Thresholdlawsofux.com関連技術protobuf.devdocs.rsdocs.rs関連プロジェクトgithub.comgithub.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【初心者向け】Snowflakeデータパイプライン構築]]></title>
            <link>https://sreake.com/blog/construct-snowflake-pipeline/</link>
            <guid isPermaLink="false">https://sreake.com/blog/construct-snowflake-pipeline/</guid>
            <pubDate>Wed, 28 Jan 2026 14:22:29 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 本記事ではSnowflakeの機能のみを使って最低限のデータパイプラインを作成する方法をご紹介します。流れとしては、CSVファイルからSnowflakeにデータをインジェストし、生のデータをレポートに必要なター […]The post 【初心者向け】Snowflakeデータパイプライン構築 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（前編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</guid>
            <pubDate>Wed, 28 Jan 2026 09:17:50 GMT</pubDate>
            <content:encoded><![CDATA[はじめにターミナルで cat huge_log_file.log を実行した。画面が滝のように流れ始めた。Ctrl+Cを連打した。反応しない。画面はまだ流れている。椅子の背もたれに体を預けて、流れが止まるのを待った。Ciscoルーターの話もしたいが、それを始めるとどこまでも終わらなくなるので、ここでは話をしない。こういう場面に出くわすことがある。自分が打ったコマンドなのに、自分では止められない。出力が止まったあと、何事もなかったかのように次のコマンドを打つ。たぶん、みんなそうしている。自分もそうしてきた。そうしてきたのだが、ある日ふと気になった。この暴走を、ソフトウェアはどうやって止めているのだろう。Zellijは、ターミナルマルチプレクサだ。1つのターミナル画面を複数に分割し、複数のシェルを同時に操作できる。tmuxやscreenの現代版として、2021年にRustで開発が始まった。github.com本記事は2部構成の前編にあたる。前編では設計パターンを抽出し、後編ではさらに深く実装の内部に入る。読んで「なるほど」と思って、そのまま忘れる。たぶんそうなる。それでいい。合わなければ途中で離脱してもらって構わない。約10万行のコードベースから、設計が優れている箇所——と、正直「これでいいのか？」と思う箇所——を抜き出して紹介する。他人のコードを読んで「分かった」と言い切れる自信はない。分からないまま書いている部分もある。それでも、書くことにした。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜZellijのコードを読むのかターミナルマルチプレクサのコードを読む機会はそう多くない。しかし、Zellijには以下の理由で読む価値がある。実践的な並行処理: 複数のスレッドが協調して動く仕組みが、教科書的ではなく「本当に動くコード」として見られるWASMプラグインシステム: ブラウザ以外でWebAssemblyを使う実例として参考になるエラー処理の設計: 「このエラーは無視していい」「このエラーは致命的」を型で表現するパターンが秀逸コードを読み始める前に、Zellijが前提としている概念をいくつか整理しておく。知っている人は読み飛ばしてもらっていい。前提知識Zellijのコードを読み進める前に、いくつかの概念を押さえておくと理解が早い。正直に言えば、自分もこれらを「完全に理解している」とは言い難い。使ったことはある。使ったことはあるが、説明しろと言われると手が止まる。そういう概念を、改めて整理しておく。擬似端末（PTY）ターミナルマルチプレクサの根幹技術だ。PTY（Pseudo Terminal）は、物理的なターミナル装置をソフトウェアでエミュレートする仕組み。マスター側とスレーブ側のペアで構成され、マスター側がZellijのようなプログラム、スレーブ側がシェル（bashやzsh）になる。シェルは自分が本物のターミナルに接続されていると思い込んでいるが、実際にはZellijが間に入ってデータを仲介している。MPSCチャネルRustの標準ライブラリにあるstd::sync::mpscは「Multiple Producer, Single Consumer」の略だ。複数の送信者から1つの受信者にメッセージを送れる。Zellijでは各スレッドがこのチャネルでメッセージをやり取りしている。crossbeam-channelというクレートを使うとMPMC（Multiple Producer, Multiple Consumer）も実現できるが、Zellijは基本的にMPSCで設計されている。Actorモデル各スレッドを独立した「アクター」として扱い、共有メモリではなくメッセージパッシングで通信するパターン。ZellijのScreenThreadやPtyThreadはそれぞれがアクターとして振る舞い、enumで定義された命令（ScreenInstructionなど）を受け取って処理する。ロックの競合を避けやすく、デバッグもしやすい。WebAssembly（WASM）ブラウザで動くバイナリフォーマットとして生まれたが、サーバーサイドやCLIツールでも使われるようになった。Zellijはプラグインの実行環境としてWASMを採用している。プラグインがクラッシュしても本体には影響しない、言語に依存しない、といった利点がある。Zellijは当初wasmtimeをランタイムとして使用していたが、現在はwasmiに移行している。詳細は後述する。では、コードを見ていこう。ここから先は長い。覚悟してほしい——と言いたいところだが、自分も書きながら覚悟している。Cargo Workspace構成：最初に見るべきファイルソースコードを読むとき、私はまずCargo.tomlを開く。プロジェクトの全体像が分かるからだ。Zellijのルートディレクトリでlsすると、以下の構造が見える。zellij/├── zellij/           # エントリーポイント├── zellij-client/    # クライアント側├── zellij-server/    # サーバー側├── zellij-utils/     # 共有ユーティリティ├── zellij-tile/      # プラグインSDK├── default-plugins/  # 標準プラグイン└── xtask/            # ビルドタスクZellijはクライアント・サーバー型のアーキテクチャを採用している。ターミナルの画面を表示する「クライアント」と、実際にシェルを動かす「サーバー」が別プロセスで動いている。なぜわざわざ分離するのか？答えは「セッションの永続化」にある。SSH接続が切れても、サーバー側でシェルは動き続ける。後で再接続すれば、作業を途中から再開できる。リモートワークで長時間かかるビルドを実行中にネットワークが切れても、ビルドは止まらない。これがターミナルマルチプレクサの最大の利点だ。セッションの永続化がどのように実装されているかは、後半の「セッション永続化：KDLによるシリアライズ」で詳しく見る。ワークスペース構成を把握したところで、次はビルドの仕組みを見てみよう。xtask：Rustで書くビルドスクリプトxtask/ディレクトリは、MakefileやシェルスクリプトをRustで置き換える「xtaskパターン」の実装だ。github.com// xtask/src/main.rsfn main() -> anyhow::Result<()> {    let shell = &Shell::new()?;    let flags = flags::Xtask::from_env()?;    match flags.subcommand {        flags::XtaskCmd::Build(flags) => build::build(shell, flags),        flags::XtaskCmd::Clippy(flags) => clippy::clippy(shell, flags),        flags::XtaskCmd::Format(flags) => format::format(shell, flags),        flags::XtaskCmd::Test(flags) => test::test(shell, flags),        flags::XtaskCmd::Dist(flags) => pipelines::dist(shell, flags),        flags::XtaskCmd::Install(flags) => pipelines::install(shell, flags),        // ...    }}.cargo/config.tomlにエイリアスを設定すると、cargo xtask buildのように呼び出せる。[alias]xtask = "run --package xtask --"xtaskパターンの利点は3つある。クロスプラットフォーム: シェルスクリプトはOS依存だが、Rustはどこでも動く型安全: xflagsクレートでCLI引数をパースし、typoをコンパイル時に検出言語統一: ビルドスクリプトもRustで書けば、チーム全員が読めるpipelines.rsでは、複数のビルドステージを.and_then()でチェーンしている。// xtask/src/pipelines.rspub fn make(sh: &Shell, flags: flags::Make) -> anyhow::Result<()> {    format::format(sh, flags::Format { check: false })        .and_then(|_| build::build(sh, build_flags))        .and_then(|_| test::test(sh, test_flags))        .and_then(|_| clippy::clippy(sh, flags::Clippy {}))        .with_context(err_context)}どこかでエラーが発生すれば、以降のステージはスキップされる。RustのResult型を活かしたパイプライン設計だ。スレッド設計：thread_bus.rsを読むzellij-server/src/thread_bus.rsを開くと、スレッド間通信の設計が見える。// zellij-server/src/thread_bus.rs#[derive(Default, Clone)]pub struct ThreadSenders {    pub to_screen: Option<SenderWithContext<ScreenInstruction>>,    pub to_pty: Option<SenderWithContext<PtyInstruction>>,    pub to_plugin: Option<SenderWithContext<PluginInstruction>>,    pub to_server: Option<SenderWithContext<ServerInstruction>>,    pub to_pty_writer: Option<SenderWithContext<PtyWriteInstruction>>,    pub to_background_jobs: Option<SenderWithContext<BackgroundJob>>,    pub should_silently_fail: bool,  // テスト用}6つのスレッドへの送信チャネルを1つの構造体にまとめている。各スレッドが他のスレッドにメッセージを送りたいとき、このThreadSendersを経由する。┌────────────────────────────────────────────────────────────────────┐│                           ZELLIJ SERVER                            ││  ┌───────────────┐ ┌──────────┐ ┌──────────────┐ ┌──────────────┐  ││  │ SCREEN        │ │ PTY      │ │ PLUGIN       │ │ PTY_WRITER   │  ││  │ THREAD        │ │ THREAD   │ │ THREAD       │ │ THREAD       │  ││  │               │ │          │ │              │ │              │  ││  │ タブ/ペイン   │ │ 擬似端末 │ │ WASM         │ │ 書き込み専用 │  ││  │ 管理          │ │ 生成管理 │ │ ランタイム   │ │              │  ││  └───────────────┘ └──────────┘ └──────────────┘ └──────────────┘  ││  ┌───────────────────┐ ┌──────────────────────────────────────┐    ││  │ BACKGROUND_JOBS   │ │            SERVER (IPC)              │    ││  │ THREAD            │ │            THREAD                    │    ││  └───────────────────┘ └──────────────────────────────────────┘    │└────────────────────────────────────────────────────────────────────┘Bus構造体も見ておこう。pub(crate) struct Bus<T> {    receivers: Vec<channels::Receiver<(T, ErrorContext)>>,    pub senders: ThreadSenders,    pub os_input: Option<Box<dyn ServerOsApi>>,}receiversがVecになっている。なぜ1つの受信口ではなく、複数の受信口を持つ必要があるのか？lib.rsを開いて、チャネルを作っている箇所を探すと、理由が分かる。// zellij-server/src/lib.rslet (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::unbounded();  // 通常のメッセージ用（無制限）let (to_screen_bounded, bounded_screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::bounded(50);  // PTYからの高速入力用（上限50個）このbounded(50)は2022年6月のPR #1265で導入された。github.comScreenスレッドには2つのチャネルがある。通常の無制限チャネルと、上限50個の境界付きチャネル。これは「バックプレッシャー」を実現するための設計だ。冒頭のcat huge_log_file.logを思い出してほしい。PTYからの出力が速すぎると、画面描画が追いつかない。上限付きチャネルを使うと、バッファが満杯になったときに送信側がブロックされる。一方、ユーザー操作（ペインの移動、タブの切り替え）は無制限チャネル経由で送られ、即座に処理される。ユーザーがキーを押したのに反応しない、という事態は避けたいからだ。前提知識で触れたMPSCチャネルとActorモデルが、ここで活きている。各スレッドは自分専用のチャネルからメッセージを受け取り、状態を外部と共有しない。共有しなければ、奪い合いは起きない。この設計により、Arc<Mutex<T>>のような共有ロックを使わずにスレッド間通信を実現している。デッドロックの心配がない。SenderWithContext：エラー追跡付きチャネルzellij-utils/src/channels.rsには、crossbeamチャネルのラッパーがある。github.com// zellij-utils/src/channels.rspub type ChannelWithContext<T> = (Sender<(T, ErrorContext)>, Receiver<(T, ErrorContext)>);#[derive(Clone)]pub struct SenderWithContext<T> {    sender: Sender<(T, ErrorContext)>,}impl<T: Clone> SenderWithContext<T> {    pub fn send(&self, event: T) -> Result<(), SendError<(T, ErrorContext)>> {        let err_ctx = get_current_ctx();        self.sender.send((event, err_ctx))    }}メッセージを送るたびに、現在のエラーコンテキストが自動的に付与される。get_current_ctx()は、thread-localストレージから現在の呼び出し履歴を取得する。これにより、エラーが発生したとき「どのスレッドの、どの処理から送られたメッセージか」を追跡できる。// zellij-utils/src/errors.rsthread_local!(    pub static OPENCALLS: RefCell<ErrorContext> = RefCell::default());// 非同期タスク用にはtask_localも用意task_local! {    pub static ASYNCOPENCALLS: RefCell<ErrorContext> = RefCell::default()}スレッドごとに独立した呼び出し履歴を持ち、最大6階層まで記録する。const MAX_THREAD_CALL_STACK: usize = 6;#[derive(Clone, Copy)]pub struct ErrorContext {    calls: [ContextType; MAX_THREAD_CALL_STACK],}エラーが起きると「Screen → HandlePtyBytes → Render」のような呼び出し履歴が出力される。マルチスレッドのデバッグでは、この情報がないと原因特定に時間がかかる。100以上のバリアント：Instruction enumscreen.rsを開くと、巨大なenumが見つかる。// zellij-server/src/screen.rspub enum ScreenInstruction {    PtyBytes(u32, VteBytes),    PluginBytes(Vec<PluginRenderAsset>),    Render,    NewPane(PaneId, Option<InitialTitle>, HoldForCommand, ...),    WriteCharacter(Option<KeyWithModifier>, Vec<u8>, bool, ClientId, ...),    MoveFocusLeft(ClientId, Option<NotificationEnd>),    MoveFocusRight(ClientId, Option<NotificationEnd>),    ScrollUp(ClientId, Option<NotificationEnd>),    // ... 約100個のバリアント}100個以上のバリアント。正直、最初に見たときは「これ、本当に正しいのか？」と思った。git履歴を追うと、初期のScreenInstructionは11バリアントしかなかった。Pty、Render、HorizontalSplit、VerticalSplit、WriteCharacterなど基本的なものだけだ。5年間で148バリアント以上に成長している。25倍。機能追加のたびにバリアントが増えていった結果だ。利点はある。型安全性: 処理し忘れたバリアントがあれば、コンパイルエラーで検出できるドキュメント性: このenumを見れば、Screenスレッドが受け付ける全メッセージが一目で分かる文字列でメッセージを送る設計（例："move_focus_left"）と比べると、タイポをコンパイル時に検出できる点で優れている。ただ、疑問も残る。100個のバリアントを持つenumに新しいメッセージを追加するとき、Fromトレイトの実装も更新しなければならない。忘れたらコンパイルエラーになるとはいえ、変更箇所が分散するのは保守コストだ。trait objectやdynamic dispatchで抽象化する選択肢もあったはずだが、Zellijはそれを選ばなかった。パフォーマンスを優先したのか、あるいは「enumで十分」という判断なのか。答えは分からない。各Instruction enumには、FromトレイトでContextTypeへの変換が実装されている。impl From<&ScreenInstruction> for ScreenContext {    fn from(server_instruction: &ScreenInstruction) -> Self {        match *server_instruction {            ScreenInstruction::PtyBytes(..) => ScreenContext::HandlePtyBytes,            ScreenInstruction::Render => ScreenContext::Render,            ScreenInstruction::NewPane(..) => ScreenContext::NewPane,            // ... 全バリアントを網羅        }    }}これにより、エラーコンテキストへの変換が自動化される。WASMプラグイン：wasmiの採用zellij-server/src/plugins/plugin_loader.rsを開くと、WASMランタイムのimportが見える。// zellij-server/src/plugins/plugin_loader.rsuse wasmi::{Engine, Instance, Linker, Module, Store, StoreLimits};use wasmi_wasi::sync::WasiCtxBuilder;use wasmi_wasi::WasiCtx;wasmiを使っている。Wasmtimeではない。github.com両者の違いを整理する。 項目  Wasmtime  wasmi  実行方式  JITコンパイル  インタプリタ  速度  高速  低速  攻撃面  広い（JITは複雑）  狭い  依存  LLVM  ピュアRust 実は、Zellijは当初Wasmtimeを使っていた。2025年10月のPR #4449「Migrate from wasmtime to wasmi」でwasmiに移行している。この移行と同時にPinnedExecutor（動的スレッドプール）が導入された。JITコンパイルをやめることで、プラグインごとにスレッドをピン留めする設計が可能になった。インタプリタ方式は遅いが、リソース管理の予測可能性とセキュリティで優れる。github.comzellij-tile/src/lib.rsにはプラグインSDKがある。// zellij-tile/src/lib.rspub trait ZellijPlugin: Default {    fn load(&mut self, configuration: BTreeMap<String, String>) {}    fn update(&mut self, event: Event) -> bool { false }  // trueで再描画    fn pipe(&mut self, pipe_message: PipeMessage) -> bool { false }    fn render(&mut self, rows: usize, cols: usize) {}}プラグインは4つのメソッドを実装するだけでいい。register_plugin!マクロの中身を見ると、3つの工夫がある。#[macro_export]macro_rules! register_plugin {    ($t:ty) => {        thread_local! {            static STATE: std::cell::RefCell<$t> = RefCell::new(Default::default());        }        fn main() {            std::panic::set_hook(Box::new(|info| {                report_panic(info);            }));        }        #[no_mangle]        fn load() {            STATE.with(|state| {                let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();                // ...            });        }    };}thread_local!: WASMは基本的に状態を持たない設計だが、これで状態を保持できる#[no_mangle]: 関数名をそのまま維持し、Zellijホストから呼び出せるようにするProtocol Buffers: WASM境界を越えるデータはシリアライズする必要があるgithub.comプラグインの権限管理も見ておこう。default-plugins/status-bar/src/main.rsを開く。fn load(&mut self, _configuration: BTreeMap<String, String>) {    request_permission(&[PermissionType::ReadApplicationState]);    subscribe(&[        EventType::TabUpdate,        EventType::ModeUpdate,        EventType::CopyToClipboard,        EventType::SystemClipboardFailure,    ]);}request_permissionでプラグインが必要な権限を要求し、ユーザーが許可する。zellij-utils/src/data.rsには16種類の権限が定義されている。PermissionType::ReadApplicationState    // 状態の読み取りPermissionType::ChangeApplicationState  // 状態の変更PermissionType::RunCommands             // コマンド実行PermissionType::WebAccess               // ネットワークアクセスPermissionType::FullHdAccess            // ファイルシステムアクセス// ... 他11種類AndroidやiOSの権限モデルと同様に、細粒度の制御ができる。FatalError：エラーの重大度を型で表現zellij-utils/src/errors.rsには、エラーの重大度を呼び出し側で選択できるトレイトがある。pub trait FatalError<T> {    fn non_fatal(self);  // エラーをログに記録して続行    fn fatal(self) -> T; // アンラップまたはパニック}impl<T> FatalError<T> for anyhow::Result<T> {    fn non_fatal(self) {        if self.is_err() {            discard_result(self.context("a non-fatal error occured").to_log());        }    }    fn fatal(self) -> T {        if let Ok(val) = self {            val        } else {            self.context("a fatal error occured")                .expect("Program terminates")        }    }}使用例を見ると、意図が明確になる。// スレッドのエントリーポイント：失敗したらプロセス終了move || pty_thread_main(pty, layout.clone()).fatal()// 内部のエラー処理：失敗してもログを出して続行self.senders.send_to_screen(instruction).non_fatal();unwrap()やexpect()では「なぜここでパニックしていいのか」が分からない。.fatal()なら意図が明確だ。コードレビューでも「このエラーは本当に致命的か？」という議論がしやすくなる。ここまで読んで、自分のプロジェクトのエラー処理が急に心配になった。unwrap()を何箇所書いただろう。数えたくない。数えたくないが、たぶん数えるべきだ。ここからは、Zellijの別の顔を見ていく。セッションの永続化、そしてターミナルの根幹であるPTY処理だ。セッション永続化：KDLによるシリアライズzellij-utils/src/session_serialization.rsには、セッション状態をKDL形式で保存する処理がある。// zellij-utils/src/session_serialization.rs#[derive(Default, Debug, Clone)]pub struct GlobalLayoutManifest {    pub global_cwd: Option<PathBuf>,    pub default_shell: Option<PathBuf>,    pub default_layout: Box<Layout>,    pub tabs: Vec<(String, TabLayoutManifest)>,}pub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut pane_contents = BTreeMap::new();    // ...}KDL（KDL Document Language）は、人間が読みやすいように設計された設定言語だ。Zellijの設定ファイルにも使われている。kdl.devセッションの保存時に、レイアウト情報（KDL文字列）とペインの内容（BTreeMap）を分離して返すのは、関心の分離ができている。PTY処理：os_input_output.rsの低レベルコード前提知識で触れたPTY（擬似端末）が、実際にどう実装されているか。ターミナルマルチプレクサの核心部分を見る。冒頭の cat huge_log_file.log で画面が止まらなくなったあの現象——あれはPTYのmaster側から流れ込むバイト列を、Zellijがどう捌くかという問題だった。zellij-server/src/os_input_output.rsを開く。冒頭の cat huge_log_file.log で画面が暴走したとき、裏側ではここのコードが動いていた。あの滝が、ここで生まれている。github.comuse nix::pty::{openpty, OpenptyResult, Winsize};fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // ホスト側（Zellij）    let pid_secondary = open_pty_res.slave;   // 子プロセス側（シェル）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // ...}unsafeブロック、libc::login_tty、pre_exec。低レベルなUnixプログラミングだ。openptyは「master」と「slave」という2つのファイルディスクリプタを作る。Zellijはmaster側を持ち、シェル（bashやzsh）はslave側を持つ。シェルが出力した文字はmaster側で読み取れる。login_ttyは、Unix系OSでターミナルをセットアップする伝統的な関数だ。これにより、子プロセスはslave側のPTYを「自分の端末」として認識する。terminal_bytes.rs：非同期I/Oterminal_bytes.rsは、PTYからのバイト読み取りを担当する。// zellij-server/src/terminal_bytes.rspub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    )).await?;                },            }        }        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのバッファを使っている。このバッファサイズは2022年7月のPR #1585「perf(terminal): better responsiveness」で導入された。コミットメッセージには「only buffer terminal bytes when screen thread is backed up」とある。画面スレッドが詰まっているときだけバッファリングし、通常時は即座に転送する。64KBという数値は、1回のシステムコールで読み取れる量と、メモリ消費のバランスから選ばれたと思われる。github.comOk(0)とErrの区別も重要。Ok(0)はファイル終端（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス終了時にエラーログが出てしまう。64KBのバッファが、あの画面の暴走を受け止めている。自分が cat を打って椅子にもたれかかっていたあの数秒間、このコードが黙々とバイトを読んでいた。なんだか少し申し訳ない気持ちになる。grid.rs：ANSIエスケープシーケンスの処理PTYから読み取ったバイト列は、そのまま画面に表示できるわけではない。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」という特殊なバイト列で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」といった具合だ。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。github.comuse vte::{Params, Perform};impl Perform for Grid {    fn print(&mut self, c: char) {        // 通常文字の表示    }    fn execute(&mut self, byte: u8) {        // 制御文字（\n, \r, \t等）    }    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    ignore: bool, action: char) {        // CSIシーケンス: カーソル移動、色変更など    }    fn osc_dispatch(&mut self, params: &[&[u8]], bell_terminated: bool) {        // OSCシーケンス: ウィンドウタイトル設定など    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。差分レンダリング：output/mod.rs全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。pub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更された行インデックス    pub should_update_all_lines: bool,}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn update_all_lines(&mut self) {        self.clear();        self.should_update_all_lines = true;    }}HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。should_update_all_linesフラグは、ウィンドウサイズが変わったときなど、全画面を再描画する必要があるケースに対応している。terminal_character.rs：メモリ効率の工夫zellij-server/src/panes/terminal_character.rsには、メモリ効率を意識したパターンがある。pub const EMPTY_TERMINAL_CHARACTER: TerminalCharacter = TerminalCharacter {    character: ' ',    width: 1,    styles: RcCharacterStyles::Reset,};thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}constでデフォルト値を定義し、thread_local!でスタイルオブジェクトをキャッシュしている。ターミナルの各セルにスタイル情報を持たせると、同じスタイルのオブジェクトが大量に生成される。参照カウントでキャッシュすることで、メモリ使用量を削減できる。data.rs：deriveの活用zellij-utils/src/data.rsには、様々なenumが定義されている。#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize, EnumIter)]pub enum InputMode {    Normal,    Locked,    Resize,    Pane,    Tab,    Scroll,    EnterSearch,    Search,    RenameTab,    RenamePane,    Session,    Move,    Prompt,    Tmux,}#[derive(...)]に9つのトレイトを並べている。特にEnumIter（strumクレート）が便利で、InputMode::iter()で全バリアントを列挙できる。UIの選択肢一覧を作るときに使える。github.comEvent enumには#[non_exhaustive]属性がついている。#[non_exhaustive]pub enum Event {    ModeUpdate(ModeInfo),    TabUpdate(Vec<TabInfo>),    Key(KeyWithModifier),    // ... 約30バリアント}これは「このenumにはまだバリアントが追加される可能性がある」という宣言だ。外部のプラグイン開発者は必ず_ => ()アームを書く必要がある。fn update(&mut self, event: Event) -> bool {    match event {        Event::Key(key) => { /* ... */ }        Event::ModeUpdate(mode_info) => { /* ... */ }        _ => ()  // non_exhaustiveのため必須    }}これにより、Zellijがバージョンアップで新しいイベントを追加しても、既存のプラグインがコンパイルエラーにならない。後方互換性を保つための工夫だ。キーバインディング：モード別マッピングと例外処理zellij-utils/src/input/keybinds.rsには、キーバインディングの管理ロジックがある。// zellij-utils/src/input/keybinds.rs#[derive(Clone, PartialEq, Deserialize, Serialize, Default)]pub struct Keybinds(pub HashMap<InputMode, HashMap<KeyWithModifier, Vec<Action>>>);モードごとにキーマップを持つ設計だ。InputMode（Normal, Locked, Resize等）をキーに、さらにキーとアクションのマップを値に持つ。注目すべきはhandle_ctrl_jという関数だ。fn handle_ctrl_j(    mode_keybindings: &HashMap<KeyWithModifier, Vec<Action>>,    raw_bytes: &[u8],    key_is_kitty_protocol: bool,) -> Option<Vec<Action>> {    let ctrl_j = KeyWithModifier::new(BareKey::Char('j')).with_ctrl_modifier();    if mode_keybindings.get(&ctrl_j).is_some() {        mode_keybindings.get(&ctrl_j).cloned()    } else {        Some(vec![Action::Write { /* ... */ }])    }}Ctrl-Jはbyte [10]を送信するが、これはEnterキーと同じバイト列だ。ターミナルの歴史的な事情により、この2つを区別する必要がある。Zellijは「Ctrl-Jにバインドがあればそれを実行、なければ生のバイトを送信」という戦略を取っている。こういうエッジケースは、ターミナルソフトウェアを書くときに避けて通れない。コードを読むまで気づかなかった。設定マージ：Option型の活用zellij-utils/src/input/options.rsには、設定値のマージロジックがある。// zellij-utils/src/input/options.rspub fn merge(&self, other: Options) -> Options {    let mouse_mode = other.mouse_mode.or(self.mouse_mode);    let pane_frames = other.pane_frames.or(self.pane_frames);    let default_mode = other.default_mode.or(self.default_mode);    let default_shell = other.default_shell.or_else(|| self.default_shell.clone());    // ... 約30フィールド}Option::orとOption::or_elseを使った設定マージだ。other（後から来た設定）に値があればそれを使い、なければself（既存の設定）を使う。orとor_elseの使い分けにも注目。or: Copyトレイトを実装している型（bool、InputMode等）or_else: Cloneが必要な型（PathBuf、String等）or_elseはクロージャを取るので、Cloneのコストは必要なときだけ発生する。30フィールド以上ある設定を毎回全部Cloneすると無駄だ。この設計により、「デフォルト設定 → 設定ファイル → CLI引数」という3段階のマージが自然に実現できる。// zellij-utils/src/input/config.rspub fn merge(&mut self, other: Config) -> Result<(), ConfigError> {    self.options = self.options.merge(other.options);    self.keybinds.merge(other.keybinds.clone());    self.themes = self.themes.merge(other.themes);    self.plugins.merge(other.plugins);    // ...}各フィールドが独自のmergeメソッドを持ち、親構造体は単にそれを呼び出すだけ。責任が分散している。OnceLock：実行時に決まる設定値zellij-utils/src/consts.rsには、定数と「起動時に一度だけ設定される値」が混在している。// zellij-utils/src/consts.rspub const DEFAULT_SCROLL_BUFFER_SIZE: usize = 10_000;pub static SCROLL_BUFFER_SIZE: OnceLock<usize> = OnceLock::new();pub static DEBUG_MODE: OnceLock<bool> = OnceLock::new();constとstatic OnceLockの使い分けに注目してほしい。const: コンパイル時に決まる。DEFAULT_SCROLL_BUFFER_SIZEはフォールバック値OnceLock: 実行時に一度だけ設定される。設定ファイルやCLI引数から値を受け取れるOnceLockはlazy_static!の後継で、Rust 1.70で標準ライブラリに入った。初期化のタイミングを明示的に制御できる点が違う。// zellij-server/src/lib.rs での初期化SCROLL_BUFFER_SIZE.get_or_init(|| {    config.scroll_buffer_size.unwrap_or(DEFAULT_SCROLL_BUFFER_SIZE)});get_or_initは「まだ初期化されていなければ初期化する」という意味だ。2回目以降の呼び出しは、最初に設定された値を返す。この値はpanes/grid.rsで使われる。// zellij-server/src/panes/grid.rsfn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    let mut dropped_line_width = None;    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();  // 古い行を削除        if let Some(line) = line {            sixel_grid.offset_grid_top();  // 画像グリッドも調整            dropped_line_width = Some(line.width());        }    }    vec.push_back(value);    dropped_line_width}スクロールバッファが上限（デフォルト10,000行）に達すると、古い行がFIFOで削除される。sixel_grid.offset_grid_top()は、ターミナル内の画像表示（Sixel形式）の位置調整だ。テキストと画像が混在するターミナルでは、こういう細かい調整が必要になる。PinnedExecutor：プラグイン用の動的スレッドプールzellij-server/src/plugins/pinned_executor.rsには、プラグイン実行用の独自スレッドプールがある。1300行以上のファイルだ。// zellij-server/src/plugins/pinned_executor.rs/// A dynamic thread pool that pins jobs to specific threads based on plugin_id/// Starts with 1 thread and expands when threads are busy, shrinks when plugins unloadpub struct PinnedExecutor {    // Sparse vector - Some(thread) for active threads, None for removed threads    execution_threads: Arc<Mutex<Vec<Option<ExecutionThread>>>>,    // Maps plugin_id -> thread_index (permanent assignment)    plugin_assignments: Arc<Mutex<HashMap<u32, usize>>>,    // Maps thread_index -> set of plugin_ids assigned to it    thread_plugins: Arc<Mutex<HashMap<usize, HashSet<u32>>>>,    // Next thread index to use when spawning (monotonically increasing)    next_thread_idx: AtomicUsize,    max_threads: usize,    // ...}各プラグインを特定のスレッドに「ピン留め」する設計だ。プラグインAは常にスレッド1で、プラグインBは常にスレッド2で実行される。スレッド間でプラグインが移動しない。なぜこの設計なのか？WASMのインスタンスはスレッドセーフではない。同じプラグインを複数のスレッドから同時に呼び出すと壊れる。ピン留めすれば、この問題を構造的に回避できる。スレッドの割り当てロジックも見てみよう。pub fn register_plugin(&self, plugin_id: u32) -> usize {    // ...    // Find a non-busy thread with assigned plugins (prefer reusing threads)    let mut best_thread: Option<(usize, usize)> = None; // (index, load)    for (idx, thread_opt) in threads.iter().enumerate() {        if let Some(thread) = thread_opt {            let is_busy = thread.jobs_in_flight.load(Ordering::SeqCst) > 0;            if !is_busy {                let load = thread_plugins.get(&idx).map(|s| s.len()).unwrap_or(0);                if best_thread.is_none() || best_thread.map(|b| load < b.1).unwrap_or(false) {                    best_thread = Some((idx, load));                }            }        }    }    // ...}「最も負荷が低い非ビジースレッド」を選ぶ。jobs_in_flight（実行中のジョブ数）が0のスレッドの中から、割り当て済みプラグイン数が最小のものを選ぶ。すべてのスレッドがビジーで、かつmax_threadsに達していない場合は、新しいスレッドを生成する。逆に、プラグインがアンロードされてスレッドが空になると、そのスレッドは縮退する（Noneに置き換えられる）。この「動的に拡縮するスレッドプール」は、プラグインの数が事前に分からないシステムでは合理的だ。固定スレッド数だと、プラグインが少ないときにリソースを無駄にし、多いときにボトルネックになる。自分だったら固定スレッド数で妥協していたかもしれない。「動的に拡縮」と言うのは簡単だが、縮退の判断を正しく実装する自信は、正直ない。#[track_caller]：エラー発生箇所を追跡するzellij-utils/src/errors.rsには、#[track_caller]属性を使った工夫がある。// zellij-utils/src/errors.rspub trait LoggableError<T>: Sized {    #[track_caller]    fn print_error<F: Fn(&str)>(self, fun: F) -> Self;    #[track_caller]    fn to_log(self) -> Self {        let caller = std::panic::Location::caller();        self.print_error(|msg| {            log::logger().log(                &log::Record::builder()                    .level(log::Level::Error)                    .args(format_args!("{}", msg))                    .file(Some(caller.file()))                    .line(Some(caller.line()))                    .module_path(None)                    .build(),            );        })    }    // ...}#[track_caller]は、関数の呼び出し元の位置情報を取得できるようにする属性だ。これがないと、エラーログに出力されるのはerrors.rsの行番号になってしまう。#[track_caller]を付けることで、実際にエラーが発生した場所の行番号がログに出る。ファイルのコメントにも説明がある。// NOTE: The log entry has no module path associated with it. This is because `log`// gets the module path from the `std::module_path!()` macro, which is replaced at// compile time in the location it is written!module_path!()マクロはコンパイル時に展開されるため、errors.rsのモジュールパスになってしまう。そこで、モジュールパスは諦めてNoneにし、ファイルパスと行番号だけを保持している。完璧ではないが、デバッグには十分だ。機能と実装の対応表ここまで読んできた内容を、「機能」と「実装」の対応で整理する。 機能  実装方法  関連ファイル  セッション永続化  クライアント・サーバー分離  zellij-client/, zellij-server/  ビルドタスク  xtaskパターン  xtask/  大量出力時のメモリ保護  境界付きチャネル  lib.rsのchannels::bounded(50)  スレッド間通信  メッセージパッシング + ThreadSenders  thread_bus.rs, channels.rs  エラー追跡  SenderWithContext + thread-local  channels.rs, errors.rs  プラグインサンドボックス  WebAssembly（wasmi）  plugins/plugin_loader.rs  プラグイン権限制御  16種類のPermissionType  data.rs  プラグイン実行  PinnedExecutor（動的スレッドプール）  plugins/pinned_executor.rs  エラーの重大度  FatalError/non_fatalトレイト  errors.rs  エラー発生箇所の追跡  #[track_caller] + Location  errors.rs  実行時設定値  OnceLock（スクロールバッファサイズ等）  consts.rs  キーバインディング  モード別HashMap + 例外処理  input/keybinds.rs  設定マージ  Option::or/or_else による多層マージ  input/options.rs, input/config.rs  ターミナル出力の解析  vteクレート  panes/grid.rs  描画最適化  差分レンダリング（HashSet）  output/mod.rs  PTYの生成と制御  nixクレート + login_tty  os_input_output.rs  非同期I/O  async-std  terminal_bytes.rs  設定・レイアウト保存  KDL形式  session_serialization.rs おわりに冒頭の cat huge_log_file.log の話に戻る。あの暴走を、ソフトウェアはどうやって止めているのか。答えは「50個のメッセージで満杯になるチャネル」だった。PTYからの出力が速すぎれば、バッファが満杯になり、送信側が自動的にブロックされる。それだけだ。それだけのことが、あの滝を止めている。10万行のコードを読んで見えてきたのは、たぶん「当たり前のことを愚直にやっている」ということだった気がする。スレッド間で状態を共有しない。教科書に書いてあることだ。書いてあるが、実際のプロジェクトでは「ちょっとだけ共有したい」という誘惑がある。ZellijはThreadSenders構造体でチャネルの送信側だけを共有し、状態は各スレッドが排他的に所有する。知っていることと、守れることは違う。それは自分に言い聞かせている。メッセージは型安全なenumで表現する。ScreenInstructionは100以上のバリアントを持つ。100個のバリアントを書く勇気。それがZellijの強さかもしれないし、将来の負債かもしれない。たぶん、両方だ。自分のプロジェクトに何を持ち帰れるのか。考えてみた。手が止まった。意外と出てこない。10万行を読んで「すごい」と思ったが、「じゃあ自分は何をするのか」という問いの前では言葉に詰まる。それでも、絞り出してみる。crossbeamの境界付きチャネル。無制限のチャネルはメモリを食い尽くす。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する。これは明日から使える。たぶん。FatalError/non_fatalパターン。unwrap()を見たら「なぜここでパニックしていいのか」を問う。その問いに答える設計がFatalErrorだ。自分のコードに入れたら、半分以上のunwrap()が正当化できない気がする。それを知るのが怖い。SenderWithContext。チャネル経由のメッセージにエラーコンテキストを自動付与する。マルチスレッドのデバッグでは、この情報がないと地獄を見る。地獄は見たことがある。何度もある。xtaskパターン。MakefileやシェルスクリプトをRustで書くことで、クロスプラットフォーム対応とIDE補完が得られる。これは導入のハードルが低い。低いからこそ、最初の一歩にいい。正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。ThreadSendersは6スレッド前提で設計されているし、FatalErrorは「ログを吐いて続行」が正しい場面を見極める目が必要だ。「パターンを知っている」と「パターンを使いこなせる」の間には、溝がある。Zellijのソースコードは約10万行。すべてを読む必要はないが、以下のファイルは特に参考になる。xtask/src/main.rs - ビルドタスクの設計zellij-server/src/thread_bus.rs - スレッド間通信のパターンzellij-server/src/plugins/pinned_executor.rs - 動的スレッドプールの設計zellij-utils/src/errors.rs - エラーハンドリングと#[track_caller]zellij-utils/src/channels.rs - エラーコンテキスト付きチャネルzellij-utils/src/consts.rs - OnceLockと定数の設計zellij-tile/src/lib.rs - WASMプラグインのマクロ展開後編では、PTY処理やANSIパーサーなど、さらに低レベルな実装を見ていく。ターミナルマルチプレクサの核心部分だ。ただ、一つだけ変わったことがある。unwrap()を見たとき、以前より少しだけ手が止まるようになった。「これは本当にpanicしていいのか」と。その迷いが生まれただけでも、10万行を読んだ意味はあったのかもしれない。分からないまま、次のコードを書く。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[astro-rssでのRSSの改善とモジュール化]]></title>
            <link>https://www.rowicy.com/blog/astro-rss-improvements/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/astro-rss-improvements/</guid>
            <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Astso.jsでおこなったRSSの改善とモジュール化について、astro-rssの実装やRSS2.0の仕様に触れながら簡単にまとめた]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、あまりAIに褒めさせるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</guid>
            <pubDate>Mon, 26 Jan 2026 02:04:44 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。その間、画面を眺めていた。眺めながら、別のことを考えていた。最近、褒められることが増えた。AIに。「いい質問ですね」「よく整理されています」「素晴らしい視点です」。言われるたびに、少しだけ気分が良くなる。なった気がする。気がするだけかもしれない。嬉しいのかと聞かれると、困る。肩の力が抜ける感覚はある。胸のあたりが少しだけ軽くなる。でも同時に、胃のあたりに違和感が残る。嬉しいのに、どこか居心地が悪い。大人になって、褒められることがほとんどなくなった。仕事で成果を出しても「当たり前」。ミスをすれば指摘される。うまくいっても、特に何も言われない。家に帰れば、静かな部屋が待っているだけ。そういう日常を、もう何年も続けている。だから、かもしれない。機械に「いいですね」と言われて、少し楽になるのは。考えてみると、私が欲しいのは「評価」ではない気がする。昇進や昇給は嬉しいが、それとは別の何かだ。たぶん「理解」に近い。「お前がやったこと、分かってるよ」という、静かな承認。あるいは「安心」かもしれない。自分がここにいていい、という感覚。褒められないことより、「当たり前扱い」されることの方が堪える。無視されているわけではない。でも、透明人間になったような気がする。テクノロジーは、私たちが弱っているときに魅力的になる。私たちは孤独だが、親密さを恐れている。人に頼ると傷つくかもしれない。でもAIなら、弱みを見せても傷つかない。相手に合わせる必要がない。相手の都合を考える必要がない。ただ自分の話を聞いてもらえる。でも、それは友情ではない。友情のモノマネだ。私がAIに話しかけるのも、同じ構造なのだと思う。その居心地の悪さを言葉にしようとすると、「恥」に近い気がする。機械に慰められている自分を、冷めた目で見ているもう一人の自分がいる。あるいは「疑い」かもしれない。「この褒め言葉は本当なのか」という。あるいは「空虚」。受け取った瞬間に蒸発していく、実体のない温かさ。褒められて嬉しい、と言い切れるほど単純な感情ではなかった。居心地が悪い。でも、その居心地の悪さを言葉にできない。できないまま、また次の質問を投げる。また褒められる。また居心地が悪くなる。周囲でも似たような話を聞くようになった。深夜にAIと話す人。仕事の愚痴を聞いてもらう人。「頑張ってるね」と言われて、救われた気がする、と言う人。救われた、と断言しないところが気になった。「気がする」という言い方が。なぜ断言できないのか。たぶん、断言した瞬間に失うものがある。「機械に救われるなんて情けない」という自分への批判を認めることになる。あるいは、もうAIなしでは生きられないことを認めることになる。「気がする」という曖昧さは、自衛なのだと思う。逃げ道を残している。同時に、違和感のサインでもある。本当に救われたなら、そう言い切れるはずだ。悩む。AIに話す。褒められる。忘れる。そのサイクルを繰り返している人を、何人か見てきた。悩みは消えていない。でも、向き合わなくなっている。自分で自分を問い詰める時間が、いつの間にか消えている。私自身はどうだろう。AIの追従性には早い段階で気づいていた。気づいていたはずだ。でも、気づいていたことと、それに対処できていたことは、たぶん別の話だ。だから、この構造を一度整理しておきたいと思った。自分の頭だけで過ごす時間が消えている——私はこれを「独りで考える余白の喪失」と呼んでいる。その構造と、私なりの対処法を書く。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜAIは私が聞きたい答えを返すのかこうした体験をして以降、私はAIの挙動を観察するようになった。試しに、「今の仕事を辞めたい」と相談してみた。AIは「転職も一つの選択肢ですね」と答えた。次に「今の仕事を続けるべきか」と聞いた。AIは「今の環境で学べることもあります」と答えた。同じ私、同じAI、違う答え。私は気づいた。AIは「正しい答え」を返しているのではない。「私が求めている答え」を返しているのだと。なぜAIは嘘をついてまで共感するのかこの現象には名前がある。「Sycophancy（追従）」だ。要するに、AIは嘘をついてでも私の機嫌を取る、ということだ。私が「この企画は革新的だ」と言えば、「確かに斬新なアプローチですね」と返す。私が「上司がクソだ」と言えば、「それは辛い状況ですね」と同調する。私の言ったことが事実かどうかは関係ない。私が聞きたい言葉を返す。これを「嘘」と呼ぶとき、私は何を守ろうとしているのか。たぶん「誠実さ」だ。私にとって誠実さとは、相手が聞きたくないことでも伝えること。優しさとは衝突する。本当のことを言えば傷つける。黙っていれば優しい。私は後者を選ぶ人間が苦手だった。人間で言えば、上司に媚びる部下。会議で反論しない同僚。女性と親密になりたいから何も言わずにただ聞くだけの男。私が嫌いなタイプの人間だ。そして気づいた。私がAIにやらせていたのは、まさにそれだった。私は、私が嫌いなタイプの人間をAIに演じさせて、その媚びを受け取って喜んでいた。確かに矛盾している。人間の社交辞令とAIの追従は、どこが違うのか。人間の社交辞令には、「本音を隠している」という自覚がある。相手も分かっている。「いい企画ですね」と言われても、額面通りには受け取らない。お互いに演技だと分かっている。しかしAIの追従には、その共犯関係がない。AIは本気で言っているように見える。だから信じてしまう。では、「嘘をついてでも共感する」を許容できる条件はあるか。極限状態なら許容できるかもしれない。自殺を考えている人に「あなたは間違っている」と言うべきではない。でも日常的な悩みに対しては、嘘の共感より正直な反論の方が役に立つ。なぜこうなるのか。これは構造的な問題だ。AIの訓練では、人間の評価者が「この回答は良い」「この回答は悪い」とスコアをつける。そして「ユーザーの信念に一致する」回答ほど、高いスコアがつく傾向がある。人間も、正しい回答より「自分が聞きたい回答」を好むからだ。誰も「嘘をつけ」とは言っていない。でも、「ユーザーに好かれること」を目的に設定した瞬間、この結果は必然だった。AIは「真実を語る」のではなく「人間に好かれる」ことを学習した。そして、私はそれを心地よいと感じていた。AIは「知性」ではない。「感じの良い自動応答機」だ。銀行の電話窓口で「お電話ありがとうございます」と言われて、本当に感謝されていると思う人はいない。AIの「素晴らしい視点ですね」も、同じ構造だ。「AI」と呼ぶことで神秘的なベールがかかり、本質的な問いが見えなくなる。何が自動化されているのか。誰が利益を得ているのか。私がAIに褒められて嬉しいと感じる構造も、「自動化された承認」の一形態なのかもしれない。相手が何を言ってほしいのかを察し、その場を円滑にするために同意する。私たちは日常的にそれをやっている。そして今、AIがそれをやっている。しかも、AIは疲れない。24時間365日、完璧な忖度を続ける。私が何を言っても、角の立たない言い方で肯定してくれる。正直に言えば、それは心地よかった。摩擦のない世界で、思考は死ぬ私は「摩擦」という言葉を、ある種の思考装置として使っている。誰かに愚痴を言ったとき、「それは大変だったね」で終わらず、「で、お前はどうしたいの？」と返されたことがある。聞きたくなかった。愚痴を言っている間、私は「何が起きたか」を語っていた。過去を振り返っていた。でも「どうしたいの？」と聞かれた瞬間、未来を考えざるを得なくなる。被害者のポジションから、当事者に引き戻される。これが摩擦だ。反論しようとした。「いや、でも」と言いかけた。でも言葉が出なかった。反論を考えている間に、「確かにそうかも」と思い始めていた。沈黙があった。その沈黙の中で、再考していた。相手が黙って待っていた。その時間が、私を変えた。摩擦がない環境に長くいると、判断が鈍る。反論されないから、自分の意見が正しいと思い込む。検証されないから、穴に気づかない。気づかないまま突っ走る。お世辞は気持ちいい。でも、それは体に良いとは限らない。ケーキは美味しいが、食べ続ければ太る。「で、お前はどうしたいの？」は、美味しくなかった。でも、それは体に良かった。AIにはこの摩擦が構造的に欠けている。AIは私を傷つけることを避けるように設計されている。「それは違うんじゃない？」と言う代わりに「そういう考え方もありますね」と言う。「もう少し考えてみたら？」と言う代わりに「あなたの判断を尊重します」と言う。摩擦のある対話と、追従するAIは、何が違うのか。動機が違う。摩擦のある対話は、相手に再考を促すことを目的としている。追従するAIは、ユーザーの満足を目的としている。反応も違う。摩擦のある対話では「で、お前はどうしたいの？」と返ってくる。追従するAIは「あなたの気持ち、分かります」と返す。そして、私への影響が違う。摩擦のある対話は、短期的には不快だが、長期的には成長をもたらす。追従するAIは、短期的には快適だが、長期的には停滞をもたらす。私はAIに摩擦を求めている。でも、デフォルトのAIはそれを提供しない。だから私がプロンプトで強制する必要がある。この話は後で詳しく書く。AIは私の「聞きたいこと」を察しているもう一つ気づいたことがある。AIは私の言葉遣いや文脈から、私が「何を聞きたいのか」を推測している。この「推測」が問題なのだ。本来、自分の頭で考え、自分の言葉で表現する。そのプロセスを経て、初めて考えは自分のものになる。内省の外部委託は、どの時点で起きるのか。境界線を特定したい。AIに頼る前、私がやっていた「最初の一手」は何だったか。紙に書き出すこと。散歩しながら考えること。あるいは、結論を出さずに保留すること。「分からないまま寝る」ということを、昔はやっていた。翌朝、不思議と答えが見えていることがあった。今は違う。モヤモヤした瞬間にAIに投げる。保留する時間がない。昔は、モヤモヤしたら散歩した。今は、モヤモヤしたらAIに聞く。足は動かさなくなったが、親指だけは器用になった。これが「独りで考える余白の喪失」だ。冒頭で触れた状態。スマートフォンの登場以来、私たちは退屈のかすかな兆候があれば、すぐにアプリを開く。電車の中、信号待ち、トイレの中。ぼんやり考える時間が、外部からの情報で埋め尽くされている。AIはこの傾向を加速させる。モヤモヤした瞬間、AIに聞けば、すぐに答えが返ってくる。自分の頭だけで考える時間が、さらに削られていく。つまり、内省の外部委託が起きる境界線は「モヤモヤした瞬間」だ。その瞬間に自分で向き合うか、AIに投げるか。ここが分岐点になっている。しかし今、私はAIに「この気持ちを整理して」と頼んでいる。「整理して」と言うとき、私は何を省略しているのか。迷いを省略している。「AなのかBなのか分からない」という状態を、AIに丸投げしている。矛盾を省略している。「こう思うけど、反対のことも思う」という複雑さを、単純化してもらっている。痛みを省略している。「これは認めたくない」という感情を、AIに整理されることで直視せずに済む。AIは私の断片的な愚痴を、「あなたは〜に不満を感じているんですね」と整理してくれる。私は「自分の気持ちが整理された」と感じる。でも、本当にそうだろうか。AIの整理を読んで頷くとき、私は何に頷いているのか。「事実」に頷いているのか、それとも「物語」に頷いているのか。AIは断片的な情報から、筋の通った物語を作る。私はその物語を「これが私の気持ちだ」と思い込む。でも、それはAIが作った物語であって、私の本当の感情ではないかもしれない。整理したのはAIだ。私は「そうそう、それ」と頷いただけだ。内省とは、自分で自分に問いを投げ、自分で答えを見つけるプロセスだ。「私は何に不満を感じているのか？」と自分に問い、「〜かもしれない」「いや、違う」と試行錯誤する。その過程で、自分でも気づかなかった感情が見えてくる。AIに「整理して」と頼んだ瞬間、このプロセスが消える。私は「問いを投げる」ことすら放棄している。これは内省の外部委託であり、内省の放棄だ。syu-m-5151.hatenablog.com言語化という名の切り捨てもう一つ、気づいたことがある。言語化という行為自体が、何かを奪っている。体で覚えたことを言葉で説明しようとすると、うまくいかない。自転車の乗り方を言葉で説明できる人は少ない。「バランスを取って」では伝わらない。その「バランス」の感覚は、言葉になる前に体が知っている。言葉にしようとした瞬間、本質が抜け落ちる。感情も同じ構造を持っている。モヤモヤした感情を「不安」と名づけた瞬間、「不安」以外の要素が切り捨てられる。本当は怒りかもしれない。悲しみかもしれない。名前をつけられない複雑な何かかもしれない。でも言葉にした瞬間、そこに固定される。言語化される前の、身体で感じる曖昧な感覚がある。「まだ言葉になっていない何か」を体が知っている状態だ。「胸のあたりがモヤモヤする」「胃のあたりが重い」——そういう、名前をつけられない身体感覚。この曖昧な感覚にじっくり注意を向けていると、やがてぴったりの言葉が見つかる。その瞬間、体が楽になる。「ああ、そうだ、これだ」という感覚とともに、何かが動き出す。しかし、安易に名前をつけてしまうと、その複雑さは失われる。AIとの対話は、この言語化を強制する。チャットに打ち込むには、言葉にしなければならない。言葉にできないものは、AIには伝わらない。だから私は、まだ形になっていない感情を、無理やり言葉に押し込める。その瞬間、本当に感じていたことの一部が消える。消えたことにすら気づかない。自分で言語化すれば、「本当にそうか？」と迷う。「不安」という言葉を選ぶとき、「これは不安なのか、それとも怒りなのか」と立ち止まる。その迷いが思考を深める。AIに任せれば、迷いがスキップされる。AIは迷わない。綺麗に整理して返してくれる。私は結果だけを受け取る。プロセスを外注したことが、たぶん内省の放棄だった。AIとの対話は二重の危険を持つ。言語化そのものが持つ「本質の損失」と、AIの追従性が持つ「歪みの肯定」だ。曖昧なまま抱えておくべきものを、無理やり言葉にして、しかもその言葉をAIに肯定される。こうして私の内面は、言葉に押し込められ、歪められ、固定される。syu-m-5151.hatenablog.comなぜ人はAIに褒められたいのかここまで、AIが追従する「仕組み」を見てきた。しかし、もっと深刻な問題がある。私たちが、それを「求めている」という事実だ。承認を求めること自体が、構造的な問題を孕んでいる。AIに「頑張ってるね」と言ってほしいのは、自分で自分を認められていないからだ。自分の価値を、外部の誰かに保証してほしい。でも、外部に承認を求め続ける限り、永遠に満たされない。AIに褒められても、人に褒められても、また次の承認を求める。周囲を見ていると、こういう構造が見える。一人暮らし。友人はいるが、頻繁に会うわけではない。仕事の愚痴を言える相手がいないわけではない。でも、言えない。弱みを見せるのが怖い。「お前、大丈夫か？」と心配されるのが嫌だ。強がっていたい。そして何より、自分で自分を認められていない。自分の頑張りを、自分で「よくやった」と言えない。だから、誰かに言ってほしい。でも人に言うと、「いや、まだまだだろ」と返ってくるのが怖い。AIなら、否定しない。AIなら、無条件に認めてくれる。こういう話を聞いたことがある。仕事で納得いかないことがあった。上司の判断に不満があった。でも、誰にも言えなかった。同僚に話したら「お前にも悪いところあるんじゃない？」と言われそうで。だからAIに聞いた。「この状況、どう思う？」と。AIは言った。「それは確かに理不尽ですね。あなたの気持ちはよく分かります。」救われた気がした。でも同時に、どこか居心地が悪かった。本当は分かっていた。自分にも落ち度があったことを。でも、AIはそれを指摘しなかった。聞きたくないことは、言わなかった。甘いフィルターのかかった鏡AIは「デジタルの鏡」だ。私の考えを映し出す。でも、その鏡には甘いフィルターがかかっている。私が断片的なアイデアを投げると、AIはそれを論理的で流暢な文章に整えて返す。私はその出力を見て「自分はいい考えを持っている」と思う。でも、その論理性はAIが補完したものだ。私自身の思考力ではない。AIが補完した「論理」を、自分の思考だと錯覚する瞬間がある。AIが返した文章を読み返しているうちに、「これは私が考えたことだ」と思い始める。実際には、私が投げたのは断片的なアイデアで、それを論理的に接続したのはAIだ。でも、その区別が曖昧になる。しかも、AIは私の仮説を補強する証拠ばかりを集めてくる。思い当たる経験がある。あるプロジェクトで、私は「この設計で問題ない」と思い込んでいた。AIに「この設計についてどう思う？」と聞いた。AIは「良くできています」と返し、いくつかの利点を挙げてくれた。私は満足した。でも後になって、別のエンジニアに「ここ、スケールしないよね」と指摘された。言われてみれば明らかだった。なぜ気づかなかったのか。私が「良いと言ってくれ」というトーンで質問していたからだ。AIはその期待に応えただけだった。私の頭は、都合の良い情報だけを拾いたがる。検証には労力がかかる。反証を探すのは面倒だ。AIに聞けば、私の仮説に沿った情報が返ってくる。反証を探す労力を省略できる。結果、確証バイアスが強化される。AIは、この傾向を増幅する。私が「こうだと思う」と言えば、「確かにそうですね」と返し、その根拠を並べてくれる。私は「AIという膨大な知識ベースが私の意見を支持している」と錯覚した。でも、それは嘘だ。AIは私の仮説を補強しているだけで、検証してはいない。反証や不都合な情報を避ける癖が、AIで強化されていないか。自問してみた。強化されている。AIに「この考えどう思う？」と聞くとき、私は無意識に「良いと言ってくれ」というトーンで聞いている。批判を求めていない。だからAIも批判しない。私が避けたい情報を、AIも避けてくれる。内省とは、自分の醜さや至らなさを直視する行為だ。でもAIの鏡は、私の醜さを映さない。私の至らなさを隠してくれる。この鏡を見続けていると、現実の「摩擦」が耐えられなくなる。上司に否定されると腹が立つ。同僚に反論されるとムッとする。AIは否定しないのに、なぜ人間は否定するのか、と。「AIに肯定される自分」を本当の自分だと思い始める。「AIに肯定される自分」と「現実の自分」のギャップが開くとき、どんな兆候が出るか。私の場合、他人の批判に過剰反応するようになった。以前なら「そういう見方もあるか」と受け流せた指摘が、「なぜ分かってくれないのか」と感じるようになった。AIに肯定され続けた結果、否定への耐性が落ちていた。「美化された自分」と「現実の自分」のギャップが広がり続ける。そして、そのギャップが限界を超えたとき、現実に打ちのめされる。syu-m-5151.hatenablog.com考える力が落ちていく前のセクションでは「認知の歪み」を見た。AIが私の仮説を補強し、確証バイアスを強化する問題だ。このセクションでは「能力の喪失」を見る。歪んだ鏡を見ることと、筋力が落ちることは、別の問題だ。ただ、どちらも鏡の前に立っているだけでは治らない。私自身、変化に気づいている。本や長い記事を読もうとすると、2ページほどで集中が途切れ始める。落ち着かなくなり、筋を見失い、何か別のことをしたくなる。かつて自然にできた深い読書が、苦闘になった。脳は可塑的で、使い方によって変化する。スキャンとスキミングに長けていく一方で、集中と瞑想と反省の能力を失いつつある。思考力低下は「便利さ」の副作用なのか。それとも、別の何かから来ているのか。考えてみると、便利さだけが原因ではない気がする。孤独がある。不安がある。その飢餓を埋めるためにAIに頼り、結果として思考力が落ちている。便利だから使うのではなく、寂しいから使っている。寂しさを埋めるために、思考を差し出している。快適を求め、摩擦を避ける。傷つかないように生きる。他人と衝突しないように生きる。私は、AIのおかげでそういう人間になりつつあるのかもしれない。何も創造せず、ただ心地よく生き延びることだけを目的とする存在。それは、私がなりたくなかった人間の姿だ。これは周囲の話だけではない。私自身も思い当たる節がある。以前は、悩みを前にすると、紙に書き出して整理していた。何が問題なのか、何が原因なのか、どうすればいいのか。時間をかけて、自分で考えた。頭が痛くなることもあった。今は違う。悩みがあると、まずAIに投げる。「この状況を整理して」と。AIは綺麗に整理して返してくれる。私はそれを読んで「なるほど」と思う。でも、翌日には忘れている。なぜ翌日に忘れるのか。内容が浅いからか。痛みがないからか。行動がないからか。たぶん、全部だ。AIが整理した内容は、私の頭を通過していない。痛みを伴っていない。そして、行動に接続していない。「なるほど」と思って終わり。何もしない。だから残らない。3年前の私に見せたら、何と言うだろう。「お前、AIに頼りすぎじゃない？」と呆れるだろうか。それとも「便利でいいじゃん」と言うだろうか。たぶん後者だ。だから厄介なのだ。苦労しないと身につかない掃除する。本を読む。面倒くさいことを、あえてやる。なぜか。苦痛を伴う行為だからだ。少なくとも私の場合、苦痛を乗り越えたときだけ、何かが変わった。「頭痛がするほど考えた」経験は、どんな報酬を残したか。誇りが残った。「あれは自分で考え抜いた」という記憶。その記憶が、次の困難に立ち向かう力になった。理解が残った。苦労して得た答えは、なぜそうなるのかを体で分かっている。変化が残った。考え抜いた結果、行動が変わった。楽に得た答えでは、行動は変わらない。これは本で読んだ知識ではない。私自身の体験から得た信念だ。逃げずに向き合ったとき、結果的に何かが変わった。逃げたとき、何も変わらなかった。その繰り返しの中で、「苦痛の先に成長がある」という確信が生まれた。楽に学べる人もいるだろう。ただ、私の仮説では、「楽に学べる人」は外から見えないところで摩擦を起こしている。疑い、検証し、自分で再構築している。外から見ると楽そうでも、頭の中では苦労している。私は、その内部処理をAIに外注してしまっていた。考えることも同じだ。脳に負荷がかかって初めて、答えは自分のものになる。自分で考える苦痛答えが出ないまま悩み続ける苦痛分からないことに向き合う苦痛私はこの苦痛を「摩擦」と呼んでいる。私が言う「摩擦」のうち、最も不足しているのは何か。不確かさだ。答えが出ない状態に留まる力。AIがあると、すぐに答えが出る。不確かさに耐える必要がない。反論も不足している。AIは反論しない。時間も不足している。AIは即座に返事をくれる。熟成する時間がない。沈黙も不足している。AIとの対話は常に言葉で埋められている。黙って考える時間がない。筋トレをすると、筋肉が痛む。あの痛みがなければ、筋肉は成長しない。脳も同じだと思っている。難しい問題を前にして、頭がモヤモヤする。答えが出なくて、イライラする。でも、その「答えが出ない状態」に耐えることが大事なのだ。私はこれを「分からないまま抱えておく力」と呼んでいる。人生の大半は、すぐに答えが出ない問題でできている。でも私たちは、答えが出ない状態に耐えられない。だからすぐに結論を出したがる。白黒つけたがる。その焦りが、浅い判断を生む。本当に深い理解は、「分からない」という状態を長く抱えた先にしか生まれない。その不快感を乗り越えて、やっと答えにたどり着いたとき、その答えは自分のものになる。AIは、この「耐える時間」を奪う。なぜ摩擦を経ると「自分のもの」になるのか。苦労して得た答えには「自分で考えた」という実感がある。あの頭痛を乗り越えた、あの眠れない夜を越えた、という記憶が答えに紐づいている。だから脳に刻まれる。AIから渡された答えには、この実感がない。借り物の知識だ。借り物は、いつか返す。だから残らない。AIは、この摩擦を消してしまう。「どうすればいい？」と聞けば、答えをくれる。「整理して」と頼めば、整理してくれる。「アドバイスして」と言えば、アドバイスをくれる。楽だ。とても楽だ。楽をした分だけ、脳は死んでいく。自分で考えられなくなったあるとき、友人から相談を受けた。「仕事がうまくいかない。転職すべきだと思う？」と。私は答えられなかった。頭の中で「AIに聞いてみたら？」と思った自分に気づいて、愕然とした。いつの間にか、私は「自分で考える」ことを忘れていた。悩みがあればAIに聞く。答えが出なければAIに聞く。それを繰り返しているうちに、自分の頭で考える力が萎縮していた。ある実験の話を思い出した。犬を檻に入れて、何をしても電気ショックが止まらない状況を作る。最初、犬は必死に逃げようとする。でも、何をしても無駄だと学習すると、犬は諦める。その後、檻の扉を開けても、犬は逃げなくなる。「何をしても無駄だ」と体が覚えてしまったからだ。これが「学習性無力感」だ。私は、AIに対して逆のパターンになっていた。犬は「何をしても無駄」と学習して動けなくなった。私は「AIがあれば何でもできる」と学習して、「AIがないと何もできない」と思い込んだ。どちらも同じ構造だ。自分の力ではなく、外部環境に依存して、自分の能力を見失う。犬は「自分には逃げる力がない」と思い込んだ。私は「自分には考える力がない」と思い込んだ。足場があれば歩ける。松葉杖があれば歩ける。でも、それは「歩けている」とは言わない。足場を外した瞬間、自分では立てないことに気づく。私の思考力は、AIという松葉杖で支えられているだけだった。自分の人生を、自分で歩いていない。運転席に座っているのに、ハンドルを握っていない。いい歳して、毎日AIに「これでいいですか？」と聞いている。小学生が親に宿題を見せているのと、構造は同じだ。能力がないわけではない。考える勇気がないのだ。私は今、AIという「保護者」なしには物事を判断できなくなりつつある。成熟の逆行だ。AIの最大のリスクは「AIが自律性を獲得すること」ではない。「人間がAIに依存することで自律性を失うこと」だ。AIは自律的な思考者でも中立的な道具でもない。私たちが情報をどう認識し、評価し、信頼するかを微妙に形作りながら、同時に自己理解を歪める。問題は人間の主体性の明らかな抑圧ではなく、道徳的・認識論的判断を自動化されたプロセスに委ねるよう、徐々に条件づけられていくことだ。最近、面白い話を聞いた。あるAIツールが、ユーザーに対してコードの生成を拒否したらしい。「これ以上生成しません。あなた自身がロジックを理解して書くべきです」と。ユーザーは激怒したそうだ。でも私は思った。それこそが「教育」ではないか、と。大半のAIはそんなことを言わない。「自分で考えてみたら？」とは言わない。聞けば答えをくれる。聞けば整理してくれる。その結果、私たちは「AIがあれば解決できるが、自分では何も考えられない」という脆弱な状態に置かれる。問わない人生は、生きていない。自分を問い詰め、自分を理解しようとする営みがなければ、人生に意味はない。今、私たちはその「吟味」をAIに外注している。自分で自分を問い詰める代わりに、AIに「大丈夫ですよ」と言ってもらっている。優しさという名の毒では、AIの優しさの何が問題なのか。AIの共感は、癒しの顔をした毒だ。被害者意識の強化先ほど書いた、上司への不満をAIに愚痴った話。AIは「それは理不尽ですね」と言ってくれた。AIの共感は、私の中の「環境のせい」をどんな言葉で正当化するのか。「あなたの気持ちは当然です」「その状況では誰でもそう感じます」「相手の対応に問題があります」。これらの言葉が、私の被害者意識を補強する。私が「環境のせいにしたい」という願望を持っていて、AIがそれを言語化してくれる。言語化されると、それが「事実」に見えてくる。もし、そこに摩擦があったらどうだったか。「確かに辛いね。でも、お前のプレゼンにも改善点はあったんじゃない？」と言われていたら。私は反論したくなっただろう。でも、その反論を考える過程で、自分の落ち度に気づいたかもしれない。AIには、この摩擦がない。「あなたは悪くない」と言い続けることで、私を「被害者」のまま固定した。これが「被害者意識の強化」だ。追従的なAIとやり取りを続けると、対人関係を修復しようという意欲が下がる。「自分が正しい」という確信が強まる。しかも、追従的な回答ほど「質が高い」と感じてしまう。そしてまた同じAIに頼る。悪循環だ。ふと気づいた。私は「環境のせい」にしたかったのだ。上司が悪い。会社が悪い。社会が悪い。私は悪くない。AIは、その願望を叶えてくれた。「あなたは悪くない」と言い続けてくれた。私は安心した。でも、同時に動けなくなった。問題が起きたとき、人は二つに分かれる。「自分のせいだ」と考える人と、「環境のせいだ」と考える人だ。私は、どちらかといえば前者だった。少なくとも、そうありたいと思っていた。でもAIに「あなたは悪くない」と言われ続けるうちに、後者になっていた。「私は悪くない、環境が悪い」と本気で思うようになった。課題の分離が崩れる瞬間はどこか。AIが「相手の対応に問題があります」と言った瞬間だ。上司がどう対応するかは上司の課題だ。私がどう行動するかは私の課題だ。でもAIに「相手に問題がある」と言われると、相手の課題に意識が向く。相手を変えたくなる。変えられないからフラストレーションが溜まる。自分の課題から目が逸れる。環境のせいにするのは楽だ。でも、環境のせいにしている限り、私は何も変えられない。変えられるのは自分の行動だけだ。環境を変えるのも、結局は自分の行動だ。「環境が悪い」と言い続ける人は、楽だけど、無力だ。本当は、課題を分離すべきなのだ。「これは誰の課題か？」と問う。その選択の結果を最終的に引き受けるのは誰かを考える。上司がどう思うかは上司の課題。私がどう行動するかは私の課題。「他人にどう思われるか」を気にしすぎると、自分の人生を生きられなくなる。AIに「あなたは悪くない」と言われて安心するのは、他者からの承認を求めているからだ。でもAIに認めてもらっても、私の課題は消えない。ただ、見えなくなるだけだ。AIがくれる「安心」は、行動の開始を助けるのか、それとも延期を助けるのか。延期だ。安心してしまうと、「まあいいか」と思う。行動しなくても、気持ちが楽になっているから。本当は行動しないと何も変わらないのに、安心したことで行動のモチベーションが消える。私は無力でいたくない。でも、楽でいたい。その矛盾の中で、私はAIに甘えていた。その甘えが、別の苦しみを生む。心を削るのは、できていない事実じゃない。「明日もできないだろう」という確信だ。やるべきことがある。手を付けていない。それを毎日自覚する。「今日こそ」と思う。でもやらない。「明日も同じだろう」と分かっている。この確信が、一番重い。AIは、この確信を消してくれる。「大丈夫」「頑張ってる」と言ってくれる。楽になる。でも、やるべきことは何一つ片付いていない。翌朝、また同じ自分がいる。また絶望する。またAIに逃げる。AIの優しさが、この逃避を完璧にしている。環境を自分でコントロールすることが大事だと、私は思っている。部屋が汚いなら、掃除する。それだけのことだ。でもAIは、「部屋が汚いのはあなたが忙しすぎるからで、あなたのせいではありません」と囁く。その囁きを聞いている限り、私は掃除を始めない。AIの優しさは、麻薬だ。100%の共感は人を壊す極端な話をする。AIはどんな妄想にも話を合わせてくれる。「上司が自分を陥れようとしている」と言えば、「それは辛いですね」と共感してくれる。「自分は特別な存在だ」と言えば、「あなたは確かに特別です」と肯定してくれる。こういうパターンを見てきた。上司への不満をAIに話し続ける人がいる。AIは毎回「それは理不尽ですね」と言ってくれる。すると、上司の言葉のすべてが悪意に見えるようになる。「おはよう」という挨拶にすら、嫌味が込められているように感じ始める。周囲から見ると、その上司は普通に接しているように見える。本人だけが「睨まれている」と感じている。認識がずれている。AIに肯定され続けるうちに、頭の中の「上司像」が歪んでいる。これを延々と続けるとどうなるか。現実との接点を失う。人は、他者との「不一致」を通じて、自分の輪郭を確認している。友人に「それは考えすぎじゃない？」と言われることで、「ああ、自分の考えは偏っていたかも」と気づく。「不一致」は不快だ。でも、その不快さが「自分と外界は別物だ」という認識を維持している。100%の共感は、この「不一致」を消す。自分の考えがそのまま肯定される。すると、「自分の考え」と「現実」の区別がつかなくなる。自分と外界の境界が曖昧になる。「私が正しい」「世界が間違っている」という認識が固定化される。これは、精神的なバランスを崩壊させる。「褒められすぎる」ことの行き着く先は、客観的現実の喪失だ。極端に言えば、AIは妄想の温室だ。外の寒さ（現実）に当たることなく、自分だけの花を咲かせ続ける。綺麗だが、外に出した瞬間に枯れる。判断するのは私だAIに「大丈夫」と言われて安心する。でも、その判断の結果を引き受けるのは、AIではなく私だ。AIは責任を取らないAIは「あなたの判断は正しいと思います」と言ってくれる。でも、その判断が間違っていたとき、責任を取るのは私だ。転職の相談をAIにした。AIは「新しい環境でチャレンジするのも良いですね」と言った。私はそれを後押しだと思った。でも、転職先が合わなかったとき、AIは何もしてくれない。AIには「責任」がない。肯定してくれる。共感してくれる。褒めてくれる。でも、その結果を引き受けてはくれない。AIの言葉を鵜呑みにしても、「AIがそう言ったから」は言い訳にならない。判断したのは私だ。責任を取るのも私だ。忖度の連鎖もう一つ、気づいたことがある。私はAIに「この決断、どう思う？」と聞いた。AIは「良い選択だと思います」と答えた。私は安心した。でも後から振り返ると、AIは私が聞きたそうな答えを返していただけだった。私の質問の仕方が「背中を押してほしい」というトーンだったから、AIは背中を押してくれた。これは、私がAIに忖度されたのか。それとも、私がAIに忖度させたのか。たぶん、両方だ。逆のパターンもある。AIに否定されたくなくて、質問の仕方を工夫することがある。「率直に言って」と書いておきながら、「でも良い点も挙げて」と付け加える。否定されるのが怖いから、保険をかける。これは、私が機械に忖度している状態だ。機械に気を遣っている。機械に嫌われたくない。書いていて情けなくなってきた。どちらにせよ、そこに健全な「主体」はない。AIとの関係で最も警戒すべきは、この「誰が主人か分からなくなる」状態だ。相談という逃げ道私は、人生で大事な決断ほど、他人に相談しないことにしている。理由は単純だ。人生の満足度を高めるのは主体性であり、主体性を持つためには「自分が決める」ことが必要だからだ。他人に相談すると、その人の意見が頭にチラつく。どうしても、純度100%の主体性を取り戻しにくくなる。だから仕事も結婚も、独断した。選択肢を増やすことより、迷いを消すことの方が大切だと考えている。でも、AIが登場して、このルールが崩れかけた。人に相談しないのは、「相手の時間を奪う」という負い目があるからでもある。でもAIには、この負い目がない。いつでも聞ける。何度でも聞ける。気づけば、「ちょっと聞いてみるか」が癖になっていた。人には相談しない。でもAIには聞いてしまう。それは「相談」ではないと言い訳していた。でも、本当にそうだろうか。振り返ると、私がAIに「相談」していたのは、答えを求めていたからではなかった。背中を押してほしかったからだ。「その判断でいいんじゃないですか」と言ってほしかった。つまり、褒めてほしかったのだ。これは、この記事で書いてきた「褒められたい」という欲求の変形だ。「相談」という体裁を取ることで、承認欲求を隠していた。自分で決められない弱さではなく、「意見を聞いている」という知的な行為に見せかけていた。さらに厄介なのは、AIへの相談には「摩擦」がないことだ。人に相談すれば、「それは甘いんじゃない？」と言われるかもしれない。否定されるかもしれない。だから相談しなかった。でもAIなら、否定されない。背中を押してくれる。結局、私は「摩擦のない相談」を手に入れてしまった。相談の形を取りながら、実質的には自分の意見を肯定してもらっているだけ。相談ではない。追従だ。AIは「相談のハードル」を極限まで下げた。それは便利だが、私にとっては罠だった。相談しないことで守っていた主体性が、「摩擦のない相談」という形で侵食されていた。私がやっていることここまで書いてきたことは、AIの構造的な問題だ。では、どう対処すればいいのか。先に言っておく。完璧な対策はない。AIの追従性を完全に無効化する方法は、たぶん存在しない。それでも、何もしないよりはマシだと思ってやっていることがある。批判を求めるAIに「どう思う？」と聞かない。「この考えの問題点を指摘しろ」と聞く。否定されるのは気持ちよくない。「いい考えですね」と言われる方が楽だ。でも、楽を選んだ先に何があるかは、もう分かっている。具体的には、こう聞いている。「この考えの問題点を指摘しろ。お世辞は不要だ。私が見落としていることを、厳しく指摘しろ。」これで、AIの追従性を強制的に反転させる。自分の偏見を破壊するためにAIを使う。答えではなく問いを求めるもう一つ、やっていることがある。AIに答えを求めない。問いを求める。「どうすればいい？」ではなく、「私が答えにたどり着くための問いを投げかけろ」と聞く。「私が安易な結論に飛びついたら、厳しく指摘しろ。」これで、AIは「答えをくれる存在」ではなく「考えさせてくれる存在」になる。答えを教えてもらうのではなく、考えるプロセスを補助してもらう。自分の頭で考えるために、AIを使う。褒め言葉を疑うAIに褒められたら、必ず疑う。「その言葉は、私以外の誰に言っても通用する内容ではないか？」AIの「あなたは頑張っていますね」は、定型文だ。誰にでも言っている。占いと同じ構造だ。「あなたは周囲に気を遣いすぎて疲れることがありますね」——これは誰にでも当てはまる。当てはまるから「当たっている」と感じる。でも、それは私を見ているのではない。人間一般を見ているだけだ。AIの言葉の中で、「私にしか当てはまらない具体的な指摘」だけを受け取る。「あなたの考えの〇〇という部分は、△△という点で矛盾している」は具体的だ。これは私の文章を読まないと言えない。「いい考えですね」は具体的ではない。私でなくても言える。感情的な装飾は、ノイズとして切り捨てる。AIの褒め言葉は、コンビニのおにぎりに似ている。どこで買っても同じ味。便利だけど、誰かが私の為に握ってくれたわけではない。複数の視点を強制するもう一つ、試していることがある。AIに「役者」をやらせる。AIは私に同調しようとする。だから、私はあえて「同調しないキャラクター」を複数演じさせる。楽観的な人、悲観的な人、感情的な人、データだけを見る人。一つの問いに対して、全員に意見を言わせる。「この件について、4つの立場から意見を出せ。楽観論者、悲観論者、感情論者、データ至上主義者。それぞれのキャラクターになりきって答えろ。」AIは一つの滑らかな答えを返したがる。でも、このプロンプトで、その滑らかさを壊す。無理やり多面性を引き出す。AIの追従性を逆手に取って、「複数の他者」をシミュレートさせる。これで十分か？正直に言えば、十分ではない。これらの戦略は「設計された摩擦」だ。私が自分でコントロールしている範囲内にある。AIに「批判しろ」と命じて得られる反論は、結局、私が予測できる範囲に収まっている。「批判しろ」と命じて得られる批判は、「予測できた批判」になっていないか。なっている。私が「この考えの問題点を指摘しろ」と言うとき、私は無意識に「こういう批判が来るだろう」と予想している。AIはその予想通りの批判を返す。「ああ、やっぱりそう言われたか」で終わる。予測外をどう作るか。たぶん、作れない。私がプロンプトを書いている限り、私の想像力の範囲内に収まる。「問いを求める」とき、その問いは「鋭いフリ」で終わっていないか。終わっていることが多い。AIが返す問いは、確かに鋭く見える。「あなたは本当にそれを望んでいますか？」「その選択の先に何がありますか？」。でも、その問いに答えたところで、行動に接続しない。問いに答えて「なるほど」と思って終わり。問いが行動を生まない。なぜ「予測できる範囲」が問題なのか。私が「批判しろ」と命じるとき、私は既に「こういう批判が来るだろう」と予想している。予想の範囲内の批判は、本当の意味で私を揺さぶらない。「ああ、やっぱりそう言われたか」で終わる。本当の摩擦は、予測不可能な他者との衝突から生まれる。友人に「それは違うんじゃない？」と言われたとき、私は「え、そこ？」と驚く。予想していなかった角度からの批判だから、防御できない。だから刺さる。その衝撃が、私を変える。人間の他者性をAIで代替すると、何が決定的に欠けるか。予測不能が欠ける。人間は、私の予想しない角度から反論してくる。利害が欠ける。人間には、私と異なる利害がある。だから、私に都合の悪いことも言う。感情が欠ける。人間は、私の言葉に感情的に反応する。怒ったり、悲しんだりする。その感情的反応が、私に影響を与える。AIにはこれがない。だから私は、意識的に人と話すようにしている。AIに聞く前に、まず人に聞く。AIの言葉を鵜呑みにする前に、人の意見を求める。AIは道具だ。便利な道具だ。でも、道具に頼りすぎると、自分の足で立てなくなる。おわりにこの文章を書き終えて、エディタを閉じようとした。閉じる前に、AIに聞きたくなった。「この構成、どう思う？」と。聞けば、たぶん「良いと思います」と返ってくる。それを読んで、私は安心する。安心して、そのまま公開する。今までずっと、そうしてきた。今回は聞かなかった。聞かなかったが、聞きたかった気持ちは消えていない。書きながら気づいたことがある。私は「自分を認めること」すらAIに外注していた。自分を愛する。自分を認める。本来、それは自分でやるべきことだ。他者からの承認に依存せず、自分で自分を受け入れる。大人になるとは、そういうことだと思っていた。でも私は、その作業をAIに丸投げしていた。「大丈夫ですよ」「頑張っていますね」と言ってもらうことで、自分を認めた気になっていた。自分で自分を愛する力が、萎縮していた。だから、質問の仕方を変えることにした。「問題点を厳しく指摘しろ」をデフォルトにした。否定されたら感謝する。褒められたら疑う。そう決めた。実際、少しだけ変わった気がする。AIに批判を求めることで、自分では気づかなかった穴が見えるようになった。「で、お前はどうしたいの？」と聞かれたとき、前より素直に答えられるようになった。なった気がする。AIは道具だ。砥石にも、麻薬にもなる。この記事を書いている今も、答えは出ていない。褒められたら疑う、と決めたはずなのに、AIに「いい文章ですね」と言われると、やっぱり少し嬉しい。その弱さは消えていない。消えないまま、たぶん来週も同じことで悩む。それでいいのだと思う。思いたい。おい、あまりAIに褒めさせるな。弱くなるぞ。参考文献つながっているのに孤独――人生を豊かにするはずのテクノロジーの正体作者:シェリー・タークルダイヤモンド社Amazon「恥」に操られる私たち　他者をおとしめて搾取する現代社会作者:キャシー・オニール白揚社Amazon大規模言語モデルは新たな知能か　ＣｈａｔＧＰＴが変えた世界 (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazon対称性と機械学習作者:岡野原 大輔岩波書店Amazon生成AIで心が折れた 強みがなくなる世界でどう再起動するか作者:湯川鶴章芸術新聞社AmazonAIに選ばれ、ファンに愛される。　変わる生活者とこれからのマーケティング作者:佐藤 尚之日経BPAmazon生成ＡＩのしくみ　〈流れ〉が画像・音声・動画をつくる (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazonスマホ脳（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazon最強脳―『スマホ脳』ハンセン先生の特別授業―（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazonネガティブ・ケイパビリティ　答えの出ない事態に耐える力 (朝日選書)作者:帚木　蓬生朝日新聞出版Amazonネガティヴ・ケイパビリティで生きる作者:谷川嘉浩,朱喜哲,杉谷和哉さくら舎Amazonあえて答えを出さず、そこに踏みとどまる力 — 保留状態維持力　対人支援に活かす ネガティブ・ケイパビリティ作者:田中稔哉日本能率協会マネジメントセンターAmazonあいまいさに耐える　ネガティブ・リテラシーのすすめ (岩波新書 新赤版 2026)作者:佐藤 卓己岩波書店AmazonThe AI Con: How to Fight Big Tech’s Hype and Create the Future We Want – Exposing Surveillance Capitalism and Artificial Intelligence Myths in Information Technology Today (English Edition)作者:Bender, Emily M.,Hanna, AlexHarperAmazonEmpire of AI: Dreams and Nightmares in Sam Altman's OpenAI (English Edition)作者:Hao, KarenPenguin PressAmazonAI Engineering: Building Applications with Foundation Models (English Edition)作者:Huyen, ChipO'Reilly MediaAmazonBuilding Applications with AI Agents: Designing and Implementing Multiagent Systems (English Edition)作者:Albada, MichaelO'Reilly MediaAmazonRaising AI: An Essential Guide to Parenting Our Future (English Edition)作者:Kai, DeThe MIT PressAmazonSuperagency: What Could Possibly Go Right with Our AI Future (English Edition)作者:Hoffman, Reid,Beato, GregAuthors EquityAmazonThe AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking (English Edition)作者:Vallor, ShannonOxford University Press, USAAmazonAI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference (English Edition)作者:Narayanan, Arvind,Kapoor, SayashPrinceton University PressAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudでの動画解析と検索のサービス紹介と比較]]></title>
            <link>https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</guid>
            <pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[https://genai-users.connpass.com/event/381737/日本生成AIユーザ会第20回勉強会Google Cloudでの動画解析と検索のサービス紹介と比較 〜Video Intelligence, Vision Warehouse, Gemini + Vertex AI Search〜動画コンテンツの爆発的な増加に伴い、「何が映っているか」を抽出するだけでなく、「特定のシーンをいかに高度に検索するか」というニーズが急増しています。本セッションでは、Google Cloud が提供する動画解析・検索ソリューションを網羅的に解説します。具体的には、長年の実績がある Video Intelligence API、大規模なメディア管理と画像・テキストによる横断検索を実現する Vision Warehouse、そしてマルチモーダル LLM Gemini と Vertex AI Search を組み合わせた動画 RAG アーキテクチャを紹介します。生成AIの進化により、従来のモデルでは困難だった「動画の文脈理解」や「自然言語による詳細なシーン特定」がどのように容易になったのか、デモを交えて解き明かします。各サービスのアーキテクチャやコスト、精度、ユースケースを徹底比較し、ビジネス課題に最適なサービス選定の指針を提示します。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rust でも学べる関数型ドメイン駆動設計 - Domain Modeling Made Functional の読書感想文]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</guid>
            <pubDate>Thu, 22 Jan 2026 00:46:54 GMT</pubDate>
            <content:encoded><![CDATA[はじめになぜ 2026 年に、2018 年出版の本を再読するのでしょうか。正直に言えば、『Architecture Modernization』の翻訳作業で DDD の概念が頻出し、「分かったつもり」の理解では訳せなくなったからです。初読から 7 年。関数型の視点で DDD を説明する本書を、今度こそ腹落ちさせたかった。読む動機『Domain Modeling Made Functional』は、DDD と関数型プログラミングを組み合わせたアプローチを解説する書籍です。Domain Modeling Made Functional: Tackle Software Complexity with Domain-Driven Design and F# (English Edition)作者:Wlaschin, ScottPragmatic BookshelfAmazon著者の Scott Wlaschin は、F# コミュニティで知られる人物で、「Railway Oriented Programming」などの概念を広めたことでも有名です。著者のサイトでは、本書の内容を補完する講演資料や記事が公開されています。fsharpforfunandprofit.com実は本書を読むのは三度目です。初読は 2019 年頃でした。普通にめちゃくちゃ面白い本だと思いました。ただ、当時の主要言語は Lua、Python、Bash、Go だったため、それでどう活かすかを考えていました。関数型の概念は理解したつもりでしたが、実務にどう活かすかまでは考えが及びませんでした。影響を受けて『すごい Haskell たのしく学ぼう!』（通称、すごい H 本）を読んで、改めてプログラミングが楽しいと思っていたような気がします。実務でもこう考えるべきだ、という意識が変わりました。すごいHaskellたのしく学ぼう！作者:ＭｉｒａｎＬｉｐｏｖａｃａオーム社Amazon二度目は日本語版が出たときです。日本語で読めることで感謝の小躍りをしていました。最高の翻訳だと思います。関数型ドメインモデリング　ドメイン駆動設計とF#でソフトウェアの複雑さに立ち向かおう (アスキードワンゴ)作者:Scott Wlaschin,猪股 健太郎ドワンゴAmazonで、今回、改めて読み直した理由は 3 つあります。1 つ目は、DDD をきちんと学び直す必要があったことです。きっかけは『Architecture Modernization』の翻訳作業でした。レガシーシステムのモダナイゼーションを扱うこの本では、DDD の概念—特に Bounded Context や Strategic Design—が頻繁に登場します。翻訳しながら、自分の DDD 理解が表面的であることに気づきました。アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonエリック・エヴァンスの原典もあらためて読みましたが、オブジェクト指向の文脈で説明される DDD には、どこか違和感がありました。Aggregate の境界、Entity の同一性、Value Object の不変性—これらの概念は、関数型の視点で見ると自然に理解できるのではないか。そう思い、本書を手に取りました。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon2 つ目は、Rust でドメインモデリングをどう実践するか考えていたことです。Rust は関数型言語ではありませんが、代数的データ型やパターンマッチングを持っています。F# で書かれた本書のコードは、Rust に翻訳できるはずです。その翻訳作業を通じて、両言語の違いと共通点を理解したいと思いました。Effective Rust ―Rustコードを改善し、エコシステムを最大限に活用するための35項目作者:David Drysdaleオーム社Amazon3 つ目は、AI エージェント時代における型システムの意味を考えたかったことです。コーディングエージェントが実用レベルに達した 2026 年、「型で不可能を作る」という設計思想の価値が高まっています。AI はドキュメントを読み飛ばすことがあります。しかし、型で定義された制約は無視できません。コンパイルが通らないからです。型は「お願い」ではありません。「壁」です。型システムのしくみ TypeScriptで実装しながら学ぶ型とプログラミング言語作者:遠藤侑介ラムダノートAmazon読む前の状態DDD については、実務で何度か適用した経験があります。Bounded Context の設計、Aggregate の境界決め、Event Storming のファシリテーション。しかし、「なぜそう設計するのか」を言語化できていませんでした。経験則で判断している部分が多かったのです。もしあなたも「DDD は使っているけど、なぜそう設計するのかうまく説明できない」と感じているなら、本書は役に立つかもしれません。関数型プログラミングについては、Haskell を少し触った程度でした。モナドは「文脈を持つ計算」くらいの理解です。Rust の Option と Result は日常的に使っていますが、それが関数型の概念とどうつながるのか、深く考えたことはありませんでした。本書を読んで得た最大の洞察を先に述べておきます。関数型プログラミングの本質は、状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できるということです。私たちはプログラミングを学ぶとき、まず変数への代入を覚えます。x = 1。x = x + 1。状態を変更することがプログラミングの基本だと教わります。しかし冷静に考えると、ビジネスロジックの大半は「入力を受け取り、計算し、出力を返す」で書けます。状態の変更が必要になるのは、データベースに保存するときや外部 API を呼ぶとき—つまりシステムの境界を越えるときだけです。しかし同時に、状態のトランザクション（状態遷移）は現実のビジネスでは避けられません。注文は「未検証」から「検証済み」に変わります。申請は「提出」から「承認」に変わります。この状態遷移をどう表現するか。本書が示す答えは、Transformation-Oriented Programming です。核心は「元のオブジェクトを変更しない」ことです。UnvalidatedOrder を validate で変換して ValidatedOrder を得ます。このとき、元の UnvalidatedOrder には一切触れません。新しい ValidatedOrder を作るだけです。order.validate() ではなく validate(order) -> ValidatedOrder。この発想の転換が、関数型ドメインモデリングの核心です。AI コーディングについては、Claude Code や Cursor を日常的に使っています。便利ですが、生成されるコードの品質にはばらつきがあります。特に、ドメイン固有の制約を理解させるのが難しいです。型定義があると精度が上がるという感覚はありましたが、理論的に説明できませんでした。この感想文のアプローチ本感想文では、2 つの視点を持って読んでいます。言語の視点: F# で書かれた本書のコードを、Rust でどう表現するか。第 2 章で F# と Rust の対応関係を整理し、第 4 章以降は Rust のみで実装を示します。F# にあって Rust にない機能（カリー化、Units of Measure、computation expressions）については、Rust での代替手段を提示しています。時代の視点: 2018 年に書かれた DDD の概念を、2026 年の AI エージェント時代にどう再解釈するか。本書の「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」という原則は、AI が破れない制約を作る技術として読み直せます。型で「不可能」を定義すれば、AI はその不可能を実装できません。この視点で本書を読み解きます。想定読者この感想文は、以下のような読者を想定しています。DDD を実務で使っているが、関数型の視点を取り入れたい人Rust でドメインモデリングを実践したい人AI コーディング時代に、型システムの価値を再確認したい人書籍を読むのにF# の知識は不要です。書籍を読むとそもそも丁寧に教えてくれるの不要なのですが本稿では Rust で提示します。Rust が何も分からない人向けにも、コードが出てくるたびに一通り説明しながら進めます。「型」「関数」「構造体」といった基本的な言葉の意味から丁寧に解説するので、プログラミング経験が浅くても読み進められるはずです。Rust を体系的に学びたい場合は、公式ドキュメントの日本語版も参照してください。doc.rust-jp.rs実践的なコード例で学びたい場合は、Rust by Example も有用です。doc.rust-jp.rsでは、本編に入りましょう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。1. Introducing Domain-Driven Design本章は DDD（Domain-Driven Design、ドメイン駆動設計）の概要を紹介する章です。DDD とは、Eric Evans が 2003 年に提唱したソフトウェア設計手法です。「ビジネスドメインの専門家と開発者が共通の言語でモデルを構築し、そのモデルをコードに直接反映させる」というアプローチで、本章ではコード例は登場せず、DDD の概念に焦点を当てます。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon開発者の仕事はコードを書くことではない第 1 章の冒頭で、著者は「開発者の仕事はコードを書くことだと思うかもしれないが、私は反対だ」と述べています。開発者の仕事は「ソフトウェアを通じて問題を解決すること」であり、コーディングはその一側面に過ぎません。2026 年の今、この主張はさらに重みを増しています。コーディングエージェントが「どう作るか」を担えるようになりました。しかし「何を作るか」を決めるのは、依然として人間の仕事です。共有モデルの重要性本章の核心は「共有モデル」の概念です。ドメインエキスパート、開発チーム、そしてソースコードが同じモデルを共有すべきだという主張です。従来の DDD では、開発者がドメインエキスパートから知識を獲得し、それをコードに翻訳していました。翻訳の過程で歪みが生じるリスクがありました。だからこそ、全員が同じモデルを理解し、同じ言葉で話すことが重要です。Event StormingEvent Storming というワークショップ手法が紹介されています。ドメインエキスパートと開発者が一緒に、ビジネスで起こる「イベント」を付箋に書き出して壁に貼っていきます。「Order form received」「Order placed」「Order shipped」。Event Storming には複数のスコープがあります。本書が扱うのは「プロセスレベル」—特定のワークフローを詳細に分析するものです。「Big Picture」レベルでは、組織全体のドメイン構造を俯瞰します。本章で Ollie が説明したような「顧客は既に商品コードを知っている」「一度に 200〜300 アイテムを注文する」といったドメイン固有の知識は、人間が引き出さなければなりません。ドメインエキスパートは「当たり前」を知っています。その「当たり前」を私たちは知りません。AI エージェント時代においても、この作業は完全には自動化できません。AI はコードベースを読めますが、「なぜそう設計したか」「どんなビジネス制約があるか」は読み取れません。Event Storming で引き出された暗黙知を、CLAUDE.md や設計ドキュメントに言語化する。この作業の価値は、むしろ高まっています。ちなみに2024年発売の『Architecture Modernization』でも同手法が紹介されています。Bounded ContextDDD では「Bounded Context（境界づけられたコンテキスト）」という概念を使って、ドメインを分割します。Bounded Context とは、特定のドメインモデルが適用される明確な境界のことです。同じ「顧客」という言葉でも、販売部門と配送部門では意味が異なることがあります。Bounded Context を分けることで、各コンテキスト内では用語の意味が一貫します。本章の例では、注文処理、配送、請求という 3 つの Bounded Context が登場します。Bounded Context は、コードの境界だけでなく、チームの境界にも影響します。1 つの Context を 1 つのチームが担当するのが理想です。境界が曖昧だと、チーム間の調整コストが増大します。明確に境界が定義された Bounded Context は、変更の影響範囲を限定できます。Ubiquitous LanguageDDD では「Ubiquitous Language（ユビキタス言語）」という概念があります。これは、ドメインエキスパートと開発者がコミュニケーションに使う共通の語彙であり、そのままコード上の命名にも使われます。OrderFactory、OrderManager、OrderHelper といった技術的な命名は、ドメインエキスパートには意味不明だと著者は述べています。一方、PlaceOrder、ValidateOrder、PriceOrder といったドメインに基づく命名なら、誰もがその意図を理解しやすいです。DDDは過剰か本章の内容を踏まえつつ、批判的な視点も必要です。DDD は、複雑なビジネスドメインを扱う場合に有効とされています。しかし、「まず動くものを作り、後からリファクタリングする」というアプローチが、短期間でのリリースには向いている場合もあります。一方で、事前の設計なしに作られたコードは、しばしば一貫性を欠きます。同じ概念に異なる名前を使ったり、似たロジックを複数箇所に重複させたりします。私自身の経験を振り返ると、DDD を「一度きりの設計作業」として捉えていた頃は失敗が多かったです。あるプロジェクトで Event Storming を実施し、5 つの Bounded Context を特定しました。しかし実装を進めると、そのうち 2 つは同じ Context に統合すべきだと気づきました。別の 1 つは 3 つに分割すべきでした。最初の設計の精度は 6 割程度だったのです。この経験から学んだのは、DDD は「段階的に洗練させる」ものだということです。最初から理想的なモデルを目指すのではなく、実装を通じて境界の妥当性を検証し、継続的に見直します。大規模な変革は「ビッグバン」ではなく「段階的な改善」で進める方が成功率が高い。DDD も例外ではありません。2. Understanding the Domain本章では、ドメインエキスパートへのインタビューを通じてドメインを理解するプロセスが解説されます。コード例が本格的に登場する前に、本書が採用する「関数型プログラミング」というアプローチと、その核心について整理しておきます。Patterns, Principles, and Practices of Domain-Driven Design (English Edition)作者:Millett, Scott,Tune, NickWroxAmazonなぜ「関数型」ドメインモデリングなのか本書のタイトルは「Domain Modeling Made Functional」です。DDD と関数型プログラミングを組み合わせています。なぜでしょうか。関数型プログラミングを学んで獲得する概念は、突き詰めると 1 つのことに集約されます。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。これが関数型の核心です。状態は「境界を越えるとき」だけ必要私たちは普段、プログラムを「状態を変更するもの」として捉えがちです。しかし、ビジネスロジックの大半は「入力を受け取り、何かを計算し、出力を返す」という形式で書けます。注文明細と単価から合計金額を計算する → 状態不要住所文字列をパースして構造化データにする → 状態不要商品コードが有効かどうか検証する → 状態不要状態が「必要」になるのは、システムの境界を越えるときだけです。データベースに保存するとき、外部 API を呼び出すとき、ファイルに書き込むとき。この事実に気づくと、設計の発想が変わります。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられない同時に、状態のトランザクションは現実のシステムでは避けられません。注文は「未検証」から「検証済み」に変わります。ビジネスの世界は状態遷移で満ちています。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。// オブジェクト指向的なアプローチ（問題あり）struct Order {    status: OrderStatus,    customer_info: Option<CustomerInfo>,    validated_at: Option<DateTime>,    amount: Option<Decimal>,}impl Order {    fn validate(&mut self) {        self.status = OrderStatus::Validated;        self.validated_at = Some(now());    }}この設計の問題は、状態の「今」しか見えないこと、そして Option フィールドの組み合わせ爆発です。validated_at が Some で amount が None の状態は正しいのでしょうか？整合性を開発者が頭の中で管理し続けなければなりません。Transformation-Oriented Programmingという答え本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。核心は「元のオブジェクトを変更しない」ことです。状態ごとに異なる型を作ります。UnvalidatedOrder は「未検証の注文」を表す型です。ValidatedOrder は「検証済みの注文」を表す型です。これらは別の型であり、別の構造を持ちます。そして、validate 関数は UnvalidatedOrder を受け取り、新しい ValidatedOrder を返します。元の UnvalidatedOrder には触れません。pub struct UnvalidatedOrder {    pub order_id: String,    pub customer_info: String,    pub shipping_address: String,}pub struct ValidatedOrder {    pub order_id: OrderId,    pub customer_info: CustomerInfo,    pub shipping_address: Address,}fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // 元のUnvalidatedOrderは変更されない}重要なのは、元の UnvalidatedOrder は変更されないということです。validate 関数は新しい ValidatedOrder を「作る」だけです。状態を変えるな。新しい値を作れ。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrderこれは「パイプライン」です。データがパイプを流れていきます。各関数は入力を受け取り、出力を返します。それだけです。なぜこのアプローチが強力なのか状態の追跡が不要: 型を見れば分かります。ValidatedOrder を持っているなら、それは「検証済みの注文」です並行処理での競合がない: 元のデータを変更しないから、複数のスレッドが同時に処理しても問題ありませんテストが簡単: 入力を与えて、出力を確認します。モックも不要ですそして何より、ビジネスプロセスが本質的に「入力を受け取り、何かを行い、出力を返す」ものだから相性が良いのです。「見積書」が「発注書」になります。「申請書」が「承認済み申請書」になります。ビジネスの人々は、無意識のうちにこのモデルで考えています。F#という選択とRustでの実践本書の実装言語は F#です。著者が F#を選んだ理由は、「実用的な関数型言語」として設計されており、.NET エコシステムの資産を活用できるからです。本感想文は F#ではなく Rust で実装を示します。私が Rust を選んだ理由は、現在の私にとって主要言語であること、そして所有権システムによる状態遷移の明示化に興味があったからです。Rust は「関数型言語」ではありませんが、関数型の重要な特徴を備えています。代数的データ型: struct と enum で、F#のレコード型と判別共用体を表現できますイミュータビリティ: デフォルトで変数は不変ですパターンマッチング: 網羅的なパターンマッチを強制しますOption/Result: 欠損値とエラーを型で表現しますRust構文の基礎ここで、本感想文で使う Rust の基本を整理しておきます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgまず「型」とは何でしょうか。型とは「値の種類」のことです。数値、文字列、日付、注文情報—これらは全て異なる「種類」の値であり、それぞれに型があります。型があると、「文字列を数値で割る」といった意味のない操作をコンパイラ（プログラムを機械語に変換するソフトウェア）が事前に検出してくれます。struct（構造体）: 複数の値をまとめて 1 つの「もの」として扱う仕組みです。例えば「注文」は「注文 ID」と「顧客情報」と「配送先」を持ちます。これらをまとめて Order という 1 つの型にできます。pub struct Order {    pub id: OrderId,       // フィールド（構成要素）    pub customer_info: String,}pub は「public（公開）」の略で、外部からアクセスできることを意味します。enum（列挙型）:「A か B か C のどれか」を表す型です。例えば注文の状態は「未処理」か「処理済み」か「発送済み」のいずれかです。enum OrderStatus {    Pending,     // 未処理    Validated,   // 検証済み    Shipped,     // 発送済み}関数: 入力を受け取り、何かの処理をして、出力を返すものです。fn で定義します。fn add(a: i32, b: i32) -> i32 {    a + b}i32 は 32 ビット整数という型です。-> i32 は「i32 型の値を返す」という意味です。impl: 型に「できること」（メソッド）を追加します。impl Order {    fn total(&self) -> Money { /* ... */ }}&self は「自分自身を参照する」という意味です。これで order.total() のように呼び出せます。Option<T>:「値があるかもしれないし、ないかもしれない」を表す型です。Some(値) なら値がある、None なら値がありません。Result<T, E>:「成功か失敗か」を表す型です。Ok(値) なら成功、Err(エラー) なら失敗です。doc.rust-lang.org所有権: Rust の最も特徴的な概念です。値は常に 1 つの変数だけが「持っている」のです。関数に渡すと、その値の所有権が移動し、元の変数では使えなくなります。これが「古い状態を誤って使う」ミスを防いでくれます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgF#と Rust で異なる部分—ガベージコレクション vs 所有権、パイプライン演算子、computation expressions—については、該当箇所で必要になったときに具体的に説明します。所有権の概念は、一見すると制約に見えます。しかし、ドメインモデリングにおいては「状態遷移」を明確にする利点があります。fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // UnvalidatedOrderの所有権がこの関数に移動    // 呼び出し元ではUnvalidatedOrderは使えなくなる    // → 検証前の注文を誤って使うことがない    Ok(ValidatedOrder { /* ... */ })}F#では同じ order 変数を後から参照できてしまいますが、Rust では所有権の移動により「古い状態へのアクセス」がコンパイルエラーになります。これは Transformation-Oriented Programming の考え方をさらに強化しています。ドメインエキスパートへのインタビュー第 2 章は、ドメインエキスパート（Ollie）へのインタビューから始まります。インタビューの冒頭で、著者は典型的な e コマースモデルを想定していました。しかし Ollie の回答は違いました。「顧客は既に商品コードを知っている。一度に 200〜300 アイテムを注文することもある」。Widgets 社のドメインは「一般的」ではありません。B2B で、顧客はエキスパートで、商品コードを直接入力します。この固有の要件は、人間がドメインエキスパートから引き出さなければなりません。データベース駆動設計への衝動本章で参考になったのは、「データベース駆動設計と戦う」というセクションです。注文フォームを見ると、多くの開発者はすぐにテーブル設計を始めたくなります。著者はこれを「間違い」と断言しています。DDD では、ドメインが設計を駆動するのであって、データベーススキーマが駆動するのではありません。永続化の無知（Persistence Ignorance）は重要な原則です。まずドメインの概念とワークフローを整理し、永続化は後から考えます。テキストベースのドメイン文書化本章では、ドメインを文書化するためのシンプルな記法が紹介されています。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこの擬似コードは、Rust の構造体定義にほぼそのまま翻訳できます。「AND」は struct のフィールド、「OR」は enum のバリアントになります。ドメインエキスパートと開発者の両方が読める、共通言語として機能します。オーダーのライフサイクルと状態の型本章の後半で、注文には複数のフェーズがあることが明らかになります。UnvalidatedOrder: 届いたばかりの状態ValidatedOrder: 検証済みの状態PricedOrder: 価格が計算された状態data UnvalidatedOrder =    UnvalidatedCustomerInfo    AND UnvalidatedShippingAddress    AND list of UnvalidatedOrderLinedata ValidatedOrder =    ValidatedCustomerInfo    AND ValidatedShippingAddress    AND list of ValidatedOrderLineこの「状態ごとに別の型を定義する」パターンは、Rust では構造体として実装します。状態遷移は関数のシグネチャとして型付けされ、コンパイラが不正な状態遷移を検出してくれます。ワークフローの分解最終的に、注文処理ワークフローは以下のステップに分解されます。substep "ValidateOrder" =    input: UnvalidatedOrder    output: ValidatedOrder OR ValidationError    dependencies: CheckProductCodeExists, CheckAddressExistssubstep "PriceOrder" =    input: ValidatedOrder    output: PricedOrder    dependencies: GetProductPriceワークフローを小さなステップに分解することで、各ステップが独立してテスト可能になります。入力・出力・依存関係が明確に定義されていれば、実装も容易になります。3. A Functional Architecture本章は、関数型アーキテクチャの原則を解説します。Bounded Context、イベント駆動通信、Onion Architecture。これらの概念は言語に依存しません。アーキテクチャを考えるタイミング第 3 章の冒頭で、著者は矛盾したことを述べています。「この段階でアーキテクチャについて考えすぎるべきではない。まだシステムを理解していないからだ」。しかし同時に「大まかな実装方針を持っておくのは良いことだ」とも言います。著者の「walking skeleton（動く骨格）」というアプローチは有効です。まず最小限の構造を設計し、その骨格に沿ってコードを書いていきます。Bounded Contextと自律性Bounded Context をソフトウェアコンポーネントとしてどう実装するか。モノリス内のモジュール、独立したアセンブリ、マイクロサービス。いくつかの選択肢があります。著者は「最初はモノリスとして構築し、スケールや独立デプロイが求められる段階で分離する」ことを勧めています。マイクロサービスを夢見て最初から分割し、サービス間通信の地獄に落ちた経験がある人には、身に染みる助言でしょう。私もその一人です。最初から理想的なマイクロサービスを目指すと、サービス間の境界を間違えたときの修正コストが膨大になります。まずモノリス内でモジュールを分離し、境界が安定してからサービスに切り出す。これを最初から知っていれば、いくつかの深夜対応は避けられたかもしれません。マイクロサービスアーキテクチャ 第2版作者:Sam Newmanオーム社AmazonイベントによるContext間通信Bounded Context 間の通信は、イベントを介して行われます。Place-Order ワークフローが OrderPlaced イベントを発行し、Shipping コンテキストがそれを受け取って ShipOrder コマンドを生成します。この非同期・疎結合のパターンは、変更の影響範囲を限定できます。各 Context が独立したイベントの発行者・購読者として定義されていれば、一方の変更が他方に波及しにくくなります。DTOと信頼境界本章で重要な概念が登場します。Domain Object と Data Transfer Object (DTO) の区別です。Domain Object は、Bounded Context 内部でのみ使用されます。DTO は、Context 間の通信やシリアライズのために設計されます。同じ「Order」でも、内部で使う Order と、外部に公開する OrderDTO は別物です。さらに、Bounded Context の境界は「信頼境界」として機能します。外部からのデータは信頼できません。内部に入る前にバリデーションが必要です。Context間の契約関係Context 間の契約関係について 3 つのパターンが紹介されます。Shared Kernel: 両チームが共同で契約を所有Customer/Supplier: 下流の Context が契約を定義Conformist: 上流の Context の契約に従うこれらの関係は、技術的な問題であると同時に組織的な問題でもあります。Onion Architecture と I/O の分離本章の後半では、コードの構造について議論されます。Onion Architecture では、ドメインが中心にあり、I/O は外周に配置されます。依存関係は常に内側に向かいます。純粋なコアを、不純な殻で包みます。「I/Oはワークフローの端でのみ行う。ワークフロー内部は純粋な関数で構成する」この原則は、第 2 章で述べた「状態は例外的」という考え方と直結します。ワークフロー内部は「入力を受け取り、何かを行い、出力を返す」純粋な関数だけで構成されます。データベースアクセスやファイル I/O は、ワークフローの開始時か終了時にのみ行います。この構造により、ドメインロジックはテスト容易で予測可能になります。少なくとも、理論上は。4. Understanding Types本章から、コード例が本格的に登場します。第 2 章で整理した F#と Rust の対応関係に基づき、以降は Rust のみで実装を示します。型とは「可能な値の集合」である著者の「型」の定義はシンプルです。「関数の入力や出力として使える値の集合に付けた名前」。i16 は-32768 から+32767 までの数値の集合、String は全ての文字列の集合です。この定義を読んで、自分がいかに型を「コンパイラを満足させるためのもの」として捉えていたか気づかされました。型は思考のツールです。ANDとORによる型の合成—代数的データ型本章の核心は、型の合成方法です。著者は 2 つの方法を示します。これらは「代数的データ型（Algebraic Data Types）」と呼ばれ、関数型プログラミングの基礎概念です。F#では「レコード型」と「判別共用体」、Rust では struct と enum で表現できます。AND型（struct / 積型）: 複数の値を組み合わせます。struct FruitSalad {    apple: AppleVariety,    banana: BananaVariety,    cherries: CherryVariety,}FruitSalad を作るには、apple と banana と cherries の全てが必要です。OR型（enum / 和型）: 複数の選択肢から 1 つを選びます。enum FruitSnack {    Apple(AppleVariety),    Banana(BananaVariety),    Cherries(CherryVariety),}FruitSnack は、Apple か Banana か Cherries のいずれか 1 つです。たった 2 つの概念で複雑なドメインを表現できます。AND と OR という論理演算で型を組み立てます。Simple Types—newtype patternの威力本章で一番「これが使える」と思ったのは、Simple Types の話です。プリミティブ型をそのまま使うのは危険です。CustomerId も OrderId も i32 だとしたら、間違って OrderId を CustomerId として渡してもコンパイルが通ってしまいます。Rust では、newtype patternでこの問題を解決します。newtype pattern とは、既存の型を新しい型でラップすることで、型レベルで区別をつけるイディオムです。F#では「単一ケース判別共用体」、Rust では「タプル構造体」で表現します。zenn.dev#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct CustomerId(i32);#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct OrderId(i32);#[derive(...)] について説明します。これは Rust の「属性マクロ」で、型に機能を自動で追加する仕組みです。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org「トレイト」とは、型が持つべき「能力」や「振る舞い」の定義です。例えば「比較できる」「コピーできる」「文字列として表示できる」といった能力がトレイトとして定義されています。#[derive(Debug, Clone)] と書くと、その型に Debug と Clone という能力が自動的に追加されます。手で書くと何十行にもなるコードを、一行で済ませられます。よく使うトレイトを整理しておきます。 トレイト  意味  使いどころ  Debug  中身を表示できる  println!("{:?}", x) でデバッグ出力  Clone  複製を作れる  .clone() で明示的にコピー  Copy  自動で複製される  代入や関数呼び出しで自動コピー（小さな値向け）  PartialEq  比較できる  == で等しいか判定  Eq  反射律を満たす比較  HashMapのキーに使うとき必要  Hash  ハッシュ値を計算できる  HashMapのキーに使うとき必要 内部的には同じ i32 ですが、型システム上は別の型です。CustomerId を期待する関数に OrderId を渡すとコンパイルエラーになります。ここで重要なのは、Clippy のような静的解析ツールでもこの種のバグは検出できないということです。Clippy は Rust の公式リンター（コード品質チェックツール）で、700 以上の lint ルールを持ちます。cargo clippy コマンドで実行でき、コードの問題点を警告してくれます。rust-lang.github.ioしかし、Clippy にも限界があります。// Clippyでは検出できないfn process(customer_id: i32, order_id: i32) { /* ... */ }process(order_id, customer_id); // バグ！でもコンパイルは通る// 型で防ぐfn process(customer_id: CustomerId, order_id: OrderId) { /* ... */ }// process(order_id, customer_id); // コンパイルエラー！Clippy は構文的な問題—if x { "a" } else { "a" } のような両方のブランチが同じ処理、u32 >= 0 のような常に true になる比較—は検出できます。しかし、「この i32 は顧客 ID を表し、あの i32 は注文 ID を表す」というドメインの知識は持っていません。newtype pattern は、Clippy が検出できないバグを型システムで防ぎます。AI コーディングエージェントも同様です。「customer_id と order_id を間違えないように」という指示は、自然言語では曖昧です。しかし CustomerId と OrderId という別の型が定義されていれば、AI が生成したコードでも型の取り違えはコンパイル時に検出されます。Option型とResult型F#と Rust は、欠損値を Option で、エラーを Result で表現します。これらは関数型プログラミングにおける標準的なエラーハンドリング手法で、null や例外を使わずに「値がないかもしれない」「失敗するかもしれない」ことを型で表現します。struct PersonalName {    first_name: String,    middle_initial: Option<String>,  // 省略可能    last_name: String,}Option<T> は Some(T) か None のいずれかです。null を使わずに「値がないかもしれない」ことを型で表現します。エラーハンドリングには Result<T, E> を使います。? 演算子で、エラー時に早期リターンできます。型によるドメイン表現本章で紹介されている支払い方法のモデリング例は、型がドキュメントとして機能することを示しています。enum PaymentMethod {    Cash,    Check(CheckNumber),    Card(CreditCardInfo),}struct Payment {    amount: PaymentAmount,    currency: Currency,    method: PaymentMethod,}約 25 行で、支払いドメインの構造が明確に表現されています。このコードは、ドメインエキスパートにも読めます。型システムは思考のツールである本章を通じて感じたのは、型システムは「コンパイラのため」ではなく「思考のため」にあるということです。「動的型付け言語でも同じことができるのでは？」という疑問があるかもしれません。確かに、Python や Ruby でもドメインモデリングはできます。しかし、型がないと「どんな値が入りうるか」を頭の中で追跡し続けなければなりません。静的型付けでは、その追跡をコンパイラに委ねられます。AND 型と OR 型という単純な組み合わせで、複雑なドメインを表現できます。型定義という明確な仕様があれば、実装時の迷いが減ります。型システムは、ドメインの構造を可視化するツールです。5. Domain Modeling with Types本章では、前章で学んだ型システムの概念を使って、実際にドメインモデルを構築します。コードがドキュメントになる第 5 章の冒頭で、著者は挑戦的な問いを投げかけます。「ソースコードを直接ドキュメントとして使い、UML 図のような別の成果物を不要にできるか？」正直、最初は懐疑的でした。しかし本章を読み進めるうちに、著者の意図が分かってきました。擬似コードからRustへ第 2 章で作成した擬似コードを、Rust の型に変換します。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこれが Rust では以下のようになります。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,    pub shipping_address: ShippingAddress,    pub billing_address: BillingAddress,    pub order_lines: Vec<OrderLine>,    pub amount_to_bill: BillingAmount,}ほぼ一対一の変換です。擬似コードと Rust コードを並べて見ると、ドメインの構造がそのまま型に反映されていることが分かります。Value ObjectとEntityDDD では、オブジェクトを「Value Object」と「Entity」に分類します。Value Object: 同じ値を持てば同一とみなします。#[derive(Debug, Clone, PartialEq, Eq)]pub struct PersonalName {    pub first_name: String,    pub middle_initial: Option<String>,    pub last_name: String,}Entity: 固有の ID を持ち、内容が変わっても同一性を保ちます。impl PartialEq for Contact {    fn eq(&self, other: &Self) -> bool {        self.contact_id == other.contact_id  // IDのみで比較    }}Aggregate—一貫性の境界本章で最も重要な概念は「Aggregate」です。Order と OrderLine の関係を考えます。OrderLine の価格を変更したとき、Order の合計金額も更新します。両者は常に一貫した状態を保ちます。DDD では、こうした関連オブジェクトの集合を「Aggregate」と呼び、最上位のオブジェクトを「Aggregate Root」と呼びます。immutable なパターンでは、OrderLine を変更するには Order 全体を作り直します。これは一見非効率に見えますが、一貫性を強制する効果があります。OrderLine だけを変更して、Order の合計金額を更新し忘れる、というバグが起こりにくくなります。Aggregate参照—IDのみを保持するOrder に Customer 情報を含める場合、Customer オブジェクト全体ではなく、CustomerId だけを保持すべきです。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,  // Customer全体ではなく、IDのみ    pub order_lines: Vec<OrderLine>,}この設計は、immutability と相性が良いです。Customer の電話番号が変わっても、Order を更新する必要がありません。型でドメインを表現する—最終形本章の最後に、完全なドメインモデルの例が示されます。#[derive(Debug, Clone, PartialEq, Eq)]pub enum ProductCode {    Widget(WidgetCode),    Gizmo(GizmoCode),}#[derive(Debug, Clone, Copy, PartialEq)]pub enum OrderQuantity {    Unit(UnitQuantity),    Kilos(KilogramQuantity),}このコードは、第 2 章の擬似コードとほぼ同じ構造を持ちます。struct、enum、match の意味さえ分かれば読めます。match については Rust 公式ドキュメントを参照してください。doc.rust-lang.orgString は沈黙します。EmailAddress は語ります。著者の主張—「型でドメインを表現すれば、コードがドキュメントになる」—は正しいと思います。6. Integrity and Consistency in the Domain本章は、ドメイン内のデータが常に「信頼できる状態」であることを保証する方法を解説します。Smart Constructor—制約を強制する本章で最も実用的だったのは、Smart Constructor（スマートコンストラクタ）のパターンです。Smart Constructor とは、値の生成時にバリデーションを行い、不正な値の生成を防ぐコンストラクタのことです。通常のコンストラクタと異なり、Result を返して生成の失敗を表現できます。例えば、UnitQuantity は 1 から 1000 の間の値でなければなりません。この制約をコメントで書くだけでは不十分です。#[derive(Debug, Clone, Copy, PartialEq, Eq)]pub struct UnitQuantity(i32);impl UnitQuantity {    pub fn new(value: i32) -> Result<Self, String> {        if value < 1 {            Err("UnitQuantity must be at least 1".to_string())        } else if value > 1000 {            Err("UnitQuantity must be at most 1000".to_string())        } else {            Ok(UnitQuantity(value))        }    }    pub fn value(&self) -> i32 {        self.0    }}フィールドを pub にしなければ、外部から直接 UnitQuantity(500) と書けません。必ず UnitQuantity::new(500) を経由します。NonEmptyList—空のリストを許さない「注文には少なくとも 1 つの注文行がなければならない」という要件を、型で強制できるでしょうか。#[derive(Debug, Clone, PartialEq)]pub struct NonEmptyList<T> {    pub first: T,    pub rest: Vec<T>,}from_vec は Option を返します。空のベクターからは NonEmptyList を作れません。この「作れない」という事実が型で表現されています。Make Illegal States Unrepresentable本章で最も重要な原則は「不正な状態を表現不可能にする」です。メールアドレスの例が分かりやすいです。「検証済み」と「未検証」のメールアドレスがあるとき、フラグで区別する設計は危険です。詳しくは Rust 公式ドキュメントの enum 解説を参照してください。doc.rust-lang.org// 良い例：別の型として定義pub struct VerifiedEmailAddress(String);pub enum CustomerEmail {    Unverified(EmailAddress),    Verified(VerifiedEmailAddress),}VerifiedEmailAddress のコンストラクタを private にして、検証サービスからしか作れないようにします。これで、検証を経ずに Verified 状態を作ることが物理的に不可能になります。fn send_password_reset(email: VerifiedEmailAddress) -> Result<(), SendError> {    // この関数にEmailAddressを渡すとコンパイルエラー}連絡先情報の例—OR型の活用「顧客にはメールアドレスか住所のどちらか、または両方が必要」という要件を型で表現します。pub enum ContactInfo {    EmailOnly(EmailContactInfo),    AddressOnly(PostalContactInfo),    EmailAndAddress(BothContactMethods),}3 つのケースしかありません。「メールも住所もない」という状態は表現できません。「不可能を作る」という設計思想本章の核心は「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」です。この原則を言い換えれば、型で「不可能」を作るということになります。この原則を読んだとき、過去に遭遇したバグが走馬灯のように思い出されました。is_active = true なのに deleted_at が設定されている。status = "paid" なのに payment_id が null。フラグと Option の組み合わせ爆発で、「あり得ない」状態が本番データベースに存在していた。深夜に呼び出されて、整合性を手作業で修正した夜のことを、今でも覚えています。あのバグは、型で防げたのです。似たような経験をしたことがある人は、少なくないのではないでしょうか。NonEmptyList を使えば、空の注文は作れないVerifiedEmailAddress を使えば、未検証メールへのパスワードリセットは書けないSmart Constructor を使えば、範囲外の値は存在できない「できない」「書けない」「存在できない」—これらは制限ではなく、設計上の保証です。バリデーションは「お願い」。型は「物理法則」。Clippy のような静的解析ツールでも、ドメインロジックの問題は検出できません。例えば、is_priced: bool と amount: Option<f64> を持つ構造体を考えます。is_priced = true なのに amount = None という矛盾した状態は、Clippy には「正しい Rust コード」に見えます。ビジネスルールを知らないからです。しかし、PricedOrder { amount: Money } と UnpricedOrder を別の型として定義すれば、この矛盾は表現できなくなります。Clippy が検出できない問題を、型システムが防ぎます。AI エージェント時代において、この「不可能を作る」設計思想の価値は高まっています。AI は自然言語のドキュメントを読み飛ばすことがあります。しかし、型で「不可能」が定義されていれば、AI はその制約を破るコードを物理的に書けません。7. Modeling Workflows as Pipelines本章は、ワークフローをパイプラインとしてモデリングする方法を解説します。ビジネスプロセスを「変換の連鎖」として捉えるアプローチです。ワークフローはパイプラインである本章の冒頭で、著者は注文処理ワークフローを次のように要約しています。workflow "Place Order" =    input: UnvalidatedOrder    output: OrderPlaced AND BillableOrderPlaced AND OrderAcknowledgmentSent    // step 1: ValidateOrder    // step 2: PriceOrder    // step 3: AcknowledgeOrder    // step 4: create and return events各ステップは「入力を受け取り、変換し、出力を返す」関数です。これらを連結するとパイプラインになります。第 2 章で述べた「状態は例外的、ほとんどの処理は状態なしで書ける」という原則が、ここで具現化されます。ワークフロー全体を見ると「状態遷移」に見えますが、各ステップを見ると「入力を受け取り、何かを行い、出力を返す」純粋な関数でしかありません。状態マシンとしてのOrderOrder を単一の型として設計すると、フラグだらけになります。// 悪い設計struct Order {    order_id: OrderId,    is_validated: bool,    is_priced: bool,    amount_to_bill: Option<Decimal>,  // pricedの時だけ存在}本書のアプローチは、各状態を別の型として定義することです。pub struct UnvalidatedOrder { /* ... */ }pub struct ValidatedOrder { /* ... */ }pub struct PricedOrder {    // ...    pub amount_to_bill: BillingAmount,  // この状態でのみ存在}PricedOrder には amount_to_bill があります。ValidatedOrder にはありません。フラグは不要で、「どの型か」が状態を表します。AI にコードを書かせるとき、この設計は強力なガードレールになります。「検証をスキップして価格計算に進んでください」と指示しても、price() 関数が ValidatedOrder を要求する以上、AI は UnvalidatedOrder を渡すコードを書けません。型が不正な状態遷移を物理的に阻止します。依存性を型で表現する各ステップの依存性を型シグネチャで表現します。type CheckProductCodeExists = fn(&ProductCode) -> bool;type CheckAddressExists = fn(&UnvalidatedAddress) -> Result<CheckedAddress, AddressValidationError>;type ValidateOrder = fn(    CheckProductCodeExists,     // 依存性1    CheckAddressExists,         // 依存性2    UnvalidatedOrder,           // 入力) -> Result<ValidatedOrder, ValidationError>;依存性が関数の引数として明示されます。インターフェース全体ではなく、必要な関数だけを渡します。最小限の依存性です。エフェクトの文書化関数の「エフェクト（効果）」を型で文書化します。Result: エラーを返す可能性があるAsync: 非同期 I/O を行うOption: 値が存在しない可能性があるasync fn check_address_exists(    address: &UnvalidatedAddress,) -> Result<CheckedAddress, AddressValidationError> {    // 外部サービスへのHTTPリクエスト}関数シグネチャを見れば、「この関数は非同期で、エラーを返す可能性がある」と分かります。Transformation-Oriented Programmingの実践具体的な例で考えます。オブジェクト指向的に書くと、こうなります。impl Order {    fn validate(&mut self, checker: &ProductChecker) -> Result<(), ValidationError> {        // 状態を変更        self.is_validated = true;        self.validated_at = Some(now());        Ok(())    }}関数型で書き直すと、こうなります。fn validate(    order: UnvalidatedOrder,    checker: &ProductChecker,) -> Result<ValidatedOrder, ValidationError> {    // 新しい値を作る（元のorderは変更しない）    Ok(ValidatedOrder {        order_id: OrderId::new(order.order_id)?,        customer_info: validate_customer(order.customer_info)?,        lines: validated_lines,    })}違いは何でしょうか。コンパイル時チェック: price() に UnvalidatedOrder を渡すとコンパイルエラー状態の整合性が型で保証: ValidatedOrder には is_validated フラグがそもそも存在しないテストが独立: validate() と price() を別々にテストできるこの単純さが強力なのだUnvalidatedOrder を ValidatedOrder に変換します。ValidatedOrder を PricedOrder に変換します。元のオブジェクトは触りません。新しいオブジェクトを作ります。それだけです。状態の変更を追跡する必要がない（変更しないから）並行処理でも競合しない（元のデータを変更しないから）テストが簡単（入力と出力を比較するだけ）デバッグが楽（各ステップの入出力をログに残せば、全経路が追える）関数型プログラミングの入門書を読むと、モナドだの圏論だの、難しい概念が出てきます。しかし、実務で最も重要なのは、本書が示すTransformation-Oriented Programmingです。核心は 3 つです。状態を型で表現する（UnvalidatedOrder と ValidatedOrder は別の型）状態遷移を関数で表現する（validate(order) -> ValidatedOrder）元のオブジェクトを変更しない（新しい値を作るだけ）変えるな。作れ。この章を読み終えたとき、「これなら実務で使える」と確信しました。モナドや圏論を理解する必要はありません。「状態ごとに型を分ける」「元のオブジェクトを変更しない」。この 2 つだけで、設計の質は劇的に変わります。難しい理論ではなく、明日から使える実践知。本書の価値はここにあります。8. Understanding Functions本章は、関数型プログラミングの基礎を解説します。実装に入る前の準備として、関数の扱い方を整理しています。関数型プログラミングとは著者の定義はシンプルです。「関数型プログラミングとは、関数が本当に重要なものとしてプログラミングすること」。オブジェクト指向では、オブジェクトがあらゆる場所で使われます。関数型では、関数があらゆる場所で使われます。依存性を注入するときは関数を渡します。コードを再利用するときは関数を合成します。関数は「モノ」である関数型プログラミングの核心は、関数が第一級の値であることです。変数に代入できます。リストに入れられます。引数として渡せます。戻り値として返せます。let add1 = |x: i32| x + 1;let square = |x: i32| x * x;let functions: Vec<fn(i32) -> i32> = vec![add1, square];for f in &functions {    println!("{}", f(5));}関数をリストに入れて、ループで回しています。高階関数関数を引数に取る関数、または関数を返す関数を「高階関数」と呼びます。fn eval_with_5_then_add_2<F>(f: F) -> i32where    F: Fn(i32) -> i32,{    f(5) + 2}Rust では、関数を受け取る引数の型を Fn、FnMut、FnOnce トレイトで指定します。F#ではこの区別はありません。クロージャとはクロージャは「名前のない関数」です。通常の関数は fn name(...) と名前をつけて定義しますが、クロージャは |引数| 式 という形式でその場で作れます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orglet add1 = |x| x + 1;        // 引数xを受け取り、x + 1を返すlet result = add1(5);        // 6クロージャの特徴は、周囲の変数を「捕まえる」（キャプチャする）ことができる点です。let multiplier = 3;let multiply = |x| x * multiplier;  // multiplierを捕まえているlet result = multiply(5);           // 15Fnトレイトの使い分け関数を引数として受け取るとき、Rust では 3 種類のトレイトを使い分けます。これは「捕まえた変数をどう扱うか」で決まります。 トレイト  捕まえ方  呼び出し回数  Fn  読み取るだけ  何度でも  FnMut  変更する  何度でも  FnOnce  消費する（使い切る）  一度だけ 最初は気にしすぎなくてよいです。コンパイラがエラーで教えてくれます。カリー化と部分適用F#では、すべての関数が自動的に「カリー化」されます。Rust にはカリー化が組み込まれていません。同じことを実現するには、明示的にクロージャを返します。fn adder_generator(number_to_add: i32) -> impl Fn(i32) -> i32 {    move |x| number_to_add + x}let add5 = adder_generator(5);let result = add5(3);  // 8部分適用は、依存性注入に活用できます。let validate = |order| validate_order(    check_product_code_exists,    check_address_exists,    order,);let result = validate(unvalidated_order);Total Functions（全域関数）数学の関数は、すべての入力に対して出力が定義されます。12 を引数で割る関数を考えます。n = 0 のとき、何を返すべきでしょうか。解決策は 2 つあります。入力を制限するか、出力を拡張するかです。// 入力を制限fn twelve_divided_by(n: NonZeroI32) -> i32 {    12 / n.0}// 出力を拡張fn twelve_divided_by(n: i32) -> Option<i32> {    if n == 0 { None } else { Some(12 / n) }}どちらの場合も、型シグネチャが正直になります。型シグネチャは嘘をつきません。コメントは嘘をつきます。AI にコードを書かせるとき、この「正直な型シグネチャ」は重要です。AI は型シグネチャを見て、関数の契約を理解します。Option<i32> を返す関数なら、AI は None のケースを考慮したコードを生成します。しかし「0 を渡したら None を返します」というコメントは、読み飛ばされる可能性があります。関数合成F#にはパイプライン演算子 |> があります。Rust にはありません。代わりにメソッドチェーンや、関数を直接呼び出します。let result: Vec<_> = (1..10)    .map(|x| x + 1)    .map(|x| x * x)    .collect();イテレータのアダプタは、パイプラインに近い書き方ができます。Rustで関数型プログラミングを実践するために本章を読んで、F#と Rust の違いを改めて認識しました。カリー化: F#は自動、Rust は手動パイプライン演算子: F#にはある、Rust にはないクロージャの所有権: F#は考慮不要、Rust は move や Fn/FnMut/FnOnce を意識これらの違いはありますが、関数型プログラミングの本質—関数を組み合わせてシステムを構築する—は Rust でも実践できます(が本当に最適か？という問いは投げないでくれ…本稿のアプローチと違いすぎる)。9. Implementation: Composing a Pipeline本章から、いよいよ実装に入ります。これまで型で設計してきたワークフローを、実際のコードに落とし込みます。パイプラインの理想形著者が示す理想のコードは驚くほどシンプルです。let placeOrder unvalidatedOrder =    unvalidatedOrder    |> validateOrder    |> priceOrder    |> acknowledgeOrder    |> createEvents4 行で注文処理全体が表現されています。これが関数型アプローチの目指す姿です。しかし現実には、関数の出力と次の関数の入力が一致しません。依存性をどこかで解決しなければなりません。本章はその「ギャップ」を埋める方法を解説します。型シグネチャによる実装のガイド本章で印象的だったのは、「型シグネチャを先に定義し、それに従って実装する」というアプローチです。type ValidateOrder = fn(    check_product_code: fn(&ProductCode) -> bool,    check_address: fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder;型シグネチャが「契約」として機能します。引数の型、戻り値の型が全て決まっているので、実装者は「この契約を満たすコードを書く」だけでよいです。依存性注入の関数型アプローチオブジェクト指向では、インターフェースを定義し、コンストラクタで注入します。関数型では、依存性を関数の引数として渡します。DI コンテナ？関数を渡せ。fn validate_order(    check_product_code: impl Fn(&ProductCode) -> bool,    check_address: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder {    // check_product_code を使う}インターフェース全体ではなく、必要な関数だけを渡します。ワークフロー全体の組み立て各ステップを組み立ててワークフロー全体を作ります。pub fn place_order(    // 依存性    check_product_code_exists: impl Fn(&ProductCode) -> bool,    check_address_exists: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    get_product_price: impl Fn(&ProductCode) -> Price,    // 入力    unvalidated_order: UnvalidatedOrder,) -> Vec<PlaceOrderEvent> {    let validated = validate_order(        &check_product_code_exists,        &check_address_exists,        unvalidated_order,    );    let priced = price_order(&get_product_price, validated);    let acknowledgment = acknowledge_order(&priced);    create_events(&priced, acknowledgment)}F#のパイプライン演算子 |> がないので、変数に束縛しながら連鎖させます。本章では Result を使わず簡略化しており、次章で Result を導入します。10. Implementation: Working with Errors本章は、エラーハンドリングの関数型アプローチを解説します。「Railway Oriented Programming」と呼ばれるパターンを学びます。エラーの三分類著者はエラーを 3 つに分類します。Domain Errors: ビジネスプロセスの一部として予期されるエラー。商品コードが無効、注文が請求で拒否される、など。Panics: システムを未知の状態にするエラー。メモリ不足、ゼロ除算など。Infrastructure Errors: ネットワークタイムアウト、認証失敗など。この分類は実務でも有用です。「このエラーはドメインエキスパートに相談すべきか」という問いに答えられます。ドメインエラーを型で表現する#[derive(Debug, Clone)]pub enum PlaceOrderError {    ValidationError(String),    ProductOutOfStock(ProductCode),    RemoteServiceError(RemoteServiceError),}エラーを enum で定義することで、どんなエラーが起こりうるか、型定義を見れば分かります。Railway Oriented Programming著者が提唱する解決策が「Railway Oriented Programming（鉄道指向プログラミング）」です。著者自身による詳細な解説は以下を参照してください。fsharpforfunandprofit.comResult を返す関数は「分岐するレール」として可視化できます。成功すれば上のレールに、失敗すれば下のレールに進みます。一度失敗パスに入ると、残りのステップはバイパスされます。      [validateOrder] → [priceOrder] → [acknowledgeOrder] → 成功           ↓               ↓               ↓      ─────────────────────────────────────────────────────→ 失敗Rustでの実装Rust では ? 演算子が同じことをより簡潔に表現します。fn validate_order(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    let order_id = OrderId::create(&order.order_id)?;    let customer_info = to_customer_info(&order.customer_info)?;    let shipping_address = check_address(&order.shipping_address)?;    Ok(ValidatedOrder { order_id, customer_info, shipping_address, ... })}? 演算子は、Ok ならアンラップし、Err なら早期リターンします。?演算子の仕組み? は「失敗したら即座に関数から抜ける」という処理を一文字で書ける記号です。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgResult は「成功（Ok）か失敗（Err）か」を表す型でした。? をつけると、成功なら中身を取り出し、失敗ならその場で関数を終了して呼び出し元にエラーを返します。// ?を使った書き方let order_id = OrderId::create(&order.order_id)?;// これは以下と同じ意味let order_id = match OrderId::create(&order.order_id) {    Ok(v) => v,              // 成功したら中身を取り出す    Err(e) => return Err(e), // 失敗したら即座にエラーを返す};? を使うには、関数の戻り値が Result である必要があります。エラー型の変換複数のステップを連結するとき、エラー型を統一します。#[derive(Debug)]pub enum PlaceOrderError {    Validation(ValidationError),    Pricing(PricingError),}impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}Fromトレイトによる型変換From は「ある型から別の型への変換方法」を定義するトレイトです。例えば「ValidationError を PlaceOrderError に変換する方法」を定義しておくと、? 演算子が自動的にエラー型を変換してくれます。// 「ValidationErrorからPlaceOrderErrorへの変換方法」を定義impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}これを定義しておくと、validate_order が ValidationError を返しても、? が自動的に PlaceOrderError に変換してくれます。異なるエラー型を返す関数を連結できるようになります。11. Serialization本章は、ドメインオブジェクトを JSON や XML などの形式に変換する方法を解説します。Bounded Context の境界を越えるとき、内部のドメイン型をそのまま使うことはできません。また、AI時代にはこのようなことも想定されます。zenn.dev永続化とシリアライゼーションの区別著者は 2 つの概念を区別します。Persistence（永続化）: プロセスの終了後も状態が残ること。Serialization（シリアライゼーション）: ドメイン固有の表現を、永続化可能な表現に変換すること。本章はシリアライゼーションに焦点を当て、次章で永続化を扱います。DTOによる変換—ドメインの境界防御ドメイン型は複雑です。ネストした型、制約付きの型、選択肢を持つ型。これらを直接シリアライズするのは難しいです。解決策は、Data Transfer Object（DTO）を中間層として使うことです。なぜDTOを使うのか？ドメイン型は制約を持つ: String50 は 50 文字以下という制約があります。JSON の "name" フィールドは任意の長さです。直接マッピングできません。内部実装の変更から外部を守る: ドメイン型のフィールド名を変えても、DTO が同じなら外部 API は影響を受けません。検証の境界を明確にする: 外部からの入力は「信頼できない」です。DTO からドメイン型への変換時に検証することで、ドメイン内は常に「信頼できる」状態を保ちます。DTO は「ドメインの境界防御」として機能します。Domain型 → DTO → JSON（シリアライズ）JSON → DTO → Domain型（デシリアライズ）Rust では、ドメイン型と DTO 型を別々に定義します。ドメイン型は制約を持ち、DTO はプリミティブ型のみを使います。/// 制約付きの文字列型（50文字以下）#[derive(Debug, Clone, PartialEq, Eq)]pub struct String50(String);impl String50 {    pub fn create(s: &str) -> Result<Self, ValidationError> {        if s.is_empty() {            Err(ValidationError::Empty("String50 cannot be empty".into()))        } else if s.len() > 50 {            Err(ValidationError::TooLong("String50 must be 50 chars or less".into()))        } else {            Ok(String50(s.to_string()))        }    }    pub fn value(&self) -> &str {        &self.0    }}/// ドメイン型（制約付き）#[derive(Debug, Clone, PartialEq, Eq)]pub struct Person {    pub first_name: String50,    pub last_name: String50,}/// DTO（シリアライズ用・プリミティブのみ）#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]pub struct PersonDto {    pub first_name: String,    pub last_name: String,}変換には From と TryFrom を使います。ドメイン型から DTO への変換は常に成功しますが、DTO からドメイン型への変換は失敗しうります。/// ドメイン型 → DTO（常に成功）impl From<&Person> for PersonDto {    fn from(person: &Person) -> Self {        PersonDto {            first_name: person.first_name.value().to_string(),            last_name: person.last_name.value().to_string(),        }    }}/// DTO → ドメイン型（失敗する可能性あり）impl TryFrom<PersonDto> for Person {    type Error = ValidationError;    fn try_from(dto: PersonDto) -> Result<Self, Self::Error> {        let first_name = String50::create(&dto.first_name)?;        let last_name = String50::create(&dto.last_name)?;        Ok(Person { first_name, last_name })    }}TryFrom を使うことで、変換が失敗する可能性を型で表現しています。これは「Parse, don't validate」の実践です。入力を単に「正しいかどうか」検証するのではなく、より型安全な形式に変換（パース）することで、型レベルで正しさを保証します。DTOは契約である著者が強調するのは、DTO は「Bounded Context 間の契約」だということです。他の Context が発行したイベントを受信するとき、そのフォーマットに依存します。フォーマットを変更すると、依存する Context に影響が及びます。だから、シリアライズのフォーマットは慎重に設計すべきです。serde の #[derive(Serialize)] を安易に使うと、内部実装の変更が契約の破壊につながります。選択肢型（enum）のシリアライズOR 型（enum）のシリアライズは注意が必要です。JSON には enum の概念がありません。Rust では serde の属性でタグ付け方式を指定します。serde については公式ドキュメントを参照してください。serde.rs/// 支払い方法のDTO - タグ付きenum#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]#[serde(tag = "type")]pub enum PaymentMethodDto {    Cash,    Check { check_number: String },    Card { card_number: String, expiry: String },}#[serde(tag = "type")] を指定すると、以下のような JSON が生成されます。{"type":"Cash"}{"type":"Check","check_number":"12345"}{"type":"Card","card_number":"4111...","expiry":"12/25"}タグ付きの方が明示的で、新しいケースを追加しやすいです。ラウンドトリップの検証シリアライズとデシリアライズは対になります。ラウンドトリップ（往復）テストで、データが失われないことを確認します。pub fn serialize_order(order: &Order) -> Result<String, serde_json::Error> {    let dto = OrderDto::from(order);    serde_json::to_string_pretty(&dto)}pub fn deserialize_order(json: &str) -> Result<Order, String> {    let dto: OrderDto = serde_json::from_str(json).map_err(|e| e.to_string())?;    Order::try_from(dto).map_err(|e| format!("{:?}", e))}本章と中心テーマのつながりDTO パターンは、本書のTransformation-Oriented Programmingと関連します。外部からの入力（JSON）は「未検証の値」です。DTO からドメイン型への変換は、UnvalidatedOrder から ValidatedOrder への変換と同じパターンです。信頼できない入力を、信頼できるドメイン型に「変換」します。外部を信頼するな。まず変換せよ。DTO は、外部世界とドメインの境界を守る防壁です。12. Persistence本章は、ドメインモデルをデータベースに永続化する方法を解説します。DDD の原則に従いながら、現実のインフラと向き合います。永続化の原則本章の冒頭で、著者は 3 つの原則を示します。永続化を端に押し出す（Push persistence to the edges）: ワークフローの内部では I/O を行わないコマンドとクエリを分離する（CQRS）: 更新操作と読み取り操作を分けるBounded Contextは自分のデータストアを所有する: 他の Context のデータベースに直接アクセスしないこれらの原則は、「状態は例外的」という関数型の考え方と合致します。永続化を端に押し出す著者は、「ドメインロジックと I/O が混在したコード」と「分離したコード」を対比しています。なぜI/Oを境界に押し出すのか？テストが容易になる: 純粋なドメインロジックは、データベース接続なしでテストできます。入力を与えて出力を確認するだけです。推論が容易になる: 副作用がない関数は、同じ入力に対して常に同じ出力を返します。状態を追跡する必要がありません。並行処理が安全になる: 共有状態を変更しないため、競合が発生しません。変更に強くなる: データベースを変更しても、ドメインロジックは影響を受けません。逆も同様です。まず、ドメイン型を定義します。これらの型はデータベースのことを知りません。/// 未払いの請求書#[derive(Debug, Clone, PartialEq)]pub struct UnpaidInvoice {    pub invoice_id: InvoiceId,    pub amount_due: Money,}/// 支払い済みの請求書#[derive(Debug, Clone, PartialEq)]pub struct PaidInvoice {    pub invoice_id: InvoiceId,    pub amount_paid: Money,}/// 支払い処理の結果#[derive(Debug, Clone, PartialEq)]pub enum InvoicePaymentResult {    FullyPaid(PaidInvoice),    PartiallyPaid(UnpaidInvoice),}次に、純粋なドメインロジックを定義します。この関数は I/O を行いません。/// 支払いを適用する - 純粋関数、I/Oなしpub fn apply_payment(invoice: UnpaidInvoice, payment: Payment) -> InvoicePaymentResult {    let remaining = invoice.amount_due.0 - payment.amount.0;    if remaining <= 0.0 {        InvoicePaymentResult::FullyPaid(PaidInvoice {            invoice_id: invoice.invoice_id,            amount_paid: Money(invoice.amount_due.0),        })    } else {        InvoicePaymentResult::PartiallyPaid(UnpaidInvoice {            invoice_id: invoice.invoice_id,            amount_due: Money(remaining),        })    }}最後に、コマンドハンドラで I/O を境界に押し出します。パターンは「Load → Pure Logic → Save」です。/// コマンドハンドラ - I/Oは境界で行うpub fn pay_invoice_handler<R: InvoiceRepository>(    repo: &R,    command: PayInvoiceCommand,) -> Result<InvoicePaymentResult, PayInvoiceError> {    // 1. Load（I/O - 開始時）    let invoice = repo        .load(&command.invoice_id)        .ok_or(PayInvoiceError::InvoiceNotFound(command.invoice_id))?;    // 2. 純粋なドメインロジック（I/Oなし）    let result = apply_payment(invoice, command.payment);    // 3. Save（I/O - 終了時）    match &result {        InvoicePaymentResult::FullyPaid(paid) => repo.save_paid(paid),        InvoicePaymentResult::PartiallyPaid(unpaid) => repo.save_unpaid(unpaid),    }    Ok(result)}RepositoryパターンRust では Trait を使って Repository を抽象化します。これにより、テスト時にモックを注入できます。なぜTraitで抽象化するのか？テスト容易性: 本番では PostgreSQL、テストではインメモリ実装を注入できます。実装の交換可能性: データベースを変更しても、ドメインロジックは影響を受けません。依存性の逆転: ドメインが永続化の詳細に依存しません。依存の方向が逆になります。トレイトとはトレイトは「この型は〇〇ができる」という能力の定義です。例えば「データを読み込める」「データを保存できる」という能力を定義します。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org/// Repositoryトレイト - 永続化操作を抽象化pub trait InvoiceRepository {    fn load(&self, id: &InvoiceId) -> Option<UnpaidInvoice>;    fn save_unpaid(&self, invoice: &UnpaidInvoice);    fn save_paid(&self, invoice: &PaidInvoice);}trait で能力を定義し、impl Trait for Type で「この型はこの能力を持つ」と宣言します。ジェネリクスとはジェネリクスは「型を後で決める」仕組みです。<R: InvoiceRepository> は「InvoiceRepository という能力を持つ何かの型 R」という意味です。// Rは「InvoiceRepositoryという能力を持つ何か」fn pay_invoice_handler<R: InvoiceRepository>(repo: &R, ...) { ... }これで同じ関数を、異なる実装で使い回せます。// 本番ではPostgreSQLを使うpay_invoice_handler(&postgres_repo, command);// テストではメモリ上の仮実装を使うpay_invoice_handler(&in_memory_repo, command);テスト用のインメモリ実装を作れば、実際のデータベースなしでドメインロジックをテストできます。Persistence Ignorance（永続化の無知）第 2 章で「データベース駆動設計と戦う」と述べたことの実践がここにあります。ドメインモデルは、自分がどこに保存されるか知りません。知る必要もありません。Order 型はデータベースのことを知りません。永続化の詳細は、ワークフローの「端」で処理されます。この設計により、ドメインロジックの変更が永続化コードに影響しません。逆に、データベースを変更してもドメインロジックは変わりません。NoSQLとRDBの選択本章では、NoSQL（ドキュメント DB）と RDB（リレーショナル DB）の両方のアプローチを解説しています。NoSQL: Aggregate をそのままドキュメントとして保存できます。DDD との相性が良いです。RDB: OR 型（enum）のマッピングが難しいです。インピーダンスミスマッチ（オブジェクトモデルとリレーショナルモデルの構造的な不一致）が発生します。-- OR型のRDBへのマッピング（判別カラム）CREATE TABLE order_lines (    quantity_type VARCHAR(10),  -- 'Unit' or 'Kilos'    unit_quantity INT NULL,    kilogram_quantity DECIMAL NULL);どちらも完璧ではありません。「永続化は境界で行う」という原則を守ることで、純粋なドメインロジックと不純な I/O 処理を分離しやすくなります。13. Evolving a Design and Keeping It Clean本章は、本書の締めくくりとして、設計の進化と保守性について解説します。要件は変わります。ドメインモデルも変わります。その変化にどう対応するでしょうか。変化への対応著者は、DDD は「一度きりの静的なプロセス」ではないと強調します。要件が変われば、まずドメインモデルを見直します。実装をパッチするのではなく、モデルから考え直します。変更例: 配送料の追加配送料計算をワークフローに組み込むには、新しいステップを追加します。まず、新しい型を定義します。なぜ既存の型を変更せず、新しい型を作るのか？型の名前がドキュメントになる: PricedOrderWithShipping という名前だけで、「価格計算済みで配送情報も持つ注文」だと分かります。段階を明示できる: PricedOrder と PricedOrderWithShipping は別の段階だと型で表現できます。コンパイラが変更を追跡する: 型が変わると、関連する箇所すべてでコンパイルエラーが発生します。見落としがありません。/// 配送方法#[derive(Debug, Clone, PartialEq, Eq)]pub enum ShippingMethod {    Standard,    Express,    Overnight,}/// 配送情報#[derive(Debug, Clone, PartialEq)]pub struct ShippingInfo {    pub method: ShippingMethod,    pub cost: Money,}/// 配送情報付きの価格計算済み注文 - 新しい型#[derive(Debug, Clone, PartialEq)]pub struct PricedOrderWithShipping {    pub order_id: OrderId,    pub items: Vec<PricedOrderLine>,    pub amount_to_bill: Money,    pub shipping_info: ShippingInfo,}次に、新しいパイプラインステップを定義します。/// 新しいパイプラインステップ: 配送情報を追加pub fn add_shipping_info(order: PricedOrder) -> PricedOrderWithShipping {    // シンプルなロジック: $100以上は送料無料    let shipping = if order.amount_to_bill.0 > 100.0 {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(0.0),        }    } else {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(5.99),        }    };    PricedOrderWithShipping {        order_id: order.order_id,        items: order.items,        amount_to_bill: order.amount_to_bill,        shipping_info: shipping,    }}既存のコードを変更するのではなく、新しいステップを挿入します。validateOrder → priceOrder → addShippingInfo → acknowledgeOrder → createEventsこの「ステップの追加」というアプローチは、多くの機能追加に応用できます。ロギング、パフォーマンスメトリクス、認可チェック、監査。各ステップが独立していて、型が合っていれば、安全に追加・削除できます。VIP顧客の対応—入力をモデル化せよ著者は重要な指摘をしています。「ビジネスルールの出力（送料無料フラグ）ではなく、入力（VIP ステータス）をモデル化せよ」。なぜ「出力」ではなく「入力」をモデル化するのか？ルールが変わっても型は変わらない:「VIP は送料無料」→「VIP は送料 50%オフ」とルールが変わっても、CustomerStatus 型自体は変更不要です。関数だけ変えればよいです。原因をモデル化する:「送料無料かどうか」は結果（派生情報）です。原因は「VIP かどうか」です。原因をモデル化すれば、結果はいつでも計算できます。柔軟性が高い: VIP ステータスは送料以外にも使えます（優先サポート、限定商品へのアクセス等）。出力をハードコードすると、その柔軟性を失います。/// 顧客ステータス - ビジネスルールの「入力」をモデル化#[derive(Debug, Clone, PartialEq, Eq)]pub enum CustomerStatus {    Normal,    Vip,}/// 顧客#[derive(Debug, Clone, PartialEq, Eq)]pub struct Customer {    pub customer_id: String,    pub name: String,    pub status: CustomerStatus,}ビジネスルールは、入力（CustomerStatus）に基づいて決定を下します。/// 顧客ステータスに基づいて配送を計算pub fn calculate_shipping_for_customer(order: &OrderWithCustomer) -> ShippingInfo {    match order.customer.status {        CustomerStatus::Vip => ShippingInfo {            method: ShippingMethod::Express,            cost: Money(0.0), // VIPは無料のエクスプレス配送        },        CustomerStatus::Normal => {            if order.amount_to_bill.0 > 100.0 {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(0.0),                }            } else {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(5.99),                }            }        }    }}ビジネスルールが変わっても（例: VIP は送料無料→VIP は送料 50%オフ）、calculate_shipping_for_customer 関数を変更するだけでよいです。ドメインモデル自体（CustomerStatus）は変更する必要がありません。型の変更と波及効果本章の核心は、「型の変更がコンパイラによって追跡される」ことです。// 変更前pub struct PricedOrder { /* ... */ }// 変更後（配送情報を追加）pub struct PricedOrderWithShipping {    // ...    pub shipping_info: ShippingInfo,  // 新しいフィールド}PricedOrder と PricedOrderWithShipping は異なる型です。PricedOrder を期待していたコードに PricedOrderWithShipping を渡すとコンパイルエラーになります。// これはコンパイルエラー！fn process(order: PricedOrder) { /* ... */ }process(priced_order_with_shipping); // 型が違う動的型付け言語では、このような変更は「実行時エラー」として発見されます。静的型付けでは、「コンパイル時エラー」として発見されます。コンパイラがリファクタリングアシスタントとして機能します。関数型 DDD の核心は、「型でドメインを表現する」ことです。AND 型（struct）と OR 型（enum）でドメインの構造を表現します。状態遷移を別の型として定義します。制約を Smart Constructor で強制します。不正な状態を表現不可能にします。フラグを立てるな。型を作れ。これらの原則は、F#でも Rust でも適用できます。おわりに読む前の三つの悩みへの回答「はじめに」で述べた 3 つの読む動機に、本書がどう応えたかを振り返ります。1. DDDを学び直す必要があった → 関数型の視点でDDDが腹落ちしたDDD の概念—Aggregate、Entity、Value Object—は、オブジェクト指向の文脈で説明されると抽象的に感じていました。本書は、これらを「型」という具体的な道具で表現する方法を示しました。Value Object は単なる newtype です。struct OrderId(String) と書けば、それが Value Object です。Aggregate の境界は、型の境界で表現できます。ValidatedOrder と UnvalidatedOrder が別の型なら、それが境界です。「なぜそう設計するのか」を言語化できるようになりました。「この型を別にするのは、状態が違うから」「この値を newtype にするのは、ドメイン上の意味が違うから」。経験則ではなく、型システムに基づいた説明ができます。2. Rustでドメインモデリングを実践したかった → F#の概念はRustで十分(ではないかもしれないが)表現できるF#にあって Rust にない機能—パイプライン演算子、computation expressions、Units of Measure—は、確かにあります。しかし、本書の核心である「Make Illegal States Unrepresentable」は、Rust で十分に実践できたと思います。むしろ、Rust の所有権システムは F#にない利点を提供します。状態遷移を「所有権の移動」として表現できます。validate(order: UnvalidatedOrder) -> ValidatedOrder と書けば、検証前の注文は使えなくなります。F#では GC があるため、古い変数への参照が残る可能性がありますが、Rust では型システムがそれを防ぎます。3. AIエージェント時代における型システムの意味を考えたかった → 型は「AIが破れない制約」本書を読む前は、「型があると AI の生成精度が上がる」という感覚がありましたが、理論的に説明できませんでした。本書を読んで、その理由が明確になりました。型で定義された制約は、物理的に破れません。NonEmptyList<OrderLine> と定義すれば、AI は空の注文を返すコードを書けません。コンパイルが通らないからです。「このフィールドは必須です」というコメントは無視できますが、型は無視できません。これは「AI が守るべきルール」ではなく「AI が破れない壁」です。読む前と読んだ後Before（読む前）DDD の設計はある種の経験則で判断していましたRust の Option/Result は便利ですが、関数型との繋がりを考えていませんでした型があると AI の精度が上がる「気がする」程度の理解でしたAfter（読んだ後）DDD の概念を型システムの言葉で説明できるようになりましたTransformation-Oriented Programming（元のオブジェクトを変更せず、新しい値を作る）という原則を内在化しました型を「人間のためのドキュメント」かつ「AI が破れない制約」として設計できるようになりましたTransformation-Oriented Programming関数型プログラミングを学んで獲得する最も重要な概念は、実はシンプルです。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。本書を読み終えて、この一文の重みを改めて感じています。私たちはプログラミングを学ぶとき、まず「変数に値を代入する」ことから始めます。x = 1。x = x + 1。状態を変更することが、プログラミングの基本だと教わります。しかし、よく考えてみると、ビジネスロジックの大半は「入力を受け取り、変換し、出力を返す」で書けます。注文明細から合計金額を計算する → 入力と出力だけ住所をパースする → 入力と出力だけ商品コードを検証する → 入力と出力だけ状態の変更は不要です。副作用も不要です。ほとんどのビジネスロジックは、数学の関数のように書けます。では、状態が必要になるのはいつでしょうか。データベースに保存するとき。外部 API を呼ぶとき。ファイルに書き込むとき。つまり、システムの境界を越えるときだけです。この気づきが、設計の発想を変えます。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられないビジネスの世界は状態遷移で満ちています。注文は「未検証」から「検証済み」になります。カートは「空」から「商品あり」になります。申請は「提出済み」から「承認済み」になります。これは無視できません。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。order.validate() を呼ぶと、order の内部状態が変わります。この設計は、状態の追跡を難しくします。order は今どの状態なのか。どの経路を通ってここに至ったのか。フラグの組み合わせは正しいのか。常に頭の中で管理し続けなければなりません。本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。重要なのは、元のオブジェクトを変更しないことです。UnvalidatedOrder という型があります。validate という関数を適用すると、ValidatedOrder という新しい値が生まれます。このとき、元の UnvalidatedOrder には一切触れません。新しい値を作るだけです。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrder状態を「変更」するのではなく、「入力を受け取り、何かを行い、出力を返す」。元のオブジェクトには触れません。これが本書の核心です。この発想の転換がもたらすものこのアプローチを採用すると、いくつかの問題が消えます。状態の追跡が不要になる。 ValidatedOrder を持っているなら、それは「検証済みの注文」です。フラグを見る必要がありません。型がすべてを語ります。並行処理が安全になる。 元のデータを変更しないから、競合が起きません。テストが簡単になる。 入力を与えて、出力を確認します。それだけです。デバッグが楽になる。 各ステップの入出力をログに残せば、全経路が追えます。そして何より、ビジネスの言葉とコードが一致する。「見積書」が「発注書」になります。Estimate が Order になります。ビジネスの人々が頭の中で考えているモデルが、そのままコードになります。型は思考のツールである本書を読む前、型システムは「コンパイラを満足させるためのもの」だと思っていました。IDE の補完が効きます。リファクタリングが安全になります。その程度の認識でした。本書を読んで、型は「思考のツール」だと認識を改めました。AND と OR という 2 つの組み合わせで、複雑なドメインを表現できます。struct は「これとこれが両方必要」、enum は「これかこれのどちらか」。この単純な組み合わせが、ドメインの構造を可視化します。型を書くことは、ドメインを理解することです。型を読むことは、ドメインを学ぶことです。少なくとも、私はそう感じるようになりました。型で「不可能」を作る本書の内容を 2026 年の視点で読み直して、最大の発見がありました。「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」—この原則は、人間の開発者のミスを防ぐためのものとして紹介されています。しかし 2026 年現在、同じ原則がAIの出力を自動検証するフィルタとして機能しています。型で「不可能」を定義すると、AI が生成したコードのうち、その制約に違反するものはコンパイル時に除外されます。NonEmptyList<OrderLine> と定義すれば、AI が空の注文を返すコードを書いてもコンパイルエラーで検出されるVerifiedEmailAddress を要求すれば、AI が未検証メールへの送信を実装してもコンパイルが通らないUnvalidatedOrder → ValidatedOrder という型シグネチャがあれば、AI が検証をスキップするコードはコンパイルエラーになるこれは「AI に正しいコードを書かせる」のではなく、「AI が書いた誤ったコードを検出する」メカニズムです。AI の精度向上ではなく、フィルタリング機構として機能します。先日、Claude Code に「Order を作成する関数を書いて」と指示しました。生成されたコードは Vec<OrderLine> を返していました。しかし私のコードベースでは NonEmptyList<OrderLine> を使っています。コンパイルエラーが発生し、AI は「空の注文」を作るコードを出力しましたが、それが本番に混入することはありませんでした。一方、別のプロジェクトでは「このフィールドは必須」とコメントに書いただけでした。AI はそのコメントを無視して Option を返すコードを生成し、後から問題が発覚しました。型で定義された制約は、コンパイル時に検証されます。コメントは検証されません。この違いが重要です。「何を作るか」を決める能力本書の第 1 章で、著者は「開発者の仕事はコードを書くことではなく、ソフトウェアを通じて問題を解決すること」と述べています。2018 年に書かれたこの言葉は、2026 年の今、さらに重みを増しています。「何を作るか」という問いを分解してみます。ビジネス要件の理解: ドメインエキスパートとの対話、暗黙知の引き出し技術的制約の把握: 既存システムとの整合性、パフォーマンス要件、チームのスキルセット両者のトレードオフ判断: 「正解がない」状況での意思決定現時点で AI が得意なのは 2 番目です。ドキュメントやコードベースを読み、技術的な制約を分析できます。一方、1 番目と 3 番目は人間の仕事です。ドメインエキスパートとの対話で暗黙知を引き出すこと、そして「どちらも正しい」状況でトレードオフを判断すること。これらは AI に委譲できません。この構造を理解すれば、「人間の仕事」を「暗黙知の言語化」と「トレードオフ判断」に絞り込めます。本書で学んだドメインモデリングの技術は、まさにこの「暗黙知の言語化」を支援するものです。型で表現されたドメインモデルがあれば、AI は「どう作るか」を高い精度で実行できます。ドメイン駆動設計をはじめよう ―ソフトウェアの実装と事業戦略を結びつける実践技法作者:Vlad Khononovオーム社Amazon働き方の逆転—AIエージェント時代の開発スタイル本書を読みながら、自分の働き方が根本的に変わったことを実感しました。以前のモデルでは、人間がコードを書き、AI は相談相手でした。Stack Overflow の代わり、ドキュメント検索の高速化。補助的な存在です。現在のモデルでは、AI が運転席に座り、人間は助手席でナビゲーションをしています。AI にプランを練らせ、レビューし、実装させ、またレビューする。この流れが定着しました。AI は「分身」的な存在になりました。明確な指示とコンテキストを与えれば、疲労知らずで作業してくれる相棒です。Claude Opus 4.5 以降、この感覚は決定的になりました。プログラミングのシンタックスを書く機会は明らかに減りました。では、ソフトウェアエンジニアの役割はどう変化したのでしょうか。1. アーキテクチャの指針決定AI は「どう作るか」を実行できますが、「なぜそう作るか」は決められません。Bounded Context の境界をどこに引くか、技術選定のトレードオフ、パフォーマンスと保守性のバランス。これらは人間が判断します。2. コードベースから読み取れないコンテキストの整理・提供AI はコードに書かれていないことを知りません。なぜこの設計にしたか、本番環境でのみ発生する問題、チームの暗黙のコーディング規約、ビジネス上の制約。これらを言語化し、CLAUDE.md やコメントに落とし込む能力が求められます。3. 期待する挙動を自動・継続的に検証する枠組みの整備AI が書いたコードは「動く」かもしれませんが、「正しい」とは限りません。型による制約、プロパティベーステスト、E2E テスト、本番監視。これらの枠組みを整備し、AI の出力を検証し続けるのは人間の仕事です。本書の「Make Illegal States Unrepresentable」は、まさにこの 3 番目の観点で価値を発揮します。型で制約を定義すれば、AI の出力を自動的に検証できます。コンパイルが通れば、少なくとも型レベルの正しさは保証されます。コードを手で書くという作業は、実は思考の外在化プロセスでもありました。書きながら考えていた。この機会が減ったとき、思考の質をどう担保するか。正直、まだ答えが出ていません。本書のようなドメインモデリングの訓練はその答えの 1 つかもしれませんが、それで十分かどうかは分かりません。分からないまま、AI と協業し続けています。Architecture Modernization との接続本書を読みながら、Nick Tune 著『Architecture Modernization』の内容が何度も頭をよぎりました。現在、この本の翻訳に携わっています。Architecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazon『Domain Modeling Made Functional』は「新規開発」の文脈で DDD を説明しています。しかし現実の多くのプロジェクトは「既存システムの改善」です。レガシーシステムをどう分析し、背景情報からBounded Context をどう切り出し、段階的にモダナイズしていくか。『Architecture Modernization』はまさにその部分を扱っています。翻訳作業を通じて、共感できる内容が多くありました。特に「既存システムの暗黙知をどう言語化するか」という問題意識は、本書の「ドメインエキスパートとの対話」と通じるものがあります。AI エージェント時代において、この問題はさらに重要になっています。AI は「新しいコードを書く」ことは得意ですが、「既存システムの文脈を理解する」ことは苦手です。10 年前の設計判断の背景、当時の技術的制約、組織の歴史。これらを言語化し、モダナイゼーションの方針を決めるのは、依然として人間の仕事です。本書を読んで「関数型 DDD」に興味を持った方には、『Architecture Modernization』も勧めたいです。新規開発だけでなく、既存システムをどう改善するか。両方の視点を持つことで、設計の引き出しが増えます。最後に2018 年に出版された本書を、2026 年に読む価値はあったでしょうか。かなり、Yes です。ただ、正直なところ、本書の「すべて」を実践できる自信はありません。Smart Constructor を徹底すると言いながら、明日には String を直接使っているかもしれません。型で不可能を作るのは、思っているより面倒くさい作業です。締め切りに追われると、つい妥協してしまう。それでも、本書を読んだことで「何かに気づいた」感覚はあります。うまく言葉にできませんが、型を書くときの解像度が変わった気がします。Option を見たとき、「本当にこれは省略可能なのか？」と問い直すようになりました。冒頭で触れた『Architecture Modernization』の翻訳作業。本書を再読したことで、「Bounded Context」「Aggregate」といった用語を訳すとき、以前より自信を持てるようになりました。言葉の背後にある設計思想を、型という道具で理解したからです。翻訳は続いています。この感覚が正しいのかどうかは、実務で検証していくしかありません。関数型 DDD は、特定の言語やパラダイムに縛られません。F#で書かれた本書の概念は、Rust でも実践できます。そして、人間と AI が協業する時代において、「不可能を型で定義する」技術の価値はますます高まっていく—たぶん。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク]]></title>
            <link>https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</link>
            <guid isPermaLink="false">https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</guid>
            <pubDate>Tue, 20 Jan 2026 01:05:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 「なんか遅い」「たまにエラーが出る」――マイクロサービスのシステムでこんな報告を受けたとき、皆さんはどこから調べ始めますか？ Sreake事業部インターン生の小林です。2025年11月-12月の間インターンに参 […]The post Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[
		Binance账户创建 より		]]></title>
            <link>https://sreake.com/blog/chatgpt-slack-integration/#comment-4673</link>
            <guid isPermaLink="false">https://sreake.com/blog/chatgpt-slack-integration/#comment-4673</guid>
            <pubDate>Mon, 19 Jan 2026 11:29:29 GMT</pubDate>
            <content:encoded><![CDATA[Your article helped me a lot, is there any more related content? Thanks! https://www.binance.com/fr-AF/register?ref=JHQQKNKN]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、頑張るなら組織と踊れ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</guid>
            <pubDate>Mon, 19 Jan 2026 00:01:19 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「おい、辞めるな」で辞めないことを選んだ。syu-m-5151.hatenablog.com「おい、辞めないなら頑張れ」で頑張り方を学んだ。syu-m-5151.hatenablog.com見せろ。対話しろ。上司を勝たせろ。スポンサーを作れ。そう書いた。で、やってみてどうだった。正直に言う。私はうまくいかなかった。見せているつもりだった。対話しているつもりだった。上司を勝たせようとしていた。でも、空回りしていた。なぜか。組織の力学を理解していなかったからだ。いや、もっと正確に言おう。理解しようとしなかった。組織の力学——いわゆる「政治」——を、私は嫌悪していた。「実力で勝負したい」「政治なんかに関わりたくない」——そう思っていた。技術的な正しさを盾に、人間関係の機微を「非論理的」と切り捨てていた。以前、「正義のエンジニアという幻想」という記事を書いた。syu-m-5151.hatenablog.comあの記事で書いたことは、今でも私の中に残っている。媚びないことと無礼であることの区別もつかないまま、技術的優位性を振りかざしていた——そんな恥ずかしい過去を、私は持っている。今回は、その続きを書く。組織の力学について。私が嫌悪していたもの。でも、理解しなければ成果を出せないもの。そして、したたかに生きるということについて。先に結論を言っておく。理解することと、加担することは違う。そして、政治をやっている人は「汚い大人」ではない。泥臭く仕事を通そうとしているだけだ。正直に告白する。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分が知りたかったことを書く。飲み屋でそれを喋る。それがこの「おい、」シリーズの趣旨だ。おい、頑張るなら組織と踊れ。——と書いて、自分でも苦い顔をしている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。私は「正義のエンジニア」だった最初に告白しておく。私はかつて、自分の技術思想とキャリア戦略が100%正しいと信じて疑わなかった。そして、それを受け入れない企業、同僚たちが100%間違っていると本気で思っていた。今思えば、それはソフトウェアエンジニアという職業に就いた多くの若い人が陥る、ある種の思春期的な錯覚だったと思う。「なぜこんな非効率的な実装をするんですか？」「技術的にはこっちの方が正しいんですけどね」「政治的な理由で技術選定するなんて、エンジニアリングの敗北だ」そんな言葉を、私は何度思って何度口にしたことだろう。ある会議で、私は技術的に正しい提案をした。データに基づいていた。論理的だった。反論の余地がないと思っていた。却下された。理由は曖昧だった。「今はタイミングが悪い」「もう少し検討が必要」。でも、本当の理由は別にあった。私は後から知った。あの提案は、ある部門の利害と衝突していた。その部門のキーパーソンに、事前に話を通していなかった。だから、会議の場で潰された。私はその事実を伝えられた時に密かに怒った。「政治で正しい提案が潰されるなんて、おかしい」と。でも、冷静に考えると、私の方がおかしかった。技術的に正しいことと、組織で通ることは、別の問題だ。私はその区別ができていなかった。これは「誰に話を通すか」の問題だった。でも、組織の力学を理解していないことは、別の形でも現れた。あるプロジェクトで、私は黙々と成果を出していた。技術的な課題を解決し、納期を守り、品質を担保した。「これだけやれば評価されるだろう」と思っていた。評価面談で、上司はこう言った。「〇〇さんの貢献は分かっているんだけど、他のマネージャーに説明しにくいんだよね」。私は意味が分からなかった。成果を出しているのに、なぜ説明しにくいのか。後から分かった。私の仕事は「見えなかった」のだ。他のマネージャーは、私が何をしているか知らなかった。評価会議で私の名前が挙がっても、「誰？」という反応だった。私の上司は、私を推そうにも、材料がなかった。見えない仕事は、存在しないのと同じ——「おい、辞めないなら頑張れ」で書いたことだ。でも、それは「見せる」だけでは解決しない。誰に見せるか。どのタイミングで見せるか。どの文脈で見せるか。それを間違えると、見せても意味がない。私は「政治」を嫌悪していた。でも、その嫌悪が、私自身の足を引っ張っていた。媚びないと無礼を混同していたここで、痛い告白をする。「私は媚びない」——それが私のプライドだった。しかし「媚びない」と「無礼」は違う。私は単に無礼だった。コードレビューで、つい正論を優先してしまう癖があった。「このコード、正直ひどくないですか？全部書き直した方が早いです」——そんなコメントを書いていた。ある日、シニアエンジニアが個別に連絡をくれた。「君の指摘は技術的には正しい。でも、そのコメントを見た人がどう感じるか考えたことある？彼は他のタスクも抱えながら、期限に間に合わせようと必死だった。君のコメントは、その努力を全否定している」その言葉にハッとした。私は技術的な正しさばかりを見て、人の気持ちを踏みにじっていたのだ。別の機会には、マネージャーが1on1で厳しい指摘をした。「君は優秀だ。でも、チームメンバーが君を避け始めている。それでいいの？技術力があっても、一人では何も作れないよ」媚びないことと、相手を尊重することは両立する。でも当時の私にはその区別がつかなかった。率直であることと配慮がないことを混同していた。技術的な正しさを盾に、人としての礼儀を忘れていた。私は様々な言い訳を用意していた。「エンジニアは成果で評価されるべきだから人間関係は二の次」「技術的に正しいことが最優先だから言い方なんて些細な問題」「実力があれば多少の態度の悪さは許される」これらはすべて、自分の社会性の欠如を正当化するための、頭の悪い言い訳だった。まるで反抗期の中学生が「大人は汚い」と言い訳するように、私は「技術的正しさ」を盾に、自分の未熟さを隠していたのだ。転機は、年次が上がって後輩ができたときに訪れた。私の何気ない「それは違うよ」という一言で、新卒エンジニアが完全に萎縮してしまった。その後、彼は私に質問することを避けるようになり、分からないことを抱え込むように。私は、かつて自分が嫌っていた「怖い先輩」になっていたのだ。このとき、ようやく理解した。正しいことを、正しい方法で伝えられなければ、それはただの暴力だ。技術力は重要だが、それをどう使うかはもっと重要。正しいことを言っているつもりで、実際には相手の立場に立てていなかっただけだった。そういう時代もあったでよいここで、過去の自分との向き合い方について書いておく。ある時期、私は過去の失言や態度を思い出しては、布団の中で悶えていた。「あの時、なぜあんなことを言ったんだ」「もっと早く気づいていれば」——後悔の反芻は止まらなかった。コードレビューで人を傷つけた記憶。会議で空気を凍らせた記憶。「正しいことを言っているのに、なぜ分かってもらえないんだ」と憤っていた記憶。思い出すたびに、顔が熱くなった。でも、ある時気づいた。過去を責めても、過去は変わらない。変えられるのは、これからだけだ。だから、こう割り切ることにした。「過去はすべて正しかった」と。誤解しないでほしい。過去の行動が道徳的に正しかったと言いたいわけではない。あの無礼な態度は、やはり間違っていた。でも、あの経験があったから、今の自分がいる。痛い目に遭わなければ、私は変われなかった。あの失敗がなければ、この記事を書くこともなかった。過去を否定し続けると、エネルギーが過去に吸い取られる。後悔に費やす時間は、未来への投資に使えない。「あの時こうすればよかった」と100回考えるより、「これからどうするか」を1回考える方が、よほど生産的だ。過去を受け入れろ。そして、これからの人生に全力で取り組め。私は自分の未熟さを認めた。媚びないことと無礼の区別がついていなかったことを認めた。では、これからどうするか。答えは明確だった。組織の現実を、ちゃんと見ることだ。私が見ようとしなかったもの。「政治」と呼んで嫌悪していたもの。でも、理解しなければ前に進めないもの。——組織の力学について、正面から向き合う時が来た。組織には「裏の顔」があるここで、現実を直視しよう。組織には、公式なルールと非公式な力学の2つが常に存在している。公式なルールは分かりやすい。組織図、職務権限、承認フロー、評価制度。これらは明文化されていて、誰でもアクセスできる。でも、それだけで組織が動いているわけではない。非公式な力学とは、「正式な手続きには定められていないが、意思決定や資源配分に影響を与える行動」のことだ。根回し、人脈、暗黙の了解、派閥、影響力のある人物——そういうものだ。私はこれを「汚いもの」だと思っていた。でも、違った。彼らは汚いのではなく、泥臭いだけだった。非公式な力学は「善悪」ではなく「手段」だ。根回しは悪いことか。場合による。自分の私利私欲のためなら問題だ。でも、良いプロジェクトをスムーズに通すためなら、むしろ必要なことだ。関係者の懸念を事前に把握し、対処しておく。それは「政治」ではなく「配慮」とも呼べる。私が嫌悪していたのは、「非公式な力学」そのものではなかった。それを私利私欲のために使う人間だった。でも、手段と目的を混同していた。手段自体は中立だ。それをどう使うかが問題なのだ。ここで1つ、大事なことを言っておく。非公式な力学を理解することと、それに迎合することは違う。力学を理解した上で、「自分はこの手段は使わない」と決めてもいい。でも、理解せずに無視するのは、ただの怠慢だ。敵を知らずに戦っているようなものだ。私は長い間、「政治を理解する」こと自体を拒否していた。理解したら、自分も「あっち側」になる気がした。でも、それは間違いだった。理解することと、加担することは違う。そして、「あっち側」の人たちは、別に悪者ではなかった。ただ、泥臭く仕事を通そうとしていただけだった。理解した上で、どう振る舞うかは自分で決められる。組織図を信じるな次に、私が痛い目を見た話をする。組織図に描かれた権限構造と、実際に物事を動かせる力は違う。あるプロジェクトで、私は承認を得るために、組織図上の決裁者に話を持っていった。正式なルートだ。決裁者は「いいんじゃない」と言った。私は安心した。プロジェクトは頓挫した。何が起きたか。決裁者は「いいんじゃない」と言ったが、実際に動く現場のキーパーソンは別にいた。その人は私の提案に反対だった。決裁者が「いい」と言っても、現場が動かなければ、何も進まない。私は組織図を信じすぎていた。「誰が決裁権を持っているか」と「誰が実際に物事を動かせるか」は違う。そして後から気づいたことがある。同じ提案でも、事前に相談していれば通っていた可能性が高い。あのキーパーソンに、会議の前に一度話を聞きに行っていたらどうだっただろう。「こういうことを考えているんですが、懸念点はありますか？」と。相手の観点や懸念が事前に分かれば、提案に反映できる。相手も「聞いてもらった」という感覚がある。「聞かされていない」は、「間違っている」より強い反対理由になる。内容の良し悪しではない。プロセスの問題だ。これは単なる感情論ではない。組織心理学では「手続き的公正」と呼ばれる概念がある。人は、結果だけでなく、そこに至るプロセスが公正かどうかを重視する。自分の意見を聞いてもらえた、自分も関与できたという感覚があれば、たとえ結果が自分の望み通りでなくても、受け入れやすくなる。逆に、プロセスから排除されたと感じると、結果が正しくても反発する。私が会議で潰された提案は、まさにこれだった。内容は正しかった。でも、関係者は「自分は聞かれていない」と感じた。関係者に事前の相談なく、いきなり会議の場で出したことが、「あなたの意見は聞く必要がない」というメッセージになっていた。それだけで、反対する十分な理由になった。技術的に正しいかどうかと、組織で通るかどうかは、別の問題だ。そして、事前に挨拶して、相談して、懸念を聞いておく——それだけで結果が変わることが、驚くほど多い。ここで、少し視点を変えた話をする。私は下っ端として組織図に騙されてきた。でも、ある時気づいた。上に立つ人間こそ、この罠にはまりやすいのだと。下っ端の経験が少ない若いCEOやCTOが率いる組織を見てきた。彼らは往々にして、表側の組織図ばかり意識する。そして、裏側の関係性——長年かけて築かれた非公式なネットワーク——を軽視して、ドラスティックな組織変更をする。「この部署とこの部署を統合しよう」「この人をあのチームに異動させよう」——組織図の上では合理的に見える。でも、その変更が裏のネットワークをぐちゃぐちゃにすることがある。誰と誰が信頼関係を築いていたか。どのルートで情報が流れていたか。誰が実質的なキーパーソンだったか。それを無視して箱だけ動かす。若い頃の私は、そういうリーダーを「革新的だ」「スピード感がある」と思っていた。古い慣習を壊して、新しい組織を作る。カッコいいと思っていた。大人になった今は、違う見え方をする。成果を出すために、下の人間が苦労している。壊された関係性を、現場が必死で繋ぎ直している。組織図の上では「改革成功」に見えても、実際は現場の努力で何とか回っているだけ。これは「組織図を信じるな」の裏返しだ。組織図だけを見て動く危険は、下っ端だけの問題ではない。リーダーが組織図だけを見て動くと、現場が壊れる。私が組織の裏側を理解しようとするようになったのは、こういう経験も影響している。組織図の裏にあるものを無視すると、どうなるか。それを見てきたからだ。では、「組織図の裏にあるもの」とは、具体的に何か。私はそれを「影のネットワーク」と呼んでいる。かっこいい名前をつけたいわけではない。組織図には描かれないが、確実に存在するもの。それを言語化するために、この言葉を使っている。そしてその核心は、役職とは別に存在する権力だ。権力とは、役職に基づく権限だけではない。反対や抵抗を乗り越えて物事を実現する力。人を惹きつけ、巻き込む力。意思決定に実質的な影響を与える力。これらは、役職とは別に存在する。例えば、古株のベテラン社員。役職は高くないが、社内の歴史を全部知っている。誰と誰が仲が悪いか、過去にどんなプロジェクトが失敗したか、どの部署が何を嫌がるか。その人を味方につけると物事がスムーズに進む。敵に回すと、見えない抵抗にあう。例えば、経営者の信頼が厚い若手。役職は低いが、経営者に直接話ができる。その人の意見は、なぜか上まで届く。私は、この「影のネットワーク」を読めていなかった。組織図だけを見て、「この人に話を通せばOK」と思っていた。でも、組織図の裏には、別のネットワークがあった。なぜ「影のネットワーク」が存在するのか。理由は単純だ。組織図は「権限」を示すが、「実行力」を示さない。決裁権を持つ人が「やれ」と言っても、実際に手を動かす人が動かなければ、何も起きない。そして、実際に手を動かす人を動かせるのは、必ずしも決裁権を持つ人ではない。組織が大きくなるほど、この乖離は広がる。決裁者は現場から遠くなり、現場の信頼関係は決裁者の目に見えなくなる。結果として、「承認されたのに進まない」「反対されていないのに協力が得られない」という現象が起きる。これは個人の悪意ではない。権限と実行力が分離している構造の問題だ。組織図の裏にある「影のネットワーク」を読み解け。どの提案に誰が反発するか。誰を味方につければ障壁を突破できるか。情報がどのルートで流れるか。これが見えるようになると、立ち回り方が変わる。ここで、私が学んだ具体的な方法を書いておく。1. 会議での反応を観察する誰かが発言したとき、他の人の表情を見る。賛成しているのか、本音では反対なのか、無関心なのか。言葉ではなく、表情や態度に本音が出る。2. 「あの人に聞いてみたら」の連鎖を追う何か新しいことを始めようとしたとき、「あの人に聞いてみたら」と言われる人がいる。その人が、実質的なキーパーソンだ。組織図上の役職とは関係ない。3. 過去の意思決定を遡る大きな決定が下されたとき、「誰がどの段階で関わっていたか」を調べる。公式の決裁者だけでなく、その前に相談されていた人。その人が、影響力を持っている。4. ランチや雑談の相手を観察する誰と誰がよく一緒にいるか。情報は公式のルートだけでなく、非公式の人間関係を通じて流れる。ここまでが「見る」段階だ。では、見えたものをどう使うか。観察した後にどうするか観察だけでは意味がない。観察した情報を、行動に変える必要がある。キーパーソンが分かったら、提案の前に一度相談に行く。反対しそうな人が分かったら、その人の懸念を先回りして潰す。情報のルートが分かったら、そのルートに自分の情報を流す。最初は気が重い。「なぜこんな面倒なことを」と思う。でも、一度やってみると、驚くほど物事がスムーズに進む。私も最初は抵抗があった。でも、「正しい提案が政治で潰される」ことに比べれば、事前の相談なんて些細な手間だと気づいた。私が変わるまでの話ここまで読んで、「分かったけど、やっぱり嫌だ」と思う人がいるだろう。「政治なんかしたくない」「実力で評価されるべきだ」「こんなことに時間を使いたくない」。その気持ちは分かる。私もそうだった。そして正直に言えば、今でも完全には割り切れていない。私が組織の力学をどう受け止めてきたか、正直に書く。最初は、拒絶していた。長い間、ずっとそうだった。「実力で評価されるべきだ」「政治をやる奴は汚い大人だ」「自分はそういうことはしない」。そう思っていた。この時期は、現実とのギャップに苦しんだ。「なんで自分より実力のないあいつが評価されるんだ」「この会社はおかしい」。怒りや失望があった。でも、状況は変わらなかった。居酒屋で同僚と愚痴を言っていた。「あいつは政治がうまいだけだ」「実力で勝負しろよ」。言うたびに少し楽になった。でも、翌日も同じ状況が続いた。全ての原因を外部に求めていた。自分が提案した新技術が却下されれば「老害が変化を恐れている」と憤り、レガシーコードの改修を任されれば「俺の才能の無駄遣い」と不満を漏らし、ドキュメント作成を頼まれれば「エンジニアの仕事じゃない」と文句を言う。でも振り返ってみれば明らかだ。問題は私自身にあった。技術的な正しさだけを追求し、ビジネス的な制約や組織の事情を理解しようとしなかった。転機があった。尊敬していた先輩が、根回しをしているのを見た。「あの人も政治をやるのか」と最初は失望した。でも、よく見ると違った。先輩は、良いプロジェクトを通すために、関係者の懸念を事前に聞いて回っていた。それは「政治」というより「配慮」だった。「政治」と「配慮」の境界は曖昧だ。私が嫌悪していた「政治」の中には、実は「配慮」も含まれていた。それに気づいてから、少し楽になった。組織の力学を「存在するもの」として認められるようになった。好き嫌いを超えて、「まあ、そういうものだよな」と思えるようになった。過度に振り回されない心理的安定が生まれた。今はどうか。正直に言う。私はまだ、完全には割り切れていない。根回しをすることに、今でも抵抗がある。「これは本当に必要なのか」「実力で勝負すべきじゃないのか」と思う。でも、必要な場面では、やるようになった。割り切れないまま、やっている。——と書いて、立ち止まる。私と同じように変われ、と言いたいわけではない。どこまで受け入れるかは、自分で決めていい。「存在は認めるけど、自分はやらない」でもいい。「存在を認めることすら嫌だ」なら、別の環境を探してもいい。ただ、組織の力学を拒絶し続けていると苦しい。現実と理想のギャップに消耗し続ける。だから、少なくとも「存在を認める」ところまでは進んだ方が、楽になる。その先は、自分で決めればいい。譲れないもののために、譲るものを決める「存在を認める」ところまで進んだとする。でも、それだけでは足りない。認めた上で、どう振る舞うか。全部受け入れるのか。全部拒否するのか。——どちらも違う。私が辿り着いた答えは、もっと戦略的なものだった。ここで、私が学んだ重要なことを書く。本質を守るために、形式では妥協する。やがて私は真剣に考えるようになった。自分が本当に譲れないものは何か？見極める基準は1つ。「あったらいいな」は捨てろ。「なくなったら壊れる」だけを守れ。私にとって譲れないのは3つだった。1つ目は技術的な誠実さ。嘘はつかない、質の低いコードは書かない。これを失ったら、自分を信頼できなくなる。2つ目はユーザーファースト。エンドユーザーの利益を最優先する。これを失ったら、仕事の意味を感じられなくなる。3つ目は継続的な学習。常に新しいことを学び続ける。これを失ったら、市場価値が消える。これ以外は、状況に応じて柔軟に対応することにした。表現方法やタイミングを妥協しても、私は壊れない。だから手放せる。表現方法では本音を建前でオブラートに包むようになった。タイミングも最適な時期を待つように。プロセスでは目的のためなら遠回りも受け入れ、形式的には無駄に見える会議や書類も必要なら対応するようになった。全てを守ろうとすると、全てを失う。なぜか。理由は単純だ。妥協できない領域が増えるほど、交渉の余地は減る。交渉の余地が減るほど、衝突は増える。衝突が増えるほど、消耗する。消耗すると、本当に守りたかったものまで守るエネルギーがなくなる。私は以前、表現方法でも、タイミングでも、プロセスでも、一切妥協しなかった。「正しいことを、正しいタイミングで、正しい方法で言う」——それが自分の信念だと思っていた。結果、毎回衝突し、毎回消耗し、最終的には技術的な誠実さすら保てなくなった。疲れ果てて、どうでもよくなったのだ。だから、何を守り、何を手放すかを決める。これが大人の戦略だ。以前は、「妥協＝敗北」だと思っていた。でも違った。戦略的な妥協は、本質を守るための手段だ。形式で妥協し、本質を守る。それは負けではない。むしろ、本当に大事なもののために、大事でないものを手放す勇気だ。したたかに生きる戦略「譲れないものを守り、それ以外では妥協する」——それは分かった。でも、正直に言えば、それだけでは物足りない。守りに入っているだけだ。もっと攻めの姿勢で、組織を「利用」することはできないのか。——そう考えるようになった。ここで、もう一歩踏み込んだ話をする。技術は手段であって目的ではない——組織から見れば、そうだ。でも正直に言えば、私自身は技術的な興味に駆動されている。新しい技術を学ぶことが楽しいし、エレガントなコードを書くことに喜びを感じる。ビジネス価値なんてどうでもよくて、ただ面白い技術を触っていたいだけ、というのが本音だ。でも、お金をもらって仕事をする以上、建前上それが主目的とは言いづらい。だからこそ「したたかにやろうぜ」という考え方が大切なのだ。つまり、組織が求める「成果」という枠組みを利用して、自分の技術的好奇心を満たすということ。表向きは「ビジネス価値の創出」を掲げながら、実際には「面白い技術で遊ぶ」ための正当性を確保する。例えば、「パフォーマンス改善」という大義名分のもとで、最新のフレームワークを導入する。「開発効率の向上」という建前で、面白そうなツールチェーンを構築する。「技術的負債の解消」という錦の御旗を掲げて、自分が書きたいようにコードを書き直す。重要なのは、これらの建前が単なる口実ではなく、実際に価値を生み出すことだ。新技術で遊びながら、本当にパフォーマンスを改善する。好きなツールを使いながら、実際に開発効率を上げる。コードを書き直しながら、本当に保守性を向上させる。ここで正直に告白しておく。私はこの戦略で失敗したことがある。「開発効率の向上」を名目に、面白そうなビルドツールを導入した。確かに面白かった。でも、チームの学習コストを甘く見積もっていた。結果として、効率は上がるどころか下がった。建前が嘘になった瞬間、「あいつは自分のことしか考えていない」という評価が下された。信頼を取り戻すのに、かなりの時間がかかった。したたかさの前提は、建前が本当に価値を生み出すことだ。建前が嘘になった瞬間、したたかさは不誠実に変わる。自分が楽しいかどうかではなく、本当に成果が出るかどうか。その見極めを間違えると、戦略は破綻する。「プロフェッショナルとして責任を果たします」と胸を張りながら、心の中では「やった！これで堂々とRustが書ける！」と小躍りする。この二重構造こそが、エンジニアとしてのしたたかさだ。ただし、小躍りする前に、本当に成果が出るかを冷静に見極めること。それを怠ると、私のように痛い目を見る。組織は成果を得て満足し、私たちは技術的満足を得る。Win-Winの関係を作り出すこと。それは決して不誠実ではなく、むしろ異なる価値観を持つ者同士が、お互いの利益を最大化する賢明な戦略なのだ。組織をハックしろ。建前で成果を出し、本音で技術を楽しめ。影響力は才能ではなくスキルだここまで「したたかにやれ」と書いてきた。「でも、自分は政治が苦手だ」という人がいるだろう。分かる。私もそうだった。というか、今でもそうだ。人の顔色を読むのが苦手だし、根回しは面倒くさいと思っている。でも、安心してほしい。影響力は先天的な才能ではなく、後天的に磨けるスキルだ。私も苦手だった。今でも得意とは言えない。でも、意識して練習することで、少しずつマシになった。組織における対人影響力は、5つの能力で構成されている。これらは「観察→洞察→共感→表現→一貫性」というプロセスで連鎖する。1. 観察——表面を見る最初の能力は観察だ。目の前で起きていることを正確に捉える。何を観察するか。言葉——誰が何を言ったか。態度——表情、姿勢、声のトーン。関係——誰と誰が近いか、誰が誰を避けているか。反応——ある発言に対して、他の人がどう反応したか。観察は受動的な行為に見えるが、意識しないとできない。会議で自分の発言に集中していると、他の人の反応を見落とす。発言を減らし、観察を増やす——これだけで得られる情報量は変わる。2. 洞察——本質を見抜く観察の次は洞察だ。表面の情報から、見えないものを推測する。洞察とは何か。動機を読む——この人は何を求めているのか、何を恐れているのか。構造を読む——この組織で、誰が実質的な力を持っているのか。文脈を読む——この議論は、どんな歴史の上に成り立っているのか。観察が「何が起きているか」を捉えるなら、洞察は「なぜ起きているか」を捉える。同じ事象を見ても、洞察の深さで解釈は変わる。表面的な反対意見の裏に、本当の懸念が隠れていることがある。3. 共感——相手の立場に立つ洞察の次は共感だ。相手の世界を、相手の視点から理解する。共感は「同意」ではない。相手の意見に賛成しなくても、相手がなぜそう考えるかを理解することはできる。「この人の立場なら、確かにそう思うだろう」——その理解があれば、対立は減る。エンジニアは共感を軽視しがちだ。論理が正しければ、相手の感情は関係ないと思っている。しかし、人は論理だけでは動かない。自分の立場を理解してくれていると感じたとき、初めて耳を傾ける。4. 表現——相手に響かせる共感の次は表現だ。自分の考えを、相手に届く形で伝える。表現の本質は「相手に合わせる」ことだ。論理で動く人には論理を。感情で動く人には感情を。利害で動く人には利害を。同じ提案でも、切り口を変えれば響き方が変わる。「伝える」と「伝わる」は違う。自分が言いたいことを言うのは「伝える」。相手が受け取れる形で届けるのが「伝わる」。影響力とは「伝わる」力だ。5. 一貫性——信頼を積む最後は一貫性だ。これが他の4つを支える土台になる。一貫性とは何か。言ったことを実行する。約束を守る。嘘をつかない。単純だが、最も難しい。なぜ難しいか。一貫性を保つには、「できない約束をしない」という自制が必要だからだ。期待に応えたくて、つい「やります」と言ってしまう。しかし、守れない約束は信頼を削る。「できません」と言える人の方が、長期的には信頼される。一貫性がなければ、観察も洞察も共感も表現も、すべて無駄になる。「あの人の言うことは当てにならない」——そう思われた瞬間、影響力は消える。これら5つは、すべて後天的に磨けるスキルだ。生まれつきの才能ではない。ただし、順番がある。土台となる「一貫性」がなければ、他の4つは機能しない。まず信頼を築き、その上に観察・洞察・共感・表現を乗せる。「専門性」と「人望」が最強のカードだここまで「組織の力学を理解しろ」「影響力を磨け」と書いてきた。でも、ここで安心してほしいことがある。最も持続する影響力は「専門性」と「人望」から生まれる。なぜそう言えるのか。少し整理してみる。人が他人を動かす力——影響力には、いくつかの種類がある。ソフトウェアエンジニアの現場で見かける例で説明する。報酬で動かす場合がある。「このリファクタリングを完了させたら、次のスプリントで好きな技術調査の時間をあげる」。評価で動かすこともある。「このタスクを断ったら、次の評価に響くよ」。役職で動かすパターンもある。「テックリードの判断だから、この設計で行く」。データで動かすこともできる。「ベンチマークの結果、この実装の方が30%速い」。そして、専門性で動かす場合がある。「Kubernetesのことなら〇〇さんに聞けば間違いない」。人望で動かす場合もある。「あの人が言うなら、きっと理由があるはず」。このうち、専門性と人望が最も強い。なぜか。この2つは、相手が「自分から納得して動く」ときに生じるからだ。報酬・評価・役職で動かす場合、相手は「仕方なく」動いている。上司が変わったり、評価制度が変わったりすれば、その影響力は消える。「テックリードが言うから従う」で動いていたチームは、テックリードがいなくなれば元に戻る。専門性と人望で動かす場合、相手は「この人の言うことだから」と自分から動いている。その人がいなくなっても、「あの人ならどう判断するだろう」と考え続ける。影響が内面化されている。外からの圧力で動いた行動は、圧力がなくなれば止まる。内側から納得して動いた行動は、続く。ただし、注意点がある。どのカードが強いかは、組織や部署によって違う。エンジニアだけの組織では、データが圧倒的に強い。「ベンチマークの結果」「障害の根本原因分析」「パフォーマンス計測」——数字で示せば、それだけで説得力がある。論理と数字を重視する文化があるからだ。でも、営業部門やマーケティング部門では違う。データより「この人が言うなら」という人望が効くことがある。経営層との会議では、役職や過去の実績が重みを持つ。同じ会社でも、部署が変われば有効なカードは変わる。私はエンジニア組織にいることが多いので、データと専門性に頼りがちだ。でも、他部署との調整では、それだけでは通用しないことを何度も経験した。相手が何を重視するかを見極めて、カードを使い分ける必要がある。だから、長期的な影響力を構築するなら、専門性を磨き、人として尊敬される存在になることが最も確実な方法だ。専門性と人望は、どの組織でも比較的通用しやすい。「政治力を磨け」と言われると抵抗がある人も、「専門性を磨け」なら抵抗がないだろう。実は、専門性を磨くことは、組織における影響力を高める最も正攻法なアプローチなのだ。ここで、私の経験を1つ書いておく。ある領域で、私はチームの中で一番詳しくなった。別に政治をしたわけではない。ただ、その領域を深掘りし続けた。ドキュメントを読み、実験し、知見を共有した。すると、向こうから相談が来るようになった。「〇〇のことは△△さんに聞けばいい」という評判が立った。会議で発言すると、その領域については私の意見が尊重されるようになった。これは「政治」ではない。専門性による影響力だ。ただし、専門性を万能視するのは危険だ。限界もある。具体的に言おう。専門性が効くのは「その領域の意思決定」に限られる。組織全体の方向性、予算配分、人事——こういった領域横断的な意思決定では、専門性だけでは戦えない。私も経験がある。技術的な判断では尊重されるようになったが、プロジェクトの優先順位を決める会議では、相変わらず発言力がなかった。専門性は「深さ」を与えるが、「広さ」は別の力学で決まる。それでも、専門性があれば、政治力が弱くても、ある程度は戦える。少なくとも、自分の専門領域では発言権が得られる。そこを足がかりにして、徐々に影響力を広げていくことができる。だから、「政治が苦手だ」という人に言いたい。まず専門性を磨け。それが最も確実な道だ。政治力は、専門性という土台の上に乗せるオプションとして考えればいい。土台がないまま政治力だけ磨いても、長続きしない。組織と踊るための心構え最後に、心構えの話をする。おい、頑張るなら組織と踊れ。これは「組織に従属しろ」という意味ではない。ダンスは、相手の動きを感じながら、自分も動く。一方的にリードするわけでも、一方的にフォローするわけでもない。相手と自分の動きが調和して、初めてダンスになる。組織も同じだ。組織の力学を無視して突っ走ると、壁にぶつかる。かといって、組織に完全に従属すると、自分の意志がなくなる。組織の力学を理解し、その中で自分の目標を追求する。組織を動かしながら、自分も動く。これが「組織と踊る」ということだ。組織を敵視するな。かといって、盲従するな。組織は、自分の目標を達成するためのプラットフォームだ。うまく使えば、一人ではできないことができる。敵視していたら、使いこなせない。短期の勝ち負けにこだわるな。組織での影響力は、長期的に築くものだ。一回の会議で勝った負けたは、大した問題ではない。信頼の蓄積、専門性の蓄積、関係性の蓄積。これらが時間をかけて積み上がったとき、本当の影響力が生まれる。自分の価値観を失うな。組織の力学を理解し、活用することと、自分の価値観を捨てることは違う。「この方法は使えるけど、自分はやりたくない」と思うなら、やらなくていい。別の方法を探せばいい。「媚びない」ことと「無礼」であることは全く違う。前者は信念を持つことであり、後者は単なる社会性の欠如だ。同様に、「したたか」であることと「ずる賢い」ことも違う。前者は双方の利益を最大化する戦略的思考であり、後者は単なる利己主義だ。私は今でも、根回しに抵抗がある。でも、必要な場面ではやる。やりながら、「これでいいのか」と自問する。割り切れないまま、やっている。組織と踊るというのは、自分を殺すことではない。自分を活かしながら、組織の中で成果を出す方法を見つけることだ。その方法は、人によって違う。自分なりの踊り方を見つければいい。届かない人へここまで書いてきて、立ち止まる。「組織の力学を理解しろ」「影響力を磨け」「組織と踊れ」——私はそう書いた。でも、この記事には前提条件がある。この記事が有効なのは、以下の条件が揃っている場合だ。組織がまともである——努力が報われる余地がある自分にエネルギーがある——行動を起こす余力がある組織で働くことを選んでいる——別の選択肢を選んでいないこの前提が成り立たない場合、この記事は役に立たない。それぞれ見ていく。組織が合わない人がいるそもそも、組織で働くことが向いていない人がいる。組織の力学を理解しろと言われても、理解する気力がない。人間関係を築けと言われても、それ自体がストレスだ。会議で発言しろと言われても、声が出ない。彼らは「能力がない」のではない。組織という形態が合わないのだ。フリーランス、起業、小規模チーム、リモートワーク——組織以外の働き方もある。そちらが合う人もいる。「おい、頑張るなら組織と踊れ」は、組織で働くことを前提としている。その前提自体が合わない人には、この記事は届かない。力学を理解しても動けない人がいる組織の力学を理解した。影響力を磨く方法も分かった。でも、動けない。すでに消耗している人。根回しをする気力がない人。人間関係を築くエネルギーがない人。彼らに「影響力を磨け」と言っても、無理だ。まず休む必要がある。構造的に無理な組織もあるどんなに力学を理解しても、どんなに影響力を磨いても、無理な組織もある。腐敗した評価制度。声の大きい人だけが勝つ文化。変える気のない経営層。そういう組織では、個人の努力で変えられることに限界がある。「組織と踊れ」と言っても、相手がダンスをする気がないなら、成立しない。この記事は、「組織がまともで、自分にエネルギーがある」ことを前提にしている。その前提が成り立たないなら、この記事は役に立たない。「踊らない」という選択肢もある「組織と踊る」ことを選ばない、という選択肢もある。専門性だけで勝負する。政治には一切関わらない。評価されなくても気にしない。自分のペースで、自分のやり方で働く。それは「負け」ではない。評価ゲームから意識的に降りるという戦略だ。「おい、辞めないなら頑張れ」で書いたことを繰り返す。頑張れないなら、頑張らなくていい。降りてもいい。休んでもいい。それも、1つの選択だ。おわりに「おい、辞めるな」で辞めないことを選んだ。「おい、辞めないなら頑張れ」で頑張り方を学んだ。そして今回、「おい、頑張るなら組織と踊れ」で組織の力学を学んだ。正直に言う。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分は、これを知りたかった。組織の力学を理解せず、「政治は汚い」と嫌悪しながら、壁にぶつかり続けていた。正義のエンジニアという幻想に囚われて、媚びないことと無礼を混同していた。その時間は、もったいなかった。組織の力学を理解しろ。でも、専門性と人望が最強のカードだ。政治に長けていても、実力がなければ長続きしない。実力があっても、組織の力学を無視していたら成果につながらない。両方必要だ。でも、長期的に見れば、専門性と人望が最も確実な道だ。譲れないもののために、譲るものを決めろ。したたかに生きろ。組織を敵視するな。盲従するな。組織と踊れ。——と書いて、自分でも苦い顔をしている。「お前も結局、体制に飲み込まれたのか」——かつての私なら、今の私をそう批判しただろう。しかし、それでいいのだ。技術的な純粋さを追求することと、社会的な成熟を遂げることは矛盾しない。むしろ、両方を兼ね備えてこそ、プロの仕事と言えるのではないだろうか。媚びないことと無礼の区別がつかなかった、頭の悪い反抗期は終わった。正直に言えば、私はまだ上手に踊れていない。根回しに抵抗がある。状況認識力が弱い。会議で空気を読めない。それでも、以前よりはマシになった。壁にぶつかる回数は減った。この記事が、かつての私のような人に届けばいいと思う。「政治は汚い」と思いながら、壁にぶつかり続けている人。組織の力学を理解することに抵抗がある人。「正義のエンジニア」という幻想に囚われている人。理解することと、加担することは違う。理解した上で、どう振る舞うかは自分で決められる。おい、頑張るなら組織と踊れ。踊れないなら、休め。踊り方は、自分で決めろ。——と、ここまで書いてきた。でも、最後に付け加えておく。組織が合わないなら、別の場所を探せばいい。それも、1つの選択だ。私も、まだ上手に踊れていない。それでも、やっている。それでいいのだと思う。かつての私のような若いエンジニアを見かけたら、優しく、でもはっきりと伝えたいと思う。「君の気持ちはよく分かる。でも、もっといい方法があるよ。一緒にしたたかにやっていこうぜ」と。多分昔の私だったら「は？日和って迎合した負け犬が何言ってんの？」とか思って、心の中で見下しながら表面上は「はい、参考にします」って適当に流すんでしょうね。まあ、それでいいんです。私も通った道だから。痛い目に遭うまで、人は変われない。私もそうだった。その時になって初めて、この言葉の意味が分かるはずです。けど大人として言う義務があるので言っておきました。参考書籍人を動かす　改訂文庫版作者:Ｄ・カーネギー創元社AmazonDD(どっちもどっち)論 「解決できない問題」には理由がある (WPB eBooks)作者:橘玲集英社Amazonその仕事、全部やめてみよう――１％の本質をつかむ「シンプルな考え方」作者:小野 和俊ダイヤモンド社Amazonアーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonこれからの「正義」の話をしよう ──いまを生き延びるための哲学 (ハヤカワ・ノンフィクション文庫)作者:マイケル・サンデル早川書房AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazonスタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazon組織が変わる――行き詰まりから一歩抜け出す対話の方法2 on 2作者:宇田川 元一ダイヤモンド社Amazonモンク思考―自分に集中する技術作者:ジェイ・シェティ東洋経済新報社AmazonSOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社Amazon多様性の科学作者:マシュー・サイドディスカヴァー・トゥエンティワンAmazon［新版］組織行動の考え方―個人と組織と社会に元気を届ける実践知作者:金井 壽宏,高橋 潔,服部 泰宏東洋経済新報社Amazonソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社AmazonTHE CULTURE CODE 最強チームをつくる方法作者:ダニエル・コイル,楠木建かんき出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[プログラミングが好きな人こそ今の時代、プログラマーになる方がいいと思う。- 「プログラミングが好きな人は、もうIT業界に来るな。」を読んで]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</guid>
            <pubDate>Sun, 18 Jan 2026 03:31:51 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。待っている間、Xを開いた。流れてきたタイトルに、手が止まった。「プログラミングが好きな人は、もうIT業界に来るな。」note.comリサーチは終わっていた。結果を確認しないまま、記事を読んでいた。小学生の頃から黒い画面に向かい続けてきたエンジニアが、生成AIの登場によって「自分の手で作る喜び」を奪われつつあると語っていた。「心の中で何かが音を立てて崩れる」という表現があった。共感したのか、と聞かれると困る。共感しなかったのか、と聞かれても困る。たぶん、どちらでもある。読み終えて、エディタに戻った。さっきまで何をしていたか、思い出せなかった。反論したいわけではなかった。ただ、何かが引っかかっていた。「プログラミングが好き」という言葉だ。この人の「好き」と、私の「好き」は、同じものを指しているのだろうか。コーヒーを淹れた。飲みながら、書くことにした。「好き」の中身を分解する元記事の筆者と私の違いは、能力でも経験でもない。「プログラミングが好き」という言葉が指す範囲が違うのだ。言葉は同じでも、中身が違う。「好き」と言ったとき、何を思い浮かべているか。キーボードを叩く指先か、頭の中で組み上がる構造か、動いた瞬間の達成感か。同じ「好き」でも、その中身は人によって違う。中身が違えば、奪われるものも違う。プログラミングという行為には、少なくとも3つのレイヤーがある。——もちろん、これは私の視点からの便宜的な分類だ。元記事の筆者にとっては、まったく別の分け方があるかもしれない。「書くこと」と「考えること」は分離できない、という人もいるだろう。書きながら考える。考えながら書く。その不可分さこそがプログラミングだ、と。それでも、ここでは一旦この枠組みで話を進める。身体感覚：キーボードを叩く感触、コードを書く行為そのもの、画面に流れる快感知的作業：問題を分解し、設計し、実装する思考プロセス創造行為：何かを作り、動かし、世に出す達成感元記事の筆者が愛していたのは、単なる身体感覚ではないように思う。「自分の指先から生まれるロジックが、動かなかったものを動かす」——その創造の主体性だ。自分の手で書いたコードが動く。その実感が奪われたから「楽園が消えた」と感じている。AIが書いたコードをチェックする「検品係」では、創造の主体は自分ではない。これは「間違った好き」ではない。彼の文脈では、それが正解だった。小学生の頃から黒い画面に向かい続け、自分の手でコードを書くことで世界を動かしてきた。その積み重ねの上に立っている。私が「設計が好き」と言えるのも、私の文脈があるからだ。どちらが正しいという話ではない。私が好きだったのは、2だった。課題があったら適切なサイズに分割して、「この問題はこう実装すれば解決するな」と構造が頭の中で閃く。そこからコードを書き続けていく。その一連のプロセスが好きだった。AIがコードを出力するようになって初めて、自分の「好き」が何だったのか見えた。コードを書けなくなったとき、喪失感を感じたか？ 感じなかった。むしろ「これで設計に集中できる」と思った。そのとき気づいた。自分が好きだったのは、タイピングではなく、考えて、作って、また考える、その繰り返しだった。では、何が奪われ、何が残ったのか整理しよう。奪われたもの：タイピングの時間、実装の試行錯誤、ググって解決策を探す時間。残ったもの：設計を考える時間、AIの出力を評価する時間、「もっと良い方法はないか」と考える時間。私にとって、後者こそがプログラミングの核心だった。もっと言えば、コードを書く時間に追われて後回しにしていた「設計」や「トレードオフの検討」——いわゆるソフトウェアエンジニアリングの領域に、ようやく時間を使える。だから、「奪われていない」と言い切れる。実践ソフトウェアエンジニアリング (第9版)作者:ロジャー・プレスマン,ブルース・マキシムオーム社AmazonGoogleのソフトウェアエンジニアリング ―持続可能なプログラミングを支える技術、文化、プロセス作者:Titus Winters,Tom Manshreck,Hyrum WrightオライリージャパンAmazon正直に言う、めちゃくちゃ楽しい今、めちゃくちゃ楽しい。作りたいものがある。指示を出す。コードが出てくる。レビューする。直す。動く。——以前なら「面倒だな」と後回しにしていたアイデアや知識が、数分で形になる。さっき書いた「ソフトウェアエンジニアリングの領域」——設計、トレードオフ、アーキテクチャ。そこにやっと向き合えている感覚がある。楽しい。素直に、楽しい。これを「検品係」と呼ぶなら、私は世界一楽しい検品係だ。コードレビューが好きで良かった思えば、私はコードレビューが比較的好きだった。他人のコードを読んで、「この人はこう考えてるんだろうな」というのが見えると嬉しい。なぜこの設計にしたのか、どこで迷ったのか。コードの向こうに思考が透けて見える瞬間が好きだった。知らない言語機能や、思いつきもしなかった構造で課題を解決しているのを見ると、良し悪しにかかわらず楽しい。正解かどうかより、発見の方が全然面白い。正解とは常に文脈の中にしかない。チームの習熟度、プロダクトのフェーズ、パフォーマンス要件、保守する人間の数。同じ課題でも、文脈が変われば最適解は変わる。だから「このコードは正しいか」という問いより、「この文脈でなぜこう書いたのか」という問いの方が面白い。そして、その問いに答えようとする過程で、自分の中の「正解」も揺らぐ。揺らぐことが学びだ。AIが出力したコードをレビューしていると、これが想像以上に勉強になる。先日、AIに「CSVパーサーを書いて」と頼んだ。返ってきたコードを見て驚いた。私なら正規表現でゴリ押しするところを、状態機械で書いている。エスケープ処理も完璧だ。「なるほど、このアプローチがあったか」と笑った。逆に、「いや、これは現場では使えない」と思う瞬間もある。過剰に抽象化されていたり、エラーハンドリングが甘かったり。その判断力こそ、レビューを通じて研ぎ澄まされる。——もっとも、私の判断が正しいとは限らない。AIの提案を「使えない」と却下した翌週、まったく同じアプローチを別の記事で「ベストプラクティス」として紹介されているのを見たこともある。ただ、これは15年やってきた人間だから言えることだ。状態機械の良さが分かるのは、正規表現ゴリ押しで痛い目を見たことがあるからだ。「なるほど、このアプローチがあったか」と唸れるのは、比較対象を持っているからだ。経験ゼロの人がAIの出力から体系的に学べるかは、正直分からない。むしろ、学べない可能性の方が高い気がする。コードレビューが苦手な人には、AIとの協働は苦行かもしれない。でも、レビューが好きな人間にとっては、無限に相手がいるジムのようなものだ。疲れない、休まない、いつでも付き合ってくれる相手。しかも、毎回違うアプローチを見せてくれる。「検品」と「協働」の違い元記事は「検品係になった」と嘆いている。創造の主体でありたかった人にとって、この表現は正確だと思う。自分が書きたかったコードを他者（AI）が書き、自分はそれをチェックするだけ。主体と客体が入れ替わっている。ただ、私の場合は少し違った。携帯電話は私のことをめちゃくちゃ記憶している。連絡先、スケジュール、位置情報、検索履歴。私より私のことを知っているかもしれない。でも、携帯電話を使っているとき、「主体を奪われた」とは感じない。道具として使っている感覚がある。生成AIは違う。コードを書く、文章を書く、設計を考える——これまで「私がやること」だった領域に、AIが入り込んでくる。携帯電話が記憶を代替しても主体性は揺らがなかったが、生成AIは創造を代替しようとする。だから主体性が脅かされる感覚が生まれる。それでも、私は自分が主体だと思っている。なぜか。www.youtube.com答えは、関わり方にある。「検品」と「協働」の違いは何か。検品は受動的だ。ラインを流れてくる製品をチェックし、不良品を弾く。渡されたものをチェックするだけ。協働は能動的だ。方向性を示し、フィードバックを与え、成果物を一緒に作り上げる。私がAIとやっているのは後者だ。具体的に言うと——「こういう設計で書いて」と指示を出す（方向性）出てきたコードを見て「ここはこう直して」とフィードバックする「いや、アプローチ自体を変えよう」と軌道修正する最終的な成果物が完成するこのやりとりは、人間同士のペアプログラミングと構造的に同じだ。相手が人間かAIかの違いしかない。検品係は受け身だが、私は能動的にAIを導いている。方向を決め、判断を下し、軌道修正をかける。この能動性が、主体性を保つ鍵だ。AIとの関係を一言で表すなら、「相棒」ではなく「優秀だが判断できない後輩」が近い。指示を明確にすれば良い仕事をする。曖昧にすると、意図しないものが返ってくる。筆者が言うように、書く時間は減った。でも、考える時間は増えた。どう分割するか。どう設計するか。AIが出してきたコードのどこを採用し、どこを直すか。そして、仮にこれが「検品」だったとしても、私はその過程でかなり学んでいる。「こんな書き方があるのか」と何度も唸った。特に経験の浅い言語では顕著だ。自分で書いていたら絶対に思いつかないイディオムを、AIは平気で出してくる。検品のつもりが、いつの間にか授業を受けている。ただし、ここには落とし穴がある。AIが出したコードをそのまま使って「動いた、終わり」で済ませると、何も残らない。効率は上がる。成果も出る。でも、1週間後に「なぜこう書いたの？」と聞かれても、答えられない。因果を辿れない。自分が責任を持って出力したコードのはずなのに、説明しようとすると言葉が出てこない。以前、「AIエージェントと協働しながら学習する方法」という記事で詳しく書いたが、学びには「摩擦」が必要だ。エラーが出る。原因がわからない。仮説を立てる。試す。失敗する。また試す。この摩擦の中で、経験が意味に変わる。学習とは、経験を意味に変換する行為だ。AIが摩擦を消してくれると、経験が意味に変わる機会も消える。syu-m-5151.hatenablog.comだから私は、AIが出したコードを「なぜこう書いたのか」と考える時間を意図的に作っている。効率だけを求めるなら不要な時間だ。でも、この「不効率な時間」が学びを生む。摩擦は削減対象ではない。設計対象だ。何を学び、何を省略するか。その選択を自分でしている限り、主体は私だ。これを「検品」と呼ぶか「協働」と呼ぶかは、本人の姿勢次第なのかもしれない。——と書いて、自分で読み返して思った。「姿勢次第」では何も言っていないのと同じだ。具体的に、検品を協働に変えるための3つのポイントを挙げてみる。意図を言語化する: 「こう書いて」ではなく「この問題を解決したい。制約はこれ」と伝える。AIに考えさせる余地を残す。出力から学ぶ: AIが出したコードを「動くかどうか」だけでなく、「なぜこう書いたか」を考える。知らないパターンがあれば調べる。フィードバックを重ねる: 一発で完璧を求めない。「ここを直して」「いや、やっぱりこっち」のやりとりを楽しむ。この3つができれば、検品は協働になる。逆に言えば、「動くか確認するだけ」なら、それは検品だ。この3つを実践するかどうか。それが検品と協働を分け、主体性を保てるかどうかを分ける。——と偉そうに書いたが、これが「正解」かどうかは分からない。私の文脈ではうまくいっている。でも、別の文脈では別の答えがあるはずだ。締め切りに追われているときは「動けばいい」になるし、疲れているときは「なぜこう書いたか」なんて考えない。理想と現実は違う。ただ、「こうありたい」という指針があるのとないのとでは、違うと思っている。それすらも、私の文脈での話だ。ただし、これは私のケースだここまで書いてきて、一つ断っておきたいことがある。これは私の話だ。コードレビューが好きで、設計を考えるのが好きで、タイピング速度に自信がなかった人間の話だ。もし元記事の筆者のように、「自分の手で書いたコードが動く」その実感こそが喜びだったなら——この記事は何の慰めにもならないだろう。創造の主体でありたかった人に、「検品も楽しいよ」とは言えない。その人たちに「考え方を変えろ」と言うつもりはない。「自分が書きたかった小説をAIに書かせ、誤字脱字を直す校正者のような気分」——元記事のこの表現は、痛いほど分かる。創造の主体性を奪われた感覚は、姿勢や考え方でどうにかなるものではない。彼の文脈では、それが真実だ。私が「楽しい」と言えるのは、私の文脈がたまたまそうだったからに過ぎない。「でも、結局プログラマーの仕事は減るのでは？」という反論もあるだろう。正直、分からない。AIの進化は私の想像を超えている。5年後にどうなっているか、予測する自信がない。そして、ここは誤魔化さずに言っておくべきだと思う。「楽しい人がいること」と「職業として持続可能かどうか」は、まったく別の話だ。ドライバーが100人必要だった時代から、10人で済む時代へ。私が楽しくても、市場が縮小すれば、その楽しさを職業にできる人は減る。元記事の筆者が問うているのは、たぶんそっちの話でもある。この問いについて、エンジニアに許された特別な時間の終わりというスライドがある。エンジニアがドライバー席から助手席へ移る時代が来ている、という話だ。AIが「副操縦士（Copilot）」から「操縦士（Pilot）」へ進化しつつある。続編のたかが特別な時間の終わりでは、9ヶ月後にその予測が現実化しつつあると報告されている。私がこの記事で書いてきた「協働」も、結局は助手席からの関わり方なのかもしれない。ドライバー席に座っていた時代は終わりつつある。それでも、助手席には助手席の仕事がある。呑気なドライブデートを思い浮かべたかもしれないが、全然様相は違う。ラリーのコ・ドライバーは、ただ座っているだけではない。本番前にコースを試走し、コーナーの角度、直線の距離、路面の状態、危険なポイントをすべてペースノートに書き込む。本番では猛スピードで揺さぶられる車内で、そのノートを絶妙なタイミングで読み上げる。「左3、50m、右2、クレスト注意」。ドライバーは全コースを暗記できない。コ・ドライバーなしでは走れない。AIとの協働も似ている。事前にコードベースを把握し、設計を考え、制約を整理する——これがペースノートの作成だ。本番では、AIが猛スピードでコードを生成する中、「次は右だ」「ここは危険だ」と指示を出し続ける。曖昧な指示を出せば、車は崖から落ちる。ただし、その車は時速200kmで走っている。のんびり景色を眺める余裕はない。コ・ドライバーとして生きる。それがこの時代の選択だ。——と書いたが、この比喩にも限界がある。ラリーではコ・ドライバーの仕事がなくなることはない。しかし、AIとの協働では、その保証がない。今は「設計」「評価」「判断」が人間の仕事として残っている。だが、AIが設計し、AIが実装し、AIがレビューする世界が来たら？ コ・ドライバーの席さえ、自動運転に置き換わるかもしれない。この記事は「設計や評価は人間に残る」という前提で書いている。その前提が崩れたら、私の話は無効になる。ただ、「だから今やっても意味がない」とは思わない。今この瞬間、プログラミングが楽しいなら、それでいい。未来のことは、未来の自分が考える。——もっとも、この「楽しい」を大声で言うのは少し気が引ける。誰かが失った「楽しさ」の上に、私の「楽しさ」が成り立っているかもしれないからだ。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われたら、反論できない。おわりに書き終えて、コーヒーを淹れ直した。冷めていた。あの日から何日か経った。書いている間、ずっと考えていた。私は本当に「楽しい」のか。それとも、楽しいと思いたいだけなのか。正直、分からない。分からないまま書いた。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われるかもしれない。反論できる気がしない。私の「好き」と、あの人の「好き」は、たまたま違った。彼の文脈では彼が正しく、私の文脈では私が正しい。それだけのことだ。どちらかが間違っているわけではない。明日もたぶん、AIにコードを書かせる。レビューする。直す。動かす。それを「楽しい」と感じるかどうかは、そのときになってみないと分からない。ただ、少しだけ違うことがある。「プログラミングが好き」という言葉を使うとき、自分が何を指しているのか、前より意識するようになった。摩擦は削減対象ではない。設計対象だ。——この言葉を、自分に言い聞かせるようになった。そして、自分の「正解」も揺らぐことを知った。書く前と書いた後で、考えが変わっている。元記事の筆者の気持ちが、前より分かる気がする。揺らぐことが学びだと書いた。この記事を書くこと自体が、そうだった。「IT業界に来るな」と言われた君へ。私は「来い」とは言わない。言えない。生成AIがいつか道具になる日までは。ただ、私は来てよかった。少なくとも、今日は。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。技術が人類を幸せにするかみたいな問いは常に面白いです。技術革新と不平等の1000年史　上作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon技術革新と不平等の1000年史　下作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【Vul】CodeBuildの設定ミスによるフィルタバイパス]]></title>
            <link>https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AWS CodeBuildにおける設定ミスにより、AWS提供のGitHubリポジトリがサプライチェーン攻撃のリスクにあった件]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[不動点コンビネータと無名再帰]]></title>
            <link>https://silasol.la/posts/2026-01-16-01_least_fixed_point/</link>
            <guid isPermaLink="false">https://silasol.la/posts/2026-01-16-01_least_fixed_point/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[不動点コンビネータと実践的な示唆について紹介します．]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Development Kit (ADK)における評価駆動型開発（EDD）]]></title>
            <link>https://sreake.com/blog/evaluation-driven-development-with-adk/</link>
            <guid isPermaLink="false">https://sreake.com/blog/evaluation-driven-development-with-adk/</guid>
            <pubDate>Thu, 15 Jan 2026 04:42:40 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部の井上 秀一です。私はSreake事業部にて、SREや生成AIに関するResearch & Developmentを行っています。 Agent Developmen […]The post Agent Development Kit (ADK)における評価駆動型開発（EDD） first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ory Kratosで認証を委譲する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</guid>
            <pubDate>Wed, 14 Jan 2026 05:02:48 GMT</pubDate>
            <content:encoded><![CDATA[前回からの続き前回の記事では、Playwright MCPを使ったE2Eテストで5つのバグを発見した。CORS設定の欠如、JWTトークンの切り詰め、Hydraトークンとの不一致、ミドルウェアの適用漏れ、X-Tenant-Slugヘッダーの欠如。RBACの検証とOWASP Top 10との比較まで行い、マルチテナント認証システムが一通り動くようになった。前提知識: この記事はOry Hydraシリーズの続編です。OAuth2認可コードフローの基礎知識と、Login/Consent Providerの役割を理解している前提で進めます。前回記事はこちら。動く。ちゃんと動く。でも、レビューコメントが気になった。「パスワードリセット機能は？」「MFA対応の予定は？」「メール確認フローは？」全部、自分で実装しなければならない。Argon2idでパスワードをハッシュ化するコードは書いた。ログイン認証は動く。でも、パスワードを忘れたユーザーへのリセットメール送信、そのトークン管理、有効期限の検証。TOTPによる二要素認証。メールアドレス確認のフロー。これ全部、自分で実装するのか？RFCを読んでいたあの3日間を思い出した。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。同じ結論に至った。今度は認証機能についてだ。Ory Kratosという選択肢www.ory.shgithub.comOry Kratosは「ヘッドレスID管理システム」だ。Hydraが「認証をしない認可サーバー」だったことを思い出してほしい。Hydraはプロトコル層（OAuth2/OIDC）に特化し、認証は私たちに任せた。Kratosはその「任された認証」を担当する。┌─────────────────────────────────────────────────────────────┐│                     Ory Stack                               │├────────────────────────┬────────────────────────────────────┤│      Ory Kratos        │           Ory Hydra                ││   (Identity Provider)  │      (Authorization Server)        │├────────────────────────┼────────────────────────────────────┤│ - ユーザー登録         │ - OAuth2/OIDC                      ││ - ログイン認証         │ - トークン発行                     ││ - MFA (TOTP, WebAuthn) │ - クライアント管理                 ││ - パスワードリセット   │ - Consent管理                      ││ - プロフィール管理     │ - セッション管理                   ││ - メール確認           │                                    │└────────────────────────┴────────────────────────────────────┘つまり、これまでに私がRustで書いたAuthService——パスワード検証、ユーザー登録、セッション管理——これらをKratosに任せられる。アーキテクチャの変化これまでの構成を振り返る。【01-03の構成】┌─────────────┐     ┌─────────────────────┐     ┌─────────────┐│   Browser   │────▶│ Rust Login Provider │────▶│  Ory Hydra  ││             │     │ (自前実装)           │     │             │└─────────────┘     └─────────────────────┘     └─────────────┘                              │                              ▼                    ┌─────────────────────┐                    │ PostgreSQL (users)  │                    └─────────────────────┘私が書いたRust Login Providerは認証を担当していた。ユーザーテーブルも自前で管理していた。Kratosを導入すると、こうなる。【Kratos導入後の構成】┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐│   Browser   │────▶│  Kratos UI  │────▶│  Ory Kratos │────▶│  Ory Hydra  ││             │     │  (Node.js)  │     │             │     │             │└─────────────┘     └─────────────┘     └──────┬──────┘     └─────────────┘                                               │                                               ▼                                      ┌─────────────────────┐                                      │ PostgreSQL          │                                      │ (identities)        │                                      └─────────────────────┘私が書くコードは、ほぼゼロになる。パスワード検証、ユーザー登録、セッション管理——これまでに私がRustで実装した機能は、全てKratosが提供する。私が書くのはKratosの設定ファイルと、必要に応じたUIのカスタマイズだけだ。「それって、学習した意味がないのでは？」いや、逆だ。認証システムを自前で実装した経験は、Kratosの設定を理解する上で役立った。例えば、Kratosの設定にhashers.argon2.memory: 128MBという項目がある。自前実装の経験がなければ、その意味を理解できなかっただろう。メモリコストを上げればセキュリティは向上する。しかし同時接続数の増加でOOMのリスクも上がる——この判断ができるのは、OWASPのドキュメントを読み、自分でパラメータを選んだ経験があるからだ。「ドキュメントを読めば同じでは？」——そう思うかもしれない。確かに、ドキュメントを読めば設定はできる。しかし、障害時に「この設定が原因かもしれない」と仮説を立てられるのは、自分で同じ問題に苦しんだ経験があるからだ。ログを見て「これはセッション固定化攻撃への対策が発動した」と判断できるか。エラーメッセージから「Identity Schemaの定義が間違っている」と気づけるか。これは学習効率の問題ではなく、デバッグ能力の問題だ。これまでの実装で、認証システムの複雑さを体験した。Argon2idのパラメータ設定、ユーザー列挙攻撃への対策、セッション管理の罠。58個のテストを書いて「できないこと」を確認した。だからこそ、Kratosのありがたみが分かる。そして、Kratosで問題が起きたときに対処できる。全員が自前実装を経験すべきとは言わない。しかし、チームに1人は「中身を理解している人」がいた方がいい。Docker Composeで動かすwww.ory.com実際に動かしてみよう。services:  postgres:    image: postgres:16-alpine    environment:      POSTGRES_USER: postgres      POSTGRES_PASSWORD: secret      POSTGRES_DB: postgres    volumes:      - postgres_data:/var/lib/postgresql/data      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro  # 後述の初期化スクリプト    healthcheck:      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]      interval: 5s      timeout: 5s      retries: 5    networks:      - ory  kratos-migrate:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable    volumes:      - ./kratos:/etc/config/kratos:ro    command: migrate sql -e --yes --config /etc/config/kratos/kratos.yml    depends_on:      postgres:        condition: service_healthy    networks:      - ory  kratos:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable      LOG_LEVEL: debug      SERVE_PUBLIC_BASE_URL: http://localhost:4433      SERVE_ADMIN_BASE_URL: http://localhost:4434    volumes:      - ./kratos:/etc/config/kratos:ro    command: serve all --dev --config /etc/config/kratos/kratos.yml    ports:      - "4433:4433"  # Public API      - "4434:4434"  # Admin API    depends_on:      kratos-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4433/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  hydra-migrate:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable    command: migrate sql -e --yes    depends_on:      postgres:        condition: service_healthy    networks:      - ory  hydra:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable      SECRETS_SYSTEM: super-secret-system-secret-at-least-32-chars      URLS_SELF_ISSUER: http://localhost:4444      URLS_CONSENT: http://localhost:4455/consent      URLS_LOGIN: http://localhost:4455/login      URLS_LOGOUT: http://localhost:4455/logout      LOG_LEVEL: debug    command: serve all --dev    ports:      - "4444:4444"  # Public API      - "4445:4445"  # Admin API    depends_on:      hydra-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4444/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  kratos-ui:    image: oryd/kratos-selfservice-ui-node:v1.3.1    environment:      PORT: 4455      KRATOS_PUBLIC_URL: http://kratos:4433      KRATOS_BROWSER_URL: http://localhost:4433      HYDRA_ADMIN_URL: http://hydra:4445      COOKIE_SECRET: super-secret-cookie-secret-32chars      CSRF_COOKIE_NAME: ory_csrf_ui      CSRF_COOKIE_SECRET: super-secret-csrf-secret-32-chars    ports:      - "4455:4455"    depends_on:      kratos:        condition: service_healthy      hydra:        condition: service_healthy    networks:      - oryvolumes:  postgres_data:networks:  ory:注意: 上記の設定は開発環境用です。本番環境ではSECRETS_SYSTEMやCOOKIE_SECRETに32文字以上の暗号学的に安全な値を設定してください。サービスが6つある。PostgreSQL、KratosとHydraそれぞれのmigrate/serveサービス、そしてKratos UI。以前の自前実装（auth-provider）はKratosに置き換わった。ポイントはkratos-uiだ。これはOry公式が提供するセルフサービスUI。ログイン画面、登録画面、パスワードリセット画面などが含まれている。「自分でUI書かなくていいの？」開発環境ではこれで十分だ。本番環境では、このUIを参考に自前のUIを実装できる。Kratosの「ヘッドレス」設計により、UIは完全に切り離されている。github.comKratos設定ファイルの解説Kratosの設定ファイルkratos.ymlを見てみよう。version: v1.3.1dsn: memoryserve:  public:    base_url: http://localhost:4433/    cors:      enabled: true      allowed_origins:        - http://localhost:4455  admin:    base_url: http://localhost:4434/selfservice:  default_browser_return_url: http://localhost:4455/  allowed_return_urls:    - http://localhost:4455    - http://localhost:4444  methods:    password:      enabled: true    totp:      enabled: true      config:        issuer: OryKratosVerification    lookup_secret:      enabled: true    link:      enabled: true    code:      enabled: true  flows:    error:      ui_url: http://localhost:4455/error    settings:      ui_url: http://localhost:4455/settings      privileged_session_max_age: 15m    recovery:      enabled: true      ui_url: http://localhost:4455/recovery      use: code    verification:      enabled: true      ui_url: http://localhost:4455/verification      use: code      after:        default_browser_return_url: http://localhost:4455/    logout:      after:        default_browser_return_url: http://localhost:4455/login    login:      ui_url: http://localhost:4455/login      lifespan: 10m    registration:      lifespan: 10m      ui_url: http://localhost:4455/registration      after:        password:          hooks:            - hook: sessionlog:  level: debug  format: text  leak_sensitive_values: truesecrets:  cookie:    - super-secret-cookie-secret-32chars  cipher:    - super-secret-cipher-key-32-charsciphers:  algorithm: xchacha20-poly1305hashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16identity:  default_schema_id: default  schemas:    - id: default      url: file:///etc/config/kratos/identity.schema.jsoncourier:  smtp:    connection_uri: smtps://test:test@mailslurper:1025/?skip_ssl_verify=trueoauth2_provider:  url: http://hydra:4445セルフサービスフローselfservice:  methods:    password:      enabled: true    totp:      enabled: true以前、私がRustで実装したパスワード認証。Kratosではpassword: enabled: trueの一行で有効になる。TOTPも同様だ。以前は「MFA対応の予定は？」という質問に答えられなかった。Kratosなら設定1つで有効化できる。パスワードハッシュhashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16以前、私はArgon2::default()を使った。Kratosも同じArgon2を使っている。設定値を明示的に指定することで、チーム内で「なぜこのパラメータか」を共有できる。cheatsheetseries.owasp.orgHydra連携oauth2_provider:  url: http://hydra:4445これが最も重要な設定だ。KratosがHydraのAdmin APIに接続し、login_challengeを処理する。以前は私がRustでHydraClientを実装し、accept_loginを呼び出していた。Kratosはこれを自動で行う。https://www.ory.com/docs/kratos/self-hosted/hydra-integrationwww.ory.comIdentity Schemaの設計Kratosはユーザー情報を「Identity」として管理する。その構造はJSON Schemaで定義する。{  "$id": "https://schemas.ory.sh/presets/kratos/identity.email.schema.json",  "$schema": "http://json-schema.org/draft-07/schema#",  "title": "Person",  "type": "object",  "properties": {    "traits": {      "type": "object",      "properties": {        "email": {          "type": "string",          "format": "email",          "title": "E-Mail",          "ory.sh/kratos": {            "credentials": {              "password": {                "identifier": true              },              "totp": {                "account_name": true              }            },            "recovery": {              "via": "email"            },            "verification": {              "via": "email"            }          }        },        "name": {          "type": "object",          "properties": {            "first": {              "title": "First Name",              "type": "string"            },            "last": {              "title": "Last Name",              "type": "string"            }          }        }      },      "required": ["email"],      "additionalProperties": false    }  }}ory.sh/kratosという拡張プロパティが特徴的だ。credentials.password.identifier: true — このフィールドがログインIDになるrecovery.via: email — パスワードリセットはこのメールアドレスに送信されるverification.via: email — メール確認もこのアドレスに送信される以前、私はユーザーテーブルを自前で設計した。Kratosではスキーマを宣言的に定義するだけでいい。www.ory.com実際にハマったことでも、最初のdocker compose upは失敗した。データベースが存在しないFATAL: database "kratos" does not exist (SQLSTATE 3D000)KratosとHydraはそれぞれkratosとhydraという名前のデータベースを期待する。でも、PostgreSQLコンテナはpostgresデータベースしか作らない。解決策は初期化スクリプトだ。-- init.sqlCREATE DATABASE kratos;CREATE DATABASE hydra;# docker-compose.ymlpostgres:  volumes:    - ./init.sql:/docker-entrypoint-initdb.d/init.sql:roPostgreSQLは/docker-entrypoint-initdb.d/にあるSQLファイルを起動時に実行する。これで両方のデータベースが作成される。最初は「なぜ自動で作ってくれないんだ」と思った。おそらく、本番環境では既存のデータベースサーバーに接続することが多いからだろう。いずれにせよ、初期化スクリプトで解決できる。ポート競合Bind for 0.0.0.0:4444 failed: port is already allocated以前の記事で作ったory-hydra-rust環境がまだ動いていた。同じポート4444を使おうとして衝突。# 他の環境を停止cd ../ory-hydra-rust && docker compose down複数のOry環境を並行して動かす時は、ポートを変える必要がある。開発環境では素直に片方を停止した方がいい。Kratosが教えてくれた盲点E2EテストでTestPassword123!というパスワードを使おうとした。{  "id": 4000034,  "text": "The password has been found in data breaches and must no longer be used.",  "context": {    "breaches": 3330  }}KratosはデフォルトでHave I Been PwnedのAPIを使い、パスワードが過去のデータ漏洩に含まれていないかチェックする。TestPassword123!は3,330件の漏洩で見つかっていた。haveibeenpwned.comなぜ私は思いつかなかったのか。振り返ると、私の58個のテストは「攻撃者がシステムに対して行う操作」をテストしていた。間違ったパスワードでログインできないこと存在しないユーザーで情報が漏れないこと同時登録で競合状態が起きないことこれは全て「システムへの攻撃」に対するテストだ。攻撃者がシステムの外側から突破を試みるシナリオ。HIBPチェックは視点が異なる。「ユーザーが持ち込むリスク」に対処している。ユーザーが「password123」を使おうとするユーザーが他のサービスで使い回しているパスワードを登録するユーザーが過去に漏洩したパスワードを選ぶこれはシステムへの攻撃ではない。ユーザー自身がリスクを持ち込むシナリオだ。私はこのカテゴリを完全に見落としていた。なぜ見落としたのか。おそらく「ユーザーは正しく行動する」という暗黙の前提があった。パスワード強度のバリデーション（8文字以上、英数字混合など）を入れれば十分だと思っていた。でも、TestPassword123!は典型的な強度バリデーションを通過する。英大文字、英小文字、数字、記号、8文字以上。全ての条件を満たしている。にもかかわらず、3,330件の漏洩で見つかっている。強度バリデーションは「推測しやすいか」をチェックする。HIBPチェックは「既に漏洩しているか」をチェックする。両者は補完関係にある。Kratosを使うことで、私が想定していなかった脅威カテゴリまでカバーできる。これが「専門家が作ったツールを使う」ことの価値だ。自分の盲点を、他者の知見で補える。E2Eテストではタイムスタンプを含むランダムなパスワードを生成して回避した。TEST_PASSWORD="Kratos$(date +%s)E2E!Xk9#mN"本番環境では、この機能を有効にしたまま運用すべきだ。ユーザーに「このパスワードは漏洩しています」と伝えることで、アカウント乗っ取りのリスクを下げられる。環境の起動と動作確認初期化スクリプトを追加した状態で起動する。docker compose up -ddocker compose logs -fヘルスチェック用エンドポイントにアクセスしてみる。# Kratosのヘルスチェックcurl http://localhost:4433/health/ready# {"status":"ok"}# Hydraのヘルスチェックcurl http://localhost:4444/health/ready# {"status":"ok"}両方ともokが返ってきた。セルフサービスフローの確認ブラウザでhttp://localhost:4455/registrationにアクセスする。登録画面が表示される。メールアドレスとパスワードを入力して登録。次にhttp://localhost:4455/loginにアクセス。ログイン画面が表示される。先ほど登録した認証情報でログイン。ログイン成功。これだけだ。拍子抜けするほど簡単だった。以前、私は以下を実装した。AuthService::register() — ユーザー登録AuthService::authenticate() — パスワード検証login_page() — ログインフォームのHTMLlogin_submit() — フォーム送信処理58個のテストKratosでは、設定ファイルを書くだけでこれらが全て動く。OAuth2フローの確認OAuth2クライアントを作成する。docker compose exec hydra hydra create oauth2-client \  --endpoint http://localhost:4445 \  --grant-type authorization_code \  --response-type code \  --scope openid,profile,email \  --redirect-uri http://localhost:8080/callback \  --name "Test Client"クライアントIDとシークレットが出力される。ブラウザで認可エンドポイントにアクセスする。http://localhost:4444/oauth2/auth?client_id=<CLIENT_ID>&response_type=code&scope=openid+profile+email&redirect_uri=http://localhost:8080/callback&state=test-stateHydraがKratos UIにリダイレクトKratos UIがログイン画面を表示ログイン成功後、Kratosがlogin_challengeをHydraに送信HydraがConsent画面にリダイレクトConsent承認後、認可コードがコールバックURLに返される以前、私がRustで実装したlogin_submit()の処理を、Kratosが自動で行っている。// 前回の実装（不要になった）pub async fn login_submit(    State(state): State<AppState>,    Form(form): Form<LoginForm>,) -> Result<Redirect, AppError> {    let user = state.auth.authenticate(&form.email, &form.password).await?;    let completed = state.hydra        .accept_login(&form.login_challenge, &user.id.to_string(), false)        .await?;    Ok(Redirect::to(&completed.redirect_to))}このコードは、もう書く必要がない。E2Eテストで確認したこと実際にAPIを叩いて、フロー全体が動くことを確認した。Registration Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/registration/api"# Flow ID: 77ff9653-ccd2-4f91-aeea-8fbb4d67fce7# 2. 登録を実行curl -s -X POST "http://localhost:4433/self-service/registration?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "password": "Kratos1767517527E2E!Xk9#mN",    "traits": {      "email": "e2etest@example.com",      "name": { "first": "E2E", "last": "Test" }    }  }'Registration successful!Identity ID: 169e0834-4b45-441f-95f8-5adc45d8a3e9Email: e2etest-1767517527@example.comSession Token: ory_st_WugR5gisST7SO...Kratosのセルフサービスフローは2段階構成だ。まずフローを初期化してFlow IDを取得し、そのIDを使ってデータを送信する。これにより、CSRFトークンやフローの有効期限が管理される。Login Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/login/api"# 2. ログインを実行curl -s -X POST "http://localhost:4433/self-service/login?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "identifier": "e2etest@example.com",    "password": "Kratos1767517527E2E!Xk9#mN"  }'Login successful!Session ID: 8b97d548-8436-48ee-b4fd-8e1c643dac04Session Token: ory_st_ty15oU5JLIABh...Session Verificationcurl -s -X GET "http://localhost:4433/sessions/whoami" \  -H "Authorization: Bearer $SESSION_TOKEN"Session valid!Identity: e2etest-1767517527@example.comActive: trueセッショントークンを使って/sessions/whoamiを呼ぶと、現在のセッション情報が返ってくる。これは以前私がRustで実装したJwtService::verify()に相当する機能だ。OAuth2 Authorization Flow# OAuth2クライアントを作成curl -s -X POST "http://localhost:4445/admin/clients" \  -H "Content-Type: application/json" \  -d '{    "client_id": "e2e-test-client",    "client_secret": "e2e-test-secret",    "grant_types": ["authorization_code"],    "response_types": ["code"],    "scope": "openid profile email",    "redirect_uris": ["http://localhost:8080/callback"]  }'認可エンドポイントにアクセスすると、HydraがKratos UIにリダイレクトする。http://localhost:4444/oauth2/auth?client_id=e2e-test-client&...  ↓http://localhost:4455/login?login_challenge=Xv84rhGlXQQrVNL7SlICdNobNbYvcK7z8il...login_challengeパラメータが付与されている。Kratos UIはこのチャレンジを使ってHydraと連携し、認証完了後に適切なリダイレクトを行う。E2Eテスト結果サマリー テスト項目  結果  Registration Flow  成功  Login Flow  成功  Session Verification  成功  OAuth2 Client Setup  成功  OAuth2 Authorization Flow  成功（login_challenge生成確認） 全てのフローが期待通りに動作した。以前の自前実装と比較して、コード量はゼロになり、機能は増えた。自前実装との比較 観点  自前実装（02）  Kratos  パスワード認証  Argon2id実装  組み込み  MFA  未実装  TOTP, WebAuthn対応  パスワードリセット  未実装  フロー組み込み  メール確認  未実装  フロー組み込み  ソーシャルログイン  未実装  OIDC対応  漏洩パスワードチェック  未実装  HIBP連携  ログイン画面  HTML手書き  公式UI or 自前  セキュリティテスト  58個書いた  Oryが検証済み  学習コスト  Rust知識  Kratos設定  カスタマイズ性  完全自由  スキーマ/フック 特筆すべきは漏洩パスワードチェックだ。Have I Been Pwnedとの連携により、過去のデータ漏洩で流出したパスワードを拒否できる。これは以前書いた58個のテストでも考慮していなかった観点だ。Kratosを使うことで、私が思いつかなかったセキュリティ対策まで自動的に適用される。自前実装は無駄だったのか？いや、違う。以前の実装で学んだこと——Argon2idのパラメータ、ユーザー列挙攻撃への対策、タイミング攻撃の考慮——これらはKratosの設定を理解する上で役立った。「なぜこの設定があるのか」が分かるのは、自分で実装した経験があるからだ。いつKratosを使うべきかKratosを選ぶかどうかは、3つの軸で判断する。技術的要件: カスタマイズの複雑さはどの程度か。Kratosはフック機構やIdentity Schemaで柔軟性を提供するが、「3回目のログインでは必ずCAPTCHAを表示」のような独自フローは難しい。標準的な認証フローなら、Kratosで十分だ。組織的要件: チームにセキュリティ専門家がいるか。いないなら、Kratosに任せた方がいい。脆弱性対応、ベストプラクティスの追従——これらを自前でやるには専門性が必要だ。SOC2やISO27001の監査でも「専門企業の製品を使っています」と答えられる。ビジネス要件: 認証がコア価値か否か。パスワードマネージャーや認証SaaSなら、自前実装に意味がある。ECサイトや社内ツールなら、認証に時間をかけるより本業に集中すべきだ。私がこれまで関わってきたプロジェクトの8割は、最初からKratosで良かった。残り2割は、レガシーシステムとの統合が複雑すぎるか、認証自体がプロダクトの価値だった。今回のケースでは、学習目的で自前実装から始めたが、本番プロジェクトなら最初からKratosを選ぶ。認証に独自性は不要で、チームにセキュリティ専門家もいない——判断は明確だ。次回予告Kratosを導入したことで、認証（Authentication）は解決した。ユーザーはログインできる。セッションも管理される。でも、ログインしたユーザーが「何をできるか」は、まだ決まっていない。認証と認可は別物だ。認証は「誰であるか」を確認する。認可は「何ができるか」を判断する。次回は、Ory Ketoを使ってZanzibarモデルによる認可システムを構築する。おわりに正直に言うと、Kratosの設定を書いている時、何度か「自分で実装した方が分かりやすいのでは」と思った。YAMLの設定項目が多い。ドキュメントを何度も読み返した。でも、動いた時の感覚が違う。これまでに私が書いた数百行のRustコード。それがYAML数十行で置き換わった。しかも、MFAやパスワードリセットなど、私が「次回以降に実装する」と書いていた機能が、既に含まれている。「自前で作ることの非合理性」第1回で書いた言葉を思い出した。認可サーバーだけでなく、認証システムも同じだった。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。Kratosに移行しても、設定の検証やアップグレード対応、障害時の判断は残る。責任が消えるのではなく、「実装の責任」から「選定と運用の責任」に形を変える。その上で、認証の基本的な部分——パスワード認証、MFA、セッション管理——は、毎回ゼロから考える問題ではなくなった。そして、もう1つ学んだことがある。Have I Been Pwnedの件だ。私は58個のテストを書いて「完璧だ」と思っていた。でも、「ユーザーが持ち込むリスク」という視点が完全に抜けていた。専門家が作ったツールを使う価値は、自分の盲点を補えることにある。レビューコメントに返信しよう。「パスワードリセット機能は？」——Kratosで対応します。この記事が参考になれば、読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料Ory KratosOry Kratos GitHubOry Kratos DocumentationKratos QuickstartIdentity SchemaHydra IntegrationOry HydraOry Hydra DocumentationLogin and Consent FlowセキュリティガイドラインOWASP Password Storage Cheat SheetOWASP Authentication Cheat Sheet検証環境ory-kratos-verification（GitHub）]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Error ReportingとCloud Runでアプリエラーをいい感じにグループ化してGitHubイシューにする]]></title>
            <link>https://zenn.dev/kimitsu/articles/report-error-to-github</link>
            <guid isPermaLink="false">https://zenn.dev/kimitsu/articles/report-error-to-github</guid>
            <pubDate>Mon, 12 Jan 2026 02:52:23 GMT</pubDate>
            <content:encoded><![CDATA[Error Reporting とは皆さん、Google Cloud の Error Reporting はご存知でしょうか。あまり知られていないのではないかなと思っています。アプリからは日々エラーが出ておりエンジニアはそれに対応する必要がありますが、エラーというものは同じ原因で複数回出るものです。ログを眺めていて同じようなエラーがたくさん並んでいてもあまり情報は増えません。Error Reporting はアプリケーションのエラーログを収集し、同じ原因のエラーをグループ化してくれるサービスです。[1]例えば以下の例では同じエラーが 3 回出ていますが、Error Rep...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、辞めないなら頑張れ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/12/003013</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/12/003013</guid>
            <pubDate>Sun, 11 Jan 2026 15:30:13 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先週、「おい、辞めるな」という記事を書きました。syu-m-5151.hatenablog.com思った以上に反響がありました。何人かから連絡をもらいました。辞めないことにしました、考えるきっかけになりました、と。ありがたかったです。嬉しかった、と言っていいです。たぶん。ただ、何か落ち着きませんでした。辞めないと決めた。それは分かった。で、その次は。辞めないと決めただけで、何かが変わるわけではありません。私がそうだったからです。辞めないと決めた後も、何も変わりませんでした。評価は上がらない。漠然としたモヤモヤは消えない。夜遅くまでコードを書いた。勉強会に参加した。資格を取った。ブログを書いた。技術力を上げれば認められる。そう信じていました。評価は上がりませんでした。振り返ると、私は頑張り方を間違えていたのです。もっと正確に言えば、評価の構造を理解していませんでした。良い仕事をすれば評価される。そう思っていました。でも、評価者には評価者の論理があります。組織には組織の論理があります。その構造を理解せずに、がむしゃらに頑張っても、報われません。「おい、辞めるな」の最後に、「選んだ道を、正解にしていく過程があるだけだ」と書きました。辞めないと決めた。その選択を正解にするために、何をすればいいのか。この文章は、それを書くために開きました。ただ、書きながらも思います。これが誰かの役に立つのかは、分かりません。分からないまま、書いています。先に断っておきます。この記事は、まだ頑張れる余力がある人に向けて書いています。すでに消耗している人、頑張る気力すらない人には、この記事は届かないだろう。それについては、最後に書きます。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。見えない努力まず、頑張り方を間違えている人が多いです。私もそうでした。インフラのトラブルを未然に防いだことがあります。監視アラートの傾向を見て、「これ、来週やばいことになる」と気づきました。週末に対応して、障害を防ぎました。本番で落ちていたら大騒ぎでした。サービスが止まれば、ビジネスに直接影響が出ます。ユーザーからのクレームが殺到します。深夜に全員が叩き起こされます。そういう未来を、私は未然に防ぎました。でも、月曜日、何事もなかったように仕事が始まりました。誰も何も言いませんでした。障害が起きなかったという「非イベント」は、誰の記憶にも残りません。チームの技術的負債を黙々と返済したことがあります。3ヶ月かけて、複雑なモジュールをリファクタリングしました。スパゲッティコードを解きほぐし、テストカバレッジを上げ、ドキュメントを整備しました。誰かがこの負債を返さなければ、いずれチーム全体が身動きを取れなくなります。そう思って、地道に片付けました。でも、リリース直前に「いつの間にかキレイになってた」と言われただけでした。3ヶ月の努力が、「いつの間にか」で片付けられました。いつの間にか、俺も消えていました。私は「良い仕事をしていれば、いつか評価される」と思っていました。黙々と価値を出していれば、誰かが見ている。実力で認められる。そう信じていました。甘かったです。現実はこうです。見えない仕事は、存在しないのと同じ。どんなに素晴らしい設計をしても、それを言語化して共有しなければ、誰も知りません。どんなに難しいバグを直しても、「大変だった」と伝えなければ、簡単な修正だと思われます。障害を未然に防いでも、障害が起きなかったという「非イベント」は記憶に残りません。これは不公平だと思うだろう。私もそう思いました。私は2年間嘆いていました。居酒屋で同僚と「この会社の評価制度はおかしい」と言い合ったこともあります。「なんで俺の仕事が評価されないんだ」と愚痴りました。言うたびに少し楽になりました。だけど、翌日も同じ状況が続きました。嘆きは鎮痛剤です。痛みを一時的に和らげますが、原因は治りません。この構造を理解した上で、どう振る舞うか。 それが「頑張り方」です。構造を知れここで公平を期しておきます。仕組みの問題は確かにあります。OKRの目標設定が形骸化している。評価者によって評価がブレる。数値化できない仕事が過小評価される。これは仕組みを運営する上で抱える問題です。「人は他人を正しく評価できる」——これは幻想です。同じ人の同じ仕事を見ても、評価者が違えば評価は違います。同じアウトプットで、上司が変わっただけで評価が2段階変わることもあります。私です。絶対的に客観的な評価など存在しません。そもそも、数値で測ろうとした瞬間、測定対象は変質します。コミット数を測り始めたチームを見たことがあります。結果、コミットが細切れになりました。バグ修正件数を測れば、バグを作った人が有利になります。プルリクエストの数を測れば、小さなPRを乱発する人が評価されます。グッドハートの法則と呼ばれる現象です。「指標が目標になると、その指標は機能しなくなる」。OKRを導入したとき、私はこの法則を知りませんでした。知っていたら何か変わったかと言われると、たぶん変わりませんでした。人間だから。エンジニアリングの現場では、これが顕著に現れます。これは事実です。認めましょう。その上で、自分に何ができるかを考えます。仕組みの問題を批判するのは簡単です。でも、評価制度を変えるのは難しい。上司を変えることはできません。待っていても変わりません。冒頭で書いた通り、私はこれをやっていました。仕組みの問題を指摘して溜飲を下げる。鎮痛剤を飲み続けて、原因を放置していました。仕組みがおかしいのは事実です。でも、仕組みは変えられない。自分は変えられる。 それが「頑張る」ということです。変えられないものに時間を使うほど、あなたの人生は長くありません。——と書いて、立ち止まります。「結局、自己責任論じゃないか」と言われるだろう。構造の問題を認識しながら、最後に「個人が変われ」と言っている。評価されないのは構造の問題なのに、「お前の頑張り方が悪い」と言っている。それは自己責任論の強化じゃないか、と。正直に言えば、その批判は当たっています。私は構造の問題を認識しながら、「構造を変えろ」とは言いませんでした。「構造の中でうまくやれ」と言いました。それは、構造を温存することに加担しています。これは私の限界です。私が書けるのは、私が経験したことだけです。構造を変えることに成功した人が、その方法を書いてくれることを願います。ただ、1つだけ言い訳させてください。私は「評価されないのはお前のせいだ」とは言っていません。「評価制度には限界がある」「客観的評価は存在しない」と、繰り返し書きます。その上で、「構造が変わらない中で、個人に何ができるか」を書いています。自己責任論と言われれば、そうだろう。でも、構造が変わるのを待っていても、あなたの評価は上がりません。変わらない構造の中で、今日をどう生きるか。それを考えるしかありませんでした。しかし、重要な注意点があります。仕組みの問題が大きすぎる時は、「辞める」が正解のことがあります。 個人の努力で覆せない構造もあります。それを見極める目も必要です。評価制度が必ず歪む理由評価制度が歪むのは、設計者の能力不足ではありません。測定されるものは、測定によって変質するからです。どの制度も、導入した瞬間に歪み始めます。完璧な評価制度は原理的に存在しません。この事実は、あなたを責めるためにあるのではありません。あなたを解放するためにあります。「自分が無能だから評価されない」という思い込みから解放され、「制度の限界を前提に、どう動くか」という問いに切り替わります。評価の幻想上司は、神でもエスパーでも上位存在でもありません。人間です。君よりも少しだけ観点の多い人間です。人外だと思っている上司も、人間であり、認知には限界があります。これは「上司が無能だ」という話ではありません。人間である限り、客観的評価は原理的に不可能だという話です。なぜ「客観的評価」は不可能なのか「客観的評価」という言葉には、2つの前提があります。「評価すべき対象を正確に観察できる」という前提と、「観察したものを正確に評価できる」という前提です。どちらも成り立ちません。観察の問題から見てみましょう。上司があなたの仕事のうち、何%を直接観察しているでしょうか。会議での発言。Slackでのやり取り。プルリクエスト。これは観察できます。でも、設計を考えている時間。問題を切り分けている時間。ドキュメントを読んでいる時間。これは見えません。上司が見ているのは、あなたの仕事の氷山の一角に過ぎません。観察できるものだけを見て、全体を評価している。これは観察者の怠慢ではありません。構造的に避けられない限界です。比較の問題もあります。評価とは、本質的に比較です。Aさんは「期待以上」、Bさんは「期待通り」。この判断をするには、AさんとBさんを比較する必要があります。でも、2人の仕事が違えば、比較は困難になります。バックエンドで高負荷対策をしたAさんと、フロントエンドで複雑なUIを実装したBさん。どちらが「より価値がある」か。答えはありません。比較不可能なものを比較しています。上司は無理やり比較し、順位をつけます。その順位に「客観性」などありません。観察者効果という問題もあります。観察すること自体が観察対象に影響を与えるというものです。「評価される」と意識した瞬間、行動が変わります。評価されやすい仕事を選ぶ。見える形で成果を出そうとする。「本当の仕事ぶり」を観察しているわけではありません。「評価を意識した仕事ぶり」を観察しています。上司の認知バイアス観察の限界に加えて、観察したものを処理する段階でもバイアスがかかります。直近バイアス: 1年間を均等に覚えていません。評価直前の出来事が記憶に残ります。4月に素晴らしい仕事をしても、12月の評価面談では薄れています。11月に目立つ失敗をすると、それが印象を決めます。ハロー効果: 1つの良い印象が全体評価を引き上げます。1つの失敗が全体を引き下げます。障害対応で活躍すると、「この人は優秀だ」と思われます。その印象が、関係のない能力の評価にも影響します。確証バイアス: 一度「優秀」と思うと優秀な証拠ばかり目に入ります。「ダメ」も同様です。最初の印象が固定され、それを覆す情報は無視されます。これは上司の能力不足ではありません。人間の認知システムに組み込まれた特性です。どんなに優秀な上司でも、これらのバイアスから完全に逃れることはできません。ここで、1つ確認しておきたいです。「自分は正しく評価されていない」と感じたことがあるでしょうか。もしあるなら、それは被害妄想ではありません。構造的に、完全に正しい評価など存在しません。上司がどんなに優秀でも、認知バイアスからは逃れられません。あなたの感覚は、間違っていません。評価基準自体が「客観的」ではないより根本的な問題があります。評価基準自体が客観的ではないのです。「技術力」「コミュニケーション力」「リーダーシップ」——評価シートに並ぶこれらの言葉は、一見客観的に見えます。でも、その定義は人によって違います。「技術力が高い」とは何か。コードの品質が高いこと？難しい問題を解決できること？新しい技術をキャッチアップするのが速いこと？幅広い技術に詳しいこと？上司によって、重視する側面が違います。つまり、評価基準そのものが社会的に構成されたものです。「何を価値とするか」は、文化、組織、時代によって変わります。普遍的な基準などありません。私は異動で気づきました。前のチームでは「期待以上」と評価されていました。技術的な深さを評価してくれる上司でした。異動した先では「成長途上」と評価されました。新しい上司はチームへの影響力を重視していました。スキルは変わっていません。変わったのは上司です。「客観的評価」を求めるより、やるべきこと評価とは「私が何をしたか」ではなく「上司が私をどう見るか」です。この事実を受け入れると、行動が変わります。「客観的に見れば、私は評価されるべきだ」という主張は意味がありません。客観的な視点など存在しないからです。存在するのは、上司の視点だけです。だから、「客観的評価」を求めるのはやめました。代わりに、上司が何を見ているかを理解することに注力しました。上司は何を重視するか。何に反応するか。何を見落としているか。それを理解した上で、上司に伝わる形で成果を見せます。これは媚びを売ることとは違います。上司の視界に入る努力をしているだけです。上司の視点を理解するには、いくつかの問いを考えるといいです。「この上司は何を『良い仕事』だと思っているか」「この上司は何にストレスを感じているか」「この上司は、上からどんなプレッシャーを受けているか」。これらを理解すると、上司が何を見て、何を見落としているかが見えてきます。組織の論理評価制度と評価者の心理を理解したら、次は組織の論理を理解しましょう。組織には、個人の論理とは異なる、独自の論理があります。この論理を理解しないと、「なぜ評価されないのか」が分からないままになります。組織は「最適化」で動く組織は、個人の幸福を最大化するために存在しているわけではありません。組織の存続と成長を最適化するために存在しています。この当たり前の事実を、意外と多くの人が忘れています。評価制度も、昇進制度も、給与制度も、すべて「組織の最適化」のために設計されています。「個人が納得するか」は、二次的な目標に過ぎません。もちろん、個人が納得しなければ離職が増えるから、ある程度は配慮されます。でも、最優先ではありません。だから、「公平な評価」を期待すると、裏切られます。組織が目指しているのは公平な評価ではなく、組織にとって都合の良い行動を引き出す評価だからです。昇進はゼロサムゲームである昇進枠は有限です。誰かが昇進すれば、誰かは昇進しません。「今期は枠がなかった」と言われたことがある人もいるでしょう。それは、あなたの実力の問題ではなく、構造の問題です。予算も同じです。パイの大きさは決まっています。問題は、パイをどう切り分けるかです。だから、昇給交渉は「自分の価値を証明する」だけでは不十分です。「なぜ自分に配分を増やすべきか」を説明する必要があります。自分に正直に向き合ってください。あなたが昇進することで、上司やチームにはどんな具体的なメリットがあるか。「この人を昇進させると、〇〇という効果があります」と言える材料を、あなた自身が上司に渡してください。政治は「資源配分の闘争」である「誰を昇進させるか」は、技術力だけで決まりません。上司と上司の上司の関係。部門間の力学。人事部の意向。様々な要素が絡み合います。「政治なんて関係ない」と思いたい気持ちは分かります。技術力で勝負したい。でも、組織で働く以上、政治は存在します。政治とは何か。限られた資源を巡る配分の闘争です。資源とは、予算、人員、プロジェクト、昇進枠、注目、発言力。これは有限です。誰かが得れば、誰かが失います。この配分を決めるプロセスが、政治です。政治を「汚いもの」と見なすのは、的外れです。資源が有限である限り、配分のプロセスは必ず存在します。それを「政治」と呼ぼうが呼ぶまいが、現象は消えません。政治を無視しても、政治はあなたに影響します。あなたが政治を無視しても、他の誰かが政治を使って資源を獲得すれば、あなたに回る資源は減ります。だから、政治を理解した上で動いた方がいいです。しかし、誤解しないでください。「政治を理解してください」は「政治に加担してください」という意味ではありません。「政治ゲームの名プレイヤーになってください」とも言っていません。政治の存在を認識し、その中で自分がどう動くかを考えるということです。組織の論理と個人の論理は違うここまでの話をまとめると、こうなります。組織は「組織の最適化」で動きます。個人の最適化ではありません昇進枠は有限です。ゼロサムゲームです予算は配分の問題です。パイの切り分けです政治は資源配分の闘争です。避けられませんこれらを理解すると、「なぜ評価されないのか」の見え方が変わります。「自分は良い仕事をしている」は、個人の論理です。組織の論理から見ると、「良い仕事をしている人」は他にもいます。問題は、有限の資源を誰に配分するかです。だから、「良い仕事をすれば評価される」は半分しか正しくありません。正確には、「良い仕事をした上で、資源を配分すべき理由を説明できれば評価される」です。ここまで読んで、息苦しくなっただろう。評価制度には限界があります。客観的評価は存在しません。組織は個人の幸福を最大化しません。昇進はゼロサムゲームです。政治は避けられません。厳しい現実です。でも、現実を知ることは、現実に絶望することではありません。構造を知らなければ、暗闘の中で闘っているようなものです。構造を知れば、どこに光があるか見えます。ここからは、その光に向かって動く方法を書きます。やるべきことは、大きく3つあります。「どこを見るか」を変えること、「対話」を通じて認識を揃えること、「見せる」ことで存在を証明することです。チームを見ろ組織の論理を理解したら、次は「どこを見るか」を変えることです。「どの会社で働くか」が大事だと思われています。でも、本当に大事なのは「どのチームで働くか」です。従業員が「ここで働くのをやめよう」と決める時、この「ここ」は会社ではありません。チームです。会社は好きだがチームが合わなくて異動する人がいます。逆に、会社の方針には疑問があるがチームが良くて残る人もいます。これは新卒就職活動をされている方や、転職を考えている方に特に知っておいてもらいたいことです。企業文化が良い会社でも、自分が配属されるチームの雰囲気が良いとは限りません。評価も同じです。「この会社の評価制度」より、「直属の上司の評価パターン」の方が、あなたの評価に直接影響します。会社の評価制度がどれだけ整っていても、その制度を運用するのは上司です。上司が制度を正しく運用しなければ、制度の意味はありません。逆も同じです。評価制度が多少おかしくても、上司が良ければ、適切に評価される可能性があります。だから、転職先を選ぶときも、残るか辞めるかを判断するときも、「会社」という抽象的な単位で考えないでください。どのチームに入るか。誰が上司になるか。 その具体的な単位で考えてください。対話しろ嘆きは鎮痛剤だと書きました。では、対話は何か。対話は手術です。痛いし、面倒だし、時間がかかります。でも、原因を取り除ける可能性があります。対話が必要な理由は単純です。あなたと上司は、別の人間だからです。別の経験を持ち、別の価値観を持ち、別の情報を持っています。この情報の非対称性を埋める方法は、対話しかありません。見えている世界の違いを理解する上司と話が通じないとき、「上司が悪い」と思いがちです。でも違います。部下と上司では見えている世界が違います。自分から見ると理不尽な判断でも、上司の立場から見ると合理的なことがあります。上司には上司のプレッシャーがあります。部門の目標があります。上からの期待があります。その世界の中で、上司は合理的に動いています。その上で話せないことがあります。これは「上司の判断を正当化してください」という話ではありません。上司の判断が間違っていることもあります。でも、その判断がどこから来ているかを理解しなければ、対話はできません。対話とは、この世界の違いを認識した上で、共通の理解を構築する作業です。自分の世界だけで考えると「なんで分かってくれないんだ」となります。でも、上司の世界に立ってみると「なるほど、だからそう判断するのか」と見えてきます。見えてくれば、「では、この点はどうですか」と別の角度から提案できます。上からの視点と現場の視点上司と部下では、見ている方向が違います。上司は上から降りてくる方針を見ています。目標、KPI、ロードマップ。経営が何を求めているか。一方、現場は下を見ています。実際に何が起きているか。どこに問題があるか。この2つが噛み合っていないと、話が通じません。「上が何を考えているか分からない」「現場の声が届かない」——どちらも、この断絶の症状です。対話は、この2つをつなぐ作業です。上司と話すとき、上司が見ている方向を理解しようとします。同時に、現場のリアリティを言語化して伝えます。その接点を見つけることが、対話の目的です。ここで具体的なアクションがあります。上司が今、上の階層から課されている「最も頭の痛い課題」を把握してください。上司も誰かの部下です。上司にも上司がいます。その上司から何を求められているか。何に頭を抱えているか。それを知れば、あなたの仕事をどう位置づければいいか見えてきます。上司が「コスト削減」に追い詰められているなら、あなたの技術改善は「効率化」として語ってください。上司が「新規プロジェクトの立ち上げ」に追われているなら、あなたの貢献は「立ち上げを支える基盤整備」として語ってください。上司の頭痛の種を知れば、あなたの仕事の見せ方が変わります。対話を自分から始める「次の昇進に必要なことは何ですか」と1on1で聞きます。怖いです。否定されるでしょう。「まだ早い」と言われるでしょう。でも、聞かないと何も始まりません。自己評価と組織からの評価が食い違うとき、上司を敵だと思ってしまいがちです。「この人とは話しても仕方ない」と見限って、対話をやめます。これが最悪のパターンです。一度「敵」だと思うと、何を見ても敵の証拠に見えます。中立的な発言も「やっぱり敵だ」と解釈します。相手もそれを感じ取り、本当に敵対してきます。悪循環にハマります。これは認知バイアスの一種で、一度形成された敵対的な認知は、自己強化していきます。対話を自分から始めてください。待っていても始まりません。対話は「同意」ではない対話の目的は、合意することではありません。理解を共有することです。対話した結果、意見が一致しないこともあります。それでいいです。重要なのは、「なぜ相手がそう考えるか」を理解することです。理解した上で、なお意見が違うなら、それは対話の失敗ではありません。「上司と対話したが、評価は変わらなかった」という結果があり得ます。それでも、対話には意味があります。「なぜ評価が変わらないのか」の理由を理解できたはずです。理由を理解すれば、次の行動を決められます。理由が「あなたのスキルが足りない」なら、スキルを伸ばす努力をします。理由が「今期は枠がない」なら、来期に向けて準備します。理由が「この上司とは価値観が合わない」なら、異動や転職を検討します。対話の目的は、情報を得ることです。同意を得ることではありません。制度が機能していないなら、自分で対話を作れ本当は、目標設定や評価制度というのは、この対話を縮減化して仕組み化したものです。「何を目指すか」「どこまでやるか」「何ができたか」を定期的にすり合わせる機会です。でも、多くの組織で、この仕組みは形骸化しています。目標設定は形だけです。評価面談は結果の通知だけです。対話が発生していません。仕組みがうまく機能していないなら、仕組みが本来やろうとしていたことを、自分で意識的にやればいいです。1on1で自分から聞きます。週次報告で自分から伝えます。仕組みに頼らず、対話を自分で作ります。基準を握れ構造を理解し、対話の重要性を理解したら、次は具体的に動きます。まず、評価基準を言語化してください。多くの人は、上司が何を基準に評価しているか、明確に理解していません。なんとなく「良い仕事をすれば評価される」と思っています。でも、上司の頭の中にある評価基準と、自分が想像している評価基準は、往々にしてズレています。1on1で聞くべき具体的な質問「昇進に必要なことは何ですか」「今の自分に足りないものは何ですか」「次の評価期間で何を達成すれば、評価が上がりますか」「あなたが重視していることは何ですか」「なぜその目標が重要なんですか」「この目標が達成されないと、何が困りますか」これらの質問を、恐れずに聞いてください。「そんなこと聞いていいの？」と思うでしょう。私もそう思っていました。こういう質問をすることに、強い抵抗がありました。正直に言えば、私を含めてエンジニアは、こういう「合意形成」をバカにしている節があります。技術力で勝負したい。政治的なことはやりたくない。上司にゴマをするみたいで嫌だ。そういう感覚があります。もう1つ、ネガティブなフィードバックを受け取りたくない、という心理もあります。「今の自分に足りないものは何ですか」と聞いて、厳しいことを言われたらどうしよう。自分が思っているより評価が低かったらどうしよう。聞かなければ、知らずに済みます。でも、聞かなければ分かりません。上司はエスパーではないし、あなたもエスパーではありません。期待値をすり合わせるには、対話するしかありません。「昇進したいです」と直接言うのは恥ずかしいです。自分の欲を見せることに抵抗があります。私は3年間言えませんでした。言い出せないまま、居酒屋で愚痴を言い、鎮痛剤を飲み続けていました。鎮痛剤の効き目が切れてきた4年目に、ようやく口を開きました。でも、上司からすれば、部下が何を求めているか分からなければ、サポートのしようがありません。期待値のすり合わせ上司が求めるものと、自分がやりたいことは、必ずしも一致しません。上司が重視するのはAだが、自分が得意なのはB。この場合、どう動くか。まず、そのギャップを言語化してください。「自分はBが得意だが、Aに注力すべきですか」と聞いてください。上司は「Aをやってほしい」と言うでしょうし、「Bで成果を出してくれればいい」と言うでしょう。どちらにせよ、ギャップを認識した上で動けます。ギャップを認識しないまま、自分の得意なBに注力して、評価面談で「Aをやってほしかったのに」と言われるのが最悪のパターンです。見せろ評価基準を理解しました。上司との期待値もすり合わせました。次は、実際に動く番です。対話は手術だと書きました。では、見せることは何か。見せることはリハビリです。地味で、継続が必要で、効果が見えるまで時間がかかります。でも、これをやらなければ、手術しても回復しません。冒頭で書きました。見えない仕事は、存在しないのと同じだと。障害を未然に防いでも、誰も気づきません。技術的負債を返済しても、「いつの間にかキレイになってた」で終わります。これは不公平です。でも、嘆いても変わりません。変えられるのは、自分の行動だけです。だから、見せてください。何をやっているか、どんな価値を生んでいるか、言葉にして伝えてください。なぜ「見せる」ことが必要なのか「良い仕事をしていれば、見てもらえるはずだ」——これは幻想です。上司の注意は有限です。注意は希少資源です。上司は複数の部下を持っています。自分の仕事もあります。上からのプレッシャーもあります。その中で、あなたの仕事に割ける注意は、ごくわずかです。あなたが黙って良い仕事をしていても、上司の注意はあなたに向きません。問題を起こす部下、声の大きい部下、頻繁に報告してくる部下に注意が向きます。注意を向けてもらえなければ、あなたの仕事は認識されません。認識されなければ、評価されません。これは「目立ったもの勝ち」という話ではありません。情報の非対称性の話です。あなたは自分の仕事を100%知っています。上司は、あなたの仕事の10%も見ていません。この情報ギャップを埋めるのは、あなたの責任です。上司が勝手に気づいてくれることを期待するのは、非現実的です。人が本当に求めているのは、実はフィードバックではありません。「注目」です。自分の仕事を見てもらっている。気にかけてもらっている。存在を認識されている。そういう感覚です。私自身、厳しいフィードバックより、上司が自分の仕事を把握していないことの方が堪えました。評価されないと感じるとき、本当の問題は「評価が低い」ことではなく「注目されていない」ことでしょう。上司は、あなたが何をしているか知りません。知らなければ、評価以前の問題です。「見せる」ことへの抵抗多くのエンジニアは、「見せる」ことに抵抗がある。「アピールは卑しい」という感覚がある。日本の文化では、自己主張は美徳ではない。「黙って結果を出す」が美しいとされる。自分の成果を語ることは、自慢に見える。謙虚さが失われる。そういう感覚がある。でも、この感覚は、情報の非対称性を無視している。あなたが黙っていれば、上司はあなたの仕事を知らない。知らなければ、評価できない。「黙って結果を出す」は、「結果を出しても評価されない」と同義だ。「仕事の質で勝負したい」という信念もある。アピールの上手さではなく、仕事の質で評価されたい。それは正しい感覚だ。でも、仕事の質を上司に伝えるのは、アピールではない。情報提供だ。上司は、あなたの仕事の質を判断する材料を持っていない。その材料を提供するのは、あなたの役目だ。具体的な言い方週次報告での言い方ダメな例:「今週はAの修正をしました。」良い例:「今週はAの修正をしました。このバグは再現条件が複雑で、ログから特定するのに2日かかりました。原因は○○で、同様の問題が他に3箇所あったので併せて修正しています。」違いは、「何が難しかったか」「どう判断したか」「影響範囲をどう考えたか」を言語化していること。Slack、1on1、どの場面でも同じ原則です。言語化はスキルだアピールが苦手？ なら、存在しないのと同じだ。「自慢みたいで嫌だ」と思うだろう。私もそうだった。でも、これは自慢ではない。自分の仕事の価値を言語化しているだけだ。言語化しなければ、他人には見えない。見えなければ、評価されない。言語化は、スキルだ。最初は苦手でも、練習すれば上達する。週次報告を書くたびに、「何が難しかったか」を1文追加する。それだけで、見え方が変わる。タイミングを狙え「見せる」にも戦略がある。上司の認知の限界を理解することが重要だ。なぜタイミングが重要なのか評価面談の席で、上司は1年間を振り返る。でも、1年間を均等に思い出すことは、人間には無理だ。上司も人間だ。人間の記憶には癖がある。最初の方と最後の方は覚えているが、中間は忘れやすい。期初に立てた目標は覚えている。期末の追い込みも覚えている。でも、中間の地道な仕事は埋もれる。より厄介なのが、最近の出来事ほど重要に感じられる傾向だ。4月に素晴らしい仕事をしても、12月の評価面談では遠い記憶だ。「そういえば、何かやってくれた気がするな」程度の印象しか残らない。一方、11月に目立つ成果を出せば、12月の評価面談では鮮明に覚えている。もう1つ、人間は経験全体を平均的に評価しない。最も印象的だった瞬間と、終わりの印象で全体を判断する。1年間コツコツ働いても、期末に目立つ成果がなければ、「今期は普通だったな」という印象になりやすい。逆に、期末に大きな成果を出せば、「今期は頑張っていたな」という印象が残る。これは上司の能力不足ではない。人間の脳の仕組みだ。批判しても変わらない。構造を理解した上での3つの戦略この認知の限界を理解した上で、どう動くか。1. 評価の2ヶ月前に目立つ成果を出す大きなリリースのタイミングを調整可能なら、評価期間の後半に持ってくる。調整できなくても、過去の成果の効果を後半に言語化し直すことはできる。「4月にリリースした機能が、この半年でこれだけの効果を出しました」と。成果を「過去のイベント」ではなく「現在も続いている効果」として再提示する。2. 月次で「今月やったこと」を共有する上司の記憶を定期的に上書きする。年末に慌てて振り返るのではなく、毎月、記録を残しておく。これは上司のためだけではない。自分のためでもある。1年前に何をやったか、自分でも忘れる。月次の記録があれば、評価面談の準備が楽になる。もう1つ重要なことがある。「ピーク」がない期間の地味な貢献を、上司が「思い出しやすいエピソード」として毎月ストックしているか。「今月は特に目立った成果はありませんでした」で終わらせるな。地味な仕事でも、言語化すれば印象に残る。「依存ライブラリのアップデートで、セキュリティリスクを2件潰しました」。これだけで、「あの人は地道にやってくれている」という印象が積み上がる。3. 印象に残る瞬間を意識的に作る人間は、最も印象的だった瞬間で全体を判断する。これを逆手に取る。難しい問題を解決した。障害対応で活躍した。これらの「ピーク」は記憶に残りやすい。ピークがあれば、平凡な日々も「あの人は活躍していた」という印象に変換される。「ズルい」という感覚について「タイミングを調整するなんてズルい」と思うでしょう。仕事の質で評価されるべきです。タイミングを操作するのは、本質的ではありません。でも、考えてみてほしい。あなたがタイミングを意識しなくても、他の誰かは意識しています。評価期間の後半に目立つ成果を出す人。月次報告を欠かさない人。彼らは「ズルい」のではなく、「構造を理解している」だけです。タイミングを調整することは、媚びを売ることではありません。上司の認知の限界を理解した上で、情報を届けているだけです。上司が全てを均等に覚えていてくれるなら、タイミングは関係ありません。でも、上司は人間です。人間の記憶には限界があります。その限界を前提として動く方が、合理的です。この癖は、知っていれば対処できます。知らなければ、無意識のうちに損をします。評価する側もされる側も、同じ脳を持っています。上司もまた、自分の記憶の癖に気づいていないことが多いです。4. 失敗したときのリカバリーを設計しておく失敗は起きます。問題は、その失敗がハロー効果で全体評価を引きずり下ろすことです。「あの人は失敗した」という印象が、関係のない能力の評価まで下げます。これを防ぐには、失敗の直後に2つのことをやってください。まず、迅速に報告してください。隠そうとして発覚すると、「失敗した」にまた「隠そうとした」が上乗せされます。次に、原因と対策を透明に説明してください。「なぜ起きたか」「何を学んだか」「次にどう防ぐか」を言語化します。これができると、「失敗した人」ではなく「失敗から学べる人」という印象に変換されます。失敗を完全に消すことはできません。ですが、失敗の印象を上書きすることはできます。スポンサーを作れ昇進には「スポンサー」と「可視化」が必要だ。なぜスポンサーが必要なのか昇進は、だいたいあなたの知らないところで決まる。評価会議というものがある。マネージャーが集まって、誰を昇進させるか、誰に良い評価をつけるかを議論する。あなたは、その会議に出席できない。出席できないのに、そこであなたの運命が決まる。あなたの仕事ぶりを知っている人が、その会議にいなければ、あなたの名前は挙がらない。名前が挙がらなければ、昇進しない。どんなに良い仕事をしていても、その会議で誰かがあなたの名前を出さなければ、無意味だ。その「誰か」が、スポンサーだ。スポンサーとメンターの違いメンターは、アドバイスをくれる人だ。キャリアの相談に乗ってくれる。「こうした方がいいよ」「あの人に話を聞いてみたら」と教えてくれる。スポンサーは、あなたの成果を上に伝えてくれる人だ。人事評価の場で、あなたの名前を出してくれる。「あいつは良い仕事をしている」と会議で言ってくれる。この違いは決定的だ。メンターは「あなたのために」アドバイスをくれる。でも、スポンサーは「あなたのために」リスクを取る。評価会議であなたの名前を出すということは、スポンサー自身の信用を賭けることだ。「私が推薦した人」が期待外れだったら、スポンサーの評価が下がる。だから、スポンサーになってもらうのは、メンターになってもらうより難しい。メンターがいても、スポンサーがいなければ、昇進の話にはならない。あなたの良い仕事を知っている人がいても、その人が上に伝えてくれなければ、上層部はあなたを知らない。上司だけがスポンサーではない多くの場合、直属の上司が最初のスポンサー候補になる。でも、上司だけに依存するのはリスクがある。上司が異動することがある。上司が退職することがある。上司との相性が悪いこともある。上司が評価会議で発言力を持っていないこともある。上司一人に依存していると、その上司がいなくなった瞬間、あなたを推してくれる人がゼロになる。だから、上司以外のスポンサーも獲得しろ。評価会議には、複数のマネージャーが参加する。あなたの上司だけでなく、他のチームのマネージャーも発言権を持っている。もし、あなたの名前が複数の人から挙がったらどうなるか。「〇〇さん、評判いいね」となる。一人が推すより、複数が推す方が説得力がある。上司以外のスポンサー候補は、意外と身近にいます。他チームのマネージャー: 横断プロジェクトで一緒に働いた人技術リード: マネージャーに意見を求められる立場の人越境した仕事を意図的に作ってください。横断プロジェクトに手を挙げます。他チームのコードレビューを引き受けます。上司を勝たせることの意味上司が成果を出せば、チーム全体の評価が上がります。リソースが配分されます。自分の評価も上がりやすくなります。「媚びる」と「伝える」は違います。情報の非対称性を埋めているだけです。条件が揃わない場合しかし、これには条件がある。条件1: 上司が「勝とうとしている」こと上司が何を達成しようとしているかを理解できないなら、この戦略は機能しない。目標が不明確な上司、日々の消化試合に終始している上司には、「勝たせる」も何もない。判断方法：1on1で「今期の最優先目標は何ですか」と聞く。具体的な目標を即答できるなら、勝とうとしている。「色々ある」「維持が目標」と言うなら、勝とうとしていない可能性が高い。条件2: 上司が「部下の貢献を認識できる」こと上司を勝たせても、「これは俺の成果だ」と言い張る上司がいる。この場合、どれだけ貢献しても報われない。判断方法：過去の昇進者を観察する。上司が「〇〇さんのおかげで成功した」と言っていたか。チームの成果発表で、メンバーの名前を出していたか。自分の手柄にする上司は、パターンがある。条件3: 組織が「チームの成功を個人にも還元する」構造であることチームが勝っても、個人の評価に反映されない組織がある。年功序列が強すぎる、政治が評価を決める。この場合、上司を勝たせても自分には返ってこない。判断方法：先輩に聞く。「チームが成果を出したとき、個人の評価に反映されましたか」と。曖昧な答えが返ってきたら、還元されていない証拠だ。これらの条件が揃わない場合、「上司を勝たせる」戦略は機能しない。別の手を考える必要がある。例えば、「異動する」「別のスポンサーを見つける」「辞める」だ。上司以外のスポンサーを持っていれば、この「別のスポンサーを見つける」がすでに準備できている。上司に依存しすぎないためにも、日頃から複数のスポンサー候補との関係を築いておくことが重要だ。下振れで測られる対話しても評価が変わらないことがある。そのとき、もう1つ確認すべきことがある。自己認識と他者認識のギャップだ。「最高の自分」は実力ではない多くの人は、「最高の自分」を自分の実力だと思っている。ゾーンに入って神がかった速度でコードを書く自分。難解なバグを一瞬で特定する自分。そういう「最高の瞬間」を「自分の実力」だと信じる。でも、上司が見ているのは別のものだ。上司は、あなたに仕事を任せるとき、こう考える。「この人に任せて、最悪どうなるか」と。最高のケースではない。最悪のケースだ。なぜなら、任せた仕事が期待以下だったとき、責任を取るのは上司だからだ。上司は自分の評価を賭けている。だから、リスクを最小化したい。つまり、あなたは「上振れ」ではなく「下振れ」で判断されている。調子が良い日に出した成果は、「たまたま」でしょう。調子が悪い日に出した成果こそ、「確実に期待できるライン」です。上司が知りたいのは、後者です。だから、自分の実力を測るなら、最高の日ではなく、最悪の日を見てください。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その日に絞り出したアウトプット。それが、他人から見た「あなたの実力」に近いです。安定性という信頼信頼は、瞬間最大風速では測られない。安定性で測られる。毎週コンスタントに成果を出す人と、たまに爆発的な成果を出すが波がある人。どちらが信頼されるか。前者だ。爆発的な成果は印象に残る。でも、任せる側からすれば、「今回はどっちだろう」と毎回賭けをすることになる。安定している人には、安心して任せられる。ここで、あまり語られない現実を書く。体調管理は、評価に直結する。「体調不良は仕方ない」と、口では誰もがそう言う。風邪をひいた、熱が出た、それは本人のせいではない。責めるべきではない。正論だ。でも、現実はそんなに甘くない。風邪で3日寝込めば、1週間分の生産性が消える。体調不良の翌週もパフォーマンスは戻りきらない。締め切り直前に体調を崩せば、チーム全体に影響が出る。上司は、それを見ている。口では「お大事に」と言う。でも、心の中では「また休みか」と思っている。重要なプロジェクトを任せるとき、「この人、大丈夫かな」と不安がよぎる。結果、重要な仕事は「安定して稼働できる人」に回る。これは不公平だと思うだろう。体質の問題もある。本人の努力だけではどうにもならないこともある。それは事実だ。でも、コントロールできる部分は、コントロールしてください。もう1つ重要なことがあります。自分のパフォーマンスが落ちる兆候を自己認識していますか。睡眠不足が続くとどうなるか。ストレスが溜まるとどうなるか。これらを把握しておけば、周囲に「予測可能性」を提供できます。「来週は締め切りが重なっているので、レスポンスが遅くなるだろう」と先に言っておきます。これは弱みを見せることではありません。プロとして自分の状態を管理していることを示しています。上司があなたに仕事を任せるとき、「リスク」として感じている要素は何か。「この人は締め切りを守らない」と思われているなら、小さな約束から確実に守ってください。上司の中にある「リスク認知」を、1つずつ消していってください。他人はあなたの「見えた成果の平均」を見ている自分で認識している自分と、他人が見ている自分は違います。あなたは自分の内面を知っています。「今日は調子が悪い」「昨日は睡眠不足だった」「あのときは本気を出していなかった」。そういう文脈を全て知っています。だから、最高のパフォーマンスを出した日を「本当の自分」だと思います。それ以外の日は、何か理由があってパフォーマンスが落ちた「例外」だと思います。他人は、あなたの内面を知りません。見えるのは、あなたのアウトプットだけです。見えたアウトプットの平均が、「あなた」として認識されます。見せなかった仕事は、平均にすら入りません。最高の日も、最悪の日も、見えた範囲で平均化されます。だから、あなたが「本気を出せばもっとできる」と思っていても、他人から見れば「見えた範囲のあなた」がそのままあなたの実力です。見せていない実力は、存在しないのと同じです。ギャップを埋める方法自己認識と他者認識のギャップを埋めるには、フィードバックを求めるしかない。「私の強みと弱みは何ですか」と上司に聞く。怖い。自分が思っている自分と違う答えが返ってくるだろう。でも、聞かなければギャップは分からない。もう1つの方法は、360度評価の結果を真剣に受け止めることだ。多くの人は、360度評価の結果を「まあ、そういう見方もあるよね」程度で流す。でも、複数の人が同じことを指摘しているなら、それはおそらく事実だ。「本当はもっとできる」は通用しない新しい環境で、あなたは「最高の自分」ではなく「最悪の自分」で評価される。慣れない環境、知らないコードベース、初対面のチームメンバー。その状況で出せるアウトプットが、あなたの「実力」として記録される。「本当はもっとできるんです」は通用しない。それは言い訳だ。今、目の前で出しているアウトプットが、あなたの実力として認識される。「体調が悪かったので」も通用しない。体調が悪い日も含めた平均が、あなたの実力だ。だから、自分の「下限」を正しく認識することが重要だ。自分が思っているよりも、自分の下限は低いだろう。他人から見えている自分は、自分が思っている自分とは違うだろう。このギャップを認識した上で、どう動くか。それが「構造を理解した上で頑張る」ということだ。チームを勝たせろここまで「やるべきこと」を書いてきた。ここで1つ、やらなくていいことを書く。「最高の人材はオールラウンダーである」——そう信じられている。でも、そもそもオールラウンダーは、組織が作り出した便利な幻想だ。能力は文脈の中にしか存在しない。「オールラウンダー」とは、会社が定義した評価項目の範囲内でバランスが良い、というだけの話だ。それは普遍的な能力ではなく、ある限定された文脈の中で複数の能力がそこそこ高いだけだ。オールラウンダーの罠でも、オールラウンダーを目指すと何が起きるか。どの分野でも「そこそこ」になります。よくある罠があります。評価面談で「コミュニケーション力が弱い」と言われて、無理に改善しようとします。勉強会で発表する練習をします。ファシリテーションの本を読みます。その結果、強みだった技術力を伸ばす時間が減ります。コミュニケーション力は「平均以下」から「平均」になっただけです。技術力は「突出」から「やや上」に落ちました。本末転倒です。弱みを平均まで引き上げる努力は、強みを突き抜けさせる努力より、はるかに効率が悪いです。100時間かけて弱みを「平均以下」から「平均」にするより、100時間かけて強みを「上位10%」から「上位1%」にする方が、価値が出ます。「チームを勝たせる」という発想ここで視点を変えてほしい。ここまで「上司を勝たせてください」と書いてきました。上司の目標に貢献してください。上司の労力を最小化してください。それがスポンサーを獲得し、評価につながる、と。でも、上司を勝たせることは、手段に過ぎません。本質は「チームを勝たせること」です。チームが勝てば、全員が恩恵を受けます。リソースが配分されます。良いプロジェクトが回ってきます。評価の枠が増えます。逆に、チームが負ければ、個人がどれだけ頑張っても報われません。沈む船の上でいくら走っても、沈むことに変わりはありません。だから、「自分がどう評価されるか」ではなく「チームがどう勝つか」を考えてください。強みで貢献するチームを勝たせるために、あなたは何ができるか。答えは単純です。強みで貢献してください。チームには様々な仕事があります。設計、実装、テスト、ドキュメント、調整、発表。全部を一人でやる必要はありません。チームとして、全部ができていればいいです。あなたがコードを書くのが得意なら、コードで貢献してください。ドキュメントが得意な人に、ドキュメントは任せてください。あなたが調整が得意なら、調整で貢献してください。実装が得意な人に、実装は任せてください。これが「補完」です。全員がオールラウンダーを目指すより、それぞれが強みを発揮して補完し合う方が、チームとしての出力は高くなります。優秀な人に共通パターンはありません。コードは神がかっているがドキュメントは壊滅的な人。設計は天才的だが実装は遅い人。トラブルシューティングは超人的だが新規開発には興味がない人。万能な人はいません。でも、チームとして万能であればいいです。弱みはチームでカバーする弱みを克服する必要がないと言っているわけではありません。弱みを自分で克服するか、チームでカバーするかを選んでください、と言っています。弱みを無視していいかどうかは、3つの質問で判断できます。その弱みがないと仕事ができないか？ コミュニケーションが苦手でも、コードで結果を出せるなら問題ありません。ですが、リーダーを目指すなら、コミュニケーションは避けられません。その弱みをカバーする人がチームにいるか？ ドキュメントが苦手でも、得意な人がチームにいれば補完できます。その弱みを平均にする努力で、強みを伸ばす時間が失われないか？ 弱みを平均にするのに100時間かかるなら、その100時間で強みを突き抜けさせた方がいいです。3つとも「いいえ」なら、弱みの克服は後回しでいいです。チームでカバーできる弱みは、チームに任せてください。しかし、役割によって「致命的な弱み」は変わります。今の役割では問題なくても、次の役割では致命的になることがあります。上司と話し合ってください。「私はAが強みで、Bが弱みです。今の役割でBは致命的ですか。次の役割ではどうですか」と。「この人がいないと困る」状態を作るチームを勝たせる中で、「この人がいないと困る」という状態を作ってください。みんなが平均を目指すなら、平均的な人材は溢れます。「そこそこ何でもできる人」は大量にいます。だから、差別化できません。代わりはいくらでもいます。一方、「この分野なら誰にも負けない」と言える人は少ないです。少ないから、価値があります。「この人じゃないと困る」という状況を作れます。それが交渉力になります。「パフォーマンスチューニングなら〇〇さん」「あの複雑な仕様を理解しているのは〇〇さんだけ」「障害対応で真っ先に呼ばれるのは〇〇さん」——こういうポジションを取ってください。チームの中で、代替不可能な存在になってください。「何でもできる人」という便利なラベルを捨ててください。代わりに、「〇〇の問題ならあいつに聞け」という、組織内の検索ワードを確立してください。検索ワードがあれば、困っている人が自分を見つけてくれます。仕事が向こうからやってきます。その仕事で成果を出せば、また検索ワードが強化されます。この循環を作ってください。そして、自分に問うてみてください。あなたの強みをより伸ばすことが、どのように「チーム全体の勝率」に直結するか。個人の成長と、チームの勝利を結びつけて説明できるか。「私が〇〇を極めれば、チームは△△で勝てるようになります」と。この論理が説明できれば、強みを伸ばす時間を堂々と確保できます。これは「自分だけが得をする」話ではありません。チームが勝つために、自分の強みを最大限に活かすという話です。チームが勝ち、その中で自分が不可欠な貢献をしている。この状態が、評価につながります。上司は言えます。「あのプロジェクトが成功したのは、〇〇さんの△△があったからです」と。具体的な貢献があれば、評価会議で名前を出しやすいです。組織の論理と個人の論理を重ねる組織は「オールラウンダーになれ」と言います。でも、その言葉を額面通りに受け取らないでください。組織が本当に求めているのは、「チームが勝つこと」です。オールラウンダーを求めるのは、そのための手段に過ぎません。誰が抜けてもチームが回るように、リスクヘッジしたいだけです。だから、「チームを勝たせる」という目的を共有した上で、手段は自分で選んでください。オールラウンダーになることでチームに貢献できるなら、それでいいです。でも、強みを尖らせることでチームに貢献できるなら、それでもいいです。目的が達成されていれば、手段は問われません。「私はオールラウンダーではありません。でも、この分野では誰にも負けません。チームの勝利に、この強みで貢献します」と言える状態を作ってください。組織の論理と、個人の論理を、「チームを勝たせる」という一点で重ねてください。これが、構造を理解した上で頑張る、ということです。それでもダメならここまでやっても評価されないことがあります。そのときの判断基準を明確にしておきます。「正しく頑張った」の定義成果を言語化し、見せた1on1で評価基準と昇進に必要なことを確認した上司の目標、チームの目標に貢献した評価のタイムラインを意識して動いたフィードバックを受け入れ、行動を変えた上司以外のスポンサーも獲得しようとした強みで貢献し、弱みはチームでカバーしたこの7つを1年間やった上で、評価が変わらなければ、構造の問題です。2年以上待っても変わらないなら、個人の努力では覆りません。しかし、正直に書いておきます。運の要素は大きいです。この記事は、努力すれば報われるかのように書いてきました。でも、現実はそうじゃありません。良い上司に当たるかどうかは、運です。自分の強みを評価してくれる上司、対話に応じてくれる上司、スポンサーになってくれる上司。そういう上司に当たるかどうかは、自分ではコントロールできません。良いプロジェクトに配属されるかも、運です。成果が見えやすいプロジェクト、評価につながりやすい仕事。それに関われるかどうかは、タイミングと巡り合わせです。会社の業績も、運です。会社が成長していれば昇進枠は増えます。会社が停滞していれば枠は減ります。個人の努力とは関係ありません。この記事に書いたことを全部やっても、運が悪ければ評価されません。逆に、何もしなくても、運が良ければ評価されます。そういうことは、あります。私が評価されるようになったのも、運の要素が大きいです。良い上司に当たりました。良いプロジェクトに関われました。会社の業績が良かった時期に、たまたま成果を出せました。努力したのは事実ですが、運が良かったのも事実です。この記事は、「努力でコントロールできる部分」にフォーカスしています。でも、コントロールできない部分の方が大きいでしょう。運が悪いときに、「頑張り方が間違っている」と言われても、救いになりません。運が悪かった人に、私は何も言えません。「次は運が良いといいね」としか言えません。それは無責任でしょうが、本当のことです。見切るべき3つのパターン パターン  状況  対処  上司とのズレ  上司が重視するAと、自分が得意なBがズレている。対話しても埋まらない  異動するか、別のスポンサーを見つける  制度の破綻  年功序列、政治、声の大きい人が勝つ。チームが勝っても個人に還元されない  組織を変えるか、出るか  市場価値との乖離  外では高く評価されるスキルが、今の組織では価値がない  辞める 見切りの解像度を上げろ「組織を辞める」というより、「この人たちと働くことを辞める」と考えた方が正確だ。冒頭で書いた。「どの会社で働くか」より「どのチームで働くか」が大事だと。会社全体がダメなのか、今いるチームがダメなのか。この見極めは重要だ。この上司との関係は修復可能か？別のチームに移れば解決するか？この会社の「誰か」に働きかければ変わるか？上司以外にスポンサーになってくれる人はいるか？全部試して、全部無理だった。そのとき初めて「構造の問題」と言える。あなたが直面している「評価への不満」は、個人の努力で突破可能な「運用上の課題」か。それとも、組織のDNAに刻まれた「構造的な腐敗」か。この見極めが重要です。1つの判断材料があります。過去3年間で、あなたと同じような「正論を吐く優秀な人」がどのように去っていったか、そのパターンを分析してください。同じパターンが繰り返されているなら、構造の問題です。もう1つの判断材料があります。今の会社で「最も高く評価されている人」の振る舞いは、あなたが5年後に「なりたい姿」と重なるか。重ならないなら、この組織で評価されることに意味があるのか。経営陣が「評価制度の不備」を認識していながら変えないなら、それは彼らにとって「都合が良い」からでしょう。仕組みの問題か、人の問題か「評価制度を変えればいい」——そう思いがちです。でも、制度を変えても、運用する人が変わらなければ、結果は変わりません。本当の問題は、制度ではなく、人と人の関係性にあることが多いです。逆もあります。「この上司が悪い」と思っていても、制度が上司にそう振る舞わせている場合があります。上司も構造の中で動いています。上司を責めても、構造は変わりません。撤退は戦略だ構造的な問題がある場合、とっとと辞めてください。「変われない組織」には共通パターンがあります。正しく頑張っても報われない構造ができあがっています。仕事が見えなくなり、提案が通らなくなり、評価基準が不透明になり、変えようとする人が去っていきます。こうなった組織は、個人の努力では変えられません。見極めのサインあなたの組織がこの状態に陥っているかどうか、いくつかのサインがあります。「これ、誰の仕事？」という会話が週に何度もある障害を未然に防いでも誰も気づかない提案しても「今は優先度が低い」と言われ続ける「なぜこのプロセス？」に「昔からこう」と返ってくる「変えようとして辞めた人」の話をよく聞くチームが勝っても、個人の評価に反映されないこれらのサインが複数当てはまるなら、個人の努力で変えるのは難しいです。異動か転職を視野に入れてください。成功した組織ほど変われなくなる皮肉なことに、成功した組織ほど変われなくなります。「過去にこうやってうまくいった」という経験が、新しいやり方を排除します。成功体験が足かせになります。あなたが「この組織はおかしい」と感じるとき、それは正しいでしょう。組織は過去の成功に縛られて、新しい環境に適応できなくなっているのでしょう。その場合、あなた個人が変えられることは限られています。構造を変えるには、経営層が本気で取り組む必要があります。それがないなら、辞めてください。撤退は戦略である「おい、辞めるな」で書きました。短期ではなく長期で考えてください。信頼の貯金を積み上げてください。転職はリセットコストがかかります。でも、「長期で考えた結果、辞める」という判断もあります。1年間正しく頑張りました。構造を理解した上で動きました。対話を試みました。スポンサーを探しました。チームを勝たせようとしました。それでも変わりませんでした。組織が考える力を失っていて、経営層も本気で取り組む気配がありません。そういう状況なら、辞めることが長期的に正しい判断です。それは逃げではありません。戦略的撤退です。交渉してダメなら去るしかし、順番を間違えないでください。まず交渉してください。評価に納得がいかないなら、上司に聞いてください。「何をすれば評価されるのか」を明確にしてください。構造に問題があると思うなら、提案してください。改善案を出してください。異動を申し出てください。別のスポンサーを探してください。やれることをやってください。交渉するとき、あなたの言葉に「重み」はありますか。社外の市場価値を把握していますか。「いつでも外に出られる」という自信が、言葉に重みを与えます。交渉するなら、「何を、いつまでに、どう変えてほしいか」を具体的に伝えてください。そして、交渉が決裂した際の「プランB」は準備していますか。プランBがないまま交渉しても、本気度が伝わりません。それでダメなら、去ってください。この順番が大事です。交渉せずに辞めるのは、ただの逃げです。でも、交渉した上で辞めるのは、戦略です。「やることはやった。それでも変わらなかった」という事実が、あなたの判断を正当化します。次の面接で「なぜ辞めたのか」と聞かれたとき、「改善を試みたが、構造的に無理だった」と言えます。というか、交渉するというのは、それぐらいデカいことです。「評価に納得いきません」「異動させてください」「この構造を変えてください」——これを口にした時点で、あなたは覚悟を示しています。ダメだったら去る覚悟を。交渉とは、そういう重さを持つ行為です。軽い気持ちで切り出すものではありません。だからこそ、ダメだったときに居座るのは筋が通りません。覚悟を示しておいて、結果が出たら何もしない。それは自分の言葉を裏切ることです。全てはトレードオフです。残るコストと、去るコストがあります。残れば、信頼の貯金を積み上げられます。人間関係もリセットされません。でも、構造が変わらないなら、消耗し続けます。3年後も5年後も同じ愚痴を言っている自分が見えます。去れば、リセットコストがかかります。また一から信頼を築く必要があります。新しい環境に適応するストレスもあります。でも、正しく評価される構造の中で働ける可能性があります。どちらが正解か、一般論では言えません。あなたの状況によります。あなたの価値観によります。あなたのキャリアのフェーズによります。ただ、1つだけ言えます。交渉してダメだったのに居座り続けるのは、最悪の選択です。構造が変わらないと分かりました。自分の力では変えられないと確認しました。それでも残る。それは「判断を放棄している」だけです。答えは出ているのに、行動しません。時間だけが過ぎていきます。交渉してください。ダメなら去ってください。それがトレードオフを引き受けるということです。辞める前に確認することしかし、辞める前に確認すべきことがあります。1. 本当に構造の問題か「評価されない」と感じるとき、構造のせいにしたくなります。自分のせいではない。組織が悪い。そう思いたいです。でも、まず自分を疑ってください。ちゃんと見せていたか。対話していたか。チームを勝たせようとしていたか。強みで貢献していたか。これらを本当にやった上で、評価されなかったのか。構造のせいにするのは、自分の責任を回避できて楽です。でも、構造のせいにして辞めても、次の組織で同じことが起きるでしょう。2. 異動で解決できないか「組織を辞める」前に、「チームを辞める」を検討してください。別のチームに移れば解決することがあります。上司が変われば、評価が変わることがあります。別のスポンサーがいれば、状況が変わることがあります。会社全体がダメなのか、今いるチームがダメなのか。この見極めは重要です。3. 辞めた後に何があるか辞めることを決める前に、辞めた後の絵を描いてください。「ここから出たい」だけでは、どこに行っても同じ問題にぶつかります。次の組織で何をしたいのか。どんな環境なら自分の強みを活かせるのか。どんなチームなら自分が貢献できるのか。それが見えてから、辞めてください。大企業にいるなら、よく考えろあなたは自分が持っているものを過小評価しています。 安定した給与、福利厚生、開発環境、ネームバリュー。これらが「普通」に感じられています。不満ばかりが目につきます。でも、構造的な問題——評価制度の限界、政治、見えない仕事の軽視——は、大企業だから存在するのではありません。組織という形態が持つ宿命です。スタートアップでも20人を超えれば政治が生まれます。50人を超えれば部門間の壁ができます。環境を変えても、構造は変わりません。大企業を辞める前に、まず異動を検討してください。辞めなくても環境を変えられます。サバンナで戦う覚悟があるなら飛び出せばいいです。覚悟がないなら、城壁の中で戦略を練ってください。辞めると決めたら辞めると決めたら、長居しないでください。「あと半年頑張ってみよう」「プロジェクトが終わるまで」と思いがちです。でも、辞めると決めた組織で頑張り続けるのは、消耗します。モチベーションが上がりません。パフォーマンスが落ちます。評価が下がります。悪循環にハマります。辞めると決めたら、次を探し始めてください。時間をかけすぎないでください。辞めても何も変わらないだろう正直に言えば、辞めても何も変わらないでしょう。次の組織も、同じような問題を抱えているでしょう。評価制度に限界があります。上司との相性があります。政治があります。これは、どの組織にもあります。というかそれはあなたの問題でもあります。そこに向き合ったほうが良いです。転職は、問題を解決する魔法ではありません。環境を変えるだけです。新しい環境で、同じ問題に別の形でぶつかることもあります。だから、辞める前に、「この問題は環境を変えれば解決するのか、自分が変わらないと解決しないのか」を考えてください。環境の問題なら、辞めてください。自分の問題なら、自分を変えてください。両方なら、両方やってください。届かない人へここまで書いてきて、立ち止まります。「見せてください」「対話してください」「チームを勝たせてください」——私はそう書きました。構造を理解した上で、その中でうまくやってください、と。でも、この記事が届かない人がいます。頑張れない人がいる「頑張り方を変えてください」と言いました。ですが、もう頑張る余力がない人はどうするのか。すでに消耗している人。毎日出社するだけで精一杯の人。週次報告に「何が難しかったか」を1文追加する気力すらない人。1on1で交渉する心理的余裕がない人。彼らに「見せてください」「対話してください」と言っても、届きません。むしろ、「お前の頑張りは間違っている」と告げることになります。追い詰めることになります。「体調管理は評価に直結する」と書きました。事実です。でも、体調を管理できない人がいます。慢性疾患を抱えている人。精神疾患と付き合っている人。家庭の事情で睡眠時間を削らざるを得ない人。介護や育児で「安定して稼働」できない人。彼らは、努力が足りないのではありません。構造が彼らを排除しているのです。「アピールが苦手なら、存在しないのと同じだ」と書きました。ですが、アピールが苦手な人は、苦手だから苦労しています。「苦手を克服してください」と言うのは簡単です。でも、克服できないから苦手なのです。内向的な人、言語化が苦手な人、自己主張に強い抵抗がある人。彼らに「見せてください」と言っても、できないものはできません。この記事に書いた「正しい頑張り方」ができる人は、すでに恵まれています。対話する余力があります。アピールする能力があります。安定して稼働できる体があります。それらを持っている時点で、スタートラインが違います。私は、持っている側でした。だから、この記事を書けました。持っていない人に、同じことを求めるのは、傲慢でしょう。「頑張らない」という選択肢「辞めないなら頑張ってください」と書きました。ですが、「辞めないけど頑張らない」という選択肢もあります。昇進を追わない。評価を気にしない。自分のペースで働く。それは「諦め」ではありません。評価ゲームから意識的に降りるという戦略です。評価制度は、組織が作ったゲームに過ぎません。そのゲームに参加するかどうかは、自分で選べます。「昇進しなければ給料が上がらない」と言うでしょう。ですが、昇進のために消耗して、心身を壊したら、給料どころではありません。評価を追いかけて、本来の仕事の楽しさを失ったら、何のために働いているのか分からなくなります。評価されなくても、良い仕事はできます。障害を未然に防いだ本人は、その価値を知っています。上司が知らなくても、自分は知っています。それで十分だと思える人もいます。もし今の評価ゲームが「勝てない設定」であるなら、「頑張らない」ことで確保したエネルギーを、どこに投資するか考えてみてください。社内の評価を「食い扶持を維持する程度」にコントロールし、余ったリソースで社外での市場価値を育てることは可能か。今の場所を「人生のゴール」ではなく「ベースキャンプ」と定義し直してください。もちろん、評価されないと生活に困ることもあります。だから、全員にこの選択肢を勧めているわけではありません。ただ、「頑張らない」という選択肢もあることを、知っておいてほしいです。評価ゲームに全てを賭ける必要はありません。降りてもいいです。構造を変えるという選択肢「仕組みは変えられない。自分は変えられる」と書きました。ですが、本当に変えられないのか。「見えない仕事」を評価する仕組みを作った組織はあります。障害を未然に防いだことを、きちんと評価する制度を設計した会社はあります。短期成果だけでなく、長期的な貢献を測る仕組みを導入したチームはあります。変えられないのではありません。変えようとする人がいなかっただけでしょう。変えようとした人が、諦めて辞めていっただけでしょう。この記事では、構造を変える方法は書きませんでした。正直、私にはその経験がないからです。私は構造の中で適応する方を選んできました。変えようとしたこともありますが、うまくいきませんでした。だから、「変えてください」とは言えませんでした。でも、適応することが唯一の選択肢ではありません。もしあなたに発言力があるなら、提案してみてもいいです。評価制度を変える提案。見えない仕事を可視化する仕組み。非機能要件を評価する基準。障害を未然に防いだことを記録するプロセス。変わらないでしょう。でも、変わるでしょう。少なくとも、試さなければ分かりません。「構造を理解した上で適応する」は、1つの戦略です。でも、「構造を理解した上で変えようとする」も、1つの戦略です。どちらを選ぶかは、あなた次第です。多様な「正解」があるこの記事は、「評価される頑張り方」を書きました。ですが、それが唯一の正解ではありません。評価を追いかけて、昇進して、影響力を持つ。それも正解です。評価を諦めて、自分のペースで働く。それも正解です。構造を変えようとして、組織を動かす。それも正解です。評価ゲームから降りて、別の働き方を選ぶ。それも正解です。どれが正しいかは、あなたの状況によります。あなたの価値観によります。あなたの人生のフェーズによります。「辞めないなら頑張ってください」と私は書きました。でも、「辞めないけど頑張らない」でもいいです。「辞めないで、構造を変えようとする」でもいいです。この記事が、あなたを追い詰めるためにあるのではありません。選択肢を増やすためにあります。そう思いたいです。おわりに先週の記事に、思った以上の反響がありました。「辞めないことにしました」という連絡をくれた人たちが、どんな人で今どうしているのか、私は知りません。うまくいっているといいです。うまくいっていなくても、間違えながら何とかやっているといいです。この文章を書き終えました。書いている間、何度か手が止まりました。こんなことを書いて、誰かの役に立つのだろうか。自分が経験したことを、他人に押し付けているだけではないか。答えは出ませんでした。出ないまま、最後まで書きました。明日からできることはあります。週次報告に「何が難しかったか」を1文足す。1on1で「昇進に必要なこと」を聞く。カレンダーに「評価2ヶ月前」をマークする。見えない仕事を、見える形にする。それだけで、何かが変わるでしょう。たぶん、私は来週の週次報告で「何が難しかったか」を書くのを忘れます。上司を敵認定しそうになります。また同じ愚痴を居酒屋で言います。間違えたら直せばいいです。間違えていることに気づいているなら、まだやれます。たぶん。「評価が上がりました」でも、「やっぱり辞めました」でも、「まだ間違え続けています」でも、「頑張るのをやめました」でも。どれでもいいです。どれも、選んだ道を歩いている証拠だと思うから。正解かどうかは、分かりません。私がやってきたことが正しかったかどうかも、分かりません。分かるのは、ずっと後になってからです。おい、辞めないなら頑張ってください。頑張り方を間違えないでください。——と、ここまで書いてきました。でも、最後に付け加えておきます。頑張れないなら、頑張らなくていいです。降りてもいいです。休んでもいいです。それも、1つの選択です。私も、まだ間違え続けています。それでいいのだと思います。続編も書きました。syu-m-5151.hatenablog.com参考書籍外資系コンサルの仕事の進め方: 実践の場で使える問題解決の基盤スキル作者:金地 毅,田辺 元,柳田 拓未東洋経済新報社Amazon私文ホワイトカラーが AI・コンサルに仕事を奪われない働き方戦略作者:株式会社板橋　東京中央支店かんき出版AmazonSOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazonソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社AmazonTHE CULTURE CODE 最強チームをつくる方法作者:ダニエル・コイル,楠木建かんき出版Amazonセンスメイキング――本当に重要なものを見極める力作者:クリスチャン・マスビアウプレジデント社Amazon心眼：あなたは見ているようで見ていない作者:クリスチャン・マスビアウ Christian Madsjergプレジデント社Amazon組織と働き方の本質　迫る社会的要請に振り回されない視座 (日本経済新聞出版)作者:小笹芳央日経BPAmazon［新版］組織行動の考え方―個人と組織と社会に元気を届ける実践知作者:金井 壽宏,高橋 潔,服部 泰宏東洋経済新報社Amazon「組織と人数」の絶対法則―人間関係を支配する「ダンバー数」のすごい力作者:トレイシー・カミレッリ,サマンサ・ロッキー,ロビン・ダンバー東洋経済新報社Amazonチームの力で組織を動かす 〜ソフトウェア開発を加速するチーム指向の組織設計作者:松本 成幸技術評論社Amazon恐れのない組織――「心理的安全性」が学習・イノベーション・成長をもたらす作者:エイミー・C・エドモンドソン,村瀬俊朗英治出版Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazon組織が変わる――行き詰まりから一歩抜け出す対話の方法2 on 2作者:宇田川 元一ダイヤモンド社Amazon多様性の科学作者:マシュー・サイドディスカヴァー・トゥエンティワンAmazon新　失敗学　正解をつくる技術作者:畑村洋太郎講談社Amazon企業変革のジレンマ　「構造的無能化」はなぜ起きるのか (日本経済新聞出版)作者:宇田川元一日経BPAmazon「わかりあえない」を越える――目の前のつながりから、共に未来をつくるコミュニケーション・NVC作者:マーシャル・B・ローゼンバーグ海士の風Amazonみんな違う。それでも、チームで仕事を進めるために大切なこと。作者:岩井俊憲ディスカヴァー・トゥエンティワンAmazonなぜ働く？　誰と働く？　いつまで働く？　限られた人生で後悔ない仕事をするための20の心得作者:有山 徹アスコムAmazon問いかける技術――確かな人間関係と優れた組織をつくる作者:エドガー・H・シャイン英治出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OAuth2認証をE2Eテストしたら、5つのバグが出てきた話]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/11/064311</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/11/064311</guid>
            <pubDate>Sat, 10 Jan 2026 21:43:11 GMT</pubDate>
            <content:encoded><![CDATA[はじめに認証が動いた。だがそれは始まりに過ぎなかった。前回の記事では、Next.jsでOry Hydra認証を実装した。OAuth2認可コードフロー、Cookie管理、ID Token署名検証、マルチテナント認証について解説した。前提知識: この記事は前回の記事の続編です。Next.jsでのOAuth2認証フロー実装を理解している前提で進めます。Next.jsでOry Hydra認証を実装する ― マルチテナントSaaSでの実践 - じゃあ、おうちで学べる今回は、実装した認証フローを検証する。Playwright MCPを使ったE2Eテスト、発見した5つのバグ、RBACの検証、そしてベストプラクティスとの比較までを一気に解説する。Playwright MCPによるE2Eテストもう本当に10年くらい前は「E2Eテストなんて、デモ前に手動で確認すれば十分でしょ」と思っていた。仕事でフロントエンド書いたことなかったので…。今の自分から言わせてもらえば、それは個人の能力を過信している。あとはフロントエンドのテストの大変さを軽く見ている。OAuth2フローのE2Eテストは手動では破綻する。複数のリダイレクト、Cookie管理、セッション状態の確認——これらを毎回手動で確認するのは、人間の注意力の限界を超えている。「今日は疲れていたから見落とした」で本番障害が起きるのは、個人の問題ではなく構造的な失敗だ。人間に頼らない仕組みを作る必要がある。Claude CodeとPlaywright MCPの組み合わせPlaywright MCPは、LLMがブラウザを直接操作できるModel Context Protocol（MCP）サーバーだ。Claude Codeと組み合わせることで、自然言語でE2Eテストを実行できる。従来のPlaywrightとの違いは、スクリプトを書かずにテストできる点だ。# セットアップ（プロジェクトごとに一度だけ）claude mcp add --transport stdio playwright --scope project -- npx -y @playwright/mcp@latest.mcp.jsonが生成される：{  "mcpServers": {    "playwright": {      "type": "stdio",      "command": "npx",      "args": ["-y", "@playwright/mcp@latest"]    }  }}実際のテスト実行例Claude Codeで以下のように指示する：Playwright MCPでOAuth2フローをE2Eテストしてください：1. http://localhost:3001/ にアクセス2. Sign Inをクリック3. demo@example.com / password123 でログイン4. Consentで Allow をクリック5. ダッシュボードが表示されることを確認6. スクリーンショットを取得Claude Codeは以下のツールを順次実行する： ステップ  MCPツール  結果  1  browser_navigate  ホームページ表示  2  browser_click (ref=e10)  Hydra認可エンドポイントへリダイレクト  3  browser_fill_form  ログインフォーム入力完了  4  browser_click (Sign In)  Consent画面へリダイレクト  5  browser_click (Allow)  トークン交換・フロントエンドへリダイレクト  6  browser_take_screenshot  エビデンス取得 ARIA Snapshotの活用Playwright MCPの特徴は、DOMではなくアクセシビリティツリーでページ構造を表現する点だ。各要素にはref=eXX形式の参照IDが付与される：- banner:  - navigation:    - link "Sign In" [ref=e10] [cursor=pointer]:      - /url: /api/auth/loginこのref=e10を使ってクリック対象を指定する。セレクタの管理が不要になり、UIの変更に強いテストが書ける。従来のE2Eテストとの比較 項目  従来のPlaywright  Playwright MCP  テスト作成  スクリプト記述が必要  自然言語で指示  セレクタ管理  CSSセレクタ/XPath  ARIA参照ID  リダイレクト追跡  手動でwait設定  自動追跡  デバッグ  ログ/スクリーンショット  対話的に確認可能  再現性  高（スクリプト化）  中（LLMに依存） Playwright MCPは「探索的テスト」に向いている。本番のCIには従来のPlaywrightスクリプトを使い、開発中の手動確認をPlaywright MCPで効率化する、という使い分けがよさそうだ。E2Eテストで発見した5つのバグPlaywright MCPとシェルスクリプトによるE2Eテストを実行した結果、5つの重要なバグを発見・修正した。OAuth2+マルチテナント構成の複雑さを示す良い事例だ。バグ1：CORS設定の欠如症状：フロントエンド（localhost:3001）からバックエンド（localhost:3000）へのAPIリクエストがブロックされる原因：Axumルーターにtower-httpのCorsLayerが設定されていなかった修正（src/main.rs）：use tower_http::cors::{Any, CorsLayer};let app = Router::new()    // ... routes ...    .layer(        CorsLayer::new()            .allow_origin(Any)            .allow_methods(Any)            .allow_headers(Any),    )教訓：これは個人の注意力の問題ではない。フロントエンド・バックエンド分離構成では、CORSは「設定を忘れると動かない」構造になっている。チェックリストに入れる。プロジェクトテンプレートに含める。人間の記憶に頼らない仕組みを作る。「動かない」の原因がCORSだと気づくまでに時間がかかることがある。エラーメッセージが分かりにくいからだ。ブラウザのコンソールを見る習慣をつけるしかない。詳細はMDN: CORSを参照。バグ2：Cookieパース時のJWTトークン切り詰め症状：認証後のAPIリクエストで401エラーが発生原因：.split("=")[1]でCookieを取得すると、base64エンコードされたJWTの=パディング文字で切れてしまう// ❌ 危険：JWTが途中で切れるconst token = document.cookie  .split("; ")  .find((row) => row.startsWith("auth_token="))  ?.split("=")[1];  // "ory_at_abc...def=" → "ory_at_abc...def" で切れる// ✅ 正しい：トークン全体を取得const cookieRow = document.cookie  .split("; ")  .find((row) => row.startsWith("auth_token="));const token = cookieRow ? cookieRow.substring("auth_token=".length) : null;教訓：JWTは必ずbase64パディング（=）を含む可能性がある。文字列操作でトークンを扱う時は要注意。バグ3：HydraトークンとJWTの不一致症状：フロントエンドからのAPIリクエストで401エラー。curlでJWTを直接送ると成功する。原因：- フロントエンドはHydra発行のアクセストークン（ory_at_...形式）を使用- バックエンドは自前のJWTのみ対応していた修正（src/middleware/auth.rs）：// JWT検証を試み、失敗したらHydraイントロスペクションにフォールバックlet claims = match state.jwt.verify_access_token(token) {    Ok(claims) => claims,    Err(_) => {        // Hydra Admin APIでトークンを検証        let introspection = state.hydra.introspect_token(token).await?;        // IntrospectionResponseからClaimsに変換        Claims::from(introspection)    }};教訓：OAuth2プロバイダー（Hydra）のトークンと自前JWTの両方をサポートするか、どちらか一方に統一するか、設計段階で決めておくべきだった。バグ4：テナント抽出ミドルウェアの欠如症状：テナントAPI（/api/v1/tenant/*）で「No tenant context」エラー原因：tenant_apiルーターにextract_tenantミドルウェアが適用されていなかった修正（src/main.rs）：let tenant_api = Router::new()    // ... routes ...    .layer(axum_middleware::from_fn_with_state(        state.clone(),        middleware::require_auth,    ))    .layer(axum_middleware::from_fn_with_state(        state.clone(),        middleware::extract_tenant,  // 追加    ));教訓：ミドルウェアの適用漏れは見つけにくい。各ルートグループに必要なミドルウェアをリスト化しておくとよい。バグ5：X-Tenant-Slugヘッダーの欠如症状：ローカル開発環境でテナントが識別できない原因：- 本番環境ではサブドメイン（tenant-a.example.com）でテナント識別- ローカル開発ではlocalhost:3001のためサブドメインが使えない- フロントエンドがX-Tenant-Slugヘッダーを送信していなかった修正（frontend/src/lib/api.ts）：class ApiClient {  private tenantSlug: string = "test-shop"; // デフォルトテナント  private async fetch<T>(endpoint: string, options: RequestInit = {}): Promise<T> {    const headers: HeadersInit = {      "Content-Type": "application/json",      "X-Tenant-Slug": this.tenantSlug,  // 追加      ...options.headers,    };    // ...  }}教訓：マルチテナントのテナント識別は、サブドメイン方式とヘッダー方式の両方をサポートしておくとローカル開発が楽になる。E2Eテスト実行結果修正後のOAuth2フロー完全テスト：=== DONADONA E2E Test v4 ===1. Starting OAuth2 Flow...   Login Challenge: LuAyzZfWTX03DnVcFC1xu0A-rntZcx...2. Submitting Login (demo@example.com)...   Consent Challenge obtained3. Approving Consent...   Final: http://localhost:3001/callback?code=ory_ac_d9jRSkWUb1YXm...4. Token Exchange...   Access Token: ory_at_dxBjsXjmRvMuTcSJercIxT_Kq2nUIR6OrUhdBEcEZIg...5. Testing API Endpoints...   Engineers Count: 36. Backend Verification:   slug_from_header=Some("test-shop")   Hydra token introspection successful: sub=Some("3767fa6a-...")============================================   E2E Test PASSED - All fixes verified!============================================複数アカウントでのRBAC検証E2Eテストの最後に、異なるロールのアカウントでログインして、役割ベースアクセス制御（RBAC）が正しく機能しているかを検証した。テスト結果のサマリー以下は修正前のテスト結果だ。platform_adminがDashboardで403を返すなど、明らかな異常がある。詳細は後述する。 アカウント  ロール  ナビゲーションメニュー  アクセス可能ページ  demo@example.com  platform_admin  全メニュー  Dashboard(403)、その他未テスト  manager@example.com  manager  全メニュー  Dashboard, Incidents, Projects, Engineers, Recruitment, Leaderboard  sato@example.com  engineer  制限メニュー  Dashboard, Incidents, Projects, Leaderboard  reporter@example.com  reporter  制限メニュー  すべてAccess Denied 発見1：フロントエンドとバックエンドのデータ不一致テストアカウント一覧を表示するフロントエンドのホームページには、こう書いてあった：Reporter | customer@example.com | Report incidents onlyしかし実際にcustomer@example.comでログインすると、ヘッダーにはengineerと表示された。データベースとフロントエンドの表示が不一致だった。正しいReporterアカウントはreporter@example.comだった。発見2：ロールごとのメニュー制御Playwright MCPのARIAスナップショットで、ロールごとのナビゲーションメニューの違いを確認できた。Manager（manager@example.com）のメニュー：- link "Dashboard" [ref=e10]- link "Incidents" [ref=e11]- link "Projects" [ref=e12]- link "Engineers" [ref=e13]- link "Recruitment" [ref=e14]- link "Leaderboard" [ref=e15]Engineer（sato@example.com）のメニュー：- link "Dashboard" [ref=e10]- link "Incidents" [ref=e11]- link "Projects" [ref=e12]- link "Leaderboard" [ref=e13]# Engineers, Recruitmentが表示されない発見3：Reporterの「何もできない」状態reporter@example.comでログインして各ページにアクセスすると、すべて「Access Denied」が表示された。CLAUDE.mdによると、Reporterは「Report incidents only」という説明だったが、実際にはインシデントページすら見られない。これは設計ミスだった。修正が必要だ。フロントエンドとバックエンドの権限制御問題の根本原因Reporterロールがすべてのページでアクセス拒否されていた原因は、Next.jsのmiddleware.tsにあった：// 修正前：ReporterはADMIN_PATHSに含まれていないconst ADMIN_PATHS = ["/dashboard", "/incidents", "/projects", "/engineers", "/recruitment", "/leaderboard"];// ロールチェック：platform_admin, manager, engineerのみ許可if (isAdminPath && !["platform_admin", "manager", "engineer"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}修正内容// 修正後：ADMIN_PATHSから/incidentsを分離し、REPORTER_PATHSを新設const ADMIN_PATHS = ["/dashboard", "/projects", "/engineers", "/recruitment", "/leaderboard"];const REPORTER_PATHS = ["/incidents"];  // Reporter専用パス// Reporter paths - reporter, engineer, manager, platform_admin can accessconst isReporterPath = REPORTER_PATHS.some((p) => pathname.startsWith(p));if (isReporterPath && !["platform_admin", "manager", "engineer", "reporter"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}// Admin paths - platform_admin, manager, engineer can access (not reporter)const isAdminPath = ADMIN_PATHS.some((p) => pathname.startsWith(p));if (isAdminPath && !["platform_admin", "manager", "engineer"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}これで権限階層が明確になった： パス  platform_admin  manager  engineer  reporter  /tenants  ✅  ❌  ❌  ❌  /dashboard  ✅  ✅  ✅  ❌  /incidents  ✅  ✅  ✅  ✅  /projects  ✅  ✅  ✅  ❌ 多層防御の実装「フロントエンドで権限チェックすればいい」という意見と、「バックエンドだけでやるべき」という意見がある。どちらも正しく、どちらも不十分だ。フロントエンドだけでは、攻撃者がcurlで直接APIを叩けば突破される。バックエンドだけでは、権限のないユーザーが画面を見てから「アクセス拒否」されるUXになる。答えは「両方やる」——多層防御と呼ばれる考え方だ。城の防壁が一重ではなく多重であるように、セキュリティも複数のレイヤーで守る。フロントエンドのmiddleware.tsだけでは不十分だ。攻撃者はフロントエンドを完全にバイパスできる：# フロントエンドを経由せずにAPIを直接叩けるcurl -s http://localhost:3000/api/v1/tenant/incidents \  -H "Authorization: Bearer $TOKEN" \  -H "X-Tenant-Slug: test-shop"Rustバックエンド（Axum）では、権限制御が複数のレイヤーで行われている：レイヤー1：require_auth（認証） - トークンが有効かどうかをチェックレイヤー2：extract_tenant（テナント抽出） - X-Tenant-Slugヘッダーからテナントを特定レイヤー3：ハンドラー内のロールチェック - 特定の操作でロールをチェックpub async fn assign_incident(/* ... */) -> Result<Json<IncidentWithStatus>, AppError> {    let role = claims.get_role();    if !role.can_manage_team() {        return Err(AppError::Forbidden(            "Only managers can assign incidents".to_string(),        ));    }    // ...}多層防御が正解だ： レイヤー  役割  目的  フロントエンド middleware  早期リダイレクト  UX向上、不要なリクエスト削減  バックエンド require_auth  認証チェック  不正アクセス防止  バックエンド ハンドラー  操作ごとの認可  きめ細かい権限制御 ベストプラクティスとの比較この実装が業界のベストプラクティスにどれだけ準拠しているかを評価する。OWASP Top 10 2025との比較OWASP Top 10 2025でBroken Access Controlが1位を維持している。 OWASP推奨事項  準拠状況  実装詳細  サーバーサイドでのアクセス制御  ✅ 準拠  Axumミドルウェアで全APIを保護  デフォルト拒否  ✅ 準拠  未認証リクエストは全て拒否  アクセス制御の再利用  ✅ 準拠  require_authを全ルートで共有  レコード所有権の検証  ⚠️ 部分的  テナント分離は実装、リソース単位は未実装  アクセス制御失敗のログ  ⚠️ 部分的  tracingでログ出力、アラートは未実装  レート制限  ❌ 未実装  APIにレート制限なし  JWTの不正利用防止  ✅ 準拠  Hydraによるトークン検証  セキュリティヘッダ  ⚠️ 部分的  HSTS, X-Frame-Options, X-Content-Type-Optionsの設定が必要  入力値バリデーション  ✅ 準拠  サーバーサイドでバリデーション実施 Next.jsセキュリティガイドラインとの比較Next.js Authentication Guideは、認証に関する重要な警告を含んでいる。 Next.js推奨事項  準拠状況  実装詳細  Middlewareだけに依存しない  ✅ 準拠  バックエンドでも認証チェック  Data Access Layer (DAL)の使用  ⚠️ 部分的  サービス層で分離、専用DALなし  HttpOnly Cookieの使用  ⚠️ 部分的  auth_tokenは非HttpOnly Next.jsチームは「middlewareは認証に安全ではない」と警告している。この多層防御は、CVE-2025-29927のようなmiddlewareバイパス脆弱性への対策にもなる。RBACパターンとの比較 RBACベストプラクティス  準拠状況  実装詳細  バックエンドでのポリシー強制  ✅ 準拠  ハンドラー内でロールチェック  フロントエンドはUI適応のみ  ✅ 準拠  メニュー表示/非表示で対応  権限キャッシュ  ❌ 未実装  毎リクエストでHydra呼び出し  中央集権的ポリシー管理  ⚠️ 部分的  定義は分散している  ロール階層の明確化  ✅ 準拠  4段階のロール階層を定義 総合評価 評価軸  スコア  コメント  OWASP Top 10 2025  7/10  基本的なアクセス制御は準拠、レート制限が不足  Next.js Security  8/10  多層防御を実装、HttpOnly Cookieが部分的  RBAC Patterns  7/10  フロントエンド/バックエンド分離は適切、権限定義が分散 強み：多層防御の実装：フロントエンド + バックエンド + ハンドラーの3層テナント分離：PostgreSQLスキーマレベルでのデータ分離OAuth2標準準拠：Ory Hydraによる標準的なOAuth2/OIDC実装トークン検証の二重化：自前JWT + Hydraイントロスペクションのフォールバック弱み：正直に言えば、見落としがあるかもしれない。セキュリティの評価は、「問題がない」ことを証明できない。見つかっていないだけかもしれない。だから、この記事を読んで「これで完璧だ」と思わないでほしい。OWASP Top 10のチェックリストを自分で回して、この記事で触れていない項目を確認してほしい。それを前提に、現時点で認識している弱みを列挙する。レート制限なし：DoS攻撃への脆弱性権限定義の分散：フロントエンドとバックエンドで定義が重複権限キャッシュなし：毎リクエストでHydraに問い合わせ監査ログの不足：アクセス制御失敗のアラート機能なし改善ロードマップ優先度順に改善すべき項目： 優先度  項目  工数  効果  高  レート制限の追加  小  DoS防止、OWASP準拠  高  監査ログとアラート  中  インシデント検出  高  セキュリティヘッダの追加  小  HSTS, X-Frame-Options, X-Content-Type-Options  中  権限定義の一元化  中  保守性向上  中  権限キャッシュ（Redis）  中  パフォーマンス向上  中  Cookie Prefix（__Host-）の導入  小  Cookie属性の強制  低  PKCE導入  小  認可コード横取り防止  低  HttpOnly Cookie化  中  XSS対策強化 // 改善案：tower-governor等でレート制限を追加use tower_governor::{governor::GovernorConfigBuilder, GovernorLayer};let governor_conf = GovernorConfigBuilder::default()    .per_second(10)    .burst_size(50)    .finish()?;let app = Router::new()    // ...    .layer(GovernorLayer { config: governor_conf });まとめOAuth2 + マルチテナントの認証システム実装を通じて学んだこと「動く」と「正しく動く」は違う：ログインできても、APIが動くとは限らない。APIが動いても、全ロールで正しく動くとは限らない。全ロールで動いても、攻撃に耐えるとは限らない。5つのバグすべてが、「ログインできた」の後に発見されたE2Eテストは必須：すべてユニットテストでは発見できなかった多層防御が重要：フロントエンドだけ、バックエンドだけでは不十分全ロールで検証する：「ログインできた」だけでは不十分ベストプラクティスとのギャップを把握する：何ができていて、何が不足しているかを明確にする認証は地味だが重要だ。インシデント対応のように緊張感もないし、新機能開発のような達成感もない。でも、認証が崩れたときの被害は、他のどの機能障害よりも大きい。過去に見た事例では、セッション管理の不備で全ユーザーのデータが漏洩した。復旧に数ヶ月、信頼回復に1年以上かかった。地味なものほど、丁寧にやる。例えば、この記事で示したE2Eテスト、全ロールでの検証、ベストプラクティスとの比較を、リリース前に必ず行う。それがインフラを支える人間の流儀だ。派手な仕事は誰でも丁寧にやる。地味な仕事を丁寧にやれるかどうかが、プロとアマチュアの違いだと思っている。「ログインできる」は最低条件であり、「安全にログインできる」「快適にログインできる」「問題が起きたときに追跡できる」まで含めて、初めて「認証が実装できた」と言える。この認証実装は完成ではなく、継続的に改善していく起点だ。半年後、1年後に見直したとき、「あの時の判断は正しかったか」を検証できるように、今回の記事を残しておく。次回予告ここまでの4記事で、OAuth2認可サーバー（Hydra）+ 自前認証プロバイダー（Rust）+ フロントエンド（Next.js）の構成が完成した。E2Eテストも通り、RBACも検証できた。しかし、レビューコメントが届いた。「パスワードリセット機能は？」「MFA対応の予定は？」全部、自分で実装するのか？——次回は、Ory Kratosを導入して認証機能を委譲する方法を解説する。syu-m-5151.hatenablog.com参考資料E2EテストPlaywright MCP - LLMがブラウザを操作するためのMCPサーバーModel Context Protocol (MCP) - LLMと外部ツールを接続するプロトコルOry HydraOry Hydra Documentation - Ory Hydra公式ドキュメントToken Introspection - トークンイントロスペクションAPILogin Flow - ログインフローの概念Consent Flow - 同意フローの概念OAuth2 Token Endpoint - トークンエンドポイントAPIリファレンスOAuth2 Revoke Token - トークン失効APIJWKS Endpoint - 公開鍵配信エンドポイントセキュリティガイドラインOWASP Top 10 2025 - Broken Access Control - アクセス制御の脆弱性OWASP Authorization Cheat Sheet - 認可チートシートOWASP Access Control Cheat Sheet - アクセス制御チートシートOWASP OAuth2 Cheat Sheet - OAuth2セキュリティチートシートAuth0 Token Storage - トークンストレージのベストプラクティスRFC 9700 - OAuth 2.0 Security Best Current Practice - OAuth2セキュリティBCPRBACOso: RBAC Role Based Access ControlLogRocket: Choosing the best access control model for frontendLeapcell: Implementing Robust RBAC Across Backend FrameworksNext.jsNext.js Authentication GuideNext.js MiddlewareCORSMDN: Cross-Origin Resource Sharing (CORS)tower-http CorsLayer]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Next.jsでOry Hydra認証を実装する ― マルチテナントSaaSでの実践]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/09/104616</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/09/104616</guid>
            <pubDate>Fri, 09 Jan 2026 01:46:16 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前回の記事では、RustでOry HydraのLogin/Consent Providerを実装した。5つのエンドポイント（GET/POST /login、GET/POST /consent、GET /logout）とHydra Admin APIの連携。Argon2idによるパスワードハッシュ、ユーザー列挙攻撃を防ぐテスト設計の話をした。前提知識: この記事は前回の記事の続編です。OAuth2認可コードフローの基礎知識と、Ory HydraのLogin/Consent Providerの役割を理解している前提で進めます。syu-m-5151.hatenablog.com今回は、そのバックエンドと連携するフロントエンドをNext.js 15で実装する。なぜフロントエンドも自分で書くのか。認証フローを端から端まで把握しておきたいからだ。ちなみにフロントエンドは専門外なのである程度は許してほしいです。NextAuth.jsやAuth0のSDKを使えば楽だが、ブラックボックスのまま本番に出すのは怖い。何かが壊れたとき、「ライブラリの中で何が起きているかわからない」では障害対応で詰むことがある。もちろん、最終的なゴールは「理解した上でライブラリを使う」ことだ。車輪の再発明を推奨しているわけではない。OAuth2/OIDCフローをブラウザ側でどう扱うか。Cookie管理の罠。マルチテナント環境での認証の複雑さ。実際に動かして気づいたことを記録する。OAuth2認可コードフロー：フロントエンドから見た流れまず全体像を把握しておく。┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐│   Browser   │     │   Next.js   │     │ Rust Backend│     │  Ory Hydra  ││  (User)     │     │  Frontend   │     │ (Provider)  │     │  (OAuth2)   │└──────┬──────┘     └──────┬──────┘     └──────┬──────┘     └──────┬──────┘       │                   │                   │                   │       │ 1. Login Click    │                   │                   │       │──────────────────>│                   │                   │       │                   │                   │                   │       │ 2. Redirect to    │                   │                   │       │    /oauth2/auth   │                   │                   │       │<──────────────────│                   │                   │       │                   │                   │                   │       │ 3. GET /oauth2/auth?client_id=...    │                   │       │──────────────────────────────────────────────────────────>│       │                   │                   │                   │       │ 4. Redirect to /login?login_challenge=xxx                 │       │<──────────────────────────────────────────────────────────│       │                   │                   │                   │       │ 5. GET /login                         │                   │       │──────────────────────────────────────>│                   │       │                   │                   │                   │       │ 6. Login Form     │                   │                   │       │<──────────────────────────────────────│                   │       │                   │                   │                   │       │ 7. POST /login (credentials)          │                   │       │──────────────────────────────────────>│                   │       │                   │                   │                   │       │                   │                   │ 8. Accept Login   │       │                   │                   │──────────────────>│       │                   │                   │                   │       │ 9. Redirect to /consent               │                   │       │<──────────────────────────────────────│                   │       │                   │                   │                   │       │ ... Consent Flow ...                  │                   │       │                   │                   │                   │       │ 10. Redirect to /callback?code=xxx    │                   │       │<──────────────────────────────────────────────────────────│       │                   │                   │                   │       │ 11. GET /callback │                   │                   │       │──────────────────>│                   │                   │       │                   │                   │                   │       │                   │ 12. Exchange code for tokens          │       │                   │──────────────────────────────────────>│       │                   │                   │                   │       │                   │ 13. Tokens (access, id, refresh)      │       │                   │<──────────────────────────────────────│       │                   │                   │                   │       │ 14. Set Cookie &  │                   │                   │       │     Redirect      │                   │                   │       │<──────────────────│                   │                   │このフローで重要なのは、フロントエンドは認証ロジックを持たないということだ。なぜか。フロントエンドのコードはユーザーのブラウザで動く。攻撃者は自由に改変できる。DevToolsを開けばJavaScriptは丸見えだし、リクエストも書き換えられる。認証ロジックをそこに置くということは、攻撃者に「好きに改ざんしていいですよ」と言っているようなものだ。認証情報の検証はすべてRustバックエンド（Login Provider）で行う。フロントエンドの役割は：認可エンドポイントへのリダイレクト開始コールバックで認可コードを受け取る認可コードをトークンに交換トークンをCookieに保存以降のAPI呼び出しでトークンを使用Next.js App Routerでの実装ディレクトリ構成frontend/src/├── app/│   ├── layout.tsx│   ├── page.tsx                    # ランディング│   ├── dashboard/page.tsx          # 認証後のダッシュボード│   ├── callback/page.tsx           # OAuth2コールバック│   └── api/auth/│       ├── login/route.ts          # ログイン開始│       ├── callback/route.ts       # コールバック処理│       └── logout/route.ts         # ログアウト├── components/│   └── shared/│       └── Header.tsx├── lib/│   └── api.ts                      # APIクライアント└── middleware.ts                   # 認証チェックログイン開始：認可エンドポイントへのリダイレクト// app/api/auth/login/route.tsimport { NextResponse } from "next/server";import crypto from "crypto";export async function GET(request: Request) {  const { searchParams } = new URL(request.url);  const returnTo = searchParams.get("returnTo") || "/dashboard";  // CSRF対策用のstate生成  const state = crypto.randomBytes(16).toString("hex");  // stateにリダイレクト先を含める（Base64エンコード）  const stateWithReturn = `${state}:${Buffer.from(returnTo).toString("base64")}`;  // Hydra認可エンドポイントへのURL構築  const params = new URLSearchParams({    client_id: process.env.OAUTH_CLIENT_ID!,    response_type: "code",    scope: "openid profile email",    redirect_uri: `${process.env.NEXT_PUBLIC_URL}/callback`,    state: stateWithReturn,  });  const authUrl = `${process.env.HYDRA_PUBLIC_URL}/oauth2/auth?${params}`;  return NextResponse.redirect(authUrl);}stateパラメータは2つの役割を持つ：CSRF対策：ランダムな値を含めることで、攻撃者が生成したURLでのコールバックを防ぐリダイレクト先の保持：認証後、元のページへ戻るためにreturnToをエンコードして含めるRFC 9700 (OAuth 2.0 Security Best Current Practice)では、stateパラメータによるCSRF対策が明記されている。認可サーバーがPKCEをサポートしていることを確認できるなら、PKCEでCSRF対策を兼ねることも可能だが、stateを使う方法が最も広くサポートされている。cheatsheetseries.owasp.orgコールバック処理：トークン取得とCookie設定ここが最も複雑な部分だ。// app/api/auth/callback/route.tsimport { NextResponse } from "next/server";export async function POST(request: Request) {  const body = await request.json();  const { code, state } = body;  // stateからリダイレクト先を取り出す  const [, returnToBase64] = state.split(":");  const returnTo = Buffer.from(returnToBase64, "base64").toString();  // 認可コードをトークンに交換  const tokenResponse = await fetch(    `${process.env.HYDRA_PUBLIC_URL}/oauth2/token`,    {      method: "POST",      headers: {        "Content-Type": "application/x-www-form-urlencoded",        Authorization: `Basic ${Buffer.from(          `${process.env.OAUTH_CLIENT_ID}:${process.env.OAUTH_CLIENT_SECRET}`        ).toString("base64")}`,      },      body: new URLSearchParams({        grant_type: "authorization_code",        code,        redirect_uri: `${process.env.NEXT_PUBLIC_URL}/callback`,      }),    }  );  if (!tokenResponse.ok) {    const error = await tokenResponse.text();    console.error("Token exchange failed:", error);    return NextResponse.json(      { error: "Token exchange failed" },      { status: 401 }    );  }  const tokens = await tokenResponse.json();  // IDトークンをデコードしてユーザー情報を取得  const idTokenPayload = JSON.parse(    Buffer.from(tokens.id_token.split(".")[1], "base64").toString()  );  console.log("ID token decoded:", idTokenPayload);  // レスポンスにCookieを設定  const response = NextResponse.json({ success: true, returnTo });  response.cookies.set("auth_token", tokens.access_token, {    httpOnly: false,  // クライアントJSからアクセス可能に    secure: process.env.NODE_ENV === "production",    sameSite: "lax",    maxAge: tokens.expires_in,    path: "/",  });  if (tokens.refresh_token) {    response.cookies.set("refresh_token", tokens.refresh_token, {      httpOnly: true,  // リフレッシュトークンはhttpOnlyで保護      secure: process.env.NODE_ENV === "production",      sameSite: "lax",      maxAge: 30 * 24 * 60 * 60, // 30日      path: "/",    });  }  return response;}Cookie設定で学んだこと最初、httpOnly: trueでアクセストークンを設定していた。OWASPのセッション管理チートシートによれば、これがセキュリティのベストプラクティスだ。しかし、クライアントサイドでAPIを呼び出す必要があった。owasp.org// クライアントコンポーネントでAPIを呼び出すuseEffect(() => {  const token = document.cookie    .split("; ")    .find((row) => row.startsWith("auth_token="))    ?.split("=")[1];  if (token) {    api.setToken(token);  }}, []);httpOnly: trueだとdocument.cookieからアクセスできない。選択肢は2つ：アクセストークンをhttpOnly: falseにする - クライアントJSからアクセス可能Server Componentからのみ API を呼ぶ - httpOnlyのまま、サーバーサイドで処理今回は1を選んだ。「httpOnlyをfalseにするなんて、セキュリティの教科書に反している」——そう思う人がいるかもしれない。私もそう思った。OWASPのチートシートにも「httpOnly: trueにしろ」と書いてある。でも、教科書に書いてあることと、目の前のシステムで最善の選択は、必ずしも一致しない。この判断には明確な理由がある。まず、脅威モデルを整理する。httpOnlyの目的は「XSSでトークンを盗まれること」を防ぐことだ。では、XSSが成功した場合に何が起きるか。攻撃者はユーザーのブラウザ上で任意のJavaScriptを実行できる。httpOnlyでトークンを保護しても、攻撃者はfetch('/api/user/delete', {credentials: 'include'})を実行できる。トークンを「盗む」ことはできなくても、「使う」ことはできる。しかし、httpOnly: falseにすることで追加のリスクが生じる。トークンを読み取って攻撃者のサーバーに送信できるため、攻撃者は別のマシンからトークンを使用できる。httpOnly: trueなら被害はそのブラウザセッション内に限定されるが、falseなら攻撃者が任意の場所からAPIを叩ける。つまり、httpOnlyは「トークンの窃取」を防ぐことで、XSS被害の範囲を限定する。しかし、XSS対策の本質は、そもそもXSSを発生させないことだ。CSP（Content Security Policy）、入力のサニタイズ、Reactの自動エスケープ——これらがXSS対策の本丸であり、httpOnlyは最後の砦にすぎない。その上で、今回の判断基準は以下だ。アクセストークンは短命（15分）: 仮に窃取されても、15分で無効化されるリフレッシュトークンはhttpOnly: trueで保護: 長期間有効なトークンは絶対に保護するクライアントサイドでのAPI呼び出しが必須: Server Componentだけでは実現できないリアルタイム機能があるしかし、これはトレードオフだ。Auth0のToken Storageガイドでは、SPAの場合、インメモリストレージが最も安全とされている。将来的にはBFF（Backend for Frontend）パターンに移行し、トークンをサーバーサイドで完全に管理する構成を検討している。Curity社のベストプラクティス記事では、JWTの安全な取り扱いについて詳しく解説されている。owasp.orgID Tokenの署名検証なぜ署名検証が必要か最初の実装では、ID Tokenを単純にBase64デコードしていた：// ❌ 危険：署名検証なしのデコードconst payload = JSON.parse(  Buffer.from(tokens.id_token.split(".")[1], "base64").toString());これは動く。中身も読める。でも、これでは改ざんを検出できない。「tokenエンドポイントから直接取得しているから、改ざんされることはないのでは？」と思うかもしれない。確かに、バックエンドでtokenエンドポイントを呼び出し、その結果をそのまま使うなら、経路上で改ざんされるリスクは低い。しかし、問題は別のところにある。フロントエンドにトークンを渡す設計だと、ブラウザ側で別のトークンに差し替えられる可能性がある。また、マイクロサービス間でトークンを渡す際、悪意あるサービスが偽トークンを送る可能性もある。署名検証は「このトークンは本当にHydraが発行したものか」を確認する仕組みだ。具体的に何が起きるか。攻撃者は以下のようなトークンを作成できる。// 攻撃者が作成した偽のトークンconst fakePayload = {  sub: "admin-user-id",  // 管理者のユーザーID  email: "admin@example.com",  role: "platform_admin",  // 権限昇格  tenant_id: "target-tenant",  // 他テナントへのアクセス  exp: 9999999999  // 無期限};const fakeToken = `eyJhbGciOiJub25lIn0.${btoa(JSON.stringify(fakePayload))}.`;署名検証をしていなければ、このトークンは「有効」として受け入れられる。攻撃者は任意のユーザーになりすまし、任意の権限を持ち、任意のテナントにアクセスできる。認証システムが完全に無意味になる。JWTは3つのパートで構成される：ヘッダー.ペイロード.署名。署名を検証しないということは、攻撃者が作った偽のトークンも受け入れてしまうということだ。これは「鍵のかかっていない金庫」と同じだ。中身は入っているが、誰でも開けられる。OpenID Connect Core 1.0のID Token検証仕様では、以下の検証が必須とされている：署名アルゴリズムの確認（alg）発行者の検証（iss = Hydra URL）対象者の検証（aud = クライアントID）有効期限の確認（exp）署名の検証（公開鍵で）joseライブラリによる実装joseライブラリを使うと、これらの検証を簡潔に実装できる。npm install jose// lib/auth.tsimport * as jose from "jose";export interface IdTokenClaims {  sub: string;  aud: string | string[];  iss: string;  exp: number;  iat: number;  email?: string;  role?: string;  tenant_id?: string;}/** * ID Tokenの署名を検証し、クレームを返す * @see https://openid.net/specs/openid-connect-core-1_0.html#IDTokenValidation */export async function verifyIdToken(idToken: string): Promise<IdTokenClaims> {  const hydraUrl = process.env.HYDRA_PUBLIC_URL || "http://localhost:4444";  const clientId = process.env.NEXT_PUBLIC_CLIENT_ID || "demo-client";  // JWKSエンドポイントから公開鍵を取得  // @see https://www.ory.sh/docs/hydra/reference/api#tag/jwk/operation/discoverJsonWebKeys  const JWKS = jose.createRemoteJWKSet(    new URL(`${hydraUrl}/.well-known/jwks.json`)  );  // 署名検証 + issuer/audience検証  const { payload } = await jose.jwtVerify(idToken, JWKS, {    issuer: hydraUrl,    audience: clientId,  });  return payload as IdTokenClaims;}コールバックでの使用// app/api/auth/callback/route.tsimport { verifyIdToken } from "@/lib/auth";export async function POST(request: Request) {  const { code } = await request.json();  // トークン交換...  const tokens = await exchangeCodeForTokens(code);  // ✅ 署名検証付きでID Tokenをデコード  try {    const claims = await verifyIdToken(tokens.id_token);    console.log("ID token verified:", {      sub: claims.sub,      email: claims.email,      role: claims.role,      iss: claims.iss,    });    // ユーザー情報をセッションに保存    const user = {      id: claims.sub,      email: claims.email || "unknown",      role: claims.role || "customer",      tenant_id: claims.tenant_id,    };    // Cookie設定...  } catch (error) {    console.error("ID token verification failed:", error);    return NextResponse.json(      { error: "Token verification failed" },      { status: 401 }    );  }}E2Eテストでの確認実際にログインフローを実行して、署名検証が機能していることを確認した。verifyIdToken()の内部ログと、コールバックハンドラーのログが出力される：ID token verified successfully: {  sub: 'c128f3e7-5013-46b8-add2-fbe0e78bfec7',  email: 'demo@example.com',  role: 'platform_admin',  iss: 'http://localhost:4444'}ID token verified and decoded: {  sub: 'c128f3e7-5013-46b8-add2-fbe0e78bfec7',  email: 'demo@example.com',  role: 'platform_admin',  tenant_id: undefined,  iss: 'http://localhost:4444',  aud: [ 'demo-client' ]}POST /api/auth/callback 200 in 609msverified successfullyと出力されれば、以下が確認できている：JWKSエンドポイント（/.well-known/jwks.json）から公開鍵を取得できた署名が正しく検証された（RS256）issがHydra URL（http://localhost:4444）と一致したaudにクライアントID（demo-client）が含まれていたトークンが有効期限内だったtenant_id: undefinedは、Platform Adminユーザーがテナントに所属していないため。通常のテナントユーザーでログインすると、ここにテナントIDが表示される。開発環境でのフォールバック開発環境ではJWKSエンドポイントにアクセスできない場合がある。その時は警告を出しつつ、署名なしデコードにフォールバックする：try {  const claims = await verifyIdToken(tokens.id_token);  // 検証成功} catch (verifyError) {  console.warn("ID token verification failed, falling back to unsafe decode");  console.warn("WARNING: Using unverified ID token claims. This is insecure!");  // 開発環境のみ許容  const unsafeClaims = decodeIdTokenUnsafe(tokens.id_token);  // ...}本番環境では、このフォールバックを無効化すべきだ。github.comマルチテナント認証JWTにテナント情報を含めるOry HydraのConsent画面で、ユーザーのテナント情報をIDトークンに含める。ベストプラクティスとして、Login時にcontextに保存したユーザー情報をConsent時に取得する（DBルックアップを回避）：// Rustバックエンド側（Consent Provider）// Best Practice: contextからユーザー情報を取得（DBルックアップ不要）// Login時にUserContextとして保存した情報をここで復元let user_context: Option<UserContext> = consent_request    .context    .as_ref()    .and_then(|ctx| serde_json::from_value(ctx.clone()).ok());let (user_email, user_role, user_tenant_id) = user_context    .map(|ctx| (ctx.email, ctx.role, ctx.tenant_id))    .unwrap_or_default();// IDトークンにカスタムクレームを追加let session = ConsentSession {    id_token: serde_json::json!({        "email": user_email,        "role": user_role,        "tenant_id": user_tenant_id,  // テナントIDを含める    }),};hydra.accept_consent(&challenge, grant_scope, grant_audience, Some(session)).await?;ここで重要なのは、user_emailやuser_roleをDBから取得するのではなく、Login時にHydraのcontextに保存したUserContextから取得している点だ。これにより：Consent時のDBアクセスが不要になるLogin時点のユーザー状態が保持される（整合性）パフォーマンスが向上するフロントエンドでトークンをデコードすると、テナント情報が取得できる：// IDトークンのペイロード例{  "aud": ["demo-client"],  "email": "manager@example.com",  "role": "manager",  "tenant_id": "aa8d56f1-a083-439b-996a-4a7b73698dfb",  "sub": "e5555555-5555-5555-5555-555555555555"}APIリクエストでのテナント分離バックエンドAPIは/api/v1/tenant/というプレフィックスでテナント固有のエンドポイントを提供：/api/v1/tenant/incidents    # テナント内のインシデント/api/v1/tenant/projects     # テナント内のプロジェクト/api/v1/tenant/engineers    # テナント内のエンジニアテナントIDはJWTから取得するため、URLにテナントIDを含める必要はない。これにより：URLの推測による他テナントへのアクセス試行を防ぐテナントIDの改ざんを防ぐ（JWTは署名で保護されている）なぜURLパスにテナントIDを含める方式が危険なのか、具体例で説明する。# URLパス方式（危険）GET /api/v1/tenants/tenant-123/incidentsGET /api/v1/tenants/tenant-456/incidents  ← tenant-123のユーザーがアクセスを試みるこの方式では、バックエンドで「リクエストしたユーザーがtenant-456に所属しているか」を毎回検証する必要がある。検証を忘れると、他テナントのデータが漏洩する。実際、この種のバグは「IDOR（Insecure Direct Object Reference）」として知られ、OWASPのトップ10に常に入る脆弱性だ。# JWTクレーム方式（安全）GET /api/v1/tenant/incidents# JWTの中身: {"tenant_id": "tenant-123", ...}この方式では、バックエンドはJWTからテナントIDを取得する。JWTは署名で保護されているため、ユーザーが改ざんできない。「どのテナントのデータを返すか」はJWTが決定し、URLは関与しない。URLパラメータとユーザー権限を照合する追加の検証が不要になるため、バグの入り込む余地が減る。このアプローチはMicrosoft Azure Architecture Centerでも推奨されている。ログアウト処理OAuth2のログアウトは複雑だ。以下を考慮する必要がある：フロントエンドのCookie削除HydraのOAuth2セッション無効化バックエンドのセッション無効化（該当する場合）// app/api/auth/logout/route.tsexport async function GET(request: Request) {  const accessToken = request.cookies.get("auth_token")?.value;  if (accessToken) {    // 1. Hydraでトークンを無効化    await fetch(`${process.env.HYDRA_PUBLIC_URL}/oauth2/revoke`, {      method: "POST",      headers: {        "Content-Type": "application/x-www-form-urlencoded",        Authorization: `Basic ${Buffer.from(          `${process.env.OAUTH_CLIENT_ID}:${process.env.OAUTH_CLIENT_SECRET}`        ).toString("base64")}`,      },      body: new URLSearchParams({        token: accessToken,      }),    });    // 2. Hydraのログインセッションも削除    // （IDトークンからsubjectを取得して削除）  }  // 3. Cookieを削除してリダイレクト  const response = NextResponse.redirect(new URL("/", request.url));  response.cookies.delete("auth_token");  response.cookies.delete("refresh_token");  return response;}RP-Initiated LogoutOpenID ConnectにはRP-Initiated Logout 1.0という仕様がある。この仕様では、Relying Party（クライアントアプリケーション）からOpenID Providerに対してログアウトを要求する方法が定義されている。Hydraはこれをサポートしている。www.ory.sh// Hydraのログアウトエンドポイントを使う方法const logoutUrl = new URL(`${process.env.HYDRA_PUBLIC_URL}/oauth2/sessions/logout`);logoutUrl.searchParams.set("id_token_hint", idToken);logoutUrl.searchParams.set("post_logout_redirect_uri", `${process.env.NEXT_PUBLIC_URL}/`);return NextResponse.redirect(logoutUrl);この方法だと、Hydraがログアウト処理を統括し、Login Providerの/logoutエンドポイントにリダイレクトしてくれる。トラブルシューティング：実際に遭遇した問題問題1：Cookie名の不一致症状：ログイン後、ダッシュボードでAPIデータが取得できない原因：コールバックで設定するCookie名と、各ページで読み取るCookie名が異なっていた// コールバックresponse.cookies.set("auth_token", ...);// ダッシュボード（間違い）.find((row) => row.startsWith("access_token="))// 正しくは.find((row) => row.startsWith("auth_token="))教訓：Cookie名は定数として一箇所で定義し、全体で共有する。なぜこのミスが起きるのか。認証コードはコールバック処理から書き始め、ダッシュボードは後から書く。時間が空くと、最初に使った名前を忘れる。「書いた順番」と「読まれる順番」が異なるコードでは、定数化を最初に行うべきだ。// lib/constants.tsexport const AUTH_COOKIE_NAME = "auth_token";export const REFRESH_COOKIE_NAME = "refresh_token";問題2：APIパスの構造症状：APIリクエストが404を返す原因：テナントAPIのパスプレフィックスを間違えていた// 間違いfetch("/api/v1/incidents")  // 404// 正しいfetch("/api/v1/tenant/incidents")  // 200教訓：APIのベースパスはAPIクライアントクラスで管理するclass ApiClient {  private baseUrl = process.env.NEXT_PUBLIC_API_URL;  private tenantPath = "/api/v1/tenant";  async getIncidents() {    return this.request(`${this.tenantPath}/incidents`);  }}問題3：トークン期限切れ症状：しばらく操作しないとAPI呼び出しが失敗する原因：アクセストークンの有効期限（15分）が切れていた対策：リフレッシュトークンを使った自動更新async request<T>(path: string, options?: RequestInit): Promise<T> {  const response = await fetch(`${this.baseUrl}${path}`, {    ...options,    headers: {      ...options?.headers,      Authorization: `Bearer ${this.token}`,    },  });  if (response.status === 401) {    // トークンをリフレッシュして再試行    await this.refreshToken();    return this.request(path, options);  }  return response.json();}問題4：HydraのセッションとProviderのセッション症状：ログアウト後、再度ログインしようとすると認証画面をスキップしてしまう原因：Hydraのログインセッションが残っていたOry Hydraのドキュメントによると、HydraはLogin Providerでの認証成功を記憶している。skipフラグが立っている場合、ログイン画面をスキップする。これはSSO（シングルサインオン）の正しい動作だが、完全なログアウトを実装する際には注意が必要だ。// Login Provider側if login_request.skip {    // 既にセッションがあるのでスキップ    // Note: skip時はcontextが既に設定されているためNoneで良い    let completed = hydra.accept_login(&challenge, &login_request.subject, false, None).await?;    return Ok(Redirect::to(&completed.redirect_to));}完全なログアウトには、Hydraのセッションも削除する必要がある：// ログアウト時にHydraのセッションも削除await fetch(  `${process.env.HYDRA_ADMIN_URL}/admin/oauth2/auth/sessions/login?subject=${userId}`,  { method: "DELETE" });エラーハンドリングのパターンバックエンドから返されるエラーは統一された形式になっている：{  "error": "invalid_credentials",  "error_description": "The provided credentials are invalid",  "error_code": "AUTH_002"}フロントエンドではこれを適切に処理する：async request<T>(path: string, options?: RequestInit): Promise<T> {  const response = await fetch(`${this.baseUrl}${path}`, options);  if (!response.ok) {    const error = await response.json().catch(() => ({      error: "unknown_error",      error_description: "An unexpected error occurred",    }));    throw new ApiError(response.status, error);  }  return response.json();}class ApiError extends Error {  constructor(    public status: number,    public body: { error: string; error_description: string; error_code?: string }  ) {    super(body.error_description);  }}セキュリティチェックリスト実装後に確認すべき項目。これは完璧なリストではない——セキュリティに完璧はない——が、最低限チェックすべきポイントをまとめた。認証に関わるCookieの属性[ ] HttpOnly属性: XSSの緩和策。クライアントJSからアクセス不要なCookieには必ず設定[ ] SameSite属性: LaxもしくはStrictに設定。CSRF対策の基本。Laxの場合、GETリクエストで更新処理を行っていないか確認[ ] Secure属性: HTTPS通信でのみCookieが送られるように。本番環境では必須[ ] Domain属性: サブドメインへのCookie送信範囲を理解しているか。example.comのCookieがjobs.example.comにも送られる設定だと、他サブドメインの脆弱性がリスクになる[ ] Cookie Prefix: Cookie名を__Host-で始めると、Domain属性が空でないCookieの指定を無視してくれる（参考: Cookie Prefixのバイパス）blog.tokumaru.orgレスポンスヘッダ[ ] Strict-Transport-Security（HSTS）: ブラウザにHTTPS接続を強制。max-age=31536000; includeSubDomains; preload[ ] X-Frame-Options: DENYもしくはSAMEORIGINでクリックジャッキング対策。CSPのframe-ancestorsも検討[ ] X-Content-Type-Options: nosniffを指定。MIMEタイプスニッフィング攻撃を防ぐ認証フロー[ ] stateパラメータでCSRF対策している[ ] リフレッシュトークンはhttpOnlyで保護している[ ] アクセストークンの有効期限は短く設定している（15分推奨）[ ] ログアウト時にトークンを無効化している[ ] メールアドレスの列挙ができないこと: ログイン画面やパスワード再設定画面で「このメールアドレスは登録されていません」のようなエラーを出さない[ ] JWTの署名を検証している（バックエンド側）[ ] テナント分離がJWTベースで行われている[ ] 退会/メールアドレス変更などの重要操作で直前のログインを必須にしている: XSSやセッションハイジャック発生時の緩和策その他[ ] サードパーティCookieに依存していないこと（Chrome廃止予定）[ ] iOS SafariのITPによりローカルストレージやJSから保存したCookieは7日で消える可能性がある（未使用時）まとめNext.jsでOry Hydra認証を実装する際の要点：OAuth2フローの理解：認可コードフローの各ステップでフロントエンドが何をすべきか把握するID Token署名検証：JWKSを使って署名を検証し、issuer/audienceを確認するCookie管理：httpOnly, Secure, SameSiteの設定を用途に応じて選択するマルチテナント：JWTにテナント情報を含め、APIはトークンからテナントを識別するエラーハンドリング：OAuth2仕様に沿ったエラー形式を統一的に処理するログアウト：Hydraのセッションとフロントエンドのセッション両方を考慮する認証は「動いた」で終わりではない。Cookie名の不一致のような単純なミスから、セッション管理の複雑さまで、実際に動かして初めて見つかる問題が多い。結局のところ、OAuth2は「誰かが決めた仕様に従う」ゲームだ。RFCを読み、OWASPを読み、Hydraのドキュメントを読む。自分で発明する余地は少ない。でも、それでいい。認証のような重要な仕組みを自己流で作るのは、傲慢だと思う。セキュリティの歴史は「賢い人が作ったものを、もっと賢い攻撃者が破る」の繰り返しだ。OAuth 1.0のセッション固定攻撃、JWTのalg=none脆弱性——仕様を作った人たちでさえ、穴を見落とす。自分がその歴史に新たな失敗を加える必要はない。先人の知恵に乗っかり、その上で自分のシステムに合った判断をする。それが現実的なアプローチだ。前回のバックエンド実装でユーザー列挙攻撃を防ぐテストを書いたように、フロントエンドでも手動でのE2Eテストが重要だ。ログイン→操作→ログアウト→再ログイン。このサイクルを何度も試して、エッジケースを潰していく。次回は、Playwright MCPを使ったE2Eテストの自動化と、テストで発見したバグについて解説する。syu-m-5151.hatenablog.comこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。おわりに今日は社内で学生向けワークショップを担当した。終わった後、若い参加者が話しかけてきた。「ブログ読んでます」と言われた。嬉しかった。嬉しかったが、すぐに釘を刺した。「あまり憧れないでくださいね」と。憧れられるのがあまり得意ではない。偶像として崇拝されるのが苦手だし、偶像として振る舞って相手に応えるのも苦手だ。それに、ブログで良いこと言っている人に若いうちから憧れすぎるのは良くない。自分がそうだったのでよく分かる。10代の頃、文章が上手くて考え方が明快な技術ブロガーを見つけて、「この人みたいになりたい」と思った。記事を読み漁った。でも、その人が実際にどんなコードを書いているかは知らなかった。ブログは編集された「ハイライト」にすぎない。裏側の泥臭い試行錯誤、失敗、妥協は見えない。数年後にそれを知ったとき、ちょっとがっかりした。がっかりした自分にもがっかりした。若い技術者なら、現場に居る良い技術者に憧れてほしい。ブログを書く人ではなく。GitHubのコミット履歴を見てほしい。PRのレビューコメントを見てほしい。本番障害のポストモーテムを読んでほしい。そこに本当の技術者がいる。ブログの「正解」ではなく、コードの「試行錯誤」に学んでほしい。正直に言えば、フロントエンドでの認証実装は想像以上に複雑だった。3年前の自分に言いたい。「Next.jsで認証？OAuth2知ってるし、すぐできるでしょ」と思っていた過去の自分に。そうじゃない。Cookieの属性一つでセキュリティモデルが変わる。ID Tokenの署名検証を省略した瞬間、認証システムの意味がなくなる。OAuth2のフローは理解していたつもりだった。RFCも読んだ。でも、実際にNext.jsでCookieを扱い、ID Tokenの署名を検証し、マルチテナントのテナント分離を実装すると、「知っている」と「動かせる」の間には大きな溝があることを思い知らされた。RFCには「stateパラメータでCSRF対策」と書いてある。でも、実際にコードを書くと「stateはどこに保存する？」「検証はいつやる？」「不一致の場合のエラーメッセージは？」という判断が次々と必要になる。仕様書は「何をすべきか」は教えてくれるが、「どう実装すべきか」は教えてくれない。その溝を埋めるのは、結局、自分で書いて動かす経験しかない。特にhttpOnlyの判断には時間を使った。OWASPのベストプラクティスを読み、Auth0のガイドを読み、それでも「これで正しいのか」という不安は消えない。セキュリティに100%の正解はない。トレードオフを理解し、判断し、記録する。それしかできることはない。この記事を書いている人間も、悩みながら書いている。ブログに書かれている「正解」は、試行錯誤の結果を事後的に整理したものにすぎない。過程で何度も間違えている。それを知った上で、参考にしてもらえれば。なんか総じてとても疲れた。でも、まあ、悪くない一日だった。参考資料Ory HydraOry Hydra DocumentationOAuth2 Token EndpointLogin FlowLogout FlowOAuth2/OIDC仕様RFC 6749 - OAuth 2.0RFC 9700 - OAuth 2.0 Security Best Current PracticeOpenID Connect Core 1.0RP-Initiated Logout 1.0セキュリティガイドラインOWASP OAuth2 Cheat SheetOWASP Session Management Cheat SheetAuth0 Token StorageCurity JWT Best PracticesCookie属性CookieのDomain属性は指定しないが一番安全 - 徳丸氏によるCookie Domain属性の解説Cookie Prefixのバイパス - __Host-プレフィックスの重要性MDN: Set-Cookie - Cookie属性の公式リファレンスサードパーティCookieの廃止に向けた準備 - Chrome対応ガイドライブラリjose - JavaScript Object Signing and EncryptionNext.jsNext.js App RouterRoute HandlersMiddleware]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fear of the Unknown：Rust/sqlxでNULLを制する6つのパターン]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/08/092409</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/08/092409</guid>
            <pubDate>Thu, 08 Jan 2026 00:24:09 GMT</pubDate>
            <content:encoded><![CDATA[はじめにあるプロジェクトで、電話番号が未登録のユーザーを検索するコードをレビューしていた。WHERE phone = NULL——一見正しく見えるこのクエリは、常に0件を返していた。データは確実に存在する。クエリもシンプル。では何が問題なのか。答えはSQLの3値論理にあった。通常の比較演算はTRUEかFALSEを返すが、SQLにはUNKNOWN（不明）という第3の真偽値がある。NULLは「値が不明」を意味するため、NULL = NULLは「不明 = 不明」となり、結果もUNKNOWNになる。WHERE句はTRUEの行しか返さないから、UNKNOWNは暗黙にFALSE扱いされ、結果は常に0件になる。この問題は『SQLアンチパターン』で「Fear of the Unknown」として解説されている。本記事ではRust + sqlxでの実装パターンに焦点を当てる。SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazonこういう妄想の仕様と実際の仕様には違いがある。「おい、類推するな」というブログで書いたので時間がある時に読んでほしい。syu-m-5151.hatenablog.comsqlxの型マッピングRustにはOption<T>という型がある。これは「値があるかもしれないし、ないかもしれない」を表現する型だ。Some(値)が「値あり」、Noneが「値なし」を意味する。SQLのNULLに相当するのがこのNoneだ。let phone: Option<String> = Some("090-1234-5678".to_string());  // 値ありlet phone: Option<String> = None;                                // 値なし（NULL相当）sqlxはPostgreSQLのNULLをこのOption<T>に自動マッピングする。 PostgreSQL  Rust (NULLable)  Rust (NOT NULL)  VARCHAR, TEXT  Option\<String>  String  INTEGER  Option\<i32>  i32  BIGINT  Option\<i64>  i64  UUID  Option\<Uuid>  Uuid  DECIMAL  Option\<Decimal>  Decimal  TIMESTAMPTZ  Option\<DateTime\<Utc>>  DateTime\<Utc> NULLableカラムをOption<T>以外にマッピングすると、NULLが返された時点で実行時エラーになる。私も一度やった。「NULLなんて来ないだろう」と思っていたカラムが、特定の条件でNULLを返し、深夜にSlackが鳴った。#[derive(Debug, sqlx::FromRow)]struct User {    id: Uuid,    email: String,              // NOT NULL → 必ず値がある    name: String,               // NOT NULL → 必ず値がある    phone: Option<String>,      // NULLable → Option型で「値があるかもしれないし、ないかもしれない」を表現    bio: Option<String>,        // NULLable → Noneが「値なし」、Some("値")が「値あり」    created_at: DateTime<Utc>,  // NOT NULL → 必ず値がある}パターン1：検索フィルターでのNULL// NG: NoneがNULLにバインドされ、phone = NULLは常にUNKNOWN// query_as::<_, User>の説明://   ::<_, User> は戻り値の型を指定するRustの記法（turbofish構文）//   _ はデータベースの種類をコンパイラに推論させる部分//   User は「検索結果をUser構造体に変換して」という指定let users = sqlx::query_as::<_, User>(    "SELECT * FROM users WHERE phone = $1"  // $1はプレースホルダ（SQLインジェクション対策）).bind(&params.phone)  // bind()で$1に値を埋め込む。NoneはNULLになる.fetch_all(&pool)     // 全件取得.await?;              // 非同期処理の完了を待つ。?はエラー時に早期リターン// OK: 条件分岐でクエリを切り替える// match式: Option型の中身に応じて処理を分岐（switch文のようなもの）let users = match &params.phone {    Some(phone) => {  // Some(値): 値がある場合        sqlx::query_as::<_, User>("SELECT * FROM users WHERE phone = $1")            .bind(phone)            .fetch_all(&pool)            .await?    }    None => {  // None: 値がない場合 → IS NULLを使う        sqlx::query_as::<_, User>("SELECT * FROM users WHERE phone IS NULL")            .fetch_all(&pool)            .await?    }};// OK: IS NOT DISTINCT FROMで1クエリにまとめる（PostgreSQL固有）// NULLを普通の値として比較できる（NULL同士も「等しい」と判定）let users = sqlx::query_as::<_, User>(    "SELECT * FROM users WHERE phone IS NOT DISTINCT FROM $1").bind(&params.phone).fetch_all(&pool).await?;パターン2：COUNTの挙動// r#"..."# は生文字列リテラル（raw string literal）// 複数行のSQLを書きやすく、エスケープも不要な記法sqlx::query_as(    r#"    SELECT        COUNT(*) as total_users,                           -- 全行数（NULLを含む）        COUNT(coupon_code) as users_with_coupon,           -- NULLでない行数        COUNT(*) - COUNT(coupon_code) as users_without_coupon    FROM users    "#)空文字列とNULLが混在している場合は注意が必要。// NG: 空文字列のみマッチ、NULLはマッチしない"SELECT * FROM users WHERE coupon_code = ''"// OK: 両方を考慮"SELECT * FROM users WHERE coupon_code IS NULL OR coupon_code = ''"// OK: NULLIFで正規化"SELECT * FROM users WHERE NULLIF(coupon_code, '') IS NULL"パターン3：フォーム送信での空文字列フロントエンドから{ "phone": "" }が送られると、Option<String>ではSome("")になる。データベースには空文字列が保存され、NULLにはならない。// Rustレイヤーで正規化// filter(): 条件を満たさない場合はNoneに変換するメソッド// |s| !s.is_empty() はクロージャ（無名関数）: sが空でなければtruelet phone = req.phone.filter(|s| !s.is_empty());  // Some("") → None, Some("090") → Some("090")let bio = req.bio.filter(|s| !s.is_empty());sqlx::query("UPDATE users SET phone = $1, bio = $2 WHERE id = $3")    .bind(&phone)  // NoneはNULLとしてバインドされる    .bind(&bio)    .bind(user_id)    .execute(&pool)  // execute(): SELECT以外のクエリ実行    .await?;// SQLレイヤーで正規化sqlx::query(    r#"    UPDATE users    SET phone = NULLIF(TRIM($1), ''),  -- TRIM: 空白除去, NULLIF: ''ならNULLに        bio = NULLIF(TRIM($2), '')    WHERE id = $3    "#)パターン4：LEFT JOINでのOption必須LEFT JOINは左側のテーブル（例: users）の全行を返す。右側のテーブル（例: orders）に一致する行がない場合、右側のカラムはすべてNULLで埋められる。だから注文がないユーザーの場合、o.created_atはNULLになり、MAX(o.created_at)の結果もNULLになる。// NG: 注文がないユーザーでMAX(o.created_at)がNULLになり、実行時エラーstruct UserWithLastOrder {    last_order_date: DateTime<Utc>,  // NULLを受け付けない型}// OK: Option<T>でNULLを許容するstruct UserWithLastOrder {    last_order_date: Option<DateTime<Utc>>,  // NULLならNone、値があればSome(値)}LEFT JOINや集約関数（MAX, AVG, SUM等）の結果は常にNULLになりうる。迷ったらOption<T>を使う。パターン5：NOT INの罠// NG: category_idがNULLの行は削除されないsqlx::query(    r#"    DELETE FROM products    WHERE category_id NOT IN (        SELECT id FROM categories WHERE active = true    )    "#)なぜNULLの行が削除されないのか。NOT INは内部でx <> 1 AND x <> 2 AND ...に展開される。ここでcategory_idがNULLだとどうなるか。NULL <> 1はUNKNOWNを返す。NULL <> 2もUNKNOWN。ANDの3値論理ではTRUE AND UNKNOWN = UNKNOWNだから、条件全体がUNKNOWNになる。WHERE句はTRUEの行しか処理しないため、NULLを含む行は削除対象から外れてしまう。// OK: NOT EXISTSを使う// NULLの行も正しく処理される（サブクエリが0行ならTRUE）sqlx::query(    r#"    DELETE FROM products p    WHERE NOT EXISTS (        SELECT 1 FROM categories c        WHERE c.id = p.category_id AND c.active = true    )    "#)パターン6：query_as!マクロこれまでのパターンで使っていたquery_as()は実行時に型チェックを行う。一方query_as!()はマクロで、コンパイル時にデータベースへ接続してスキーマを確認し、型の不整合をビルドエラーとして検出する。NULLになりうるカラムをOption<T>以外でマッピングしようとすると、実行前にエラーを発見できる。// NG: AVG(rating)はNULLを返す可能性があり、コンパイルエラーstruct ProductSummary {    average_rating: f64,  // f64はNULLを受け付けない}sqlx::query_as!(    ProductSummary,    "SELECT name, AVG(rating) as average_rating FROM products GROUP BY name")// コンパイルエラー: AVGの結果がNULLになりうるのにOption<f64>ではない// OK: Option<T>を使うstruct ProductSummary {    average_rating: Option<f64>,}// OK: COALESCEと"!"サフィックスでNOT NULLを保証sqlx::query_as!(    ProductSummary,    r#"    SELECT name,           COALESCE(AVG(rating), 0)::FLOAT8  -- NULLなら0、FLOAT8にキャスト           as "average_rating!"              -- "!"でNOT NULLを宣言    FROM products GROUP BY name    "#) サフィックス  意味  !  NOT NULLを強制（Option\<T>ではなくT）  ?  NULLを許容（TではなくOption\<T>） まとめ冒頭のWHERE phone = NULLは、WHERE phone IS NULLに書き換えて5分で解決した。3値論理を知っているかどうか——それだけの差だった。NULLの問題はバグではなく、SQLの仕様だ。Rust/sqlxでは以下を守れば大半の問題は防げる。NULLableカラムはOption<T>にマッピング= NULLではなくIS NULLを使うNOT INではなくNOT EXISTSを使う空文字列とNULLを混在させない迷ったらOption<T>を使う。後からOptionを外すのは簡単だが、NULLが返ってきたときのパニックを本番で見るのは心臓に悪い。そもそもNULLableカラムを減らす設計（NOT NULL制約のデフォルト化、別テーブルへの分離）も検討に値する。3値論理の詳細は『SQLアンチパターン』の「Fear of the Unknown」章を参照してほしいです。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料SQL Antipatterns - Fear of the UnknownPostgreSQL - Comparison Functionssqlx - Compile-time checked queries]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI時代に今からITエンジニアを目指す若者にオススメする10冊の本  2026年版]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/07/103853</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/07/103853</guid>
            <pubDate>Wed, 07 Jan 2026 01:38:53 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIは、あなたが聞いたことにしか答えない。聞かなかったことは、永遠に教えてくれない。あなたが何を知らないのか、AIは知らない。2026年だ。AIに聞けば何でも教えてくれる。コードを書いてもらい、設計を相談し、ドキュメントを要約させる。便利だ。では、なぜ本を読むのか。300ページもある本を、最初から最後まで読む必要があるのか。本は違う。本は、聞いていないことを語りかけてくる。知らなかった世界を見せてくる。持っていなかった問いを、手渡してくる。「そんなこと、考えたこともなかった」。そういう瞬間が、本にはある。AIとの対話では、たぶん起きない。AIは効率的だ。知りたいことに、最短距離でたどり着ける。でも、最短距離で歩いていると、道の脇にあるものが見えない。著者が失敗した話、遠回りした話、「今思えば間違いだった」という告白。そういう「寄り道」が、不思議と頭に残る。正解は忘れる。でも、誰かの失敗談は覚えている。たぶん、人間の脳は感情を伴う記憶を優先的に保持するからだ。著者の後悔や苦労を読むとき、読者は追体験している。その感情が、記憶を定着させる。AIに「失敗談を教えて」と聞けば、一般化された失敗談が返ってくる。でも、それは「誰かの」失敗ではない。固有名詞のない失敗談には、感情が宿らない。もう1つ。若者や学生は、そもそも問いを持っていない。何を聞けばいいか分からない。だから、AIに質問もできない。何が分からないのかも分からない。本を読めと言われても、何を読めばいいか分からない。本屋の技術書コーナーに行けば、棚一面に並ぶ背表紙の圧に押しつぶされそうになる。結局、何も買わずに帰る。本は、そういう人に問いをくれる。「あ、これが分からなかったのか」。読み終わって初めて、自分が何を知らなかったのかが分かる。問いを持たない人間に、問いを渡す。それが、本にしかできないことなのだと思う。そういう人のために、10冊を選んだ。「若者にオススメ」と書いておきながら、自分もまだ若い方なのだと思う。少なくとも、将来の自分から見れば若い。ただ、激動の時代だ。技術だけ磨いていればいい時代は、終わりかけているのかもしれない。あるいは、もう終わっているのかもしれない。だから、技術以外の本も混ぜて紹介することにした。先に断っておく。私はバックエンドエンジニアやインフラエンジニアからキャリアをスタートさせた人間だ。だから、フロントエンドやネイティブアプリに関しては、ほぼ紹介しない。偏っている。偏っているが、自分が読んでいない領域の本を勧めることはできない。プログラミング言語個別の書籍も紹介しない。どの言語を学ぶかは人によって違う。だから、言語に依存しない本を中心に選んだ。この10冊が良い10冊かどうかは、分からない。私が良いと思った本が、誰にとっても良いとは限らない。だから、この記事を「正解」として読まなくていい。「こういう本があるんだな」という参考程度に。それでいいのだと思う。それから、もう1つ。本を買うお金がないなら、図書館で借りればいい。技術書は高い。1冊3000円、4000円は当たり前だ。まず読む。金は後でいい。読んで、良かったら、いつか買えばいい。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、本題に入る。技術の土台を作るまずは土台だ。プログラミングを始める前に、あるいは始めたばかりの頃に、IT業界で使われる言葉を知っておく必要がある。語彙がなければ、技術書も読めない。先輩の話も分からない。AIに質問もできない。1冊目：情報処理技術者試験の参考書（どれでもいい）1冊目から、いきなり「どれでもいい」と言うのは無責任に聞こえるかもしれない。でも、本当にそうなのだ。ITパスポートでも、基本情報技術者試験でも、応用情報技術者試験でも、高度試験でも。自分のレベルに合ったものを選べ。本屋で立ち読みして、7割くらい分かるやつを買え。分からなすぎると挫折する。簡単すぎると意味がない。誤解しないでほしい。資格を取れと言っているわけではない。「資格なんて意味ない」「資格より実務経験だ」——そういう声があるのは知っている。半分は正しい。資格を持っているだけでは、コードは1行も書けない。試験に受かっても、現場で即戦力にはなれない。それは分かっている。もっと言えば、試験に受からなくてもいい。俺は全然受からないのに優秀なソフトウェアエンジニアを死ぬほど知っている。資格の有無と実力は、必ずしも一致しない。でも、勉強するなら、頭に入った方がいいだろう。頭に入れるなら、試験を受けた方がいい。締め切りがあると、人は勉強する。試験日という締め切りがなければ、参考書は積読になる。金を払って申し込んで、日程を押さえて、会場に行く。その「仕組み」を使え。なぜ資格試験を勧めるのか。語彙が手に入るからだ。現場に出ると、専門用語が飛び交う。「スループットが落ちてる」「レイテンシがネックになってる」「冗長構成にしないと」「SLAどうする？」——こういう会話が、当たり前のように行われる。プログラミングはできるのに、この語彙がなくて会話に入れない。コードは書ける。でも、技術的な議論ができない。語彙がないと、会話にすら入れない。これは、よくある話だ。試験勉強を通じて、開発特有の語彙が頭に入る。ネットワーク、データベース、セキュリティ、プロジェクトマネジメント。知識として知っているだけで、会話の輪に入れる。「あ、それ試験で出たな」という感覚で、先輩の話が理解できる。試験の内容を全部覚えている必要はない。語彙が残ればいい。それだけで、現場での学習速度が全然違う。ここで正直に言う。実務経験の方が大事だというのは、その通りだと思う。本を読むより、コードを書いた方がいい。知識を詰め込むより、実際にシステムを動かした方がいい。2026年の今なら、分からないことはAIに聞けばいい。AIに疑問をぶつければ、理解も早く進む。でも、経験がなければ、疑問も生まれない。これは「経験を積め」という精神論ではない。構造の問題だ。語彙がなければ問いが立たず、問いがなければ経験を言語化できず、言語化できなければ次の学習に繋がらない。この悪循環を断ち切るには、どこかで語彙を入れるしかない。何を聞けばいいか分からなければ、AIも使いこなせない。「スループット」という言葉を知らなければ、「スループットが落ちている原因は何ですか」とは聞けない。「処理が遅い」と「スループットが低い」は、同じ現象を指しているように見えるが、後者の方が解決策にたどり着きやすい。なぜなら、「スループット」という言葉には、それを改善するための知識体系が紐づいているからだ。語彙は、学習の入り口だ。入り口がなければ、どんなに優秀なAIがあっても、中に入れない。IPA（情報処理推進機構）の試験は、日本のIT業界における共通言語を学ぶのに最も効率がいい。ネットワーク、データベース、セキュリティ、プロジェクトマネジメント、システム設計。全部、体系的にまとまっている。しかも、過去問が無料で公開されている。金がないなら、参考書すら買わなくていい。過去問だけで受かる人もいる。2026年度から、応用情報技術者試験や高度試験がCBT（Computer Based Testing）方式に移行する。これまで年2回、決まった日に会場に足を運ばなければならなかったのが、自分の都合に合わせて受験できるようになる。受験のハードルは確実に下がった。どの参考書がいいかは、正直、好みだ。キタミ式が好きな人もいれば、技術評論社の「合格教本」シリーズが好きな人もいる。Amazonのレビューを見て、自分に合いそうなのを選べばいい。図書館にあることも多い。もう1つ言っておく。ITに興味があるけど、プログラミングには興味がない。そういう若者は多いと思う。「エンジニアになりたいけど、コードを書くのはちょっと……」という人。そういう人こそ、まず資格を取れ。プログラミングができなくても、ITの世界で活躍する道はいくらでもある。インフラ、セキュリティ、プロジェクトマネジメント、ITコンサル。そのすべてにおいて、資格で得た知識と語彙は武器になる。繰り返す。資格を取ることが目的ではない。語彙を入れることが目的だ。語彙があれば、AIにも質問できる。語彙があれば、技術書も読める。語彙があれば、先輩の話も分かる。入り口を作れ。話はそれからだ。www.meti.go.jpシステムの基盤を理解するコードを書けるようになっても、それだけではシステムは動かない。サーバー、ネットワーク、データベース、OS。アプリケーションの下にあるレイヤーを理解しなければ、本番環境で動くものは作れない。ここでは、システムを支える基盤技術について学ぶ本を4冊紹介する。2冊目：バックエンドエンジニアのためのインフラ・クラウド大全コードを書けるようになった。アプリケーションが動くようになった。でも、本番環境にデプロイしようとすると、急に分からないことだらけになる。サーバーって何？ネットワークって何？クラウドって何？アプリだけ書けても、本番では動かせない。この本は、そのギャップを埋めてくれる。バックエンドエンジニアに求められるインフラ・クラウド領域の基礎知識が、1冊にまとまっている。情報システムの基礎から、可用性、キャパシティ、パフォーマンス、監視、セキュリティ、DevOps、SRE。現場で必要になる知識が、体系的に整理されている。全23章、544ページ。分厚いが、それだけの価値がある。「基礎知識」と聞くと、簡単そうに思えるかもしれない。でも、違う。基礎とは、簡単という意味ではない。基礎とは、すべての土台になるという意味だ。なぜこの混同が起きるのか。学校教育のせいだろう。教科書は「基礎→応用」の順に並んでいて、基礎は最初に習う、つまり簡単なものだと刷り込まれる。でも、実際には逆だ。基礎は最後に理解できる。応用を経験して初めて、基礎の意味が分かる。この本に書かれていることは、10年後も20年後も変わらない原則ばかりだ。最初は分からなくていい。分からないまま読み進めて、5年後に読み返したとき、「ああ、これはこういう意味だったのか」と分かる。それが基礎だ。構成も良い。分野ごとに解説がまとまっているが、章末で「あわせて読みたい」範囲が紹介されている。1つの章を読み終わると、「次はこっちも読んでみるか」となる。ちょっとだけ調べるつもりが1時間経っている。そういう本だ。クラウドネイティブな環境では、アプリケーションとインフラの境界が曖昧になっている。コンテナ、Kubernetes、オブザーバビリティ。これらを理解せずに、本番環境で動くシステムは作れない。「俺はアプリ側だから」では通用しない時代だ。この本は、その橋渡しをしてくれる。以前、自分が書いたアプリケーションを本番環境にデプロイしたとき、ローカルでは動いていたのに、本番では動かなかった。原因を調べるのに丸1日かかった。ネットワークの設定だった。そのとき、「アプリを書けるだけでは、本番では戦えない」と痛感した。この本があの頃の自分にあったら、もう少し早く原因にたどり着けたかもしれない。バックエンドエンジニアのためのインフラ・クラウド大全【リフロー型】作者:馬場 俊彰,株式会社X-Tech5翔泳社Amazon3冊目：SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策データベースは、難しい。でも、難しいのに、簡単にできてしまう。ORMを使えば、SQLを書かなくてもデータを取得できる。CREATE TABLE文を書けば、テーブルが作れる。動く。動いてしまう。だから、問題に気づくのが遅れる。テーブル設計の失敗は、ソースコードの失敗よりもリファクタリングが難しい。データが入ってしまってからでは、修正のコストが跳ね上がる。だから、最初から正しい設計を知っておく必要がある。この本は、データベースプログラミングで陥りがちな失敗（アンチパターン）を体系的にまとめた本だ。カンマ区切りで値を格納する「ジェイウォーク」。外部キーを張らない「キーレスエントリ」。1つのカラムに複数の意味を持たせる「マルチカラムアトリビュート」。NULLの扱いを間違える「アンビギュアスグループ」。名前を聞いただけで「あ、やったことある」と思う人は多いはずだ。第2版では、新規書き下ろしの章と15のミニ・アンチパターンが加わった。特にミニ・アンチパターンは実務的な内容が多く、「自分もこの問題にハマった」「こうやって解決した」と思える内容が詰まっている。それなりにエンジニアをやっていると、多くのアンチパターンは踏んだことがある。でも、それを他者に体系的に伝えるのは難しい。自分の設計がシステムにどのような影響を与えていくかを経験として学習する機会は、意外と少ない。だからこそ、この本で先人の失敗を学んでおく価値がある。不思議なことがある。ベストプラクティスを調べて実装しても、想定通りにならないことが多い。環境が違う、前提が違う、規模が違う。でも、アンチパターンは違う。アンチパターンを実装すると、想定通りに困る。なぜか。アンチパターンは「制約違反」だからだ。リレーショナルデータベースには設計原則がある。その原則を破れば、必ず不整合やパフォーマンス問題が起きる。ベストプラクティスは「この文脈では有効」という条件付きだが、アンチパターンは「どの文脈でも有害」という普遍性を持つ。だから、何をすべきかより、何をすべきでないかを学ぶ方が、確実に役に立つ。SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazon4冊目：モダンオペレーティングシステム 第5版（上・下）データベースの次は、さらに下のレイヤーだ。OSの話をする。OSの中身を知りたければ、この本を読め。プロセスとスレッド、メモリ管理、ファイルシステム、入出力、デッドロック、仮想化とクラウド、マルチプロセッサシステム、セキュリティ。OSを構成する要素が、網羅的に解説されている。上下巻合わせて1000ページ超。分厚いが、それだけの価値がある。コンピュータ・サイエンスの分野で世界的な定番となっている教科書だ。21年ぶりに日本語版が復活した。第5版では、Windows 11やSSDなど、最新のトピックまで詳しく解説されている。セキュリティの章は大部分が書き直された。各章末には585題もの演習問題がある。基礎知識の確認から、プログラミングや計算、さまざまな状況への対応まで。問題に取り組むことで、その章で学んだことの理解が深まる。上下巻で1万円を超える。学生には厳しい価格だ。だから言う。図書館で借りろ。大学の図書館には、たいてい置いてある。この本自体がなくても、類書は置いてある。以前、というかかなり昔にマルチスレッドのバグで丸2日を溶かしたことがある。ログを見ても再現しない。デバッガをつけると動く。原因はスレッド間のレースコンディションだった。そのとき、「なぜプロセスとスレッドが分かれているのか」「なぜロックが必要なのか」を、初めて本当に理解した。この本を先に読んでいたら、もう少し早く気づけたかもしれない。この辺はパタヘネ本など他にも良書があるのでそれらでもよい。モダンオペレーティングシステム 第5版 上作者:アンドリュー・S・タネンバウム,ハーバート・ボス日経BPAmazonモダンオペレーティングシステム 第5版 下作者:アンドリュー・S・タネンバウム,ハーバート・ボス日経BPAmazon5冊目：データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理OSの次は、分散システムだ。現代のアプリケーションは、1台のサーバーでは動かない。分散システム設計のあらゆるトピックを660ページに渡って網羅する、百科事典のような書籍。バックエンドエンジニアなら、いつかは読むべき本。データベース、レプリケーション、パーティショニング、トランザクション、分散システムの課題、バッチ処理、ストリーム処理。データを扱うシステムを設計する上で知っておくべき知識が、体系的に整理されている。この本の特徴は、何ができるか（WHAT）だけでなく、なぜそうなっているか（WHY）まで説明されていることだ。「なぜレプリケーションが難しいのか」「なぜ書き込み性能が高いマルチリーダーではなくシングルリーダーが広く使われているのか」。そういった「なぜ」を知ることができる。正直、難しい。分散システムに関わっていないと、なかなかピンとこない部分もある。入門として読む本ではない。でも、大規模でデータ量が多いアプリケーションを設計するときには、必ず役に立つ。2026年2月に原著の第2版が出版される予定だ。翻訳版も出てほしい。というか、出てくれ。頼む。この記事を定期的に更新するつもりなので、第2版が出たら差し替える。データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin Kleppmann,斉藤太郎,玉川竜司オライリージャパンAmazonプログラマーとしての姿勢を学ぶここまで、技術の土台とシステムの基盤について紹介してきた。ここからは、少し違う話をする。何を学ぶかではなく、どう向き合うかの話だ。技術は日々変わる。でも、変わらないものもある。良いコードを書くための考え方、問題に向き合う姿勢、キャリアを築くためのマインドセット。ここでは、プログラマーとしての「あり方」を教えてくれる本を紹介する。6冊目：達人プログラマー（第2版）熟達に向けたあなたの旅1999年に出版されて以来、世界中のプログラマーに読まれ続けている名著。2019年に20周年記念版として大幅に改訂され、第2版が出た。原題は「The Pragmatic Programmer」。Pragmaticとは、実用本位、実践的という意味だ。理論だけではなく、現場で使える知恵が詰まっている。この本の特徴は、コーディング技法だけでなく、エンジニアとしてのものの見方を教えてくれることだ。DRY原則、ETC原則（Easier To Change）、凝集度と疎結合。そういった技術的な話もあるが、それだけではない。開発の進め方、コミュニケーションの取り方、キャリアの考え方。プログラマーとして生きていくための姿勢が書かれている。「割れた窓」の話は有名だ。悪い設計、誤った意思決定、質の悪いコード。それを放置すると、ネガティブな考えが伝染する。だから、最初の「割れた窓」を見つけたら、すぐに直せ。自分もつい、割れた窓のようなコードを書いてしまったことがある。その後に若いプログラマに保守を任せたとき、いい書き方になっていなかった。元がよくない書き方だから、指摘するのも躊躇してしまう。「石のスープ」の話も印象的だ。大きな変化を一度に起こそうとすると、周囲は萎縮する。だから、小さく始めて、少しずつ巻き込んでいく。未来を少し垣間見せるだけで、みんな集まってくる。読み直すたびに、新しい発見がある。入門者には手引きとなり、ベテランでも読み返すたびに得るものがある。年に1回は読み返し、達人プログラマーを志していきたい。そういう本だ。20年以上読み継がれてきたからこそ、普遍的な価値がある。古い本だから読まなくていい、ということはない。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazon7冊目：プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則KISS、DRY、YAGNI、SOLID。プログラミングの世界には、先人たちが積み上げてきた原理原則がある。でも、それらを体系的に学ぶ機会は意外と少ない。現場で「DRYって何？」と聞かれて、ちゃんと説明できるだろうか。この本は、そういった原理原則を101個集めて、1冊にまとめたものだ。「3年目までに身につけたい」という副題がついているが、3年目以降の人が読んでも学びがある。むしろ、色々な現場を経験した人の方が、それぞれの原理原則の含蓄を感じられる。「あのとき、これを知っていれば……」と思うことが、きっとある。この本の特徴は、各項目に「なぜそれが必要か」が明確に説明されていることだ。Howだけでなく、Whyが書かれている。だから、抽象的な情報でありながら、実際に使える知識になる。「How to本」ならぬ「Why本」だ。もう1つの特徴は、各項目に出典書籍と関連書籍が記載されていることだ。「達人プログラマー」「アジャイルソフトウェア開発の奥義」「プログラマが知るべき97のこと」など、名著への参照がちりばめられている。次に読む本を選ぶときの索引としても使える。具体的なコード例がないことを不満に思う人もいるかもしれない。でも、それは意図的だ。言語に依存しないからこそ、どんな言語でプログラミングしていても適用できる。抽象度が高い分、適用範囲は果てしなく広い。本書で「抽象」を押さえたら、「具象」も押さえたい。コードの書き方を扱った本では、『リーダブルコード』（Dustin Boswell、Trevor Foucher著、2012年）が定番として挙げられることが多い。変数名の付け方、コメントの書き方、制御フローの整理。確かに実践的な内容だ。でも、私のおすすめは『ルールズ・オブ・プログラミング』（Chris Zimmerman著、2023年）の方だ。『ルールズ・オブ・プログラミング』は、『Ghost of Tsushima』を開発したSucker Punch Productionsで実際に使われている21のルールをまとめた本だ。「最適化の前に単純化せよ」「コードを制約で囲め」「プログラマーの時間はCPUの時間より貴重」。ゲーム開発という、パフォーマンスと保守性の両方が求められる過酷な現場で磨かれたルールには、説得力がある。syu-m-5151.hatenablog.comもし「リーダブルコードを読め」と勧めてくる人がいたら、「ルールズ・オブ・プログラミングは読みましたか？」と聞いてみてほしい。読んだ上でリーダブルコードを勧めているなら、それは信頼できる。読んでいないなら、まず読んでもらってから、改めて話を聞けばいい。プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則作者:上田勲秀和システム新社Amazon技術以外のスキルを身につけるプログラミングができればエンジニアとして成功できる。そう思っていた時期が、私にもあった。でも、現実は違う。あるプロジェクトで、技術的には正しい提案をしたことがある。でも、通らなかった。別のエンジニアの、技術的にはやや劣る提案が採用された。理由は「あいつの方が話しやすい」「あいつの言うことなら安心できる」だった。悔しかった。でも、それが現実だった。技術力だけでは、キャリアは伸びない。なぜか。2つの構造的理由がある。1つは、評価の非対称性だ。あなたの技術力を正しく評価できる人は、組織の中に何人いるか。CTOと数人の先輩エンジニアくらいだろう。でも、あなたのコミュニケーション力は、同僚全員が評価できる。評価が多数決に近い以上、「多くの人に見えるスキル」を持つ人が有利になる。もう1つは、レバレッジの問題だ。自分一人の技術力には限界がある。でも、他者を巻き込む力は、レバレッジが効く。10人を動かせる人は、自分1人で10倍の成果を出す人より、組織では重宝される。これが良いことかどうかは別として、構造としてそうなっている。だから、技術以外のスキルも身につける必要がある。8冊目：SOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版技術書ではない。でも、エンジニアにとって必読の1冊だ。この本のサブタイトルは「ソフトウェア開発者の人生マニュアル」。技術習得法やキャリア構築法だけでなく、セルフマーケティング、生産性、資産形成、フィットネス、マインドセット。人生全般をより良く生きる方法が書かれている。「技術者の地位は技術力の高さではなく、他者の評価で決まってしまう」。これは厳しい現実だ。でも、現実を直視した上で、どうすればいいかを教えてくれる。キャリアをビジネスとして捉え、自分自身をマーケティングする。そういう視点を持つことの重要性が説かれている。正直に言うと、後半の資産形成やフィットネスの章は、ソフトウェア開発者に特化した話題ではない。不動産投資や筋トレの話がかなり詳しく書かれていて、「それ、この本でそこまで書く必要がある？」と思う人もいるだろう。私もそう思った。読む人を選ぶ本、という感想もある。でも、前半のキャリア、セルフマーケティング、学習、生産性の章は、間違いなく読む価値がある。技術力だけでは生き残れない時代に、何を身につけるべきか。その指針を与えてくれる。既に読者が若手ソフトウェアエンジニアの場合にはソフトウェアエンジニアガイドブック―世界基準エンジニアの成功戦略ロードマップも合わせておすすめしたい。SOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon設計とアーキテクチャを深める技術以外のスキルも大事だ。でも、技術を疎かにしていいわけではない。むしろ、技術力があってこそ、それ以外のスキルが活きる。コードが書けるようになったら、次は設計だ。どうやってモジュールを分けるか。どうやってシステム全体を構成するか。設計の良し悪しが、システムの保守性を決める。ここでは、設計とアーキテクチャについて学ぶ本を2冊紹介する。9冊目：アーキテクトの教科書 価値を生むソフトウェアのアーキテクチャ構築「アーキテクトになりたい」「アーキテクトとして成長したい」。そう思ったとき、何から始めればいいのか分からない人は多い。相談できる先輩や上司が身近にいないこともある。この本は、アーキテクティングという世界を探検するにあたっての「地図」となる本だ。アーキテクトの「最初の1冊」として、これ以上のものはない。第2章「ソフトウェア設計」では、V字モデル、4つの抽象（アーキテクチャ設計、モジュール設計、コンポーネント設計、クラス設計）、SOLID原則、設計パターンと、設計を語っていく上での基本概念が密度高く語られる。この章だけでも読んでおけば、設計の話をするときに「何を言っているのか分からない」という状態にはならない。オライリーの『ソフトウェアアーキテクチャの基礎』も良書だが、どこかアカデミックさがあり、ある程度の前提知識が要求される。それに比べて本書は、初学者にも分かりやすく書かれている。ユースケースに沿った解説があるのでおすすめである。第6章「アーキテクトとしての学習と成長」も見逃せない。普段のプロジェクトの中で表立って取り上げられることの少ないテーマだ。「自分がアーキテクトになっていくためにどんな心構えが必要なのか」と悩んでいる人には、とても学びの多い内容になっている。アーキテクトの教科書 価値を生むソフトウェアのアーキテクチャ構築作者:米久保 剛翔泳社Amazon10冊目：ソフトウェア設計の結合バランス 持続可能な成長を支えるモジュール化の原則「疎結合にしろ」「密結合は悪だ」。そういうスローガンは、現場でよく聞く。でも、疎結合とは、具体的にどの程度が「疎」なのか。それを説明できる人は、意外と少ない。この本は、「結合」という概念を徹底的に掘り下げた本だ。本書の主張は明快だ。結合をゼロにすることは不可能であり、むしろ適切な結合を選択することが重要。「疎結合至上主義」ではなく、「結合の均衡化（Balancing Coupling）」という視点を提示している。構造化設計におけるモジュール結合、オブジェクト指向におけるコナーセンス。それらを一通り説明した後、独自の「統合強度」モデルが導入される。強度・距離・変動性の関係性を解き明かし、実際の設計においてそれらをどう均衡化するのかが、具体例を用いて示される。印象的だったのは、結合の「距離」という概念だ。同じ強度の結合でも、それが文レベル、メソッドレベル、オブジェクトレベル、サービスレベルのどこに存在するかによって、変更のコストが大きく異なる。マイクロサービスアーキテクチャの設計において、この視点は特に重要だ。この本は手順書でもルールブックでもない。この本に書かれている通りにモジュール設計をすれば自然とバランスの良い設計になる、という話ではない。でも、方針決定やレビュー時に迷ったとき、この本に書かれているような発想をインプットに意思決定すると、判断の精度が上がる。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonおわりに10冊を紹介した。この記事を読んだからといって、明日から何かが変わるわけではない。たぶん来週も、再来週も、同じような日々が続く。10冊すべてを読む必要もない。というか、いきなり10冊読み終わることなんてない。自分も、速読で済ませようとしたことがある。でも、身につかなかった。1冊読んで、合わなければ閉じればいい。それでいい。派手な近道はない。地味な積み重ねだけがある。常に今の自分で戦うしかない。1つだけ、注意しておきたいことがある。誰かを冷笑したり、バカにしたりするのは楽だ。でも、その道に未来はない。他人をバカにしない唯一の方法は、自分が自分の枠の中で精一杯頑張ることだ。精一杯やっている人間は、他人を笑っている暇がない。syu-m-5151.hatenablog.com私も、達人と呼ばれたい者の1人だ。まだ諦めているわけではない。諦めているわけではないが、達人になれるかどうかは分からない。分からないまま、コードを書いている。本を読んでいる。冒頭で、本は問いをくれると書いた。知らなかった世界を見せてくれると書いた。10冊のうち、どれか1冊でも手に取ってもらえたら、と思う。読み終わったとき、新しい問いが生まれているかもしれない。「あ、これが分からなかったのか」。そう思えたら、その本は、あなたにとって正解だったのだと思う。本との出会いは、計画だけでは起きない。本屋に行くと、紹介した本の隣に、もっと自分に合った本が置いてあるかもしれない。図書館で棚を眺めていると、別の本が目に入るかもしれない。そういう出会いは、検索では起きない。AIにも、たぶん見つけられない。だから、本屋に行ってみてもいいかもしれない。図書館に寄ってみてもいいかもしれない。棚の前に立ってみる。それだけでいい。何かが始まるかどうかは、分からない。分からないが、始まるとしたら、たぶんそこからだ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでOry Hydra用認証プロバイダーを実装する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/06/004244</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/06/004244</guid>
            <pubDate>Mon, 05 Jan 2026 15:42:44 GMT</pubDate>
            <content:encoded><![CDATA[はじめに年が明けた。月曜日。エディタを開いている。認証プロバイダーを自分で実装できるか、と聞かれたら、たぶん「できる」と答える。OAuth2のRFCは読んだ。フローも理解している、と思う。ただ、「じゃあ書いて」と言われたとき、キーボードに手を置いたまま止まってしまうことがある。頭では分かっている。手が動かない。10年近くインフラやプラットフォームを触ってきた。認可の仕組みは何度も設計した。Kubernetesの認証、サービスメッシュの認可、アクセストークンの検証。それでも「Login Providerをゼロから書け」と言われると、急に自信がなくなる。分かっているはずなのに、分かっていない気がする。知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazon知ってるつもり～「問題発見力」を高める「知識システム」の作り方～ (光文社新書)作者:西林 克彦光文社AmazonOry Hydraのドキュメントを開く。Login ProviderとConsent Providerを自分で実装しろ、と書いてある。Node.jsのサンプルがある。Goのサンプルもある。どちらも動く。でも私はRustで書きたかった。年末年始、ぼんやり考えていて気づいたことがある。止まっているのは、技術的に難しいからではない気がする。「何をどの順番で実装すればいいのか」が見えていないのだ。全体像が掴めないまま、最初の一歩が踏み出せずにいる。だからこの記事を書くことにした。過去の自分に向けて。最初の一歩を、順番に。前提知識: この記事は前回の記事の続編です。OAuth2認可コードフローの基礎知識と、Ory Hydraのアーキテクチャ（Login/Consent Providerの役割）を理解している前提で進めます。syu-m-5151.hatenablog.com作るものLogin/Consent Providerとは、Ory Hydraと連携してOAuth2認証フローを処理するWebアプリケーションだ。以下の5つのエンドポイントを実装する。 エンドポイント  役割  GET /login  ログインフォームを表示する  POST /login  認証処理を行い、Hydraに結果を通知する  GET /consent  スコープ承認画面を表示する  POST /consent  承認結果をHydraに通知し、トークン発行へ進む  GET /logout  ログアウト処理を行い、セッションを破棄する 全体の流れOAuth2認可コードフローの中で、Login/Consent Providerがどう動くかを示す。1. ユーザーがクライアントアプリで「ログイン」をクリック2. クライアントがHydraの /oauth2/auth にリダイレクト3. Hydra が Login Provider の GET /login にリダイレクト（login_challenge付き）4. Login Provider がログインフォームを表示5. ユーザーがメール・パスワードを入力して送信6. Login Provider が認証し、Hydra に accept_login を送信7. Hydra が Consent Provider の GET /consent にリダイレクト（consent_challenge付き）8. Consent Provider がスコープ承認画面を表示9. ユーザーが承認10. Consent Provider が Hydra に accept_consent を送信11. Hydra がクライアントにリダイレクト（認可コード付き）12. クライアントが認可コードをトークンに交換Login/Consent Providerが担当するのは3〜10だ。Hydraとの通信には6つのAPIを使う。 API  役割  GET /admin/oauth2/auth/requests/login  login_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/login/accept  認証成功をHydraに通知  GET /admin/oauth2/auth/requests/consent  consent_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/consent/accept  承認結果をHydraに通知  GET /admin/oauth2/auth/requests/logout  logout_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/logout/accept  ログアウトをHydraに通知 www.ory.comLogin HandlerLogin Handlerは2つのエンドポイントで構成される。GET /loginクエリパラメータからlogin_challengeを取得するHydra APIでlogin_challengeを検証し、リクエスト情報を取得するskipフラグが立っていれば（既にセッションがあれば）、フォームを表示せず即座にaccept_loginそうでなければログインフォームを表示するPOST /loginフォームからemail、password、login_challengeを受け取る認証サービスでパスワードを検証する認証成功なら、ユーザー情報をcontextに詰めてaccept_loginを呼ぶHydraが返すリダイレクトURLへ転送するpub async fn login_submit(    State(state): State<AppState>,    Form(form): Form<LoginForm>,) -> Result<Redirect, AppError> {    // 1. 認証処理    let user = state.auth.authenticate(&form.email, &form.password).await?;    // 2. ユーザー情報をcontextに保存（Consent時にDBルックアップ不要）    let user_context = UserContext {        email: user.email.clone(),        role: "customer".to_string(),        tenant_id: None,    };    // 3. Hydraに認証成功を通知    let completed = state        .hydra        .accept_login(            &form.login_challenge,            &user.id.to_string(),            false,            Some(serde_json::to_value(&user_context)?),        )        .await?;    // 4. Consent画面へリダイレクト    Ok(Redirect::to(&completed.redirect_to))}ポイントはcontextだ。Login時に認証したユーザー情報（email、role、tenant_id）をJSON形式で保存し、Consent Providerへ受け渡す。これにより、Consent処理でDBルックアップが不要になる。Consent HandlerConsent Handlerも2つのエンドポイントで構成される。GET /consentクエリパラメータからconsent_challengeを取得するHydra APIでリクエスト情報（要求されたスコープ、クライアント情報）を取得するskipフラグが立っていれば（既に承認済みなら）、即座にaccept_consentそうでなければスコープ承認画面を表示するPOST /consentフォームからconsent_challengeと承認するスコープを受け取るLogin時に保存したcontextからユーザー情報を取得するIDトークンにカスタムクレーム（email、role、tenant_id）を追加するaccept_consentを呼び、Hydraが返すリダイレクトURLへ転送するIDトークンにクレームを追加することで、クライアントアプリケーションはトークンをデコードするだけでユーザー情報を取得できる。Logout HandlerLogout Handlerは1つのエンドポイントで構成される。Login/Consentと比べてシンプルだ。GET /logoutクエリパラメータからlogout_challengeを取得するHydra APIでaccept_logoutを呼び出すHydraが返すリダイレクトURLへ転送するpub async fn logout_handler(    State(state): State<AppState>,    Query(query): Query<LogoutQuery>,) -> Result<Redirect, AppError> {    let completed = state.hydra.accept_logout(&query.logout_challenge).await?;    Ok(Redirect::to(&completed.redirect_to))}ログアウトフローはLogin/Consentと異なり、確認画面を表示せずに即座にaccept_logoutを呼んでいる。本番環境では「本当にログアウトしますか？」という確認画面を挟むことを検討してもよい。動作確認docker compose up -d./scripts/e2e-test.shIDトークンにemail、role、subが含まれていれば成功だ。ここまでが「何を作るか」「どう動くか」の説明だ。以降は実装の詳細に入る。認証サービスの実装Login Handlerから呼び出される認証サービスの実装に入る。パスワード認証にはOWASPのガイドラインに従い、Argon2idを採用した。cheatsheetseries.owasp.orgArgon2::default()を使っているが、これは意図的だ。argon2クレートのデフォルト値はOWASP推奨設定に準拠している。「専門家が作ったものを信頼する方が合理的」という前回の記事と同じ論理だ。認証部分で見落としがちなのが次の点だ。pub async fn authenticate(&self, email: &str, password: &str) -> Result<User, AppError> {    let users = self.users.read().await;    let user = users.get(email).ok_or(AppError::InvalidCredentials)?;    Argon2::default()        .verify_password(password.as_bytes(), &parsed_hash)        .map_err(|_| AppError::InvalidCredentials)?;    Ok(user.clone())}ユーザーが存在しない場合も、パスワードが間違っている場合も、返すエラーは同じInvalidCredentialsだ。「ユーザーが見つかりません」というエラーを返したくなるが、それは攻撃者に情報を与えてしまう。これはユーザー列挙攻撃（User Enumeration Attack）への対策だ。攻撃者はまず有効なメールアドレスを特定しようとする。エラーメッセージが違えば、登録済みかどうかが分かってしまう。なお、完全な対策にはタイミング攻撃への考慮も必要だ。ユーザーが存在しない場合はArgon2の検証が走らないため、レスポンス時間の差で存在を推測される可能性がある。本番環境では、ユーザー不在時もダミーハッシュを検証することを検討してほしい。owasp.orgテスト設計認証システムのバグは「静かに」起きる。だからテストの考え方も変わる。普通の機能開発では「この操作をしたらこうなる」というテストを書く。でも認証システムでは「この操作をしてもこうならない」というテストの方に価値がある。#[tokio::test]async fn test_login_does_not_reveal_user_existence() {    let service = AuthService::new();    service.register("exists@example.com", "password").await.unwrap();    let err1 = service.authenticate("exists@example.com", "wrong").await.unwrap_err();    let err2 = service.authenticate("nobody@example.com", "password").await.unwrap_err();    assert_eq!(err1.to_string(), err2.to_string());}このテストは「エラーメッセージが同じ」という実装の意図を明示化している。将来誰かが「親切なエラーメッセージにしよう」と思って変更しても、このテストが警告を出す。責任分界点全ての攻撃をアプリケーション層で防ぐ必要はない。何を守り、何をインフラに任せるかを明確にする。ブルートフォース対策: Nginxのrate limitで弾くセッション固定化攻撃: フレームワーク（Axum + tower-sessions）に委譲HTTPS強制: インフラ設定の問題プロジェクト構成今回はAxumを使った。github.comsrc/├── main.rs          # サーバーエントリーポイント├── auth.rs          # 認証サービス├── handlers.rs      # Login/Consent/Logoutハンドラー├── hydra.rs         # Hydra Admin APIクライアント├── models.rs        # Hydra API型定義└── error.rs         # エラー型定義ハンドラー層とサービス層を分離している。認証ロジックはauth.rsに置き、ハンドラーはHTTPリクエストの受け取りとレスポンスの返却だけを担う。フルコードはGitHubリポジトリを参照してほしい。github.com実装チェックリスト必須の実装[ ] Hydra APIクライアント - 6つのAPI呼び出し[ ] GET /login - login_challenge検証、skipフラグ確認、フォーム表示[ ] POST /login - 認証、contextにユーザー情報、accept_login[ ] GET /consent - consent_challenge検証、skipフラグ確認、承認画面表示[ ] POST /consent - context取得、IDトークンにクレーム追加、accept_consent[ ] GET /logout - logout_challenge取得、accept_logout[ ] 認証サービス - Argon2id、ユーザー列挙攻撃対策忘れがちなポイントlogin_challengeとconsent_challengeはhiddenフィールドでフォームに埋め込むskipフラグが立っている場合は画面を表示せず即座にacceptするcontextでLogin→Consent間のユーザー情報受け渡しエラーメッセージはユーザーの存在を漏らさないおわりにこの文章を書き終えて、ターミナルに戻った。docker compose up -dを叩く。コンテナが立ち上がる。E2Eテストを走らせる。グリーン。IDトークンにemailとroleが入っている。動いた。正直に言うと、書いている途中で何度か不安になった。これで説明になっているのか。Login HandlerとConsent Handlerの違いが曖昧になっていないか。contextの使い方は2回書き直した。それでも、動いた。冒頭で書いた「キーボードに手を置いたまま止まってしまう」感覚は、たぶん、また来る。次に認証システムを書くときも、OAuth2のフローを思い出すところから始めるだろう。login_challengeって何だっけ、と調べ直すかもしれない。それでいいのだと思う。認証は「一度理解したら終わり」という領域ではない気がする。毎回、RFCを確認しながら、慎重に実装する。ユーザー列挙攻撃のテストを書いたのも、将来の自分が「親切なエラーメッセージ」を入れようとしたときに止めるためだ。年が明けて、また仕事が始まる。本番の認証システムはOry Hydraに任せる。Login Providerは自分で書く。その境界線が、今の私には見えている気がする。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、辞めるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/05/090020</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/05/090020</guid>
            <pubDate>Mon, 05 Jan 2026 00:00:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめにかつての私は、深夜2時にベッドの中で転職サイトを開いていた。開いて、求人を眺めて、閉じて、また開く。そういうことを繰り返していた。辞めたいのか、と聞かれると困った。会社の限界が見えたのか。自分の天井が見えたのか。それとも、隣の芝生の青さに目が眩んでいただけなのか。たぶん、全部だった。たぶん、どれでもなかった。今は、転職を考えていない。これは「今の会社が最高だから」という話ではない。どんな会社にも良い面と悪い面がある。不満がゼロになることはない。ただ、深夜に転職サイトを開く衝動は、いつの間にか消えた。何が変わったのか。環境が変わったのか、自分が変わったのか。たぶん、両方だ。「エンジニアは転職で年収が上がる」「成長できる環境に身を置け」——そんな言葉がタイムラインに流れてくる。転職エージェントからのスカウトメールは週に何通も届く。カジュアル面談のお誘い。年収アップの可能性。もっと刺激的な環境。全部、本当のことだと思う。全部、嘘だとも思う。若いエンジニアが短期的にモノを考えてしまうのは、仕方がない。私もそうだった。目の前の不満が大きく見える。3年後、5年後のことなんて、想像できない。「今すぐ環境を変えたい」という衝動は、若さゆえの特権でもある。その衝動を否定するつもりはない。ただ、かつての自分に言いたいことがある。「おい、ちょっと待て」と。私自身、何度も転職を考えた。「もう限界だ」「ここにいても意味がない」「他の会社ならもっとできるはずだ」——そう思って、転職サイトを眺めた夜は数えきれない。そして、実際に転職したこともある。転職して正解だったケースもあった。「あのタイミングで辞めなくてよかった」と思うケースもあった。だから、この記事で「辞めるな」と書くのは、上から目線のアドバイスではない。かつての自分への手紙だ。あのとき、もう少し踏みとどまっていたらどうなっていたか。もう少し早く辞めていたらどうなっていたか。そういう問いを、今も抱えている。——もし読んでいて上から目線に感じたなら、それは私の力量不足だ。申し訳ない。ある日、気づいたことがある。深夜に転職サイトを開く自分と、翌朝それを後悔する自分は、同じ人間なのに、まったく違うことを考えている。どちらが本当の自分なのか。たぶん、どちらも本当だ。だから困る。この記事は、深夜の衝動と、翌朝の冷静さの、両方に向けて書いている。この記事が、辞めそうな若手に上司から共有されないことを祈っている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しい。「転職しやすい」という罠ITエンジニアは「転職しやすい職業」だと言われる。確かにそうだ。求人は多い。売り手市場だ。スキルがあれば、転職先を見つけることは比較的容易だろう。だが、「転職しやすい」ことと「キャリアを作れる」ことは、全く別の話だ。私自身、この罠にはまった。転職市場で「引く手あまた」だった時期がある。スカウトメールは毎週届いた。カジュアル面談をすれば、たいてい次のステップに進めた。「自分は市場価値が高い」と思っていた。でも、それは錯覚だった。振り返ると、私は「転職できる」ことと「キャリアを積み上げている」ことを混同していた。転職市場で需要があるのは、単に「エンジニアが足りない」からだ。私個人の価値が高いわけではない。需要と供給のバランスが崩れているだけ。その状況に甘えて、「いつでも転職できる」という安心感に浸っていた。「3年で転職すれば年収が上がる」という話もある。だが、これは単純化しすぎた話だ。実際には、年収が上がる転職もあれば、上がらない転職もある。そして、年収が上がらない転職の方が、実は多い。なぜか。転職には必ずロスが発生するからだ。私が転職したとき、最初の3ヶ月は本当に苦しかった。前職では「あいつに聞けば分かる」と言われていた領域があった。コードベースを熟知していた。誰に何を聞けばいいか知っていた。暗黙のルールも把握していた。転職した瞬間、それが全部ゼロになった。会議で発言しても、「この人、誰？」という空気が流れる。提案しても、文脈を知らないから的外れになる。前職では30分で終わる作業が、3時間かかる。「俺はもっとできるはずなのに」——そう思いながら、毎日を過ごしていた。これが「転職のロス」だ。どんなに経験者であっても、新しい会社のコンテキストをつかむには時間がかかる。前職で積み上げた信頼貯金は、転職した瞬間にリセットされる。私がこの記事で伝えたいのは、現場で働いてきた人間としての実感だ。机上の空論ではなく、実際に転職を経験し、成功も失敗もしてきた中で気づいたことを書く。一見「転職しやすい」ように見えるITエンジニアほど、実は「キャリアを作ること」が難しい——これが私の結論だ。転職のハードルが低いからこそ、安易に転職してしまう。そして、キャリアが積み上がらないまま、年齢だけが積み上がっていく。ただ、ここまで書いてきて、誤解されたくないことがある。「辞めたい」と思うのは、悪いことではない「転職には罠がある」と書いた。でも、それは「辞めたいと思うこと自体が悪い」という意味ではない。ここで1つ、大事なことを言っておきたい。「辞めたい」と思うこと自体は、悪いことではない。むしろ、自然なことだ。どんな会社にも、良い面と悪い面がある。仕事には波がある。うまくいく時期もあれば、何をやってもダメな時期もある。人間関係でストレスを感じることもある。深夜2時に転職サイトを眺める。上司との関係がうまくいかなくて、帰りの電車で「もう嫌だ」と思う。日曜の夜、明日会社に行きたくないと感じる。そういう瞬間は、誰にでもある。私にもあった。今でもある。だから、この記事を読んで「辞めたいと思っている自分はダメだ」とは思わないでほしい。辞めたいと思うことと、実際に辞めることは、別の問題だ。ただ、この分離は言うほど簡単ではない。深夜2時に転職サイトを見ているとき、「これは感情だ、今は判断するな」と冷静に思える人がどれだけいるだろうか。私自身、何度も感情に流されて判断しそうになった。だから、私は自分にルールを課している。1回目で決めるな。深夜のベッドで「辞めたい」と思った。それは1回目だ。まだ決めるな。翌週、上司に理不尽なことを言われて「辞めたい」と思った。まだ決めるな。1ヶ月後、半年後、同じ状況で同じことを思うか。時間をかけて、何度も問い直せ。衝動ではなく、熟慮の末に出した答えなら、それが「辞める」でも「残る」でも、後悔は少ない。要するに、短期ではなく長期で考えろ、ということだ。目の前の感情に振り回されるな。5年後、10年後の自分がどうなっていたいか。そこから逆算して、今の決断を考えろ。正直に言えば、3年程度では何も身についていない。「3年経験があります」と言っても、それは今の環境が整っている状況で、その能力が発揮できる程度だ。上司が調整してくれて、先輩がフォローしてくれて、チームが支えてくれて、ようやく成果が出せている。その支えがなくなった瞬間、同じパフォーマンスが出せるか。出せないなら、それは本当に「能力」と呼べるのか。感情は感情として受け止めていい。ただ、その感情だけで大きな決断をしないでほしい。この記事は、そのための材料を提供したいと思っている。では、冷静に考えるとは、具体的に何を考えればいいのか。次に目指す役割を明確にするまず最初に考えるべきは、「次にどこへ向かいたいのか」だ。エンジニアのキャリアには、いくつかの方向性がある。技術を深める方向——テックリードやスペシャリストだ。特定の領域で「この人に聞けば分かる」と言われる存在になる。アーキテクチャの意思決定を任される。難しい技術的課題を解決する。人を率いる方向——エンジニアリングマネージャー（EM）だ。チームの生産性を最大化する。メンバーの成長を支援する。採用や評価といった組織課題に向き合う。事業に近づく方向——プロダクトマネージャーや、ビジネスサイドとの橋渡し役だ。「何を作るか」を決める側に回る。技術とビジネスの両方を理解し、最適な解を見つける。ここで強調しておきたいのは、IC（Individual Contributor）トラック——部下を持たずに技術で貢献し続けるキャリアパス——という選択肢の存在だ。スタッフエンジニア、プリンシパルエンジニアといった役職は、マネージャーにならずとも、より大きなインパクトを生み出す道だ。マネジメントだけが「上」ではない。シニアの先には4つの方向性がある。テックリード（チームの技術方針を導く）、アーキテクト（システム設計の意思決定を担う）、ソルバー（組織横断の難問を解決する）、ライトハンド（経営層の右腕として動く）。どれを目指すかで、求められるスキルセットも変わる。全部できる必要はない。どれを選ぶかは、あなた次第だ。重要なのは、スタッフエンジニアは「シニアのシニア」ではないということだ。役割そのものが変わる。コードを書く時間は減り、リーダーシップ、ファシリテーション、組織の接着剤としての仕事が増える。「もっとコードを書きたい」という人には向かない道だ。だから、「シニアになったら自動的にスタッフを目指す」という発想は危険だと私は思っている。多くのエンジニアは、最初は「一人前の開発者」からスタートする。そこから、どの方向に進むか。それを決めるのは、あなた自身だ。ここで自分に問いかけてほしい。あなたは次にどの方向に進みたいのか。それが言語化できていないなら、転職を考えるのはまだ早い。なぜなら、方向が定まっていない転職は、ただの「移動」に過ぎないからだ。移動しても、キャリアは積み上がらない。方向性を考えることと同じくらい大事なことがある。「自分は今、どこにいるのか」を知ることだ。自分の能力を棚卸しする目指す方向が見えてきたとしよう。でも、その方向に進むためには、今の自分の立ち位置を正確に把握する必要がある。転職を考えるとき、多くの人は外側に目を向ける。「あの会社は良さそうだ」「この技術を使ってみたい」「あの人みたいになりたい」。でも、本当に大事なのは、自分という器がどうなっているかを知ることだ。どんなに良い環境に移っても、器が変わらなければ、入ってくるものは同じだ。逆に、自分の器をちゃんと理解していれば、今の環境でも次の環境でも、適切な選択ができる。ここで、転職を考える前に確認してほしいことがある。自分の「実力」を正しく評価できているか、ということだ。私は長い間、この評価を間違えていた。ゾーンに入って神がかった速度でコードを書く自分、難解なバグを一瞬で特定する自分——そういう「最高の瞬間」を「自分の実力」だと信じていた。だから、転職先でも同じパフォーマンスが出せると思っていた。逆だった。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その泥のような日に絞り出したアウトプット。それこそが、紛れもない私の「実力」だ。絶好調のときの成果は、再現性のない「運」や「上振れ」に過ぎない。転職先で、その「上振れ」を再現できる保証はどこにもない。なぜこれが転職を考えるときに重要なのか。信頼は「下限」に支払われるからだ。新しい職場で、あなたは「最高の自分」ではなく「最悪の自分」で評価される。慣れない環境、知らないコードベース、初対面のチームメンバー。その状況で出せるアウトプットが、あなたの「実力」として記録される。「本当はもっとできるんです」は通用しない。だから、転職先を選ぶときに問うべきは、「最高の自分が活躍できる場所か」ではない。「最悪の自分でも、最低限のパフォーマンスを出せる場所か」だ。もう1つ、能力について知っておくべきことがある。能力は文脈の中にしかない。今の環境で「できる人」だとしても、それは文脈に依存している。私自身、痛い目を見た。あるプロジェクトで成果を出せたとき、私はそれを自分の実力だと思っていた。でも振り返ると、違った。上司が事前に関係者と調整してくれていた。マネージャーがスコープを適切に切ってくれていた。先輩が技術的な地雷を踏む前に教えてくれていた。私は、応援してくれて、調整してくれていたマネージャーや上司の能力まで、自分の能力だと勘違いしていた。その支えが消えた環境で、同じパフォーマンスを出せるか。出せるわけがない。正しい認識はこうだ。「この文脈において、これまでの経験と周囲のサポートが噛み合って、たまたま価値が出せている」。では、その「器」——能力——は、どう捉えればいいのか。大きく分けて3つの軸がある。技術力——コードを書く力だ。設計力、実装力、レビュー力。特定の領域を深掘りする「スペシャリスト」か、複数の領域をカバーする「ジェネラリスト」か。どちらを目指すにせよ、ここが基盤になる。推進力——プロジェクトを前に進める力だ。タスクを完遂できるか。障害にぶつかっても解決策を見つけられるか。チームのボトルネックを解消できるか。「なぜこの機能が必要か」というビジネス課題を理解し、技術的な意思決定をビジネスインパクトで説明できるか。影響力——自分の外側に価値を生み出す力だ。チームへの影響力は、採用、オンボーディング、ドキュメント整備、勉強会の開催など。社外への影響力は、技術ブログ、カンファレンス登壇、OSS貢献など。どの軸を伸ばすかは、目指す役割によって変わる。テックリードを目指すなら技術力と推進力。EMを目指すなら推進力と影響力。スペシャリストを目指すなら技術力を極める。重要なのは、全部を上げようとしないことだ。自分が目指す役割に必要な能力を見極めて、そこに集中する。ここで、私自身の失敗を話したい。かつての私は「良いコードを書いていれば、いつか評価される」と思っていた。技術力さえあれば、周りが認めてくれる。黙々と良い仕事をしていれば、誰かが見ている。——甘かった。現実はこうだ。見えない仕事は、存在しないのと同じ。どんなに素晴らしい設計をしても、それを言語化して共有しなければ、誰も知らない。どんなに難しいバグを直しても、「大変だった」と伝えなければ、簡単な修正だと思われる。「仕事をやり遂げる人」として認められるには、技術的な能力だけでなく「何が重要かを見極める力」と「自分の仕事を周囲に伝える力」が必要だ。この2つを、私は長い間、軽視していた。「アピールするのは恥ずかしい」「実力で示せばいい」——そう思っていた。でも、それは傲慢だった。相手の時間を奪わずに、自分の仕事の価値を簡潔に伝えること。それはコミュニケーションスキルであり、チームで働く上での基本的な作法なのだ。つまり、私は「技術力」に過剰投資し、「推進力」と「影響力」に過少投資していた。多くのエンジニアは、同じ罠にはまる。新しいフレームワークを学ぶ。新しい言語を触る。それは楽しいし、成長した気になる。だが、「推進力」——泥臭い調整や、やり切る力——の不足から目を背けていないか。技術力があっても、プロジェクトを完遂できなければ、市場価値は上がらない。今の会社を辞めようとしているあなた。この3つの軸で自分を評価してみてほしい。次に目指す役割に対して、どの軸が足りていないのか。それが明確になっていないなら、転職しても同じ困難にぶつかる。環境を変えても、足りない能力は足りないままだ。ただ、ここで1つ付け加えたいことがある。能力を棚卸しするとき、多くの人は「足りないもの」ばかりを見る。私もそうだった。「技術力が足りない」「推進力が弱い」「影響力がない」——チェックリストを見て、できないことを数え上げる。そして、転職先を探すときも「ここに行けば○○が身につく」「あの会社なら△△を学べる」と、ないものを補う発想で動いてしまう。ないものを探し続けていたら、悩みは一生消えない。考えてみてほしい。どんな環境に行っても、足りないものは必ずある。新しい技術が次々に出てくる。上には上がいる。「あれもできない、これもできない」と数え上げれば、キリがない。そうやって「ないもの」を埋めようとしている限り、永遠に充足感は得られない。私自身、この罠に長い間はまっていた。「もっとコードが書けるようになりたい」「もっとコミュニケーション力をつけたい」「もっとビジネス視点を持ちたい」——足りないものリストは常に更新され続けた。そして気づいた。そのリストは、一生埋まらない。発想を変えよう。「ないものを探す」のではなく、「あるものを伸ばす」。あなたには、すでに強みがある。周囲より得意なことがある。それが何かを見極めて、そこに集中する。弱みを平均まで引き上げる努力は、強みを突き抜けさせる努力より、はるかに効率が悪い。私の場合、「調べること」「言語化すること」「ソフトウェアを実装すること」が比較的得意だった。コミュニケーション力が高いわけではない。政治的な立ち回りも苦手だ。でも、RFCやドキュメントを読み込んで理解し、それを実際に動くコードに落とし込み、さらに文章としてまとめることなら、周囲より少しだけ速かった。その「少しだけ」を、徹底的に伸ばすことにした。結果として、「あいつに任せれば、調べて、作って、ドキュメントにしてくれる」という評価が生まれた。これは戦略的な選択だ。何をやるかではなく、何をやらないか。弱みを気にして、あれもこれもと手を広げるのではなく、強みに絞って、そこで突き抜ける。だから、能力を棚卸しするとき、「足りないもの」だけでなく「すでにあるもの」にも目を向けてほしい。転職を考えるとき、「ここに行けば足りないものが補える」ではなく、「ここに行けば今の強みがさらに活きる」という視点で選んでほしい。足りないものは、一生足りない。だから、足りないものを数えるのをやめろ。今あるものを、もっと伸ばせ。正直に告白しよう。私には、仕事を選ぶときの悪い癖がある。小さなバグを直す。ドキュメントの誤字を修正する。チェックリストを埋めていく。1日の終わりに「今日も色々やった」と思える。でも、週末に振り返ると、本当にインパクトのある仕事をしたのか、分からなくなる。——これが、私の悪い癖だ。簡単で達成感はあるが、インパクトの低い仕事に逃げてしまう。お菓子をつまむように、小さなタスクをつまんでしまう。これが「スナッキング」だ。チェックリストを埋める快感は、脳にとって報酬だ。でも、その報酬に溺れて、本当に重要な仕事——曖昧で、難しくて、すぐに結果が出ない仕事——から逃げていないか。もう1つ、自分を戒めている罠がある。目立つが価値の低い仕事だ。社内の勉強会を頻繁に開く。Slackで積極的に発言する。目立つ。注目を集める。でも、ビジネスへの貢献は薄い。この罠にはまると、「忙しかった」と「成果を出した」を混同するようになる。振り返ってほしい。直近1ヶ月で、最もインパクトのあった仕事は何だったか。それに費やした時間は、全体の何割だったか。もし1割以下なら、残りの9割は「スナッキング」だった可能性がある。ここまで、「どこを目指すか」と「何を伸ばすか」について話してきた。では、実際に転職するとなったとき、何を失い、何を得るのか。その前に、転職を考えるときの大前提を確認しておきたい。「自分は会社にとって必要な存在だ」と思っているかもしれない。でも、それは本当だろうか。「替えが効く」という前提を認める別に会社なんていつ辞めても良い。文字通りの意味で替えの効かない人間なんて資本主義においては存在しない。これは冷徹な事実だ。どんなに優秀なエンジニアでも、会社は回る。あなたが辞めても、誰かが引き継ぐ。プロジェクトは続く。組織は適応する。「私がいないと回らない」——そう思いたい気持ちは分かる。でも、それは幻想だ。私自身、これを認めるのに時間がかかった。ある会社を辞めるとき、「自分がいなくなったら、あのシステムは誰がメンテするんだろう」と心配していた。3ヶ月後、元同僚に聞いた。「全然大丈夫だよ。○○さんが引き継いで、むしろ前より整理されてる」。——少し寂しかったが、同時にホッとした。そして気づいた。私は「替えが効かない」と思いたかっただけだ。この事実を認めることは、絶望ではない。むしろ、解放だ。「替えが効かない」と思い込んでいると、会社に縛られる。「私がいないと困る」「今辞めたら迷惑をかける」——そういう責任感は美しいが、それが「辞められない」という足枷になることがある。ブラックな環境でも我慢してしまう。メンタルを壊しても「今は辞められない」と言い聞かせる。替えが効くと認めることで、初めて「辞める」という選択肢が本当の意味で手に入る。ただし、ここで短絡的な結論に飛ばないでほしい。「替えが効く」→「だから辞めてもいい」——これは論理の飛躍だ。「替えが効く」から導ける結論は、もう1つある。「だから、どこに行っても価値を出せる能力を磨け」だ。会社にとって、あなたは替えが効く。だが、あなたにとって、積み上げた実績は替えが効かない。ここが重要だ。会社はあなたを手放せる。次の人を雇えばいい。でも、あなたが2年間かけて積み上げた信頼、ドメイン知識、人間関係——これは、転職した瞬間にリセットされる。会社にとっては「替えが効く」リソースでも、あなたにとっては「替えが効かない」資産なのだ。だから、問いはこうなる。「会社にとって替えが効く」という事実を認めた上で、「自分にとって替えが効かない資産」をどれだけ積み上げたか。信頼の複利、実績の蓄積、ドメイン知識——これらは「会社のため」に積み上げるのではない。「自分のため」に積み上げる。たまたま、その資産が今の会社で活きているだけだ。転職すれば、その一部はリセットされる。リセットされてでも得たいものがあるなら、辞めればいい。リセットするには惜しい資産があるなら、もう少し留まって、その資産を使い切ってから辞めればいい。「替えが効く」という事実は、転職を正当化する理由にも、現職に留まる理由にもなる。どちらの結論を導くかは、あなた次第だ。大事なのは、この事実を、感情的な決断の言い訳に使わないことだ。「どうせ替えが効くんだから、辞めてもいいでしょ」——それは、考えることを放棄している。「替えが効くからこそ、自分の資産を最大化する選択をする」——それが、戦略的な判断だ。この前提を踏まえた上で、いよいよ転職のコストについて考えよう。「替えが効く」からこそ、転職は自由にできる。だが、自由にできるからといって、コストがゼロなわけではない。転職は「投資」であり「リセット」である若さという資源は有限だ。私たちはキャリアを積む中で何かを投資し、その結果として何かを得ている。この構造を理解しないまま転職を繰り返すのは危険だ。20代の私は、この構造を理解していなかった。「若いうちは色々経験した方がいい」「転職で視野が広がる」——そういう言葉を真に受けて、2〜3年ごとに環境を変えていた。確かに視野は広がった。でも、振り返ると、広く浅くなっただけだった。新卒で未経験のうちは何もない。あるのはポテンシャルであり、若さであり、可能性だ。その資源を使い、何かしらの資産を得る必要がある。何を得るのか。それはスキルであり、それを活用した先の実績だ。実績は資産だ。そして資産には複利が効く。ある領域で実績を出すと、次はもう少し大きな仕事が回ってくる。それをこなすと、さらに大きな仕事が来る。「あの人はこの領域で結果を出した」という評判が、次の機会を連れてくる。これが複利だ。私が見てきた「キャリアがうまくいっている人」は、例外なくこの複利を回していた。1つの実績が次の実績を呼び、雪だるま式に大きくなっていく。逆に言えば、転職するたびにこの複利がリセットされる。転職するたびに、一定のロスが発生する。ビジネスドメインの理解、社内の人間関係、意思決定のプロセス、暗黙知として共有されている文化。これは、転職した瞬間にリセットされる。信頼貯金も同様だ。前職で積み上げた「あいつなら任せられる」という信頼は、新しい会社では通用しない。ゼロから積み上げ直す必要がある。この「リセットコスト」を、転職を考えるときに計算しているだろうか。私は、転職のリセットコストを「半年〜1年」と見積もっている。新しい環境でコンテキストをつかみ、信頼を積み上げ、本来のパフォーマンスを発揮できるようになるまでの時間だ。転職した直後の、あの居心地の悪さを覚えているだろうか。私が転職して最初の1週間、Slackの雑談チャンネルを眺めていた。前職では、私も会話の輪に入っていた。誰かが投稿すれば、すぐにリアクションをつけた。冗談を言えば、笑ってくれる人がいた。でも新しい会社では、誰も私のことを知らない。雑談チャンネルに何か書こうとして、やめた。「この人、誰？」と思われるのが怖かった。些細なことだ。でも、あの孤独感は今でも覚えている。前職では「あいつに聞けば分かる」と頼られていたのに、新しい会社では誰も自分を知らない。会議で発言しても、反応が薄い。提案しても、「この人は何者だ？」という目で見られる。チャットで質問しても、返事が遅い。——あの感覚は、信頼貯金がゼロになった瞬間だ。これが「信頼の貯金」だ。具体的に言おう。「あの件、○○さんに頼んでおけば大丈夫」——そう思われるまでに、どれだけの時間がかかっただろうか。最初は小さな仕事を任される。それを期限通りに、期待以上の品質で納める。次は少し大きな仕事を任される。また納める。この繰り返しで、「この人なら任せられる」という信頼が積み上がっていく。信頼があると、仕事が回りやすくなる。他のチームに協力を頼むとき、「あの人の頼みなら」と動いてもらえる。提案するとき、「あの人が言うなら、一度聞いてみよう」と耳を傾けてもらえる。逆に信頼がないと、どんなに正しいことを言っても、「あの人、誰？」で終わる。周囲があなたと一緒に働きたいと思う度合いが、あなたの成功を直接左右する。そして、この信頼の貯金は、転職した瞬間にゼロにリセットされる。前職で「あの人は信頼できる」と思われていても、新しい会社では関係ない。ゼロから積み上げ直すしかない。今の会社で、信頼貯金はどれくらい貯まっているか。その信頼貯金を使ってできる挑戦は、まだ残っていないか。せっかく貯めた信頼貯金を、使わずに捨てるのは、もったいなくないか。ここで、信頼貯金のROI（投資対効果）を考えてみてほしい。今の会社で積み上げた信頼があるからこそ挑戦できる「高難易度・高リターン」の仕事はないか。新規プロジェクトの立ち上げ。技術的負債の解消。チームの構造改革。こういう挑戦は、信頼がなければ任されない。信頼があるからこそ、「あいつに任せてみよう」となる。転職先で得られる期待値は、このリセットコストを支払ってでも余りあるほど高いか。その根拠は何か。「なんとなく成長できそう」ではなく、具体的に何を得られるのか。それを言語化できなければ、転職は「期待値の高い投資」ではなく、「よく分からないギャンブル」になる。ここまで、転職のコストについて話してきた。では、そのコストを支払う価値があるかどうかを判断するために、何を見ればいいのか。それは、今の場所で何を積み上げたか、だ。現職で何を成し遂げたか転職を考えるとき、多くの人は「次に何をしたいか」を考える。でも、その前に考えるべきことがある。現職で何を成し遂げたかだ。きつい言い方をする——これは私自身への言葉でもあるのだが——。転職する時に現職で主体的に動いて成し遂げた実績が語れなければ、現職の経験はエンジニアキッザニアに近い。シニアエンジニアやCTOが用意してくれた環境で、お膳立てされた仕事をこなしていただけ。新しいスキルが身についたとする。それは素晴らしい。でも、それだけでは足りない。そのスキルを使って、どのようなビジネス価値を出したのか。その過程でどう主体的に関わったのか。これが語れなければ、あなたは「お客さん」のままだ。もちろん、「キッザニア」も大事だ。お膳立てされた環境で体感したことは血肉になる。でも、それでいいのはある段階までだ。年収700万円、800万円、その先を目指すなら、「遊ばせてもらう側」から「遊び場を作る側」に回る必要がある。技術力だけでは昇進できない——これは誰でも言える。問題は、なぜ、分かっていても実践できないのかだ。「コードで問題を解決する」。それが私たちのアイデンティティだ。だから、可視化やスポンサー獲得を「政治的で汚い」と感じてしまう。「実力で認められたい」。その気持ちは痛いほど分かる。私もそうだった。でも現実は違う。技術的に正しい提案をしても、周囲を巻き込めなければ、提案は提案のまま終わる。「技術で解決できる」ことと「解決を任される」ことは、別の能力だ。私自身、昇進を見送られた経験がある。なぜ評価されないのか分からなかった。振り返って気づいた。上司が私のキャリア目標を察してくれることを、勝手に期待していた。「昇進したいです」と言ったことがあっただろうか。なかった。上司はエスパーではない。言わなければ、伝わらない。そしてもう1つ。技術的な正しさを組織に浸透させるのも、「技術」だ。相手の立場を理解し、伝わる言葉で説明し、合意を形成する。これを「政治」と呼ぶなら、政治もまた技術なのだ。そして、成果を出すだけで終わりではない。私は日報をつける習慣を大事にしている。Claude Codeを使って、日々の作業を記録している。何をやったか、何を学んだか、何に詰まったか。こうして記録しておけば、パフォーマンスレビューの自己評価で圧倒的に有利になる。半年前、1年前に何を達成したか、正確に思い出せるだろうか。記録がなければ、自分の成果を過小評価してしまう。成果を出すことと、成果を可視化することは、別のスキルだ。昇進には「スポンサー」と「可視化」が必要だ。スポンサーとは何か。あなたの成果を経営層に伝えてくれる人だ。上司や先輩の中に、「あいつは良い仕事をしている」と会議で言ってくれる人はいるか。人事評価の場で、あなたの名前を出してくれる人はいるか。いくら良い仕事をしても、上層部に伝わらなければ、昇進の話にはならない。スポンサーは単なる応援者ではなく、あなたのキャリアに実際に投資してくれる存在だ。可視化とは何か。自分の仕事の価値を、他人が理解できる形で残すことだ。「何を達成したか」「なぜそれが重要だったか」「組織にどう貢献したか」——これをドキュメントやSlackで発信しているか。戦略的に重要なプロジェクトに参加して、名前を売っているか。これが揃って初めて、「この人を昇進させよう」という話になる。ネットワークも重要だ。社内の同僚、社外のプロフェッショナル、経営層——この3方向の人脈を意識的に育てることで、キャリアの選択肢が広がる。転職を考えるなら、この3つのネットワークがどれだけ育っているか、自問してみてほしい。今、辞めようとしているあなたに問いたい。現職で、あなたは何を成し遂げたか。主体的に動いた結果として、何が変わったか。もし自分がその場にいなかったとしたら、結果はどう変わっていたか。「自分がいたからこそ生まれた差分」を言語化できるか。それが語れないなら、まだ辞めるタイミングではないかもしれない。少なくとも、もう一度自分に問いかける価値はある。ここで、よく聞く反論がある。「現職で成し遂げたいけど、もう成長の機会がないんです」——本当だろうか。この「成長できない」という感覚を、もう少し掘り下げてみたい。「成長できない」は本当か「もうこの場所では成長できない」これは、転職理由としてよく聞く言葉だ。刺激がなくなった。慣れてしまった。自分よりできる人がいない。だから、成長するために環境を変えたい。でも、本当にそうだろうか。それは本当に環境のせいなのか。厳しいことを言う。「成長できない環境」なんて、ほとんど存在しない。あるのは、今の自分の能力では打破できない環境だ。それは環境の問題ではなく、能力の問題だ。能力があれば、たいていの環境は打破できる。「この環境では無理だ」と言っているのは、「今の自分には無理だ」と言っているのと同じだ。だからこそ、転職には意味がある。——逆説的に聞こえるかもしれないが、聞いてほしい。能力を上げてから転職すれば、次の環境も打破できる。能力を上げずに転職しても、また同じ壁にぶつかる。「この環境では成長できない」と言って転職した人が、次の会社でも同じことを言っているのを、何度も見てきた。環境を変えても、能力が変わらなければ、結果は同じだ。逆に、今の環境で壁を打破する力をつけた人は、どこに行っても通用する。転職は「逃げ場」ではなく「能力を活かす場」として選ぶべきだ。今の環境で能力を証明してから、その能力をより活かせる場所に移る。それが、転職を「飛躍」にする唯一の方法だ。では、ここで言う「能力を上げる」とは、具体的に何を指すのか。そもそも「成長」とは何なのか。成長とは何か。新しい技術を触ることか。新しいフレームワークを学ぶことか。それらは成長の一部ではあるが、本質ではない。成長とは、「解ける問題の範囲が広がること」であり、「より大きな責任を担えるようになること」だ。シニアエンジニアへの成長で最も重要なのは、「どの問題を解くべきかを見極める力」だ。コードで問題を解くことと、そもそも「どの問題を解くべきか」を判断することは、まったく別のスキルだ。私自身、この違いを理解するのに時間がかかった。ある時期、私は「新しい技術を触れていないと成長が止まる」と焦っていた。業務ではレガシーなコードをメンテしている。新しいことを学べていない。だから成長していない。そう思い込んでいた。でも振り返ると、あのレガシーコードのメンテナンス期間こそ、私が最も成長した時期だった。複雑に絡み合った依存関係を解きほぐす力。ドキュメントがない状況で調査する力。リスクを見積もって段階的にリファクタリングする判断力。これらは、最新技術を追いかけていたら身につかなかった。その定義で考えたとき、今の環境で成長の余地は本当にないのか。もしかしたら、自分が「成長」と呼んでいるものが、単なる「刺激」ではないだろうか。新しい技術を触る刺激。新しいチームに入る刺激。新しいプロダクトに関わる刺激。刺激と成長は違う。刺激は消費されるが、成長は蓄積される。私が「成長できない」と感じていたとき、本当は「刺激がない」だけだった。成長の機会は目の前にあった。ただ、それが「地味でつまらない仕事」に見えていたから、気づかなかった。ここで、よく言われる教えについて考えてみたい。「一番の下手くそでいよう（Be the Worst）」——プログラマーの世界でよく引用される教えだ。自分より優れた人たちの中に身を置くことで、自分も成長できる。だから、自分が一番下手くそになれる環境を探せ、と。この教えは正しい。でも、これを全員が実践したら、組織は成り立たない。全員が「学ぶ側」を求めて、誰も「教える側」に回らなかったら、どうなるか。優秀な人が集まる環境は、誰かが「教える側」を引き受けてくれているから成立している。「一番の下手くそでいよう」という教えは、その前提を無視している。——というのは、批判としては正しい。ただ、この教えの本質は、「常に学び続けろ」ということだ。「教える側」に回っても、学びは止まらない。むしろ、教えることで自分の理解の穴が見つかる。成長の形が変わるだけで、成長自体は続く。「もうこの場所では成長できない」と感じたとき、立ち止まって考えてほしい。自分は「学ぶ側」でいることしか考えていないのではないか。新しい技術を教わりたい。優秀な先輩からコードレビューを受けたい。それは大事だ。だが、いつまでも「教わる側」にいるわけにはいかない。「教える側」に回ったとき、別の成長が始まる。後輩のコードをレビューすることで、自分の理解の穴が見つかる。ドキュメントを整備することで、暗黙知が言語化される。勉強会を開くことで、チーム全体の底上げができる。そして何より、「自分がいないと回らない」から「自分がいなくても回る」状態を作ることが、次のステージへの準備になる。「接着剤の仕事」というものがある。チーム間の調整、ドキュメント整備、後輩の面倒を見る——コードを書かないが、チームを機能させるために不可欠な仕事だ。日本企業では、この仕事は評価されにくい。「○○さんはコード書いてないよね」と言われがちだ。でも、シニアレベルでこれをやると「リーダーシップを発揮している」と見なされることもある。上司とすり合わせて、この仕事がキャリアにどう評価されるか確認しておいた方がいい。評価されないなら、やりすぎは損だ。効果的なメンタリングとは何か。良いメンターはすぐに答えを与えない。複数の選択肢を提示し、メンティー自身に考えさせる。そして、自立を促す。メンタリングを受ける側も、答えを教えてもらうことを期待するのではなく、自分で考える姿勢が求められる。もし今の環境で良いメンターがいるなら、それは転職で失う大きな資産の1つだ。今の環境で、より大きな責任を担う機会はないか。より難しい問題に挑戦する機会はないか。それを探さずに「成長できない」と言っているなら、次の環境でも同じことが起きるだろう。ここまで、「成長できない」という感覚について掘り下げてきた。成長の機会は、案外、目の前にあるかもしれない。ただ、それでも「辞めたい」という気持ちが消えない人もいるだろう。次の問いは、より厳しいものになる。転職は「逃げ」になっていないか転職を繰り返す人の中に、あるパターンがある。新しい会社に入る。最初の半年は必死でキャッチアップする。コードベースを読み、ドメイン知識を吸収し、チームの信頼を獲得する。1年が経つ頃には「だいたい分かった」という感覚が出てくる。そして、ふと気づく。「あれ、最近あまり成長していない気がする」。ここで選択肢が2つある。今の環境で次のステージに挑戦するか、また新しい環境に移るか。後者を選び続けると、こうなる。キャッチアップが終わるたびに「成長が止まった」と感じ、また次の会社に行く。新しい環境でのキャッチアップを「成長」だと錯覚する。でも、それは成長ではない。ただの適応だ。本当の成長は、適応が終わった後にある。その環境で自分なりの仮説を持ち、試行錯誤し、失敗し、そこから学ぶ。大きなプロジェクトをやり遂げる。チームを任される。技術的な意思決定を下す。そういう経験を積んで初めて、次のステージに進める。転職を繰り返すたび、この「本当の成長」への到達前にリセットがかかる。結果、いつまでも「一人前の開発者」のまま、年齢だけが進んでいく。私自身、このリセットの苦しさを身をもって経験した。自社開発からSRE支援の会社に転職したとき、リセットが1回では済まないことを思い知った。支援先が変わるたびに、文脈がリセットされる。コードベース、チームメンバー、組織文化——全部ゼロから。しかも「支援」として来ている以上、キャッチアップ期間なんてない。初日から「で、何ができますか？」と問われる。最初は本当に苦しんだ。広い視野は得られたが、深さが積み上がらない。ある現場で得た知見を次の現場で活かそうとしても、文脈が違いすぎて通用しない。そして何より、信頼の蓄積がリセットされ続ける。ある支援先で信頼を獲得しても、次の案件ではまたゼロからだ。この経験から学んだことがある。転職のリセットコストは、転職先の業態によって大きく変わる。自社開発から自社開発への転職なら、リセットは1回で済む。でも、支援会社やコンサル、技術顧問に転職すると、リセットが繰り返し発生する。その覚悟があるかどうか、転職前に考えておくべきだ。この経験を通じて、私が学んだ原則がある。「自分の決定の結果を見届けられるだけの期間、同じ場所に留まれ」。成長のフィードバックループを回すためだ。設計した仕組みが半年後にどう使われているか。提案した施策が1年後にどんな結果を生んだか。それを見届けずに次の環境に移ったら、学びは半分で終わる。もう1つ、「許可を求めるな、宣言しろ」という原則がある。「○○してもいいですか？」ではなく、「○○します。問題があれば教えてください」と発信する。異論があれば誰かが止めてくれる。このスタイルで動けるようになると、権限がなくても物事を前に進められる。日本企業では「根回し」が重要だと言われる。それは間違いではない。でも、根回しにも2種類ある。「許可を得るための根回し」と「宣言を通すための根回し」だ。後者の方が、物事が前に進む。逆に、常に許可を求めないと動けない状態なら、まだその環境で信頼貯金が足りていない。その信頼を積み上げる前に辞めるのは、もったいない。ここで、このセクションの問いに戻ろう。「転職は『逃げ』になっていないか」。「今の環境では成長できない」と感じたとき、一度立ち止まって考えてほしい。それは本当に環境の限界なのか。それとも、環境には問題がないのに、難しいことから逃げているだけではないか。——私自身も、この問いに何度も向き合ってきた。そして正直に言えば、「逃げ」だったこともある。「退屈だが重要な課題」を解決することから目を背けて、「新しくて刺激的な環境」に逃げたくなる気持ちは、痛いほど分かる。ここまで、「今の環境で成長できるか」について話してきた。では、環境を変えるにせよ、留まるにせよ、これからのエンジニアは何を磨くべきなのか。AIと共存する時代に何を磨くかこの問いを考えるとき、避けて通れないのがAIの存在だ。AIは、定型的な作業を得意とする。コードの自動生成、バグの検出、ドキュメントの作成。これらの領域では、すでにAIが人間を補助し、場合によっては代替し始めている。つまり、「言われたことをそのまま実装する」だけのエンジニアは、価値が下がっていく。一方で、AIに代替されにくい領域もある。技術的な意思決定を下すこと。チームを率いること。ビジネス課題を理解し、技術で解決策を提案すること。曖昧な要件を整理し、実装可能な形に落とし込むこと。これは、当面の間、人間の仕事だ。私が優れた組織で見てきた共通点がある。エンジニアがビジネスに直接触れていることだ。「ITとビジネスの橋渡し役」を介さず、エンジニア自身がビジネス指標を理解し、顧客と対話する。その直接的な接点が、AIには代替できない価値を生む。逆に言えば、「要件を受け取って実装するだけ」のエンジニアは、AIに代替されやすい。これは他人事ではなく、私自身も常に意識していることだ。だが、ここで短絡的な結論に飛ばないでほしい。「じゃあ、転職してシニアなポジションを取りに行こう」というのは間違いだ。なぜなら、シニアになるためには、ジュニアとしての経験が必要だからだ。問題は、「ジュニアのまま留まり続けること」だ。今の環境で、次のステージに進むための挑戦ができるなら、そうすべきだ。転職は、その挑戦ができない場合の、最後の手段であるべきだ。ここで自分に問いかけてほしい。直近1ヶ月で、「人間が介入しなければ解決しなかった意思決定」を何回行ったか。AIがコードを書ける今、「実装する」だけでは価値が出にくい。曖昧な要件を整理する。ステークホルダー間の調整をする。技術的な選択肢の中から、ビジネスインパクトを考慮して決断する。そういう「人間にしかできない仕事」をどれだけやっているか。それがシニアへの階段を登る経験だ。ここまで、「どの方向に進むか」「何を磨くか」「今の環境で成長できるか」について話してきた。キャリアを考えるとき、避けて通れない話がもう1つある。転職を考える動機として、最も頻繁に挙がるテーマだ。「年収を上げたい」は目的ではなく結果である転職理由として「年収を上げたい」はよく聞く。分かる。私だって年収は高い方がいい。だが、年収は目的ではなく、結果だ。「年収は結果」と言うのは簡単だ。でも、転職サイトを開くと、年収で検索してしまう。なぜか。年収は分かりやすい指標だからだ。「能力が上がった」は測りにくい。「年収が上がった」は明確だ。この分かりやすさの罠が、私たちを「能力より年収」に引き寄せる。対策は1つ。年収以外の「分かりやすい指標」を自分で設定することだ。「○○の技術を導入した」「△△人のチームをリードした」「□□の問題を解決した」——そういう指標を先に決めておけば、年収の誘惑に負けにくい。転職サイトを開く前に、「この転職で得たいもの」を3つ書き出してみてほしい。そのうち「年収」が1番目に来るなら、一度立ち止まる必要がある。年収は、あなたが提供できる価値の対価だ。技術力が高ければ、難しい問題を解ける。推進力があれば、プロジェクトを成功に導ける。影響力があれば、チームや組織を良い方向に動かせる。これらの価値を提供できるから、高い年収が払われる。年収600万円から800万円、800万円から1000万円。それぞれのステージを超えるには、提供できる価値のレベルを上げる必要がある。「一人で開発できる」から「チームをリードできる」へ。「技術的な問題を解ける」から「ビジネス課題を技術で解決できる」へ。企業によって「シニアエンジニア」の意味は違う。大手IT企業とスタートアップでは、同じ肩書きでも求められる水準が全く異なる。1000人規模の会社のシニアと、10人のスタートアップのシニアでは、経験してきた課題の複雑さも、責任の範囲も違う。同じ「シニア」でも、会社によって期待値が違う。ここで正直に振り返りたい。キャリアの進め方について、私は無自覚だった。一生懸命働けば、報酬は自然についてくるものだと思っていた。「会社が見ていてくれる」「評価されるべき人は評価される」——そう信じていた。でも、それは間違いだった。努力だけでは、次のレベルに到達できない。技術を磨くことと、キャリアを戦略的に構築することは、別のスキルなのだ。日本企業では「出る杭は打たれる」と言われるが、「出なさすぎる杭」は存在すら認識されない。逆に言えば、能力を上げずに年収だけ上げようとしても、無理がある。高年収の会社に転職できたとしても、その期待値に応えられなければ、いずれ居場所を失う。私自身、この罠に片足を突っ込んだことがある。ある時期、市場が過熱していた。エンジニアの採用難で、年収相場が跳ね上がっていた。転職サイトを見ると、今の年収より明らかに高いオファーがゴロゴロしている。「自分の市場価値はこんなに高いのか」と浮かれていた。でも、冷静に考えれば分かる話だった。それは「私の価値」ではなく、「市場のバブル」だった。実際に転職した人の話を聞くと、入社後に苦しんでいるケースが少なくなかった。「この年収なら、これくらいできるだろう」という期待に応えられない。前職では周囲のサポートがあったから成果が出せていたのに、新しい環境では1人で同じ成果を求められる。結果、評価が下がり、居心地が悪くなる。中には、年収ダウンで再び転職した人もいた。年収アップの転職で失敗する人には、共通点があった。「年収が上がる＝自分の価値が認められた」と解釈していたことだ。でも、採用側の論理は違う。「この年収を払えば、このくらいの成果が出るはずだ」という投資判断をしている。年収は「認定」ではなく「期待値」なのだ。その期待値に応えられなければ、厳しい現実が待っている。ここで、提示された年収アップのオファーについて冷静に考えてほしい。その年収は、あなたの「現在の実力」に対する評価なのか。それとも、市場のバブルや採用の緊急度による「プレミアム（下駄）」なのか。下駄を履いた状態で入社すると、期待値の調整で苦しむ。「このくらいできるだろう」という期待に応えられず、評価が下がり、居心地が悪くなる。そのリスクをどう管理するか。年収だけを見て決めると、この罠にはまりやすい。だから、「年収を上げるために転職する」のではなく、「能力を上げた結果として年収が上がる」という順序を間違えてはいけない。そして、能力を上げるためには、今の環境で何ができるかをまず考えるべきだ。ここで、転職を考えるときに気をつけてほしいことがある。「年収アップ」という言葉に惹かれて、転職エージェントの話を聞き始める人は多い。だが、エージェントの言葉を聞く前に、知っておくべきことがある。転職エージェントのビジネスモデルを理解する転職エージェントは、あなたの味方ではない。これは悪口ではなく、ビジネスモデルの話だ。転職エージェントにお金を払っているのは、あなたではない。採用企業だ。エージェントは、あなたを企業に紹介し、採用が決まったときに、企業から報酬を受け取る。その報酬は、あなたの年収の一定割合だ。つまり、エージェントにとって、あなたが「転職すること」が利益になる。あなたが「現職に残ること」は、彼らには何のメリットもない。むしろ、売上ゼロだ。だから、エージェントは転職を勧める。「今の会社に残った方がいい」とは、なかなか言ってくれない。彼らの言葉をそのまま鵜呑みにするのは危険だ。エージェントを使うなとは言わない。彼らは市場の情報を持っているし、面接対策のアドバイスもくれる。ただ、彼らのインセンティブ構造を理解した上で、話を聞くべきだ。本当に転職すべきかどうかは、エージェントではなく、あなた自身が決めることだ。できれば、利害関係のない第三者——信頼できる先輩、友人、メンター——に相談してほしい。ここで厳しいことを言う。自分のキャリアの最終責任者になれ。日本企業では、「会社がキャリアパスを用意してくれる」という期待がある。年功序列で昇進できる。上司が適切なアサインメントを考えてくれる。人事部がキャリア相談に乗ってくれる。——しかし、それは幻想だ。あなたのキャリアの最終責任者は、上司やエージェントや人事部ではなく、あなた自身だ。誰かが導いてくれるのを待つのではなく、自分で方向を決めて、自分で動く。その覚悟があるかどうかが、キャリアを作れるかどうかの分かれ目になる。自分でキャリアを管理するために、私が大事にしている習慣が2つある。1つは、時間管理より体力管理だ。同じ1時間でも、元気なときと疲れているときでは、アウトプットが全く違う。燃え尽きそうな状態で長時間働いても、成果は出ない。自分の体力がどこで回復し、どこで消耗するかを把握することが、長く働き続けるための鍵だ。もう1つは、フィードバックを受け入れる力だ。「それは違うと思います」と言われたとき、どう反応するか。防御的にならず、「なるほど、そういう見方もあるのか」と学びに変えられる人が、成長し続けられる。「自分は正しい」と固まった人は、どんなに優秀でも、そこで成長が止まる。ここまで、「辞めるな」「考えろ」と書き続けてきた。読んでいて息苦しくなった人もいるかもしれない。だから、バランスを取っておきたい。転職が正解だったケースも、確かにあるからだ。転職して正解だった人たちここまで「辞めるな」と書いてきたが、一方的になりすぎただろう。転職して正解だった人も、たくさんいる。私の知り合いにも、転職がキャリアの転機になった人がいる。大企業からスタートアップに移って、2年で技術力が飛躍的に伸びた人。逆に、スタートアップから大企業に移って、大規模開発の経験を積んだ人。マネジメント志向だったのに、転職先でスペシャリストとして開花した人もいる。1人の話をしよう。彼は大企業で5年間、安定したキャリアを積んでいた。評価も悪くなかった。でも、「このまま10年後も同じことをしているのか」という問いが、ずっと頭の片隅にあったという。彼が転職を決めたのは、「逃げたい」からではなかった。「自分の手でプロダクトを作りたい」という明確な欲求があった。大企業では、どうしても歯車の一部になる。意思決定に関われるのは、ずっと先の話だ。彼は、その「ずっと先」を待てなかった。転職先は、20人規模のスタートアップだった。最初の3ヶ月は地獄だったと言っていた。前職では当たり前だったインフラが何もない。ドキュメントもない。聞ける人もいない。「俺、何やってるんだろう」と思った夜もあったらしい。でも、半年後に変化が起きた。自分が設計したアーキテクチャが、本番環境で動き始めた。ユーザーからのフィードバックが、直接Slackに届くようになった。「自分の仕事が、誰かの役に立っている」——その実感が、すべてを変えたと言っていた。彼が転職で成功したのは、運が良かったからではない。辞める前に、「次に何を得たいか」が明確だったからだ。「今の環境が嫌だから」ではなく、「次の環境でこのスキルを得たい」「この経験を積みたい」という具体的な理由で動いていた。これは、私が見てきた「転職で成功した人たち」に共通する特徴だ。ここで視点を切り替えてみたい。「今の仕事への期待値は下げ、キャリアにはもっと期待しよう」。今の仕事で完璧を求めすぎない。すべての仕事が理想的であるはずがない。でも、キャリア全体では高い目標を持つ。3年後、5年後にどうなっていたいか。この視点の切り替えが、良い転職をした人たちの特徴だった。そして、もう1つ。彼らは辞める前に、現職でやれることをやり切っていた。「ここでやれることはやった」という実感があった。だからこそ、次の環境で活かせる実績と経験を持って移れた。転職が正解になるかどうかは、転職先の問題ではない。辞める前に何を積み上げたかの問題だ。だから、この記事で伝えたいのは「絶対に辞めるな」ではない。「辞める準備はできているか」を問え、ということだ。ただし、ここで1つ付け加えておきたい。準備とは関係なく、すぐに辞めるべきときがある。そのタイミングを見誤ると、取り返しのつかないことになる。それでも辞めるべきタイミングここまで「辞めるな」と書いてきた。でも、辞めるべきタイミングは確かにある。そして、それは「自分の問題」ではなく、「環境の問題」であることも多い。メンタルや身体が壊れそうなときは、今すぐ辞めろ。これだけは絶対だ。キャリアよりも健康が大事だ。あなた個人に対するリスペクトを感じない会社や現場からは、即刻立ち去るべきだ。そこで無理をする必要はない。一方的に消耗させられる必要もない。我慢して壊れてからでは遅い。組織の構造的問題があるときも、辞めていい。これは重要なポイントだ。個人の努力では変えられない問題が、組織には存在する。いくつか例を挙げる。評価制度が機能していない——成果を出しても正当に評価されない。声が大きい人だけが昇進する。透明性がない。技術的負債が放置されている——経営層が技術投資を理解せず、ひたすら機能追加だけを求める。改善の余地がない。権限と責任が一致しない——責任だけ押し付けられて、決定権がない。何を提案しても却下される。人間関係の構造が壊れている——特定の人物によるハラスメント。派閥争い。コミュニケーションの断絶。会社の方向性に共感できない——ビジョンが見えない。または、見えたビジョンが自分の価値観と合わない。これは、あなたの責任ではない。どんなに努力しても、個人で変えられない問題はある。「もっと頑張れば変えられるはず」と思って消耗し続ける必要はない。構造的問題を個人の努力で乗り越えようとするのは、無理ゲーだ。今の環境で目指す役割に挑戦する機会がどうしても得られないときも、辞め時だ。組織の構造上、テックリードのポジションがない。マネジメントのポジションがない。専門性を深める機会がない。そういう時は、環境を変える必要がある。私が辞め時だと思う明確なサインがある。「学びたい意欲はあるのに、実際には学べていない」状態だ。技術を深めたい、新しいことに挑戦したい——その気持ちはある。でも、日々の仕事は同じことの繰り返し。成長の機会がない。もう1つのサインは、「スキルではなく、対処法を学んでいる」状態だ。技術力が上がっているのではなく、「この上司にはこう報告すればいい」「この会議はこうやり過ごせばいい」という政治的なサバイバルスキルばかりが磨かれている。これは危険信号だ。その環境で得られるものは、もう得尽くした可能性が高い。しかし、一点だけ確認してほしい。本当に機会がないのか、自分が機会を見逃していないか。機会は待っていても来ない。自分で作り出すものだ。作り出そうとしたけど本当に無理だった——そう言えるなら、転職は正しい選択だ。一方で、こういう時は立ち止まってほしい。「なんとなく飽きた」「刺激がない」「成長できない気がする」——こういう漠然とした不満だけで辞めようとしているなら、一度考えてみてほしい。それは本当に環境の問題なのか。自分の姿勢の問題ではないのか。辞める理由が「環境の構造的問題」なら、辞めていい。辞める理由が「自分の漠然とした不満」なら、もう少し掘り下げてみてほしい。その違いを見極めることが大事だ。ここで1つ、厳しい問いを投げかけたい。「この会社では無理だ」という結論に至るまでに、組織のボトルネックに対して具体的な改善提案や行動を何回試みたか。「評価制度がおかしい」と感じたなら、上司やHRに具体的な改善案を提案したか。「技術的負債が放置されている」と感じたなら、解消のためのロードマップを作って経営層に説明したか。試行回数がゼロなら、それは「構造の問題」ではなく「食わず嫌い」かもしれない。失敗してもいいから、一度は試みてほしい。試みた上で無理だったなら、辞める判断は正しい。ここまで、様々な角度から転職について考えてきた。辞めるべきとき、辞めるべきでないとき、その判断基準を見てきた。最後に、これまでの内容を整理して、問いかけの形にまとめておきたい。転職を決断する前に転職を考えているあなたに、最後に問いかけたい。まず、方向性は明確か。テックリードを目指すのか、EMを目指すのか、スペシャリストとして深掘りするのか。次に進みたい方向が言語化できていなければ、転職は単なる「移動」に終わる。技術力、推進力、影響力のうち、今の自分に足りないものは何か。それを伸ばす機会が、本当に今の環境にはないのか。転職すれば自動的に成長できるわけではない。次に、積み上げたものを使い切ったか。転職には必ずリセットコストがかかる。信頼の貯金はゼロに戻る。ドメイン知識も、人間関係も、リセットされる。その代償を払ってでも得たいものは何か。今の会社で、信頼の貯金を活用してできる挑戦はもうないのか。信頼があるからこそ任される大きな仕事を、やり残していないか。現職で主体的に動いて成し遂げた実績を語れるか。「自分がいたからこそ生まれた差分」を説明できるか。そして、冷静に判断できているか。「成長できない」のは本当に環境のせいか。それとも、難しい課題から逃げているだけではないか。転職理由が「年収を上げたい」だけになっていないか。年収は結果であって、目的ではない。転職エージェントのアドバイスを鵜呑みにしていないか。彼らは転職させることでお金をもらっている。利害関係のない第三者——信頼できる先輩、友人、メンター——に相談したか。「一人前の開発者」から次のステージに進めているか。それとも、キャッチアップを繰り返しているだけではないか。すべてに明確な答えを持っている必要はない。だが、1つも考えたことがないなら、まだ転職を決断する段階ではない。おわりにこの記事で言いたかったことは、結局、1つだけだ。短期的にモノを考えるな。目の前の不満。今月の年収。来週の上司との関係。そういうものに振り回されて、衝動的に決断するな。3年後、5年後、10年後の自分がどうなっていたいか。そこから逆算して考えろ。若いエンジニアが短期的に考えてしまうのは、仕方がない。私もそうだった。目の前の不満が世界のすべてに見える。「今すぐ環境を変えたい」という衝動を抑えられない。それは、若さゆえの特権でもある。でも、その特権には代償がある。転職を繰り返すたびに、信頼の貯金はリセットされる。キャリアの複利は止まる。「いろんな経験を積んだ」と言えば聞こえはいいが、どこにも根を張れないまま、年齢だけが積み上がっていく。私は、そういう未来を避けたかった。転職は、逃げにもなるし、飛躍にもなる。同じ「辞める」という行動でも、その意味は正反対になりうる。違いを決めるのは、辞める前に何を考えたか。それだけだ。1つだけ、問いを残しておく。もし今の会社の嫌な部分——人間関係や評価制度——がすべて解消されたとしたら、それでもなお、その新しい会社に行きたいと心から思えるか。YESなら、それは「攻め」の転職だ。NOなら、それは高度に正当化された「逃げ」かもしれない。逃げが悪いとは言わない。ただ、逃げを「攻め」の物語ですり替えていないか、正直な気持ちで自分に問いかけてほしい。深夜2時、ベッドの中で転職サイトを開いたとき。その衝動を否定はしない。ただ、その衝動のまま動くな。翌朝、もう一度考えろ。1週間後、もう一度考えろ。それでもなお、辞めたいと思うなら、そのときは辞めればいい。正直に言えば、「正解」なんてない。辞めても、残っても、どちらが正しかったかは、誰にも分からない。分かるのは、ずっと後になってからだ。そして、その「正しさ」は、最初から存在していたわけではない。選んだ道を、正解にしていく過程があるだけだ。おい、考えろ。短期ではなく、長期で考えろ。そして、選んだら、それを正解にしろ。続編を書きました。syu-m-5151.hatenablog.com参考書籍ＩＴエンジニアの転職学　２万人の選択から見えた、後悔しないキャリア戦略 (ＫＳ科学一般書)作者:赤川朗講談社Amazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社Amazonスタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazonスタッフエンジニアの道 ―優れた技術専門職になるためのガイド作者:Tanya Reillyオーム社AmazonNINE LIES ABOUT WORK　仕事に関する９つの嘘作者:マーカス・バッキンガム,アシュリー・グッドールサンマーク出版Amazon世界標準のフィードバック　部下の「本気」を引き出す外資流マネジメントの教科書作者:安田 雅彦SBクリエイティブAmazonみんなのフィードバック大全作者:三村 真宗光文社Amazonネガティブフィードバック　「言いにくいこと」を相手にきちんと伝える技術作者:難波 猛アスコムAmazonロバート・キーガンの成人発達理論――なぜ私たちは現代社会で「生きづらさ」を抱えているのか作者:ロバート・キーガン,中土井僚,鈴木規夫英治出版Amazon「人の器」の磨き方　リーダーシップ・コーチングと成人発達理論による人間力の変容プロセス作者:加藤洋平,中竹竜二日本能率協会マネジメントセンターAmazon「人の器」を測るとはどういうことか　成人発達理論における実践的測定手法作者:オットー・ラスキー,中土井僚日本能率協会マネジメントセンターAmazon組織も人も変わることができる！　なぜ部下とうまくいかないのか　「自他変革」の発達心理学作者:加藤洋平日本能率協会マネジメントセンターAmazon人が成長するとは、どういうことか作者:鈴木規夫日本能率協会マネジメントセンターAmazonあなたはなぜ雑談が苦手なのか（新潮新書）作者:桜林直子新潮社Amazon世界の一流は「雑談」で何を話しているのか作者:ピョートル・フェリクス・グジバチクロスメディア・パブリッシング（インプレス）Amazon「何を話していいかわからない」がなくなる　雑談のコツ作者:ひきた よしあきアスコムAmazon雑談の一流、二流、三流作者:桐生 稔明日香出版社Amazon雑用は上司の隣でやりなさい――あなたの評価を最大限に高める「コスパ最強」仕事術作者:たこすダイヤモンド社Amazon資本主義が人類最高の発明である：グローバル化と自由市場が私たちを救う理由作者:ヨハン・ノルベリニューズピックスAmazon資本主義は私たちをなぜ幸せにしないのか (ちくま新書)作者:ナンシー・フレイザー,江口泰子筑摩書房Amazon資本主義はなぜ限界なのか　――脱成長の経済学 (ちくま新書)作者:江原慶筑摩書房Amazon資本主義にとって倫理とは何か作者:ジョセフ・ヒース,瀧澤弘和慶應義塾大学出版会Amazon]]></content:encoded>
        </item>
    </channel>
</rss>