<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Thu, 29 Feb 2024 18:31:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[酸いも甘いもある Shared VPC（共有VPC） ~ GKE をShared VPC で構築する際の苦悩~]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/1818360b9c7821</link>
            <guid>https://zenn.dev/yokoo_an209/articles/1818360b9c7821</guid>
            <pubDate>Thu, 29 Feb 2024 07:23:09 GMT</pubDate>
            <content:encoded><![CDATA[このブログは、【Google Cloud】GDG Tokyo Monthly Online Tech Talksにて発表した内容を元にしています。登壇時の資料はこちらになります。https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao はじめにGoogle Cloud Shared VPC（共有VPC）はネットワークリソースの集中管理ができる一方で、リソース利用に関して制約があったり、エンプラ企業...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Astroでブログを作ってる]]></title>
            <link>https://abnoumaru.com/tech/2024-02-27-create-blog-from-astro-template/</link>
            <guid>https://abnoumaru.com/tech/2024-02-27-create-blog-from-astro-template/</guid>
            <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AstroのBlog templateをベースに自分が欲しい機能を検討してブログ作ってる]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Container Apps Jobs を Self-hosted GitHub Actions Runner として使う]]></title>
            <link>https://blog.1q77.com/2024/02/container-apps-jobs-self-hosted-github-actions-runner/</link>
            <guid>https://blog.1q77.com/2024/02/container-apps-jobs-self-hosted-github-actions-runner/</guid>
            <pubDate>Fri, 23 Feb 2024 10:05:41 GMT</pubDate>
            <content:encoded><![CDATA[GitHub Actions の Self-hosted Runner を安く用意する方法を探していたところ、 Azure の Container Apps Jobs というのが便利に使えるらしいというのを見つけたので試してみる。 チュートリアル:Azure Container Apps ジョブを使]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[酸いも甘いもある Shared VPC（共有VPC） ~ GKE Autopilot を Shared VPC で構築する際の苦悩 ~]]></title>
            <link>https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao</link>
            <guid>https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao</guid>
            <pubDate>Thu, 22 Feb 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Google Cloud Digital Leader合格！]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/02/21/225145</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/02/21/225145</guid>
            <pubDate>Wed, 21 Feb 2024 13:51:45 GMT</pubDate>
            <content:encoded><![CDATA[Google Cloud Platdorm(GCP)認定資格の第一歩、Cloud Digital Leaderに合格しました！ちなみに、これまでの私の保有IT資格は、基本情報、応用情報、AWSクラウドプラクティショナーです。AWS資格の第一歩、クラウドプラクティショナーはUdemyの模擬試験を1周したら合格できたので、同様にGoogle Cloud Digital LeaderもUdemyの模擬試験を1周以上したら合格できました。なお、AWSは最初、ソリューションアーキテクトアソシエイト受けたものの勉強不足で落ち、クラウドプラクティショナーならなんとかなるのでは？と思い、その2週間後に受けたら受かりました。Udemyの模擬試験は途中保存もでき、採点、不正解のみ見直しなどができて便利ですよね。次はGCPのAssociate Cloud Engineerに挑戦したいと思います！皆さんもUdemyで勉強してIT資格をとりましょう！AWSやGCPの資格をとるならUdemyで学ぼう！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform workspace使って思ったこと]]></title>
            <link>https://sreake.com/blog/terraform-workspace/</link>
            <guid>https://sreake.com/blog/terraform-workspace/</guid>
            <pubDate>Sun, 18 Feb 2024 14:28:59 GMT</pubDate>
            <content:encoded><![CDATA[背景 そこまで大きな案件でもなく、 環境間の差分もあまりなさそうだったため 何より使ったことないから試してみようっていう好奇心 ある案件にて上記の理由から、Terraform workspaceを採用しました。 今回は、 […]The post Terraform workspace使って思ったこと first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[個人開発でWebアプリの開発とデプロイの流れ]]></title>
            <link>https://kechigon.hatenablog.com/entry/2024/02/13/125853</link>
            <guid>https://kechigon.hatenablog.com/entry/2024/02/13/125853</guid>
            <pubDate>Tue, 13 Feb 2024 03:58:53 GMT</pubDate>
            <content:encoded><![CDATA[個人でWebサービスを開発したいけど、どのような流れで作っていけばいいのかわからない方向けです。個人開発でWebアプリを開発、デプロイをしたのでその流れを共有したいと思います。作ったもの麻雀戦績管理アプリ名付けて「PungPals」。雀荘などのオフラインでの対戦結果を残し、個人成績やランキングを確認できます。pungpals-service-xstpolfd4q-an.a.run.app開発とデプロイの流れ1.要件定義、設計実装がスムーズに進むために、しっかりとしておきましょう。以前記事を書いたので、参考にしてください。kechigon.hatenablog.com2.技術選定今回作ったアプリケーションはDjangoで開発し、Cloud Runにデプロイしています。選定理由は、Django: 経験があるから。Cloud Run: Djangoアプリのデプロイ方法の公式ドキュメントがあった(後ほど説明します)、マネージドな部分とカスタムできる部分のバランスがちょうどよかったから。でした。以下これらの技術を使って、開発デプロイまでの流れを説明していきます。3.Djangoを使ってアプリケーションを作成Djangoにはチュートリアルがあり、はじめての Django アプリ作成、その 1 | Django ドキュメント | Djangoはじめての Django アプリ作成、その2 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 3 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 4 | Django ドキュメント | Djangoを読めば開発方法がわかると思います。環境構築をし、実装し、ローカルで動作確認をしながら開発していきます。4.Cloud run へのデプロイDjangoアプリのCloud runへのデプロイ方法は公式ドキュメントにまとめられているので、これを見ながら進めます。cloud.google.comDjangoアプリケーションを環境に合わせて設定した後コンテナ化し、Cloud Runに載せます。それに伴い、Cloud SQL(データベース)、Secret Manager(シークレット管理)、Cloud Storage(静的アセットの保存など)、Cloud Build(CI/CD)、Artifact Registry(コンテナレジストリ)の作成、設定も行います。ドキュメントではGCRを使っていますが、現在非推奨なので、Artifact Registryをコンテナレジストリとして使用します。cloud.google.comオプションですが、GCPへのリソースの作成はTerraformを利用すると、構成管理ができ便利です。作成するインフラの図以上のことを行った後のGitHubリポジトリPungPalsのコードは公開しているので、参考にしていただければと思います。github.comこれから今後は、運用面の課題解決や集客などを行っていく予定なので、ブログにしていくつもりです！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudの管理を楽にする、 トイルを減らすクラウドガバナンス]]></title>
            <link>https://speakerdeck.com/parupappa2929/google-cloudnoguan-li-wole-nisuru-toiruwojian-rasukuraudogabanansu</link>
            <guid>https://speakerdeck.com/parupappa2929/google-cloudnoguan-li-wole-nisuru-toiruwojian-rasukuraudogabanansu</guid>
            <pubDate>Sun, 11 Feb 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Tech Blogが生えました]]></title>
            <link>https://abnoumaru.com/tech/2024-02-10-start-tech-blog/</link>
            <guid>https://abnoumaru.com/tech/2024-02-10-start-tech-blog/</guid>
            <pubDate>Sat, 10 Feb 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Astroのほぼテンプレで作ったブログにTech Blogを生やした]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私のメンターがくれた初めてのターミナル管理、それはtmuxで私は新卒でした。]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/02/06/110341</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/02/06/110341</guid>
            <pubDate>Tue, 06 Feb 2024 02:03:41 GMT</pubDate>
            <content:encoded><![CDATA[はじめに2024年2月5日夜の東京 外は雪が降っている。tmuxとの出会いは、新卒としての初めての職場でした。メンターがターミナルの管理において最初に紹介してくれたのがtmuxで、この出会いが私の開発効率と作業環境を大きく変革しました。tmuxの基礎を学んだ後、私は自分の開発環境をさらにカスタマイズし、tmuxを日々の作業効率化のために積極的に使い始めました。ここでは、私が実際に使っているtmuxの設定と、日常的に使うコマンドを紹介します。これらは、より快適なターミナル操作環境を実現するために役立ちます。この過程で、tmuxはただのツール以上のものになり、私の開発作業における最適なパートナーになりました。tmuxを使いこなすことで、複数のプロジェクトを同時に管理する能力が向上し、長時間の作業も中断せずに続けられるようになりました。リモートワークが増えた今では、tmuxのセッション管理機能が特に役立っています。サーバーに接続した状態で作業を行い、一時的に他のタスクに切り替えても、再びtmuxセッションに戻れば瞬時に作業を再開できます。tmuxを通じて、私はターミナル操作に関してプロフェッショナルな開発者としての成長していると実感しています。あとやっている感がとても出ているので好きです。tmuxとはtmuxは「ターミナルマルチプレクサ（Terminal Multiplexer）」の略称で、Linux系OSを中心に利用されています。このツールを使うと、一つのターミナルウィンドウ内で複数のセッション、ウィンドウ、ペインを効率的に管理することが可能になります。github.comセッションの管理： 一つのターミナルで複数のセッションを持ち、それぞれ独立した作業スペースとして利用できます。仕事とプライベート、複数のプロジェクト間でセッションを分けることができるため、タスクの切り替えがスムーズになります。ウィンドウとペイン： 一つのセッション内で、複数のウィンドウを開くことができ、さらにウィンドウをペインと呼ばれる小分けにすることが可能です。これにより、同一画面内で複数の作業を並行して行うことができ、効率的なマルチタスクが実現します。セッションの維持： tmuxの最大の特徴の一つは、ターミナルを終了してもセッションが維持されることです。これにより、長時間かかるコマンドを実行中にログアウトしてしまったり、接続が切れてしまったりしても、作業が中断される心配がありません。tmux設定のカスタマイズ私の.tmux.confファイルには、効率的なターミナル操作を可能にするための様々なカスタマイズが施されています。これらの設定を通じて、tmuxを自分にとって最適な作業環境に変えることができました。github.comプレフィックスキーの変更: デフォルト設定のCtrl+bをCtrl+qに変更しました。これは、より操作性の良いキーバインドに変更することで、他のショートカットキーとの競合を避け、操作のスムーズさを向上させるためです。キーバインドのカスタマイズ: vimを頻繁に使用することから、ペインの移動やリサイズをvim風に設定しています。これにより、キーボード操作の一貫性を保ちながら、直感的で迅速なウィンドウ管理を実現しています。ペインの分割: よく使用する|キーでペインを縦に分割し、-キーでペインを横に分割するように設定しました。これにより、柔軟かつ迅速に作業スペースをカスタマイズすることが可能になります。ステータスバーのカスタマイズ: ステータスバーには、現在のセッションの状態や時刻など、必要な情報を表示するよう設定しています。これにより、作業中に一目で状況を確認できるようになり、生産性の向上に貢献しています。プラグインの利用: tmux-resurrectやtmux-continuumなどのプラグインを導入しています。これらのプラグインは、セッションの自動保存や復元を可能にし、長時間にわたる作業や一時的な中断からのスムーズな再開を支援します。セッションの保存と復元は、プレフィックスキーに続けてCtrl+sで保存、Ctrl+rで復元することができます。これにより、突然のシステム停止や作業の中断が発生しても、簡単に前の状態に戻ることができます。さらに、tmuxのプラグインエコシステムは非常に豊富で、tmux-pluginsのリストからは、あらゆるニーズに応える特別なプラグインを見つけることができます。自分の作業フローに合わせて、最適なプラグインを選択し、tmux環境をさらにパワフルで柔軟なものにカスタマイズすることが可能です。よく利用するtmuxコマンドtmuxを効率的に使用するためには、日常的に役立つコマンドを知っておくことが重要です。ここでは、特に重宝するコマンドを紹介します。どんな時にでも味方になってくれるチートシートです。ちなみにチートシートには入れてないのですがprefix + e で全てのペインの操作、prefix + Eでそれらの解除などもインフラエンジニアとしては非常に重宝します。github.com以下は、日々の作業で特に役立つコマンドです。新規セッションの開始: tmux new -s <セッション名>コマンドで、特定の名前を持つ新規セッションを開始します。この機能を活用することで、プロジェクトやタスクごとにセッションを分け、作業を整理しやすくなります。セッションの一覧表示と切り替え: tmux lsコマンドで現在のセッション一覧を表示し、tmux attach -t <セッション名>またはtmux a -t <セッション名>で特定のセッションにアタッチします。これにより、複数のプロジェクトや作業を効率的に管理し、スムーズに切り替えることができます。ペインとウインドウの操作: tmuxでは、ペインの分割やウインドウの作成、移動、リサイズを柔軟に行うことができます。これらの操作をカスタマイズしたキーバインドで行うことで、必要に応じて作業スペースを自由に調整し、マルチタスク作業を効率的に進めることができます。マウス操作の有効化: set-option -g mouse onコマンドにより、tmux内でのマウス操作を有効にすることができます。マウスでペインを選択したり、サイズを調整したりすることが可能になり、キーボードとマウスを組み合わせた直感的な操作が実現します。これらのコマンドは、tmuxを使ってターミナル操作を効率化し、生産性を高めるための基本となります。tmuxをより深く理解し、活用することで、開発作業をより快適に、より効率的に行うことができるでしょう。さいごにtmuxとの出会いは、私の開発効率と作業環境を大きく変えただけでなく、インフラエンジニアとしての成長にも大きく寄与しました。日々の作業でtmuxを使いこなすことで、システムの監視、ログの分析、複数のサーバーへの同時操作など、インフラ管理の幅広いタスクを効率的にこなすスキルを身につけることができました。また、セッションの持続性は、長時間実行するプロセスの管理や、中断された作業の再開といった面で、インフラ作業の品質を向上させるのに役立ちました。tmuxのカスタマイズ性と拡張性を活かして、個人の作業環境を最適化することは、単に作業を効率化するだけではなく、技術者としての視野を広げ、問題解決能力を養う機会となりました。tmuxを深く理解し活用することで、インフラエンジニアリングの知識を実践的に拡張し、より複雑なシステムと効果的に向き合う力を養うことができました。tmux は3.4 がリリースされており今でも進化を続けている。愛している。github.comtmuxは、単なるツール以上の存在となり、私の技術的な成長を支えてくれる貴重なパートナーです。これからもtmuxを活用し続けることでしょう。しかし、人は変わる。実はZellijやターミナルもMAC標準なものを利用しているがAlacrittyが気になっているので検証と導入を進めている。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OWASP Top 10 for Large Language Model Applications をまとめる]]></title>
            <link>https://sreake.com/blog/owasp-top-10-for-llm-application/</link>
            <guid>https://sreake.com/blog/owasp-top-10-for-llm-application/</guid>
            <pubDate>Mon, 05 Feb 2024 09:29:32 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake 事業部インターン生の中林です。私は、Sreake 事業部長期インターン生として SRE 技術の技術検証を行っています。 今回は、Sreake 事業部で作成している LLM アプリケーションに対する […]The post OWASP Top 10 for Large Language Model Applications をまとめる first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDB ManagedのAlways Free枠を試そう]]></title>
            <link>https://zenn.dev/nnaka2992/articles/play_with_yugabytedb_managed_sandbox</link>
            <guid>https://zenn.dev/nnaka2992/articles/play_with_yugabytedb_managed_sandbox</guid>
            <pubDate>Sun, 04 Feb 2024 15:02:28 GMT</pubDate>
            <content:encoded><![CDATA[YugabyteDB Managedにフリートライアルがあるのは知っていたのですが、期間が限られたものしか無いと思っていました。YugabyteDBについて調べごとをしていたら機能制限はあるもののSandboxクラスターというクレジットカード登録すら不要でAlways Freeな利用枠があることを知りました。いままでローカルでYugabyteDBを建てたりminikube上で遊んでいたのですが、簡単な検証であればSandboxクラスターで十分です。この記事ではそんなYugabyteDB ManagedのSandboxクラスターを紹介します。 Sandbox Clusterの制限...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google CloudのObservability : Personalized Service Health]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/8cc96cb8acb4bb</link>
            <guid>https://zenn.dev/yokoo_an209/articles/8cc96cb8acb4bb</guid>
            <pubDate>Sun, 04 Feb 2024 11:02:42 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先日、これまでプレビュー版だった、Google Cloudの稼働監視サービスのPersonalized Service HealthがGAになりました！https://cloud.google.com/blog/products/devops-sre/personalized-service-health-is-now-generally-available/?hl=en基本的な使用方法や設定はこちらのブログにて詳しく解説されていますので、ここでは運用面の機能を中心にPersonalized Service Healthを見ていきます。Google Cloudの障害情...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[renovate で CircleCI の terraform_version を更新する]]></title>
            <link>https://blog.1q77.com/2024/02/update-terraform-version-in-circleci-with-renovate/</link>
            <guid>https://blog.1q77.com/2024/02/update-terraform-version-in-circleci-with-renovate/</guid>
            <pubDate>Sun, 04 Feb 2024 10:37:36 GMT</pubDate>
            <content:encoded><![CDATA[Circle CI の terraform Orb で terraform の version を指定するには次のようにしますが、この terraform_version の値に変数を 使うことが出来ず、tf ファイルや .tool-versions から読み出した値を使うことが出来ませんでした。 - terraform/install: terraform_version: 1.7.2 GitHub]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[raspberry piで学ぶ組込みLinuxデバイスドライバ開発Part2]]></title>
            <link>https://zenn.dev/satoken/articles/try-lkm-raspi2</link>
            <guid>https://zenn.dev/satoken/articles/try-lkm-raspi2</guid>
            <pubDate>Sun, 04 Feb 2024 02:39:55 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前回からの続きです。前回までで基本的なデバイスドライバを作成して動作確認をしましたが、Linux上で完結するドライバであり、ラズパイ自体は使っていませんでした。今回はラズパイにLEDとスイッチを簡単な回路で接続して、それを操作するデバイスドライバを作成してみます。セミナーで使用したボートには4つのLEDと4つのスイッチが付いていたので1つのドライバで4つ同時に制御するものを作りましたが、回路を作るのが面倒なので1つずつです。ご承知おきくださいませ。 LEDを操作するデバイスドライバGPIO18番ピンにLEDを接続してこれを点けたり消したりできるモジュールを作...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AI活用の事例共有 - 大企業と中小企業・スタートアップでの違いに参加]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/02/01/094735</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/02/01/094735</guid>
            <pubDate>Thu, 01 Feb 2024 00:47:35 GMT</pubDate>
            <content:encoded><![CDATA[2023年12月1日にスリーシェイクに入社し、SRE構築総合支援サービスSreake事業部のエンジニアとして2ヶ月が経ちました。私は生成AIアプリケーション開発を行なっており、ちょうど良さげな勉強会に参加したので、参加レポートを書きたいと思います。chatgptllm.connpass.com参加したのは、LLM Tokyo主催の「生成AI活用の事例共有 - 大企業と中小企業・スタートアップでの違い」。主催者は株式会社pipon代表の北爪聖也さん( @Seiyan1 )。北爪さんともう一人の登壇者が株式会社LangCore代表の北原麦郎さん(@yuno_miyako2 )。LangCore北原さんの事例発表最初は、LangCore北原さんの事例発表。LangCoreはAIの受託開発を行なっており、モジュール化して様々な案件に対応できるようにしている。生成AIで個人の生産性向上よりも大きなインパクトを出せるようにすることが目標。苦戦する案件もあるが、思った以上の成果を出せた案件もあった。大企業が生成AIを活用するモチベーションは他社から遅れを取らないようにするため、というのが最も大きい。検証項目は大きく分けて以下の3つ。中小企業、スタートアップにおけるAI案件の課題としては、予算が限られていること。案件の流れとしては以下。ヒアリングpipon北爪さんの事例発表・化粧品メーカーでの事例化粧品メーカーが動物実験を減らすため、生成AIで実験結果の推測をする・コンサルティングファームで社内の業務効率化大量の類似した質問をChatGPT+RAGでさばく・製薬会社のマーケティング売れる営業とそうでない営業の効果測定・自動車会社生成AIを使った新規事業の模索質疑応答質問「スケジューリングまでLLMでできるのか？」www.itmedia.co.jp質問「行政の生成AIPoCで94%の精度では使えないという事例があったが、100%の精度が求められることはあるのか？」アバターが間違えても、可愛いと捉えられるので導入しやすい。」「生成AIを使わなくても既存技術で解決できる課題も多かった。」「何でもLLM回答を得るのではなく、コスト削減のために、キャッシュを使って、同じ質問に同じ回答を返すようなことをしている。」検証ツール：AzureだとAzure Machine Learning プロンプト フローがあり、テストに使える。「ファインチューニング、アノテーションはコストがかかるのであまりやらない。RAGを使う。」pipon北爪さんはKindleでLLM書籍を出版されています。ボリュームタップリで99円はお買い得！Kindle書籍の詳細はこちら ⇒ https://amzn.to/4bhlmBQ2月13日(火)に出版記念イベントを開催されるそうです。（参加費は懇親会込みで4,000円）chatgptllm.connpass.com感想とまとめ私も生成AI開発をやり始めたところですが、数多くの案件を手がけてきた方々の話を聞けて、イメージが湧くとともにモチベーションが上がりました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[raspberry piで学ぶ組込みLinuxデバイスドライバ開発]]></title>
            <link>https://zenn.dev/satoken/articles/try-lkm-raspi</link>
            <guid>https://zenn.dev/satoken/articles/try-lkm-raspi</guid>
            <pubDate>Sat, 27 Jan 2024 11:48:11 GMT</pubDate>
            <content:encoded><![CDATA[はじめに1/24~26の3日間 仕事をサボっ.... 調整をしてポリテクセンター関東で行われた組込みLinuxデバイスドライバ開発技術というセミナーを受講してきました。カーネルのVersionが2.6、対象のマイコンボードがSH-4というとても古いものだったので今回はラズパイで復習しながら、セミナーの内容を共有したいと思います。↑がセミナーで使用したボードです。LEDやタクトスイッチ、赤外線センサやモータがボートに付いているのでそれを制御するドライバを作成しました。セミナーのテキストは2部構成で内容は以下の通りです。第1部CPUボード編1章 ターゲットボードの確認...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Mac に Homebrew で docker pluings をインストールする]]></title>
            <link>https://blog.1q77.com/2024/01/install-docker-plugins-on-mac/</link>
            <guid>https://blog.1q77.com/2024/01/install-docker-plugins-on-mac/</guid>
            <pubDate>Fri, 26 Jan 2024 12:36:56 GMT</pubDate>
            <content:encoded><![CDATA[Docker Desktop for Mac であれば何もしなくても docker compose コマンドは使えるようになっているのですが、Lima で docker を使っている場合などで Homebrew で docker をインストールしていると docker compose や docker buildx を使えるよ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[このSRE本がすごい！2024年版]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/01/26/165255</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/01/26/165255</guid>
            <pubDate>Fri, 26 Jan 2024 07:52:55 GMT</pubDate>
            <content:encoded><![CDATA[はじめに有用な知識の特性Google SRE リソースSite Reliability Engineering: How Google Runs Production SystemsThe Site Reliability Workbook: Practical Ways to Implement SREBuilding Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining SystemsSLO Adoption and Usage in SRECreating a Production Launch PlanTraining Site Reliability Engineers: What Your Organization Needs to Create a Learning ProgramAnatomy of an Incident – Google – Site Reliability EngineeringEnterprise Roadmap to SRE – Google – Site Reliability EngineeringIncident Metrics in SRE – Google – Site Reliability EngineeringPractical Guide to Cloud Migration – Google – Site Reliability EngineeringGoogle以外の重要な書籍紹介97 Things Every SRE Should KnowSeeking SREPractical MonitoringDatabase Reliability EngineeringObservability EngineeringChaos EngineeringBuilding Microservices, 2nd EditionAPI Design PatternsSystems Performance, 2nd EditionEfficient GoImplementing Service Level ObjectivesModern Software Engineering: Doing What Works to Build Better Software FasterLearning Test-Driven Developmentシステム障害対応 実践ガイドWebエンジニアのための監視システム実装ガイドSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本さいごにはじめに2024年、情報技術の世界は革新的な変化を続け、特にSRE（サイト信頼性エンジニアリング）の分野では新しい概念や技術が絶えず生まれています。この急速に進化する環境において、効率的に最新の知識を吸収する方法を見つけることは非常に重要です。その一つの答えが、「タイパ（タイムパフォーマンス）」という概念です。タイパとは、投入した時間に対する成果の効率を意味し、限られた時間を最大限に活用することの重要性を示しています。このブログでは、SRE分野で高いタイパを達成するための役立つ書籍を探求します。これらの書籍を全て読むのは、SREに深い情熱を持つ者か、非常に勤勉な人に限られるかもしれませんが、オススメの順番などはあえて紹介しません。また、「class SRE implements DevOps」は、「SREはDevOpsというインターフェースの実装である」という意味を持ちます。「DevOps = 思想」という定義に対して、それを具体化し実装したものがSREであると考えます。本ブログでは、DevOpsやその進化形であるPlatform Engineeringについては触れません。それは、既に多岐にわたる議論がある中で、さらに混迷を招く可能性があるためです。また、全ての情報は完璧ではないと思うので補足情報などがあれば教えてください。映画を早送りで観る人たち～ファスト映画・ネタバレ――コンテンツ消費の現在形～ (光文社新書)作者:稲田 豊史光文社Amazon有用な知識の特性有用な知識が持つべき三つの性質について考えます。第一に「一般性」です。これは知識が多様な状況で適用できることを意味します。例えば、コンテナ技術やクラウドインフラの原則は、異なるプラットフォームやアプリケーションでも適用可能です。これが一般性のある知識です。第二の性質は「関係性」です。孤立した知識はあまり役に立ちません。例えば、単に多くのモニタリングツールを知っているだけでは不十分で現場の課題を知っている必要があります。また、それらがシステムのパフォーマンスやセキュリティとどのように関連し、全体的な信頼性を向上させるかを理解することが重要です。最後に「場面応答性」があります。知識は、それが必要とされる特定の状況で適切に活用されるべきです。例えば、システムのスケーラビリティを改善する際には、負荷分散やキャッシングなどの特定の技術や知識が必要です。これらの技術や知識は、それぞれの状況に応じて適切に選択し、活用されるべきです。場面応答性がある知識は、適切な状況でのみその真価を発揮することができます。この有用な知識の枠組みに加えて、特に学生や新卒の方々には「基礎の重要性」を強調したいと思います。技術的な深い理解や幅広い応用は、コンピュータサイエンスの基本原理、アルゴリズム、データ構造などの基礎知識があってこそ可能です。SREやオブザーバビリティなどの先進的な分野への関心も良いことですが、その前にしっかりとした基礎を学び、築くことが非常に重要です。基礎知識がしっかりしていれば、新しい技術やトレンドにも柔軟に対応し、深く理解することができるでしょう。学びとは何か――〈探究人〉になるために (岩波新書)作者:今井 むつみ岩波書店AmazonGoogle SRE リソースGoogleは、SRE（Site Reliability Engineering、サイト信頼性エンジニアリング）の知識と実践を業界全体で共有し、普及させるために積極的な取り組みを行っています。このページで紹介されている書籍やリソースはその一部に過ぎません。Google SREリソースとして、さらに多くの資料が公開されていますので、興味のある方は以下のリンクから探求してみてください。sre.googleSite Reliability Engineering: How Google Runs Production Systems「Site Reliability Engineering: How Google Runs Production Systems」は、Googleが開発したシステム管理とサービス運用の方法論を学ぶことができる、非常に重要なSREの書籍です。この書籍はO’Reillyから出版されており、全552ページにわたって豊富な内容が展開されています。発売日は2016年4月で、原書なのでリリースから時間が経過していますが、その内容の深さと実践的なアプローチは、今日でも多くのSRE専門家やソフトウェアエンジニアにとって非常に価値のあるものです。sre.googleこの書籍では、ソフトウェアのライフサイクル全体にわたるコミットメントと、世界最大規模のソフトウェアシステムの構築、導入、監視、維持方法について詳細に解説しています。リスク管理、サービスレベル目標、リリースエンジニアリングなどSREの基本原則から始まり、インシデント管理、障害の根本原因分析、SREチーム内でのソフトウェア開発についても深く掘り下げています。さらに、SREのトレーニングやコミュニケーション管理についても紹介しており、急速にスケールするサービスを高い信頼性で運用する方法についての理解を深めることができます。この書籍は、大規模なシステムの運用における複雑な課題への実践的な解決策を提供しており、非常に有益です。リリースエンジニアリングやインシデント管理の部分は特に、日常業務に直接応用できる知見が豊富に含まれており、実務においても大いに役立つ内容となっています。また、SREチーム内でのソフトウェア開発プロセスに関する記述は、チームワークと効率性を高めるための貴重で有用な参考資料となります。Googleによって無料で公開されていることも特筆すべき点で、こちらのリンクから無料で読むことが可能です。さらに、日本語での翻訳版も存在し、日本の読者にとっても2倍の感謝を持って読むことができるでしょう。SRE サイトリライアビリティエンジニアリング ―Googleの信頼性を支えるエンジニアリングチームオライリージャパンAmazonさらに、Googleに直接関連はありませんが「もう一度読むSRE」というポッドキャストもあり、この書籍の内容を深く理解するための補足資料として聞いてみるのも良いでしょう。ポッドキャストを通じて、書籍の内容をさらに掘り下げたり、実際の業務における適用例を聞くことができます。The Site Reliability Workbook: Practical Ways to Implement SRE「The Site Reliability Workbook: Practical Ways to Implement SRE」は、SREの原則と実践方法に深く踏み込んだ、O’Reillyから出版された812ページにも及ぶ充実した内容の書籍です。その分厚さは、まさに鈍器のよう。2018年7月の発売で、「Site Reliability Engineering: How Google Runs Production Systems」のリリースから3年が経過し、この期間を経てさらに充実した内容に進化しています。sre.googleこの書籍では、具体的な事例を通じてSREの原則と実践方法に深く踏み込んでいます。前作で紹介されたSREの基本原則からさらに一歩進み、Google内部で培われた技術的ノウハウだけでなく、Evernote、The Home Depot、New York Timesなどさまざまな企業の事例も紹介しています。クラウド環境での信頼性の高いサービス実行方法や、サービスレベル目標に基づくサービスの運用、既存の運用チームをSREに変換する方法など、実践的かつ詳細な解説がなされています。この書籍は、既にSREを導入している企業やこれから導入を考えている企業の開発者、運用管理者、マネージャーにとって、理論から実践へと移行するための貴重な手引きとなります。実際の業務に役立つ豊富な知識と具体的なガイドラインを提供しており、SREの実践を深めたいすべてのプロフェッショナルにとって有用な内容です。Googleによって無料で公開されていることも特筆すべき点で、こちらのリンクから無料で読むことが可能です。このような資料を無償で提供することは、業界全体の技術向上に大きく貢献しています。GoogleがSREの知識と実践を広く共有し、普及させようとする意図が明確に伺えます。さらに、日本語で翻訳されているため、日本の読者も2倍の感謝を持って読むことができるでしょう。サイトリライアビリティワークブック ―SREの実践方法オライリージャパンAmazonBuilding Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining Systems「Building Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining Systems」は、O’Reillyから出版された519ページに及ぶ内容で、セキュリティを中心に据えた構成となっています。2020年3月の発売で、聖典とも言える「Site Reliability Engineering: How Google Runs Production Systems」のリリースから4年が経過しており、この間にセキュリティ意識の高まりを強く感じさせる書籍です。google.github.ioこの書籍では、システムのセキュリティと信頼性が一体であることを明示し、スケーラブルなシステムの設計と運用におけるセキュリティの重要性を深く掘り下げています。GoogleのセキュリティとSREのエキスパートが、セキュアでスケーラブルかつ信頼性の高いシステムを根本から設計するためのベストプラクティスを紹介しており、システムの設計、実装、保守に関する考え方と実践法を詳しく解説しています。また、組織の文化がベストプラクティスに取り組む上でいかに重要かについても言及されています。この書籍はセキュリティと信頼性を軸にしたシステム構築のための貴重な知見を提供しています。特に、セキュリティをシステム設計の初期段階から考慮する重要性を説く内容は、現代のセキュリティ意識が高まる中でのシステム設計において非常に参考になります。また、組織文化とそのベストプラクティスへの適用に関する洞察は、チームや組織全体のセキュリティ意識向上に大いに役立つでしょう。「Building Secure and Reliable Systems」も、Googleによって無料で公開されており、こちらのリンクから無料で読むことが可能です。Googleがセキュリティと信頼性に関する知識を広く共有しようとする姿勢が、このような形で表れているのです。また、Google SRE本は3冊とも翻訳されていてありがたい限りですね！！！セキュアで信頼性のあるシステム構築 ―Google SREが考える安全なシステムの設計、実装、保守オーム社Amazonセキュリティに関する本の一環として、Googleとは直接関連はありませんが、『体系的に学ぶ 安全なWebアプリケーションの作り方 第2版 脆弱性が生まれる原理と対策の実践』の読書をお勧めします。この本では、Webアプリケーションのセキュリティにおける脆弱性の原理と対策について詳細に解説されています。SLO Adoption and Usage in SRE「SLO Adoption and Usage in SRE」は、サービスレベル目標（SLO）のSREにおける採用と使用に焦点を当てた104ページの実践的なレポートです。2020年4月1日に発売されたこのレポートは、SREのフレームワークを活用して運用コストの削減や開発生産性の向上に役立つ方法を提供します。SREやソフトウェアエンジニアとして、このレポートはSLOの重要性とその実践法を深く理解するのに大変役立ちます。SLO、SLI、エラーバジェットをSREの実践の中核として位置づけ、サービスの信頼性をどのように測定し管理するかを具体的に示しています。Googleの調査結果や実際のケーススタディを基に、許容可能な信頼性のレベルを定義し、それに基づいてシステム変更を適切に管理する方法は、日々の業務に直接適用できる知識です。参考リンク: SLO Adoption and Usage in SRESRE担当者、エグゼクティブ、開発者、アーキテクトなど、幅広い関係者にとって、SLOを取り入れたSREの実践を深めるための有用なリソースとなるでしょう。Creating a Production Launch Plan「Creating a Production Launch Plan」は、実稼働環境の立ち上げにおける計画策定に焦点を当てた45ページの実践的なレポートです。2020年1月に発売されたこのレポートは、製品のローンチプランをテンプレートとして使用することで、多くの時間、費用、そして頭痛を節約する方法を提供します。SREやソフトウェアエンジニアとして、このレポートは実稼働環境の立ち上げにおける計画策定の重要性と実践法を深く理解するのに非常に役立ちます。Googleが実際にどのように本番発売計画を策定したかを紹介し、自社製品を導入する際のリスクを低減するための実践的な方法を学べます。ローンチプランは、すべての関係者とプロセスを巻き込み、ローンチの進行を確実にコントロールすることで、さまざまな問題を防ぐことができます。参考リンク: Creating a Production Launch Plan開発者やサイト信頼性エンジニア（SRE）を対象に、Googleのローンチプランの基本的な構成要素を探り、自社製品を導入する際のリスクを低減するための実践的な方法を提供する本レポートは、企業規模や製品のユーザーベースに関係なく、消費者向けサービスにも適応可能です。これらの教訓は、製品のローンチを成功に導くための貴重で有用な学習リソースとなるでしょう。Training Site Reliability Engineers: What Your Organization Needs to Create a Learning Program「Training Site Reliability Engineers: What Your Organization Needs to Create a Learning Program」（日本語名：サイト信頼性エンジニアの育成：学習プログラムを作成するために組織に必要なこと）は、サイト信頼性エンジニアの育成に関する116ページの詳細なガイドです。2020年2月に発売されたこのレポートでは、一般的な内容からドメイン固有の内容まで、組織でのSREトレーニング方法について解説しています。GoogleのSREチームによるこのガイドでは、Googleが新しいSREを育成するために使用しているトレーニングのベストプラクティスを学ぶことができます。また、SRE（またはSREに類似した機能）のトレーニングを成功させた小規模な組織での使用例も紹介されています。効果的なSREトレーニングを実施するためには、自社のニーズと受講者の両方に合うように意図的に設計する必要があります。本レポートの大部分はGoogle SREの具体的な経験に焦点を当てていますが、トレーニング設計の背景にある理論についても説明し、過去数年間に業界全体で得られたベストプラクティスや教訓を紹介しています。参考リンク: Training Site Reliability Engineersこのレポートは、SREの育成に取り組む組織にとって、トレーニングプログラムの設計と実施における重要な指針を提供し、より効果的なSREトレーニングの実現をサポートします。Googleの実践に基づく具体的なアプローチは、業界におけるSREトレーニングのスタンダードを形成していると言えるでしょう。Anatomy of an Incident – Google – Site Reliability Engineering「Anatomy of an Incident – Google – Site Reliability Engineering」（日本語名：インシデントの解剖 – Google サイト信頼性エンジニアリング）は、インシデント対応に関する70ページの実践的なレポートで、2016年4月に発売されました。このレポートは、システム設計における失敗の避けられない側面を探り、科学者やエンジニアが未来を完全に把握することなく解決策を実行する現実を浮き彫りにしています。次のゼロデイ脆弱性、バイラル・メディア・トレンド、気象災害、テクノロジーの変化などを予測することは難しいものですが、本レポートでは、インシデントがシステムに影響を及ぼした場合に対応するための準備方法について詳しく探求しています。SREやDevOpsの実務者、IT管理者、エンジニアリング・リーダーを対象に、Ayelet Sachto氏、Adrienne Walcer氏、Jessie Yang氏のアドバイスをもとに、組織がインシデントに備え、対応し、回復する方法について解説されています。参考リンク: Anatomy of an Incidentこのレポートは無料で公開されており、上記のリンクから原著を読むことが可能です。インシデント発生時の効果的な対応策を学ぶことは、組織のシステムの信頼性を高め、将来のトラブルへの対処能力を強化するために不可欠です。インシデント管理に関心のあるすべてのプロフェッショナルにとって価値のあるリソースと言えるでしょう。Enterprise Roadmap to SRE – Google – Site Reliability Engineering「Enterprise Roadmap to SRE – Google – Site Reliability Engineering」（日本語名：SREエンタープライズロードマップ – SREを導入し継続する方法）は、SREに関する技術的立ち位置、導入理由、必要なプロセス、文化、事例などを幅広く紹介する62ページのコンパクトなレポートです。2020年3月に発売され、日本語で読める点も非常に魅力的です。このレポートでは、Google Cloud Reliability AdvocateのSteve McGheeとGoogle Cloud Solutions ArchitectのJames Brookbankが、組織でSREを導入する際にエンジニアが直面する特定の課題について深く掘り下げています。Googleが過去に出版した「Site Reliability Engineering」と「The Site Reliability Workbook」が、サービスライフサイクル全体への取り組みによって組織がソフトウェアシステムの構築、展開、監視、保守を成功させる方法と理由を示しているのに対し、本レポートはそれらを補完する内容となっています。参考リンク: Enterprise Roadmap to SRESREの普及にもかかわらず、多くの企業ではSREに対する当初の熱意とその採用の度合いの間に大きな隔たりが生じています。このレポートは、プロダクトオーナーや信頼性の高いサービスに携わる方々がSREの採用について知りたいときに、そのプロセスを体系的に説明するものです。SREの導入を検討する企業や、より効果的な方法でSREを実践したいエンジニアにとって、重要なガイダンスを提供する資料です。Incident Metrics in SRE – Google – Site Reliability Engineering「Incident Metrics in SRE – Google – Site Reliability Engineering」（日本語名：SREにおけるインシデント評価指標 – Google – Site Reliability Engineering）は、SREにおけるインシデント評価指標に深く焦点を当てた36ページのレポートです。2021年3月に発売され、SREでの改善評価や傾向追跡に用いられるMTTxメトリクスの効果について深く掘り下げています。SREでは、MTTR（平均復旧時間）やMTTM（平均緩和時間）などのメトリクスが一般的に使用されていますが、Google SREのStepan Davidovic氏はモンテカルロ・シミュレーションを用いて、これらのメトリクスが生産インシデントのコンテキストにおいて意思決定やトレンド分析に適していないことを示しています。これらのメトリクスの適用は見かけよりも厄介で、多くの実用的なシナリオにおいて誤解を招く可能性があります。本レポートでは、これらの測定を達成するための代替方法を探ります。参考リンク: Incident Metrics in SREこのレポートは、SREの実務においてインシデント評価指標の適用に関する誤解を避け、より効果的な方法を探るための重要なリソースとなります。SRE担当者やシステム運用チームは、このレポートを通じてインシデントの評価と分析に対するより深い理解を得ることができ、より効率的かつ適切な意思決定を行うための手助けを受けることができるでしょう。Practical Guide to Cloud Migration – Google – Site Reliability Engineering「Practical Guide to Cloud Migration – Google – Site Reliability Engineering」（日本語名：クラウド移行実践ガイド – Google – サイト信頼性エンジニアリング）は、クラウドへの移行に関する実践的なアプローチを解説する124ページのレポートです。2021年2月に発売され、企業が直面する大規模なクラウド変革の課題に焦点を当てています。クラウドへの移行はしばしば、大きなリターンが期待されるものです。この移行が実現すると、働き方を根本的に変える新たなビジネスチャンスが生まれます。本レポートでは、Googleのチームメンバーがクラウドへの移行に必要な文化的および技術的変革をナビゲートする方法を示しています。Googleはクラウドで誕生した企業ですが、チームメンバーの中には、この移行を苦労して乗り越えなければならなかった組織の出身者もいます。彼らは成功したクラウド変革のさまざまな側面をカバーする13のエッセイを通じて、苦労して勝ち取った経験を共有します。参考リンク: Practical Guide to Cloud Migrationこのレポートは、クラウドへの移行を検討している企業やチームにとって、具体的なヒントやアドバイスが満載の価値あるガイドとなります。クラウド変革の課題に直面する多くの組織が、このレポートを通じて適切な戦略とアプローチを学び、成功への道を切り開くための重要な手がかりを得ることができるでしょう。Google以外の重要な書籍紹介この章では、Googleに関連しないが、サイト信頼性エンジニアリング（SRE）やソフトウェアエンジニアリングの分野で重要な書籍を紹介しています。しかし、これらの書籍は、SREとソフトウェアエンジニアリングの広大な知識と実践の世界におけるごく一部に過ぎません。市場には、読者の皆様にとってさらに多くの価値ある書籍が存在し、それぞれが独自の視点と深い専門知識を提供しています。これらの書籍は、私の独断と偏見で日々直面する課題を解決し、スキルを磨くための貴重で有用なリソースとなり得ます。97 Things Every SRE Should Know「97 Things Every SRE Should Know」は、250ページにわたる実践的な書籍で、2020年11月に出版されました。この本は、SREの新人からベテランまでが、SREの採用方法、SLOの重要性、インシデント対応のアップグレード時期、モニタリングと可観測性の違いなどについて学ぶことができる、幅広いトピックをカバーしています。編集者のJaime WooとEmil Stolarskyは、信頼されるベストプラクティスや新しい問題解決方法を含む、97の有用なヒントを集めました。これにより、SREのスキルを成長させ、洗練させることが可能です。特に、「エラーバジェットを手に入れたら、次に何をするか」 - Alex Hidalgo や 「自分の仕事を認識させる：自慢文書を書く」 - Julia Evans and Karla Burnett などのアドバイスは、SREの領域において深い理解と実践的なスキルを習得するのに役立ちます。この書籍はSREにおける深い理解と実践的なスキルの習得に大いに役立ち、技術者やチームリーダー、プロジェクトマネージャーにとって非常に参考になる内容となっています。Seeking SRE「Seeking SRE」は、サイト信頼性エンジニアリング（SRE）に関する幅広いトピックを扱う587ページの書籍で、O'Reilly Media, Inc.から2018年9月に出版されました。この書籍は、システムとアプリケーションの信頼性の重要性と、市場の要求する速度でのイテレーションを行いながら信頼性を維持する難しさを背景にしています。Googleによる「Site Reliability Engineering」という著書に触発され、SREの非常に異なる部分を探求しています。「Seeking SRE」には25以上の章が含まれ、SREの世界で行われている重要な議論に読者を引き込みます。様々な環境でのSREの実装方法、DevOpsとの関連、最先端の専門知識、ベストプラクティスとテクノロジー、さらにはSREの人間的側面について、エンジニアやその他の分野のリーダーが語る内容が盛り込まれています。David N. Blank-Edelmanがキュレーター兼編集者を務め、SREの理解を深め、実践的なスキルを習得したい技術者やリーダーにとって非常に参考になる内容となっています。日本語の書籍も出版されていてとても嬉しいですねSREの探求 ―様々な企業におけるサイトリライアビリティエンジニアリングの導入と実践オライリージャパンAmazon弊社では輪読会を行いましたが様々な案件とリンクして考える事ができてよかったです。syu-m-5151.hatenablog.comPractical Monitoring「Practical Monitoring」は、O'Reilly Media, Inc.から出版された170ページに及ぶ監視システムの設計と実装に関する実践的なアプローチを提供する書籍です。2017年10月に発売され、モニタリングの改善が必要だが、どこから手を付けるべきかわからない人々に向けて書かれています。著者のMike Julianは、企業アプリケーションからデータセンターのハードウェアに至るまで、様々なレベルでの効果的なモニタリングを設計し実装するための戦略と戦術を提供しています。この書籍は特定のツールの実装方法ではなく、モニタリングの原則と基本的な仕組みに焦点を当てており、モニタリングのアンチパターン、デザインの原則、効果的なオンコールローテーションの構築、アプリケーションからのメトリクスとログの取得といった重要なトピックをカバーしています。モニタリングは、「Service Reliability Hierarchy」 でも最も最初に取り組むべきだと記載されており、その重要性は業界全体で認識されています。本書は、モニタリングの効果的な実践に関して、あらゆるレベルの専門家に対して具体的な洞察とガイダンスを提供します。Site Reliability Engineering: How Google Runs Production Systems Part III. Practices Figure III-1. Service Reliability Hierarchy より個人的には、既に知っている内容も多かったものの、心得的な部分は「監視版リーダブルコード」と表現できるほどの価値がありました。ある程度監視を経験した人には自分の知識を再確認するのに適していますが、入門書としては抽象的で理解しにくい内容も含まれているため、即座に具体的な知識を得て活用したい人には向いていないかもしれません。ネットワーク、サーバー、フロントエンド、バックエンド、KPIなどを幅広くカバーしていますが、具体的なアクションポイントにはあまり触れられていません。こちらのリンクで、この本をチェックすることができます。日本語版もO'Reilly Platform で読めます。こちらの書籍は、日本語版では「入門 監視」と題されており、これは非常に喜ばしいことです。原題の直訳である「実践 監視」とするのではなく、「入門」としていることで、モニタリングの基礎から学びたい初心者や、監視システムの知識を深めたい方々にとってもアプローチしやすい内容となっています。初学者に優しいこのタイトル変更は、監視の世界への第一歩を踏み出す人々にとって、大いに役立つことでしょう。入門 監視 ―モダンなモニタリングのためのデザインパターン作者:Mike JulianオライリージャパンAmazon「Monitoring Anti-Patterns」は、監視システムにおける一般的な間違いや誤解を扱う非常に洞察に満ちたセクションです。このセクションでは、監視を単なるタスクではなく、システム全体の健康とパフォーマンスを維持するための重要なプロセスとして捉えることの重要性が強調されています。具体的には、監視システムの設計と実装において、ツールへの過度な依存（「ツールの崇拝」）、チェックボックス式のアプローチ（「動作している」とは何を意味するのかに焦点を当てる）、監視を一時的な対処策として使うこと（「監視を松葉杖として使う」）、また手動での設定の問題点などが取り上げられています。これらのアンチパターンは、監視システムの構築における一般的な落とし穴を示しており、これらを理解し避けることは、より効果的な監視システムの構築に不可欠です。また、OSメトリクスはアラートにはあまり役立たないことが多いため、メトリクスをより頻繁に収集することの重要性も指摘されています。このセクションを読むことで、監視に関する一般的な誤解を避け、より効果的な戦略を実践するための洞察を得ることができます。learning.oreilly.com「Prometheus: Up & Running, 2nd Edition」は、Prometheusの使い方とベストプラクティスを網羅した書籍です。Prometheusは多くの組織で実運用されているメトリクスベースのモニタリングシステムであり、この書籍はサイト信頼性エンジニア、Kubernetes管理者、ソフトウェア開発者にとって実践的な指南書となります。learning.oreilly.comDatabase Reliability Engineering「Database Reliability Engineering」は、データベース管理の進化としてのDBRE（データベース リライアビリティエンジニアリング）をテーマにしたO'Reilly Media, Inc.からの294ページの重要な書籍です。2017年10月に出版され、データベースの信頼性に対する包括的なアプローチを提示しています。この書籍は、技術的な側面だけでなく、チームや組織全体の連携や継続的改善サイクルにも言及しています。特に、DBAからDBREへの進化に焦点を当て、多様なDBのプロフェッショナルが、他部門と協力し、システムの信頼性を高めるための自律的なアプローチについて詳述しています。書籍では、インフラとデータベースの効率的な構築や運用に関する技術的な知見が紹介されています。GitやChefなどを活用して、環境構築の自動化や人的ミスの排除に重点を置いています。メモリ管理、ストレージのチューニング、データベースのアーキテクチャなどに関する具体的な説明も盛り込まれており、データベースの効率的な運用に不可欠な要素として提示されています。バックアップ、セキュリティ、データベースのアーキテクチャなど、データベース運用のさまざまな側面についても触れられており、実際のDB運用業務での協業観点からも多くを学ぶことができます。また、インフラとDBaaS（Database As A Service）に関する技術的説明も含まれており、現代のデータベース運用における新しいトレンドとチャレンジに対応しています。全体的に、この書籍は技術的な知識にとどまらず、実際のデータベース運用における現場での協力や継続的な改善に関する洞察を提供するものであり、DBREとして成長したいプロフェッショナルにとって非常に価値のある一冊です。データベースリライアビリティエンジニアリング ―回復力のあるデータベースシステムの設計と運用作者:Laine Campbell,Charity MajorsオライリージャパンAmazon「Fundamentals of Data Engineering」は、O'Reilly Media, Inc.から2022年6月に出版された447ページの書籍で、データエンジニアリングの急速な成長に対応し、ソフトウェアエンジニア、データサイエンティスト、アナリストに全体的な理解を提供します。著者のJoe ReisとMatt Housleyが、データエンジニアリングのライフサイクルを通じて読者を導き、さまざまなクラウド技術を組み合わせて、組織と顧客のニーズを満たすシステムの計画と構築方法を紹介しています。本書では、データの生成、取り込み、オーケストレーション、変換、ストレージ、ガバナンスの概念を、どのようなデータ環境でも適用する方法を理解できます。また、データエンジニアリングの全体的な風景についての簡潔な概要を得ることができ、データ技術、アーキテクチャ、プロセスを選択する際のマーケティングハイプを切り抜けるためのベストプラクティスのエンドツーエンドフレームワークを使用する方法も提供されています。さらに、データガバナンスとセキュリティをデータエンジニアリングのライフサイクル全体に組み込む方法も学べるため、この本は、データエンジニアリングの基礎を学び、Platform EngineeringやSREなどの関連分野との関連性を理解するのに適した初心者向けのガイドです。learning.oreilly.comまた、DBに関しての知識はほぼ絶対に腐らないので「達人に学ぶDB設計 徹底指南書」や「達人に学ぶSQL徹底指南書 第2版」は読んでいて絶対に良いと思います。Observability Engineering「Observability Engineering」は、O'Reilly Media, Inc.から2022年5月に出版された318ページにわたる書籍で、現代の複雑なシステムにおけるオブザーバビリティの重要性と実践について深く掘り下げています。著者であるCharity Majors, Liz Fong-Jones, およびGeorge Mirandaは、オブザーバビリティがどのようにして開発速度を加速し、不規則な振る舞いの特定、ユーザー体験の理解を深めるかについて説明しています。この書籍は、オブザーバビリティの定義、クラウドネイティブアプリケーションへの応用、ソフトウェア開発ライフサイクル全体における影響、さらにはサービスレベル目標と共に機能するチームによるオブザーバビリティの利用方法など、多岐にわたるトピックを扱っています。特に、構造化イベントの利用とOpenTelemetryによる計装の重要性が強調されており、オブザーバビリティによってエンジニアがよりプロアクティブなデバッグを行い、迅速なフィードバックサイクルを回すことが可能になると述べられています。しかし、オブザーバビリティ関連のコストが高くつく可能性も指摘されており、エンジニアと経営者の両方に、そのビジネス価値を正しく理解し伝える必要性が強調されています。オブザーバビリティの導入には「作るか買うか」の選択があり、その機能要件、適切なテレメトリーデータのサンプリング方法、テレメトリーパイプラインを用いたデータ管理についても詳細に議論されています。さらに、組織全体でのオブザーバビリティの採用には、文化的な変化が伴うとし、その投資対効果や利害関係者との協力の重要性が説明されています。この書籍は、オブザーバビリティが企業の最終損益に与える影響を明確にし、組織におけるオブザーバビリティの成熟度モデルを提供することで、オブザーバビリティを組織に根付かせるための指針を提供しています。オブザーバビリティ・エンジニアリング作者:Charity Majors,Liz Fong-Jones,George Mirandaオーム社Amazon「Learning OpenTelemetry」は、2024年3月にO'Reilly Media, Inc.から出版される、OpenTelemetryの実践的な利用に焦点を当てた250ページの書籍です。著者のAustin ParkerとTed Youngは、OpenTelemetryの各コンポーネントと、これを使ったオブザーバビリティシステムの設定、運用、トラブルシューティング方法を紹介しています。OpenTelemetryにより、複数の高品質なテレメトリーデータソースが一本化され、効率的なオブザーバビリティが実現します。この書籍は、オブザーバビリティの基本から応用までを網羅し、特にアプリケーション開発者やインフラチームにとって貴重な情報源となるでしょう。また、前著「Observability Engineering」の内容を受け継ぎながら、OpenTelemetryを通じた具体的な実践方法を提供することで、現代の複雑なシステムにおけるオブザーバビリティの理解と活用をさらに深めます。learning.oreilly.comChaos Engineering「Chaos Engineering」は、O'Reilly Media, Inc.から2020年4月に発売された305ページの書籍で、カオスエンジニアリングについての実践的なガイドです。この分野での先駆者であるCasey RosenthalとNora Jonesが共著し、Netflixでの経験を基にしています。本書は、複雑なシステムを理解し、ビジネス目標に最適化しながらナビゲートする方法をエンジニアに示します。カオスエンジニアリングとは、マイクロサービスや分散技術を採用する企業が増えるにつれて高まるシステムの複雑性に対応する方法論です。 この手法を用いることで、複雑性を取り除くことはできないものの、システムの脆弱性を発見し、顧客への影響を及ぼす前に障害を防ぐことが可能になります。また、Google、Microsoft、Slack、LinkedInなどの業界専門家からの実世界の事例を通じて理論から実践への橋渡しがなされています。書籍では、カオスエンジニアリングプログラムをゲームデイを中心に設計し、高度にターゲットを絞った自動化された実験に進む方法が紹介されています。このように、システム内の複雑性を理解するための枠組みの設計や、継続的な協力的カオス実験のデザインについても詳述されています。ただし、本書の内容は非常に有意義である一方で、カオスエンジニアリングはその文化を前提としているため、導入のハードルは高いとされています。 実際、カオスエンジニアリングの成功は組織文化や思考の枠組みに大きく依存しており、組織全体の問題解決への取り組みとして考える必要があります。カオスエンジニアリングを導入するには、単に技術的な側面だけでなく、組織としての成熟度や文化的な準備が必要になります。カオスエンジニアリング ―回復力のあるシステムの実践作者:Casey Rosenthal,Nora JonesオライリージャパンAmazon「Security Chaos Engineering」は、O'Reilly Media, Inc.から2023年3月に発売された428ページの書籍で、セキュリティカオスエンジニアリングについての包括的なガイドです。著者のKelly ShortridgeとAaron Rinehartは、複雑なソフトウェアシステムの持続可能なレジリエンス（回復力）における挑戦に対処する方法を探求しています。セキュリティカオスエンジニアリングとは、不利なイベントに備え、それらが革新、迅速な動き、エンジニアリングおよびビジネス目標の達成を妨げないようにする手法です。 この書籍では、セキュリティプログラムの設計方法、ソフトウェア配信の各フェーズでの意思決定、複雑なシステムダイナミクスの理解、システムにおける意思決定を歪める技術的および組織的トレードオフのナビゲート方法について解説しています。また、カオス実験を通じて、ソフトウェアの品質とセキュリティに関する重要な仮定を検証する方法にも焦点を当てています。 このアプローチにより、組織はセキュリティカオスエンジニアリングを利用して、システムのレジリエンスを高め、広範な攻撃から保護する方法を学ぶことができます。さらに、本書では大手企業がセキュリティカオスエンジニアリングをどのように活用しているかの事例も紹介しており、読者に現実的な応用の例を提供します。この書籍は、サイバーセキュリティの課題に直面している方にとって、非常に価値のあるリソースとなるでしょう。learning.oreilly.comBuilding Microservices, 2nd Edition新しいものが常に良いわけではありません。見掛けの進捗や成果を得るために、シンプルで高品質なものを手放す必要はありません。シンプルで高品質なモノリスは、価値あるものとして当然のように評価されるべきです。それでも、マイクロサービスへの移行が必要な場合には、「Building Microservices, 2nd Edition」を参照してください、Sam Newman氏による612ページに及ぶ包括的な書籍で、マイクロサービスの構築、管理、そしてスケーリングに関する幅広いトピックを扱っています。この書籍は、モデリング、統合、テスト、デプロイメント、監視などの最新のマイクロサービスソリューションに関して、明快な例と実用的なアドバイスを提供しています。技術の時間の経過とともの進化を追いながら、マイクロサービスに関する理解を深めるのに非常に役立ちます。マイクロサービスアーキテクチャ 第2版作者:Sam Newmanオライリー・ジャパンAmazon「Monolith to Microservices」は、マイクロサービスの理念と移行プロセスに関する実践的な内容に焦点を当てています。具体的なテクノロジーではなく、マイクロサービスへの移行を決定する基準や手順に関する経験に基づく指針を提供しており、読者にとって非常に理解しやすく、実務においても有益な情報が得られるでしょう。これらの書籍は、マイクロサービスアーキテクチャの理解を深め、実際のシステム設計や運用において役立つ貴重なリソースです。モノリスからマイクロサービスへ ―モノリスを進化させる実践移行ガイド作者:Sam NewmanオライリージャパンAmazon分散システムの信頼性を深めたい方には、『Go言語による分散サービス―信頼性、拡張性、保守性の高いシステムの構築』がおすすめです。この書籍は、データ集約型アプリケーションの設計における核心的な概念と技術を網羅的に解説し、信頼性の高い分散システム構築に必要な知識を詳細に説明しています。マイクロサービスの主軸を担うSREとして立ち回るためには、『データ指向アプリケーションデザイン』は絶対に読んでおくべき書籍です。この書籍は、分散システムの複雑さと信頼性を理解し、それらを適切に管理するための実践的な知識を提供しています。SREとしての能力を高め、システムの効率性と安定性を保つために、この書籍の学びを活用することが重要です。動画www.youtube.com発表資料 speakerdeck.comAPI Design Patterns「API Design Patterns」は、ウェブおよび内部APIの設計に関する包括的なガイドで、480ページにわたりAPIパターンの原則、リソースレイアウト、データ型の取り扱い、標準手法、セキュアなAPIのための認証・検証方法などを深く掘り下げています。GoogleのAPI専門家JJ Geewax氏によって執筆されたこの書籍は、一貫性とスケーラビリティを確保するためのAPIデザインパターンを示し、APIの基本から高度な機能、特殊なケースまでを網羅しています。読者は、APIの設計とリファクタリングに必要な知識と技術を学び、APIをより効果的に構築するための実用的なアプローチを得られます。learning.oreilly.com「Web APIの設計」はArnaud LauretによるAPI設計の実践ガイドです。この書籍では、使いやすく、柔軟で堅牢なAPIを構築する方法について詳しく解説されています。特に、コマースサイトの例を用いてデータの配置方法やAPIの拡張性の維持方法が紹介されており、実装を重視しないアプローチが特徴です。メンテナンス性やドキュメント作成の重要性も強調されています。この書籍を通じて、読者はAPIの設計と構築に必要な基本原則と実用的な手法を学ぶことができます。Web APIの設計作者:Arnaud Lauret翔泳社Amazon現代のSREというか運用では、APIやデータベースに関する知識が不可欠です。これらの技術の理解がなければ、トラブルシューティングやシステムアーキテクトとして効果的に立ち振る舞うことは困難です。APIやDBはシステムの基盤を形成し、その運用と最適化に深く関わっているため、これらの要素を熟知していないと、複雑な問題解決や効率的なシステム設計ができません。したがって、技術者はAPIとDBの知識を身につけ、常に最新のトレンドを追い続けることが重要です。Systems Performance, 2nd Edition「Systems Performance, 2nd Edition」は、システムパフォーマンスの専門家であるBrendan Greggによる928ページに及ぶ包括的な書籍です。Linuxベースのオペレーティングシステムを例に取りながら、オペレーティングシステム、ハードウェア、アプリケーションの理論を要約し、最新のツールやテクニックを用いたCPU、メモリ、ファイルシステム、ディスク、ネットワーキングの最適化やパフォーマンス分析の方法論を提供しています。クラウド環境でのパフォーマンス上の課題や、perf、Ftrace、BPF（BCCおよびbpftrace）を用いたプロファイリングとトレーシングなど、実践的な技術にも深く掘り下げています。詳解 システム・パフォーマンス 第2版作者:Brendan Greggオーム社Amazonさらに、日本語の読者にとっても、この書籍の日本語版が利用可能であることは大きな利点なので感謝しましょう。learning.oreilly.comEfficient GoSRE（Site Reliability Engineering）は、信頼性、スケーラビリティ、効率性を最大限に高めるために、システムとソフトウェアの設計、構築、運用に深く関与します。この分野で成功するためには、最新の技術トレンド、プログラミング言語、システム管理のベストプラクティスを継続的に学ぶことが不可欠です。今回紹介する書籍は、技術スタックが微妙に違ったとしてもSREが直面する多様な課題に対応するのに役立つ知識とスキルを提供します。「Efficient Go」は、O'Reilly Media, Inc.から2022年11月に出版された502ページの書籍で、技術の進歩、急速な市場の変化、およびシステムの複雑化に伴い、しばしば避けがちなソフトウェア効率の問題に対処します。戦術的で可観測性に基づくパフォーマンスの最適化は、コスト削減とビジネス成功のためにすべての製品に不可欠です。著者のBartłomiej Płotkaは、システムを高速化し、リソースを少なく消費するために必要なツールと知識を提供し、Go言語を使用して日常的な効率性を向上させる方法をガイドします。また、この書籍は、効率性の目標を明確にし、最適化する方法、CPUやメモリなどの共通リソースを効果的に使用する方法、そして効率性を評価するためのメトリクス、ログ、トレース、(継続的な)プロファイリングを通じて、Prometheus、Jaeger、Parcaなどのオープンソースプロジェクトを使用する方法を含めています。また、Go言語のスライス、ジェネリクス、ゴルーチン、割り当てセマンティクス、ガベージコレクションなどの機能を効率的に使用する方法についても詳しく解説されています。learning.oreilly.com日本語の読者には嬉しいニュースがあります。これらの書籍は、日本語版も入手可能で、日本語話者が内容をより容易に理解し活用できるようになっています。効率的なGo ―データ指向によるGoアプリケーションの性能最適化作者:Bartłomiej Płotkaオーム社Amazon「Cloud Native Go, 2nd Edition」は、O'Reilly Media, Inc.から2024年10月に出版される520ページの書籍で、Go開発者がクラウドネイティブなアプリケーションの構成と構築を、低レベルのGo機能から中間レベルのパターン、高レベルのアーキテクチャの考慮事項に至るまで探求する内容が含まれています。初版もとても良い書籍だったので紹介しておきます。本書では、中級から上級の開発者がGoを使用して簡単だが完全に機能する分散型キーバリューストアを構築する方法を段階的に説明し、Goのジェネリクス、信頼性と可用性、メモリリーク、メッセージ指向ミドルウェアについて学ぶことができます。さらに、セキュリティと分散状態に関する新しい章は、安全な分散型クラウドネイティブアプリケーションを開発する上での重要な側面に焦点を当てています。この書籍を通じて、クラウドネイティブソフトウェアを構築するのに理想的な言語としてのGoの機能を理解し、スケーラブルな分散サービスを設計する際の課題の解決方法、チャネルやゴルーチンなどGoの低レベル機能を活用した信頼性の高いクラウドネイティブサービスの設計と実装、複雑な分散システムの効果的な構築と管理のためのパターン、抽象化、ツールの適用、およびGoを使用してクラウドネイティブサービスを構築し管理する際の障害の克服方法を学ぶことができます。learning.oreilly.com「Mastering Linux Shell Scripting」は、Packt Publishingから2018年4月に出版された284ページの書籍で、Bashシェルスクリプティングの複雑さをマスターし、企業でのシェルの力を解き放つための知識を提供します。この書籍は、Linux管理者やシステム管理者を対象にしており、日常のタスクを自動化し、時間と労力を節約したい方々に最適です。基本的なシェルスクリプティングとコマンドラインの経験が必要で、自動化したいタスクに精通していることが役立ちます。本書では、最初のBashスクリプトの作成、実行、デバッグ方法やユーザー入力を求めるインタラクティブなスクリプトの作成方法、複雑なウェブ設定ファイルを動的に編集するスクリプトの開発、AWKを使用したログファイルの検索と報告、関数をビルディングブロックとして使用する効果的なスクリプトの作成方法など、シェルスクリプティングのさまざまな側面を学ぶことができます。さらに、PythonなどBASHと異なるスクリプト言語の比較による情報に基づいた選択方法も提供されています。learning.oreilly.comこの本も日本語の書籍が必要があります。嬉しいですね。マスタリングLinuxシェルスクリプト 第2版 ―Linuxコマンド、bashスクリプト、シェルプログラミング実践入門作者:Mokhtar Ebrahim,Andrew Mallettオライリー・ジャパンAmazonImplementing Service Level Objectives「Implementing Service Level Objectives」は、サービスレベル目標（SLO）の専門家であるAlex Hidalgoによる402ページの詳細なガイドです。この書籍では、SLO文化をゼロから構築する方法について、実践的なアドバイスと詳細な分析を提供しています。読者は、ユーザーの視点からサービスの信頼性を意味深く測定するサービスレベル指標（SLI）を定義する方法、統計的・確率的分析を用いた適切なSLOターゲットの選択、エラーバジェットの利用、SLOに基づくアプローチに必要なツールとリソースの構築、そして組織のリーダーシップやユーザーに対してSLOデータを用いた意味のある報告をする方法を学びます。この書籍は、SLOベースの信頼性アプローチに取り組む組織の文化とツール作成に関心のあるすべての人にとって、理想的な入門書であるとおもいます。learning.oreilly.comさらに、日本語の読者にとっても、この書籍の日本語版が利用可能であることは大きな利点なので感謝しましょう。SLO サービスレベル目標 ―SLI、SLO、エラーバジェット導入の実践ガイド作者:Alex Hidalgoオーム社AmazonModern Software Engineering: Doing What Works to Build Better Software Faster「Modern Software Engineering: Doing What Works to Build Better Software Faster」は、連続的デリバリーの先駆者であるDavid Farleyによって書かれた、効果的なソフトウェア開発の本質を探求する256ページの書籍です。この本では、プログラマー、マネージャー、テックリードを対象に、ソフトウェア開発における「学習と探索」と「複雑性の管理」という二つの主要なテーマに注目し、マインドセットからコードの品質までを改善するための原則を提案しています。Farleyは、目指すべき目標の明確化、合理的な基準に基づくツールの選択、継続的な段階的進歩を促進するための作業とシステムの組織化、繁栄するシステムへの進行状況の評価など、多岐にわたるテーマを取り上げています。さらに、実験と経験主義からの学習、システムの複雑化に対する制御、厳格さと柔軟性のバランス、歴史と経験からの学び、良いソフトウェア開発アイデアと悪いものの区別など、具体的なアプローチを提供しています。この書籍は、より良いソフトウェアをより迅速に、そして楽しみながら作成するための実践的な洞察をソフトウェアプロフェッショナルに提供します。learning.oreilly.com日本語訳があるので喜んで呼びましょう！嬉しいですね！継続的デリバリーのソフトウェア工学　もっと早く、もっと良いソフトウェアを作るための秘訣作者:David Farley日経BPAmazon「Grokking Continuous Delivery」は、GoogleのエンジニアChristie Wilsonによって書かれた424ページに及ぶ書籍で、ソフトウェアのデリバリープロセスの自動化を解説しています。本書では、新規および既存プロジェクトのための効果的な連続デリバリー（CD）パイプラインの設計、ソフトウェアプロジェクトをリリース準備完了状態に保つ方法、効果的なテストの維持、複数アプリケーションにわたるCDのスケール化、パイプラインが適切なタイミングで正しいシグナルを提供することの確保、バージョンコントロールを信頼の原点として使用、そしてメトリクスを用いたデプロイメントの安全な自動化に焦点を当てています。この書籍は、CDパイプラインの設定と運用に関する実践的なガイドとして、ツールに依存しないアプローチを採用し、イラストや明快な説明、実践的な演習を通じてCDの設計と目的を解説しています。開発者やパイプラインデザイナーに向けたこの書籍は、CDを開発プロセスに追加したいソフトウェアエンジニアにとって理想的なリソースです。learning.oreilly.comLearning Test-Driven Development「Learning Test-Driven Development」は、277ページに及ぶSaleem Siddiqui著の書籍で、テスト駆動開発（TDD）をGo、JavaScript、Pythonの3つの言語で使用する方法を解説しています。この書籍は、コードがクリーンでエレガントであり、長期間にわたって機能し続けるためのTDDの活用方法を提供します。主な焦点は、ドメインの複雑さをユニットテスト駆動のアプローチで制御する方法、言語やテストフレームワーク、ドメイン概念を超えたTDDの機能、TDDが連続インテグレーションを可能にする方法、リファクタリングと再設計をTDDでサポートする方法にあります。加えて、JavaScriptでのシンプルかつ効果的なユニットテストハーネスの書き方、TDD中に生成されたユニットテストを用いた連続インテグレーション環境の設定についても学ぶことができます。この書籍は、Go、JavaScript、Pythonを使用してTDDを実践し、クリーンで整理されたコードを書くための実用的なガイドとなっています。learning.oreilly.com『Beyond Legacy Code』（邦題：レガシーコードからの脱却 ―ソフトウェアの寿命を延ばし価値を高める9つのプラクティス）は、レガシーコードとウォーターフォールモデルの問題点、アジャイルの導入と技術的卓越性の追求、小さなバッチでの開発の利点、協力し合う文化の重要性、テストファーストのアプローチとテスト駆動開発（TDD）の役割、設計を最後に行う創発的なアプローチ、レガシーコードのリファクタリング手法など、ソフトウェア開発における重要な9つのプラクティスを詳細に説明しています。この本は、ソフトウェア開発の現場で直面する問題に対処し、高品質なコードの作成を目指す開発者にとって貴重なガイドとなります。テスト駆動開発作者:ＫｅｎｔＢｅｃｋオーム社Amazon『Test Driven Development: By Example』は、テスト駆動開発（TDD）の導入によって、アプリケーション開発における恐怖を取り除くことを目指しています。著者Kent Beckは、TDDを通じてプログラマーが恐怖を克服し、より良いコミュニケーションと建設的な批判の受け入れを学ぶことができると考えています。TDDは、コードを継続的にテストしリファクタリングすることを基本としており、プログラマーが質の高い作業を行うための実践的な例を示しています。また、日本語版『テスト駆動開発』の翻訳者であるt_wadaさんのアカウントも非常に有益な情報源ですので、フォローすることをお勧めします。レガシーコードからの脱却 ―ソフトウェアの寿命を延ばし価値を高める9つのプラクティス作者:David Scott Bernsteinオライリー・ジャパンAmazonさらに、Kent Beckが最近書いた『Tidy First?』もオススメです。この本では、大規模なリファクタリングや作り直しではなくて雑然としたコードの「整頓」、すなわち読みやすくするためにコードを管理しやすいセクションに分割する方法について具体的なガイダンスを提供します。また、ソフトウェア設計の基本理論についての洞察も含まれており、いつどのようにコードを「整頓」するかについての実践的なアプローチが提案されています。プログラミング経験の向上や、大きな変更を小さく安全なステップで行う方法などについても掘り下げられています。learning.oreilly.com『Infrastructure as Code, 3rd Edition』は、Kief Morrisによって書かれた包括的な書籍で、Infrastructure as Code（IaC）の進化と主流化を背景にしています。この第三版では、組織の戦略的目標と課題をサポートするためのインフラストラクチャの設計と実装に焦点を当て、持続可能な成長のための成熟した基盤の構築の必要性を強調しています。本書では、宣言的および手続き的インフラストラクチャ言語の探求や、インフラストラクチャコードがプラットフォーム戦略とエンタープライズアーキテクチャにどう適合するか、そしてインフラストラクチャコードのテストと提供方法について探求しています。また、ソフトウェア設計とエンジニアリングからの教訓を活用して、成長を促進しつつ変化するニーズに適応できるようにインフラストラクチャコードベースを構築する方法についても解説されています。さらに、物理ハードウェアから仮想サーバー、クラウドネイティブクラスター、サーバーレスワークロードまで、現実世界のITシステムの複雑な風景をサポートするインフラストラクチャのパターンに焦点を当てています。この書籍は、自動化とクラウドを組み合わせ、アジャイルやDevOpsなどの先進的なアプローチを活用して、コンプライアンス、コスト、セキュリティ、運用品質の厳格なガバナンスを実現するワークフローと運用モデルについても紹介しており、テスト駆動開発（TDD） の観点からも、インフラストラクチャコードの品質と保守性を向上させるための重要な概念と実践を含んでいます。learning.oreilly.comシステム障害対応 実践ガイド『3カ月で改善！システム障害対応 実践ガイド』は、システム障害対応とプロセス改善の実践的なアプローチを提供する画期的な本です。著者の野村浩司氏と松浦修治氏は、それぞれNTTデータとリクルートでの豊富な経験を基に、実際の業務に即した方法を提供しています。本書の大きな特徴は、障害対応の具体的な手法を「メソッド化」している点です。理論だけでなく、「どうすればいいのか？」という実践的な問いに答えており、情報システム担当者や運用リーダーにとって最適な内容となっています。また、本書は障害対応の本質的価値にも触れています。障害対応の改善は、顧客満足度、従業員満足度、そして財務観点からもプラスの効果をもたらします。この点を丁寧に説明しており、運用担当者のモチベーション向上にも寄与する内容です。大規模な障害対応経験がない方でも、対応のイメージがつかめるように工夫されています。障害対応の難所にも言及し、読者が共感しやすい内容となっています。システム障害が起こりうるすべての現場の人々に推奨されるこの本は、システム障害対応をどのように捉え、判断し、対応するべきかについてのフローや表を豊富に掲載しています。これらは特にシステム障害マニュアルが整備されていないチームにとって非常に有用です。1000件以上の事例を分析し生み出されたこのメソッドは、障害対応改善のための役立つ雛形と共に、3カ月での改善を可能にします。インシデント分析から障害訓練まで、各プロセスに役立つ情報が満載です。システム障害対応における課題の特定から改善ステップまで、具体的なガイダンスを提供し、障害対応を改善するための実践的な指針を提供します。3カ月で改善！システム障害対応 実践ガイド インシデントの洗い出しから障害訓練まで、開発チームとユーザー企業の「協同」で現場を変える作者:野村 浩司,松浦 修治翔泳社AmazonWebエンジニアのための監視システム実装ガイドPractical Monitoringでも言及しましたし、ここまで読んでいるWebエンジニアにとって、システムの監視は不可欠なのは自明です。どれだけ高度な技術で構築されても、システムは放置すると壊れたり、理解しがたい状態に陥ることがあります。「Webエンジニアのための監視システム実装ガイド」は、監視テクノロジの動向から組織での実装まで、現場目線で解説する実用書です。最新ツールの説明から実装パターンの紹介、組織での実装に向けた態勢づくりまで、監視に必要な情報が網羅されています。本書は、監視システムの設計から導入、運用に至るまでの全てを包括的に理解するのに役立ちます。特にインシデント対応の実践的な知識や心構え、監視システムのアーキテクチャ例には注目です。また、MSPでの経験から得られた貴重なノウハウが詰まっており、既存の監視システムの知識を現代的なものにアップデートしたい方にも最適でタイパが最高に良い書籍です。Webエンジニアのための監視システム実装ガイド (Compass Booksシリーズ)作者:馬場 俊彰マイナビ出版AmazonSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本この書籍は、SRE（サイトリライアビリティエンジニアリング）の概念を分かりやすく説明しています。特にGoogleの事例を中心に、どのようにして大規模なサービスを安全かつ迅速にリリースし続けるかを示しています。SREがGoogle独自の手法ではなく、広くクラウドを利用する企業やエンジニアにとっても役立つ内容である点が強調されています。本書は、DevOpsを担当する方々、アプリケーション開発者、さらにはプログラミング未経験者にもSREという考え方を理解しやすくしています。その結果、「ざっくりなんとなくわかる」ような浅い理解を超えて、「すっきり」した理解を得ることができ、タイパを求める読者にとっては「最高」の参考書となります。SRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本: Googleが実践している新DevOps方法論作者:GGtop.jpAmazonさいごに情報技術の世界は日々進化しており、特にSRE分野では新たな知識と技術の習得が不可欠です。当ブログではSREにおける効率的な学習（タイパ）に役立つ書籍を多数紹介してきました。これらの資料が、読者の皆様のSREに対する理解の深化と実務能力の向上に貢献することを願っています。さらに、紹介した本にとどまらず、他にも多くの優れた書籍が存在しますので、是非探求してみてください。また、優先順位をどうしてもつけてほしいという人がいる場合はDevOps Roadmapがある参考にするといいと思います。私が所属する株式会社スリーシェイクでは、好奇心が強く手が動くエンジニアを求めています。私たちはSRE分野における多岐にわたる支援を提供しており、興味のある方は是非お問い合わせください。詳細は公式サイトでご覧いただけます。来年は当ブログでDevOpsやPlatform Engineeringの分野に焦点を当てる予定です。これらの分野は技術進歩が著しく、常に新しいアプローチやベストプラクティスが登場しています。それらをいかに実務に応用するかについて、深く掘り下げていきたいと考えています。私は「現場がさき、プラクティスがあと、原則をだいじに」という考え方を重視しており、適切な専門性と多様な事例に基づく知見を持ちながらも、業界全体での普及が十分でない現状があります。このため、時には根拠の不十分な「最強のDevOps」や「SREに外側だけ準じた独自のアプローチ」のような概念が生まれがちです。これは業界における深い理解と適切なプラクティスの普及に向けた取り組みが必要であることを示しています。このブログを読んでくださった皆様に心から感謝いたします。私たちは、SREの世界での成長と発展を目指し、皆様と一緒に学び続けることを楽しみにしています。ブログの購読をお願いすることで、私たちのモチベーションにも繋がります。次回の更新を楽しみにしていてください！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudコスト増減の定期監視とアドバイスを行うCostChecker-Agentの紹介]]></title>
            <link>https://sreake.com/blog/google-cloud-cost-checker-agent/</link>
            <guid>https://sreake.com/blog/google-cloud-cost-checker-agent/</guid>
            <pubDate>Thu, 25 Jan 2024 02:17:39 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 この記事では、” […]The post Google Cloudコスト増減の定期監視とアドバイスを行うCostChecker-Agentの紹介 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SQLBoilerによるPostgreSQLの操作についての話]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/01/23/161638</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/01/23/161638</guid>
            <pubDate>Tue, 23 Jan 2024 07:16:38 GMT</pubDate>
            <content:encoded><![CDATA[はじめにデータベースは、現代のアプリケーション開発において不可欠な要素です。特にリレーショナルデータベースは、その整合性と信頼性から幅広い用途で使用されています。しかし、リレーショナルデータベースを効率的に操作するためには、複雑なSQLクエリを記述し、アプリケーションのコードとデータベースのスキーマを適切に統合する必要があります。この作業は開発者にとって時間と労力を要するものです。この背景から、ORM（Object-Relational Mapping）ライブラリの利用が一般的になりました。ORMライブラリは、プログラミング言語のオブジェクトとデータベースレコードをマッピングし、SQLクエリの生成を抽象化して開発者がデータベース操作を容易に行えるようにサポートします。この記事では、Go言語でのデータベース操作を効率化するためのORMライブラリ「SQLBoiler」の活用方法について解説します。SQLBoilerは、Go言語に特化した強力なORMライブラリで、データベーススキーマに基づいてGoのコードを自動生成します。この自動生成機能により、開発者は煩雑なボイラープレートコードの記述を削減し、ビジネスロジックに集中できるようになります。本記事では、SQLBoilerの基本的な使用方法から生成されたコードの実際の利用方法までを段階的に紹介します。それでは、SQLBoilerを活用してGo言語でのデータベース操作を効率化する方法を見ていきましょう。ORM（Object-Relational Mapping）についてORMは、リレーショナルデータベースとプログラミング言語の間の橋渡しをする技術です。通常のデータベース操作に使用されるSQLとは異なり、ORMはプログラムのオブジェクト（今回のGoでは構造体）とデータベースレコードを自動で関連付け、SQL文の組み立てを可能にします。SQLBoilerの利点SQLBoilerはGo言語に特化した強力なORMライブラリで、データベーススキーマから直接Goのコードを生成します。この自動生成機能により、ボイラープレートコードの削減と開発効率の向上が図れます。ボイラープレートコードとはいくつかの異なるコンテキストでほとんどまたはまったく変更せずに再利用できるコンピュータ言語のテキストのことを指します。SQLBoilerの使用方法SQLBoilerを利用する際は、まずデータベーススキーマを定義します。以下は、著者、出版社、利用者、書籍、貸出記録を管理するスキーマの例です。-- 著者テーブルcreate table authors (  author_id serial primary key,  name varchar(100) not null);-- 出版社テーブルcreate table publishers (  publisher_id serial primary key,  name varchar(100) not null);-- 利用者テーブルcreate table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);-- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);-- 書籍テーブルcreate table books (  book_id serial primary key,  title varchar(255) not null,  author_id integer not null,  publisher_id integer not null,  isbn varchar(20),  year_published integer);-- 貸出記録テーブルcreate table loans (  loan_id serial primary key,  book_id integer not null,  user_id integer not null,  loan_date date not null,  return_date date);-- 外部キー制約の追加alter table books add constraint fk_books_author_id foreign key (author_id) references authors(author_id);alter table books add constraint fk_books_publisher_id foreign key (publisher_id) references publishers(publisher_id);alter table loans add constraint fk_loans_book_id foreign key (book_id) references books(book_id);alter table loans add constraint fk_loans_user_id foreign key (user_id) references users(user_id);このスキーマをもとに、SQLBoilerはGoのモデル、クエリビルダー、CRUD操作を自動生成します。この自動生成により、開発者は細かなデータベース操作を手作業で行う必要がなく、ビジネスロジックに集中できます。環境構築Dockerを使用して、PostgreSQLのバージョン16を動作させる環境を構築します。以下のdocker-compose.yamlファイルを使用してPostgreSQLサーバーを立ち上げます。version: '3'services:  postgres:    container_name: postgres    image: postgres:16    restart: always    ports:      - "5432:5432"    environment:      POSTGRES_USER: "postgres"      POSTGRES_PASSWORD: "postgres"サンプルデータの投げ込みv01_insert.sql を作成します-- 著者テーブルにサンプルデータを挿入INSERT INTO authors (name) VALUES ('Sample Author 1');INSERT INTO authors (name) VALUES ('Sample Author 2');INSERT INTO authors (name) VALUES ('Sample Author 3');-- 出版社テーブルにサンプルデータを挿入INSERT INTO publishers (name) VALUES ('Sample Publisher 1');INSERT INTO publishers (name) VALUES ('Sample Publisher 2');INSERT INTO publishers (name) VALUES ('Sample Publisher 3');-- 利用者テーブルにサンプルデータを挿入INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES ('Yamada', 'Taro', 'taro@example.com', '2021-01-01');INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES ('Suzuki', 'Hanako', 'hanako@example.com', '2021-02-01');INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES ('Tanaka', 'Ichiro', 'ichiro@example.com', '2021-03-01');-- 書籍テーブルにサンプルデータを挿入INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES ('Sample Book 1', 1, 1, '1234567890', 2021);INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES ('Sample Book 2', 2, 2, '0987654321', 2020);INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES ('Sample Book 3', 3, 3, '1122334455', 2022);-- 貸出記録テーブルにサンプルデータを挿入INSERT INTO loans (book_id, user_id, loan_date, return_date) VALUES (1, 1, '2022-01-01', '2022-01-15');INSERT INTO loans (book_id, user_id, loan_date, return_date) VALUES (2, 2, '2022-01-05', '2022-01-20');INSERT INTO loans (book_id, user_id, loan_date) VALUES (3, 3, '2022-01-10');投げ込み投げ込みpsql -h localhost -U postgres -d postgres -f v01_insert.sqlSQLBoilerのインストールSQLBoilerのインストール手順は以下の通りです。go install github.com/volatiletech/sqlboiler/v4@latestgo install github.com/volatiletech/sqlboiler/v4/drivers/sqlboiler-psql@latestPostgreSQL接続情報の設定（YAML形式）psql:  dbname: "postgres"  host: "127.0.0.1"  port: 5432  user: "postgres"  pass: "postgres"  sslmode: "disable"  whitelist:    - "authors"    - "publishers"    - "users"    - "books"    - "loans"whitelistにはコード生成の対象となるテーブルを明示的に指定します。この例では、著者、出版社、利用者、書籍、貸出記録の各テーブルを指定しています。SQLBoilerによるコード生成以下のコマンドを使うことで、SQLBoilerは指定されたデータベーススキーマに基づいてGoのモデルを生成します。sqlboiler psql -c config/database.yaml -o models --no-testsこのコマンドは、config/database.yamlに指定された設定を使用して、modelsディレクトリ内にモデルファイルを生成します。--no-testsオプションにより、テストファイルの生成をスキップします。生成されたファイル構成は以下の通りです。models/├── authors.go├── boil_queries.go├── boil_table_names.go├── boil_types.go├── boil_view_names.go├── books.go├── loans.go├── psql_upsert.go├── publishers.go└── users.go生成されたファイルの解説例として、books.goファイルの一部を見てみましょう。このファイルはデータベースのbooksテーブルに対応するGoの構造体と、それに関連する関数を定義しています。Book構造体type Book struct {    BookID        int         `boil:"book_id" json:"book_id" toml:"book_id" yaml:"book_id"`    Title         string      `boil:"title" json:"title" toml:"title" yaml:"title"`    AuthorID      int         `boil:"author_id" json:"author_id" toml:"author_id" yaml:"author_id"`    PublisherID   int         `boil:"publisher_id" json:"publisher_id" toml:"publisher_id" yaml:"publisher_id"`    Isbn          null.String `boil:"isbn" json:"isbn,omitempty" toml:"isbn" yaml:"isbn,omitempty"`    YearPublished null.Int    `boil:"year_published" json:"year_published,omitempty" toml:"year_published" yaml:"year_published,omitempty"`}Book構造体はbooksテーブルの各列をフィールドとして持ち、タグを用いてデータベースの列名とのマッピングを定義しています。SQLBoilerによるコード生成の詳細解説SQLBoilerによって生成されたモデルファイルは、リレーショナルデータベースと連携するための多様な機能を提供します。ここでは、具体的なコード例を使って、関連関数やリレーションシップ、フックについて解説します。生成されたファイルには、CRUD操作（作成、読み取り、更新、削除）を行うための関数も含まれています。例えば、Insert関数はBookオブジェクトをデータベースに挿入し、Update関数は既存のレコードを更新します。また、Delete関数はレコードを削除し、Reload関数はデータベースから最新の情報を再読み込みします。関連関数の例// Insert a single record using an executor.func (o *Book) Insert(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) error {    // ...関数の本体...}// Update uses an executor to update the Book.func (o *Book) Update(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) (int64, error) {    // ...関数の本体...}// Delete deletes a single Book record with an executor.func (o *Book) Delete(ctx context.Context, exec boil.ContextExecutor) (int64, error) {    // ...関数の本体...}// Reload refetches the object from the database.func (o *Book) Reload(ctx context.Context, exec boil.ContextExecutor) error {    // ...関数の本体...}これらの関数は、Bookオブジェクトを使用して、データベースに対する挿入、更新、削除、再読み込みの操作を行います。リレーションシップの例// Author pointed to by the foreign key.func (o *Book) Author(mods ...qm.QueryMod) authorQuery {    // ...関数の本体...}// Publisher pointed to by the foreign key.func (o *Book) Publisher(mods ...qm.QueryMod) publisherQuery {    // ...関数の本体...}これらの関数は、Bookオブジェクトが参照する外部キー（Author, Publisher）に基づいて、関連するデータを取得するためのクエリを作成します。SQLBoilerはテーブル間のリレーションシップを認識し、それに対応する関数も生成します。例えば、BookがAuthorとPublisherに関連している場合、それぞれのリレーションシップに対応するLoadAuthorやLoadPublisherなどの関数が生成されます。フックの例// AddBookHook registers your hook function for all future operations.func AddBookHook(hookPoint boil.HookPoint, bookHook BookHook) {    // ...関数の本体...}フックを使用すると、データベース操作の前後に特定の処理を実行できます。例えば、AddBookHook関数は、特定のタイミングで実行されるカスタムフックを登録します。SQLBoilerは各CRUD操作の前後に実行されるフック（Hook）もサポートしています。これにより、データベース操作の前後にカスタムロジックを実行することが可能です。SQLBoilerの応用と実践的な利用方法Go言語とSQLBoilerを使用して、リレーショナルデータベースでのデータ操作を行う方法を解説します。この記事では、実際のコードを使用して、SQLBoilerで生成されたモデルを利用してデータベースの書籍テーブルを操作する一連のプロセスを紹介します。主にSelect、Insert、Update、Upsert、Delete、Reloadといった基本的なデータベース操作をカバーし、Eager Loadingやデバッグ出力などの高度な機能についても触れます。以下のサンプルコードは、PostgreSQLデータベースに接続し、複数の異なる操作を実行するGoプログラムです。このプログラムでは、SQLBoilerで生成されたモデルを使用して、書籍の情報を取得、挿入、更新、アップサート、削除し、データベースの状態を再読み込みする操作を行います。package mainimport (    "context"    "database/sql"    "fmt"    "log"    _ "github.com/lib/pq"    "github.com/nwiizo/workspace_2024/sqlboiler/models" // 生成されたモデルのインポート    "github.com/volatiletech/null/v8"    "github.com/volatiletech/sqlboiler/v4/boil"    "github.com/volatiletech/sqlboiler/v4/queries/qm")func main() {    // データベース接続    db, err := sql.Open(        "postgres",        "postgres://postgres:postgres@localhost/postgres?sslmode=disable",    )    if err != nil {        log.Fatal(err)    }    defer db.Close()    ctx := context.Background()    // SELECT: 全ての書籍を取得    allBooks, err := models.Books().All(ctx, db)    if err != nil {        log.Fatal(err)    }    for _, book := range allBooks {        log.Printf("Book: %+v\n", book)    }    fmt.Println("Select: 高度なクエリでの書籍の取得")    books, err := models.Books(        models.BookWhere.Title.EQ("Specific Title"),        models.BookWhere.AuthorID.EQ(1),        qm.Limit(10),    ).All(ctx, db)    if err != nil {        log.Fatal(err)    }    for _, book := range books {        fmt.Println("Book:", book.Title)    }    fmt.Println("Count: 書籍の数を数える")    count, err := models.Books(models.BookWhere.Title.EQ("Specific Title")).Count(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println("Count:", count)    fmt.Println("Exists: 特定の条件に一致する書籍が存在するかを確認")    exists, err := models.Books(models.BookWhere.Title.EQ("Specific Title")).Exists(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println("Exists:", exists)    fmt.Println("Insert: 書籍の挿入")    newBook := &models.Book{        Title:         "New Book",        AuthorID:      1,        PublisherID:   1,        Isbn:          null.StringFrom("1234567890"),        YearPublished: null.IntFrom(2023),    }    err = newBook.Insert(ctx, db, boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println("Update: 書籍の更新")    newBook.Title = "Updated Title"    _, err = newBook.Update(ctx, db, boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println("Upsert: 書籍のアップサート")    upsertBook := &models.Book{        BookID:        newBook.BookID,        Title:         "Upserted Title",        AuthorID:      2,        PublisherID:   2,        Isbn:          null.StringFrom("0987654321"),        YearPublished: null.IntFrom(2024),    }    err = upsertBook.Upsert(ctx, db, true, []string{"book_id"}, boil.Infer(), boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println("Delete: 書籍の削除")    _, err = newBook.Delete(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println("Reload: 書籍の再読み込み")    err = newBook.Reload(ctx, db)    if err != nil {        if err == sql.ErrNoRows {            fmt.Println("Reload: 書籍が見つかりませんでした")        } else {            log.Fatal(err)        }    }    // Eager Loading の例    // ユーザーと関連する書籍を取得    // user, err := models.FindUser(ctx, db, 1, qm.Load("Books"))    // if err != nil {    //     log.Fatal(err)    // }    // for _, book := range user.R.Books {    //     fmt.Println("Book:", book.Title)    // }    // デバッグ出力の例    // boil.DebugMode = true    // books, _ = models.Books().All(ctx, db)    // boil.DebugMode = false    // Raw Query の例    // _, err = queries.Raw("SELECT * FROM books WHERE title = 'New Book'").QueryAll(ctx, db)    // if err != nil {    //     log.Fatal(err)    // }    // Hook の例    // func myBookHook(ctx context.Context, exec boil.ContextExecutor, book *models.Book) error {    //     fmt.Println("Book Hook Triggered")    //     return nil    // }    // models.AddBookHook(boil.BeforeInsertHook, myBookHook)    // null パッケージの使用例    // newBook.Isbn = null.StringFromPtr(nil) // ISBN を null に設定}このプログラムでは、まずデータベースに接続し、全ての書籍を取得する操作から始まります。その後、特定の条件に一致する書籍の数を数えたり、特定の条件に一致する書籍が存在するかを確認したりする操作を行います。その後、新しい書籍をデータベースに挿入し、その書籍の情報を更新します。次に、アップサート操作を行い、特定の書籍を削除し、最終的には削除された書籍の情報を再読み込みします。このプロセスは、SQLBoilerを使ってGo言語でデータベース操作を行う際の典型的なフローを示しています。また、コメントアウトしていたコードに関しても一部は解説させてください。高度なクエリ構築SQLBoilerでは、qmパッケージを利用して複雑なクエリを組み立てることができます。例えば、特定の条件を満たす書籍を取得するために、以下のようなクエリを構築することが可能です。books, err := models.Books(    models.BookWhere.Title.EQ("Specific Title"),    models.BookWhere.AuthorID.EQ(1),    qm.Limit(10),).All(ctx, db)if err != nil {    log.Fatal(err)}for _, book := range books {    fmt.Println("Book:", book.Title)}このコードは、タイトルが"Specific Title"であり、かつ著者IDが1の書籍を最大10件まで取得します。Eager LoadingSQLBoilerを使うと、関連するレコードを事前にロードするEager Loadingも可能です。たとえば、あるユーザーに関連するすべての書籍を取得するには、以下のようにします。user, err := models.FindUser(ctx, db, 1, qm.Load("Books"))if err != nil {    log.Fatal(err)}for _, book := range user.R.Books {    fmt.Println("Book:", book.Title)}この例では、IDが1のユーザーに関連するすべての書籍を取得しています。デバッグ出力SQLBoilerを使用する際、boil.DebugModeを有効にすることで、実行されたSQLクエリを確認することができます。これはデバッグ時に非常に便利です。boil.DebugMode = truebooks, _ := models.Books().All(ctx, db)boil.DebugMode = falseRaw Queryの使用SQLBoilerでは、生のSQLクエリを直接実行することも可能です。これは特定のシナリオで必要となる複雑なクエリを実行する際に役立ちます。_, err = queries.Raw("SELECT * FROM books WHERE title = 'New Book'").QueryAll(ctx, db)if err != nil {    log.Fatal(err)}このコードは、タイトルが'New Book'のすべての書籍を取得します。Hookの設定SQLBoilerでは、データベース操作の前後に実行されるHookを設定することができます。これはデータの整合性を保つための追加のロジックを実行する際に便利です。func myBookHook(ctx context.Context, exec boil.ContextExecutor, book *models.Book) error {    fmt.Println("Book Hook Triggered")    return nil}models.AddBookHook(boil.BeforeInsertHook, myBookHook)この例では、書籍がデータベースに挿入される前に特定の処理を行うHookを設定しています。nullパッケージの活用SQLBoilerでは、nullパッケージを利用して、データベースのNULL値を扱うことができます。これにより、NULL許容のフィールドを安全に操作することが可能になります。newBook.Isbn = null.StringFromPtr(nil) // ISBNをNULLに設定newBook.YearPublished = null.IntFrom(2023) // 発行年を設定このコードでは、ISBNをNULLに設定し、発行年を2023に設定しています。まとめSQLBoilerは、Go言語でリレーショナルデータベースを効率的に操作するための強力なORM（Object-Relational Mapping）ライブラリです。データベーススキーマから直接Goのコードを生成し、開発者が細かなデータベース操作を手作業で行う負担を軽減します。SQLBoilerは、CRUD操作、リレーションシップの管理、フックの実装など、多様な機能を提供し、開発者がビジネスロジックに集中できる環境を整えます。この記事では、SQLBoilerの基本的な使用方法から、生成されたコードの実際の利用方法までを解説しました。まず、データベーススキーマの定義とSQLBoilerの環境設定を行い、サンプルデータの挿入を通じて実際のデータベース操作を準備しました。次に、SQLBoilerによって生成されたGoのモデルを利用して、Select、Insert、Update、Upsert、Delete、Reloadといった一連のデータベース操作を行うプロセスを紹介しました。これらの操作は、リレーショナルデータベースとGoプログラムの間のインタラクションを容易にし、開発プロセスを加速します。SQLBoilerは、Go言語でのデータベース操作を簡素化し、開発速度を向上させることが可能です。手動でのデータベース操作コードの記述が多い開発者にとって、SQLBoilerは大きな助けとなります。SQLBoilerの詳細や使い方については、SQLBoiler GitHubページを参照してください。［改訂3版］内部構造から学ぶPostgreSQL―設計・運用計画の鉄則 (Software Design plus)作者:上原 一樹,勝俣 智成,佐伯 昌樹,原田 登志技術評論社Amazon参考資料GitHub - d-tsuji/awesome-go-orms: ORMs for Go, most starred on GitHub.GitHub - avelino/awesome-go: A curated list of awesome Go frameworks, libraries and softwareGoのORMの人気ランキングを年ごとにまとめてみたGo言語 SQLBoilerでタイプセーフに複数条件のWhere句を書く #Go - QiitaGoのORM決定版 Genをはじめよう #Go - QiitaGoにおけるORMと、SQLBoiler入門マニュアル]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[K8sGPT: Prometheus Analyzers]]></title>
            <link>https://zenn.dev/tozastation/articles/71015cc5b95b4e</link>
            <guid>https://zenn.dev/tozastation/articles/71015cc5b95b4e</guid>
            <pubDate>Tue, 23 Jan 2024 03:00:00 GMT</pubDate>
            <content:encoded><![CDATA[v0.3.26 からPrometheus の Analyzer がリリースされましたデモ映像はこちらhttps://github.com/k8sgpt-ai/k8sgpt/pull/855本PR作成者の Daniel Clark さんは Google の方 (2024/01/18時点)で，prometheus-engine (Cloud Managed Service for Prometheus (GMP)) に多くのコントリビューションをされています． 先にまとめPrometheus Analyzer には現在二つの機能が含まれるConfig Analyzer ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[openssl s_client で SMTP 認証]]></title>
            <link>https://blog.1q77.com/2024/01/smtp-auth-plain-with-openssl-command/</link>
            <guid>https://blog.1q77.com/2024/01/smtp-auth-plain-with-openssl-command/</guid>
            <pubDate>Tue, 23 Jan 2024 02:44:23 GMT</pubDate>
            <content:encoded><![CDATA[Amazon SES で SMTP を使ってメール送信したい場合、IAM User の credentials をちょいと加工してやる必要があります。 Amazon SES SMTP 認証情報を取得 これで、変換した値が正しいことを確認するために実際に]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PostgreSQLのsqldefによるDBスキーマ管理で遊んだ。 ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/01/22/124414</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/01/22/124414</guid>
            <pubDate>Mon, 22 Jan 2024 03:44:14 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事では、データベーススキーママイグレーションツールであるsqldefで遊んだので使用方法とその特徴について解説します。sqldefはRidgepoleに触発されて開発されたツールで、データベースのスキーマ変更を容易に行えるように設計されています。github.com1. sqldefとはsqldefは「The easiest idempotent MySQL/PostgreSQL/SQLite3/SQL Server schema management by SQL」と謳われるDBスキーマ変更管理ツールです。GitHub上で公開されており（sqldef GitHubリポジトリ）、MySQL、PostgreSQL、SQLite3、SQL Serverに対応しています。このツールを使用することで、CREATE TABLE文を書くだけで対象テーブルの比較とALTER TABLE文の生成・実行が可能になります。sqldefは、データベースの現在のスキーマとユーザーが提供するCREATE TABLE文を比較し、必要な変更を自動的に検出します。このプロセスにより、データベースの変更をより簡単かつ迅速に行うことができます。また、sqldefはidempotent（冪等性）を持つため、同じ変更を何度適用しても、結果として得られるスキーマは同じになります。これにより、データベースの変更管理がより安全かつ予測可能になります。このツールは特に、開発環境やテスト環境での迅速な変更適用や、本番環境への安全な変更のロールアウトに非常に有用です。2. sqldefの利点従来のスキーママイグレーションツールではCREATE TABLE文とALTER TABLE文を二重管理する必要がありましたが、sqldefを使用すると新規作成DDL文のみを管理するだけで済むため、DBA（データベース管理者）および開発者の作業負担が大幅に軽減されます。また、CI/CDパイプラインにも簡単に組み込むことができるため、デプロイメントプロセスの自動化と整合性の向上が期待できます。2.1 sqldefの最大の利点は、そのシンプルさと効率性従来のツールでは、データベースの初期状態を構築するためのCREATE TABLE文と、既存のデータベースを変更するためのALTER TABLE文の両方を管理する必要がありました。しかし、sqldefを使えば、CREATE TABLE文のみを管理すれば十分で、ALTER TABLE文は自動的に生成されます。これにより、変更の管理が簡略化され、DBAと開発者の双方の作業負担が大幅に減少します。2.2 CI/CDパイプラインへの組み込みさらに、sqldefはCI/CDパイプラインとの統合が容易で、これによりデプロイメントプロセスの自動化が可能になります。データベースの変更をコードレビューとテストのプロセスに組み込むことで、変更の品質を向上させ、本番環境への変更のロールアウトをより安全に行うことができます。このように、sqldefは開発チームの生産性を高め、データベースの整合性を維持するための強力なツールです。3. やってみる実際にsqldefを使用するためには、適切な環境構築が不可欠です。このセクションでは、Dockerを活用してPostgreSQLサーバーをセットアップし、sqldefをダウンロードして設定する手順を具体的に説明します。その後、具体的なスキーマ定義を作成し、それをデータベースに適用してみます。3.1 環境構築Dockerを使用して、PostgreSQLのバージョン16を動作させる環境を構築します。以下のdocker-compose.yamlファイルを使用してPostgreSQLサーバーを立ち上げます。version: '3'services:  postgres:    container_name: postgres    image: postgres:16    restart: always    ports:      - "5432:5432"    environment:      POSTGRES_USER: "postgres"      POSTGRES_PASSWORD: "postgres"3.2 sqldefのダウンロードと設定sqldefの実行可能バイナリをGitHubからダウンロードし、設定します。これにより、任意のプラットフォームでsqldefを利用することが可能になります。curl -LO https://github.com/sqldef/sqldef/releases/download/v0.16.15/psqldef_darwin_amd64.zipunzip psqldef_darwin_amd64.ziprm psqldef_darwin_amd64.zip./psqldef --versionリリース情報はこちらから適切なアーキテクチャを選んでください。tarコマンドのオプションは覚えられず忘れているのでzipファイルを選びました。github.com3.3 スキーマの適用v01_library.sqlファイルを用意し、図書館システムの基本的なテーブルを作成します。このスクリプトでは、PostgreSQLのserial型を使用して主キーの自動インクリメントを行い、外部キー制約を設定しています。これにより、データベースの整合性が保たれ、アプリケーションの安定性が向上します。ただし、sqldefを用いたスキーマの適用は効率的ですが、特に外部キー制約に関連する場合には注意が必要です。外部キー制約はデータベースの整合性を保つために重要な役割を果たしますが、これらの制約を含むテーブルの変更を管理する際には、特定の課題が生じることがあります。今回のケースでは、これらの課題を避けるための工夫を取り入れながら、スキーマを適用していきます。-- 著者テーブルcreate table authors (  author_id serial primary key,  name varchar(100) not null);-- 出版社テーブルcreate table publishers (  publisher_id serial primary key,  name varchar(100) not null);-- 利用者テーブルcreate table users (  user_id serial primary key,  user_name varchar(100) not null,  email_address varchar(100),  registration_date date not null);-- 書籍テーブルcreate table books (  book_id serial primary key,  title varchar(255) not null,  author_id integer not null,  publisher_id integer not null,  isbn varchar(20),  year_published integer);-- 貸出記録テーブルcreate table loans (  loan_id serial primary key,  book_id integer not null,  user_id integer not null,  loan_date date not null,  return_date date);-- 外部キー制約の追加alter table books add constraint fk_books_author_id foreign key (author_id) references authors(author_id);alter table books add constraint fk_books_publisher_id foreign key (publisher_id) references publishers(publisher_id);alter table loans add constraint fk_loans_book_id foreign key (book_id) references books(book_id);alter table loans add constraint fk_loans_user_id foreign key (user_id) references users(user_id);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v01_library.sql3.4 スキーマの変更ユーザーテーブル（users）の構造を変更するための新しいDDLスクリプトを作成し、sqldefを使用して適用します。この変更には、列の追加、列の型変更、NOT NULL制約の追加が含まれ、データベースの設計を現代的な要件に合わせることができます。-- v02_library.sqlcreate table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v02_library.sql-- Apply --ALTER TABLE "public"."users" ADD COLUMN "family_name" varchar(100) NOT NULL;ALTER TABLE "public"."users" ADD COLUMN "given_name" varchar(100) NOT NULL;ALTER TABLE "public"."users" ALTER COLUMN "email_address" TYPE varchar(254);ALTER TABLE "public"."users" ALTER COLUMN "email_address" SET NOT NULL;-- Skipped: DROP TABLE "public"."authors";-- Skipped: DROP TABLE "public"."books";-- Skipped: DROP TABLE "public"."loans";-- Skipped: DROP TABLE "public"."publishers";ALTER TABLE "public"."users" DROP COLUMN "user_name";3.5 ユニークキー制約の追加最後に、ユーザーテーブルにユニークキー制約を追加するためのDDLスクリプトを適用します。ユニークキー制約を追加することで、メールアドレスの重複を防ぎ、データの一意性を保証することができます。-- v03_library.sql-- ユーザーテーブルの作成create table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);-- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v03_library.sql-- Apply ---- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);-- Skipped: DROP TABLE "public"."authors";-- Skipped: DROP TABLE "public"."books";-- Skipped: DROP TABLE "public"."loans";-- Skipped: DROP TABLE "public"."publishers";4. 結論sqldefは、データベーススキーマの変更を簡単かつ効率的に行うことができる強力なツールです。特に、継続的インテグレーション/継続的デリバリー（CI/CD）パイプラインの一部としてスキーマ変更の自動化を行う際に非常に有効であり、開発プロセスの加速とデータベース整合性の向上に大きく寄与します。ただし、すべてのケースに適用可能なわけではなく、その特性と限界点を十分理解することが重要です。今回は実験的に使用してみた結果、このようなツールの有用性を実感しました。また、実際に運用経験のある方々の貴重なフィードバックが参考文献に掲載されていますので、より深い洞察を得るためにも、ぜひ参考にしていただくと良いでしょう。プログラマのためのSQL 第4版 すべてを知り尽くしたいあなたに作者:Joe Celko翔泳社Amazon5.参考文献sqldefDocker Compose overviewpostgres | Docker Official ImageGo製マイグレーションツールまとめsqldefとpgrollを利用したPostgreSQLでのスキーマブルーグリーンデプロイメントDBスキーマ変更管理ツール sqldef を試してみたsqldefへのSQL Server対応のコントリビュート 〜OSS活動を通して紐解くDBマイグレーションツールの実装〜ミラティブのサーバサイドをGo + Clean Architectureに再設計した話マイグレーションツールをsqldefに移行した話]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[dockerで行う12ステップで作る組込みOS自作入門]]></title>
            <link>https://zenn.dev/satoken/articles/kozos-step-by-step</link>
            <guid>https://zenn.dev/satoken/articles/kozos-step-by-step</guid>
            <pubDate>Sun, 21 Jan 2024 13:10:45 GMT</pubDate>
            <content:encoded><![CDATA[はじめに冬休みに12ステップで作る 組込みOS自作入門を完走したをkozosを完走しました。そのときの備忘録になります。12STEPの各内容は以下のようになっています。第1部 ブート・ローダーの作成1stステップ 開発環境の作成2ndステップ シリアル通信3rdステップ 静的変数の読み書き4thステップ シリアル経由でファイルを転送する5thステップ ELFフォーマットの展開6thステップ もう一度，Hello World第2部 OSの作成7thステップ 割込み処理を実装する8thステップ スレッドを実装する9thステップ 優先度スケジューリング...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[アヒルに話かけると仕事をしてくれるが責任まではとってくれない]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/01/19/202246</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/01/19/202246</guid>
            <pubDate>Fri, 19 Jan 2024 11:22:46 GMT</pubDate>
            <content:encoded><![CDATA[正直に言えば、責任も取ってほしい。k0kubunさんのAIにプログラミング作業を奪われているに対する感想です。はじめに現代のソフトウェア開発は、生成AIの急速な進化によって前例のない大規模な変革を経験しています。かつては時間と労力を要していた複雑な作業が、生成AIの導入により効率的かつ迅速に行えるようになりました。特に、DevOpsやSREの分野での生成AIの利用は注目に値します。自動化されたプロセスにより、開発サイクルが加速し、ソフトウェアの品質と信頼性が向上しました。その結果、継続的インテグレーションやデリバリーのプロセスが効率化され、開発チームは新しい機能の実装やバグ修正に迅速に対応できるようになりました。この技術革新は、ユーザーだけでなく、開発者自身の作業プロセスにも大きな変化をもたらしており、良い影響を与えることが期待されます。本稿では、自動コード生成の利点と課題、開発者の責任、そして最終的には人間中心の設計と開発の重要性について探求します。自動コード生成と開発者への影響自動コード生成の分野では、生成AIは開発者の作業を劇的に簡素化し、プログラミングの高速化と精度向上を実現しています。この技術の進歩により、開発者はルーチン作業から解放され、より複雑で創造的な問題に集中できるようになりました。生成AIによる自動コード生成は、開発者が新しいアイデアやソリューションを迅速に試すことを可能にし、従来の開発手法とは異なる新しいアプローチを提供しています。ただし、このような革新的な進歩には新たなリスクも伴い、セキュリティやプライバシー、倫理的な課題に対応する必要があります。開発者はこれらの新たな課題に適切に対処するために、生成AIの活用を慎重に管理し、適切に調整する必要があります。アヒルに話かけると仕事は終わる「ラバーダック・デバッグ」というテクニックは、問題をゴム製のアヒルに説明することで解決策を見つける効果的な方法です。同様に、生成AIと対話することは、仕事を効率的に終えるためのアプローチとなります。ソフトウェア開発において、生成AIは開発者の負担を大幅に軽減し、ルーチン作業から複雑なタスク処理までをサポートします。この進化により、開発者は日常の作業から一歩離れ、より戦略的で創造的な活動に集中できるようになります。その結果、プロジェクトの進捗が加速し、開発サイクルが短縮されることが可能になります。Happy Trees アヒル お風呂用おもちゃ 大型 バスダック スクイークゴム アヒル ベビーシャワー 7インチHappy Trees MauLaikaAmazon生成AIを活用した創造的な問題解決の促進自動コード生成を含む生成AIの技術は、開発者を日常の作業から解放し、より複雑で創造的な問題解決に集中させます。生成AIによるバグ検出やパフォーマンス最適化機能は、ソフトウェアの信頼性と効率性を飛躍的に向上させています。これらの技術の進歩は、開発プロセスの速度向上だけでなく、ソフトウェアの全体的な品質向上と持続的な改善ももたらしています。生成AIは、開発者が直面する課題に新たな解決策を提供し、イノベーションの可能性を広げています。働きたくないイタチと言葉がわかるロボット 人工知能から考える「人と言葉」作者:川添愛朝日出版社Amazonアヒルは仕事をしてくれるが責任はとれない生成AIはソフトウェア開発に多くの利点を提供しますが、その適用範囲と機能には限界があります。生成AIはプロジェクトの特定の側面を助ける強力なツールとして機能しますが、最終的な製品の品質、セキュリティ、倫理的な問題解決は開発者の裁量と判断に委ねられています。例えば、生成AIは基本的なコード生成や一般的な問題の検出に優れた能力を発揮しますが、複難なシステムアーキテクチャの設計や新しい種類のセキュリティリスクに対処するためには、開発者の深い技術知識と経験が必要です。開発者の役割と責任したがって、開発者は生成AIの提案を批判的に評価し、プロジェクトのニーズに応じて適切な調整や改善を行う責任があります。生成AIは開発プロセスを支援し、効率化を促進できますが、最終的な製品の品質と機能性の確保は開発者の責任です。開発者は、生成AIによって提供されるソリューションの品質を確保し、プロジェクトの目標達成に向けて責任を持つことが求められます。また、生成AIの活用に伴う潜在的なリスクを理解し、適切に管理することも重要です。このようにして、生成AIと人間の開発者は協力しながら、高品質で安全なソフトウェアを創造できます。人間中心の設計と開発生成AIの技術進歩は、ソフトウェア開発のプロセスを変革していますが、この変革の中心には常にユーザー（社内外問わず）が存在します。人間中心の設計と開発は、技術が人間のニーズ、価値、行動に基づいて開発されるべきだという理念に根ざしています。このアプローチは、生成AIを含む技術が、ユーザーや開発者のエクスペリエンスを向上させ、よりアクセスしやすく、開発しやすく、使いやすい製品を生み出すことを目指しています。人間中心の設計では、開発初期段階からユーザーフィードバックを組み込み、継続的なテストと改善を通じて、ユーザーの実際の使用状況を反映したソリューションを提供します。開発者は、ユーザーの視点を理解し、その視点を設計と開発のプロセスに統合することで、より人間中心のアプローチを取り入れることができます。これには、多様なユーザーグループとの協力、ユーザビリティテストの実施、アクセシビリティガイドラインの遵守が含まれます。このようにして、開発者は生成AIの潜在的な利点を最大限に活用しながら、技術が人間のニーズに適応することを保証できます。同じ開発者に対しても良い行動を示すことが重要であり、互いに刺激し合い、共に成長することで、全体の業界の水準を引き上げることに寄与します。人間の仕事として最後に残るものソフトウェア開発において生成AIの役割が拡大している現在、人間中心の設計と開発の原則は、開発者が直面する責任と課題に新たな次元を加えています。技術の進歩は、単に作業の自動化や効率化を超え、開発者に対して、ユーザーの深い理解と共感を基にした意思決定を行うことを要求しています。このコンテキストでは、倫理的な判断、戦略的思考、そして人間の感情と経験を理解する能力が、人間の仕事として最後に残る核心的な要素となります。開発者は、生成AIを活用することで得られる多大な利益を享受しつつも、最終的な製品がユーザーの実生活において意味のある価値を提供することを確実にする責任を負います。このためには、技術の知識と同様に、人間と社会に関する深い洞察が必要です。生成AIの進歩によって開発者の役割が変化しても、最終的な目標は変わりません。それは、人間の生活を豊かにし、より良い未来を形成することです。したがって、人間中心の設計と開発の精神は、ソフトウェア開発における人間の仕事の未来を形作る基盤となります。このようにして、生成AIと人間の開発者は、高品質で、使いやすく、全ての人にとって有益なソフトウェアを共に創出していくことができます。プロフェッショナリズムの重要性プロフェッショナリズムは、技術の進歩と人間中心の設計の精神を融合させる上で不可欠な要素です。生成AIのような強力なツールを活用する際、開発者は単に技術的なスキルを超えた行動規範を持つ必要があります。プロフェッショナルとして、不十分な対応や適切ではない言い訳に直面した際には、我々はしばしば失望と不満を感じます。このため、開発者は常に高い倫理基準を維持し、ユーザーのニーズと安全を最優先する責任があります。問題に直面した場合、待つのではなく、積極的に解決策を探し、適切な対応を行うことが求められます。このプロフェッショナリズムは、生成AIの倫理的使用、人間中心の設計原則、そして最終的にはソフトウェアの品質とユーザー体験の向上に寄与します。開発者としてのプロ意識を維持することは、技術の可能性を最大限に引き出し、同時に潜在的なリスクを適切に管理する上で不可欠です。また、プロフェッショナリズムは、開発者が生成AIを含むあらゆるツールを使って良質なソフトウェアを生み出すための基盤を形成します。さらに、プロフェッショナリズムは、チーム内外での信頼と尊敬を築く上でも重要です。開発者が高いプロ意識を示すことで、チームメンバー間の協力が促され、プロジェクトの成功に向けた効果的なコミュニケーションと協働が可能になります。また、組織外の利害関係者やエンドユーザーに対しても、開発者の行動は組織の信頼性と評判を高めることに直接影響します。最終的に、プロフェッショナリズムの重要性は、技術的なスキルだけでなく、倫理的、人間的な価値を尊重し、それらを実務に統合することで、より良いソフトウェア製品とサービスを提供することにあります。生成AIの時代においても、開発者のプロ意識は、革新的で持続可能なソリューションの創出と、社会全体の利益に対する貢献の核心をなすものです。www.nhk.jp]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PR-Agentとその類似システムの解説]]></title>
            <link>https://sreake.com/blog/pr-agent-and-similar-systems/</link>
            <guid>https://sreake.com/blog/pr-agent-and-similar-systems/</guid>
            <pubDate>Thu, 18 Jan 2024 09:38:27 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake事業部でインターンをしている村山です。そのようなコードレビューの作業に対し、今日ではLLMを使用したレビュー用のツールが開発されています。今回はそのレビューツールの一つであるPR-Agentを中心に […]The post PR-Agentとその類似システムの解説 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【Istio⛵️】Istioによって抽象化されるEnvoyのHTTPSリクエスト処理の仕組み]]></title>
            <link>https://hiroki-hasegawa.hatenablog.jp/entry/2024/01/16/013404</link>
            <guid>https://hiroki-hasegawa.hatenablog.jp/entry/2024/01/16/013404</guid>
            <pubDate>Mon, 15 Jan 2024 16:34:04 GMT</pubDate>
            <content:encoded><![CDATA[この記事から得られる知識この記事を読むと、以下を "完全に理解" できます✌️Istioのサイドカーメッシュを題材にしたEnvoyの設定の抽象化について様々なサービスメッシュツール (特に、Istio、Consul、Cilium、など) でも流用できるEnvoyの知識についてこの記事から得られる知識01. はじめに02. 様々なリソースによるEnvoy設定の抽象化サービスメッシュ外からのHTTPSマイクロサービス間のHTTPSサービスメッシュ外へのHTTPS03. istio-proxyコンテナによるHTTPS処理Istioコントロールプレーンの仕組みサービスメッシュ外からのHTTPSマイクロサービス間のHTTPSサービスメッシュ外へのHTTPS04. EnvoyによるHTTPS処理Envoyの設定の種類フィルターフィルターの一覧フィルターチェーンの仕組み05. リソースの設定からEnvoy設定への翻訳各リソースとEnvoyの設定の関係一覧サービスメッシュ外からのHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめるマイクロサービス間のHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめるサービスメッシュ外へのHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめる06. 翻訳されたEnvoy設定値を見てみるEnvoyの現在の設定を出力するリスナーを出力するルートを出力するクラスターを出力するエンドポイントを出力する証明書を出力するサービスメッシュ外からのHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod側のistio-proxyコンテナマイクロサービス間のHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod側のistio-proxyコンテナサービスメッシュ外へのHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod (Istio EgressGateway Pod) 側のistio-proxyコンテナ07. おわりに謝辞記事関連のおすすめ書籍01. はじめにどうも、俺 (REMIX) feat. Istioニキ a.k.a. いすてぃ男です。Istioは、Envoyを使用したサービスメッシュを実装します。IstioがKubernetesリソースやIstioカスタムリソースに基づいてEnvoyの設定を抽象化してくれるため、開発者はEnvoyをより簡単に設定できます。Envoyの設定の抽象化は、Envoyを使用したサービスメッシュ (例：Istioサイドカーメッシュ/アンビエントメッシュ、Consul、Istioから得られた学びを土台に登場したCiliumサイドカーフリーメッシュ、など) に共通しています。つまり、次々に登場するEnvoyによるサービスメッシュツールに振り回されないようにするためには、ツールがどのようにEnvoyを抽象化するのかを理解しておく必要があります。そこで今回は、IstioサイドカーメッシュがEnvoyのHTTPSリクエストの処理をどのように抽象化するのかを解説します。また、抽象化されたEnvoyがHTTPSリクエストを処理する仕組みも一緒に解説します。これらの知識は、様々なサービスメッシュツールで流用できるはずです。それでは、もりもり布教していきます😗02. 様々なリソースによるEnvoy設定の抽象化まずは、どのようなリソースがHTTPSリクエストの処理に関係しているのかを、HTTPSリクエストの方向に分けて解説していきます。istio-proxyコンテナやEnvoyについては、次章以降で解説します。サービスメッシュ外からのHTTPSサービスメッシュ外から内にHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。flowchart TD    送信元 -.->|HTTPS| Gateway    Gateway([⛵️ Gateway]) -.-> VirtualService    VirtualService([⛵️ VirtualService]) -.-> DestinationRule    DestinationRule([⛵️ DestinationRule]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class Gateway,VirtualService,DestinationRule,Service,Endpoints blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。クライアントは、サービスメッシュ外からL7ロードバランサーにHTTPSリクエストを送信します。L7ロードバランサーは、Istio IngressGateway PodにHTTPSリクエストを送信します。Istio IngressGateway Podは、宛先Podとの間で相互TLS認証を実施します。Istio IngressGateway Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) に応じて、HTTPSリクエストを宛先PodにL7ロードバランシングします。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineeringマイクロサービス間のHTTPSサービスメッシュ内のPodから別のPodにHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。flowchart TD    送信元 -.->|HTTPS| VirtualService    VirtualService([⛵️ VirtualService]) -.-> DestinationRule    DestinationRule([⛵️ DestinationRule]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class VirtualService,DestinationRule,Service,Endpoints blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。送信元Podは、宛先Podとの間で相互TLS認証を実施します。送信元Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) の設定に応じて、HTTPSリクエストを宛先PodにL7ロードバランシングします。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSサービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) にHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。複数のVirtualServiceとDestinationが登場するため、これらには便宜上 X と Y をつけています。flowchart TD    送信元 -.->|HTTPS| VirtualServiceX    VirtualServiceX([⛵️ VirtualService X]) -.-> DestinationRuleX    DestinationRuleX([⛵️ DestinationRule X]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.-> Gateway    Gateway([⛵️ Gateway]) -.-> VirtualServiceY    VirtualServiceY([⛵️ VirtualService Y]) -.-> DestinationRuleY    DestinationRuleY([⛵️ DestinationRule Y]) -.-> ServiceEntry    ServiceEntry([⛵️ ServiceEntry]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class Gateway,VirtualServiceX,VirtualServiceY,DestinationRuleX,DestinationRuleY,Service,Endpoints,ServiceEntry blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。送信元Podは、HTTPSリクエストの宛先がServiceEntryでエントリ済みか否かの設定に応じて、HTTPSリクエストの宛先を切り替えます。宛先がエントリ済みであれば、送信元PodはHTTPSリクエストの宛先にIstio EgressGateway Podを選択します。宛先が未エントリであれば、送信元PodはHTTPSリクエストの宛先に外のシステムを選択します。送信元Podは、Istio EgressGateway Podとの間で相互TLS認証を実施します。(1) で宛先がエントリ済であったとします。送信元Podは、HTTPSリクエストの向き先をIstio EgressGateway Podに変更します。送信元Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) の設定に応じて、Istio EgressGateway PodにL7ロードバランシングします。Istio EgressGateway Podは、HTTPSリクエストをエントリ済システムにL7ロードバランシングします。Using Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio▶︎ Istio EgressGatewayの必要性についてHTTPSリクエストを送信できます。しかし、Istio EgressGatewayを使わないと、マイクロサービスからistio-proxyコンテナを経由せずに外部システムに直接HTTPSリクエストを送信できるようになってしまい、システムの安全性が低くなります。Istio / Accessing External Services03. istio-proxyコンテナによるHTTPS処理前章では、KubernetesリソースやIstioカスタムリソースによって抽象化されたEnvoyまで言及しませんでした。本章では、解説をもう少し具体化します。Istioは、Envoyプロセスを持つistio-proxyコンテナを作成します。このistio-proxyコンテナを使用してどのようにHTTPSリクエストを処理しているのかを、HTTPSリクエストの方向に分けて解説します。Envoyの設定については、次章以降で解説します。Istioコントロールプレーンの仕組みEnvoyの設定を抽象化する責務を担うのは、Istioコントロールプレーン (discoveryコンテナ) です。Istioコントロールプレーンは異なる責務を担う複数のレイヤーから構成されています。レイヤー名      責務    Config ingestionレイヤー            kube-apiserverからKubernetesリソースやIstioカスタムリソースの設定を取得します。Istioの初期から名前は変わっていません。          Config translationレイヤー                   リソースの設定をEnvoy設定に変換します。Istioの初期ではConfig Data Modelレイヤーという名前で、執筆時点 (2024/01/16) で名前が変わっています。          Config servingレイヤー            Envoyの設定や証明書をPod内のistio-proxyコンテナに配布します。Istioの初期では、Proxy Servingレイヤーという名前で、執筆時点 (2024/01/16) で名前が変わっています。          図中の番号に沿って、Istioコントロールプレーンの仕組みを解説します。Config ingestionレイヤーにて、 Istioコントロールプレーンはkube-apiserverにHTTPSリクエストを送信します。ここで、KubernetesリソースやIstioカスタムリソースの設定を取得します。Config translationレイヤーにて、取得したリソースの設定をEnvoyの設定に変換します。Config servingレイヤーにて、Envoyの設定や証明書をPod内のistio-proxyコンテナに配布します。双方向ストリーミングRPCのため、istio-proxyコンテナがConfig servingレイヤーにリクエストを送信し、これらを取得することもあります。istio/architecture/networking/pilot.md at 1.20.2 · istio/istio · GitHub一文带你彻底厘清 Isito 中的证书工作机制-赵化冰的博客 | Zhaohuabing Blog▶︎ Config servingレイヤーにあるXDS-APIについてAPIがあります。このXDS-APIは、Envoyの設定に関するエンドポイント (LDS-API、RDS-API、CDS-API、EDS-API、ADS-API、など) や、証明書配布のエンドポイント (例：SDS-API) を持ちます。以下の記事で解説していますため、もし気になる方はよろしくどうぞ🙇🏻‍▶︎ Istioカスタムリソースのコントローラーについてトロールプレーンは、前述の責務以外にカスタムコントローラーとしての責務も担います。以前は、IstioOperatorがカスタムコントローラーの責務を担っていましたが、執筆時点 (2024/01/16) ではIstioOperatorが非推奨となりました。IstioOperatorの代わりに、Istioコントロールプレーンがこれを担うようになりました👍🏻istio/architecture/networking/pilot.md at 1.20.2 · istio/istio · GitHubサービスメッシュ外からのHTTPSサービスメッシュ外から内にHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。クライアントは、サービスメッシュ外からL7ロードバランサーにHTTPSリクエストを送信します。L7ロードバランサーは、Istio IngressGateway PodにHTTPSリクエストを送信します。Istio IngressGateway Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナに送信します (リダイレクトは不要)。Istio IngressGateway Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。Istio IngressGateway Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。宛先Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先マイクロサービスに送信します。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineering▶︎ Pod内のiptablesについてiptablesは、リクエストが必ずistio-proxyコンテナを経由するように、istio-proxyコンテナにリクエストをリダイレクトします。iptablesのルールを書き換えるのはistio-initコンテナです。Istioは、istio-proxyコンテナと同じタイミングで、istio-initコンテナをPodにインジェクションします (Istio IngressGatewayとIstio EgressGatewayのPodは除きます)。画像引用元：SoByteistio-initコンテナは、istio-iptablesコマンドを実行し、iptablesのルールを書き換えます。また、istio-initコンテナはルールを書き換えた後に終了するため、Podの起動後にPod内に残りません👍🏻$ istio-iptables \    -p 15001 \    -z 15006 \    -u 1337 \    -m REDIRECT \    -i * \    -x \    -b * \    -d 15090,15020Sidecar injection, transparent traffic hijacking, and routing process in Istio explained in detail | by Jimmy Song | MediumIstio / pilot-agent▶︎ Istio IngressGateway Pod内のiptablesについてistio-proxyコンテナにリクエストをリダイレクトする必要がありません。そのため、Istioはiptablesのルールを書き換えるistio-initコンテナをIstio IngressGateway Podにインジェクションしません。つまり、Istio IngressGateway Pod内のiptablesのルールはデフォルトのままになっています👍🏻マイクロサービス間のHTTPSサービスメッシュ内のPodから別のPodにHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。送信元Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。送信元Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。送信元Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。宛先Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先マイクロサービスに送信します。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSサービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) にHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。送信元Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。送信元Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。この時、ServiceEntryで宛先がエントリ済みか否かに応じて、HTTPSリクエストの宛先を切り替えます。宛先がエントリ済みであれば、istio-proxyコンテナはHTTPSリクエストの宛先にIstio EgressGateway Podを選択します。宛先が未エントリであれば、istio-proxyコンテナはHTTPSリクエストの宛先に外のシステムを選択します。ここでは、宛先がエントリ済であったとします。送信元Pod内のistio-proxyコンテナは、HTTPSリクエストをIstio EgressGateway PodにL7ロードバランシングします。Istio EgressGateway Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナに送信します (リダイレクトは不要)。Istio EgressGateway Pod内のistio-proxyコンテナは、HTTPSリクエストをエントリ済システムにL7ロードバランシングします。▶︎ Istio EgressGateway Pod内のiptablesについてistio-proxyコンテナにリクエストをリダイレクトする必要がありません。そのため、Istioはiptablesのルールを書き換えるistio-initコンテナをIstio EgressGateway Podにインジェクションしません。つまり、Istio EgressGateway Pod内のiptablesのルールはデフォルトのままになっています👍🏻Using Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio04. EnvoyによるHTTPS処理前章では、istio-proxyコンテナ内のEnvoyの設定まで、言及しませんでした。本章では、もっと具体化します。EnvoyがHTTPSリクエストを処理する仕組みを解説します。Envoyの設定の種類HTTPSリクエストを処理する場合、Envoyの設定が以下の順で紐付き、HTTPSリクエストを送信元から宛先まで届けます。flowchart TD    送信元 -.->|HTTPS| リスナー    リスナー(リスナー) -.-> リスナーフィルター    subgraph  ""      リスナーフィルター(リスナーフィルター) -.-> ネットワークフィルター      ネットワークフィルター(ネットワークフィルター) -.-> HTTPフィルター    end    HTTPフィルター(HTTPフィルター) -.-> ルート    ルート(ルート) -.-> クラスター    クラスター(クラスター) -.-> エンドポイント    エンドポイント(エンドポイント) -.->|HTTPS| 宛先classDef sly fill: #CCFFFF, stroke: black;class 送信元 slyclassDef yellow fill: #FFFF88, stroke: black;class 宛先 yellowclassDef red fill: #EA6B66, font-weight :bold, stroke: black;class リスナー,リスナーフィルター,ネットワークフィルター,HTTPフィルター,ルート,クラスター,エンドポイント red各処理がどのような責務を担っているのかをもう少し詳しく見てみましょう。図中の番号に沿って、EnvoyがHTTPSリクエストを処理する仕組みを解説します。送信元からのHTTPSリクエストの宛先ポートで、リスナーを絞り込みます。通信の種類 (例：HTTP、HTTPS、TCP、UDP、Unixドメインソケット、など) に応じてフィルターを選び、各フィルターがパケットのヘッダーを処理します。もしHTTPSであれば、送信元との間でTLS接続を確立し、パケットのL7のアプリケーションデータを復号化します。フィルターを使用して、HTTPSリクエストの宛先ポートで、ルートを絞り込みます。フィルターを使用して、HTTPSリクエストの宛先ホストやパスで、クラスターを絞り込みます。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、クラスター配下のエンドポイントを選びます。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、エンドポイントにL7ロードバランシングします。Life of a Request — envoy 1.30.0-dev-372a26 documentation▶ TCPリクエストを処理する場合についてエストを処理する場合、フィルターに紐づくのはルートですが、TCPリクエストの場合はそうではありません。TCPリクエストを処理する場合、フィルターにクラスターが紐づきます👍🏻flowchart TD    送信元 -.->|TCP| リスナー    リスナー(リスナー) -.-> リスナーフィルター    subgraph  ""      リスナーフィルター(リスナーフィルター) -.-> ネットワークフィルター    end    ネットワークフィルター(ネットワークフィルター) -.-> クラスター    クラスター(クラスター) -.-> エンドポイント    エンドポイント(エンドポイント) -.->|TCP| 宛先classDef sly fill: #CCFFFF, stroke: black;class 送信元 slyclassDef yellow fill: #FFFF88, stroke: black;class 宛先 yellowclassDef red fill: #EA6B66, font-weight :bold, stroke: black;class リスナー,リスナーフィルター,ネットワークフィルター,クラスター,エンドポイント redDebugging Your Debugging Tools: What to do When Your Service Mesh Goes Down | PPTフィルターフィルターの一覧Envoyのフィルターは、Envoyの機能を拡張するための設定です。HTTPSリクエストを処理するためには、リスナーフィルター、ネットワークフィルター、HTTPフィルター、といったフィルターが必要になります。全ては解説しきれないため、HTTPSリクエストを処理するための代表的なフィルターをいくつか抜粋しました。ただ、 Istioはこれらのフィルターをデフォルトで有効にしてくれている ため、開発者がEnvoyのフィルターを設定する場面は少ないです。逆をいえば、Istioを介さずにEnvoyをそのまま使用する場合、開発者がEnvoyのフィルターを自前で設定する必要があります👍🏻フィルターの種類      HTTPSリクエストの処理に必要なフィルター(一部抜粋)      説明    リスナーフィルター      Original Destination      istio-proxyコンテナへのリダイレクト前の宛先情報をEnvoyが取得できるようにします。Pod内のiptablesがHTTPSリクエストをistio-proxyコンテナにリダイレクトすると、HTTPSリクエストの宛先がistio-proxyコンテナに変わってしまいます。ただし、iptablesはリダイレクト前の宛先をカーネル上のSO_ORIGINAL_DSTという定数に格納してくれています。Envoyは、カーネル上のSO_ORIGINAL_DSTから本来の宛先を取得し、プロキシします。    HTTP Inspector      EnvoyがHTTPを検知できるようにします。    TLS Inspector      EnvoyがTLSを検知できるようにします。TLSを検知した場合、EnvoyはTLSに関する処理を実行します。例えば、DownstreamTlsContextは、リスナーフィルター直後に、送信元との間でTLS接続を確立し、パケットのL7のアプリケーションデータを復号化します。また、UpstreamTlsContextは、クラスターの処理時に、宛先との間でTLS接続を確立し、L7のアプリケーションデータを暗号化します。    ネットワークフィルター      HTTP connection manager      Envoyが、L7のアプリケーションデータを読み取り、また後続のHTTPフィルターを制御できるようにします。    HTTPフィルター      Router      Envoyがポート番号でルート、ホストやパスでクラスターを絞り込めるようにします。    gRPC-Web      EnvoyがHTTP/1.1で受信したHTTPSリクエストをHTTP/2に変換し、gRPCサーバーにプロキシできるようにします。    Filters — envoy 1.30.0-dev-372a26 documentation▶︎ Istioがデフォルトで有効にするEnvoyの設定についてistio-proxyコンテナは、イメージのビルド時に、あらかじめ用意しておいたEnvoyの設定ファイルを組み込みます。そのため、istio-proxyコンテナ内のEnvoyは、多くの設定をデフォルトで有効にできます。Istioを利用する開発者が、EnvoyがHTTPSリクエストを処理するために必要なフィルターを有効にしなくてよいのも、Istioのおかげです。Istioほんまにありがとな🙏🙏🙏  istio/pilot/docker/Dockerfile.proxyv2 at 1.20.2 · istio/istio · GitHubistio/tools/packaging/common/envoy_bootstrap.json at 1.20.2 · istio/istio · GitHubフィルターチェーンの仕組みEnvoyは、複数のフィルターからなるフィルターチェーンを実行し、HTTPSを処理します。図中の番号に沿って、Envoyのフィルターチェーンの仕組みを解説します。各フィルターの機能は、前述したフィルターの一覧を参考にしてください🙇🏻リスナーフィルター (Original Destination、HTTP Inspector、TLS Inspector、など) を実行します。(1) でTLS InspectorがTLSを検知した場合、DownstreamTlsContextで宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。ネットワークフィルター (HTTP connection manager、など) を実行します。HTTPフィルター (Router、gRPC-Web、など) を実行します。Life of a Request — envoy 1.30.0-dev-372a26 documentation▶ TCPリクエストを処理する場合についてHTTPSリクエストを処理する場合にのみ使用します。それ以外の通信の種類 (例：TCP、UDP、Unixドメインソケット、など) の場合は、HTTPフィルターを使用しません。例えば、TCPリクエストの場合、ネットワークフィルターのTCP proxyフィルターを使用します👍🏻TCP proxy — envoy 1.30.0-dev-372a26 documentation05. リソースの設定からEnvoy設定への翻訳いよいよです🔥Istioが各リソースをいずれのEnvoyの設定に翻訳しているのかを解説します。表で対応関係の一覧を示した後、istio-proxyコンテナ内のEnvoyに当てはめました。各リソースとEnvoyの設定の関係一覧Istioコントロールプレーンは、KubernetesリソースやIstioカスタムリソースの設定をEnvoyの設定に翻訳し、処理の流れに当てはめます。以下の通り、各リソースがいずれのEnvoyの設定を抽象化するのかを整理しました。リソースによっては、Envoyの複数の設定を抽象化します。なお、Istioの用意したEnvoyのフィルターのデフォルト値を変更するユースケースが少ないため、これを抽象化するEnvoyFilterについては言及しません。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅                  ✅    ルート      ✅                  ✅                      クラスター      ✅                        ✅      ✅      ✅    エンドポイント            ✅                  ✅      ✅          Debugging Your Debugging Tools: What to do When Your Service Mesh Goes Down | PPTWebinar: Debugging your debugging tools; What to do when your service mesh goes down in production? - YouTubeサービスメッシュ外からのHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ外からのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。Gatewayは、Istio IngressGatewayの一部として使用します。ServiceEntryは、使用しないリソースのため、×としています。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅            ×      ✅    ルート      ✅                  ✅            ×          クラスター      ✅                        ✅      ×      ✅    エンドポイント            ✅                  ✅      ×          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      PeerAuthentication    送信元      リスナー      ✅            ✅      ✅            ✅    ルート      ✅                  ✅                クラスター      ✅                        ✅      ✅    エンドポイント            ✅                  ✅          宛先      リスナー      ✅                  ✅            ✅    ルート      ✅                  ✅                クラスター      ✅                        ✅      ✅    エンドポイント            ✅                  ✅          ▶︎ 送信元と宛先のNamespaceについてistio-ingress) においた方が良いです。マイクロサービスとは異なるNamespaceにIstio IngressGatewayを置くことで、Istio IngressGatewayをアップグレードしやすくなったり、他から障害の影響を受けにくくなります🙆🏻‍♂️istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyでほとんど同じリソースが登場しますが、 Gatewayは送信元Envoyだけで登場します。リソースの種類だけに着目すると、以下になります。Gatewayが送信元Envoyだけで登場することがわかりやすくなりました。マイクロサービス間のHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ内のPodから別のPodへのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。GatewayとServiceEntryは、使用しないリソースのため、×としています。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ×      ✅            ×      ✅    ルート      ✅            ×      ✅            ×          クラスター      ✅            ×            ✅      ×      ✅    エンドポイント            ✅      ×            ✅      ×          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      VirtualService      DestinationRule      PeerAuthentication    送信元      リスナー      ✅            ✅            ✅    ルート      ✅            ✅                クラスター      ✅                  ✅      ✅    エンドポイント            ✅            ✅          宛先      リスナー      ✅            ✅            ✅    ルート      ✅            ✅                クラスター      ✅                  ✅      ✅    エンドポイント            ✅            ✅          istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyで、同じリソースが登場します。リソースの種類だけに着目すると、以下になります。送信元または宛先Envoyで同じリソースが登場することがわかりやすくなりました。サービスメッシュ外へのHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) へのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。Gatewayは、Istio EgressGatewayの一部として使用します。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅                  ✅    ルート      ✅                  ✅                      クラスター      ✅                        ✅      ✅      ✅    エンドポイント            ✅                  ✅      ✅          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。他の場合とは異なり、送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、異なるリソースが抽象化します。PeerAuthenticationだけは、話を簡単にするために送信元と宛先が同じNamespaceであると仮定しているので、同じリソースが抽象化します。送信元Envoyの設定の抽象化で登場するリソースが宛先では登場せず、逆も然りです。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      Gateway      VirtualServiceX      〃Y      DestinationRuleX      〃Y      ServiceEntry      PeerAuthentication    送信元      リスナー      ✅                  ✅                              ✅    ルート      ✅                  ✅                                  クラスター      ✅                              ✅                  ✅    エンドポイント            ✅                        ✅                      宛先      リスナー                  ✅            ✅                        ✅    ルート                              ✅                            クラスター                                          ✅      ✅      ✅    エンドポイント                                          ✅      ✅          ▶︎ 送信元と宛先のNamespaceについてistio-egress) においた方が良いです。マイクロサービスとは異なるNamespaceにIstio EgressGatewayを置くことで、Istio EgressGatewayをアップグレードしやすくなったり、他から障害の影響を受けにくくなります🙆🏻‍♂️istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、異なるリソースが抽象化します。PeerAuthenticationだけは、話を簡単にするために送信元と宛先が同じNamespaceであると仮定しているので、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyで同じリソースが登場しません 。リソースの種類だけに着目すると、以下になります。送信元または宛先Envoyで同じリソースが登場しないことがわかりやすくなりました。06. 翻訳されたEnvoy設定値を見てみる前章では、Envoyの具体的な設定値まで、言及しませんでした。本章では、さらに具体化します。各リソースの設定の翻訳によって、Envoyの具体的にどのような設定値になっているのかを解説します。Envoyの現在の設定を出力するEnvoyは、現在の設定を確認するためのエンドポイント (/config_dump) を公開しています。これにHTTPSリクエストを送信し、具体的な設定値を出力してみましょう👍🏻リスナーを出力する/config_dumpのクエリストリングにresource={dynamic_listeners}をつけると、Envoyのリスナーを出力できます。$ kubectl exec \    -it foo-pod \    -n foo-namespace \    -c istio-proxy \    -- bash -c "curl http://localhost:15000/config_dump?resource={dynamic_listeners}" | yq -PAdministration interface — envoy 1.30.0-dev-372a26 documentationConfigDump (proto) — envoy 1.30.0-dev-372a26 documentation▶ 宛先情報を見やすくするyqコマンドについてJSON形式で設定を出力します。JSON形式だと見にくいため、yqコマンドでYAMLに変換すると見やすくなります👍ルートを出力する/config_dumpのクエリストリングにresource={dynamic_route_configs}をつけると、Envoyのルートを出力できます。$ kubectl exec \    -it foo-pod \    -n foo-namespace \    -c istio-proxy \    -- bash -c "curl http://localhost:15000/config_dump?resource={dynamic_route_configs}" | yq -PAdministration interface — envoy 1.30.0-dev-372a26 documentationConfigDump (proto) — envoy 1.30.0-dev-372a26 documentationクラスターを出力する/config_dumpのクエリストリングにresource={dynamic_active_clusters}をつけると、Envoyのクラスターを出力できます。$ kubectl exec \    -it foo-pod \    -n foo-namespace \    -c istio-proxy \    -- bash -c "curl http://localhost:15000/config_dump?resource={dynamic_active_clusters}" | yq -PAdministration interface — envoy 1.30.0-dev-372a26 documentationConfigDump (proto) — envoy 1.30.0-dev-372a26 documentationエンドポイントを出力する/config_dumpのクエリストリングにinclude_edsをつけると、Envoyのエンドポイントを出力できます。$ kubectl exec \    -it foo-pod \    -n foo-namespace \    -c istio-proxy \    -- bash -c "curl http://localhost:15000/config_dump?include_eds" | yq -PAdministration interface — envoy 1.30.0-dev-372a26 documentationConfigDump (proto) — envoy 1.30.0-dev-372a26 documentationSupported load balancers — envoy 1.30.0-dev-372a26 documentation証明書を出力する/config_dumpのクエリストリングにresource={dynamic_active_secrets}をつけると、証明書を出力できます。$ kubectl exec \    -it foo-pod \    -n foo-namespace \    -c istio-proxy \    -- bash -c "curl http://localhost:15000/config_dump?resource={dynamic_active_secrets}" | yq -PConfigDump (proto) — envoy 1.30.0-dev-372a26 documentationサービスメッシュ外からのHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：50000) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50000) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50000) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：50000) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：foo-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|50010|foo-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:50000) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：50000) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50000)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50000) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50000||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：example.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートで管理しています (例：inbound|50000||) エンドポイントを選びます。Envoyは、エンドポイントをローカルホストや宛先ポートで管理しています (例：127.0.0.6:50000) 。  ローカルホストにHTTPSリクエストを送信します。結果的に、宛先マイクロサービスにHTTPSリクエストが届きます。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineering▶︎ istio-proxyコンテナのプロキシ先のIPアドレスについてistio-proxyコンテナは、ローカルホストを127.0.0.6とし、HTTPSリクエストをマイクロサービスに送信します。これは、127.0.0.1を指定してしまうと、istio-proxyコンテナからマイクロサービスへの通信がiptables上でループしてしまうためです。istio-proxyコンテナからマイクロサービスへの通信では、正しくはiptables上でISTIO_OUTPUTからPOSTROUTINGに通信を渡します。一方で、もしローカルホストが127.0.0.1であると、ISTIO_OUTPUTからISTIO_IN_REDIRECTに通信を渡すことになり、istio-proxyコンテナに再びリダイレクトしてしまいます。hatappi1225さんの解説が鬼わかりやすかったです🙏🙏🙏画像引用元：mercari engineeringInbound Forwarding - Google ドキュメントiptables から理解する Istio 1.10 から変更された Inbound Forwarding | メルカリエンジニアリングマイクロサービス間のHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：50010) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50010) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50010) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：50010) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：foo-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|50010|foo-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:50010) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：50010) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50010)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50010) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50010||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：example.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートで管理しています (例：inbound|50010||) エンドポイントを選びます。Envoyは、エンドポイントをローカルホストや宛先ポートで管理しています (例：127.0.0.6:50010) 。  ローカルホストにHTTPSリクエストを送信します。結果的に、宛先マイクロサービスにHTTPSリクエストが届きます。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：443) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_443) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：443) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：443) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：istio-egressgateway-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターをIstio EgressGateway 宛先ポートやホストで管理しています (例：outbound|443|istio-egressgateway-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Istio EgressGateway Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:443) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、Istio EgressGateway PodにL7ロードバランシングします。宛先Pod (Istio EgressGateway Pod) 側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：443) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_443)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：443) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50010||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：external.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|443|external.com) 。エンドポイントを選びます。Envoyは、エンドポイントをエントリ済システムのIPアドレスや宛先ポートで管理しています (例：:50010) 。エントリ済システムのIPアドレスは、開発者が設定する必要はなく、EnvoyがDNSから動的に取得します。  エントリ済システムにHTTPSリクエストを送信します。Using Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio07. おわりにIstioサイドカーメッシュがEnvoyのHTTPSリクエストの処理をどのように抽象化するのか、またEnvoyがどのようにHTTPSリクエストを処理するのかを解説しました。次々とサービスメッシュツールが登場したとしても、それがEnvoyを使用したサービスメッシュである限り、最終的にはEnvoyの設定値に行き着きます。そのため、抽象化されたEnvoyがどのように通信を扱うのかを一度でも理解すれば、様々なサービスメッシュツールで知識を流用できると思います。Istioはもちろん、他のEnvoyによるサービスメッシュツール (Consul、Cilium、など) を使っている方の参考にもなれば幸いです👍🏻謝辞今回、Kubernetesのネットワークを調査するにあたり、以下の方に知見をご教授いただきました。@ken5owata さんこの場で感謝申し上げます🙇🏻‍記事関連のおすすめ書籍Istio in Action (English Edition)作者:Posta, Christian E.,Maloku, RinorManningAmazonIstio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe作者:Calcote, Lee,Butcher, ZackO'Reilly MediaAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[スリーシェイク、「AWS アドバンストティアサービスパートナー」に昇格]]></title>
            <link>https://sreake.com/blog/aws-advancedtier/</link>
            <guid>https://sreake.com/blog/aws-advancedtier/</guid>
            <pubDate>Fri, 12 Jan 2024 00:50:00 GMT</pubDate>
            <content:encoded><![CDATA[株式会社スリーシェイクは、アマゾン ウェブ サービス（以下、AWS）のAWSパートナーネットワーク（APN）において「AWS アドバンストティアサービスパートナー」に認定されたことをお知らせいたします。The post スリーシェイク、「AWS アドバンストティアサービスパートナー」に昇格 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[達人と呼ばれる技術力を持ったソフトウェアエンジニアになりたくて]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/01/10/132326</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/01/10/132326</guid>
            <pubDate>Wed, 10 Jan 2024 04:23:26 GMT</pubDate>
            <content:encoded><![CDATA[zyさんの「技術力が高い」という幻覚を読んでの感想。sizu.me技術力とは私の経験でも、技術力は単なる専門知識や技術の習得を超えた、もっと包括的で深い概念です。実用日本語表現辞典において「技術力」とは、「手段や手法を用いて物事を成し遂げる能力」と定義されています。この定義は、技術を単に適用することではなく、それを利用して広範囲な目標を達成する、特に問題解決能力を含む幅広い能力を示唆しています。言い換えれば、技術力は具体的な技術的スキルだけではなく、それらを応用し、実際の課題に対して実効性のある解決策を生み出す能力を意味します。これには、新しい技術を学び、既存の技術を創造的に応用することも含まれます。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazon技術力への信頼とその重要性私が考えるに、高い技術力は複雑な技術の使用能力だけに限らず、他者からの信頼と依頼を獲得する能力にも反映されます。人々が「あの人に任せたい」「あの人に相談したい」と感じる時、それは技術的な能力だけではなく、人間性やコミュニケーション能力が高いことを示します。こうした能力は、チームワークやプロジェクトの成功に不可欠です。技術力を持つ人は、単にタスクをこなすだけでなく、チームメンバーと効果的に協力し、プロジェクトの目標に貢献します。「技術力が高い」「技術力が低い」という一般的な評価はしばしば狭い視野に基づいています。真の技術力の評価は、直接的な事業貢献やコミュニケーション能力を超えた、より幅広い能力に基づくべきです。これは、個々のポジションや視点によって異なり、難解なコードを書く能力だけでなく、既存のコードを効率的に活用し、迅速に顧客に価値を提供できる能力も含まれます。技術力は、新しい課題に対応する柔軟性と、既存のソリューションを改善する創造性を併せ持つことです。このように、高い技術力は、技術的なスキルに加えて、人間性、信頼性、問題解決能力、コミュニケーション能力などの多面的な資質が組み合わさったものです。これにより、他者から頼りにされ、尊敬されるエンジニアとなることが可能です。心理的安全性のつくりかた　「心理的柔軟性」が困難を乗り越えるチームに変える作者:石井遼介日本能率協会マネジメントセンターAmazonソフトウェアエンジニアの役割ソフトウェアエンジニアは、一人で全てを行う天才ハッカーや家で働きもせずに自称ソフトウェアエンジニアを名乗っている場合を除いて、技術を活用して問題を解決し、新しい価値を創造する重要な役割を担っています。彼らは技術的な問題に直面するだけでなく、企業の一員としてのプロフェッショナルな責任も持ちます。これには、会社の目標達成、業務の効率化、社内外の関係構築など、より広範な責務が含まれます。社内で問題が発生した際、彼らは解決策の策定と推進における主導的な役割を果たし、問題点の指摘にとどまらず、修正パッチの提供など具体的かつ建設的な貢献を行います。この積極的で前向きなアプローチは、問題の根本的な解決につながり、ソフトウェアの品質向上や優れたソリューションの提供に貢献します。総じて、ソフトウェアエンジニアとしての技術力と、企業の一員としてのプロフェッショナルな貢献は、それぞれが重要な役割を果たします。彼らは技術的なスキルと共に、組織内での協力と責任感を兼ね備えていることが求められます。サラリーマン金太郎 第1巻作者:本宮 ひろ志サード・ラインAmazonまとめ私が若い頃、私は寡黙で不器用だが技術に深く向き合う、達人と呼ばれるソフトウェアエンジニアの存在を信じていました。しかし、時が経つにつれて、「技術力が高い」という言葉が単に技術的なスキルの高さだけを意味するのではないことを理解しました。この言葉には、問題解決能力、信頼性、コミュニケーション能力など、多面的な資質が含まれています。たとえそれが誤解や幻想に基づいていたとしても、持ちうる技術力を活かして他者を支援し、互いにリスペクトを持つことが、ソフトウェアエンジニアとしての成長への道であると気づきました。このような姿勢が、周囲から達人として認識されるための重要な要素となっています。イシューからはじめよ――知的生産の「シンプルな本質」作者:安宅和人英治出版Amazonあとは、「技術力が高い」という幻覚を読んでたらそーだいさんのソフトウェアエンジニアと技術力を思い出した。読み返してもとても良かったので合わせて紹介しておきたいです。 speakerdeck.com私、もしくは私たちが技術力と呼んでいるナニカについての話でした。追記したこと後日このブログを読み返してみると、僕にとって、あの日ハッカーに憧れた自分が、「ハッカーの呪縛」から解き放たれるまで 的な意味合いもあったのだと思います。ただの思いつきかもしれませんが、技術分野でトップに立てなかった自分に対する、ある種の弁解や自己正当化の意味もあるのかもしれません。もしかしたらこれから私が歩む道によっては別の意味を持ってくるのか？ハッカーと画家 コンピュータ時代の創造者たち作者:ポール グレアムオーム社Amazonハッカーになろう (How To Become A Hacker）ハッカーと画家 ---Hackers and Painters---]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[k8sgpt-operator 開発メモ (ARM Mac 向け)]]></title>
            <link>https://zenn.dev/tozastation/articles/711f2bff2cc656</link>
            <guid>https://zenn.dev/tozastation/articles/711f2bff2cc656</guid>
            <pubDate>Wed, 10 Jan 2024 00:17:57 GMT</pubDate>
            <content:encoded><![CDATA[Kubernetes クラスタ構築 AMD64 コンテナ環境セットアップ ~ Lima VM ~https://github.com/lima-vm/limaGetting Started については README.md 参照Limaでは、事前に定義した内容でVMを作ることができますDocker 環境を構築する場合のサンプルも公開されていますhttps://github.com/lima-vm/lima/blob/master/examples/docker.yaml今回は、amd64 の VM を作成したいため、docker.yaml に以下の行を追記...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PipeCDのインストールとカスタマイズ]]></title>
            <link>https://sreake.com/blog/pipecd-installation/</link>
            <guid>https://sreake.com/blog/pipecd-installation/</guid>
            <pubDate>Tue, 09 Jan 2024 04:09:23 GMT</pubDate>
            <content:encoded><![CDATA[はじめに はじめまして。Sreake事業部インターン生の荒木です。2023年10月から長期インターン生としてKubernetes関連技術の習得とSRE技術の調査・検証を行っています。 前回の記事では、Kubernetes […]The post PipeCDのインストールとカスタマイズ first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
    </channel>
</rss>