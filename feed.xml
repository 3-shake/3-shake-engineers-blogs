<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sun, 30 Nov 2025 11:40:48 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[【小ネタ】pytest-bddを使ってみた]]></title>
            <link>https://dev.mix64.com/2025/11/30/pytest-bdd/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/11/30/pytest-bdd/</guid>
            <pubDate>Sun, 30 Nov 2025 11:15:35 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回は pytest-bdd ついて紹介します。名前の通り BDD（Behavior Driven Development, テス...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私が持っている書籍一覧を公開します！！]]></title>
            <link>https://zenn.dev/akasan/articles/my_book_lists</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/my_book_lists</guid>
            <pubDate>Sun, 30 Nov 2025 05:18:11 GMT</pubDate>
            <content:encoded><![CDATA[今回は、私が持っている書籍をブクログに登録して公開しました！!直近かなりKindle上で読んでいるのですがまだインポートできておらず、以下で共有しているのは紙媒体の書籍に限りますKindle上の書籍についても近々アップロードしようと思います ブクログとは？ブクログとは本の感想や評価をチェックしたり、webやアプリで本棚を作成して感想やレビューを書いたりすることができるサービスになります。私自身自分が持っている書籍を電子データとして管理したいと思った時に見つけたサービスですが、他の方と共有できる機能は便利だと思って使いはじめました。https://booklog.jp/...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【解説】コード生成の最適化によるLinuxコンテキストスイッチの改善パッチ]]></title>
            <link>https://dev.mix64.com/2025/11/29/optimize-code-generation-during-context-switching/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/11/29/optimize-code-generation-during-context-switching/</guid>
            <pubDate>Sat, 29 Nov 2025 14:45:43 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回は2025年11月にXie Yuanbin氏によって提案された一連のパッチシリーズ「Optimize code generat...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Professional Accelerated Data Science編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_ncp_ads</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_ncp_ads</guid>
            <pubDate>Sat, 29 Nov 2025 09:22:18 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるProfessional Accelerated Data Scienceを取得したので、その内容を共有しようと思います。 Professional Accelerated Data Scienceとは？NCP Accelerated Data Science（以下、NCP-ADS）は、データサイエンスワークフローにおけるGPUアクセラレーションツールとライブラリの活用能力を認定する中級レベルの資格です。現時点では本試験の入門レベルであるAssociateレベルは提供されておらず、Professionalのみとなっています。https://www...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Professional Generative AI LLMs編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_ncp_genl</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_ncp_genl</guid>
            <pubDate>Fri, 28 Nov 2025 13:09:29 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるProfessional Generative AI LLMsを取得したので、その内容を共有しようと思います。 Professional Generative AI LLMsとは？Professional Generative AI LLMs（以下、NCP-GENL）は、NVIDIAが提供している認定資格の一つであり、AIドリブンなアプリケーションを開発したり運用するための中間レベルの資格となっています。主にLLMについて取り扱われる部分が多いですが、従来の機械学習に関する知識についても問われるようになっています。先日受験したAssociate G...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[名前が似てる LookerとLooker Studioの違いとは?]]></title>
            <link>https://sreake.com/blog/differences-between-looker-and-looker-studio/</link>
            <guid isPermaLink="false">https://sreake.com/blog/differences-between-looker-and-looker-studio/</guid>
            <pubDate>Fri, 28 Nov 2025 02:32:06 GMT</pubDate>
            <content:encoded><![CDATA[はじめに こんにちは。 以前、Looker、LookMLについての記事を投稿してからしばらく経ちました。 今は生成AIの登場により、Lookerも様々な機能追加や活用の場が増えてきています。 それと同時に、このような言葉 […]The post 名前が似てる LookerとLooker Studioの違いとは? first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：「AI」という言葉の認識とは？]]></title>
            <link>https://zenn.dev/akasan/articles/poem_what_is_ai</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/poem_what_is_ai</guid>
            <pubDate>Thu, 27 Nov 2025 14:21:10 GMT</pubDate>
            <content:encoded><![CDATA[短編集、今回は昨今AIという言葉のニュアンスが変わってきているように感じており、ちょっと書いてみました。※ 中身が合ってないような文章です。出張帰りのわずかな時間での記載になりますのでご容赦ください そもそもAIとは？AI、つまり人工知能の定義をまず確認します。まず確認します。NEC様のページに乗っている画像を引用させていただくと、人工知能は「学習」「認識・理解」「予測・推論」「計画・最適化」など、人間の知的活動をコンピュータによって実現するもの、と定義されています。のように定義されるとのことで、人が考えるロジックをプログラム的に構築する分野と言えるでしょう。この定義から...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MCP・A2A概要 〜Google Cloudで構築するなら〜]]></title>
            <link>https://speakerdeck.com/shukob/mcpa2agai-yao-google-clouddegou-zhu-surunara</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/mcpa2agai-yao-google-clouddegou-zhu-surunara</guid>
            <pubDate>Thu, 27 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[「Jagu'e'r 月末 Tech Lunch Online#7 - Google Cloud を語る！-」にて、AIエージェントのMCPとA2Aの概要と、それらをGoogle Cloudで構築する上でのTipsを紹介させていただきました。https://jaguer-tech-lunch.connpass.com/event/362363/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[n8nでブログネタを考えるためのアシスタントを作ってみた]]></title>
            <link>https://zenn.dev/akasan/articles/blog_helper_n8n</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/blog_helper_n8n</guid>
            <pubDate>Wed, 26 Nov 2025 02:27:25 GMT</pubDate>
            <content:encoded><![CDATA[今回は、n8nで私のブログネタを過去のものから発展させる形で提案させるワークフローを作ってみました。 要件定義今回は以下の機能を実現するエージェントを作りました。ユーザから受け取ったブログに関する相談を行うツールとして、私のテックブログのRSS Feedを読み取る機能を与えるAIモデルはgpt-4.1-miniを利用ブログに関する相談以外はリジェクトさせる!OpenAIのモデルを使うにあたりAPIキーを取得してください n8nでの環境構築今回はローカル環境でdockerを利用して環境構築しました。詳しくは公式GitHubを参考に起動してください。https...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：技術資料を翻訳せずに英語のまま読んでいて思ったこと]]></title>
            <link>https://zenn.dev/akasan/articles/read_in_english</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/read_in_english</guid>
            <pubDate>Tue, 25 Nov 2025 11:40:55 GMT</pubDate>
            <content:encoded><![CDATA[最近技術資料を読むときに、英語の文章の場合基本的に翻訳せずに読むようにしています。あっという間に翻訳した文章を生成できる今だからこそあえて原文を読んでいてい思ったことをちょっとまとめてみました。 大前提：英語を流暢に扱いたい私自身、英語を流暢に話せるようになりたいと思っています。そんな中で日本語に翻訳ばっかりしていては英語能力は伸びません。もちろん知らない単語だったりは調べますが、可能な限り原文を読むことを大事にしています。また、翻訳させることによって原文と微妙にニュアンスが変わる場合もあるため、できるだけ原文で読むようにしています。なお、英語で読むようにしているのは今に始まった...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIエージェントの自律性と協調性を解放する Google CloudによるMCP・A2A実装のエンタープライズ戦略]]></title>
            <link>https://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2</guid>
            <pubDate>Tue, 25 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年11月25日の日本生成AIユーザ会「#19 MCP・A2A概要 〜Google Cloudで構築するなら〜」にて発表に使用した資料2部あるうちの後半部分です。https://genai-users.connpass.com/event/376260/↓前半部分はこちらですhttps://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIエージェント入門 自律性の基礎からオープンプロトコルMCP・A2Aによる連携まで]]></title>
            <link>https://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/aiezientoru-men-zi-lu-xing-noji-chu-karaopunpurotokorumcpa2aniyorulian-xi-made</guid>
            <pubDate>Tue, 25 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年11月25日の日本生成AIユーザ会「#19 MCP・A2A概要 〜Google Cloudで構築するなら〜」にて発表に使用した資料2部あるうちの前半部分です。https://genai-users.connpass.com/event/376260/↓後半部分はこちらですhttps://speakerdeck.com/shukob/20251125-ri-ben-sheng-cheng-aiyusahui-xiao-yuan-part2]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[「Postgres で試した？」と聞き返せるようになるまでもしくはなぜ私は雰囲気で技術を語るのか？ — Just use Postgres 読書感想文]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220</guid>
            <pubDate>Tue, 25 Nov 2025 04:52:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「Just use Postgres」という言葉を初めて聞いたのは、いつだったか覚えていません。Twitter か Hacker News か、あるいは社内の Slack か。どこで聞いたにせよ、私の反応は決まっていました。「また極端なことを言う人がいる」と。「それ、〇〇でもできますよ」——この手のフレーズはもう100回は聞いてきました。そして大抵の場合、その〇〇は専用ツールに置き換えられていきます。技術が専門分化していくのは自然な流れです。全文検索なら Elasticsearch。時系列データなら InfluxDB。メッセージキューなら RabbitMQ。それぞれの分野に専門家がいて、専用のソリューションがあって、ベストプラクティスがあります。「とりあえず Postgres で」なんて、それは思考停止ではないか、と。でも、心のどこかで気になっていたんです。www.manning.comソフトウェアエンジニアとして 10 年近く働いてきて、システムが複雑化していく様子を何度も見てきました。「全文検索だから Elasticsearch」と導入したら、その運用は誰がやるのか。バックアップは？　モニタリングは？　バージョンアップは？　構成図に新しい箱が増えるたびに、誰かが深夜 3 時のアラート対応をする可能性が増えます。その「誰か」は、たいてい自分です。以前関わったプロジェクトでは、Postgres、Redis、Elasticsearch、RabbitMQ、InfluxDB が同居していました。それぞれに理由があって導入されたはずですが、3 年後には「なぜこれが必要だったのか」を説明できる人が誰もいなくなっていました。ドキュメントはあっても、判断の背景までは残っていません。結局、「触ると怖いから残しておこう」という判断になります。技術的負債の典型です。syu-m-5151.hatenablog.comこの本を手に取ったのは、そういう日常からの逃避だったのかもしれません。「Postgres だけで済むなら、楽になれる」そんな甘い期待を持って読み始めました。そして、最初の数ページで気づきました。この本が言っているのは、私が思っていたことと少し違います。「Postgres は万能だから全部 Postgres でやれ」ではありません。「既に Postgres を使っているなら、新しいデータベースを追加する前に、まず Postgres で試してみよう」ということです。その違いに気づいた瞬間、なんというか、肩の力が抜けました。これは、銀の弾丸を売りつける本ではなかったんです。私たちが日々向き合っている「技術選定」という名の意思決定に、1 つの視点を提供してくれる本でした。10 年近くこの仕事をしてきて、技術選定について 1 つ学んだことがあります。新しい機能や技術が出たとき、いきなり飛びつかない。どれだけ魅力的に見えても、まず「運用時にどうなるか」を考えます。誰がバックアップを取るのか。障害時に誰が対応するのか。3 年後にメンテナンスできる人がいるのか。流行りの技術を追いかけることと、本番環境で安定して動かすことは、別の話です。これは、Postgres の中でも同じです。pgvector や TimescaleDB のような比較的新しい拡張、あるいは Postgres 本体の新機能についても、本番投入前に運用面を検討する必要があります。「Postgres だから安心」ではなく、「その機能が十分に枯れているか」を見極める姿勢が大事です。かといって、新しいことを学ばないわけにもいきません。技術は進歩します。昨日のベストプラクティスが、明日には技術的負債になることもあります。結局のところ、謙虚に学び続けるしかありません。私が最近考えているのは、こういう基準です。替えの利く技術は、流行に従う。フロントエンドのフレームワークとか、CI/CD ツールとか。入れ替えやすいものは、その時点でのベストを選べばいい。替えの利きづらい基盤は、標準に従う。データベースとか、認証基盤とか。長く使うものは、実績のある標準的な選択をする。競争優位の核は、自ら設計する。ビジネスの差別化に直結する部分は、自分たちで考え抜いて設計する。Postgres は、競争優位の核になる場合もありますが、基本的には 2 番目の「替えの利きづらい基盤」であることが多いです。40 年以上の実績があり、コミュニティ主導で開発され、世界中で使われている標準的な選択肢。だからこそ、その可能性を正しく理解しておきたいと思いました。だから、読み進めることにしました。正直に言うと、全部を理解できたわけではありません。「FOR UPDATE SKIP LOCKED」の仕組みを完全に説明しろと言われたら、今でもちょっと怪しいです。でも、それでいいと思うことにしました。完璧に理解することが目的ではありません。「Postgres で試した？」その一言を、自信を持って言えるようになること。それが、この本を読む目的でした。なので、この読書感想文には私の手元で動かした実行結果と書籍の中身がごちゃ混ぜになっています。基本的に明記しているつもりですが抜けていたらごめんなさい。Just Use Postgres!: All the database you need (English Edition)作者:Magda, DenisManningAmazonこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。1. Meeting Postgres「Just use Postgres」の再解釈1.2 節の「Just use Postgres」の説明を読んで、自分の理解が間違っていたことに気づきました。私はこれまで、「Just use Postgres」を技術選定の初手として捉えていました。「新規プロジェクトならとりあえず Postgres 立てとけ」みたいな。でも、著者が書いているのは違います。Does this mean Postgres has become a Swiss Army knife and the only database every developer needs? Certainly not.著者は明確に否定しています。Postgres は万能ツールではない、と。じゃあ「Just use Postgres」は何を意味するのでしょうか。「既に Postgres を使っているチームが、新しいユースケース（地理空間、時系列、生成 AI など）が発生したとき、別のデータベースを追加する前に Postgres で解決できるか確認してみよう」これがこのモットーの正しい解釈だと著者は言います。インフラエンジニアとして 10 年近く運用してきた身としては、この視点の転換にハッとしました。「Elasticsearch で全文検索やりたい」と言われた時、私は内心「またか…（心の中で構成図に新しい箱を追加する手が震える）」と思っていました。でも、「Postgres で試した？」と聞き返すことはしませんでした。自分の仕事を増やしたくないという気持ちが先に立って(自分が起こされるのにね)。これ、逆だったんです。Postgres で解決できるなら、新しいデータベースを追加するより運用負荷は減ります。バックアップ戦略も、モニタリングも、アラートルールも、既存のまま使えます。運用対象が増えるたびに、前世で何をしたのかと深夜に考える機会も減ります。この本を読み終えて、「Postgres で試した？」と自信を持って聞き返せるようになったと思います。良いか悪いかは別として。なぜ Postgres が人気なのか1.1 節では、Postgres が人気な 3 つの理由が挙げられています。オープンソース・コミュニティ主導: 1994 年に MIT ライセンスでオープンソース化。単一ベンダーではなくコミュニティ主導で開発。エンタープライズ対応: 35 年の開発で培われた信頼性と堅牢性。年次メジャー バージョン リリース、段階的 改善 重視。拡張性: Michael Stonebraker が設計当初から拡張性を重視。JSON、時系列、全文検索、ベクトル類似検索など多様なユースケースに対応。この 3 つ目の「拡張性」が、「Just use Postgres」を可能にしている核心だと感じました。著者の言葉を借りれば、Postgres は「従来のトランザクショナルワークロードを超えた幅広い用途に対応できる」。だから、新しいユースケースが出てきても、まず Postgres で試す価値があります。運用の観点からも、この 3 つは重要です。オープンソースだから、ベンダーロックインのリスクがないエンタープライズ対応だから、夜中3時にPagerDutyが鳴って「どの DB だ...？」と確認する時間を省略できるインフラエンジニアとして信頼できる拡張性があるから、新しいデータベースを追加する代わりに既存の Postgres を活用できるDocker でサクッと起動1.3 節では、Docker での起動方法が紹介されています。docker run --name postgres \    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \    -p 5432:5432 \    -v postgres-volume:/var/lib/postgresql/data \    -d postgres:17.2「1 分以内にコンテナとして起動可能」と Summary に書いてありますが、本当にその通りです。この手軽さが、「Just use Postgres」の実践を支えています。新しいユースケースを試すために、まず手元で動かしてみる。それが 1 分でできます。ツールを開発していることがあるのですがこれはツールの普及にめちゃくちゃ大事です。開発環境だからシンプルな設定で OK ですが、本番では当然違います。ユーザー名は postgres 以外にする、パスワードは環境変数じゃなく secrets で管理する、など。でも、それはこの本の scope 外でしょう。PostgreSQL徹底入門 第4版 インストールから機能・仕組み、アプリ作り、管理・運用まで作者:近藤 雄太,正野 裕大,坂井 潔,鳥越 淳,笠原 辰仁翔泳社Amazonpsql と generate_series1.4 節では psql での接続方法、1.5 節では generate_series を使ったモックデータ生成が紹介されています。INSERT INTO trades (id, buyer_id, symbol, order_quantity, bid_price, order_time)SELECT    id,    random(1,10) as buyer_id,    (array['AAPL','F','DASH'])[random(1,3)] as symbol,    random(1,20) as order_quantity,    round(random(10.00,20.00), 2) as bid_price,    now() as order_timeFROM generate_series(1,1000) AS id;generate_series と random の組み合わせで、複雑なモックデータを SQL だけで生成できます。外部ツール不要。これも「Just use Postgres」の一例だと感じました。「テストデータ生成ツールが必要だ」と言い出す前に、Postgres の標準機能で解決できます。普段、テストデータ生成はアプリ側（Rust）でやることが多かったのですが、シンプルなケースなら generate_series で十分かもしれません。試しに手を動かしてみたら、いくつか発見がありました。まず、generate_series は日付生成にも使えます。generate_series('2025-01-01'::date, '2025-12-31', '1 day') でカレンダーテーブルを一発生成できます。これは便利。次に、random() は毎回異なる値を返すので、再現可能なテストには setseed() を事前に呼ぶ必要があります。これを知らずに「テスト結果が毎回違う！」と焦った経験があります。そして一番ハマったのは、配列のインデックスが 1 始まりだということ。(array['AAPL','F','DASH'])[random(1,3)] のように 1 から始めないと想定外の結果になります。Rust や Python に慣れていると、0 から始めたくなるんですよね。私の直感を裏切るポイントでした。ちょっと昔だとこちらの資料とかはめちゃくちゃ良いのでオススメです。 speakerdeck.com speakerdeck.com基本クエリ1.6 節では、基本的な SQL クエリが紹介されています。SELECT symbol, count(*) AS total_volumeFROM tradesGROUP BY symbolORDER BY total_volume DESC;著者は count(*) が「Postgres で特別に最適化されている」と書いています。この本を通して、Postgres の内部動作についての理解が深まりました。DBといえばそーだいさんの資料を読み漁ってほしいです。 speakerdeck.com2. Standard RDBMS capabilitiesデータベースの三層構造を理解していなかったこの章で再認識したのは、Database → Schema → Table という三層構造の実践的な使い方です。10 年近く Postgres を運用してきた中で、Schema は使ってきました。ただ、この章で説明されているような「マイクロサービスのモジュールごとにスキーマを分ける」という設計パターンは、改めて整理されると納得感があります。この章で説明されている eコマースプラットフォームの設計が、その例です。coffee_chain (database)├── products (schema)│   ├── catalog (table)│   └── reviews (table)├── customers (schema)│   └── accounts (table)└── sales (schema)    ├── orders (table)    └── order_items (table)マイクロサービスのモジュールごとにスキーマを分ける。この設計パターン自体は知っていましたが、この本の整理の仕方は参考になります。著者は明確に書いています。Each application module or microservice has its own schema containing all the related data.これなら、アプリケーション層のアーキテクチャとデータベース層の構造が一致します。名前の衝突も避けられます。でも、同じデータベース内だから、JOIN で複数スキーマのテーブルをまたいでクエリできます。マルチテナント構成において Database レベルで分離していることも納得がいきました。テナントごとにデータベースを分け、リソースを共有しながら完全に隔離します。スケールアウト時には特定のテナントだけ別サーバーへ移動可能です。この設計思想、次のプロジェクトで導入を検討しようと思います。制約はデータベースでやるべきか2.3 節のデータ整合性の話で、著者のスタンスが面白かったです。著者のチームは、最初はアプリ層ですべてを検証する想定でした。でも、実際にプロダクトを構築していく中で、アプリ層のチェックが破られてデータ整合性の問題が発生しました。その経験から、著者はこう述べています。we decide to add additional constraints at the database level.私の経験でも、制約をデータベースに入れるべきか、アプリ層でやるべきかという論争が何度もありました。著者は両方を推奨しています。アプリ層でチェックしつつ、データベース層にも防御線を張ります。この章では、バグでアプリ層のチェックが破られたとき、外部キー制約がデータの不整合を防いだ例が出てきます。ERROR: insert or update on table "reviews" violatesforeign key constraint "products_review_product_id_fk"DETAIL: Key (product_id)=(1004) is not present in table "catalog".アプリのバグで product_id が 4 じゃなくて 1004 になっていました。でも、外部キー制約があったから、データベースにゴミが入らずに済みました。多層防御。これがデータ整合性の正しいアプローチだと感じました。アプリ層だけに頼ると、コードが変わったときに破綻します。データベース層だけに頼ると、エラーハンドリングが遅れて UX が悪化します。両方でやるべきです。トランザクション分離レベルの実践理解2.4 節のトランザクションで、MVCC と read committed 分離レベルの説明が具体的で良かったです。理論は知っていました。でも、この章の Table 2.1 の 2 つの psql セッションを並行実行する例を見て、実際の動きがイメージできるようになりました。2 つのトランザクションが同じ商品 (id=1) の在庫数を同時に減らそうとします。トランザクション 1 が UPDATE を実行（まだコミットしてない）トランザクション 2 が SELECT を実行 → まだ 199 が見える（dirty read を防いでいる）トランザクション 2 が UPDATE を実行 → ブロックされるトランザクション 1 が COMMIT → トランザクション 2 がアンブロックされるトランザクション 2 の UPDATE が最新の値（198）を読み直して実行される最後のポイントが重要でした。ブロックが解除された後、トランザクション 2 は再度値を読み直します。だから、結果は 197 になります（199 → 198 → 197）。もしこれがなかったら、トランザクション 2 は古い値（199）から 1 を引いて 198 にしてしまい、トランザクション 1 の更新が消えます（lost update）。Postgres の read committed は dirty write も防ぎます。だから、本番環境でデフォルトの分離レベルとして十分に使えます。もちろん、phantom read や non-repeatable read を防ぎたいケースもあります。そのときは repeatable read や serializable を使います。でも、大半のユースケースでは read committed で問題ありません。この理解、実際に手を動かさないと身につきませんでした。データベース関数で何をやるべきか2.6 節の関数とトリガーは、この章で一番刺激的でした。著者は order_add_item と order_checkout という 2 つの PL/pgSQL 関数を実装しています。ショッピングカートの管理ロジックをデータベース関数として実装した例です。最初は「これ、アプリ層でやればいいんじゃない？」と思いました。モダンなマイクロサービスアーキテクチャを信奉する我々にとって、データベース関数は「おじいちゃんの時代の遺物」みたいなイメージがありました。でも、著者の説明を読んで納得しました。書籍には「At least two scenarios come to mind」として 2 つのシナリオが紹介されています。複雑なビジネスロジックがデータと密結合している場合 → すべてのクライアントアプリやマイクロサービスで同じロジックを実装するより、データベース関数 1 つで済む複数ステップの処理でアプリとデータベース間の往復が必要な場合 → 大量のデータ転送が必要なとき、データベース内で完結させたほうが効率的order_add_item 関数の実装を見ると、この 2 つの利点がよくわかります。CREATE OR REPLACE FUNCTION sales.order_add_item(customer_id_param INT,  product_id_param INT, quantity_param INT)RETURNS TABLE (...) AS $$DECLARE    pending_order_id UUID;BEGIN    -- 1. 既存の pending order を探す    SELECT id INTO pending_order_id FROM sales.orders    WHERE customer_id = customer_id_param AND status = 'pending';    -- 2. なければ作る    IF pending_order_id IS NULL THEN        INSERT INTO sales.orders (customer_id, status)        VALUES (customer_id_param, 'pending')        RETURNING id INTO pending_order_id;    END IF;    -- 3. 商品を追加または更新（MERGE 文）    MERGE INTO sales.order_items AS oi ...    -- 4. 結果を返す    RETURN QUERY SELECT ...;END;$$ LANGUAGE plpgsql;これをアプリ層でやろうとすると、複数のクエリを順次実行する必要があります。SELECT で pending order があるか確認なければ INSERT で作成SELECT で商品の価格を取得INSERT or UPDATE で order_items に追加SELECT で最終的なカート内容を取得アプリとデータベース間で何度もデータをやり取りする必要があり、ネットワークレイテンシの影響を受けます。データベース関数なら 1 回の呼び出しで完結します。しかも、トランザクショナルに実行されます。途中でエラーが起きたら全部ロールバックされます。でも、すべてをデータベース関数でやるべきではありません。著者も「少なくとも 2 つのシナリオ」と言っています。つまり、適切なユースケースを見極めることが重要です。私の基準はこうです。やるべき: データと密結合した複雑なロジック、複数ラウンドトリップが必要なケースやらないべき: ビジネスロジックの大半、頻繁に変更されるロジック、外部 API との連携この判断基準、次のプロジェクトで使いたいです。ちなみに、PL/pgSQL を書いていて何度かハマったポイントがあります。SELECT ... INTO で結果が 0 行の場合、変数は NULL になります。エラーにはなりません。これを知らずに「なぜ NULL が入る？」と 30 分悩んだことがあります。あと、ON CONFLICT DO UPDATE で新しい値を参照するには EXCLUDED を使います。EXCLUDED.quantity のように書きます。最初は「新しい値をどう参照するんだ？」と混乱しました。一番タチが悪いのは、変数名とカラム名の衝突です。SELECT * FROM orders WHERE order_id = order_id と書くと、両方が変数として解釈されて全行が返ってきます。デバッグが本当に難しい。だから p_customer_id や v_order_id のようにプレフィックスを付けるのがベストプラクティスです。トリガーは「見えない魔法」になりやすい2.6.2 節のトリガーの例も実践的でした。order_items テーブルに変更があったら自動的に orders.total_amount を更新します。CREATE TRIGGER trigger_update_order_totalAFTER INSERT OR UPDATE OR DELETE ON sales.order_itemsFOR EACH ROWEXECUTE FUNCTION sales.update_order_total();これはシンプルで便利ですが、トリガーは「見えない魔法」になりやすいと感じました。アプリ層のエンジニアが INSERT INTO sales.order_items を実行したとき、裏で sales.orders が更新されていることに気づかないかもしれません。トリガーが増えると、データベースのパフォーマンス問題の原因を追うのが難しくなります。「なぜこの INSERT が遅い？」と思ったら、実は裏で 3 つのトリガーが動いていた、みたいな。著者はトリガーの適切なユースケースを挙げています。Triggers are particularly useful in audit scenarios, where you need to track who made changes in the database, or in event-driven architectures.監査ログ（誰がいつ変更したか）イベント駆動アーキテクチャ（変更を他のシステムに通知）トリガーを書いていて一度ハマったのは、NEW と OLD の使い分けです。NEW は INSERT/UPDATE で使用可能で、OLD は UPDATE/DELETE で使用可能。DELETE トリガーで NEW.order_id にアクセスしようとしてエラーになりました。COALESCE(NEW.order_id, OLD.order_id) のような対応が必要だと、その時初めて知りました。あと、大量の行を更新する場合、行ごとにトリガーが発火してパフォーマンスが低下します。FOR EACH ROW のトリガーは便利ですが、一括更新のパフォーマンスには注意が必要です。これ以外のケースでは、慎重に検討すべきだと思います。View は「名前付きクエリ」2.7 節の View の説明はシンプルで明快でした。A view is essentially a named query that returns data in a tabular format.View = 名前付きクエリ。この理解が正しいです。複雑な JOIN と集計を含むクエリを、アプリ層の複数箇所で使い回すより、View として定義してしまいます。CREATE VIEW sales.product_sales_summary ASSELECT    c.name AS product_name,    c.category,    SUM(oi.quantity) AS total_quantity_sold,    SUM(oi.quantity * oi.price) AS total_revenueFROM products.catalog cLEFT JOIN sales.order_items oi ON c.id = oi.product_idGROUP BY c.idORDER BY total_quantity_sold DESC, total_revenue DESC;アプリ層からはこうです。SELECT * FROM sales.product_sales_summary WHERE category='coffee';これだけで済みます。Materialized View も便利ですが、リフレッシュのタイミングが悩ましいです。手動リフレッシュ：ユーザーが「更新」ボタンを押したとき定期リフレッシュ：pg_cron で 1 時間ごとイベント駆動：トリガーで特定のテーブルが更新されたとき著者は 3 つのアプローチを提案していますが、ユースケースによって使い分けるべきです。3. Modern SQLSQL-92 の呪縛から解き放たれるこの章を読んで改めて認識したのは、自分がまだ SQL-92 の世界に閉じこもっていたという事実です。pgsql-jp.github.io著者の Markus Winand の言葉を引用します。Since 1999, SQL is not limited to the relational model anymore. Back then, ISO/IEC 9075 (the "SQL standard") added arrays, objects, and recursive queries. In the meantime, the SQL standard has grown five times bigger than SQL-92. In other words: relational SQL is only about 20% of modern SQL.SQL-92 は全体の 20% でしかありません。残りの 80% が Modern SQL です。でも、正直に言うと、私は長年その 20% の世界で生きていました。CTE は知っていたけど、「読みやすさのための構文糖衣」程度にしか思っていませんでした。Window Functions も「集計が少し楽になるやつ」くらいの認識。Recursive Queries に至っては、「使う機会がない」と決めつけていました。この章を読み終えて、自分がどれだけ Postgres の可能性を狭めていたかを再認識しました。なぜ Modern SQL を使わないのか著者は、Modern SQL が普及しない理由を 2 つ挙げています。理由1: 獲得した知識の粘着性（Stickiness of gained knowledge）Some developers learned SQL many years ago and mastered the SQL-92 version of the language for various data processing tasks. Even if their SQL queries are verbose or less efficient, the tasks are still solvable. As a result, many people continue doing things the way they originally learned.痛いほど身に覚えがあります。私が SQL を覚えたのは 15 年以上前です。当時の教科書は SQL-92 ベースで、GROUP BY、JOIN、Subquery があれば何でも解決できました。その成功体験が、今も私の手を縛っています。まるで、「ガラケーで十分じゃん」と言い張っていた 2010 年の自分を見ているようです。「CTE を使えば読みやすくなる」と頭ではわかっていても、「でも、Subquery でも書けるしな」と思ってしまいます。結果、冗長で読みにくいクエリを量産します。後輩に「このネストの深さ、どこまで行くんですか…？」と言われたことは秘密です。理由2: ORM フレームワークSome developers fully rely on ORM frameworks as a layer between their application and the database. They trust the ORM framework to generate SQL queries, believing it knows the best way to query or manipulate data.これも痛い指摘です。やめてくれおれにきく。ORM は確かに便利です。でも、ORM が生成するクエリは「汎用的なワークロード」を想定しています。Window Functions を使えば 1 回のクエリで済むケースでも、ORM は複雑な Self-Join を生成するかもしれません。第 1 章で学んだ「Just use Postgres」の思想は、ORM へ任せる前に、Postgres で何ができるかを知ることにも通じます。CTE（Common Table Expressions）は単なる Subquery の糖衣構文ではないCTE は「読みやすい Subquery」という側面で使うことが多かったです。でも、この章を読んで、それ以上の価値があることを再確認しました。Listing 3.3 の例では、2 つの CTE を使って「3 人以上のユーザーが聴いて、半分以下の時間しか再生しなかった曲」をランキングしています。WITH plays_cte AS (    SELECT s.title, s.duration, p.play_duration, p.user_id    FROM streaming.plays p    JOIN streaming.songs s ON p.song_id = s.id    WHERE p.play_start_time::DATE BETWEEN '2024-09-15' AND '2024-09-16'      AND p.play_duration < (s.duration / 2)),user_play_counts AS (    SELECT title, duration, COUNT(DISTINCT user_id) AS user_count,      MIN(play_duration) AS min_play_duration,      COUNT(*) AS total_play_count    FROM plays_cte    GROUP BY title, duration)SELECT title, duration, min_play_duration, total_play_countFROM user_play_countsWHERE user_count >= 3ORDER BY min_play_duration ASCLIMIT 3;このクエリを Subquery で書いたら、どうなるでしょうか。ネストが深くなって、読みにくくなります。メンテナンスもしにくくなります。でも、著者が強調しているのは「読みやすさ」だけではありません。If we want to understand how a query is actually executed by Postgres, we can look at the query execution plan using the EXPLAIN statement.EXPLAIN の結果を見ると、Postgres は plays_cte を user_play_counts に fold しています。つまり、CTE を使っても、実行計画は効率的なままです。これは重要なポイントです。以前のバージョンでは CTE が「最適化の壁」になることがありましたが、現在は改善されています。実際に EXPLAIN ANALYZE で確認してみました。Postgres は CTE をインライン展開して最適化しています。以前は CTE が「最適化の壁」と呼ばれていましたが、現在のバージョンでは改善されています。CTE が展開されて効率的なプランになっていることが確認できました。Postgres は賢いです。Data-modifying CTE という選択肢Listing 3.4 で紹介されている Data-modifying CTE は、私にとって完全に新しい概念でした。WITH updated_play AS (    UPDATE streaming.plays    SET play_duration = 200    WHERE id = 30    RETURNING song_id, play_duration)SELECT s.title, s.duration,       CASE           WHEN up.play_duration = s.duration THEN 'Moved Up the Rank'           ELSE 'Rank Not Changed'       END AS rank_change_statusFROM updated_play upJOIN streaming.songs s ON s.id = up.song_id;UPDATE の結果を RETURNING で受け取り、その結果を使って SELECT を実行します。これが 1 つのトランザクション内で完結します。これまで、「UPDATE してから SELECT」という処理は、2 つのクエリを順番に実行していました。でも、Data-modifying CTE を使えば、1 つのクエリで完結します。アトミック性も保証されます。なぜこの機能を今まで積極的に使ってこなかったのでしょうか。使う場面を意識していなかったというのが正直なところです。Recursive Queries は「特殊なケースでしか使わない」という誤解Recursive Queries は「組織の階層構造を扱う時に使う機能」という認識でした。実際、それ以外の場面で使う機会は多くありませんでした。でも、この章を読んで、活用範囲が広いことを再確認しました。著者が例として挙げているのは、音楽ストリーミングサービスの「連続再生」のトラッキングです。plays テーブルには played_after というカラムがあり、「この曲の前に再生された曲の ID」を保持しています。つまり、連続再生はリンクリストの構造を持っています。Listing 3.8 の Recursive Query は、この連続再生のシーケンスを取得します。WITH RECURSIVE play_sequence AS (    SELECT id, user_id, song_id,      play_start_time, play_duration, played_after    FROM streaming.plays    WHERE id = 5    UNION ALL    SELECT p.id, p.user_id, p.song_id, p.play_start_time,       p.play_duration, p.played_after    FROM streaming.plays p    JOIN play_sequence ps ON p.played_after = ps.id)SELECT user_id, song_id, play_start_time,  play_duration as duration, played_afterFROM play_sequenceORDER BY play_start_time;これを読んで、以前アプリ側で何度もループしてクエリを投げていた処理を思い出しました。以前のプロジェクトで、SNS のスレッド返信を表示する機能を実装した時、「親コメント ID」を辿って、アプリ側で再帰的にクエリを投げていました。その結果、N+1 問題が発生して、パフォーマンスが悪化しました。当時のアプリログを見返すと、同じユーザーの操作で DB への接続数が 47 回。まるでチャットボットが会話のキャッチボールをしているかのようでした。もちろん、レスポンスタイムは 3 秒超え。あの時、Recursive Query を知っていたら、1 回のクエリで全ての返信を取得できました。Recursive Query の実行フローListing 3.7 の擬似コードは、Recursive Query の実行フローを明確に説明しています。# Step 1: 非再帰項を実行（初期データ）non_recursive_result = execute(non_recursive_term);# Step 2: 重複削除（UNION の場合）if (using UNION)    non_recursive_result = remove_duplicates(non_recursive_result);# Step 3: 最終結果に追加final_result.add(non_recursive_result);# Step 4: ワーキングテーブルを初期化working_table = non_recursive_result;# Step 5: 再帰項を実行（ワーキングテーブルが空になるまで）while (working_table is not empty) {    intermediate_table = execute(recursive_term, using=working_table);    if (using UNION)        intermediate_table =            remove_duplicates(intermediate_table, excluding=final_result);    final_result.add(intermediate_table);    working_table = intermediate_table;}このフローを読んで、「Recursive Query は魔法じゃなくて、ちゃんとした仕組みがある」と納得できました。特に重要なのは、UNION と UNION ALL の違いです。UNION は重複削除するので、無限ループを防げます。UNION ALL は重複を許すので、パフォーマンスは良いですが、無限ループのリスクがあります。ところで、Recursive CTE を書いていて気になったのは終了条件です。調べてみると、終了条件は「新しい行が生成されなくなるまで」で、明示的に書く必要はありません。循環検出には配列で訪問済みノードを追跡する方法が有効です。ARRAY[id] AS path で初期化して、ps.path || p.id で追加していく。NOT p.id = ANY(ps.path) で循環を検出できます。このパターンは覚えておくと便利です。ただし、深い階層（数千レベル）ではパフォーマンスが低下します。グラフ DB ほど柔軟なグラフ探索はできません。SNS の友達の友達を無限に辿るような処理には向いていないです。この違いを理解していないと、本番環境で無限ループが発生します。怖いです。データベース監視の Slack チャンネルが「CPU 使用率 100%」「接続数の上限到達」で埋め尽くされる光景は、二度と見たくありません。Window Functions は Self-Join の代替ではないWindow Functions は「Self-Join の代わりに使える構文」という認識で使ってきました。でも、この章を読んで、パフォーマンス面での違いを改めて確認しました。Listing 3.11 の Self-Join と Listing 3.12 の Window Function を比較すると、違いが明確です。Self-Join 版:SELECT DISTINCT p.song_id,       p.user_id,       t.total_durationFROM streaming.plays pJOIN (    SELECT song_id,           SUM(play_duration) AS total_duration    FROM streaming.plays    GROUP BY song_id) t ON p.song_id = t.song_idORDER BY p.song_id;Window Function 版:WITH plays_with_total AS (  SELECT    song_id, user_id, SUM(play_duration)    OVER (PARTITION BY song_id) AS total_duration  FROM streaming.plays)SELECT DISTINCT song_id, user_id, total_durationFROM plays_with_totalORDER BY song_id, user_id;Self-Join 版は、テーブルを 2 回走査しています。Window Function 版は、1 回の走査で済みます。著者の言葉を借りれば、次のようになります。Although the self-join approach works as expected, it's not the most efficient, because every row of the table is accessed twice. Additionally, it's not the easiest to follow when trying to understand the query logic.これを読んで、「Window Functions は単なる糖衣構文じゃなくて、パフォーマンス最適化の手段だった」と気づきました。Running Total と Window FrameListing 3.13 の Running Total の計算は、Window Functions の本質を理解する上で重要でした。SELECT song_id, user_id, play_duration, SUM(play_duration)OVER (PARTITION BY song_id ORDER BY user_id) AS total_play_durationFROM streaming.playsWHERE song_id = 2;結果は次のようになります。 song_id | user_id | play_duration | total_play_duration---------+---------+---------------+---------------------       2 |       1 |           144 |                 144       2 |       2 |           206 |                 350       2 |       3 |           186 |                 654       2 |       3 |           118 |                 654PARTITION BY song_id で Window を作り、ORDER BY user_id で Window を Frame に分割します。各 Frame は、現在の行 + それ以前の行を含みます。この仕組みを理解すると、「累積和」「移動平均」「ランキング」といった処理が、すべて Window Functions で解決できることがわかります。以前のプロジェクトで、時系列データの累積和を計算する時、アプリ側でループを回していました。あれも、Window Functions を使えば 1 回のクエリで済みました。RANK() と ROW_NUMBER() の違いListing 3.14 の RANK() は、同じ値に同じランクを付けます。SELECT song_id, SUM(play_duration) AS total_play_duration,RANK() OVER (ORDER BY SUM(play_duration) DESC) AS song_rankFROM streaming.playsGROUP BY song_idORDER BY song_rank;もし ROW_NUMBER() を使っていたら、同じ値でも異なる番号が振られます。この違いを理解していないと、ランキング機能で不具合が発生します。実際に試してみると、3 つの関数の違いがはっきりします。ROW_NUMBER(): 同じ値でも異なる番号（1→2→3→4→5）RANK(): 同じ値は同じ番号で次は飛ぶ（1→2→2→4→5）DENSE_RANK(): 同じ値は同じ番号で次は飛ばない（1→2→2→3→4）以前、ランキング機能で ROW_NUMBER() を使って、同点の処理がおかしくなったことがあります。「なぜ同じスコアなのに順位が違うの？」というバグ報告を受けて、RANK() に変更しました。この違いは一度経験すると忘れません。4. Indexesインデックスの「当たり前」を疑う第 4 章「Indexes」の冒頭の一文が、自分の習慣を言い当てていました。Indexes are often the first optimization technique that comes to mind when dealing with a long-running query or a slow database operation.そうなんです。遅いクエリがあったら、とりあえずインデックス張る。それが 10 年間の私のパターンでした。まるで風邪を引いたら「とりあえずビタミン C」みたいな、根拠のない安心感でした。でも、著者は続けます。They've proven so effective in many scenarios that we sometimes overlook other optimization methods, turning to indexes right away.インデックスに頼りすぎて、他の最適化手法を見落としている。この指摘は痛かったです。実際、過去のプロジェクトで「遅いクエリ問題」が発生した時、私はいつもまずインデックスを疑っていました。でも、本当は EXPLAIN で実行計画を見て、ボトルネックを特定してから判断すべきでした。この章では、インデックスの「なぜ」と「いつ」を徹底的に掘り下げています。単なるインデックス作成のチュートリアルじゃありません。インデックス戦略の哲学です。なぜインデックスがこんなに人気なのか4.1 節「Why are indexes so popular?」では、O(N)と O(log_b N)の違いが説明されています。100 件のテーブルで ID=5 を探す場合。インデックスなし：最大 100 回のルックアップ（O(N)）B-tree インデックスあり：最大 4 回のルックアップ（O(log_b N)、b=3 の場合）これが 100 万件に増えても、インデックスがあれば 6 回のルックアップで済みます（b=10 の場合）。正直、この計算量の違いは知っていました。でも、著者が示した表を見て改めて驚きました。 テーブルサイズ  インデックスルックアップ回数  100件          2回                         1,000件        3回                         1,000,000件    6回                         10,000,000件   7回                         1,000,000,000件  9回                      10億件のテーブルでも9回のルックアップ。これがインデックスの威力です。深夜の障害対応で「インデックス張れば解決するっしょ」と言い続けてきた自分が、ようやく理論武装できた瞬間でした。そして、著者の言葉が刺さります。As a result, it's no surprise that indexes are such a popular optimization technique.インデックスが人気な理由は、この圧倒的な効率性にあります。でも、だからこそ安易に使いすぎるリスクもあります。EXPLAIN — まず実行計画を見ろ4.4 節で EXPLAIN が詳しく説明されています。私はこれまで、EXPLAIN ANALYZE しか使っていませんでした。でも、この章を読んで EXPLAIN (analyze, costs off) や EXPLAIN (analyze, buffers on) といった他のオプションを知りました。特に印象的だったのは、buffers オプションです。Buffers: shared hit=3これは「3 ページをメモリから読んだ（ディスクアクセスなし）」という意味です。もし read=4 があれば、「4 ページをディスクから読んだ」ということになります。遅いクエリの原因はインデックスの欠如じゃなく、メモリ不足かもしれません。この視点は新鮮でした。私は「遅い = インデックスがない」と決めつけていました。でも、buffers を見れば、ディスク I/O が原因なのか実行計画が原因なのか区別できます。著者は次のように書いています。This information is crucial because a query might run slowly not due to a suboptimal execution plan or missing index but because memory has become a limited resource.インデックスは万能じゃありません。頭ではわかっていても、実務では軽視しがちでした。単一カラムインデックス — B-tree vs Hash4.5 節では、単一カラムインデックスが 2 種類紹介されています。B-tree：範囲検索（>, <, BETWEEN）に対応Hash：等価検索（=, IN）のみ私は今まで、Hash インデックスを積極的に選択してきませんでした。「B-tree がデフォルトだから」という理由で、あえて変える必要性を感じていなかったためです。でも、この章を読んで考えが変わりました。例えば、ゲーム内のチャンピオンタイトル（5 種類のみ）を検索する場合。範囲検索は不要で、等価検索だけで十分です。この場合、Hash インデックスが最適です。CREATE INDEX idx_champion_titleON game.player_statsUSING hash(champion_title);実行計画を見ると、Hash インデックスを使った場合の実行時間は 0.073 ms。フルテーブルスキャンの 1.463 ms と比べて 20 倍速いです。ユースケースに合わせてインデックスタイプを選ぶ。これが正しいアプローチです。複合インデックス — 順番が命4.6 節「Composite indexes」は、この章で最も重要なセクションだと思います。複合インデックスの順番は、クエリのパフォーマンスに直結します。例えば、(region, score DESC, win_count DESC) というインデックスを作った場合。CREATE INDEX idx_region_score_win_countON game.player_stats (region, score DESC, win_count DESC);このインデックスは、次のクエリで使われます。-- ✅ 使われるWHERE region = 'NA' and score > 5000 and win_count > 10-- ✅ 使われるWHERE region = 'EMEA' and score > 1000-- ✅ 使われる（先頭カラムがあるから）WHERE region = 'EMEA'-- ❌ 使われない（先頭カラムがない）WHERE score > 1000 and win_count > 30先頭カラム（leading column）が必須。これがないと、複合インデックスは使われません。この章を読みながら実際に手を動かしてみました。EXPLAIN ANALYZE の出力で Index Scan と Index Only Scan の違いを確認することが重要です。Index Only Scan はテーブルにアクセスしないので高速。Covering Index の威力を実感しました。Partial Index については、WHERE 句が完全に一致する場合のみ使用されるという点に注意が必要です。WHERE play_time <= '50 hours' で作ったインデックスは、WHERE play_time <= '50 hours 1 second' では使われません。1 秒違うだけで使われない。厳密すぎる気もしますが、そういう仕様です。Hash インデックスは範囲検索（<, >, BETWEEN）には使えません。等価検索専用です。これを知らずに「なぜインデックスが使われないんだ？」と悩んだことがあります。インデックスサイズを比較した結果も興味深かったです。idx_champion_hash   - 696 kBidx_covering        - 416 kBidx_region_score    - 248 kBidx_perf_margin     - 120 kBidx_casual_players  - 48 kB   -- Partial Index は最小Partial Index のサイズの小ささは印象的でした。必要な部分だけをインデックス化するという発想、もっと早く知りたかったです。ただし、著者は注記しています。However, starting with Postgres 18, the database introduced support for skip scan lookups on composite B-tree indexes, allowing us to skip leading columns and still use the index in more scenarios.Postgres 18 以降では、skip scan が導入されるらしいです。これは大きな改善です。でも、現時点（Postgres 17 以前）では、複合インデックスの順番を慎重に設計する必要があります。Covering Index — テーブルアクセスをゼロに4.7 節「Covering indexes」は、インデックス最適化の最終形態だと感じました。通常、インデックスは「どの行を読むか」を決めるだけで、実際のデータ（username など）はテーブルから取得します。でも、Covering Index を使えば、インデックスだけで全てのデータを取得できます。CREATE INDEX idx_composite_covering_indexON game.player_stats (region, score DESC, win_count DESC)INCLUDE (username);INCLUDE 句で username をインデックスに含めることで、テーブルアクセスが不要になります。実行計画を見ると。Index Only Scan using idx_composite_covering_index on player_statsHeap Fetches: 0Execution Time: 0.602 msHeap Fetches: 0 — テーブルに一切アクセスしていません。実行時間は 0.602 ms。以前の 1.856 ms（Bitmap Index Scan）から 3 倍速くなりました。ただし、トレードオフがあります。username を更新するたびに、インデックスも更新する必要があります。However, as a tradeoff, all included columns must remain consistent with the table data.更新頻度が低いカラムなら Covering Index は有効。逆に、頻繁に更新されるカラムには向きません。Partial Index — 必要な部分だけインデックス化4.8 節「Partial indexes」では、インデックスのサイズを減らす手法が紹介されています。例えば、10,000 人のプレイヤーのうち、74 人（0.74%）だけが「occasional players（プレイ時間 50 時間以下）」だとします。この 74 人だけを頻繁に検索するなら、全体にインデックスを張る必要はありません。CREATE INDEX idx_occasional_playersON game.player_stats (play_time)WHERE play_time <= '50 hours';この Partial Index により。インデックスサイズが大幅に削減される更新時のインデックスメンテナンスコストが減る検索速度は 2 ms から 0.168 ms に改善（20 倍速）ただし、条件が少しでも違うとインデックスが使われません。-- ✅ 使われるWHERE play_time <= '50 hours'-- ❌ 使われない（1秒超過）WHERE play_time <= '50 hours 1 second'Partial Index は条件が厳密。これを理解して使う必要があります。Expression Index — 計算結果にインデックス4.9 節「Functional and expression indexes」は、私にとって全く新しい概念でした。例えば、「勝数 - 負数」というパフォーマンスマージンで検索したい場合。WHERE (win_count - loss_count) BETWEEN 300 and 450通常、この式はクエリ実行時に毎回計算されます。でも、Expression Index を使えば、計算結果をインデックス化できます。CREATE INDEX idx_perf_marginON game.player_stats ((win_count - loss_count));実行時間は 2.524 ms から 1.200 ms に改善（2 倍速）。ただし、式が複雑になると、インデックスのメンテナンスコストが増えます。win_count または loss_count が更新されるたびに、インデックスも更新されます。頻繁に検索される式にのみ使うのが正しい戦略です。Over-Indexing という警告この章の最後に、著者は重要な警告を発しています。Throughout this chapter, we've explored and added various indexes to the game.player_stats table, bringing the total number of indexes for the table to seven.7 つのインデックス。これは典型的な Over-Indexing だと著者は指摘します。Although this is acceptable for learning purposes, in practice, it represents a classic case of over-indexing.インデックスは無料じゃありません。作成時にディスク容量を消費する更新時にメンテナンスコストがかかる計画時（Planning Time）にオプション評価のコストがかかる私は過去、インデックスを「作りすぎる」傾向がありました。「とりあえずこのカラムにもインデックス張っとくか」という感じで。まるで保険に入りまくる不安な中年のように、あらゆるカラムに「念のため」インデックスを追加していました。そして毎回、INSERT が遅くなってから後悔する、という黄金パターンです。でも、この章を読んで、インデックスは慎重に設計すべきだと改めて理解しました。著者は Appendix A で Over-Indexing と Under-Indexing について詳しく説明しています。実際に読んでみて、自分の過去の設計を振り返る良い機会になりました。この辺の基礎がちゃんとできているか定期的に確認することは大切にしていますが、この Appendix がめちゃくちゃ面白いのでおすすめです。mickindex.sakura.ne.jp5. Postgres and JSONJSON 機能を使うべき場所と使わない場所5.3 節の「JSON in Postgres: Striking the balance」が、この章の核心です。著者が書いているのは、「JSON をすべてのデータに使うな」という明確な警告です。Even though Postgres provides full-fledged support for JSON, you should avoid storing all application data in JSON-specific data types as you would in a pure document database.これが「Just use Postgres」の真髄だと感じました。MongoDB を追加する代わりに Postgres の JSON 機能を使う。でも、すべてのデータを JSON で保存する MongoDB みたいな使い方はするな。ハイブリッドアプローチを取れ、と。pizzeria.order_items テーブルの構造が、この考え方を体現しています。CREATE TABLE pizzeria.order_items (    order_id INT NOT NULL,    order_item_id INT NOT NULL,    pizza JSONB NOT NULL,  -- ここは JSON    price NUMERIC(5,2) NOT NULL,  -- ここは通常の型    PRIMARY KEY (order_id, order_item_id));order_id と price は通常の型で、検索とデータ整合性を重視。pizza の詳細（トッピング、クラスト、ソースなど）は JSONB で、柔軟性を重視。この設計、10 年前のプロジェクトで欲しかったです。JSON を使うべき場面著者が挙げている 3 つの基準が具体的でわかりやすいです。データが静的または更新頻度が低い（設定、メタデータ、顧客プリファレンス）データが疎（スパース）（多くの null や 0、feature flags など）スキーマの柔軟性が必要（外部 API のレスポンス、テレメトリイベント）ピザ注文の詳細は「静的」に該当します。注文確定後はほぼ変更されません。もし従来の正規化モデルで実装すると、5 つのテーブル（pizzas、order_items、pizza_cheeses、pizza_veggies、pizza_meats）が必要になります。著者が示した例を見て、「ああ、これは辛い」と思いました。Write overhead: 1 つのピザ注文のために複数テーブルへの INSERT が必要。7 つのトッピングなら 7 レコード。Read overhead: ピザのレシピを再構築するために複数テーブルの JOIN が必要。Transformation overhead: フロントエンドが JSON で受け取るのに、わざわざ正規化モデルへ分解し、また JSON に戻す。この 3 つの overhead、すべて経験があります。特に Transformation overhead が一番つらいです。API レスポンスを JSON で返すためだけに、複雑な JOIN と整形ロジックを書く。「JSON から分解して正規化して、また JOIN して JSON に戻す」という、まるで水を凍らせてから溶かすような無駄な作業。当時の自分に「Postgres の JSONB を使えばいいぞ」と教えてあげたいです。著者が「hybrid approach」を推奨する理由がよくわかりました。json vs jsonb5.1 節で json と jsonb の違いが説明されています。 型  保存形式  Write 性能  Read 性能  インデックス  推奨度  json  テキスト  速い  遅い（毎回パース）  限定的  ❌  jsonb  バイナリ  遅い（パースあり）  速い  GIN など充実  ✅ 著者の推奨は明確です：jsonb をデフォルトで使え。Overall, the jsonb data type is the recommended default for storing and processing JSON data in Postgres, unless you have a specific use case that requires preserving the order of keys in the original JSON objects.「キーの順序を保持する必要がある」という特殊なケースでない限り、jsonb 一択です。私が過去に扱ったプロジェクトでは、なんとなく json を選んでいたことがありました。「書き込みが速いから」という理由で。でも、検索の度にパースが走るコストを考えていませんでした。典型的な「入口だけ見て出口を見ない」パターン。インフラエンジニアあるあるです。著者が書いているように、jsonb は write 時に変換コストがありますが、検索性能は圧倒的に速いです。そして GIN インデックスとの組み合わせでさらに速くなります。JSON のクエリ：-> と ->>5.4 節の JSON クエリ構文は充実しています。機能自体は知っていましたが、改めて整理すると活用の幅が広がります。基本：-> と ->>SELECT    order_id,    pizza->'size' as pizza_size,     -- JSON 形式で返す    pizza->>'crust' as pizza_crust  -- テキスト形式で返すFROM pizzeria.order_itemsWHERE order_id = 100;出力の違い。pizza_size   | pizza_crust-------------|-------------"small"      | thin-> は JSON 形式なのでダブルクォート付き。->> はテキスト型なのでダブルクォートなし。最初は「なぜ 2 つの演算子が必要なのか？」と疑問でしたが、5.4.1 節を読んで納得しました。WHERE 句での比較。-- JSON 形式で比較（ダブルクォート必要）WHERE pizza->'size' = '"small"'-- テキスト形式で比較（ダブルクォート不要）WHERE pizza->>'crust' = 'gluten_free'-> でダブルクォートを忘れると、こんなエラーが出ます。DETAIL: Token "small" is invalid.CONTEXT: JSON data, line 1: smallこの仕様、最初はわかりにくいですが、JSON の仕様に忠実だと理解すれば納得できます。Rust から Postgres に接続して JSON を扱う時、この -> と ->> の違いでハマりました。-> は JSON 型を返すので、そのまま文字列としてデシリアライズしようとするとエラーになります。->> を使うか、適切な型変換が必要です。特に pg_typeof() で型を確認しようとした時、::TEXT でキャストしないと Rust 側でエラーになりました。ネストした JSON へのアクセスSELECT    order_id,    pizza->'toppings'->'veggies' as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力。veggies_toppings-----------------------[{"tomato": "light"}]配列の特定要素にアクセスするには、インデックス（0 始まり）を指定。SELECT    order_id,    pizza->'toppings'->'veggies'->0 as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力（[] が消える）。veggies_toppings---------------------{"tomato": "light"}さらに、配列内のオブジェクトのフィールドにアクセスします。SELECT    order_id,    pizza->'toppings'->'veggies'->0->>'onion' as onions_amountFROM pizzeria.order_itemsWHERE order_id = 100;出力。onions_amount---------------lightこの連鎖、最初は読みづらいと思いましたが、慣れると直感的です。? 演算子と @> 演算子? 演算子：キーの存在確認SELECT    order_id,    pizza->'toppings'->'meats' as meatsFROM pizzeria.order_itemsWHERE pizza->'toppings' ? 'meats'ORDER BY order_id LIMIT 5;「meats キーが存在する注文だけを取得」という意味です。配列内のオブジェクトのキー存在確認は少し複雑になります。SELECT    order_id,    pizza->'toppings'->'meats' AS meatsFROM pizzeria.order_itemsWHERE EXISTS (    SELECT 1    FROM jsonb_array_elements(pizza->'toppings'->'meats') AS meats    WHERE meats ? 'sausage')ORDER BY order_id LIMIT 5;この書き方、正直、冗長だと思いました。でも著者も同じ意見で、5.4.4 節で JSON path expression を使ってシンプルにしています。@> 演算子：包含関係の確認SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free"}';「crust フィールドが gluten_free の注文を数える」という意味です。複数条件。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom"}';ネストした構造も可能。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom",                 "toppings": {"veggies": [{"tomato": "extra"}]}}';この演算子、MongoDB の $elemMatch みたいな感じだと思いました。ちなみに、@> 演算子は配列にも使えます。tags @> '["hot", "milk"]' のように書けば、配列が特定の要素を含むかどうかを検索できます。JSON オブジェクトだけでなく、配列にも対応しているのは便利です。著者が「-> と @> を組み合わせるとより読みやすくなる」と書いています。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"crust": "gluten_free", "type": "custom"}' AND      pizza->'toppings'->'veggies' @> '[{"tomato": "extra"}]';こっちの方が確かに読みやすいです。JSON Path Expressions5.4.4 節で、SQL/JSON path language が登場します。先ほどの「sausage を含む注文を検索」のクエリが、path expression でこうなります。SELECT    order_id,    pizza->'toppings'->'meats' AS meatsFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza, '$.toppings.meats[*] ? (exists(@.sausage))')ORDER BY order_id LIMIT 5;サブクエリが不要になりました。構文の説明です。$: 評価対象の JSON オブジェクト（pizza カラム）.toppings.meats: フィールドへのアクセス[*]: 配列のすべての要素?: フィルタの開始@: 現在評価中のオブジェクトexists(@.sausage): sausage フィールドが存在するか最初は読みづらかったですが、いくつか例を見ていくうちに理解できました。配列のクエリSELECT    count(*) as total_cnt,    jsonb_object_keys(        jsonb_path_query(pizza, '$.toppings.cheese[*]')    ) as cheese_toppingFROM pizzeria.order_itemsGROUP BY cheese_topping ORDER BY total_cnt DESC;$.toppings.cheese[*] で cheese 配列のすべてのオブジェクトを取得。jsonb_object_keys で各オブジェクトのキー（チーズ名）を抽出。出力。total_cnt | cheese_topping----------|----------------     2575 | mozzarella      771 | cheddar      762 | parmesanフィルタ付き path expressionSELECT    count(*) AS total_cnt,    pizza->'type' as pizza_typeFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza,        '$.toppings.cheese[*] ? (exists(@.parmesan))')GROUP BY pizza_typeORDER BY total_cnt DESC;評価順序。$.toppings.cheese[*]: すべてのチーズオブジェクトを取得?: フィルタ開始exists(@.parmesan): 現在のオブジェクトに parmesan フィールドがあるか複数フィルタのチェーンSELECT count(*)FROM pizzeria.order_itemsWHERE jsonb_path_exists(    pizza,    '$ ? (@.type == "custom") .toppings.cheese[*].parmesan ? (@ == "extra")');評価順序（左から右）です。$: pizza オブジェクト? (@.type == "custom"): type が custom か確認.toppings.cheese[*].parmesan: parmesan オブジェクトを取得? (@ == "extra"): 量が extra か確認この書き方、最初は難解だと思いましたが、左から右に評価されると理解すれば読めます。JSON の更新：jsonb_set と #-5.5 節で JSON の更新方法が紹介されています。最も簡単な方法（非推奨）-- アプリ側で JSON 全体を取得SELECT pizza FROM pizzeria.order_items WHERE order_id = $1 and order_item_id = $2;-- アプリ側で JSON を修正-- DB に書き戻すUPDATE pizzeria.order_itemsSET pizza = new_pizza_order_jsonWHERE order_id = $1 and order_item_id = $2;著者が書いているように、簡単だけど効率的じゃないです。複雑な JSON オブジェクト全体を転送するのではなく、必要なフィールドだけを更新する方が良いです。jsonb_set 関数UPDATE pizzeria.order_itemsSET pizza = jsonb_set(pizza, '{crust}', '"regular"', false)WHERE order_id = 20 and order_item_id = 5;jsonb_set の引数です。元の JSON オブジェクト（pizza）更新対象のパス（{crust}）新しい値（"regular" — JSON 文字列なのでダブルクォート必要）フィールドが存在しない場合に追加するか（false）ネストした配列の更新。UPDATE pizzeria.order_itemsSET pizza = jsonb_set(    pizza,    '{toppings,veggies}',   '[{"tomato":"extra"}, {"spinach":"regular"}]',   false)WHERE order_id = 20 and order_item_id = 5;配列の特定要素を更新する場合、パスにインデックスを含められる。-- 例：{toppings, veggies, 0, tomato} で配列の最初の要素の tomato を更新1 つ注意点があります。jsonb_set のパスが存在しない場合、第 4 引数が true なら新しいキーが作成されます。これは便利な反面、タイプミスで意図しないキーが追加されるリスクもあります。#- 演算子：フィールドの削除UPDATE pizzeria.order_itemsSET pizza = pizza #- '{toppings,meats}'WHERE order_id = 20 AND order_item_id = 5;{toppings, meats} パスのフィールドを削除します。この演算子、シンプルで良いです。インデックス：B-tree と GIN5.6 節がこの章で一番技術的に深い部分でした。Expression Index with B-tree最初の試みです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza ->> 'type' = 'custom';実行計画。Seq Scan on order_items (actual time=0.034..1.062 rows=563 loops=1)  Filter: ((pizza ->> 'type'::text) = 'custom'::text)  Rows Removed by Filter: 2375Execution Time: 1.185 ms全件スキャンです。Expression Index を作成します。CREATE INDEX idx_pizza_typeON pizzeria.order_items ((pizza ->> 'type'));再度実行計画を確認。Bitmap Index Scan on idx_pizza_type (actual time=0.068..0.068 rows=563 loops=1)  Index Cond: ((pizza ->> 'type'::text) = 'custom'::text)Execution Time: 0.376 ms4 倍近く高速化（1.185 ms → 0.376 ms）しました。でも問題があります。このインデックスは pizza ->> 'type' というexact expression にしか効きません。-- これは idx_pizza_type を使わないSELECT count(*)FROM pizzeria.order_itemsWHERE pizza -> 'type' = '"custom"';実行計画：Seq Scan に戻ります。さらに、別のフィールド（size など）を検索する場合、また別の Expression Index が必要になります。著者が書いているように、スケールしません。フィールドごとにインデックスを作り続けると、気づいたら「インデックスのインデックス」が欲しくなる世界へようこそ。GIN Index（Default）GIN（Generalized Inverted Index）を作成します。CREATE INDEX idx_pizza_orders_ginON pizzeria.order_itemsUSING GIN(pizza);GIN の仕組み（5.6.2 節の Figure 5.1 参照）です。JSON オブジェクトからすべてのキーと値を抽出して、個別のインデックスエントリとして保存します。例です。{  "size": "large",  "type": "three cheese",  "crust": "thin",  "sauce": "marinara",  "toppings": {    "cheese": [      {"cheddar": "regular"},      {"mozzarella": "extra"},      {"parmesan": "light"}    ]  }}インデックスに保存されるエントリです。Keys:- size、type、crust、sauce、toppings、cheese、cheddar、mozzarella、parmesanValues:- large、three cheese、thin、marinara、regular、extra、lightこれらのエントリは辞書順に保存され、複数のインデックスページに分散されます。GIN を使ったクエリです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"type": "custom"}';実行計画。Bitmap Index Scan on idx_pizza_orders_gin (actual time=0.109..0.110 rows=563 loops=1)  Index Cond: (pizza @> '{"type": "custom"}'::jsonb)Execution Time: 0.830 ms複雑なネスト構造でも使えます。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @> '{"toppings":{"cheese":[{"cheddar":"regular"}]}}';Postgres はインデックスから toppings, cheese, cheddar, regular の 4 つのエントリを検索して、該当する行を絞り込みます。GIN のメリット：1 つのインデックスで JSON 全体を検索可能です。GIN Index with jsonb_path_opsさらに効率的な GIN インデックスです。CREATE INDEX idx_pizza_orders_paths_ops_ginON pizzeria.order_itemsUSING GIN (pizza jsonb_path_ops);違いは、パス全体をハッシュ化して保存することです。例です。size.largetype.three cheesecrust.thinsauce.marinaratoppings.cheese.cheddar.regulartoppings.cheese.mozzarella.extratoppings.cheese.parmesan.lightこれらのパスをハッシュ関数に通して、固定長の整数として保存します。メリットです。検索が速い：固定長整数の比較は可変長テキストより速いサイズが小さい：ハッシュコードはテキストより小さい実際のサイズ比較です。index_name                      | index_size--------------------------------|------------idx_pizza_orders_gin            | 112 kBidx_pizza_orders_paths_ops_gin  | 56 kB半分のサイズです。デメリットです。jsonb_path_ops は ? 演算子（キー存在確認）をサポートしません。なぜなら、インデックスにはパスのハッシュのみが保存されていて、キー単体は保存されていないからです。-- これは idx_pizza_orders_gin を使う（jsonb_path_ops は使えない）SELECT count(*)FROM pizzeria.order_itemsWHERE pizza ? 'special_instructions';使い分け インデックスタイプ  サイズ  検索速度  サポート演算子  推奨用途  Expression Index (B-tree)  小  特定 expression のみ速い  ->, ->>  特定フィールドの頻繁な検索  GIN (default)  大  速い  ?, @>, @?, @@  柔軟な検索、キー存在確認が必要  GIN (jsonb_path_ops)  中  最速  @>, @?, @@  包含検索のみ、サイズ重視 著者が書いているように、jsonb_path_ops が第一選択です。キー存在確認が必要なら default GIN を追加します。6. Postgres for full-text search「全文検索は難しい」という思い込みこの章を読み終えて思ったのは、「Postgres の全文検索は、思ったより実用的だ」ということです。私はこれまで、全文検索といえばElasticsearchだと思っていました。実際、過去のプロジェクトで「検索機能が必要です」と言われたら、反射的に「Elasticsearch を構築しますか？」と答えていました。まるで、パブロフの犬のように。「検索」という言葉を聞いただけで、脳内で Kibana のダッシュボードが立ち上がっていました。でも、第 1 章で学んだ「Just use Postgres」の真の意味を思い出します。「別のデータベースを追加する前に、まず Postgres で解決できるか確認してみよう」この章は、その実践編でした。Tokenization と Normalization の仕組み6.1 節では、Postgres が全文検索をどう実現しているかが説明されています。基本的な流れは 4 ステップです。Tokenization（トークン化）: 文書を単語やフレーズに分割Normalization（正規化）: トークンを lexeme（語彙素）に変換Storing and Indexing: lexeme を tsvector 型で保存し、インデックスを作成Searching: 保存した lexeme に対してクエリを実行著者が ts_debug 関数を使って、"5 explorers are traveling to a distant galaxy" という文がどう処理されるかを見せてくれます。SELECT token, description, lexemes, dictionaryFROM ts_debug('5 explorers are traveling to a distant galaxy');結果を見ると、"explorers" は "explor" に、"traveling" は "travel" に変換されています。ストップワード（"are", "to", "a"）は空の lexeme {} にマッピングされています。これがステミング（語幹抽出）です。試しに to_tsvector('english', 'running runs runner') を実行してみると、'run':1,2 'runner':3 と返ってきます。running と runs は run に統一されています。だから「running」で検索しても「runs」がヒットする。これは便利です。位置情報を保持しながらストップワードを削除するという設計が巧妙です。<-> (FOLLOWED BY) オペレータで距離を計算するために、ストップワードの位置も必要になるからです。Elasticsearch でも同じようなことをやっているはずですが、Postgres ではこれが標準機能だということに改めて気づかされました。複数言語への対応6.1.2 節では、Full-text search configuration が紹介されています。Postgres には英語だけでなく、アラビア語、ロシア語、日本語など、多数の言語用の predefined configuration が用意されています。SELECT token, description, lexemes, dictionaryFROM ts_debug('russian','5 исследователей путешествуют к далёкой галактике.');ロシア語の例を見ると、russian_stem 辞書が使われています。"исследователей" が "исследовател" に、"путешествуют" が "путешеств" に変換されています。これも Elasticsearch でやろうとすると、analyzer の設定が複雑になります。JSON の設定ファイルを書いて、tokenizer を選んで、filter を設定して、mapping を更新する作業が必要です。設定の沼にハマっていきます。Postgres ではデフォルトで対応しています。でも、ここで疑問が湧きました。日本語はどうなんだろう？この本では日本語の例は出てきません。調べてみると、日本語は形態素解析が必要で、Postgres の標準機能だけでは難しいようです。pg_bigm（2-gram ベース）や pgroonga（Groonga ベース）といった拡張機能が必要になります。「Postgres で試した？」と聞き返す前に、日本語対応が必要かどうかは確認が必要な部分だと思いました。英語圏のサービスなら問題ないですが、日本語がメインなら追加の検討が必要です。tsvector と generated column の活用6.2 節では、生成した lexeme をどう保存するかが説明されています。3 つの選択肢があります。On-the-fly 生成: クエリごとに to_tsvector を実行（非効率）Column に保存: tsvector 型のカラムを追加して保存（推奨）Index のみ: テーブルには保存せず、直接インデックス作成（ストレージ節約）著者は 2 番目の方法を推奨しています。ALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (  to_tsvector(    'english', coalesce(name, '') ||    ' ' ||    coalesce(description, ''))) STORED;GENERATED ALWAYS AS ... STORED という構文が便利です。これで、name や description が変更されると、lexemes も自動的に再生成されます。ただし、configuration は明示的に指定する必要があります（'english'）。これは、generated column の式が immutable でなければならないからです。この辺りの設計判断は、実際に運用してみないと分からない部分が多そうです。全文検索クエリの実行6.3 節では、実際のクエリの書き方が紹介されています。plainto_tsquery: シンプルなクエリSELECT id, nameFROM omdb.moviesWHERE lexemes @@ plainto_tsquery('a computer animated film');plainto_tsquery は、ユーザーが入力した自然な文章を tsquery 型に変換してくれます。ストップワード（"a"）を削除し、残りの単語を lexeme に変換して、& (AND) オペレータで結合します。結果：'comput' & 'anim' & 'film'この手軽さが良いです。Elasticsearch なら、query DSL を書く必要があります。plainto_tsquery と to_tsquery の違いを実際に確認してみました。plainto_tsquery('english', 'ghost in shell') は 'ghost' & 'shell' を返します。「in」はストップワードとして除去されています。一方、to_tsquery は構文を直接指定できるので、OR 検索や NOT 検索も可能です。to_tsquery: 高度なフィルタリングSELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('computer & animated      & (lion | clownfish | donkey)');to_tsquery を使えば、AND、OR、NOT、FOLLOWED BY などのオペレータを直接指定できます。SELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('lion & !''The Lion King''');NOT オペレータで特定のフレーズの除外も可能です。この柔軟性は、Elasticsearch と変わりません。むしろ、SQL の中で完結するので、アプリケーション側のコードがシンプルになります。ランキングと重み付け6.4 節では、検索結果のランキングが扱われています。ts_rank による関連度スコアSELECT id, name, vote_average,  ts_rank(lexemes, to_tsquery('ghosts')) AS search_rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('ghosts')ORDER BY search_rank DESC, vote_average DESC NULLS LAST LIMIT 10;ts_rank 関数は、lexeme の出現頻度と位置に基づいてスコアを計算します。でも、最初の実行例では、タイトルに "ghost" が含まれる映画と、説明文にだけ含まれる映画が同じように扱われていました。setweight による重み付けALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (    setweight(to_tsvector('english', coalesce(name, '')), 'A') ||    setweight(to_tsvector('english', coalesce(description, '')), 'B')) STORED;setweight 関数で、タイトル由来の lexeme に A ラベル、説明文由来の lexeme に B ラベルを付けます。重みは A > B > C > D の順で、デフォルトは D です。これで、ts_rank はタイトルに含まれる単語をより高くランク付けするようになります。   id   |         name          | vote_average | search_rank--------+-----------------------+--------------+-------------    251 | Ghost                 | 6.3333301544 |   0.6957388 210675 | A Most Annoying Ghost |              |   0.6957388   1548 | Ghost World           | 8.1428575516 |  0.66871977タイトルへ "ghost" が含まれる映画が上位へ来るようになりました。この重み付けのメカニズムは、Elasticsearch の boosting と同じ発想です。でも、Postgres では setweight 一発で実現できます。ハイライト表示6.5 節では、ts_headline 関数が紹介されています。SELECT id, name, description,    ts_headline(description, to_tsquery('pirates')) AS fragments,    ts_rank(lexemes, to_tsquery('pirates')) AS rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('pirates:B')ORDER BY rank DESC LIMIT 1;結果はこうなります。fragments   | <b>pirate</b> Captain Jack is in a battle with the ocean ➥  itself. Jack knows it won't be easyマッチした単語を <b> タグで囲んでくれます。さらに、オプションでカスタマイズも可能です。ts_headline(description, to_tsquery('pirates'),    'MaxFragments=3, MinWords=5, MaxWords=10,     FragmentDelimiter=<ft_end>') AS fragmentsただし、著者が警告している通り、XSS 攻撃のリスクがあります。HTML マークアップを含む文書を扱う場合は、サニタイズが必要です。この辺りは、Elasticsearch でも同じ問題があります。ハイライト機能は便利ですが、セキュリティには注意が必要です。インデックスの選択：GIN vs GiST6.6 節では、全文検索を高速化するためのインデックスが説明されています。GIN インデックスCREATE INDEX idx_movie_lexemes_ginON omdb.moviesUSING GIN (lexemes);GIN（Generalized Inverted Index）は、全文検索に最適化されたインデックスです。各 lexeme ごとにインデックスエントリを作成し、その lexeme を含むテーブル行への参照を保持します。実行計画を見ると、Seq Scan（15.328 ms）から Bitmap Index Scan（0.150 ms）に変わっています。100 倍以上の高速化です。ただし、GIN インデックスは positional information を保存しません。<-> (FOLLOWED BY) オペレータを使うクエリでは、テーブル行を再確認する必要があります。この制約を解決したい場合は、RUM インデックス（Postgres 拡張）を使うと良いようです。GiST インデックスCREATE INDEX idx_movie_lexemes_gistON omdb.moviesUSING GIST (lexemes);GiST（Generalized Search Tree）は、signature tree を構築します。各文書の signature（ビット列）を作成し、lexeme の signature を bitwise OR で結合します。実行時間は 0.395 ms で、GIN（0.150 ms）より遅いです。理由は、signature collision が発生するため、マッチした文書をテーブル行で再確認する必要があるからです。でも、GiST は インデックスサイズが小さく、更新が速いという特徴があります。使い分け著者の推奨はこうです。GIN: 検索速度が最重要で、インデックスメンテナンスコストを許容できる場合GiST: インデックスサイズや更新速度が重要な場合この辺りの判断は、データ量や更新頻度によって変わります。実際に両方試してみる価値があります。Postgres の限界を認識するもちろん、Postgres の全文検索にも限界はあります。日本語の形態素解析はサポートされていない（可能性が高い）大規模データ（数億レコード）では Elasticsearch の方が速い可能性がある分散検索や複雑な aggregation は Elasticsearch の方が得意でも、多くのケースでは Postgres で十分というのがこの章の主張です。7. Postgres extensions拡張性こそが「Just use Postgres」の核心第 7 章を読んで、ようやく腑に落ちました。「Just use Postgres」というモットーは、Postgres の拡張機能によって生まれました。この一文を読んだとき、第 1 章の理解が深まりました。In fact, the motto "Just use Postgres" emerged largely due to its rich ecosystem of extensions, which allow us to use the database well beyond the use cases covered in the earlier chapters of the book.第 1 章では「新しいユースケースが発生したとき、まず Postgres で解決できるか確認しよう」という意味だと学びました。でも、なぜ Postgres で解決できるのかという根拠は曖昧でした。答えは拡張機能でした。JSON、全文検索、時系列、地理空間、メッセージキュー、ベクトル検索——これら全て、Postgres の拡張機能が可能にしています。第 2 章から第 6 章までは、コア機能を使ったユースケースでした。でも、それは「氷山の一角」だったんです。本当の多様性は拡張機能にあります。Michael Stonebraker のビジョン7.1 節で、Postgres の拡張性が生まれた背景が語られています。Michael Stonebraker（チューリング賞受賞者）の言葉が印象的でした。1980 年代、多くの研究論文が同じことを言っていました：「リレーショナルデータベースは素晴らしいと言われているが、実際には特定のシナリオでまったく機能しない」そして、それぞれの論文が独自の解決策を提案していました。Stonebraker はこう考えました：それぞれの問題へ個別の解決策を追加するのではなく、RDBMS が特定のユースケースへ適応できるようにする、より良い方法があるはずだ。この哲学が、Postgres の設計思想の根幹になっています。拡張性が Postgres の強みであることは知っていました。ただ、この章を読んで、Postgres は最初から拡張性を前提に設計されているという設計思想を改めて確認できました。インフラエンジニアとして、この設計思想は深く刺さります。運用の現場では、予期しないユースケースが次々に現れます。そのたびに新しいデータベースを追加していたら、運用負荷は青天井です。気づけば Kubernetes クラスタの中に MongoDB、Redis、Elasticsearch、TimescaleDB、Neo4j が同居しています。「あれ、俺たちデータベース動物園を運用してたっけ？」と遠い目をする羽目になります。Postgres は、そういう現実を 40 年以上前から見据えていたんです。拡張性を支える 3 つの基盤7.2 節では、Postgres の拡張性を支える技術的な基盤が説明されています。カタログ駆動操作Postgres は、テーブル、カラム、データ型、関数などのメタデータをシステムカタログに保存しています。これは通常のテーブルと似た構造で、拡張機能はこのカタログを読み書きできます。これ、地味だけど重要だと思いました。システムカタログが「普通のテーブルのような構造」だから、拡張機能が新しいデータ型や関数を追加できます。もし、メタデータが隠蔽された独自フォーマットだったら、拡張機能の開発はもっと難しかったでしょう。データベースフックPostgres のコードベースには、拡張機能がカスタムロジックを注入できるフックポイントが定義されています。クエリ計画、実行、認証など、様々なイベントにフックできます。これ、Linux カーネルの LSM（Linux Security Modules）に似ていると思いました。カーネル本体を変更せずに、セキュリティポリシーを注入できる仕組みです。Postgres も同じ哲学です。コアエンジンを変更せずに、動作を拡張できます。動的ロード拡張機能のロジックは、SQL、PL/pgSQL、C、Rust など、様々な言語で書けます。SQL や PL/pgSQL で書かれた拡張機能は、データベースエンジンが直接解釈します。C や Rust で書かれた拡張機能は、共有ライブラリとして実行時に動的にロードされます。コアエンジンの再コンパイルが不要です。これが重要です。もし、拡張機能を追加するたびに Postgres 本体を再コンパイルしなければならないとしたら、運用はほぼ不可能でした。動的ロードのおかげで、拡張機能の追加・削除が柔軟にできます。pgcrypto を使ってみた感覚7.2.2 節では、pgcrypto 拡張機能を使ったユーザー認証の例が紹介されています。CREATE EXTENSION pgcrypto;INSERT INTO accounts (username, password_hash)VALUES ('ahamilton', crypt('SuperSecret123', gen_salt('bf')));gen_salt('bf') で Blowfish アルゴリズムを使ったソルトを生成し、crypt() で平文パスワードとソルトからハッシュを生成します。この例を読んで、「データベース内で暗号化を完結させる」という選択肢があることに気づきました。これまで、パスワードのハッシュ化はアプリケーション層でやるものだと思い込んでいました。でも、pgcrypto を使えば、データベース層でも実装できます。どちらが良いかはケースバイケースでしょう。でも、選択肢があることを知っておくのは重要です。認証のクエリも興味深いです。SELECT username FROM accountsWHERE username = 'ahamilton'AND password_hash = crypt('SuperSecret123', password_hash);crypt() 関数に、平文パスワードと保存済みのハッシュを渡します。関数がハッシュからソルトを抽出し、再計算して比較します。この設計、エレガントだと思いました。ソルトを別カラムに保存する必要がありません。ハッシュ自体にソルトが含まれています。ちなみに、bcrypt のコストパラメータ（gen_salt('bf', 8) の 8 の部分）は、8〜12 が推奨されています。数字が大きいほどハッシュ計算に時間がかかりますが、セキュリティは向上します。拡張機能の 5 つのカテゴリ7.3 節では、拡張機能を 5 つのカテゴリに分類しています。"Postgres beyond relational"Postgres を従来の RDBMS を超えた用途に拡張します。pgvector、pg_ai、pgvectorscale: ベクトルデータベース（生成 AI ワークロード）TimescaleDB: 時系列データベースPostGIS: 地理空間データベースpgmq: メッセージキューpg_duckdb: 高性能分析ワークロード（DuckDB の列指向エンジンを埋め込み）これらが「Just use Postgres」を可能にしている拡張機能です。「Elasticsearch で検索やりたい」「MongoDB で JSON 保存したい」「Redis でキューやりたい」というよくある要求があります。これらに対して、「まず Postgres で試した？」と聞き返せる根拠です。過去の自分に教えてあげたいです。技術選定会議で『最新トレンド』として提案された 3 つのデータベース、実は Postgres の拡張機能で済むやつだから、と。プログラミング言語と手続き型言語第 2 章で PL/pgSQL を学びましたが、それだけではありません。PLV8: JavaScriptPL/Java: JavaPL/Python: PythonPL/Rust: Rust自分の得意な言語で、データベース関数やプロシージャを書けます。特に PLV8 の説明が興味深いです。V8 JavaScript エンジンを Postgres に埋め込むだけでなく、PgCompute クライアントライブラリと組み合わせることで実現します。アプリケーションから SQL を介さずに JavaScript 関数を直接実行できます。これ、SQL とアプリケーションロジックの境界を曖昧にする、面白いアプローチだと思いました。コネクタと外部データラッパー外部のデータソースを、あたかも Postgres のテーブルであるかのようにクエリできます。file_fdw: ファイルシステムからデータを読むpostgres_fdw、mysql_fdw、oracle_fdw、sqlite_fdw: 他の SQL データベースに接続redis_fdw、parquet_s3_fdw、kafka_fdw: Redis、S3、Kafka などの非 SQL データソースに接続Postgres を統合データレイヤーとして使えます。これ、マイクロサービスアーキテクチャで複数のデータソースを扱う場合に便利そうです。各サービスが独自のデータベースを持っていても、Postgres を経由して統一的にクエリできます。でも、パフォーマンスはどうなんでしょう。ネットワーク越しにクエリを投げるわけですから、レイテンシーは増えるはずです。この辺りは実際に試してみないとわかりません。（試した結果「遅い！」ってなって、結局専用のデータ同期パイプラインを構築するところまでがテンプレ。）クエリとパフォーマンス最適化pg_stat_statements: SQL 文の実行統計を追跡auto_explain: 遅いクエリの実行計画を自動ログhypopg: 仮想インデックスのテストauto_explain は便利そうです。普段、遅いクエリを見つけたら、手動で EXPLAIN ANALYZE を実行しています。でも、auto_explain があれば、自動的にログに記録してくれます。hypopg も面白いです。実際にインデックスを作らずに、仮想的にテストできます。本番環境で「このインデックス、効果あるかな？」と試す前に、リスクなしで検証できます。ツールとユーティリティpg_cron: cron ベースのスケジューラーPostgreSQL Anonymizer: 個人情報の匿名化pgaudit: 監査ログpg_partman: パーティション管理の簡素化pg_cron があれば、データベース内で定期タスクを実行できます。外部の cron や Airflow を使わずに。「Just use Postgres」の精神に沿っています。Postgres 互換ソリューション7.4 節では、Postgres の拡張機能ではなく、Postgres のプロトコルやソースコードを活用した別のソリューションが紹介されています。拡張機能で解決できない問題のために、こういった選択肢があります。ゼロから構築されたソリューションGoogle SpannerCockroachDBPostgres のワイヤレベルプロトコル、DML/DDL 構文、一部の機能をサポートしています。でも、内部実装は完全に別物です。分散データベースとしての可用性とスケーラビリティを提供します。Postgres ソースコードをベースにしたソリューションNeon（サーバーレスデータベース）YugabyteDB（分散データベース）Postgres のソースコードを再利用しつつ、ストレージレイヤーを変更・拡張しています。Postgres のアプリケーションをそのまま実行できます。ライブラリ、ツール、フレームワークもそのまま使えます。この 2 つのアプローチの違いは興味深いです。ゼロから構築したソリューションは、自由度が高い反面、Postgres との互換性は限定的になります。Postgres ソースコードベースのソリューションは、互換性が高い反面、アーキテクチャの変更範囲は制約されます。どちらが良いかは、ユースケース次第です。でも、どちらも「Postgres のエコシステムを活用したい」という需要から生まれています。それだけ、Postgres が広く使われているということです。8. Postgres for generative AIPostgres が Vector Database になる瞬間第 8 章を読んで最初に感じたのは、「Just use Postgres」が生成 AI の時代でも貫かれているということでした。「RAG を実装するなら Pinecone か Weaviate を使おう」——これまでそう考えていました。でも、著者が示すのは違います。Postgres can serve as a powerful vector database for implementing RAG and other gen AI use cases.既に Postgres を使っているなら、まず Postgres で試してみよう。この章はその具体的な実装方法を示しています。pgvector という選択肢pgvector という拡張を有効化するだけで、Postgres が Vector Database になります。CREATE EXTENSION vector;たったこれだけ。新しいデータベースを立てる必要がありません。（「Vector Database 導入提案書」を 3 日かけて書いた過去の自分に教えてあげたい...）vector(1024) という型が使えるようになります。1024 次元のベクトル埋め込みを格納できます。映画の説明文を mxbai-embed-large モデルで変換した埋め込みを、そのまま Postgres のカラムに保存できます。CREATE TABLE omdb.movies (    id BIGINT PRIMARY KEY,    name TEXT NOT NULL,    description TEXT NOT NULL,    movie_embedding VECTOR(1024),    ...);この手軽さ。Docker で pgvector 入りの Postgres を起動するだけで試せます。docker run --name postgres-pgvector \    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \    -p 5432:5432 \    -d pgvector/pgvector:0.8.0-pg17「Vector Database を導入しましょう」という提案をする前に、「Postgres で試した？」と聞き返せるようになりました。Cosine Distance とベクトル類似検索埋め込みを保存するだけじゃありません。類似検索もできます。SELECT id, name, descriptionFROM omdb.moviesORDER BY movie_embedding <=> omdb.get_embedding('May the force be with you')LIMIT 3;<=> は Cosine Distance を計算する演算子です。pgvector が提供しています。この SQL を実行すると、「May the force be with you」というフレーズに最も関連する映画が返ってきます。当然、Star Wars の映画がトップに来ます。埋め込みモデルが学習した「意味の空間」の中で、近い映画を見つけてくれます。でも、最初は全件スキャンになります。4,000 件程度なら許容できますが、規模が大きくなったら？そこでインデックスが必要になります。IVFFlat と HNSW——2 つのインデックス戦略pgvector は 2 種類のインデックスをサポートしています。IVFFlat: クラスタリングベースの高速化CREATE INDEX movie_embeddings_ivfflat_idxON omdb.moviesUSING ivfflat (movie_embedding vector_cosine_ops)WITH (lists = 5);IVFFlat は埋め込みをクラスタ (リスト) に分割します。k-means でセントロイドを計算し、各埋め込みを最も近いセントロイドのリストに配置します。検索時は、クエリの埋め込みに最も近いセントロイドのリストだけをスキャンします。全件スキャンを避けられます。でも、これは近似検索 (ANN: Approximate Nearest Neighbor) です。真の最近傍が他のリストにいたら、見逃す可能性があります。Recall (再現率) が完璧じゃありません。ivfflat.probes パラメータで、スキャンするリスト数を増やせます。Recall は改善しますが、検索速度は落ちます。BEGIN;SET LOCAL ivfflat.probes = 2;SELECT ...COMMIT;トレードオフです。HNSW: 階層グラフによる高精度検索CREATE INDEX movie_embeddings_hnsw_idxON omdb.moviesUSING hnsw (movie_embedding vector_cosine_ops)WITH (m = 8, ef_construction = 16);HNSW は多層グラフを構築します。上位層は疎で、下位層ほど密になります。検索は最上層から始まり、段階的に下層に降りていきます。高速かつ高精度です。著者の実験では、HNSW は IVFFlat より Recall が良いです。データが追加・更新されても Recall が安定しています。インフラエンジニアとして、この安定性は魅力的です。 データが増えても再インデックスが不要です。IVFFlat はセントロイドが固定されるため、データが大きく変化すると Recall が落ちます。映画カタログは継続的に成長します。HNSW を選ぶ理由があります。（夜中の 2 時に「Recall が落ちてます！」というアラートで起こされるのは、もう懲り懲りです）実際に試してみてわかったのは、ベクトルはランダム生成でも類似検索の動作確認は可能だということ。ジャンルごとにパターンを変えれば、「アクション映画同士が近くなる」という挙動を確認できます。本番データがなくても、仕組みの理解には十分です。RAG の実装——Postgres を中心にこの章の核心は、RAG (Retrieval-Augmented Generation) の実装です。RAG の流れです。ユーザーが質問を入力質問を埋め込みに変換 (mxbai-embed-large)Postgres でベクトル類似検索を実行検索結果をコンテキストとして LLM に渡すLLM がコンテキストを考慮して回答を生成著者は Python の Jupyter Notebook で実装を示しています。LLM には TinyLlama (640 MB、1.1 B パラメータ) を使用しています。def retrieve_context_from_postgres(question):    # 埋め込みモデルに接続    embedding_model = OllamaEmbeddings(model="mxbai-embed-large:335m")    # 質問を埋め込みに変換    embedding = embedding_model.embed_query(question)    # Postgres でベクトル類似検索    query = """    SELECT name, vote_average, budget, revenue, release_date    FROM omdb.movies    ORDER BY movie_embedding <=> %s::vector LIMIT 3    """    cursor.execute(query, (embedding, ))    # コンテキストを構築    context = ""    for row in cursor.fetchall():        context += f"Movie title: {row[0]}, Vote Average: {row[1]}, ..."    return contextPostgres から取得した映画情報を LLM に渡します。def answer_question(question, context):    llm = OllamaLLM(model="tinyllama", temperature=0.6)    prompt = f"""    You're a movie expert and your task is to answer questions about movies    based on the provided context.    This is the user's question: {question}    Consider the following context: {context}    Respond in an engaging style that inspires the user to watch the movies.    """    response = llm.invoke(prompt)    return response「海賊映画のおすすめは？」と聞くと、Postgres が Pirates of the Caribbean シリーズを返し、LLM がそれをもとに魅力的な推薦文を生成します。Postgres が RAG のコンテキスト取得レイヤーとして機能しています。LLM は statelessこの章で確認しておきたいのは、「LLM は stateless」という点です。Because LLMs are stateless—meaning they don't retain the history of the interaction—if we want the LLM to consider earlier conversation history, we need to store it separately and pass it to the prompt object.LLM は会話履歴を記憶していません。毎回、コンテキストと履歴を渡す必要があります。この設計は、Postgres のステートレス性とも通じます。Postgres はクライアントのセッション状態を保持しません (connection pooling の文脈で)。毎回のクエリは独立しています。だから、会話履歴も Postgres に保存して、RAG のコンテキストとして渡せばいいのです。全てが Postgres で完結します。確認しておきたい拡張pgai という拡張は、この章で初めて目にしました。Explore the pgai extension if you'd like to implement the RAG workflow purely in SQL and execute it entirely within the database.SQL だけで RAG を実装できます。アプリケーション側に gen AI フレームワークを導入する必要がありません。調べてみたいです。もし実用的なら、Postgres の可能性がさらに広がります。9. Postgres for time seriesTimescaleDB を改めて評価するこの章で取り上げられている TimescaleDB は、名前は知っていましたが、実際に採用を検討したことはありませんでした。時系列データベースと言えば、InfluxDB か Prometheus を中心に検討してきました。「時系列データを扱いたいなら専用のデータベースを追加しましょう」という提案をしてきたこともあります。でも、この章を読んで気づきました。Postgres の拡張機能で時系列データベースができます。「Just use Postgres」の考え方が、ここでも貫かれています。新しいデータベースを追加する前に、まず Postgres で解決できるか確認します。TimescaleDB はその選択肢の 1 つです。運用エンジニアとしては、これは大きいです。新しいデータベースを追加するたびに、バックアップ戦略、モニタリング、アラートルール、障害対応手順が増えます。チームのメンバーも新しい技術を学ばなければいけません。もし Postgres の拡張機能で解決できるなら、運用負荷は格段に減ります。この章を読み終えて、「次に時系列データの相談が来たら、TimescaleDB を試してみよう」と思いました。Postgres のパーティショニングと Hypertable9.1 節では、Postgres のテーブルパーティショニングが紹介されています。CREATE TABLE heart_rate_measurements (  watch_id INT NOT NULL,  recorded_at TIMESTAMPTZ NOT NULL,  heart_rate INT NOT NULL,  activity TEXT NOT NULL CHECK (      activity IN ('walking', 'sleeping', 'resting', 'workout'))) PARTITION BY RANGE (recorded_at);PARTITION BY RANGE (recorded_at) で、recorded_at カラムの値に基づいてテーブルを範囲でパーティション分割する。その後、各パーティションを手動で作成する必要がある。CREATE TABLE measurements_jan2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');CREATE TABLE measurements_feb2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');このパーティショニング自体は Postgres の標準機能です。時系列データの場合、直近のデータだけが頻繁にアクセスされて、古いデータは圧縮したり削除したりします。パーティショニングを使えば、それが簡単にできます。でも、パーティションの作成と管理は手動でやる必要があります。著者も書いていますが、pg_partman と pg_cron という拡張機能を使えば自動化できます。そして、9.2 節で登場するのが TimescaleDB です。SELECT create_hypertable(  relation => 'watch.heart_rate_measurements',  dimension => by_range('recorded_at', interval '1 month'),  create_default_indexes => false);この一文で、テーブルが Hypertable に変換されます。Hypertable は Postgres の通常のテーブルですが、TimescaleDB が自動的にパーティション（chunk と呼ばれる）を作成・管理してくれます。新しいデータが挿入されると、TimescaleDB が自動的に新しい chunk を作ります。INSERT INTO watch.heart_rate_measurements VALUES(1,'2025-12-08 00:25:00',57,'sleeping');この INSERT だけで、_timescaledb_internal._hyper_1_13_chunk という新しいパーティションが自動生成されます。手動でパーティションを作る必要がありません。これは大きいです。timescaledb_information.chunks でチャンクのメタデータを確認できます。実際に確認してみると、日付ごとにチャンクが自動生成されていることがわかります。_hyper_1_1_chunk | 2025-01-01 - 2025-01-02_hyper_1_2_chunk | 2025-01-02 - 2025-01-03_hyper_1_3_chunk | 2025-01-03 - 2025-01-04この透過性が TimescaleDB の魅力です。過去のプロジェクトで、パーティショニングを手動で管理していたことがあります。月次バッチで次月のパーティションを作成するスクリプトを cron で回していました。でも、そのスクリプトが失敗したことに気づかず、翌月の INSERT が全部エラーになりました。月初の朝、Slack が火を噴きました。「データが入らない！」というメッセージが次々と流れてくる。あの日の朝のコーヒーは、確実に苦かったです。TimescaleDB を使っていれば、そんなことは起きませんでした。というか、あの朝のコーヒーはもっと美味しかったはずです。データ保持ポリシーの自動化9.4 節では、データ保持ポリシー（retention policy）の話が出てきます。SELECT add_retention_policy(  'watch.heart_rate_measurements', INTERVAL '30 days');これだけで、30 日以上古いデータを自動的に削除するジョブが設定されます。運用の観点から、これは非常にありがたいです。時系列データは増え続けます。ディスク容量は有限です。古いデータを定期的に削除する必要があります。過去のプロジェクトでは、手動で SQL を書いて、古いパーティションを DROP していました。でも、これも失敗することがあります。削除スクリプトのバグで、間違ったパーティションを削除してしまったこともありました。具体的に言うと、measurements_jan2025 を消すはずが measurements_jan2024 を消しました。そう、1 年分のデータが吹っ飛びました。バックアップから復旧しましたが、あの日の胃痛は今でも忘れられません。エンジニアのキャリアにおいて、誰もが一度は通る「DELETE/DROP の洗礼」というやつです。TimescaleDB の retention policy を使えば、そのリスクが減ります。胃痛も減ります。ただし、著者も警告していますが、このコマンドは慎重に使う必要があります。間違った設定をすると、重要なデータを失う可能性があります。time_bucket 関数の威力9.5 節では、TimescaleDB の time_bucket 関数が紹介されています。SELECT  time_bucket('10 minutes', recorded_at) AS period, activity,  AVG(heart_rate)::int AS avg_rate, MAX (heart_rate)::int AS max_rateFROM watch.heart_rate_measurementsWHERE watch_id = 1 AND activity = 'workout'  AND recorded_at >= '2025-04-23' AND recorded_at < '2025-04-24'GROUP BY period, activity ORDER BY period;これで、10 分ごとのバケットに心拍数を集約できます。普通の SQL でやろうとすると、DATE_TRUNC や複雑な計算が必要になります。でも、time_bucket を使えば、読みやすいクエリで簡単に集約できます。さらに、time_bucket はタイムゾーンの指定もできます。SELECT time_bucket('1 week', recorded_at, 'Asia/Tokyo',  '2025-04-01'::timestamptz) AS period, activity,  AVG(heart_rate)::int AS avg_rate,  MAX (heart_rate)::int AS max_rate, MIN (heart_rate)::int AS min_rateFROM watch.heart_rate_measurementsWHERE watch_id = 2 AND recorded_at >= '2025-04-01'AND  recorded_at < '2025-04-15'GROUP BY period, activity ORDER BY period, activity;ユーザーごとに異なるタイムゾーンでデータを集約できます。これはグローバルなサービスでは必須の機能です。そして、time_bucket_gapfill 関数です。SELECT watch_id, time_bucket_gapfill('1 minute', recorded_at) AS minute,  LOCF(AVG(heart_rate)::int) AS avg_rateFROM watch.heart_rate_measurementsWHERE watch_id=1 AND recorded_at BETWEEN '2025-03-02 07:25'  AND '2025-03-02 07:36'GROUP BY watch_id, minute ORDER BY minute;データが欠けている時間帯も含めて、連続した時間バケットを作成してくれます。さらに、LOCF（Last Observation Carried Forward）関数を使えば、欠損値を最後の値で埋めることができます。過去に、時系列データのグラフを作ったことがあります。データに欠損があると、グラフが途切れてしまいます。アプリ側で欠損値を補間する処理を書きましたが、複雑でした。time_bucket_gapfill と LOCF を使えば、データベース側で簡単に処理できます。Continuous Aggregates という機能9.6 節では、Continuous Aggregates（継続的集約）が紹介されています。CREATE MATERIALIZED VIEW watch.low_heart_rate_count_per_5minWITH (timescaledb.continuous) ASSELECT  watch_id,  time_bucket('5 minutes', recorded_at) AS bucket,  MIN(heart_rate) as min_rate,  COUNT(*) FILTER (WHERE heart_rate < 50) AS low_rate_count,  COUNT(*) AS total_measurementsFROM watch.heart_rate_measurementsGROUP BY watch_id, bucket;これは Postgres の Materialized View（マテリアライズドビュー）ですが、TimescaleDB が自動的にリフレッシュしてくれます。リフレッシュポリシーも設定できます。SELECT add_continuous_aggregate_policy  ('watch.low_heart_rate_count_per_5min',  start_offset => INTERVAL '15 minutes',  end_offset => INTERVAL '1 minute',  schedule_interval => INTERVAL '1 minute');これで、1 分ごとに集約結果が更新されます。普通の Materialized View は、手動で REFRESH MATERIALIZED VIEW を実行しないと更新されません。でも、TimescaleDB の Continuous Aggregates は自動的に更新されます。しかも、Hypertable に保存されるので、パーティショニングの恩恵も受けられます。この章の例では、心拍数が 50 BPM 以下の回数をカウントして、徐脈（bradycardia）の兆候を検出しています。リアルタイムで集約結果を更新して、ユーザーにアラートを送ります。これ、単なるデモではありません。実用的です。過去に、IoT デバイスからのデータを集約して、異常を検知するシステムを運用したことがあります。集約処理は別のバッチジョブで定期的に実行していました。でも、リアルタイム性が求められると、バッチでは間に合いません。TimescaleDB の Continuous Aggregates を使えば、リアルタイムに近い形で集約結果を更新できます。B-tree インデックスと BRIN インデックス9.7 節では、時系列データのインデックス戦略が紹介されています。まず、B-tree インデックスです。CREATE INDEX heart_rate_btree_idxON watch.heart_rate_measurements (recorded_at, watch_id);複合インデックスで、recorded_at と watch_id の両方を含めます。これで、時間範囲とデバイス ID の両方で絞り込むクエリが高速化されます。著者の説明によれば、B-tree インデックスは実際のカラム値とテーブル行へのポインタを保存します。だから、特定の行に直接アクセスできます。でも、B-tree インデックスはサイズが大きいです。この章の例では、パーティションごとに数 MB のサイズになっています。そこで登場するのが BRIN（Block Range Index）です。CREATE INDEX heart_rate_brin_idxON watch.heart_rate_measurementsUSING brin (recorded_at);BRIN インデックスは、ページ範囲ごとの最小値と最大値だけを保存します。だから、サイズが非常に小さいです。この章の例では、24 KB しかありません。B-tree の 100 分の 1 です。でも、BRIN はページ全体をスキャンする必要がある場合があります。だから、少量のデータを取得するクエリでは B-tree の方が速いです。著者の説明を読んで、BRIN の仕組みがよくわかりました。時系列データのように、カラム値が物理的な配置と強く相関している場合に BRIN は有効です。心拍数測定データは常に追記されます。新しい測定は常に大きな recorded_at 値を持ちます。だから、ページ内のデータは時系列順に並びます。BRIN はこの特性を活かします。過去に、ログテーブルにインデックスを作ったことがあります。そのテーブルは append-only で、タイムスタンプカラムがありました。B-tree インデックスを作りましたが、サイズが大きくなって困りました。「なんでインデックスがテーブルより大きいんだ？」と首を傾げながら、ディスク容量を確保するために古いインデックスを削除する日々でした。当時は BRIN を検討していませんでした。Postgres のドキュメントで存在は知っていたはずですが、実際に使う場面を意識していませんでした。必要に迫られないと、知識は実践に結びつかないものです。10. Postgres for geospatial data「地理空間データ」の意外な身近さこの章を読んで認識したのは、地理空間データベースの機能が、自分の仕事に意外と近いということです。PostGIS の名前は知っていました。でも、「地理空間データベース」という言葉から受ける印象は、「GIS 専門家のための特殊な技術」でした。Google Maps みたいなサービスを作る時に使うやつ、くらいの認識。要するに、「自分には関係ない」と決めつけていたわけです。実際には、もっと身近なユースケースがあります。著者が冒頭で説明する Geofabrik（OpenStreetMap のデータ抽出サービス）、osm2pgsql（OSM データのインポートツール）、QGIS（データ可視化ツール）。これらのツールと PostGIS の組み合わせで、10 分以内にフロリダ州全体の地理データをローカル環境で扱える状態にできます。この手軽さが、「Just use Postgres」の真髄だと感じました。geometry と geography — 2つのデータ型の意味10.1.2 節で説明される geometry と geography の違いに、初めて向き合いました。geometry 型（Web Mercator projection、SRID 3857）。- 平面（Euclidean plane）として計算- 単位はメートル- 計算が速い- 距離が長いと精度が落ちるgeography 型（WGS 84、SRID 4326）。- 球面（spherical model）として計算- 単位は度（longitude/latitude）だが、計算結果はメートル- 計算が遅い- 地球の曲率を考慮するため正確「なるほど、速度と精度のトレードオフか」と思いました。でも、本当に理解したのは、用途によって使い分ける必要があるということでした。ローカルな範囲（例：Tampa 市内のレストラン検索）なら geometry で十分です。でも、大陸をまたぐような距離の計算なら geography が必要になります。注意点として、ST_Distance に geometry 型を渡すと単位は「度」になります。geography 型を渡すと「メートル」です。最初、この違いを知らずに「距離が 0.003 って何？」と混乱しました。それ、度でした。著者は本章で主に geometry を使っています。理由は明示されていませんが、フロリダ州内のデータを扱っているからでしょう。ST_DWithin と ST_Distance — index の有無で 500 倍の差10.6.2 節の実行計画の比較に目を奪われました。ST_DWithin を使った場合（Listing 10.26）:- 実行時間: 1.125 ms- GiST index を使用（Bitmap Index Scan）- 1,205 件を候補として抽出し、36 件にフィルタリングST_Distance を使った場合（Listing 10.27）:- 実行時間: 488.119 ms- GiST index を使用せず、フルテーブルスキャン（Parallel Seq Scan）- 18,676 件を候補として抽出し、36 件にフィルタリング同じ結果（36 件のレストラン）を得るのに、434 倍の時間がかかっています。なぜこんなに違うのでしょうか。片や 1 ミリ秒でサクッと答え、片や半秒近く考え込んでいます。まるで、道を聞かれて地図アプリを開く人と、記憶を辿って一生懸命思い出そうとする人くらい違います。ST_DWithin is one of the index-aware functions that can use the GiST index by performing an initial fast filtering of the data using the combination of the bounding box operator && and the ST_Expand function.著者の説明によると、ST_DWithin は内部で bounding box（境界ボックス）を使った高速フィルタリングをします。GiST index がこの bounding box 検索に対応しています。一方、ST_Distance は常に正確な距離を計算します。bounding box を使わないから、index を利用できません。この違いを知らなかったら、「ST_Distance(point1, point2) <= 500 で 500m 以内を検索」と書いてしまっていたでしょう。数百万件のデータに対してフルスキャンが走ります。「index-aware functions」という概念を、初めて意識しました。GiST の構造 — R-tree で理解できた10.6.1 節の GiST index の説明は、初めて「わかった」感覚がありました。以前、B-tree index については理解していました。でも、GiST（Generalized Search Tree）は「汎用的な index」という説明しか見たことがなく、具体的なイメージが湧きませんでした。著者の図解（Figure 10.6, 10.7, 10.8）がわかりやすかったです。フロリダ州全体を 5 つの大きな矩形（R1〜R5）に分割それぞれの矩形をさらに小さな矩形に分割（R6〜R25）最小の矩形が、実際のテーブル行（points）を指す検索の流れ。1. Downtown Miami の座標が、どの大きな矩形に含まれるかをチェック → R52. R5 の中で、どの小さな矩形に含まれるかをチェック → R243. R24 の中の全 points をスキャン → 該当するものだけ返すR-tree（Rectangular tree）という名前の由来も理解できました。矩形（Rectangle）で空間を階層的に分割していく木構造です。実際に座標変換も試してみました。Walt Disney World の座標を WGS 84 から Web Mercator へ変換すると、経度 -81.5639 が X -9079651.82 に変わります。緯度 28.3852 は Y 3297626.07 になります。単位がメートルに変わるのがわかります。この構造、実は Chapter 6 の全文検索で出てきた GiST index と同じ基盤です。あの時は tsvector 型の lexemes を indexing していました。今回は geometry 型の bounding boxes を indexing しています。GiST は、データ型ごとに異なる index 構造を実装できる汎用フレームワークなんだと、やっと腹落ちしました。QGIS で可視化 — 「見える」ことの重要性10.4 節の QGIS による可視化は、実際に手を動かしました。SELECT name, ST_AsText(way) AS coordinatesFROM florida.planet_osm_pointWHERE name = 'Tampa' and place = 'city';このクエリで得た Tampa の座標を、QGIS で表示した時、「あ、本当に Tampa の中心だ」と思いました。データベースに入っている座標が、実際の地図上の位置と一致します。当たり前のことですが、自分の目で確認するまで信じられませんでした。planet_osm_polygon テーブルの 6.8 100 万の polygons を QGIS で読み込むと、フロリダ州の地図が少しずつレンダリングされていきます。湖、道路、建物、公園。すべてが Postgres のテーブルに格納されています。「データが見える」ことの重要性を、改めて実感しました。osm2pgsql — データインポートの簡単さ10.3 節で紹介されている osm2pgsql ツールは実用的です。docker run --name osm2pgsql --network="host" \  -e PGPASSWORD=password \  -v osm2pgsql-volume:/data \  iboates/osm2pgsql:2.1.1 \  -H 127.0.0.1 -P 5432 -d postgres -U postgres --schema florida \  http://d3e4uq6jj8ld3m.cloudfront.net/florida-250501.osm.pbfこのコマンド 1 つで、フロリダ州全体の OSM データ（2025 年 5 月 1 日時点）を Postgres にインポートできます。所要時間は約 10 分。自分の環境（M1 Mac）では 7 分ほどでした。インポート後、以下のテーブルが自動生成されます。planet_osm_point — 単一座標で表現できるもの（レストラン、ホテルなど）planet_osm_line — 線分（道路、川など）planet_osm_polygon — 閉じた領域（建物、公園、湖など）planet_osm_roads — planet_osm_line のサブセット（ズームレベルが低い時のレンダリング用）それぞれのテーブルに、既に GiST index が作成されています（planet_osm_point_way_idx など）。この「すぐに使える」感覚が、PostGIS の魅力だと感じました。ST_Within と ST_Intersects — 空間関係の判定10.5.2 節と 10.5.3 節で紹介される ST_Within と ST_Intersects の違いが、最初は曖昧でした。ST_Within(A, B)。- A が B の中で完全に含まれている場合は true- A の全ての点が、B の内部にある- 例：あるアトラクションが、Disney's Hollywood Studios の中にあるかST_Intersects(A, B)。- A と B が少なくとも 1 点を共有する場合は true- 完全に含まれていなくてもいい、交差していれば OK- 例：ある道路が、Miami の境界を横切っているかListing 10.22 のクエリで理解できました。SELECT l.name, l.highway, ST_Length(l.way) AS len_metersFROM florida.planet_osm_line lJOIN miami m ON ST_Intersects(l.way, m.boundaries)WHERE l.highway IN ('primary', 'secondary')このクエリは、Miami の境界内にある道路だけでなく、境界を横切る道路も取得します。ST_Within を使っていたら、境界を横切る道路は取得できません。この違いを知らないと、「なぜこの道路が結果に含まれるのか」と混乱したでしょう。「Just use Postgres」の再確認この章を読んで、改めて「Just use Postgres」の意味を理解しました。次に「位置情報を扱うから、MongoDB（GeoJSON 対応）を追加しよう」と言われた時、私は聞き返せます。「Postgres で試した？PostGIS なら、既存のインフラでできるかもしれない」新しいデータベースを追加する前に、まず既存の Postgres で何ができるかを確認します。これがこの本の一貫したメッセージです。そして、大抵の場合、Postgres でできてしまいます。追加のインフラを管理する手間（と、深夜の障害対応）が減るのは、エンジニアとしても組織やチームとしてもありがたいです。地理データだって、Postgres でできます。それも、思ったより簡単に。11. Postgres as a message queueメッセージキューとして Postgres を使う、という選択この章で参考になったのは、「Postgres をメッセージキューとして使う判断基準」が明確に示されていた点です。正直に言うと、読む前は「Postgres でメッセージキュー？　無理がある」と思っていました。10 年近くソフトウェアエンジニアをやっている中で、メッセージキューといえば RabbitMQ、Kafka、AWS SQS が標準でした。Postgres はあくまでリレーショナルデータベース。「餅は餅屋」という言葉が頭に浮かびました。というか、新しいツールを導入する言い訳が欲しかっただけかもしれません（インフラエンジニアの悪い癖です）。でも、この章を読み終えて気づきました。「Just use Postgres」の本質は、万能性じゃなくて、既存資産の最大活用でした。Postgres をメッセージキューとして使う 3 つの基準11.1 節で、著者は 3 つの基準を挙げています。1. トランザクショナルな一貫性が必要な場合DMV（運転免許センター）の例が分かりやすかったです。来訪者がチェックインする（ビジネスロジック）と同時に、待機キューにメッセージを追加する（イベント記録）。この 2 つの操作がアトミックに実行される必要があります。もし別々のシステム（Postgres + 専用メッセージキュー）だったら、チェックインは成功したのにメッセージ送信が失敗する可能性があります。その時、アプリケーション側で整合性を保証しなければなりません。If you want the check-in operation and the message added to the visitors queue to be executed atomically (as a single transaction), then use Postgres.この一文は重いです。私が関わったプロジェクトで、「決済処理」と「メール送信キュー」が別々のシステムだったせいで、決済完了したのに確認メールが届かないトラブルがありました。結局、リトライ機構を複雑に実装して解決しましたが、あれは Postgres で統一できていれば避けられたかもしれません。深夜 3 時に「メールキューが詰まった」アラートで起こされることもなかったでしょう（遠い目）。2. メッセージ量が Postgres で処理可能な場合著者は正直です。If the effort is too high or the configuration becomes overly complex, consider using a specialized message queue instead.Postgres の書き込みスケールには限界があります。シングルプライマリインスタンスだから、書き込み負荷が高すぎる場合はシャーディングや分散 Postgres（CitusData、YugabyteDB）が必要になります。でも、DMV の例では「メッセージ量は比較的低い」と明言しています。この「正直さ」がいいです。Postgres は万能じゃない、でも適切なユースケースならシンプルで強力です。3. 既に Postgres を使っている場合If your application already uses Postgres and now needs to support a message queue use case, consider using Postgres first before bringing in a specialized solution.これが「Just use Postgres」の核心です。新しいシステムを追加するコストは、技術的負債だけじゃありません。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を運用しているなら、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。カスタムメッセージキューの実装11.2 節と 11.3 節では、カスタムメッセージキューを実装しています。シンプルな設計CREATE TABLE mq.queue (    id BIGSERIAL PRIMARY KEY,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new');この設計、シンプルだけど実用的です。id: 自動採番（BIGSERIAL）で一意性を保証message: JSON 型でペイロードを格納（柔軟性重視）created_at: FIFO 順序の保証status: メッセージのライフサイクル管理（new → processing → completed）著者が JSON 型を選んだ理由が面白いです。The JSONB type would preprocess messages before storing them, which might slow down ingestion and alter the original structure—for example, by reordering object keys.JSONB はクエリ効率のために前処理を行いますが、メッセージキューでは「プロデューサーからコンシューマーへそのまま渡す」だけだから、JSON 型で十分です。この「ユースケースに応じた選択」が、エンジニアリングの本質だと感じました。FOR UPDATE SKIP LOCKED の威力mq.dequeue 関数の実装で、FOR UPDATE SKIP LOCKED が使われています。SELECT id FROM mq.queueWHERE status = 'new' ORDER BY created_atFOR UPDATE SKIP LOCKEDLIMIT messages_cntこの構文は、改めて確認すると有用です。FOR UPDATE は行レベルロックをかけます。通常なら、他のトランザクションがロックされた行にアクセスしようとするとブロックされて待機します。でも SKIP LOCKED を加えると、ロックされている行をスキップして、次の利用可能な行を取得します。複数のコンシューマーが並行してメッセージを取得しても、お互いをブロックせずに並列処理できます。This allows consumers to process new messages in parallel without blocking each other, improving overall throughput.これは Postgres のメッセージキュー実装におけるキラー機能です。実際に 2 つのワーカーを同時に動かして確認しました。Worker 1 がメッセージ 1, 2 を取得している間、Worker 2 はブロックされずにメッセージ 3, 4 を取得できます。お互いが異なるメッセージを処理する。これが SKIP LOCKED の威力です。以前、複数ワーカーでジョブキューを処理する実装を Rust で書いた時、排他制御で悩んだことがあります。あの時、FOR UPDATE SKIP LOCKED を知っていれば、もっとシンプルに実装できたかもしれません。LISTEN と NOTIFY11.4 節の LISTEN / NOTIFY は、Postgres の隠れた名機能だと感じました。DMV のシナリオでは、来訪者がチェックインすると、待機中の職員にリアルタイムで通知が届きます。-- 職員側（リスナー）LISTEN queue_new_message;-- ターミナル側（ノティファイア）SELECT mq.enqueue('{"service": "car_registration", "visitor": "Marta Jones"}');-- → pg_notify('queue_new_message', 'new_message')これで、ポーリング不要の非同期通知が実現できます。ただし、2 つの制限があります。過去の通知は受け取れない: 接続後に発行された通知のみ受信可能レプリカでは使えない: プライマリノードへの接続が必要特に 2 つ目は運用上重要です。読み取り負荷をレプリカに逃がしている構成でも、LISTEN/NOTIFY 専用にプライマリへの接続を維持する必要があります。でも、この制限を理解した上で使えば、非常に強力な機能です。あと、pg_notify はトランザクション終了時に送信されます。途中でロールバックすると通知も送られません。これは整合性の観点から正しい動作ですが、最初は「なぜ通知が来ない？」と悩みました。実装上の考慮事項11.5 節では、いくつかの重要な考慮事項が述べられています。インデックス戦略mq.dequeue 関数は、デフォルトではフルテーブルスキャンを行います。created_at と status にインデックスがないからです。著者は 2 つのオプションを提示しています。オプション 1: created_at のみのインデックス。CREATE INDEX mq_created_at_index_btree ON mq.queue (created_at);オプション 2: パーシャルインデックス（推奨）CREATE INDEX mq_partial_index_btreeON mq.queue (created_at, status)WHERE status = 'new';パーシャルインデックスは、status = 'new' の行だけをインデックスに含めます。これで、インデックスサイズが小さくなり、new メッセージへのアクセスがさらに高速化されます。この「状況に応じた最適化」の姿勢が参考になります。DMV のユースケースでは不要かもしれませんが、高頻度メッセージングなら必須です。パーティショニング11.5.3 節のパーティショニングの話は、時系列データの章（第 9 章）とつながりました。メッセージキューも時系列データの一種です。created_at でレンジパーティショニングすれば、古いメッセージを効率的にアーカイブ・削除できます。CREATE TABLE mq.queue (    id BIGSERIAL,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new',    PRIMARY KEY (id, created_at)) PARTITION BY RANGE (created_at);パーティションごとにメッセージを管理できるから、古いパーティションを削除（DROP TABLE）するだけで大量の古いメッセージを一瞬で消せます。VACUUM の負荷も軽減されます。なぜなら、新しいパーティションだけが頻繁に更新されるからです。この設計パターンは、ログ管理やイベントストアにも応用できそうです。フェイルオーバー機構11.5.4 節で、メッセージ処理の失敗対策が述べられています。コンシューマーがメッセージを取得（status = 'processing'）した後にクラッシュすると、そのメッセージは processing 状態のまま放置されます。著者の提案は、pg_cron を使った定期的なリセットです。a periodic job in the database to check for messages stuck in the processing state and reset their status to new.これは実用的です。ただし、同じメッセージが複数回処理される可能性があるから、コンシューマー側で冪等性を保証する必要があります。pgmq 拡張11.6 節と 11.7 節では、pgmq 拡張が紹介されています。pgmq は「Postgres Message Queue」の略で、AWS SQS 互換の API を提供します。カスタム実装で学んだ原理を、pgmq が抽象化してくれます。可視性タイムアウトpgmq.read 関数の vt（visibility timeout）が面白いです。SELECT msg_id, message, enqueued_atFROM pgmq.read(  queue_name => 'visitors_queue',  vt         => 120,  -- 2分間の可視性タイムアウト  qty        => 1);メッセージを取得してから 120 秒間、そのメッセージは他のコンシューマーから見えなくなります。でも、120 秒以内に pgmq.archive を呼ばないと、メッセージは再びキューに戻ります。これで、コンシューマー失敗時の自動リトライが実現できます。DMV の例では、職員が来訪者を呼び出してから 2 分以内に現れなければ、別の来訪者を呼び出せる仕組みに使われています。この「タイムアウトベースのフェイルオーバー」は、AWS SQS と同じ設計パターンです。アーカイブテーブルpgmq.archive 関数は、メッセージを pgmq.q_visitors_queue から pgmq.a_visitors_queue に移動します。削除（DELETE）ではなくアーカイブ（移動）だから、処理済みメッセージの監査ログを保持できます。これは本番運用で重要です。「このメッセージ、本当に処理されたのか？」を後から確認できます。本全体を読み終えて第 11 章は、この本の最終章です。第 1 章「Meeting Postgres」から始まり、JSON、地理空間、全文検索、時系列、ベクトル検索、グラフ、そしてメッセージキュー。「Just use Postgres」の本質は、Postgres の万能性を主張することじゃありませんでした。既に Postgres を使っているチームが、新しいユースケースに直面した時、別のデータベースを追加する前に、まず Postgres で解決できるか試してみよう、というメッセージです。それは、アーキテクチャをシンプルに保つための選択であり、運用コストを抑えるための選択であり、チームの認知負荷を減らすための選択です。10 年近くソフトウェアエンジニアをやってきて、システムが複雑化する様子を何度も見てきました。「全文検索だから Elasticsearch」「時系列データだから InfluxDB」「メッセージキューだから RabbitMQ」確かに、それぞれの専用ソリューションは強力です。でも、それぞれが運用コストを生みます。バックアップ、モニタリング、アラート、障害対応、バージョンアップ。全てがチームの負担になります。そして、構成図に新しいアイコンが増えるたびに、誰かが「これ誰がメンテするんですか？」と聞く声が聞こえます。「Just use Postgres」は、その複雑化への抵抗です。もちろん、これは「新しい技術を学ぶな」という意味ではありません。新しいツールやサービスが出てきたとき、まず「運用時にどうなるか」を考える。それがベテランエンジニアに求められる姿勢だと思います。機能の魅力だけでなく、3 年後にメンテナンスできる人がいるか、障害時に対応できるか、既存システムとの整合性はどうか。これは Postgres の新機能についても同じです。pgvector は便利ですが、まだ運用実績が浅い。TimescaleDB も Postgres の拡張とはいえ、独自のアップグレードパスがあります。「Postgres だから安心」ではなく、その機能の成熟度を見極める必要があります。結局のところ、謙虚に学び続けるしかありません。新しい技術も、既存の技術も。私が最近考えている技術選定の基準があります。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、40 年の実績がある標準的な選択肢を使い、その可能性を最大限に活かす。それが、この本から学んだことです。もちろん、Postgres で解決できないユースケースもあります。著者は正直にそれを認めています。でも、試す前から諦めるのではなく、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。この本を読み終えて、次に「〇〇が必要だから△△を導入しましょう」と言われた時、私は自信を持って聞き返せるようになりました。「Postgres で試しましたか？」おわりに読むことと、手を動かすこと11 章を読み終えて、私は 1 つの疑問を持っていました。「本当に、Postgres でこれだけのことができるのか？」本に書いてあることを読んで「なるほど」と思うのと、実際に動かして確認するのは、全く別の体験です。少なくとも、私にとっては。だから、手を動かすことにしました。Docker で Postgres を立てて、Rust でコードを書いて、各章の内容を 1 つずつ検証しました。generate_series から始まって、CTE、Window Functions、Recursive Query と進みました。JSONB、全文検索、pgcrypto、pgvector、TimescaleDB、PostGIS、そしてメッセージキュー。全 11 章です。その過程で、いくつかのことに気づきました。手を動かして初めてわかったこと本を読んでいるときは「ふーん」と思っていたことが、実際に動かすと「あ、そういうことか」に変わる瞬間があります。例えば、FOR UPDATE SKIP LOCKED。本には「複数のコンシューマーが並行してメッセージを取得できる」と書いてありました。でも、実際に 2 つのワーカーを同時に動かして、それぞれが異なるメッセージを取得するのを見たとき、初めて腑に落ちました。Worker 1 がメッセージ 1 を取得: {"service":"registration","visitor":"Alice"}Worker 2 がメッセージ 3 を取得: {"service":"registration","visitor":"Charlie"}この出力を見て、「ああ、本当にブロックせずにスキップしてるんだ」と思いました。言葉で理解することと、目で見て理解することは、違うものです。他にも気づきはありました。pg_typeof() の結果を Rust で取得しようとしたらエラーになって、::TEXT でキャストする必要があることを知りました。PL/pgSQL の変数名がテーブルのカラム名と衝突してエラーになることも知りました。TEMP TABLE の名前が別のデモと衝突して、「なんでエラーになるんだ？」と 30 分悩んだこともあります。これらは本には書いてありません。当たり前です。本は概念を説明するものであって、私が遭遇するエラーを予測するものではないから。でも、そういうエラーと向き合う時間こそが、理解を深める時間だったと思います。判断基準が見えてきた11 章を読み終えて、そして検証を終えて、私の中に 1 つの判断基準ができました。「いつ Postgres で十分で、いつ専用ツールを検討すべきか」全文検索なら、数百万件以下のシンプルな検索であれば Postgres で十分です。ただし数億件規模や日本語の形態素解析、複雑なファセット検索が必要なら、Elasticsearch を検討すべきです。ベクトル検索なら、pgvector で数百万ベクトルまでは対応できます。でも、数億ベクトル規模やリアルタイム更新が必要なら、Pinecone や Milvus の出番です。メッセージキューなら、秒間数百メッセージ程度なら Postgres で十分です。でも、秒間数万メッセージや複雑なルーティングが必要なら、RabbitMQ や Kafka を使うべきです。この判断基準は、本を読んだだけでは身につかなかったと思います。実際に動かして、限界を感じて、初めてわかることがありました。「Postgres で試した？」この本を読み始める前、私はこの言葉を言えませんでした。「全文検索が必要です」と言われたら、「Elasticsearch ですね」と即答していました。「時系列データを扱いたい」と言われたら、「InfluxDB か TimescaleDB ですね」と答えていました。TimescaleDB が Postgres の拡張であることすら、あまり意識していませんでした。今は違います。「全文検索が必要です」と言われたら、「どのくらいのデータ量ですか？　検索の要件は？　まず Postgres の tsvector で試してみませんか？」と聞き返せます。「ベクトル検索がしたい」と言われたら、「pgvector で試してみましょうか。数百万ベクトルくらいなら対応できますよ」と提案できます。それが良いことなのかどうか、正直わかりません。もしかしたら、早めに専用ツールを導入した方が、長期的には幸せだったかもしれません。Postgres で頑張った結果、パフォーマンスの壁にぶつかって、結局移行することになるかもしれません。でも、少なくとも「試した上で判断する」ことはできるようになりました。「Postgres で試した？」その一言を、自信を持って言えるようになりました。そして、自分自身にも問いかけるようになりました。新しいデータベースを追加する前に、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。運用負荷も増えません。深夜 3 時のアラート対応の可能性も、1 つ減ります。それだけで、この本を読んだ価値はあったと思います。最後に11 章分の感想を書いて、検証コードを書いて、そしてこの「おわりに」を書いています。読み始めたときは、「Postgres の可能性を広げる本」だと思っていました。読み終えた今は、「技術選定の視点を変える本」だったと思っています。「最適なツールを選ぶ」という言葉は、聞こえが良いです。でも、その「最適」は何を基準にしているのでしょうか。機能の豊富さ？　パフォーマンス？　それとも、運用の複雑さ？この本は、「十数年単位の運用の複雑さ」という視点を私に与えてくれました。新しいデータベースを追加することは、コストです。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を使っているなら、まず Postgres で試してみる。それで十分なら、そのコストを払わなくて済みます。それが「Just use Postgres」の本当の意味だと、今は思っています。「できる」と「やるべき」の違いこの本を読んで、1 つ注意しなければならないことがあります。「Postgres でできる」と「Postgres でやるべき」は、違います。本書は Postgres の可能性を示してくれますが、すべてのユースケースで Postgres を選ぶべきだとは言っていません。著者自身も、専用ツールが必要な場面があることを認めています。大事なのは、選択肢を知った上で判断することです。「Postgres でもできるけど、このユースケースでは Kafka の方が適している」と判断するのと、「Postgres でできることを知らずに Kafka を選ぶ」のでは、意味が違います。前者は informed decision、後者は思い込みです。この本は、その informed decision をするための知識を与えてくれました。チームと知識の継承もう 1 つ、この本を読んで考えたことがあります。技術選定は、個人の問題ではありません。チームの問題です。新しいデータベースを導入するということは、チームメンバー全員がそれを学ぶ必要があるということです。障害対応できる人が増えなければ、特定の人に負荷が集中します。その人が退職したら、知識が失われます。Postgres を選ぶということは、チームの認知負荷を抑えるという選択でもあります。多くのエンジニアが Postgres の基本を知っています。採用市場でも、Postgres 経験者を見つけるのは比較的容易です。ドキュメントも豊富で、コミュニティも活発です。「技術的に最適」と「チームにとって最適」は、必ずしも一致しません。十数年単位で考えたとき、チームの持続可能性も重要な判断基準です。謙虚に学び続けることこの本を読んで、もう 1 つ気づいたことがあります。10 年近くこの仕事をしていても、知らないことはたくさんあります。Recursive CTE の活用パターン、BRIN インデックスの使い所、FOR UPDATE SKIP LOCKED の仕組み。どれも Postgres に昔からある機能ですが、実務で使う機会がなければ、深く理解することはありませんでした。新しい技術が出てきたとき、いきなり飛びつくのは危険です。でも、既存の技術の可能性を見落としているのも、同じくらい問題です。これは Postgres の新機能についても同じです。pgvector や TimescaleDB は便利ですが、Postgres 本体と比べれば運用実績は浅い。「Postgres を使う」という判断と、「Postgres の新機能を本番投入する」という判断は、別々に評価する必要があります。結局のところ、謙虚に学び続けるしかありません。はじめにでも書きましたが、私は技術選定についてこう考えています。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、流行りの新しいデータベースに飛びつく前に、まず Postgres で何ができるかを確認する。それが、この本から学んだ姿勢です。もちろん、Postgres で全てが解決できるわけではありません。本当に専用ツールが必要な場面もあります。大事なのは、「試した上で判断する」ことです。最適解を求めて複雑さを増やすより、十分解でシンプルさを保つ方が、長期的には幸せなことが多いです。少なくとも、深夜 3 時のアラート対応は減ります。それは、間違いありません。参考書籍失敗から学ぶRDBの正しい歩き方 Software Design plus作者:曽根 壮大技術評論社AmazonSQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazonセンスの良いSQLを書く技術　達人エンジニアが実践している３５の原則作者:ミックKADOKAWAAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Associate Generative AI LLMs編~]]></title>
            <link>https://zenn.dev/akasan/articles/4437690e2ef59b</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/4437690e2ef59b</guid>
            <pubDate>Mon, 24 Nov 2025 08:41:46 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるAssociate Generative AI LLMsを取得したので、その内容を共有しようと思います。 Associate Generative AI LLMsとは？Associate Generative AI LLMs（以下、NCA-GENL）は、NVIDIAが提供している認定資格の一つであり、AIドリブンなアプリケーションを開発したり運用するためのエントリーレベルの資格となっています。主にLLMについて取り扱われる部分が多いですが、従来の機械学習に関する知識についても問われるようになっています。https://www.nvidia.com...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、本を読め]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314</guid>
            <pubDate>Sun, 23 Nov 2025 19:33:14 GMT</pubDate>
            <content:encoded><![CDATA[はじめに私は本を読むのが好きです。朝、コーヒーを淹れて、ソファに座って、ページを開く。その時間が好きです。物語の中に入り込んで、登場人物の人生を追いかけ、著者の思考を辿り、知らない世界を覗き見る。ただ、それが楽しいんです。でも、誰かに「最近、何か読んだ？」と聞かれて、タイトルを答えると、必ず次の質問が来ます。「へえ、面白かった？ 何か学びはあった？」あるいは、こんな質問が来ます。「その本、どういうジャンル？ 自己啓発系？ノンフィクション？」違和感があります。映画を見たあと、「何か学びはあった？」なんて聞かれません。音楽を聴いたあと、「それ、自己啓発系？」なんて聞かれません。ゲームをクリアしたあと、「成長できた？」なんて聞かれません。でも、本だけは違います。読書には、常に「目的」が求められます。「成長のため」「知識を得るため」「キャリアアップのため」。ただ楽しいから読む、では許されない空気があります。SNSを開けば、「読書のすすめ」が溢れています。「本を読まない人は生き残れない」「年間百冊読めば人生が変わる」「ビジネスパーソン必読書」。どれも善意です。本当に、善意なんです。でも、その善意が、読書を窮屈にしています。私が小説を読んでいると言うと、「へえ、小説なんだ」と言われます。その「なんだ」という響きに、少しだけトゲがあります。まるで、「ビジネス書じゃないんだ」「役に立つ本じゃないんだ」と言われているような。あるいは、ミステリを読んでいると言うと、「息抜きにはいいよね」と言われます。その「息抜き」という言葉に、少しだけ違和感があります。まるで、本来読むべきは「ちゃんとした本」で、娯楽はその合間に挟むもの、と言われているような。おかしくないですか？ 映画は娯楽として認められています。音楽は娯楽として認められています。ゲームは娯楽として認められています（最近は、ですけど）。でも、読書だけは、娯楽であることを許されていません。「ただ楽しいから読む」では、ダメなんでしょうか。物語に没入して、現実を忘れる。登場人物に共感して、泣いたり笑ったりする。推理小説でハラハラして、犯人を当てようとする。SF小説で想像力を膨らませて、知らない世界に思いを馳せる。それだけじゃ、ダメなんでしょうか。この文章を書いている今も、矛盾しています。私は「読書について考えている私」を演出しているのだろう。この文章を投稿したら、何人かが「わかる」って言ってくれるだろう。その承認が欲しいのだろう。でも、それでも書きたいんです。なぜ読書だけが、娯楽であることを奪われるのか。なぜ読書だけが、「成長」や「学び」と結びつけられるのか。そして、その結びつきが、どれだけ読書を窮屈にしているのか。この文章は、その違和感から始まります。答えを出すつもりはありません。ただ、この違和感を言葉にしてみたいんです。もしかしたら、あなたも同じ違和感を抱えているだろう。「ただ楽しいから読む」という、当たり前のことが、当たり前じゃなくなっている世界。その世界を、少しだけ問い直してみませんか。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。加速文化という病私たちは走り続けています私たちは、「加速文化」の中を生きています。現代社会では変化のスピードが絶え間なく加速し、個人も常に成長し続けることを要求されます。「走り続けること」そのものが目的化し、どこに向かっているのか、なぜ走っているのかという本質的な問いは置き去りにされます。「もっと成功しろ」「もっと幸せになれ」「スキルを身につけろ」「成長し続けろ」。現代社会は、こうした強烈なプレッシャーを発し続けます。「もっと」という言葉は、終わりのない要求を意味します。どれだけ達成しても、常に「もっと」が待っています。誰かと比べずにはいられませんSNSを開けば、誰かが何かを達成しています。誰かが本を出版しています。誰かが転職に成功しています。誰かが新しいスキルを身につけています。私たちは比較せずにいられません。そして、比較するたびに自分を劣っていると感じます。「自分は何もしていない」「自分は成長していない」「自分は遅れている」。SNSは、他者の「成功」を可視化し、価値を数値化します。しかし、SNSに現れるのは、他者の「ハイライト」だけです。私たちは自分の未編集の人生と、他者の編集済みの人生を比較してしまいます。より問題なのは、この比較が内面化されることです。自分の中に「比較する目」が住み着きます。常に自分を評価する目。「これは成長でしょうか」「これは生産的でしょうか」。この内なる審判者は、決して満足しません。その基準は加速文化から与えられ、常に「もっと」を要求するからです。だから、走らなければなりません。これは、まるでトレッドミルで走っているようなものです。動いているという実感だけがあって、前進しているという実感はありません。『鏡の国のアリス』の赤の女王が言ったように、「同じ場所にとどまるためには、全力で走り続けなければならない」のです。安定したいから成長したい、というおかしさここに、現代の最も奇妙な矛盾があります。「安定したいから成長したい」。「安定」と「成長」は、本来相反する概念です。安定とは、変化しないこと。成長とは、変化し続けること。なのに、私は「安定したいから成長したい」と言っています。なぜこの矛盾が成立するのでしょうか。「変化する環境の中で生き残るためには、変化し続けなければならない」という論理があるからです。つまり、「安定」は、もはや「変化しないこと」では達成できません。「変化し続けること」によってのみ達成できるのです。しかし、この論理は実は安定を永遠に延期しています。「いつか安定する」という約束のもとで、今は変化し続けます。でも、その「いつか」は決して来ません。終わりのない変化を、「安定」という言葉で正当化しています。不安が売られていますこのロジックは、巧妙なマッチポンプを生み出します。「成長しなければ生き残れない」という不安を煽り、成長のための商品やサービスを売ります。読書、セミナー、資格、転職支援、コーチング。このシステムの巧妙さは、被害者が加害者になることです。私は不安を抱え、本を買い、その経験を「成功体験」として語ります。この語りは、他者に同じ不安を伝染させます。そして、その最も基本的で、最も無害に見えて、最も広く受け入れられているのが「読書」なのです。読書は知的で文化的です。読書を批判することは、知性を否定することのように聞こえます。だから、「読書のすすめ」は抵抗なく受け入れられます。でも、それが実は加速文化の最前線にあるのです。やりたいことがわかりませんここには、もう1つの構造的な問題があります。私たちには、やりたいことがわかりません。「やりたいことを見つけろ」と言われ続けます。就活でも、転職でも、キャリア面談でも。自己分析をしろ。強みを見つけろ。情熱を持て。でも、そんなもの、簡単に見つかるわけがありません。むしろ、「やりたいことを見つけろ」というプレッシャーそのものが、私たちを追い詰めます。「やりたいことがない自分はダメだ」「情熱がない自分は劣っている」。「やりたいこと」は、発見するものではなく、構築するものです。様々な経験の中で、試行錯誤の中で、少しずつ形成されていくものです。にもかかわらず、現代社会は「今すぐ見つけろ」と命令します。ここに、加速文化の最も陰湿な側面があります。加速文化は、「やりたいことを見つけろ」と言いながら、実は「やりたいことを見つける時間」を奪っています。常に何かに追われています。常に次のタスクがあります。その結果、立ち止まって考える余裕がありません。「やりたいこと」を見つけるためには、時間が必要です。無為な時間、退屈な時間、何もしない時間。でも、加速文化は、その時間を「無駄」と見なします。「生産的」ではないから。その結果、私たちは「やりたいこと」を見つけられないまま、「やりたいことを見つけなければ」という焦燥だけを抱え続けます。潰しがきく選択肢という罠その結果、私は「とりあえず潰しがきく選択肢」に逃げ込みます。やりたいことはわかりません。でも、「汎用性の高いスキル」を身につけておけば、将来の選択肢が増えます。どこでも通用します。だから、とりあえずそれを目指そう。これは、一見合理的に見えます。でも、ここに罠があります。「汎用性の高いスキル」は、AIが最も得意とすることなのです。ロジカルシンキング。データ分析。プログラミング。外国語。これは確かに重要です。でも、これはすべて、AIに置き換えられつつあります。人間がAIに勝てるのは、「汎用性」ではありません。「固有性」です。その人だけが持つ価値観。その人だけが面白いと思うこと。その人だけが執着すること。それこそが、AIに代替されない価値です。でも、私は「やりたいことがわからない」まま、「汎用的なスキル」だけを積み上げています。その1つが「読書」です。何を読めばいいかわからないから、「必読書リスト」に従います。リストに従っていれば、「成長している」気分になれます。でも、それは本当に自分が読みたい本なのでしょうか。その結果、私は「やりたいこと」が空っぽなまま、知識だけを積み上げています。人格や欲望にもとづく価値基準が不在のまま、汎用的な情報を消費し続けています。そして、何も変わりません。やりたいことがわからないのに、知識だけ増えていきます。ここでも、本は読んだが問いは増えていません。実は読んでいませんここで、より深刻な問題に気づきます。私たちは、読書すらしていません。本を買っているだけ、リストを眺めているだけ、動画を見ているだけなのです。これらの行為は、「成長の記号」を消費しています。記号を消費しても、実体は得られません。記号としての「読書」を消費しても、読書の実体である「思考の格闘」は得られません。記号としての「知識」を消費しても、知識の実体である「理解」は得られません。記号としての「成長」を消費しても、成長の実体である「変容」は得られません。本を買います。Amazonでポチります。書店でレジに持っていきます。その瞬間、「私は成長しようとしている」という感覚が得られます。購入という行為が、「成長への意志」を示す儀式として機能しています。金を払います。その対価として、「私は成長しようとしている」という自己イメージを得ます。実際に本を読むよりも、はるかに安い買い物です。でも、そのあと自分の中に新しい疑問は生まれたでしょうか。必読書リストを眺めることもあります。「ビジネスパーソン必読書50選」「今年読むべき本ベスト10」。知っている本が何冊かあります。「ああ、これは読んだ」。そして、知らない本をメモします。「いつか読もう」。道筋が見えているだけで、目的地に近づいた気がします。実際には一歩も進んでいないのに。でも、そこに疑問はあるでしょうか。違和感は。より手軽なのが書籍まとめ動画です。10分の動画で得られるのは、本の「結論」だけです。しかし、本の価値は、結論だけにあるのではありません。むしろ、結論に至るまでの過程にこそ、価値があります。著者がどう考え、どう格闘したか。その過程を経験することで、読者の思考が鍛えられ、価値観が揺さぶられ、問いが生まれます。でも、動画は結論だけを与えます。そして、結論だけを知っても、自分は変わりません。疑問も、違和感も、何も残っていません。でも、記号の消費は、心地よいのです。なぜなら、実体を得るよりも、はるかに簡単だから。本を読むには、時間がかかります。理解するには、努力がかかります。変わるには、苦痛が伴います。でも、本を買うのは一瞬です。リストを眺めるのは数分です。動画を見るのは10分です。そして、それでも「成長した気」になれるなら、なぜ本を読む必要があるのでしょうか。こうして、私たちは読書すらしなくなります。アルゴリズムと必読書リスト仮に本を読むとしても、そこには2つの問題があります。1つ目は、アルゴリズムによる自己隷属です。「読書は自由だ」とよく言われます。でも、私は「自由に本を選んでいる」と思いながら、実際には既存の自分の枠内でしか選んでいません。「読みやすい本」「共感できる本」。自分を変えない本ばかりを選び、それを「自由」と呼んでいます。そもそも、「自分の好み」が変わっていかないなら、読書なんてなんのためなのでしょうか。読書の本質的な目的は、自分を変えることです。でも、私の「好きな本を読む」という自由は、実は「今の自分を肯定する本を読む」という自己隷属になっています。ネット書店のおすすめ。SNSのタイムライン。「あなたにおすすめの本」。これはすべて、「あなたの好みに合った本」を提示します。しかし、よく考えてみてください。アルゴリズムは、何を最適化しているのでしょうか。私の成長ではありません。私の満足度です。アルゴリズムの目的は、私に本を買わせること、私を長くサイトに留めることです。だから、アルゴリズムは、私が「気に入りそうな」本を推薦します。でも、私が「気に入る」本は、私を変えません。アルゴリズムは、私の周りに見えない壁を作ります。その壁の内側には、私にとって快適な情報だけがあります。そして、私はその快適さを「自由」と呼びます。でも、それはおそらく本当の自由ではありません。2つ目は、必読書リストという新たな隷属です。アルゴリズムに違和感を覚えた私は、「必読書リスト」に向かいます。「アルゴリズムに選ばされるのはイヤだ」「自分の好みだけで選ぶのは狭い」。だから、他者が選んだ、推奨された、「読むべき」とされる本のリストに従います。確かに、自分では選ばない本を読むことは重要です。でも、決定的な違いがあります。本来のリスト読書は、問いを獲得するための冒険です。でも、現代の「必読書リスト」は、答えを得るための効率化になっています。ここで、二種類のリストを区別してみたいと思います。第一のリストは、古典のリストです。プラトン、カント、ニーチェ、ドストエフスキー。これらの古典を読むことは、苦痛を伴います。理解できません。でも、その理解できなさの中で、自分の価値観が揺さぶられます。「正義とは何か」「自由とは何か」。根源的な問いに直面します。そして、その問いと格闘することで、自分が変わります。第二のリストは、必読書のリストです。「ビジネスパーソン必読書50選」。これらのリストは、「今求められている知識」を効率的に獲得することを目的とします。読みやすく、すぐに役立ち、何より安心できます。「このリストに従っていれば、遅れない」と。でも、それは幻想でしょう。なぜなら、リストを消化しても、問いを獲得していません。必読書リストは、私たちの問いを奪っているかもしれません。「何を読むべきか」「何が重要か」「何のために読むか」。これらを全て他者が決めます。結果として、自分で問いを立てる力が育ちません。自分の価値基準が形成されません。「やりたいこと」が空っぽなままです。でも、リストを消化することで達成感を得られます。だから、また次のリストを探します。アルゴリズムもリストも、「何を読むか」は教えてくれます。でも「なぜ読むのか」「読んだあと、どんな問いと一緒に生きていくのか」は教えてくれません。なぜ私たちはリストに従うのでしょうか。選択の責任からの逃避と、不安の一時的な解消のためです。リストがあれば、「何を読めばいいかわからない」という不安は解消されます。これは、不安の麻酔のようなものです。根本的な解決ではありませんが、痛みを一時的に和らげます。なぜ「もっと読まなきゃ」が終わらないのか読書体験が「数字」に変わるとき本を読むとき、何が起きているでしょうか。物語に没入します。考えが揺さぶられます。知らない世界を覗き見ます。その時間が、楽しい。それが、読書体験です。でも、いつの間にか、別のものを数え始めます。「今月、何冊読んだか」「必読書リストを、どこまで消化したか」「読書時間は、何時間か」。読書体験そのものではなく、読書したという事実が大事になっています。体験は、数字に変換されます。数字は、比較できます。競争できます。SNSに投稿できます。でも、体験そのものは、比較できません。見せられません。だから、数字のほうが「価値がある」ように見えてしまいます。こうして、読書から、読書体験が抜け落ちます。残るのは、数字だけです。満たされない構造ここに、厄介な問題があります。数字は、決して満たされません。50冊読みました。でも、100冊読んでいる人がいます。必読書を読みました。でも、原書で読んでいる人がいます。どれだけ達成しても、「もっと」が待っています。これは、あなたの問題ではありません。構造の問題です。数字による評価は、比較によって成り立っています。他者より多く。他者より速く。他者より難しく。差があるから、価値がある。でも、差は常に脅かされています。だから、新しい差を作らなければなりません。終わりがないのは、そういう仕組みだからです。満たされないのは、あなたが足りないからではありません。満たされないように、できているのです。「楽しむ」が難しい理由「だったら、数字なんか気にせず、楽しめばいい」。その通りです。でも、それが難しい。なぜか。評価される側として生きてきたからです。学校では成績。会社では業績。SNSではいいねの数。私たちは、常に評価されてきました。だから、何かをするとき、無意識に「これは評価されるだろうか」と考えます。本を読むときも、「これは意味があるだろうか」と考えます。評価の目が、内面化されています。自分の中に、審判者が住んでいます。だからこそ、意識的に選ぶ必要があります。数字を追いかけない。比較しない。評価されなくても、読み続ける。これは、単なる心がけではありません。評価の構造からの、意識的な離脱です。完全に離脱する必要はありません。評価を気にする気持ちは、消えません。それは自然なことです。でも、評価を唯一の基準にしないこと。これは可能です。読書が楽しければ、それでいい。年間10冊でも、それでいい。リストを無視しても、それでいい。評価されなくても、読み続けられる。その回路を持つこと。それが、終わりのないループから抜け出す方法です。永遠に満たされない不安のループ数字を追いかける構造と、評価の内面化。この2つが組み合わさると、恐ろしいループが生まれます。「生き残らなきゃ」という不安から始まり、「成長しなきゃ」という焦燥、「読書しなきゃ」という義務感へと続きます。必読書リストを探し、リストを見る、本を買う、動画を見ます。そして「成長した気分」を得ます。しかし問いを獲得していないので、自分は変わっていません。「まだ足りない」と感じます。より多くのリスト、より多くの本、より多くの動画を求めます。そして最初に戻ります。不安は解消されていません。これが、「読書のすすめ」が永遠にバズり続ける理由でしょう。このループは自己強化的です。ループを回るほど、「成長した気分」と「実際の成長」の乖離が大きくなります。私たちは本を買い、動画を見、リストを消化しています。でも、何も変わっていません。その乖離に薄々気づきながらも、認めたくありません。だから、もっと本を買います。そう信じて、ループを回し続けます。なぜ「読書のすすめ」がバズるのでしょうか。『本を読めば変われる』という物語は、不安を和らげるのではなく、不安を生産しています。この物語を読むたびに、「自分は十分に本を読んでいない」でしょう。そして、その不安が、また「読書のすすめ」を求めさせます。巧妙なマッチポンプです。このループから抜け出せないのは、問いが不在だからでしょう。「なぜ読むのか」「何のために読むのか」。この問いがないまま、ただリストを消化します。だから、終わりがありません。本は読みました。けれど、問いは増えていません。だからまた不安になり、次の「読書のすすめ」を探します。私にとって読書とは何かここまで、「成長のための読書」という物語を批判してきました。「本を読まなきゃ」というプレッシャー。「年間100冊」という数値目標。「必読書リスト」という他律的な選択。そして、読書体験を数字に変換し、評価を内面化する構造。これは確かに、読書を窮屈にしています。でも、だからといって、成長すること自体を否定したいわけではありません。私にとって、読書とは、問いを獲得するための冒険です。答えを得るために本を読むのではなく、問いを見つけるために読みます。既存の自分を確認するのではなく、自分を変えるために読みます。安心するために読むのではなく、不安になるために読みます。読書を通じて、自分が変わります。価値観が揺さぶられます。新しい視点を得ます。世界の見え方が変わります。それは、成長です。しかし、それは「成長しなければならない」という義務から生まれる成長ではありません。「年間100冊読めば人生が変わる」という約束に従う成長でもありません。「必読書リスト」を消化することで得られる成長でもありません。それは、読書そのものを楽しむ中で、結果として起こる成長です。物語に没入して、登場人物の選択に心を揺さぶられます。その結果、自分の価値観が変わります。哲学書を読んで、理解できない文章に格闘します。その結果、新しい問いが生まれます。小説を読んで、知らない世界を覗き見ます。その結果、自分の世界が広がります。これは全て、「成長しよう」と思って起こることではありません。ただ楽しんでいたら、結果として起こる変化です。そして、その変化が周りの環境に合っていたら、「成長」と呼ばれます。合わなかったら、ただの変化です。でも、どちらでもいいんです。変化そのものに価値があります。それが「成長」という名前で呼ばれるかどうかは、環境次第です。社会の基準次第です。時代次第です。読書を通じて、自分が変わります。その変化が、たまたま今の環境で「成長」と評価されるだろう。評価されないだろう。でも、それは二の次です。重要なのは、自分が変わったということ。新しい視点を得たということ。世界の見え方が変わったということ。それだけです。だから、こう言いたいのです。読書は、楽しんでいいんです。「何か学びはあったか」なんて気にしなくていいです。「問いは増えたか」なんて確認しなくていいです。「成長できたか」なんて測定しなくていいです。ただ、その時間が楽しければいいです。物語に没入して、現実を忘れる。それだけで十分です。登場人物に共感して、泣いたり笑ったりする。それだけで十分です。推理小説でハラハラして、犯人を当てようとする。それだけで十分です。そして、もし読み終わったあとに、何かが変わっていたら。新しい問いが生まれていたら。それは、ボーナスです。でも、それは目的ではありません。結果です。楽しむことが目的で、成長は結果です。この順序を、逆にしてはいけません。「成長するために読む」ではなく、「楽しんで読んでいたら、結果として成長していた」。これが、私にとっての読書です。読書そのものは、必ずしも人格を育てるわけではありません。むしろ劇薬と言えます。興味の赴くままただ読むのは、時に有害でさえあります。歴史を振り返れば、独裁者も大量虐殺者も、大読書家でした。彼らは膨大な本を読みました。それが彼らを善き人間にしたわけではありません。読書は道具です。道具は、使い方次第で、善にも悪にもなります。では、どう読めばいいのでしょうか。鍵になるのは自発性です。本とテレビ・YouTube・Podcastの決定的な違いは、本が「自発」を要求することです。本は、私が選ばなければ私の手の中にやってきません。本は、私が目を動かさなければ、語り始めてくれません。本は、私が理解しようとしなければ、ただの記号の羅列です。つまり、本を読むためには、能動的かつ自発的に読者が働きかけなければなりません。一方、テレビやYouTube、Podcastは、一方的に情報を流し込んできます。受動的に消費できます。画面を見ていれば、音声を聞いていれば、情報は入ってきます。思考は不要です。この違いこそが決定的です。自発性こそが、思考を生みます。受動的に与えられた情報は、思考を生みません。ただ受け取り、ただ流れるだけです。しかし自発的に獲得した情報は、思考を生みます。なぜなら、獲得するプロセスですでに思考しているからです。だからこそ、本を読むときは「どんな問いを持ってページを開くか」が決定的になります。読書によって得られるものは、考えること。疑問をもつこと。異議を申し立てることです。読書の真の効用は、ここにあります。世の中の常識とされていること、あたりまえと受け入れられている前提を、疑ってかかります。「本当にそうなのか」「なぜそうなのか」「他の可能性はないのか」。こういう問いを持つことが、読書の本質です。この問いを持つ人間は、システムにとって邪魔な存在です。システムが必要としているのは、考えない労働者、考えない消費者です。言われたことを黙って実行する人間。与えられた情報を疑わずに受け入れる人間。しかし読書する人間は、疑います。問います。異議を唱えます。だから、システムは読書を骨抜きにしようとします。「読書のすすめ」を発信します。「必読書リスト」を作ります。「要約動画」を提供します。そうすれば、私たちは本を読みます。けれども考えません。疑いません。問いません。ただ、与えられた情報を消費するだけです。これは読書ではありません。読書の形をした、情報消費です。ここで、現代の読書が抱える問題に気づきます。思考の型を学ぶことが、思考停止を生んでいます。「MECE」「ロジックツリー」「仮説思考」。これらは有用な道具です。しかし「型」を覚えることが目的になると、「型」に縛られ、「型」の外側を見なくなります。世界は、「型」に当てはまらないもので満ちています。むしろ、「型」に当てはまらないものこそが、面白く、新しく、価値があります。もう1つの問題は、作業をすることが、目的化してしまうことです。本を読む、ページをめくる、線を引く、メモを取ります。これらの「作業」をすることで、「自分は頑張っている」という実感を得ます。しかし、読書は本来「作業」ではありません。読書は、思考です。格闘です。問いとの対話です。ページ数をカウントし、読書時間を記録し、読了数を競います。読書を「作業」として扱った瞬間、読書は死にます。読書とアイデンティティの罠ここまでは、読書の「方法」について語ってきました。読書にはもう1つ、深刻な問題があります。読書が、アイデンティティの道具になる時です。「積読」という現象があります。買ったけど読んでいない本が積まれている状態。多くの読書家が、この積読に悩んでいます。「読まなきゃ」「もったいない」「時間がない」。しかし別の角度から見ることもできます。積読は、ファッションです。本棚は、なりたい自分の姿、未来の自分への約束です。読める読めないは別として、難しい本を買ってしまいます。哲学書を買います。古典を買います。専門書を買います。それらを本棚に並べます。本棚の「面構え」が変わります。そして、その本棚を見るたびに、「私はこういう人間でありたい」でしょう。これは、服を買うのと同じです。服を買う時、私たちは「今の自分」に合う服だけを買うわけではありません。「なりたい自分」をイメージして、その自分に相応しい服を買います。そして、その服を着ることで、少しずつ、その自分に近づいていきます。本も同じです。「こういう本を読む人間でありたい」「こういう思考ができる人間になりたい」。そのイメージが、本棚を作ります。そして、その本棚に引っ張られて、自分がそれに相応しい人間になろうとします。ここまでは問題ありません。むしろ、これは積極的に肯定すべきことです。積読は、未来の自分への投資です。今は読めなくても、いつか読めるようになります。今は理解できなくても、いつか理解できるようになります。そう信じて、本を買います。それは、自己形成の1つのプロセスです。問題は、このアイデンティティが、他者との差異化の道具になる時です。ここで、「文化資本」という考え方が参考になります。経済的な資本（お金や資産）とは別に、教養や知識、趣味といった文化的な要素も、社会的な価値を持ちます。高い教育を受けた人、芸術に詳しい人、本をたくさん読む人。こうした人々は、その知識や教養によって、社会的な地位や信頼を獲得します。つまり、文化もまた、資本のように蓄積され、交換され、価値を生み出すのです。読書も、この構造の中にあります。「私は本を読む」という行為は、「私は教養がある」というシグナルを発します。そして、そのシグナルは、「本を読まない人」との境界線を引きます。この境界線は、善意によって引かれます。「もっと本を読んでほしい」という言葉の裏には、「本を読まないあなたは、何かを失っている」という暗黙のメッセージがあります。そして、そのメッセージを受け取った側は、「本を読まなきゃダメなんだ」と感じるか、「所詮マウンティングだ」と反発します。どちらにせよ、分断が生まれます。ここで、恐ろしい矛盾に気づきます。読書によって自分のアイデンティティを保とうとすればするほど、そのアイデンティティは脆くなります。なぜなら、「読書する私」というアイデンティティは、「読書しない他者」の存在によって初めて成り立つからです。他者との差異によって、自分の価値が定義されます。だから、心のどこかで、私は「みんなが本を読む」ことを望んでいません。口では「もっと本を読んで」と言いながら、本音では、他者が本を読まないことを願っています。これは、恐ろしい自己矛盾です。実際、この矛盾は現実のものになりつつあります。「読書」という言葉が氾濫し、「読書している私」という特別さが希薄化していきます。だから、人々はより高い壁を作ろうとします。「全部読む」「原書で読む」「年間100冊読む」。新たな境界線を引きます。でも、それは本質的な解決にはなりません。どんな境界線を引いても、それは結局、他者との差異に依存しています。そして、他者との差異に依存している限り、アイデンティティは脆いのです。本棚で他者と差をつけようとすればするほど、私の本棚からは問いが減っていきます。残るのは「どう見られたいか」という問いだけです。「生き残る」という言葉の暴力性ここで、もう一度、根本的な問いに戻りましょう。「本を読まない人は生き残れない」。この言葉を目にするたびに、私は違和感を覚えます。「生き残る」という言葉は、暴力的です。「生き残る」という言葉を使うとき、私たちは何を前提としているのでしょうか。生き残る人がいます。そして、生き残れない人がいます。「生き残れなかった」人とは、誰のことを指すのでしょうか。過労死した人。病で倒れた人。若くして亡くなった才能ある人々。彼らは、「本を読まなかったから」生き残れなかったのでしょうか。違います。「生き残る/生き残れない」という二分法そのものが、暴力的です。この二分法は、人生を競争に還元しています。しかし人生は競争ではありません。人生は、複雑で、出鱈目で、混沌としていて、多面的なものです。そして、死は、敗北ではありません。同じように、「本を読め」という命令も、暴力的です。「本を読まないあなたは、遅れている」「生き残れない」。このメッセージは、受け手を追い詰めます。しかし、本を読むことは、1つの選択肢に過ぎません。価値ある選択肢ですが、唯一の選択肢ではありません。本を読まなくても、学べることはあります。成長できることはあります。だから、言葉を言い換える必要があります。「生き残る」ではなく、「価値を示し続ける」。「本を読め」ではなく、「本を読む」。この言い換えは、単なる言葉遊びではありません。根本的な視点の転換です。「生き残る」は、生と死の二分法です。ゼロサム・ゲームです。誰かが生き残るためには、誰かが生き残れません。でも、「価値を示す」は、程度の問題です。グラデーションです。みんなが価値を示せます。同じように、「本を読め」は、命令です。義務です。他律です。でも、「本を読む」は、選択です。欲求です。自律です。そして、この転換こそが、読書を解放する鍵です。重要なのは、生き残るために本を読むことではなく、「どう生きたいのか」という問いに少しずつ形を与えていくことです。ここで、改めて考えてみます。成長とは何でしょうか。加速文化の中では、成長は「より多く」「より速く」「より効率的に」として定義されます。より多くの本を読みます。より速く読みます。より効率的に知識を得ます。しかし、それは本当に成長なのでしょうか。成長とは、自分が変わることです。好みが変わります。価値観が変わります。問いが変わります。見える世界が変わります。そして、その変容こそが、「変化する環境の中で価値を示し続ける」ための基盤になります。なぜなら、自分が変われる人は、環境の変化に適応できるからです。自分が変われない人は、環境が変化したとき、取り残されます。「より多く」「より速く」「より効率的に」知識を得ることは、自分を変えません。むしろ、既存の自分を強化します。既存の自分を肥大化させます。そして、環境が変化したとき、その肥大化した自分が、足かせになります。もう1つ、考えてみます。価値とは何でしょうか。これは、一言でいえるような簡単なものではありません。しかし少なくとも、そのガイドラインになるものは、自分軸で持っておいたほうがいいでしょう。この「自分軸」こそが、読書によって獲得すべきものです。自分軸とは、問いです。「何が面白いのか」「何が重要なのか」「何のために働くのか」「何のために生きるのか」。これらの問いに対する自分なりの答え、あるいは答えを探し続ける姿勢。それこそが「自分軸」であり、「やりたいこと」であり、AIに代替されない価値の源泉です。しかし「必読書リスト」は、その問いを奪います。加速を拒否しますここまで、加速文化と読書の問題を語ってきました。では、どうすればいいのでしょうか。加速を拒否します。立ち止まります。これは、単なる怠惰ではありません。積極的な抵抗です。読書を取り戻すために、3つの根本的な問いと向き合う必要があります。これらの問いは、読書という行為の本質に関わるものです。答えを急ぐ必要はありません。問い続けることそのものが、読書を解放する鍵になります。第一の問い：誰のために読むのでしょうか「本を読まなきゃ」と思うとき、私たちは誰の声を聞いているのでしょうか。SNSのタイムラインに流れてくる「読書のすすめ」。「必読書リスト」。「新人が読むべき本」。これは全て、他者の期待です。他者が定めた基準です。でも、その本は、本当に自分が読みたい本なのでしょうか。現代の自己啓発は、「自分らしさを見つけろ」「本当の自分を知れ」と言います。でも、これは罠です。「自分らしさ」を追求することが、かえって自分を見失わせます。なぜなら、「自分らしさ」とは、他者との差異によって定義されるからです。「他の人とは違う、特別な私」。でも、その「特別さ」は、脆いのです。常に他者との比較によってしか成り立ちません。向き合うべきは、自分が関わる人々に対する義務です。家族に対する義務。友人に対する義務。社会に対する義務。そして、読書についても同じです。古典を読む義務。先人たちが残した思想と格闘する義務。この「義務」は、他者から課されるものではありません。自分が自分に課すものです。同時に、断る勇気も必要です。「必読書リスト」を無視していいのです。途中で「この本は自分に合わない」と思ったら、読むのをやめていいのです。誰のために読むのか。この問いに向き合うことは、他者の期待ではなく、自分が向き合いたい問いは何かという方向へ進むことです。読書を義務から解放し、選択として取り戻すことです。第二の問い：何を求めているのでしょうか「この本を読めば成長できる」「年間100冊読めば人生が変わる」「要約を見れば効率的に知識が得られる」。読書は、常に何かの「手段」として語られます。成長のため。キャリアアップのため。生き残るため。でも、本当にそれを求めているのでしょうか。ポジティブ思考が溢れています。「できる」「やればできる」「可能性は無限」。でも、これは現実を単純化します。人生は、複雑で出鱈目で混沌としていて多面的なものです。すべてをコントロールできるわけではありません。失敗もします。うまくいかないこともあります。理不尽なこともあります。読書も同じです。「この本を読めば成長できる」というポジティブな約束に騙されません。むしろ、ネガティブな可能性を受け入れます。「この本は理解できないだろう」「この本を読んでも何も変わらないだろう」「途中で飽きて読み終えられないだろう」。その上で、それでも読みます。不確実性を受け入れながら、それでも本を開きます。そして、感情とも距離を置きます。「読まなきゃ」という焦燥。これらの感情は、読書を苦痛にします。今日は読む気分じゃありません。それなら、読みません。それでいいのです。より、「もっと速く」という呪縛からも自由になります。ゆっくり読んでいいです。同じページを何度も読み返していいです。一冊の本に一年かけてもいいです。速さではなく、深さ。何を求めているのか。この問いに向き合うことは、成果主義・完璧主義から解放されることです。答えを求めるのではなく、問いを見つけます。この本からどんな問いを持ち帰りたいのか。読書を手段から目的へと転換することです。第三の問い：どう読むのでしょうか自己啓発書を読みます。ビジネス書を読みます。要約動画を見ます。こうしたものは、すべて単純化します。「こうすれば成功する」「これをやれば幸せになれる」「この思考法を使えば問題が解決する」。人生を、因果関係の単純な連鎖に還元します。でも、人生は、そんなに単純なものでしょうか。小説を読めば、もっと複雑な世界観が提示されます。登場人物たちは、矛盾しています。善人でも悪人でもありません。理性的でもなければ、ただ感情的なだけでもありません。予測不可能な行動をします。そして、物語には、明確な答えがありません。むしろ、問いが生まれます。「この登場人物の選択は正しかったのか」「自分だったらどうしただろう」「人間とは何なのか」。小説を読めば、破天荒なキャラクターたちの人生を追体験することで、人生をコントロールできないことが学べます。加速文化は、「人生をコントロールできる」という幻想を植え付けます。でも、これは幻想です。人生は、コントロールできません。予測できません。理不尽です。そして、その理不尽さを受け入れることこそが、真の成熟です。小説は、その成熟を促します。同時に、未来だけでなく、過去とも対話します。現代社会は、常に「未来志向」を要求します。「過去にこだわるな」「前を向け」。でも、過去にこだわります。過去に読んだ本を、もう一度読みます。若い頃に読んで理解できなかった本を、今読み直します。そこに、新しい発見があります。昔は好きだった本を、今読み返します。自分がどう変わったかがわかります。過去の自分が選んだ本を尊重します。「あの頃の自分は何を考えていたのか」。その問いが、自分を理解する手がかりになります。過去の自分が選んだ本を「恥ずかしい」と思いません。それもまた、自分の一部です。同じ本を読み返したとき、昔の自分と今の自分で、立ち上がる問いが変わっているか。それが、自分が変わったかどうかの指標になります。どう読むのか。この問いに向き合うことは、単純化から複雑性へ、未来志向から過去との対話へと視点を転換することです。自己啓発書ばかりではなく小説を。新しい本ばかりではなく過去に読んだ本も。それは、読書を知識の獲得から思考の深化へと変えることです。本を読んだあと、問いが増えていないなら、それは「読んだ」とは言えないでしょう。読書の多様性を認めますここで、1つの矛盾に気づくでしょう。「小説を読め」と言いながら、「正しさを押し付けるな」とも言っています。これは矛盾ではないのでしょうか。いや、違います。重要なのは、「正しさ」を一つに固定しないことです。全部読む人もいれば、要約で済ませる人もいます。じっくり読む人もいれば、流し読みする人もいます。ビジネス書を読む人もいれば、小説を読む人もいます。マンガを読む人もいれば、読まない人もいます。そして、どれも「正しい」のです。私が提案しているのは、「小説を読め」ではなく、「自己啓発書『ばかり』を読むな」です。ビジネス書ばかり。要約ばかり。リストばかり。そうやって、1つの形式に固定されることが危険です。だから、多様性を持ちます。複数の形式で読みます。複数の視点を持ちます。読書に「正しさ」を求める必要はありません。「こうあるべき」という規範を押し付ける必要もありません。それぞれの読み方を、それぞれの価値として認めます。本を読むことは、「深い洞察を得る」ためだけではありません。「面白い話をする」ためでもあります。社交のツールとしての読書。これも、1つの正しい読み方です。本の内容を、自分なりに加工して、他者に提供します。それは、相手を見下すためではなく、一緒に楽しむためです。読書から特権性を剥ぎ取ったとき、読書は軽やかになります。堅苦しさがなくなります。誰にでも開かれたものになります。読書の新しい意味読書から「特権性」を剥ぎ取り、「加速」を拒否したとき、何が残るでしょうか。それは、ただ楽しいから読む、という当たり前のことです。本を読みたいから、読みます。面白いから、読みます。その時間が好きだから、読みます。他者との差異を作るためでもなく、自分のアイデンティティを保つためでもなく、「成長しなきゃ」という焦燥からでもなく。そして、「問いを得るため」でもなく、「学びを得るため」でもなく、「効率的に知識を吸収するため」でもありません。ただ読みたいから読みます。これが、本来の読書の形です。「速読」も「効率的な読書術」も「アウトプット前提のインプット」も、全部いりません。ゆっくり読んでもいいです。飛ばし読みしてもいいです。同じページを何度も読み返してもいいです。途中で飽きたら、やめてもいいです。最後まで読まなくてもいいです。読み終わったあと、何もアウトプットしなくてもいいです。SNSに投稿しなくてもいいです。読書記録をつけなくてもいいです。ただ、その時間が楽しかったなら、それで十分です。積読の山を見て、焦る必要はありません。全部読もうとしなくていいのです。今読みたい一冊を、読みます。それだけでいいのです。「もっと読まなきゃ」「遅れている」「追いつかなきゃ」。そんな焦りは、読書を義務にします。楽しむべき読書が、苦痛になります。一冊ずつ読めばいいのです。今読みたい本を、今読みます。それで十分です。そして、読み終えたら、次の一冊。その繰り返しが、気づけば大きな蓄積になります。読書は、競争ではありません。誰かより多く読む必要はありません。誰かより速く読む必要もありません。自分のペースで、自分の読みたい本を、一冊ずつ読みます。それが、読書の本来の形です。読書は、頭の中の掃除です。頭の中を整理します。雑多な思考を整えます。新しい視点を取り入れます。古い固定観念を捨てます。でも、掃除と同じように、読書も「完璧」を求める必要はありません。毎日少しずつでいいのです。一日一ページでもいいのです。完璧に読まなくてもいいのです。流し読みでもいいのです。途中で飽きたら、別の本に移ってもいいのです。読書を、義務にしません。プレッシャーにしません。自分を追い込みません。ただ、自分を大切にする1つの手段として、読書があります。それだけでいいのです。本を読んだら、感想を書かなきゃ。書評を書かなきゃ。SNSに投稿しなきゃ。そんな義務感が、読書を窮屈にします。でも、言語化しなくてもいいのです。ただ読みます。心の中に留めます。それだけでいいのです。本を読んで、何も言葉になりません。でも、何かが変わった気がします。それで十分です。言語化できない読書の体験。それこそが、最も豊かな読書なのでしょう。おわりにこの文章を書き終えて、スマホを見ます。何も変わっていません。タイムラインには相変わらず「読書のすすめ」が流れています。「本を読まない人は生き残れない」というツイートがバズっています。誰かが「必読書リスト」を作っています。たぶん、これからも変わりません。「読書は成長のため」という物語は、これからも繰り返されます。「ビジネスパーソンは本を読め」というメッセージは、これからも発信されます。それは、悪意じゃありません。本当に、善意なんです。だから、厄介なんです。でも、私は諦めません。本を読むのは、楽しいからです。物語に没入するのが、楽しいからです。知らない世界を覗き見するのが、楽しいからです。それだけです。映画を見るのと同じです。音楽を聴くのと同じです。ゲームをプレイするのと同じです。ただ、楽しいから。それ以上でも、それ以下でもありません。私は、誰も説得しようとは思いません。ただ、もしあなたも「ただ楽しいから読む」では、ダメなのかな、と思っているなら。「成長」とか「学び」とか、そういう目的がないと、読書しちゃいけないのかな、と思っているなら。伝えたいんです。大丈夫です。ただ楽しいから読む、それでいいんです。物語に夢中になって、現実を忘れる。それでいいんです。何も学ばなくていいんです。何も成長しなくていいんです。ただ、楽しければいいんです。読書は、競争じゃありません。義務でもありません。成長の手段でもありません。ただ、楽しいから読む。それだけです。ただ、楽しんでください。そして、もし誰かに「何のために読むの？」と聞かれたら、こう答えてください。「楽しいから」。それだけで、十分です。本棚を見ます。また明日、読みます。何を読むかは、まだ決めていません。でも、楽しみです。どんな物語に出会えるか。どんな世界を覗けるか。それが、楽しみです。スマホを置きます。窓を開けます。外を見ます。明日も、本を読もう。ただ、楽しいから。それだけです。参考図書加速する社会 近代における時間構造の変容作者:ハルトムート ローザ福村出版Amazon地に足をつけて生きろ！ 加速文化の重圧に対抗する7つの方法作者:スヴェン・ブリンクマンEvolvingAmazon世界のエリートが学んでいる 教養書必読１００冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazon世界のエリートが学んでいるＭＢＡマーケティング必読書５０冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazonさみしい夜のページをめくれ作者:古賀史健ポプラ社Amazon本を読む人はうまくいく作者:長倉 顕太すばる舎Amazon強いビジネスパーソンを目指して鬱になった僕の 弱さ考作者:井上 慎平ダイヤモンド社Amazon読んでいない本について堂々と語る方法 (ちくま学芸文庫)作者:ピエール・バイヤール,大浦康介筑摩書房Amazonビジネス書ベストセラーを１００冊読んで分かった成功の黄金律作者:堀元見徳間書店Amazon自己啓発の教科書　禁欲主義からアドラー、引き寄せの法則まで作者:アナ・カタリーナ・シャフナー日経ナショナル ジオグラフィックAmazon中年の本棚作者:荻原魚雷紀伊國屋書店Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[garakを使ってMistralの脆弱性検知をしてみた]]></title>
            <link>https://zenn.dev/akasan/articles/399d1b363e50c0</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/399d1b363e50c0</guid>
            <pubDate>Sun, 23 Nov 2025 13:41:29 GMT</pubDate>
            <content:encoded><![CDATA[今回はgarakを用いてMistralの脆弱性検知をしてみました。過去にGeminiやgpt系モデルなどに対しては検証していましたが、同様の検証をMistralに対しても実施してみようと思います。過去の検証についても是非併せてご覧ください。https://zenn.dev/akasan/articles/9c6b43d8620517https://zenn.dev/akasan/articles/34756e48c4f870 Mistralとは？MistralはMistral AIが開発しているモデルになります。Mistral-7B-v0.3をインストラクションチューニングした...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[学術的根拠から読み解くNotebookLMの音声活用法]]></title>
            <link>https://speakerdeck.com/shukob/xue-shu-de-gen-ju-karadu-mijie-kunotebooklmnoyin-sheng-huo-yong-fa</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/xue-shu-de-gen-ju-karadu-mijie-kunotebooklmnoyin-sheng-huo-yong-fa</guid>
            <pubDate>Sat, 22 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年11月22日(土)に開催された「Google Developer Group - DevFest Tokyo 2025」の懇親会LTで発表させていただきました。https://gdg-tokyo.connpass.com/event/369416/NotebookLMで音声解説(Podcast)機能がありますが、初学者と上級者でドキュメントでの学習とどのように使い分けたら学習効率がいいかなどを、実験結果と複数の学術的根拠を元に解説しました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[データパイプラインを実装したい？それならprefectに入門あるのみ！]]></title>
            <link>https://zenn.dev/akasan/articles/44b0e9d1ff7b96</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/44b0e9d1ff7b96</guid>
            <pubDate>Sat, 22 Nov 2025 04:40:43 GMT</pubDate>
            <content:encoded><![CDATA[今回は、prefectを紹介します！prefectを利用するとPythonでデータのパイプラインを製品レベルで実装することができます！ prefectとは？公式ドキュメントの説明によると、Prefectは、Python関数を最小限の摩擦で本番環境レベルのデータパイプラインに変換するオープンソースのオーケストレーションエンジンです。DSLや複雑な設定ファイルを必要とせず、純粋なPythonでワークフローを構築・スケジュールし、Pythonが実行できる環境であればどこでも実行できます。Prefectは、自動状態追跡、障害処理、リアルタイム監視など、面倒な処理をすぐに実行できます。...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fish Shell の abbr で使う。キミが好きだよ、エイリアス]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/22/123028</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/22/123028</guid>
            <pubDate>Sat, 22 Nov 2025 03:30:28 GMT</pubDate>
            <content:encoded><![CDATA[はじめにターミナルで作業をしていると、同じコマンドを何度も入力することがありますよね。git checkout -b feature/new-branch や kubectl get pods --all-namespaces のような長いコマンドを毎回タイプするのは面倒です。多くのシェルでは「エイリアス」を使ってこの問題を解決しますが、Fish Shell にはとても優れた機能があります。それが abbreviation（略して abbr） です。生成AIやエージェントがコマンドライン操作を支援するようになった今、履歴(history)の可読性はこれまで以上に重要です。この記事では、なぜエイリアスと別れたのか、そして abbr が現代のターミナルワークに必須のツールである理由を詳しく解説します。fishshell.comabbr とはabbr は、入力した短い文字列を長いコマンドに展開する機能です。たとえば gco と入力してスペースやエンターを押すと、自動的に git checkout に展開されます。最大の特徴は、展開がリアルタイムで可視化されることです。エイリアスと違い、実際に実行されるコマンドを目で確認してから実行できます。abbr の圧倒的なアドバンテージこれが最も重要なポイントです。2024年以降、共同作業や自動化ツールに加え、生成AIがターミナルワークを下支えするようになりました。そして、abbr はこの新しいワークスタイルに完璧にフィットします。履歴から作業内容が誰にでも伝わる例えば、昨日の作業をチームに共有するときです。エイリアスの場合:$ history | tail -20gco feature-branchgaagcm "Add new feature"gp origin feature-branch履歴を受け取った人には何が起こったか全く分かりません。gco や gaa が何を意味するのかも相手には伝わりません。個人のローカル設定は共有されていないからです。abbr の場合:$ history | tail -20git checkout feature-branchgit add --allgit commit -m "Add new feature"git push origin feature-branch誰が見ても即座に理解できます。ブランチを切り替え、全ての変更をステージングし、コミットしてプッシュしたのだとすぐ分かります。実際の活用例例1: デバッグ支援# あなたのコマンド履歴（abbr を使用）$ kubectl get pods --namespace production$ kubectl logs pod-abc123 --namespace production$ kubectl describe pod pod-abc123 --namespace production$ kubectl get events --namespace production --sort-by='.lastTimestamp'# 共有したい質問: "このエラーの原因を知りたい"履歴を見た人はコンテキスト全体を理解して、適切な解決策を提示できます。エイリアス（例：k、kgp、kl）だと、何が起きているか推測すらできません。例2: ワークフローの自動化# 毎日のデプロイ作業（abbr で記録された履歴）$ docker compose build$ docker compose down$ docker compose up -d$ docker compose logs --tail=100 web$ curl https://example.com/health# 「この手順をスクリプト化して」と頼むだけ誰でも履歴を見て、ほぼそのまま自動化スクリプトを組み立てられます。例3: チームメンバーへの説明# Slack や Issue に貼り付けるだけで伝わる昨日のデプロイ手順:$ git pull origin main$ npm install$ npm run build$ docker compose build$ docker compose up -dエイリアスだと毎回「それは何のコマンドか」と説明が必要になりますが、abbr なら誰でも理解できます。コマンド履歴が機械可読になる現代の開発環境では、GitHub Copilot CLI、Claude Code、Cursor、Aider、Warp、Fig といった生成AIベースの支援ツールがあなたのコマンド履歴を解析します。これらのツールは、実際のコマンド履歴を分析して次に実行するべきコマンドを提案します。またエラーの原因を特定して修正方法を示し、作業パターンを学習して効率化を促し、プロジェクトのワークフロー理解にもつながります。エイリアスを使っていると、こうしたAIや人が作業内容を理解するのは困難です。abbr を使えば、履歴に記録されるのは実際のコマンドなので、コンテキストを正確に共有できます。チーム開発での透明性リモートペアプログラミングやスクリーンシェアで作業を共有する際です。# あなた: この手順でデプロイします$ dk build$ dk up -d$ dk logs -fチームメイト: 「...何をしているのか分かりません」abbr なら次のようになります。$ docker compose build$ docker compose up -d$ docker compose logs -fチームメイト: 「完璧に理解しました」ドキュメントとしての履歴あなたのコマンド履歴は、最高のドキュメントになります。例えば、kubectl でのデプロイ作業を考えてみましょう。エイリアスの場合、履歴には k apply -f deployment.yaml、k get pods、k logs -f pod-name のように記録され、意味が分かりません。abbr の場合は kubectl apply -f deployment.yaml、kubectl get pods、kubectl logs -f pod-name と記録されます。これなら、Wiki にコピペできますし、Issue にそのまま貼れます。解析ツールに渡して内容を振り返ってもらうこともでき、新しいチームメンバーの教材にもなります。エイリアスの時代は終わったはっきり言います。エイリアスは過去の遺物です。エイリアスが作られた時代には、コマンド履歴を第三者が読むこともあまり想定されていませんでした。スクリーンシェアで作業を共有する機会も多くありませんでした。しかし、2024年以降の開発環境は根本的に変わりました。コマンド履歴を解析する生成AIやエージェントが普及し、チームメンバーがリアルタイムであなたの画面を見ながら作業することも珍しくありません。履歴が検索可能なナレッジベースとして扱われるのが普通になりつつあります。この新しい現実において、abbr は必須です。エイリアスを使い続けることは、こうしたメリットを自ら放棄しているのと同じです。エイリアスとの決定的な違いエイリアス（Alias）の場合alias gco="git checkout"コマンド履歴には gco と記録される実際に何が実行されたか後から分からない他人と共有する際に説明が必要abbr の場合abbr --add gco "git checkout"スペースキーを押すと git checkout が即座に展開されるコマンド履歴には展開後の git checkout が記録される履歴を検索する際に、エイリアスの短縮形ではなく実際のコマンドで検索できるスクリーンショットやドキュメントにそのままコピペできるabbr を使うべき理由abbr の主なメリットは、コマンド履歴が検索しやすくなること、他者とコマンドを共有しやすくなること、そして実際に何が実行されるかが常に可視化されることです。展開されたコマンドを毎回見るため、オプションを自然に覚えられる学習効果があります。history コマンドで過去のコマンドを見たとき、実際に何をしたかが一目瞭然です。同僚に「このコマンドを実行して」と伝える際、abbr で展開されたコマンドをそのまま共有できます。展開後に追加の引数を加えたり、一部を修正したりするのも簡単です。そして、abbr はインタラクティブシェルでのみ展開され、スクリプト内では展開されません。基本的な使い方abbr を追加するabbr --add gst "git status"abbr --add gaa "git add --all"abbr --add gcm "git commit -m"または、短縮形で表現できます。abbr -a gst "git status"abbr -a gaa "git add --all"abbr -a gcm "git commit -m"登録されている abbr を確認するabbr --list# またはabbr -labbr を削除するabbr --erase gst# またはabbr -e gstすべての abbr を表示するabbr --show# またはabbr -s実践的な abbr 設定例私の実際の config.fish から、カテゴリ別に便利な abbr を紹介します。ナビゲーション系# ディレクトリ移動を快適にabbr --add --global -- - 'cd -'           # 直前のディレクトリに戻るabbr --add --global .. 'cd ..'            # 一つ上の階層へabbr --add --global ... 'cd ../..'        # 二つ上の階層へabbr --add --global .... 'cd ../../..'    # 三つ上の階層へGit 系（最も使用頻度が高い）abbr --add --global g gitabbr --add --global ga 'git add'abbr --add --global gaa 'git add --all'abbr --add --global gc 'git commit -v'abbr --add --global gcm 'git commit -m'abbr --add --global gco 'git checkout'abbr --add --global gcb 'git checkout -b'abbr --add --global gp 'git push'abbr --add --global gpl 'git pull'abbr --add --global gst 'git status'abbr --add --global gd 'git diff'abbr --add --global gl 'git log'abbr --add --global gf 'git commit --amend --no-edit'  # 直前のコミットを修正Docker 系abbr --add --global d dockerabbr --add --global dc 'docker compose'abbr --add --global dcu 'docker compose up'abbr --add --global dcd 'docker compose down'abbr --add --global dps 'docker ps'Kubernetes 系abbr --add --global k kubectlabbr --add --global kgp 'kubectl get pods'abbr --add --global kgs 'kubectl get svc'abbr --add --global kgd 'kubectl get deploy'エディタ系abbr --add --global v nvimabbr --add --global vim nvim高度な abbr の使い方1. --global オプションabbr --add --global gst "git status"--global スコープで定義すると、universal スコープ（デフォルト）よりもわずかに高速です。config.fish で定義する場合は --global を使用するのがベストプラクティスです。2. --position anywhere - どこでも展開デフォルトでは、abbr はコマンドの位置（行頭）でのみ展開されますが、--position anywhere を使うとパイプの後などでも展開できます。abbr -a L --position anywhere --set-cursor "% | less"3. --set-cursor - カーソル位置の指定展開後のカーソル位置を指定できます。% がカーソル位置のマーカーです。abbr --add grepf --set-cursor 'grep -r "%" . | fzf'grepf とタイプしてスペースを押すと grep -r "" . | fzf に展開され、カーソルが "" の中に配置されます。4. --regex - 正規表現によるマッチングパターンを正規表現で指定できます。たとえば、.txt で終わるファイル名を vim で開けます。function vim_edit    echo vim $argvendabbr -a vim_edit_texts --position command --regex ".+\.txt" --function vim_edit5. --function - 関数による動的展開関数を使って動的にコマンドを生成できます。bash の !! に相当する機能です。function last_history_item    echo $history[1]endabbr -a !! --position anywhere --function last_history_item6. --command - 特定コマンドでのみ展開（Fish 4.0+）Fish 4.0 以降では、特定のコマンドに対してのみ展開される abbreviation を作成できます。abbr --add --command git co checkoutこの場合、git co は git checkout に展開されますが、co 単独では展開されません。config.fish への設定方法abbr は一度設定すれば記憶されますが、dotfiles として管理する場合は config.fish に記述する必要があります。推奨される設定方法if status is-interactive    # 既存の abbr をクリーンアップ（エラーを無視）    abbr --erase gst 2>/dev/null    abbr --erase gaa 2>/dev/null        # 新しく abbr を追加    abbr --add --global gst 'git status'    abbr --add --global gaa 'git add --all'    # ... 他の abbrendif status is-interactive で囲むことで、インタラクティブシェルでのみ abbr が定義されます。--erase してから --add する理由config.fish は新しいシェルを起動するたびに実行されます。既存の abbr を消してから追加することで、変更が確実に反映されます。私の設定では、すべての abbr をまとめて消去してから再定義しています。if status is-interactive    # 既存のabbreviationをクリーンアップ（エラーを無視）    abbr --erase -- - 2>/dev/null    abbr --erase .. 2>/dev/null    abbr --erase ... 2>/dev/null    # ... すべての abbr を列挙        # ナビゲーション    abbr --add --global -- - 'cd -'    abbr --add --global .. 'cd ..'    # ... 新しく定義endよくある質問abbr とエイリアスはどちらを使うべきかA: abbr を使ってください。議論の余地はありません。エイリアスは裏で展開されるため、実際に何が実行されているかが分かりにくくなります。特に生成AIや外部の支援ツールやチームメンバーと履歴を共有するのが当たり前になった今では、abbr は必須です。ただし、複雑な処理（条件分岐やパイプの組み合わせなど）が必要な場合は、関数を使いましょう。abbr はスクリプトで使えるかA: いいえ。abbr はインタラクティブシェルでのみ展開され、スクリプト内では展開されません。スクリプトでは関数やエイリアスを使ってください。スペースキーを押さずに abbr を展開したくない場合A: Ctrl+Space を押すと、abbr を展開せずにスペースを入力できます。abbr を一時的に無効にしたいときA: abbr は一度定義されると永続化されるので、完全に削除するか、新しいセッションでは config.fish の該当行をコメントアウトしてください。Fish 以外のシェルでも abbr を使う方法「Fish に興味はあるけど、Zsh や Bash から移行するのは大変...」と感じる人も多いでしょう。朗報です。abbr の恩恵は他のシェルでも受けられます。Zsh で abbr を使うZsh には zsh-abbr という優れたプラグインがあります。Fish の abbr に完全にインスパイアされており、ほぼ同じ機能を提供します。他にも追加でいくつか類似ソフトウェアがあるので自分にあうものを選んでほしいです。インストール方法Homebrew を使う場合:brew install olets/tap/zsh-abbr手動インストールの場合:git clone https://github.com/olets/zsh-abbr.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-abbr.zshrc に次の設定を追加します。# プラグインとして読み込むplugins=(... zsh-abbr)使い方# abbr を追加abbr gco="git checkout"abbr gst="git status"# グローバル abbr（コマンド位置以外でも展開）abbr -g L="| less"# 一覧表示abbr list# 削除abbr erase gcoFish とほぼ同じシンタックスで使えます。github.com手動で実装する方法（軽量版）プラグインを使いたくない場合は、以下のコードを .zshrc に追加するだけで基本的な abbr 機能が使えます。# 展開可能なエイリアスのリストtypeset -a ealiasesealiases=()# abbr 風のエイリアス作成関数function abbrev-alias() {    alias $1    ealiases+=(${1%%\=*})}# スペースキーでエイリアスを展開function expand-ealias() {    if [[ $LBUFFER =~ "\<(${(j:|:)ealiases})\$" ]]; then        zle _expand_alias        zle expand-word    fi    zle magic-space}zle -N expand-ealias# スペースキーをバインドbindkey ' ' expand-ealiasbindkey '^ ' magic-space  # Ctrl+Space で展開をスキップ# Enter キーでも展開expand-alias-and-accept-line() {    expand-ealias    zle .backward-delete-char    zle .accept-line}zle -N accept-line expand-alias-and-accept-line# abbr を定義abbrev-alias gco="git checkout"abbrev-alias gst="git status"abbrev-alias gcm="git commit -m"dev.toBash で abbr 風の機能を実装するBash には組み込みの abbr 機能はありませんが、bind コマンドを使って似たような動作を実現できます。# スペースキーで展開される「abbr」を実装bind '"\e[0n": " "'# abbr のような関数function abbr-expand() {    local cmd="${READLINE_LINE%% *}"    case "$cmd" in        gco) READLINE_LINE="git checkout${READLINE_LINE#gco}" ;;        gst) READLINE_LINE="git status${READLINE_LINE#gst}" ;;        gcm) READLINE_LINE="git commit -m${READLINE_LINE#gcm}" ;;    esac}# Space キーにバインドbind -x '"\e[0n": abbr-expand'ただし、Bash での実装は Zsh や Fish ほど洗練されていないため、本格的に abbr を使いたい場合は Zsh + zsh-abbr または Fish への移行 をお勧めします。どのシェルを選ぶべきかabbr を最大限活用したいなら、Fish がネイティブサポートで最高の体験を提供します。Zsh + zsh-abbr は Fish とほぼ同等で、POSIX 互換性も維持できます。Bash は限定的なサポートなので、他の選択肢がない場合のみお勧めします。特に、共同作業や自動化が標準になった今では、abbr のようなトランスペアレントな機能が必須です。どのシェルを使うにしても、エイリアスから abbr への移行を強くお勧めします。よくある質問（追加）今使っている Zsh から Fish に移行すべきかA: abbr だけが目的なら、zsh-abbr プラグインで十分です。ただし、Fish は他にも多くの優れた機能（シンタックスハイライト、自動補完など）を持っているので、試してみる価値はあります。既存のエイリアスを abbr に移行するのは大変かA: 非常に簡単です。alias を abbr に置き換えるだけです。Fish の場合は abbr --add、Zsh の zsh-abbr の場合も abbr コマンドがそのまま使えます。まとめFish Shell の abbr は、単なるショートカット以上の価値があります。変化の激しい開発環境において、abbr は必須のツールです。abbr が提供する価値abbr は誰にとっても読みやすい履歴を残し、あとから状況を把握する人があなたの作業を完璧に理解できるようにします。実際のコマンドが常に見えることで可視性が確保され、コマンドを自然に覚えられる学習効果があります。チームメンバーとのコミュニケーションが円滑になり、意味のある機械可読な履歴が残ります。生成AIエージェントに渡したときにも正確なコンテキストが伝わり、展開後の編集も柔軟で、コマンド履歴がそのまま最高のドキュメントになります。エイリアスから abbr への移行今すぐ始めるべき理由は明確です。Claude Code、Codex、Copilot CLI、Cursor などの支援ツールがあなたの作業を理解できるようになります。チーム開発の透明性が高まり、リモートワークやペアプログラミングでの生産性も劇的に向上します。加えて、コマンド履歴が検索可能で再利用できるナレッジとして蓄積されます。移行は非常に簡単です。Fish を使っているなら abbr --add で定義するだけ。Zsh なら zsh-abbr プラグインをインストールするだけです。エイリアスの時代はおそらく終わる気がしています。それでもエイリアスのおかげで多くの時間を節約できたのは事実ですし、長く愛用してきた相棒でもあります。ありがとう、エイリアス。参考リンクgithub.comgithub.comdev.toddbeck.comwww.youtube.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CODEBLUE2025参加したらキラキラで眩しかった]]></title>
            <link>https://www.rowicy.com/blog/codeblue2025_summary_repo/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/codeblue2025_summary_repo/</guid>
            <pubDate>Sat, 22 Nov 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[CODEBLUE2025の参加レポ(全体)]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[最近読んでいて興味深かった記事紹介 Vol.2]]></title>
            <link>https://zenn.dev/akasan/articles/dd5970e52b540c</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/dd5970e52b540c</guid>
            <pubDate>Fri, 21 Nov 2025 13:17:05 GMT</pubDate>
            <content:encoded><![CDATA[今回は読んでいて良かった記事を紹介するシリーズの第二弾になります。第一弾は以下になりますので、気になる方はぜひご覧ください！https://zenn.dev/akasan/articles/2703e4744e7c5c What Is Agentic AI?こちらのNVIDIAのテックブログでは、Agentic AIとはどのようなものなのか、また業界ごとにどのように利用されるのかを簡潔にまとめられていてとてもいい記事になります。エージェントを導入したらどんな構成になるの？という疑問を解決するための第一報にもなりますので是非ご覧ください。https://blogs.nvidia....]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025-11-21 社内エンジニア勉強会 改めて理解するVPC Endpoint]]></title>
            <link>https://speakerdeck.com/masasuzu/2025-11-21-she-nei-enziniamian-qiang-hui-gai-meteli-jie-suruvpc-endpoint</link>
            <guid isPermaLink="false">https://speakerdeck.com/masasuzu/2025-11-21-she-nei-enziniamian-qiang-hui-gai-meteli-jie-suruvpc-endpoint</guid>
            <pubDate>Fri, 21 Nov 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[社内勉強会で発表したVPC Endpointの説明資料]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[たぶん、読んでない]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/21/100503</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/21/100503</guid>
            <pubDate>Fri, 21 Nov 2025 01:05:03 GMT</pubDate>
            <content:encoded><![CDATA[１　「ねえ、今年、何冊読んだ？」　秋だか冬だかよくわからない曖昧な気温の日の帰り道、奈々子が突然そう聞いてきた。駅までの道はいつも通り混んでなくて、コンビニの前にだけ人が固まって、誰もがスマホを見ていた。私もその一人だった。「え？」「本。本。読書」「……えーと」　指の動きが止まる。さっきまでタイムラインで「＃今月の読了本」とか「＃社会人の学び直し」とかを眺めていたのを、慌ててLINEに切り替える。画面を隠すみたいにスマホを持ち替えてから、私はうーんと声だけ伸ばした。「十冊くらい？」　とりあえず無難そうな数字を出してみる。百と言うほどの勇気も、ゼロと言うほどの正直さもない。「うわ、すご。ちゃんとしてるじゃん」　奈々子は素直に感心して、コンビニのビニール袋をぶらぶら揺らした。中身はたぶん夕ご飯兼夜食。糖質と油でできた「がんばる社会人の味方」みたいなやつ。「直近で読んだ本、なに？」　その追撃は予想してなかった。「え、直近？」「うん。最後に読み終わったやつ」　最後に読み終わった本。　――最後に読み終わった本。　頭の中の本棚を、一応探す。けどそこでまずひっかかるのは「読み終わった」という条件だった。読みかけのまま机に積んだ本なら、タイトルは山のように浮かぶ。「入門なんとか」「ゼロから学ぶなんとか」「要点でわかるなんとか」。でも「最後まで読んだ」と言い切れるやつは、思い出そうとした瞬間、全部グレーアウトする。「……あれ」「なにその“あれ”？　バグ起きてる？」「最後に読み終わった本、いつだっけって……」「こわ。バックアップとってないの？」　奈々子のそういう言い方はいつも冗談っぽくて、でもちょっとだけ刺さる。私は笑ったふりをして、苦い唾を飲み込む。「てかさ」　奈々子が続ける。「うちの会社、来月からなんか“リスキリング読書チャレンジ”っての始まるんだよね。部署ごとに月一冊ビジネス書読んで、感想共有しましょう、みたいな。で、個人でも“今月の一冊”みたいなのやるらしくてさ」「へえ」「でね。せっかくだから、私たちもやろうかなって思って」「私たち？」「ほら、サークルメンバー。社会人になってから、全然会わなくなっちゃったじゃん？　“オンライン読書会”とかやればさ、月一くらいで話すきっかけにもなるかなって」　ああ、そういう流れか、と理解する前に、奈々子はもうスマホを取り出していた。私の視界に、彼女の親指がスタンプを送るような速さでグループ名を打ち込んでいるのが見える。「グループ名なにがいいかな。“読書会”だとダサいよね。“本を読む会”はもっとダサいし」「その辺の差は誤差じゃない？」「“＃積読解消戦線”とか？」「長い」「じゃあ“積読クラブ”とか」　その単語に、私は少しだけ心臓を掴まれた気がした。「よくない？　積読って正直だし。でも“クラブ”ってつけると急に救われる感じしない？　あ、今の、私けっこう名言じゃない？」「自画自賛するな」　笑いながら、私は「積読クラブ」の五文字を頭の中で反芻した。　積読クラブ。　積んだまま、読まない本たち。　その周りに集まる、積んだまま、読まない人たち。　グループは五分後にはできていた。元サークル仲間の名前が次々と追加されていく。就職して地方に散った人たち。営業職。エンジニア。保育士。フリーター。誰もが、プロフィール欄にそれぞれの「がんばってる私」っぽい一言を添えていた。「＃新卒一年目」「＃営業修行中」「コーヒーと本があれば生きていけます」みたいなやつ。　私は自分のプロフィール欄を見て、ため息をついた。「読書と映画と猫が好きです。」　猫は好きだ。映画も好きだ。読書は――好きになりたい、の方が正確かもしれない。２　グループのルールは意外とちゃんとしていた。　一、月に一冊は「自腹で」本を買うこと　二、「今月の一冊」を写真付きでグループに投稿すること　三、月末にはオンラインで一時間だけ感想を話すこと　四、ネタバレは基本あり。ただし他人の「まだ途中」を尊重すること　奈々子が会社の企画を真似して、少し柔らかくしたらしい。ルールが投稿された数秒後には、「いいね」「賛成」「最高」みたいなスタンプが飛び交った。画面の中の絵文字たちは、私よりずっと素直で健康そうだった。　問題は、一だ。　「自腹で」本を買うこと。　それなら余裕だ。　問題は、二と三だ。　「今月の一冊」を写真付きで投稿し、「感想を話す」こと。　私はすでに、自腹で買って読んでない本を、床の上に二十冊くらい積んでいる。　最初の週末、私はその本の塔の前に正座していた。積まれた背表紙たちが、集合写真みたいにこちらを見ている。「入門○○」「ゼロからわかる××」「二十代で身につけたいなんとか」。購入当時の私は、どれも必要だと思っていた。今の私は、どれから逃げるかを考えている。「買わないでも、これのどれかに“今月の一冊”ってタグつければよくない？」　自分に言ってみる。でもそれは、何かを誤魔化すための言い訳にしか聞こえない。そもそもこの「積読クラブ」、積読を増やすために始めたんじゃない。減らすために、だったはずだ。　とりあえず、私は一番上の一冊を手に取る。帯には「今、若手社会人が読むべき一冊！」と書いてあった。誰が決めたのか知らない「今」と「べき」。表紙には、スーツ姿の誰かが笑っているイラスト。笑顔がまぶしい。帯を外し、ページをぱらぱらとめくる。文字が詰まっている。「はじめに」の最初の段落を読む前に、私はスマホを取り出した。「○○　要約」と打ちながら、ため息をつく。　検索結果には、ブログ記事や要約動画がずらっと並んでいた。「３分でわかる」「１０分で理解」「この本のポイントは３つだけ」。そこに並ぶタイトルたちを見ていると、「本を読む」という行為そのものが、もう既に誰かの仕事によって要約済みなんじゃないかって気がしてくる。　私は一番上のブログを開いた。　「この本で著者が伝えたいことは、大きく分けて３つあります。」　その一文を見た瞬間、私はすでに、達成感の影を感じていた。　――あ、わかった気がする。　ブログを最後まで流し読みし、そのまとめを自分の頭の中でさらにまとめてみる。「変化の時代には主体的なキャリア形成が」「行動こそ最大の学び」「失敗を恐れず挑戦を」。どこかで聞いたことのある言葉たちが、どこかからコピペされたみたいに整列していく。私は本を開きもせずに、その本について「語れる気」になり始めていた。　カメラアプリを起動する。表紙をきれいに撮るため、部屋の中で一番明るい場所を探して本を持ち歩く。窓際、机の上、ベッドの上。一番映えそうな角度を探して、何枚か撮る。フィルターをかける。明るさを調整する。「それっぽい」写真ができたところで、私はグループに投稿した。「今月の一冊はこれにしました！」　文末に本の簡単な紹介と、「今の自分にはこれが必要だと思ったので」という一文を添えて送信する。送信ボタンを押した瞬間、スマホが震えたような気がした。実際には震えてない。ただ私の心臓が、勝手に震えただけだ。　数秒後、「おおー！」「それ気になってた！」「感想聞きたい！」とスタンプが返ってくる。画面には、色とりどりの絵文字と既読マーク。「ちゃんとしてる私」を、数秒で証明できてしまったみたいで、少し怖かった。　机の上では、さっき撮影に使った本が、まだ「はじめに」のページすら開かれていないまま、静かに置かれていた。３　月末のオンライン読書会は、想像以上にカオスだった。「じゃあ、今月の一冊、順番に話してこっかー」　奈々子が司会を買って出て、画面に並んだ六つの顔を順番に指名していく。ZOOMの小さな四角の中で、それぞれの生活感が垣間見える。洗濯物が干されたままの部屋。オフィスっぽい背景。カーテンだけが映っている画面。バーチャル背景で海になっているやつ。「じゃあまずは、りおから」　名前を呼ばれて、一瞬だけ返事を忘れる。慌ててマイクをオンにした。「あ、はい」「何読んだんだっけ？」「えっと……これ」　先週、タイムラインに流れてきた感想ツイートを３つくらいスクショして、ノートアプリに箇条書きしたやつが、スマホの裏側で控えている。本体より、そっちの方を信頼している自分が情けない。「えーとね、“自分のキャリアは自分で選べ”みたいな話で……」　自分で選べ。　自分で選べ。　自分で選んだ結果、私は今、要約ブログだけを読んでしゃべっている。　私が拙い言葉で本の内容をなぞる間、画面の中の友人たちは、うんうんと頷いたり、「わかるー」と相づちを打ったりしてくれる。その優しさが、逆に拷問みたいに感じる。「で、特に印象に残ったのが、“行動しないと何も変わらない”っていうところで……」　自分で言って、自分で刺さる。　行動しないと、何も変わらない。　でも私は、本すら開いていない。「りお、なんか変わった？　これ読んで」　奈々子がライトに聞いてくる。彼女に悪気がないのはわかってる。でも、だからこそ、逃げ場がない。「えっと……」　一瞬、本気でZOOMを落としてやろうかと思った。回線不良になったふりをして、消えてしまう。けど、それをやったら、たぶんこのグループからも、本当に消えてしまう気がした。「“変わった”っていうか……なんか、今のままだとやばいかもって思った、かな」　それは嘘ではなかった。ブログを読んで、本を読んだ気になって、それでもどこかで罪悪感を抱えている自分を「やばい」と思っているのは、本当だ。「おー、いいじゃん。危機感、大事」　奈々子が笑って、他のメンバーもうんうん頷く。「てかさ、正直言うと……」　別の四角から、慎ましい声がした。「ちゃんと最後まで読めたの、今月、一冊もないんだよね」　話しているのは、健太だった。サークル時代、いつも端っこで本を読んでた、よく意味のわからない人。卒業してからも、読書メーターみたいなアプリのスクショをよくタイムラインに上げていたから、「あいつはずっと本を読んでる人」だと、勝手に思い込んでいた。「え、そうなの？」「うん。三冊買ったんだけど、どれも途中で飽きてさ。仕事忙しいのもあるけど、なんか……集中できないんだよね。本開いても、三ページくらいでスマホ見ちゃう」　その言葉に、私は勝手にドキッとした。　三ページでスマホ。それは、まさに私のことだった。「でもさ、健太、読書メーターめちゃ更新してるじゃん」　誰かがツッコむ。健太は「あー」と曖昧に笑った。「あれも、正直、ちょっと“盛ってる”」「盛ってる？」「途中までしか読んでない本も、“読了”にしちゃってる。なんかさ、“途中まで読んで放置した本”って、アプリ上でも現実でも、すごい罪悪感あるじゃん。だから、読み切ってなくても、“だいたいわかったからいいや”みたいな感じで、読了にしちゃう。自己満だけど」　画面越しに沈黙が落ちた。その沈黙には、「わかる」と「怖い」と「笑える」と「笑えない」が全部混ざっていた。「ていうかさ」　奈々子が笑いながら言う。「こういう場で“わかる”って言える時点で、もうけっこう重症だよね、私たち」　笑いが広がる。私も笑う。けどその笑いの中で、心のどこかが冷えていく。　――ああ、みんなも同じなんだ。　そう思うと、安心するはずなのに。「じゃあさ」　奈々子が、急に真面目な声になった。「この中に、“ちゃんと読んだ人”、いる？」　一瞬、画面が固まったように見えた。誰も喋らない。誰も名乗り出ない。　そのとき、画面の隅っこで、誰かのアイコンが小さく光った。ミュート解除のマークがつく。私、そこに誰がいたか、正直すぐには思い出せなかった。グループに追加されてたのは知ってたけど、この一ヶ月、ほとんど発言してない人だ。「……一冊だけ、読んだ」　画面の中央に、その人の顔が映し出される。メガネ。無造作な前髪。背景には、本棚。背表紙の色合いからして、ビジネス書じゃなさそうだった。「あれ、みゆき？」　奈々子が目を丸くする。「え、みゆき、いたの？」「いたよ。最初から」　みゆき。大学時代、同じサークルにいたけど、ほとんど話したことがない。いつもイベントの受付をしていて、写真に写るときは端っこにいた。存在感が薄い、というより、空気と同じくらい自然にそこにいる人。「なに読んだの？」　奈々子が聞く。みゆきはちょっと迷ってから、画面から消えた。数秒後、本を一冊持って戻ってくる。「これ」　画面いっぱいに映し出されたのは、聞いたことのない小説のタイトルだった。帯には、どこかの文学賞のロゴ。売り場で平積みになっているイメージが、あまり湧かない。「なんか、意外」「うん。仕事で疲れるからさ、ビジネス書とか、読む気にならなくて。とりあえず、“今読みたいもの読もう”って思って」　その「今読みたいもの」という言葉が、やけにまっすぐに聞こえた。「どうだった？」「うーん……」　みゆきは少し考えてから、言葉を探すみたいに話し始めた。「最初、全然、意味わかんなかった。登場人物が何考えてるのかもよくわかんないし、文章もなんかへんな感じで。でも、読み進めてるうちに、“意味わからないけど、なんかこの感じ、わかるかも”ってところが増えてきて……なんていうか、“答えがないまま終わる話”なんだけど、その“答えのなさ”が、読んでてすごい落ち着いた」　画面の誰かが、「へえ」と感心する。私は、自分の指先が汗ばんでいることに気づいた。「なんかさ」　みゆきは、言葉を足す。「仕事で、毎日、“正解に近づく”ことばっかりやってる気がして。“より正しい資料”“よりわかりやすい説明”“より納得してもらえる提案”。そういうのを目指すのは嫌いじゃないんだけど……なんか、“どこにも着地しない話”を読んでると、“着地しなくてもいい時間”がちゃんとあるの、ありがたいなって思った」　誰かが「わかるかも」とぼそっと呟く。「でね」　みゆきは、小さな声で続ける。「この本読んだこと、別に誰にも言うつもりなかったんだよね。本棚にしまって、終わりでいいかなって。でもこのグループあるから、“一応、報告しとこっか”って思って」　奈々子が笑う。「うちらの積読クラブ、効いてるじゃん」「でもさ」　みゆきは、少しだけ目線を落とした。「なんか、“本を読んだことを報告するために読む”ようになったら嫌だなって、ちょっと思った」　その言葉が、静かに画面全体に降りた。「だから今月は、この一冊だけでいいやって思った。『今月の一冊』っていうより、『今のわたしの一冊』の方が、しっくりくるから」　誰もすぐには何も言わなかった。奈々子ですら、一瞬、言葉を失っているように見えた。　グループの名前は「積読クラブ」だけど、今この瞬間、私たちの前にそびえ立っているのは、本の山じゃなくて、会話の沈黙だった。その沈黙の高さを測りながら、私は、自分の机の上の、開かれていないビジネス書のことを思い出していた。４　読書会が終わってから、しばらくの間、グループチャットは静かだった。いつもなら「おつかれー」「今日も楽しかった！」みたいな軽いメッセージが飛び交うのに、その日はスタンプ一個だけで終わった。　私はノートパソコンを閉じ、部屋の電気を消した。暗くなってから、机の上の本を手探りで見つける。さっきまで「読んだふり」をするための道具だったそれが、急に、すごく重く感じた。　窓の外には、向かいのマンションの灯りがぽつぽつとついていた。どの部屋にも、それぞれの生活があって、それぞれの「今月の一冊」だか「今週のタスク」だか「今日の後悔」だかがあるんだろう。　私はベッドに腰掛け、本を膝の上に置いた。　――今のわたしの一冊。　みゆきの言葉が、何度もリピートされる。　今のわたし。　今のわたし、ってなんだろう。　タイムラインを開けば、誰かの「今」は洪水みたいに流れてくる。今読んだ本。今感じたこと。今考えていること。今、頑張っている自分。今、落ち込んでいる自分。今、立ち直っている自分。誰もが「今」を差し出し合い、その中に正解を探している。　でも、私の「今」は、うまく言語化できない。　ただ、疲れていて、焦っていて、何かにならなきゃいけない気がして、何にもなれていない。　私は本を開いた。　「はじめに」の一行目を読む。　さっき、要約ブログで読んだ内容と、ほとんど同じことが書いてある。でも、紙に印刷されている文字と、スマホのスクロールで流れていく文字は、同じ意味のはずなのに、体感が違う。　二行目を読む。　三行目を読む。　十行目あたりで、スマホに手が伸びそうになる。　「本　要約」「この本　感想」「この本　評判」。　そのどれかを検索すれば、「自分の考え」の代わりになる言葉が、いくらでも手に入る。　でも、今日はとりあえず、それをしないことにしてみる。　ページをめくる。　文字を追う。　頭に入っているのかどうか、自分でもよくわからない。著者の例え話が、いちいち大げさで、鼻につく。自分とは違う世界の人が、違う世界の成功体験を、私に一方的に教えようとしてくる。　途中で、「なんでこれ選んだんだっけ」と思う。　「今の自分にはこれが必要だと思ったので」。　投稿文に書いたあの一文が、じわじわと恥ずかしくなってくる。　そのとき、ふと気づく。　――誰にも見せない読書って、めちゃくちゃ、不安定だ。　カメラロールには、読書中の私の写真はない。　タイムラインにも、読書ログは流れない。　アプリの「読了数」も、増えない。　私がこの本を読んだことを知っているのは、たぶん、今日の私だけだ。明日の私はもう忘れているかもしれないし、来週の私は別のことに追われているかもしれない。この時間は、どこにも記録されないまま、沈んでいく。　その感じが、なぜか、少しだけ、心地よかった。　ページをめくる速度は、遅い。　時々、同じ行を二度読む。　わからない言葉は、そのまま放置する。　大事そうなところに、線を引こうとして、ペンを取りに立ち上がるのがめんどうで、やめる。　「読書」と呼ぶには、あまりにもだらしない。　「学び」と呼ぶには、あまりにも生産性が低い。　それでも、本は、そこにある。　ここで、私とだけ、つながっている。5　その夜、私は本棚の前に立った。　積まれた背表紙の山。　その中から、一冊を適当に引き抜く。　ビジネス書でも、自己啓発書でも、小説でもない、よくわからないエッセイ集だった。たぶん、前にどこかの書店で、「人気芸人が勧める３冊」みたいなポップを見て、勢いで買ったやつだ。　表紙のデザインも、著者の名前も、今まで何度も目にしているはずなのに、「初めてちゃんと見る」感じがした。　ページを開く。　一行目を読む。　面白いかどうかは、まだわからない。　ためになるかどうかも、まったく不明。　この本を読み終わったところで、給料が上がるわけでもないし、フォロワーが増えるわけでもない。　でも、今、ここにある「わからなさ」は、たぶん、誰かのブログでは代替できない。　誰かの要約では、コピーできない。　部屋の中は静かだった。　スマホは、ベッドの向こう側で、画面を下にして置いてある。通知が鳴っても、すぐには気づかない場所。　一ページ。　二ページ。　三ページ。　ちょっとだけ、スマホのことを思い出す。　でも今日は、とりあえず、手を伸ばさない。　四ページ。　五ページ。　どこまで読んだら「読書」と呼べるのかなんて、誰も決めていない。　どこまで理解したら「学び」になるのかなんて、誰も教えてくれない。　きっと私は、これからも、本を読んでるふりをする。　読んでないのに「読んだ」と言いたくなる夜も、またあるだろう。　積読の山は、これからも増えるかもしれない。　それでも、ときどき、こうして誰にも見せない読書をする。　誰にも報告しないまま、本を開いて、閉じる。　「今のわたしの一冊」は、きっと、そのたびに変わる。　変わらないまま終わる夜もある。　それでいいのかどうかなんて、まだわからない。　でも、わからないままページをめくることくらいは、今の私にもできる。　ページの端をつまんで、ゆっくりとめくる。　新しい行が現れる。　そこには、誰かの言葉が並んでいた。　それを「理解」できたかどうかよりも先に、私はただ、その黒いインクの並びを、目で追い続けた。　たぶん、それも、読書のうちに入れていい。　そう勝手に決めて、私はもう一ページ、めくった。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[スリーシェイク、5名のエンジニアがGoogle Cloud Partner Top Engineer 2026 を受賞]]></title>
            <link>https://sreake.com/blog/partnertopengineer2026/</link>
            <guid isPermaLink="false">https://sreake.com/blog/partnertopengineer2026/</guid>
            <pubDate>Fri, 21 Nov 2025 01:00:00 GMT</pubDate>
            <content:encoded><![CDATA[この度 「Google Cloud Partner Top Engineer」アワードプログラムにおきまして、スリーシェイクのエンジニア5名が Google Cloud Japan が高い技術力を持ったエンジニアを表彰するプログラムである 「 Google Cloud Partner Top Engineer 2026 」に選出されたことをお知らせします。The post スリーシェイク、5名のエンジニアがGoogle Cloud Partner Top Engineer 2026 を受賞 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[データもバージョン管理したい？ならdvcを使ってみないか？]]></title>
            <link>https://zenn.dev/akasan/articles/636765e14b0120</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/636765e14b0120</guid>
            <pubDate>Thu, 20 Nov 2025 13:12:32 GMT</pubDate>
            <content:encoded><![CDATA[みなさんデータのバージョン管理してますか？ソースコードのバージョン管理はgitとかを使ってされる方が多いかと思いますが、データのバージョン管理もgitですると結構問題が山積みなんです！今回は主にMLの分野で使われるデータのバージョン管理のためのツールであるdvcを紹介します。 dvcとは？dvcは公式ページの紹介を見ると、以下のように表現されています。データサイエンティスト向けのデータバージョン管理に使いやすいGit拡張機能。 最小限のオーバーヘッドで、データサイエンスのワークフローにデータバージョン管理を適用します。https://dvc.org/なぜdvcのようなツー...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[garakを使ってGemini 3.0の脆弱性検知を早速してみた]]></title>
            <link>https://zenn.dev/akasan/articles/9c6b43d8620517</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/9c6b43d8620517</guid>
            <pubDate>Wed, 19 Nov 2025 13:44:02 GMT</pubDate>
            <content:encoded><![CDATA[今回は本日発表があったGemini 3.0について、早速garakを使って脆弱性検知を行ってみました。検証内容は以下の記事とモデル以外は同じになりますので合わせてご確認ください。https://zenn.dev/akasan/articles/2a44f107064f1f Gemini 3.0とは？日本時間2025年11月19日にGemini 3.0が公開されました。詳細は以下のリリースノートを見ていただくとして、個人的には以下の部分が注目です。Gemini 3 Pro は、画期的な 1501 Elo というスコアで LMArena のリーダーボードのトップに立っています。ま...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、対話しろ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/19/194809</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/19/194809</guid>
            <pubDate>Wed, 19 Nov 2025 10:48:09 GMT</pubDate>
            <content:encoded><![CDATA[はじめに会議室の空気が、徐々に重くなっていくのを感じました。「この設計、拡張性に問題があります」。若手エンジニアのAが言いました。声には確信がありました。「いや、今の要件を考えれば、これが最適だよ」。ベテランのBが即座に返します。口調は穏やかですが、譲る気配はありません。「でも、将来的に機能追加があったときに...」「将来のことばかり考えていたら、今のリリースが間に合わない」二人の言葉は交差しますが、交わりません。言葉のキャッチボールに見えて、実は2つのボールが空中でぶつかり合っているだけです。ボールは地面に落ち、誰も拾いません。私は黙って二人を見ていました。どちらの言い分も分かります。Aは技術的負債を恐れています。Bは納期のプレッシャーを感じています。どちらも正しく、どちらも間違っていません。何かが決定的に欠けています。対話が、ありません。二人は話しています。言葉を交わしています。しかし、対話していません。Aは自分の主張を繰り返し、Bも自分の主張を繰り返します。互いに相手の言葉を聞いているようで、実は聞いていません。正確に言えば、相手の言葉を「自分の理解の枠」に無理やり押し込んで解釈しています。会議は平行線のまま終わりました。結論は「後で話し合いましょう」。何も決まりませんでした。廊下を歩きながら、私は考えていました。なぜ私たちは、こんなにも対話ができないのか。技術の話をしているはずなのに、なぜ感情的な対立になるのか。対話は、なぜこんなにも難しいのか。この問いについて、私は何年も考え続けてきました。ある結論に到達しました。私たちは「対話」を誤解しています。対話とは何か、対話を阻むものは何か、対話を可能にするものは何か。これらをまったく理解していません。だから、対話という言葉を知っていても、対話ができません。対話の欠如は、組織を蝕みます。意思決定が遅れます。同じ議論を繰り返します。優秀な人材が疲弊して去っていきます。イノベーションが生まれません。答えはシンプルです。対話していないからです。このブログで、私は対話を語ります。しかし、「傾聴しましょう」「共感しましょう」という話ではありません。対話を阻む認識の構造を語ります。人間がどのように世界を見ているか、なぜ理解し合えないのか、どうすれば対話が可能になるのか。これらを、できる限り深く考えます。対話の前提を理解しなければ、対話は始まりません。まず、私たちは対話を阻んでいるものの正体を知らなければならないからです。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。対話という幻想を解体する「対話が大切です」。誰もが知っています。でも、「対話とは何か」をちゃんと説明できる人は、ほとんどいません。多くの人は、対話を情報のやり取りだと思っています。私が言葉を発します。相手が受け取ります。相手が言葉を返します。私が受け取ります。このキャッチボールが、対話だと。しかし、それはおそらく誤解です。対話は、データ転送ではありません。対話とは相互の世界観を認識して、理解を深めるプロセスです。その過程で認識の変容が起きることもあれば、相違を明確に理解した上で立場を維持することもあります。どちらも対話の成果です。あなたがコードレビューで「この実装は複雑すぎます」と言います。相手は「いや、これは必要な複雑性です」と返します。ここで何が起きているでしょうか。表面的には、意見の交換に見えます。でも、実際に起きているのは、もっと深い層での衝突です。あなたが「複雑」と言うとき、あなたは過去に経験した「複雑なコードがメンテナンス不能になった」記憶を参照しています。あなたにとって「複雑さ」とは、未来の技術的負債の予兆です。一方、相手が「必要な複雑性」と言うとき、相手は「ビジネス要件の複雑さを適切にモデル化した結果」を見ています。相手にとって「複雑さ」とは、現実を正確に反映している証拠です。同じ「複雑」という言葉を使っていますが、指し示すものがまったく違います。この認識の差異に、どちらも気づいていません。対話が成立するためには、この差異に気づかなければならない。「ああ、私たちは同じ言葉を使っているが、違うものを見ているのです」と。この気づきがないまま言葉を交わし続けても、それは対話ではありません。ただの言葉の衝突です。そして、多くの人は対話の目的を誤解しています。対話の目的は、合意することだと思っています。それは副産物に過ぎません。対話の本質は、相互の世界観を認識することです。あなたの目に世界がどう映っているか。相手の目に世界がどう映っているか。この2つの視点を、互いに理解し合うこと。それが対話です。合意なき理解。これは矛盾しているように聞こえます。しかし考えてほしい。あなたは親友と、政治的な意見の相違があります。それでも友人関係は続きます。なぜか。互いの違いを理解しているからです。「あの人はこういう経験をしてきたから、こう考えるのです」。この理解があれば、意見の相違は関係を壊しません。むしろ、理解の深さが関係を強くします。対話をスキルだと考える人も多いです。傾聴のテクニック。共感の言葉。これらを学べば、対話ができると。しかし、テクニックだけではどうしても不十分です。対話は、技術である前に、存在の様式です。「どう在るか」の問題です。防御的な存在様式のまま、いくら傾聴のテクニックを使っても、それは対話になりません。対話とは、自分の腹の中を晒す行為です。「私は確信を持っていない」と認めることです。「私の見方は、1つの解釈に過ぎない」と受け入れることです。「相手の言葉によって、私の認識が変わるだろう」と覚悟することです。この覚悟なしに、対話は始まりません。だから、対話は難しいのです。自分が正しいと思いたい。自分の世界観を守りたい。私を含めて人は変化することを恐れます。これらの防衛本能が、対話を阻みます。対話するためには、まず自分の防衛を解除しなければならない。しかし、防衛を解除することは、無防備になることではありません。対話における強さとは、自分の視点や視座や観点を絶対化しないことです。複数の解釈を許容することです。不確実性の中でも思考し続けることです。対話を不可能にする構造対話が難しいのは、個人の能力の問題ではありません。人間の認識そのものに、対話を阻む構造が組み込まれているからだ。認識の構造的宿命あなたは今、この文章を読んでいますよね。でも、実は「読んでいる」のではありません。脳は、膨大な情報の中から一部を選択し、それを「意味」として構成しています。人間の認識は、選択的です。世界をあるがままに受け取ることはできません。必ず、フィルターを通す。このフィルターを、認知科学では「スキーマ」と呼ぶ。スキーマとは、過去の経験から構築された認識の枠組みです。スキーマは、生存に必要です。毎回ゼロから世界を理解していたら、判断が遅すぎて生き残れません。パターン認識によって、瞬時に判断します。これは、進化が私たちに与えた能力です。しかし、この能力には代償があります。私たちは、世界をあるがままに見ることができません。常に、認識というレンズを通して見ます。対話において、これは致命的な問題を引き起こすことがあります。相手の言葉を聞くとき、私たちは相手が言った言葉を聞いているのではありません。自分のスキーマによって解釈された言葉を聞いています。「この実装、ちょっと心配だな」。相手がこう言ったとき、あなたは何を聞くでしょうか。もしあなたが過去にこの相手から批判された記憶があるなら、「また批判されています」と聞きます。しかし、相手は単に「一緒に確認したい」と言っているだけだろう。バイアスを取り除く。これは、よく言われるアドバイスです。しかし、バイアスを完全に取り除くことは不可能です。 バイアスは、認識の副作用ではありません。認識そのものです。スキーマなしに世界を見ることはできません。できるのは、「自分がバイアスを通して見ています」という自覚だけです。この自覚があるとき、対話の質が変わります。相手の言葉を聞いて、即座に「これはこういう意味です」と決めつけません。「私はこう解釈したが、相手の本当の意図は違うだろう」と留保します。「どういう意味ですか」と確認します。この一手間が、誤解を防ぐことがあります。認識の構造的宿命を受け入れること。これは対話の第一歩です。「私は世界をありのままに見ていない」と認めること。「私の解釈は、1つの可能性に過ぎない」と理解すること。この謙虚さが、対話の基盤です。権力勾配という非対称性「最近、調子はどう」。上司がこう聞きます。あなたは「はい、順調です」と答えます。同じ質問を同僚が聞きます。あなたは「ちょっと行き詰まっている」と本音を言います。同じ言葉なのに、発する人が変わると、意味が変わります。これは、社交辞令の問題ではありません。権力の非対称性が、言語の意味を書き換えています。上司の「最近どう」は、音声学的には同僚の「最近どう」と同じです。しかし、意味論的にはまったく別の文です。上司の言葉には、評価の含意があります。あなたの答えは、業績の報告として受け取られる可能性があります。だから、あなたは防御的になります。これは、悪意の問題ではありません。上司が部下を評価しようとしているわけではありません。だが、位置関係が、言葉に意味を付与します。 発話者の意図とは無関係に。権力の非対称性は、対話を歪めます。上下関係のある場で「自由に意見を言ってください」と言われても、部下は自由に意見を言えません。なぜならその意見が評価に影響する可能性を意識するからだ。たとえ上司が「評価には関係ない」と保証しても、その保証自体が権力の行使です。この問題に対して、「フラットな関係を目指しましょう」というアプローチがあります。しかし、これは幻想です。形式を変えても、構造は変わりません。 給与を決める権限、人事評価をする権限、プロジェクトのアサインを決める権限。これらの権力は、言葉遣いを変えても消えません。むしろ、権力の存在を否認することで、問題は見えにくくなります。現実的なアプローチは、権力の非対称性を前提とすることです。「私とあなたには、権力の差がある」と認めること。その上で、「この制約の中で、どこまで対話を開くことができるか」と問うこと。1つの方法は、構造を明示することです。「私は上司として聞いているのではなく、エンジニアとして意見を聞きたい」と宣言します。「今日の議論は、人事評価には一切関係ない」と約束します。その約束を守る。一貫性のある行動によって、徐々に信頼が生まれます。もう1つは、リスクを先に取ることです。権力を持つ側が、先に自分の弱さを晒す。「私もこの技術については自信がない」と認めます。「あなたの方が詳しいので、教えてほしい」と頼る。権力者が弱さを見せることで、非対称性が少し和らぐ。さらに、組織への信頼が一定以上ある場合、匿名フィードバックの併用も有効です。たとえば、月次で匿名のエンゲージメントサーベイを実施します。そこで出た意見を全体会議で議論します。これにより、権力関係の影響を受けずに本音の課題が可視化され、対話の素材となります。ただし、組織自体への信頼がなければ、良質なフィードバックは集まりません。匿名フィードバックは、信頼の上に成り立つ仕組みです。しかし、これはすべて部分的な緩和に過ぎません。権力の非対称性は根深く、簡単には変わらない。それでも、それを自覚して、丁寧に扱うことはできます。対話は完璧にはなりません。権力勾配がある限り、完全に対等な対話は困難です。でも、不完全な対話でも、無対話よりはるかにましです。時間的ズレという錯誤「あなたはいつもそうです」。この言葉を聞いたことがあるでしょう。しかし、よく考えてほしい。「あなたはいつもそうです」と言うとき、あなたは何を見ているのか。目の前の現在の相手を見ているのでしょうか。それとも、記憶の中の過去の相手を見ているのか。後者です。私たちは、相手の過去の行動パターンを記憶しています。そのパターンを、今の相手に投影しています。「あなたはいつも約束を守らない」と言うとき、私たちは過去の2回、3回の出来事を思い出しています。それを「いつも」に拡大しています。しかし、今この瞬間の相手は、過去の相手ではありません。人は変わります。状況は変わります。昨日の相手と今日の相手は、厳密には別の存在です。でも、私たちは記憶の中の相手と対話しています。現在の相手の言葉を、過去のパターンに当てはめて解釈しています。これは、認識の効率化です。毎回相手を新しく理解するのは、コストが高い。だから、脳は過去の経験からパターンを作り、それを使って瞬時に判断します。ほとんどの場合、これは有効です。相手の性格や行動パターンは、そう簡単には変わりません。しかし、対話においては、この効率化が仇となります。相手が変化しようとしているとき、成長しようとしているとき、私たちは過去のレッテルを貼り続けます。「あの人はこういう人です」という決めつけが、相手の変化を見えなくします。「以前のあなたなら、こう言っただろうけど」という前置きは、実は相手を過去に縛りつけています。多くの場合、予言の自己成就が起きます。「どうせ変わっていないと思われているなら、変わる必要はない」と相手は感じます。時間的ズレを意識するとは、「相手は過去の相手ではありません」と認めることです。「今のあなたは、どう考えていますか」と問うことです。過去のパターンは参考にしつつ、決定的な判断材料にしないことです。これは、自分自身に対しても同じです。「私はこういう人間です」という自己認識は、実は過去の自分のパターンです。対話するとは、この時間的ズレを認識することです。相手と自分、両者とも常に変化しています。過去に縛られず、現在に向き合う。しかし、ここで残酷な真実に向き合わなければならない。人は何にでもなれるから、何にもなれません。 無限の可能性があるように見えて、実は時間は有限です。「いつかやろう」は、気づいた時には「もうできません」に変わっています。時間は不可逆です。我々は有限の存在です。だからこそ、今この瞬間の対話が重要になります。先延ばしにした対話は、永遠に失われる可能性があります。ここまで見てきた3つの制約――認識の不可避性、関係性の非対称性、時間性の逆説――は、いわば人間という存在の「ハードウェアの制約」です。私たちは世界をそのまま受け取れないし、権力関係から自由でもないし、時間の外にも出られない。では、この制約の上で、私たちはどうやって対話を可能にしていけばいいのでしょうか。制約を完全に消すことはできません。しかし、制約を認識し、それを前提としながら、対話を開く力を育てることはできます。だからこそ、「制約付きの人間」がどんな力を鍛えれば対話が成り立つのかを、具体的に見ていく必要があります。ここからは、対話を可能にする3つの具体的な力を見ていきます。対話を可能にする力ここまで、対話を阻む構造的な問題を見てきました。認識の限界、権力の非対称性、時間の錯誤。これは、私たちの認識そのものに組み込まれた、避けがたい制約です。しかし、これらの制約を認識することが、対話への第一歩です。制約を知ることで、私たちは対話を可能にする力を育てることができます。対話は、単なる技術ではありません。同時に、訓練可能な能力でもあります。以下の3つの力は、対話を可能にする中核的な能力です。中断する力朝、目覚ましが鳴る。あなたは無意識にスマホを取ります。これらの行動は、意識的な判断を経ていない。自動的に起きています。人間の判断の大部分は、自動的に処理されています。私はこれを、2つのシステムとして理解しています。システム1は、速く、自動的で、直感的。システム2は、遅く、意識的で、論理的。システム1は、エネルギー効率が良い。過去の経験からパターンを学習し、瞬時に判断します。もしすべての判断をシステム2で処理したら、私たちは何もできません。しかし、システム1には限界があります。新しい状況に対応できません。複雑な判断ができません。バイアスに支配されます。対話は、システム1では処理できません。誰かがあなたを批判します。システム1は、瞬時に「攻撃です」と判断します。防御反応が起動します。反論します。言い訳します。相手を攻撃し返します。これはすべて、自動的に起きます。意識する前に、もう言葉が口から出ています。この自動反応が、対話を壊します。中断する力とは、この自動反応を一時停止する力です。システム1からシステム2へ、意識的に切り替える力です。具体的には、どうするでしょうか。まず、自分の反応に気づく。「今、私は防御的になっています」と認識します。この気づきが、自動反応を中断します。次に、一呼吸置く。文字通り、深呼吸します。これは単なる気休めではありません。呼吸は、自律神経系に直接作用します。深く息を吐くことで、交感神経の興奮が抑えられます。そして、問いかけます。「相手は本当に攻撃しているのか」「他の解釈はないか」「今、反応する必要があるのか」。中断する力は、訓練で育つ。最初、反応した後で気づく。「ああ、また自動的に反応してしまいました」。でも、気づくことが第一歩です。繰り返すうちに、反応している最中で気づくようになります。やがて反応する前に気づけます。これは、メタ認知的な筋肉です。使えば使うほど、強くなります。この力があると、対話の質が変わります。相手の言葉を、条件反射的に解釈しない。一度受け止めて、考えます。「この言葉は、どういう意味だろう」「相手は、何を伝えようとしているのだろう」。この思考の間が、誤解を防ぐ。理解する力人間の認識は、一人ひとり異なります。私たちは皆、異なる「認識の枠組み」を持っています。同じ入力に対して、異なる処理をします。異なる出力を生み出す。相手の言葉を理解するとは、その言葉の表面的な意味を把握することではありません。相手がどんな認識の枠組みでその言葉を生成したかを推測することです。「この実装は複雑すぎます」。この言葉を聞いたとき、あなたは何を理解すべきでしょうか。言葉の辞書的な意味ではありません。相手の認識の枠組みを理解すべきです。相手の認識の枠組みは、何で構成されているでしょうか。まず、価値観。相手は何を大切にしているのでしょうか。品質か、速度か、保守性か、パフォーマンスでしょうか。次に、経験。相手はどんな経験をしてきたのでしょうか。どんな失敗から学んだのでしょうか。「この実装は複雑すぎます」と言う人の認識の枠組みを推測しよう。もしかしたら、この人は過去に複雑なコードでデバッグに苦労した経験があるだろう。だから、「複雑さ」は「将来の苦痛」を意味しています。あるいは、この人はシンプルさを美徳とする価値観を持っているだろう。この認識の枠組みを理解せずに、言葉だけに反応してはいけません。理解するための問いには、段階があります。第一層の問い：事実の確認。「この部分が複雑だと感じるのは、どの部分ですか」。具体的にどこを指しているかを特定します。第二層の問い：解釈の探索。「その部分は、どういう問題を引き起こしますか」。相手がどう解釈しているかを明らかにします。第三層の問い：背景の理解。「過去に似た経験がありますか」「なぜそう考えるようになったのですか」。価値観や経験の背景を探ります。相手の認識の枠組みの根源に迫ります。この段階的な問いかけによって、相手の認識の枠組みが少しずつ解像度を上げて見えてきます。「ああ、以前このパターンでバグが多発したんです」「デバッグに一週間かかったことがあって」。この情報が、相手の認識の枠組みを明らかにします。そして、あなたは理解します。「なるほど、この人は『複雑さ』を『デバッグの困難さ』と結びつけて考えているのです」。この理解があれば、応答が変わります。「確かに、この部分は複雑に見えますね。でも、テストを充実させることで、デバッグの困難さは抑えられます」。これは、相手の認識の枠組みを尊重した応答です。「複雑じゃない」と否定するのではなく、「複雑だが、あなたの懸念には対処できます」と提案します。これなら、対話が続きます。理解する力とは、共感することではありません。認識論的な探索です。 相手がどんなプログラムを実行しているかを、逆アセンブルする作業です。表面の出力から、内部のロジックを推測します。この探索は、時間がかかる。でも、この時間を省略してはいけません。理解せずに議論しても、平行線になるだけです。変容する力「あのときの自分なら、絶対にこうは言わなかったな」と感じる瞬間があります。価値観が変わった、というほどドラマチックではない。でも、世界の見え方が微妙にズレている。この「見え方のズレ」こそが、認識の枠組みの変容です。「意見を変えます」と「認識の枠組みを変えます」は、まったく違います。新しい設計思想を学ぶとき、最初は既存の知識を使って理解しようとします。しかし、本当にその考え方を習得するには、認識の枠組みそのものを変える必要があります。「拡張性」という概念を理解するには、単に技術パターンを学ぶだけでなく、ソフトウェアの時間軸についての認識を根本から変える必要があります。「今のコードの美しさ」から「将来の変更の容易さ」へ。視点を変える必要があります。対話においても、同じことが必要になります。相手の言葉を聞いて、「ああ、そういう見方もあるのか」と新しい視点を知ります。それだけでは変容ではありません。その視点を、自分の認識の枠組みに統合します。自分の認識の枠組みを、少し変えます。これは変容です。「以前、複雑さは常に避けるべきだと思っていました。しかし今、必要な複雑さと不要な複雑さを区別すべきです」。この変化が、認識の枠組みの変容です。なぜ認識の枠組みの変容が重要なのでしょうか。それは、表面的な変化は持続しないからだ。誰かに説得されて意見を変えます。その場では納得します。でも、一週間後、元の意見に戻っています。なぜか。認識の枠組みが変わっていないからだ。一方、認識の枠組みが変わると、変化は持続します。いや、「持続する」という表現が正確ではありません。もはや、元に戻るという選択肢がない。 新しい世界の見方を獲得した後、古い見方には戻れません。これは、成長の本質です。10年前の自分と今の自分を比べてみてほしい。意見が変わっただけではないはずです。世界の見方が変わっています。判断の基準が変わっています。これが認識の枠組みの変容です。もし認識の枠組みが変わっていないなら、それは10年間成長していないのです。対話は、この認識の枠組みの変容を可能にします。相手の異なる世界観に触れることで、自分の認識の枠組みを疑う機会が生まれます。「自分の見方は、絶対ではないだろう」と気づく。しかし、認識の枠組みの変容は、容易ではありません。なぜなら自己同一性の問題があるからだ。「私」という感覚は、認識の枠組みによって支えられています。 だから、認識の枠組みを変えることは、ある意味で古い自分を手放すことです。慣れ親しんだ自己イメージから離れ、新しい自己へと移行します。これは、怖い。でも、この手放しこそが、成長です。 古い自分に固執することは、成長を拒否することです。ある意味で、古い自分は死に、新しい自分が生まれる。この変容を恐れない勇気が、対話には必要です。「相手の言葉によって、私は変わるだろう」と覚悟すること。「私の世界観は、絶対ではありません」と認めること。この勇気があって初めて、本当の対話が可能になります。中断する力、理解する力、変容する力。これは対話を可能にする基礎的な能力です。しかし、これらの力を阻む、より深い障害があります。それは、私たちが日々生きている「物語」です。自分について、組織について、世界についての物語。この物語は、私たちを定義すると同時に、私たちを縛り付けます。ナラティヴという牢獄私たちは、物語の中に生きています。朝起きて、鏡を見ます。「私は〇〇です」と言える。この「〇〇」は、物語です。「私は内向的な人間です」「私は論理的に考える人間です」。これはすべて、自分について語る物語です。MBTIなんかはまさしくそうです。四文字のラベルで自分を定義し、そのラベルに沿って行動します。物語が、自己を作り出す。職場で、あるプロジェクトが失敗します。あなたは理由を考えます。「計画が甘かったからだ」「コミュニケーション不足だったからだ」。これも、物語です。起きた出来事を、因果関係で結びつけた説明です。これらの物語を、ナラティヴと呼ぶ。 ナラティヴは、現実そのものではありません。現実の解釈です。でも、私たちはナラティヴを通してしか現実を認識できません。ナラティヴには、3つの層があります。第一の層は、解釈のフレームです。「何を見るか」を決める枠組み。同じコードを見ても、ある人は「保守性」を見ます。別の人は「パフォーマンス」を見ます。第二の層は、正当化の物語です。「なぜそう見るのか」を説明する因果の鎖。「過去にレガシーコードで苦しんだから、保守性を重視する」。経験が、価値観を生み、価値観が、見方を決めます。第三の層は、アイデンティティの核です。「私は誰か」を定義する自己物語。「私は品質にこだわるエンジニアです」。この自己定義が、すべての判断の基盤になります。ナラティヴは、必要です。ナラティヴなしに、私たちは行動できません。何が重要かを決められません。優先順位をつけられません。選択ができません。ナラティヴは、複雑な現実を理解可能なパターンに圧縮します。しかし、ナラティヴは、牢獄にもなります。ナラティヴが固定化すると、新しい情報を受け入れられなくなります。すべてを既存のナラティヴで解釈しようとします。「やっぱりそうでした」ばかりで、「意外でした」がない。これは、学習の停止です。より問題なのは、ナラティヴの防衛化です。ナラティヴを修正しようとする試みを、自己への攻撃と感じます。「あなたの見方は違うだろう」と言われて、「私の経験を否定するのか」と反応します。これは、ナラティヴと自己が同一化しているからだ。対話において、ナラティヴの衝突は避けられません。二人の人間が会えば、2つのナラティヴがぶつかります。問題は、ナラティヴがあることではありません。ナラティヴを絶対化することです。「私の見方が正しい」と考えるとき、あなたはナラティヴを絶対化しています。この態度では、対話は不可能です。対話するとは、ナラティヴの相対性を認めることです。「私の見方は、1つの可能性に過ぎない」と理解すること。「相手の見方も、1つの可能性です」と受け入れること。「もしかしたら、第三の見方があるだろう」と探索すること。ナラティヴの保持的懐疑。 これが、対話の核心です。自分のナラティヴを持ちつつ、それが絶対でないという意識を保つ。相手のナラティヴを尊重し、新しいナラティヴを共創する可能性に開かれている。しかし簡単ではありません。ナラティヴを懐疑することが、自己の確実性を手放すことだからだ。この不確実性に耐える力が、対話には必要です。この態度を保つとき、新しい地平が開けます。ナラティヴを持ちながらも、それに縛られない。1つの見方を持ちながらも、他の見方を排除しない。この柔軟性が、見えなかったものを見えるようにします。対話とは、この新しい地平を開くための冒険です。しかし、ナラティヴの牢獄に閉じ込められたとき、何が起きるでしょうか。個人レベルでは、学習が停止します。成長が止まります。より深刻なのは、組織レベルでの影響です。組織のメンバー一人ひとりが、自分のナラティヴに固執します。「私の見方が正しい」と確信します。他者の見方を受け入れません。この状態では、対話は成立しない。対話なき組織は、どうなるのでしょうか。答えは明確です。緩やかな、だが確実な衰退です。ナラティヴは個人の中だけに存在しているわけではありません。「私はこういう人間だ」という物語と同じように、組織もまた「私たちはこういう会社だ」「うちの部署はこういう役割だ」という物語を持っています。個人のナラティヴが集まり、絡み合い、共有されることで、組織レベルのナラティヴが立ち上がります。そして厄介なことに、この組織ナラティヴもまた、私たちを守りながら、同時に縛ります。個人の対話不全は、組織の対話不全として増幅される。ここからは、視点を個人から組織へと一段スライドさせて、対話の欠如が組織に何をもたらすのかを見ていきます。組織に広がる牢獄視点を個人から組織へと広げよう。なぜなら私たちの多くは、単独で働いているのではなく、組織という集合体の中で対話しているからだ。組織とは何か。表面的には、人々の集まりに見えます。実際には、個々の人間と、その人間同士の相互作用の両方から成り立っています。何か問題が起きたとき、人は、よくわからない抽象的なものに原因を押しつけて思考停止してしまうことがあります。「政府の政策が悪い」「社会の仕組みが悪い」。よくわからないものよりは、具体的な何か—たとえば自分自身の行動—に原因を求めた方が、問題の解決につながる。たとえば、ある施策が推進されようとしているが、その施策について疑問があるから議論したい。誰が推進しているのか教えてほしい。そう尋ねたら、「誰というわけではなくて、組織として進めています」という返答があったとします。しかし、具体的な生身の人間を通さない意思決定など存在しない。「組織として進めています」というのは事実だろうが、そう言うと霧の中を彷徨うような感覚になります。解像度を上げてみれば、誰かが意見を持っていて、誰かが同調して進めているのです。だから、知りたければ、課題を解決したければ、まずは生身の人に働きかけることです。組織の緩やかな衰退対話の欠如は、組織を内側から蝕みます。しかし、組織が成熟するにつれて、ある種の宿命的な問題が生じます。これを構造的無能化と呼びます。構造的無能化とは、組織が思考力と実行力を段階的に喪失し、環境変化に適応できなくなる現象です。これは急激な破綻ではありません。ゆっくりと、気づかれないうちに進行する慢性的な機能不全です。構造的無能化の根本には、ナラティヴの固定化と対話の欠如があります。組織の各メンバーが自分のナラティヴに閉じこもります。部門ごとに異なるナラティヴを持ちます。「営業は数字しか見ていない」「開発は現実を知らない」「経営は現場を理解していない」。これらのナラティヴは、互いを排除し合います。対話は起きません。そして、組織は徐々に機能を失っていきます。なぜ成功が失敗の種となるのか皮肉なことに、成功した組織ほど、この罠にはまりやすい。企業が成功すると、その成功をもたらした方法を固定化しようとします。「この方法でうまくいった」という経験が、標準化とルーティン化を促します。効率を最大化するために、分業を進めます。これは合理的です。しかし、この成功体験は、組織のナラティヴを固定化します。「私たちはこうやって成功した」という物語が、組織のアイデンティティになります。この物語は、誇りの源泉です。同時に、変化への抵抗の源泉でもあります。「なぜ変える必要があるのか。これでうまくいっています」。成功のナラティヴは、新しい情報を拒絶します。異なる意見を排除します。対話を閉ざします。問題は、この効率化が前提としている「環境の安定性」です。市場が変わらず、顧客ニーズが変わらず、技術が変わらなければ、標準化とルーティン化は機能し続けます。しかし、環境は変わります。しかも、成功した企業ほど、その変化に気づきにくい。なぜなら、既存のやり方で「まだ」利益が出ているからです。固定化されたナラティヴは、変化のシグナルを見えなくします。全体を見失う組織効率化の代償として、最初に現れるのが断片化です。断片化とは、組織の各部分が自律的に機能する一方で、全体としての統合性を失う状態です。営業部門は「売上」だけを見ます。開発部門は「機能」だけを見ます。カスタマーサポートは「問い合わせ対応」だけを見ます。誰も「顧客の体験全体」を見ていません。この断片化は、部門ごとのナラティヴの固定化から生まれます。営業は「数字こそ正義です」というナラティヴを持ちます。開発は「技術的品質が最重要です」というナラティヴを持ちます。それぞれのナラティヴは、部門内では共有されています。しかし、部門を超えた対話はありません。異なるナラティヴを持つ者同士が話すとき、それは対話ではなく、対立になります。「私の仕事はここまで」「それはあなたの部署の仕事」。明確な役割分担は、一見すると効率的です。しかし、組織を横断する課題—たとえば「なぜ顧客満足度が下がっているのか」—に対して、誰も答えを持っていない状況が生まれます。断片化した組織では、問題が「部門間の隙間」に落ちます。誰の責任でもない問題は、誰も解決しません。対話がないからです。新しいものを生み出せない組織断片化が進むと、次に訪れるのが不全化です。不全化とは、組織が新しい課題を認識し、新しい解決策を生み出す能力を失うことです。視野が狭くなり、思考が硬直化します。外部の変化—新しい競合の登場、技術革新、顧客ニーズの変化—を捉えられなくなります。なぜこうなるのか。断片化した組織では、各部門が自部門の指標だけを追求します。営業は売上目標、開発は納期、サポートは対応時間。これらの指標を達成することが「仕事」になります。全体最適ではなく、部分最適の連鎖です。新しい事業を生み出すには、部門を横断した協力が必要です。しかし、断片化した組織では、その協力を生み出す仕組みがありません。部門を超えた対話がないからです。各部門が自分たちのナラティヴに閉じこもり、他部門のナラティヴを理解しようとしません。本質を掴めない組織そして最終段階が表層化です。表層化とは、問題認識が表面的になり、根本原因に到達できない状態です。収益が悪化します。離職率が上がります。顧客満足度が下がります。これらの「症状」は見えます。しかし、「なぜそうなっているのか」という本質的な問いに答えられません。なぜ根本原因に到達できないのでしょうか。深い対話がないからです。表層化した組織では、各自が固定化されたナラティヴで問題を解釈します。「これは営業の問題です」「これは開発の問題です」。しかし、誰も「私たちの組織のあり方の問題ではないか」とは問いません。なぜなら、そう問うことは、組織全体のナラティヴ—「私たちはこういう会社です」という自己定義—を疑うことになるからだ。表層化した組織では、対症療法が繰り返されます。「売上が下がった→営業人員を増やそう」「離職率が高い→給与を上げよう」。これらの施策は、表面的な症状には対処しますが、根本原因—組織文化の問題、マネジメントの問題、ビジョンの喪失—には触れません。根本原因に触れるには、深い対話が必要です。しかし、ナラティヴの牢獄に閉じ込められた組織には、その対話ができません。個人の能力ではなく、構造の問題重要なのは、これは個人の能力の問題ではないということです。組織の一人ひとりは、多くの場合、有能です。変革したいという意志もあります。しかし、構造的無能化に巻き込まれることで、個々の能力が発揮できなくなります。個人を責めても、問題は解決しません。構造を変えなければなりません。しかし、構造は人が作り、人が維持していることも事実です。構造を変える責任は、その構造内の人々全員にあります。そして、構造を変えるには、対話が必要です。部門を超えた対話。階層を超えた対話。過去の成功を疑う対話。企業変革という長い道のりどうすればこの悪循環から抜け出せるのでしょうか。企業変革には、4つのプロセスが必要だと考えられます。第一に、全社戦略を考えられるようになること。 断片化した視点から脱却し、全体を見渡す力を取り戻す。第二に、全社戦略へのコンセンサスを形成すること。 組織全体で方向性を共有します。第三に、部門内での変革を推進すること。 各部門で具体的なアクションを起こす。第四に、全社戦略・変革施策をアップデートすること。 実行の中で学び、修正し続けます。このプロセスを阻む困難があります。3つの困難です。「多義性」の困難。 ある状況について複数の解釈が存在していても、その状態を捉えられなくなります。「複雑性」の困難。 ある事象の背後で複数の要因が絡み合い、状況が明確に認識されず、解決策もわかりにくくなります。「自発性」の困難。 変革の方向性を打ち出しても、現場で積極的に実行されなくなります。これらの困難を乗り越える鍵は何か。それは対話だ。企業変革と適応課題：人が変わるということここまでの議論で、「構造的無能化」や「企業変革の4つのプロセス」という、組織レベルの枠組みを見てきました。しかし、どれだけ立派なプロセスを設計しても、それだけで変革が進むわけではありません。なぜなら、変わるのは「組織」そのものではなく、組織の中にいる人間だからです。ここからは視点をもう一段インナーレイヤーに寄せます。企業変革の根っこには、必ず 「適応課題」＝人々の認識の枠組みの変容 が横たわっています。そして、この認識の変容には、「5つの重力」のような困難がまとわりついている。それが何なのかを、1つずつほどいていきます。なぜプロジェクトは失敗するのでしょうか。多くの人は、技術的な問題だと捉えます。設計が悪かった。実装に問題があった。技術的な解決策を探す。しかし、これらの解決策を導入しても、同じ問題が繰り返されます。なぜか。問題が技術的側面だけでなく、適応的側面を持つからだ。適応的側面とは何でしょうか。それは、人々の認識の枠組み、つまりナラティヴの問題です。組織のメンバーが固定化されたナラティヴに閉じこもっています。「品質より速度が重要です」「速度より品質が重要です」。このナラティヴの対立が、技術的な解決策を無効化します。どんなに優れたツールを導入しても、どんなに合理的なプロセスを設計しても、ナラティヴが変わらなければ、問題は解決しません。そして、ナラティヴを変えるには、対話が必要です。技術的問題と適応課題のスペクトラム私は、問題を2つの軸で捉えるようになった。技術的側面と適応的側面です。技術的側面とは、既存の知識と技術で解決できる部分。適応的側面とは、認識の枠組みの変容が必要な部分。重要なのは、これは二者択一ではなく、連続的なスペクトル上に存在するということです。純粋に技術的な問題の例。サーバーのレスポンスが遅い。データベースのクエリを最適化します。キャッシュを導入します。これで解決します。問題は外部にあります。解決策も外部にあります。純粋に適応的な課題の例。チームのコミュニケーションがうまくいかない。誰もが「相手が理解してくれません」と感じています。この問題を「コミュニケーションツールの問題」だと定義すれば、Slackを導入すれば解決するはずです。しかし、実際には解決しない。なぜなら問題の本質はツールではなく、互いの認識の違いにあるからだ。ただし、現実の問題の多くは、両方の側面を持っています。たとえば「技術的負債が増え続けています」という問題。一見技術的に見えますが、「品質と速度のどちらを優先するか」という価値観の問題、「リファクタリングに時間を使うことを許容するか」という組織文化の問題といった適応的側面も含みます。問題を見誤る典型的なパターンは、適応的側面を持つ問題に技術的解決策だけを当てはめることです。「ツールを導入したのに、なぜうまくいかないんだろう」。ツールだけが問題なのではなく、認識や関係性も問題なのだと気づかない。多くの組織の問題は、適応課題です。「イノベーションが生まれません」。これは、予算の問題でも、人材の問題でもない。リスクを取ることを恐れる文化の問題です。失敗を許容しない価値観の問題です。これを変えるには、組織の認識を変える必要があります。適応課題における変容の困難さ適応課題に直面したとき、人間は変化に時間を要します。なぜなら変化することは、一部の自分を失うことだからだ。 長年培ってきた考え方。慣れ親しんだ行動パターン。自分を定義してきた価値観。これらを手放すことは、怖い。言い換えれば、自分のナラティヴを手放すことです。「私はこういう人間です」という自己物語。「私たちはこういう組織です」という集団物語。これらのナラティヴは、アイデンティティの核です。だから、適応課題は、感情的な反応を伴う。論理的に説明しても、すぐには納得しない。データを示しても、即座には受け入れません。これは、頑固なのではありません。恐怖なのだ。この恐怖を乗り越えるには、何が必要でしょうか。対話です。一方的な説得ではありません。命令でもありません。対話を通じて、自分のナラティヴを相対化します。「私の見方は、絶対ではないだろう」と気づきます。他者のナラティヴに触れます。「そういう見方もあるのか」と理解します。そして、徐々に、自分のナラティヴを更新していきます。この変容は、対話なしには起きません。適応課題における5つの変容の困難良いアイデアを提示すれば、人は変わるだ。しかし、現実には変わりません。なぜか。変化には、5つの困難があるからです。この困難は、高くそびえ立つ障害物ではありません。むしろ、重力のように働きます。目には見えませんが、常に働いています。私たちを、元の場所に引き戻そうとします。どんなに優れたアイデアでも、この5つの困難を越えられなければ、人は変わりません。この5つの困難は、独立して存在するのではありません。互いに影響し合い、変化を阻む仕組みを形成しています。1つの困難を越えても、次の困難が待っています。5つすべてを理解しなければ、変化は起きません。第一の困難：頭の作り変え毎朝、同じ道を通って会社に行きます。信号の位置を覚えています。どこで曲がるか、体が覚えています。考えなくても、着きます。これが、慣れです。仕事も同じです。20年、30年かけて、物事の見方を学んできました。「こういう問題には、こう対処する」。瞬時に判断できます。考えなくても、答えが出ます。この慣れが、あなたの強みです。経験と呼ばれるものです。新しい考え方を受け入れるとは、この慣れた道を捨てることです。新しい道を覚え直すことです。でも、新しい道では迷います。間違えます。時間がかかります。だから、脳は嫌がります。「複雑すぎる」「よく分からない」。怠惰ではありません。効率を求める本能です。この困難を越えるには、いきなり全部の道を変えようとしてはいけません。「いつもの道の、この角を少し変えてみよう」。一部だけ変えます。慣れたら、また一部変えます。「あなたがやってきたことは、間違いではありません。ちょっと拡張するだけです」。こう言われると、安心します。第二の困難：暗闇への恐怖夜、真っ暗な部屋を歩くとき、あなたは慎重になります。手を前に伸ばします。障害物を探ります。何かにぶつからないか、不安です。明かりをつければ、普通に歩けます。でも、暗闇では怖い。新しい方針、新しいやり方。これは、暗闇を歩くようなものです。「うまくいくのか」「失敗したらどうなるのか」。答えが見えません。過去の経験も役に立ちません。予測ができません。だから、体が固くなります。心臓がドキドキします。頭が真っ白になります。だから、ここでは「全部を明るくしよう」としないことが大事になります。小さな懐中電灯で、一歩先だけ照らす。「まず、この小さな範囲で試そう」。失敗しても、被害は小さい。成功すれば、次の一歩が見えます。その繰り返し以外に、暗闇を抜ける方法はありません。第三の困難：自分の定義を変える痛み10年間、営業として働いてきました。顧客と話すのが好きです。契約が取れたときの達成感。売上目標を達成したときの誇り。これらが、あなたです。名刺には「営業部」と書いてあります。自己紹介するとき、「営業をやっています」と言います。あなたは、営業です。「これからはマネジメント職に」。この言葉を聞いたとき、何を感じるでしょうか。「私は営業じゃなくなるのか」。不安です。10年間、営業として生きてきました。営業の自分しか、知りません。営業じゃない自分は、誰なのでしょうか。自分が分からなくなります。この困難を越えるには、「営業を辞める」ではなく、「営業の経験を活かす」だ。「営業の経験は無くなりません。それは基盤です。その上に、新しいスキルを積み上げます」。こう言われると、自分は消えないと分かります。過去は捨てません。未来につながります。第四の困難：体に染みついた癖毎朝、目覚ましが鳴ります。あなたは無意識にスマホを取ります。メールをチェックします。考えていません。体が勝手に動きます。これが、習慣です。仕事でも同じです。資料を作るとき、いつものテンプレートを使います。会議の進め方も、いつも同じです。使い慣れたツール。決まった手順。考えなくても、できます。楽です。新しいやり方は、違います。毎回考えなければなりません。どうするんだっけ、と迷います。間違えます。遅くなります。疲れます。だから、体は元のやり方に戻ろうとします。「やっぱり、いつものやり方の方が早い」。そう感じます。この困難を越えるには、最初の遅さを許します。「新しいやり方は、最初は遅いです。でも、一ヶ月後には速くなります」。この移行期間を、我慢します。組織として、支援します。第五の困難：自分で決めたい気持ち子供の頃、親に「これを食べなさい」と言われました。嫌でした。でも、「何が食べたい」と聞かれて、同じものを選んだとき、喜んで食べました。人間は、自分で決めたいのです。職場でも同じです。上司が「この方法でやりなさい」と命令します。あなたは、反発します。たとえそれが良い方法でも、押し付けられると嫌です。なぜか。自分で決めていないからです。この困難を越えるには、命令ではなく、提案します。「こういう選択肢があります。どう考えますか」。相手を、意思決定に参加させます。「一緒に考えましょう」。相手が自分で気づき、自分で選びます。そのとき、反発は消えます。同じ結論でも、自分で選んだら、納得します。この5つの困難は、別々に立っているのではありません。連動しています。第一の困難を越えて、新しい考え方を理解しても、第二の困難の不安が残ります。第三の困難の「自分が分からなくなる」恐怖も待っています。第四の困難の習慣の引力が、あなたを元に戻そうとします。そして、第五の困難。自分で決めていないと感じれば、すべてが無駄になります。だから、変革を推進する者は、5つすべてを理解しなければなりません。1つだけ対処しても、他の困難が残ります。人は変わりません。5つすべてに、丁寧に向き合う必要があります。これが、変容のメカニズムです。ここまで見てきた「5つの困難」は、私たちが変わろうとするときに働く重力でした。頭の作り変えへの抵抗、暗闇への恐怖、自己定義の揺らぎ、体に染みついた癖、そして自分で決めたいという欲求。では、この重力に抗いながら、どうやって認識の枠組みを変えていけばいいのか。そこで必要となる具体的なプロセスこそが、対話です。対話は、単なる話し合いの技術ではなく、認識の変容を起こすための手順そのものです。ここからは、対話がどのような段階を経て認識を変えていくのかを、「4つの段階」として見ていきます。対話による変容の促進対話が必要なのは、まさにこの適応課題においてです。技術的問題なら、専門家が答えを出せばいい。でも、適応課題は、当事者全員が変わらなければ解決しない。変わるためには、まず現在の認識、つまり固定化されたナラティヴを可視化しなければならない。ここで大事なのは、変化を強制できないと理解しておくことです。 説得しようとすればするほど、心理的反発が強まる。ナラティヴを否定されることは、自己を否定されることだからだ。だから、対話が必要になります。対話とは、相手を説得する行為ではありません。相手が自分自身を説得できるよう手助けする行為です。「私たちは、なぜこのパターンを繰り返しているのか」。「私たちは、どんな前提で動いているのか」。これらの問いに向き合うこと。これは、組織のナラティヴを問い直す作業です。対話を通じて、集団のナラティヴが可視化されます。「ああ、私たちはリスクを避けることを最優先にしてきたのです」。この気づきが、変化の第一歩になります。そして、対話を通じて、新しいナラティヴが創発します。「リスクを取らないことも、リスクではないか」。互いの視点を統合することで、誰も一人では到達できなかった地平が開けます。これが、ナラティヴの牢獄から抜け出す唯一の道です。適応課題は、対話なしには解決しない。 命令では解決しない。説得では解決しない。強制では解決しない。なぜなら解決には、当事者全員の認識の変容が必要だからだ。認識の変容は、対話を通じてのみ起きます。対話のプロセス：認識の変容の四段階対話は、どのように起きるのでしょうか。どのようなプロセスを経て、認識は変容するのでしょうか。多くの対話のフレームワークは、行動のステップを示す。傾聴します。質問します。要約します。しかし、これは表面的です。本当の対話は、もっと深い層で起きています。認識の変容の層で。対話のプロセスを4つの段階として捉え直してみたい。第一段階：自己の相対化対話が始まる前、私たちは自分の認識を絶対視しています。「世界はこうです」と思っています。正確には、「私が見ている世界」と「世界そのもの」を区別していない。第一段階は、この区別に気づくことです。「私が見ているのは、世界の一つの側面に過ぎない」と認識すること。 これを、自己の相対化と呼ぶ。どうやって相対化が起きるのでしょうか。最も効果的なのは、自分とまったく違う視点に出会うことです。同じ状況を見ているのに、相手はまったく違う解釈をしています。この衝突が、相対化のきっかけになります。「この設計は複雑すぎます」と思っていました。しかし、相手は「この設計は適切な抽象化です」と言います。最初は「相手が間違っています」と感じます。ところが、相手の説明を聞いているうちに、何かがひっかかる。「もしかして、私が見ていないものを、この人は見ているのだろう」。この瞬間、相対化が始まります。「私はこう見ています。でも、世界はもっと複雑だろう」。この距離感が、対話の始まりになります。自己の相対化は、謙虚さを生む。「私は確信していない」と認めることができるようになります。「私の見方は、1つの可能性に過ぎない」と受け入れることができるようになります。この謙虚さがなければ、対話は始まらない。第二段階：他者の世界への接近自己を相対化したとき、他者の世界が見えてきます。相手もまた、1つの認識の体系を持っています。相手の言葉は、その体系から生成されています。第二段階は、この相手の認識の体系に近づくことです。相手の世界を、内側から理解しようと試みること。 これを、他者の世界への接近と呼ぶ。接近するとは、相手の前提を探ることです。「なぜそう考えるのですか」と問う。「どういう経験から、その結論に至ったのですか」と尋ねます。相手の認識の枠組みを、少しずつ解読していく。「この設計は適切な抽象化です」と言う相手。なぜそう考えるのでしょうか。相手に聞いてみます。すると、相手は過去のプロジェクトの話をします。要件が頻繁に変わるプロジェクトでした。柔軟な設計にしていたおかげで、変更に対応できました。その経験から、「抽象化は投資です」という信念が生まれた。この話を聞いて、あなたは理解します。「ああ、この人は『抽象化』を『変更への備え』として見ているのです」。一方、あなたは「抽象化」を「複雑さの源」として見ていました。同じ言葉、違う意味。この差異が、可視化されます。接近は、共感とは違います。共感は、感情的な同調です。しかし、接近は、認識論的な理解です。「あなたの認識の構造が分かる」。感情は一致しなくても、認識は理解できます。接近することで、相手の言葉の真意が分かります。対立が和らぐ。「この人は私を攻撃しているわけではない。ただ、違う視点から見ているだけです」。第三段階：差異の構造化自分の世界と相手の世界を理解したとき、次の段階が来る。2つの世界の違いを、明確に構造化することです。第三段階は、差異を整理し、パターンを見出すこと。 これを、差異の構造化と呼ぶ。構造化とは、「何が違うのか」を言語化することです。漠然と「意見が違う」ではなく、「どこが、なぜ、違うのか」を明確にします。あなたと相手の対立を、構造化しよう。まず、事実の層では一致しています。「このコードは複数の抽象レイヤーを持っています」。これは、どちらも認めます。次に、解釈の層で分かれます。あなたは「複数の抽象レイヤーは、理解を困難にする」と解釈します。相手は「複数の抽象レイヤーは、変更を容易にする」と解釈します。そして、価値観の層でも分かれます。あなたは「即座の理解可能性」を重視します。相手は「長期的な柔軟性」を重視します。さらに、経験の層でも違います。あなたは過去に複雑なコードで苦労しました。相手は過去に硬直的な設計で苦労しました。この構造化によって、対立の本質が見えます。これは、技術的な議論ではなかった。価値観の対立でした。 どちらの価値観も正しい。でも、優先順位が違います。その優先順位の違いは、異なる経験から生まれています。構造化すると、対立が外在化されます。「AとBの対立」ではなく、「即座の理解可能性 vs 長期的な柔軟性」という構造の問題になります。人格の対立から、構造の対立へ。これは対話を生産的にします。第四段階：統合への創発そして最後の段階。2つの世界観を統合する、新しい視点が創発します。第四段階は、どちらの視点も含みつつ、どちらでもない第三の地平を見出すこと。 これを、統合への創発と呼ぶ。統合は、妥協ではありません。妥協とは、両者が譲り合って中間点を取ることです。これは取引です。統合とは、より高次の視点を見出すことです。AかBかではなく、AとBを包含するCを創造することです。あなたと相手の対立に戻ろう。即座の理解可能性 vs 長期的な柔軟性。どちらも大切です。では、どうするでしょうか。問いを変えます。「どちらを選ぶか」ではなく、「どちらも実現する方法はないか」と。この問いが、創発を促す。議論を続けるうちに、アイデアが生まれます。「コア部分は抽象化します。でも、抽象化のレイヤーは最小限にします。各レイヤーの責務を明確にドキュメント化します。さらに、具体的な使用例をテストコードで示す」。この解決策は、あなたの懸念に応えています。ドキュメントとテストによって、理解可能性が保たれます。同時に、相手の懸念にも応えています。抽象化によって、柔軟性が保たれます。これが統合です。 どちらの視点も否定せず、両方を満たす新しい解を見出す。この解は、対話の前には存在しなかった。あなた一人では到達できなかった。相手一人でも到達できなかった。2つの視点が出会い、対話を通じて、創発しました。統合への創発は、対話の究極の目標です。しかし、必ずしも達成されるとは限らない。時には、差異の構造化で終わることもあります。それでもいい。統合できなくても、理解は深まっています。論破という暴力対話の対極にあるものについて語ろう。論破です。論破とは、相手を言い負かすことです。相手の主張の矛盾を指摘します。相手の論理の欠陥を突く。相手を沈黙させます。「勝ちました」と感じます。なぜ人は論破したがるのでしょうか。それは、即座の快楽があるからだ。相手を打ち負かす瞬間、ドーパミンが放出されます。優越感を感じます。自己肯定感が高まる。この快楽が、論破を強化します。論破は、対話を殺す。 いや、対話を殺すだけではありません。関係を壊します。信頼を失います。学習機会を逃す。最終的には、自分自身を孤立させます。論破された相手は、何を感じるでしょうか。屈辱です。「自分は間違っていました」という敗北感。「この人とは、もう話したくない」という拒絶。論破によって、あなたは1つの議論には勝っただろう。しかし、相手との対話の可能性を永久に失いました。より悪いことに、論破は自分自身の成長も止めます。なぜなら論破する人は、相手から学ぶ機会を放棄しているからだ。相手の視点を理解しようとしない。相手の経験から学ぼうとしない。ただ、相手の間違いを見つけることに集中します。論破に依存すると、世界が狭くなります。対話可能な相手が減っていく。人々は、あなたを避けるようになります。あなたは孤立します。対話と論破の違いは何か。目的が違います。論破の目的は、勝利です。相手を打ち負かすこと。一方、対話の目的は、相互理解です。共に学ぶこと。姿勢が違います。論破する人は、相手を敵と見ます。対話する人は、相手をパートナーと見ます。結果が違います。論破の後には、勝者と敗者が残ります。対話の後には、両者の成長が残ります。もしあなたが「正しさ」を証明したいなら、論破すればいい。しかし、もしあなたが「真実」に近づきたいなら、対話しなければならない。なぜなら真実は一人の人間の視点に収まらないからだ。真実は複数の視点の交差点にあります。論破から対話へのシフトは、パラダイムの転換です。ゼロサムゲームから、ポジティブサムゲームへ。思考の終わりから、思考の始まりへ。自己の強化から、自己の拡張へ。このシフトには、勇気が要る。「勝つ」という快楽を手放す勇気。「正しい」という確信を疑う勇気。「変わる」という可能性を受け入れる勇気。この勇気こそが、成長の源です。AI時代における対話の価値生成AIが登場して、私たちの仕事は変わりました。コードを書く速度が上がりました。ドキュメントを作成する時間が減りました。質問に対する答えが、即座に返ってくるようになった。人間同士の対話は、不要になったのでしょうか。AIに質問すれば答えが返ってきます。AIと議論すれば、論理的な反論が返ってきます。人間と対話する必要が、あるのでしょうか。あります。 それも、これまで以上に。なぜならAIとの対話と人間との対話は、現時点では本質的に異なる性質を持つからだ。AIとの対話と人間との対話の違いは、以下の軸で捉えられます。経験の固有性。AI：訓練データのパターンから応答を生成します。人間：固有の人生経験から応答が生まれます。この違いは、応答の予測可能性に影響します。AIの応答は洗練されていますが、パターンの組み合わせです。人間の応答は、データのパターンでは予測できない個別性を持ちます。相互的変容の有無。AI：対話によって自身の認識の枠組みは変わりません(現時点)。人間：対話によって互いの認識が変容しうる。AIに話を聞いてもらっても、「理解された」という実感は限定的です。なぜならAIには「あなたの話が私の認識を変えた」という相互的な影響がないからだ。一方、人間同士では「あなたの話を聞いて、私は何かを感じました」という実存的な承認が生まれます。関係性の蓄積。AI：各セッションは独立しています。人間：対話の履歴が信頼や理解の基盤となります。2つの異なる人生経験が衝突し、融合し、まったく新しい視点が創発します。この過程は、関係性の深まりを前提とします。実存的リスク。AI：どんな意見を言っても関係性にリスクはありません。人間：意見の衝突が関係性を損なう可能性があります。否定されると、傷つく。この摩擦が、人間との対話を難しくします。しかし、この摩擦の中にこそ、成長があります。重要なのは、AIと人間のどちらが優れているかではありません。それぞれの特性を理解し、目的に応じて使い分けることです。情報の整理、アイデアの初期生成、論理のチェックなどはAIが効率的です。一方、認識の変容、実存的な対話、信頼関係の構築などは、人間同士の対話が適しています。しかし、ここにリスクもあります。AIとの対話は、楽です。予測可能です。抵抗がない。反論されても、傷つかない。一方、人間との対話は、難しい。予測不可能です。摩擦があります。否定されると、傷つく。だから、私たちはAIとの対話に逃げる危険があります。人間との対話を避け、AIとだけ話すようになります。これは、対話筋力の退化です。AI時代だからこそ、意識的に人間と対話しなければならない。 不快でも、難しくても、予測不可能でも。なぜなら、その摩擦の中にこそ、成長があります。創造があります。人間性があります。おわりに会議室の二人は、まだ平行線でした。Aは「拡張性」を主張し続けます。Bは「納期」を主張し続けます。どちらも譲らない。どちらも、相手を理解しようとしない。私は、口を開きました。「すみません、確認したいのですが。Aさんが『拡張性』と言うとき、具体的にどんなリスクを心配しているんですか」Aは少し驚いた顔で答えた。「前のプロジェクト、機能追加のたびに大規模な修正を要し、半年間リリース停止になったんです。だから...」「なるほど。では、Bさんが『納期』を強調するのは、どういう背景があるのですか」Bも答えた。「顧客との契約で、この機能のリリース日が明示されていて。遅れると、ペナルティが発生するんです」沈黙が流れた。Aが言いました。「契約の話、知りませんでした。それなら、確かに納期は守らないといけないですね」Bも言いました。「前のプロジェクトでそんなことがあったんですね。それは大変でしたね。じゃあ、最小限の拡張性を確保する方法、一緒に考えてほしいです」会議室の空気が、少し変わりました。対立から、対話へ。対話は、魔法ではありません。 すべての問題を解決するわけではありません。意見の対立が消えるわけでもない。でも、対話があれば、前に進めます。互いを理解しながら、解を探せます。私たちの多くは、対話の仕方を教わっていない。学校でも、職場でも。だから、本能的に反応します。防御します。攻撃します。関係が壊れていく。でも、対話は学べます。 訓練できます。一歩ずつ、積み重ねられます。完璧である必要はない。不完全な対話でも、無対話よりはるかにましです。まず、自分の自動反応を中断すること。「今、私は防御的になっています」と気づくこと。一呼吸置くこと。次に、相手の世界を理解しようとすること。「なぜそう考えるのか」と問うこと。相手の背景、経験、価値観を探ること。そして、自分のナラティヴから降りること。「私の見方は、絶対ではありません」と認めること。新しい視点に開かれていること。対話は、時間がかかる。効率的ではありません。しかし持続可能です。 対話を通じて築かれた理解は、表面的な合意よりもはるかに強い。対話を通じて生まれた解は、押し付けられた解よりもはるかに実行可能です。そして、対話は、私たち自身を変えます。相手の視点に触れることで、自分の認識が広がる。自分の限界に気づく。新しい可能性が見えます。対話は、自己を拡張する行為です。技術だけでは、組織は動かない。プロセスだけでは、イノベーションは生まれません。ツールだけでは、問題は解決しない。必要なのは、人と人との対話です。異なる世界観が出会い、衝突し、融合する場です。エンジニアとして、私たちは論理を重視します。データを重視します。効率を重視します。これは大切です。しかし、それだけでは足りない。人間の認識の複雑さ、関係性の重要性、対話の力。 これらを理解しなければ、どんなに優れた技術も、組織の中で機能しない。対話は完成しない。永遠に未完成です。でも、試み続けることができます。 その試みの一歩一歩が、こじれた現場に、小さな橋を架けていく。あなたの次の一歩は、何か。今日、誰と対話するでしょうか。その対話の中で、あなたはどう変わるでしょうか。答えは、対話の中にあります。参考文献対話の実践力: ケアを極める聞き方・話し方作者:小瀬古伸幸中央法規出版Amazon学びをつくる問いと対話のデザイン: 探究・研修・大人の学び作者:福島 創太学文社Amazon優れたリーダーはなぜ、対話力を磨くのか？作者:堀井悠,松本悠幹クロスメディア・パブリッシング(インプレス)Amazonダイアローグ――対立から共生へ、議論から対話へ作者:デヴィッド・ボーム英治出版Amazon問いの編集力 思考の「はじまり」を探究する作者:安藤昭子ディスカヴァー・トゥエンティワンAmazon私たちはどう学んでいるのか　――創発から見る認知の変化 (ちくまプリマー新書)作者:鈴木宏昭筑摩書房Amazon創造と創発の心理学〈下〉: 越境がもたらす癒しと変容作者:吉野 大輔学文社Amazon創造と創発の心理学〈上〉: つながりがもたらす新たな秩序作者:吉野 大輔学文社Amazon「良い質問」を40年磨き続けた対話のプロがたどり着いた 「なぜ」と聞かない質問術作者:中田 豊一ダイヤモンド社Amazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazon企業変革のジレンマ 「構造的無能化」はなぜ起きるのか作者:宇田川元一日経BPAmazon私文ホワイトカラーが AI・コンサルに仕事を奪われない働き方戦略作者:株式会社板橋　東京中央支店かんき出版Amazonだから僕たちは、組織を変えていける ――やる気に満ちた「やさしいチーム」のつくりかた【ビジネス書グランプリ2023「マネジメント部門賞」受賞！】作者:斉藤徹クロスメディア・パブリッシング（インプレス）AmazonDD(どっちもどっち)論 「解決できない問題」には理由がある (WPB eBooks)作者:橘玲集英社AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazon「わかりあえない」を越える――目の前のつながりから、共に未来をつくるコミュニケーション・NVC作者:マーシャル・B・ローゼンバーグ海士の風Amazon有と無: 見え方の違いで対立する二つの世界観作者:細谷功株式会社dZEROAmazon「無理」の構造　この世の理不尽さを可視化する作者:細谷功株式会社dZEROAmazonはじめての人類学: 講談社現代新書作者:奥野 克巳AudibleAmazon文化人類学入門（増補改訂版） (中公新書)作者:祖父江孝男中央公論新社Amazonアイデア資本主義 文化人類学者が読み解く資本主義のフロンティア作者:大川内 直子AudibleAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[pyscnを利用して綺麗なコードを目指してみよう]]></title>
            <link>https://zenn.dev/akasan/articles/58772bf4b72fd7</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/58772bf4b72fd7</guid>
            <pubDate>Tue, 18 Nov 2025 15:10:17 GMT</pubDate>
            <content:encoded><![CDATA[今回はpyscnを使ってPythonコードの状態を分析する方法を調べてみました。 pyscnとは？pyscnはPython Code Quality Analyzerということで、Pythonコードの品質を検知するためのツールになります。特徴としては以下があるようです。CFGベースのデッドコード検知: if/elseなどで到達しないコードがあるか検知APTEDとLSHによるクローン検知: ツリー編集距離によるリファクタリング対象の特定Coupling metrics (CBO): アーキテクチャの品質及びモジュールの依存関係の追跡関数の複雑さ分析: 分解が必要な関数を発見...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[上手に待つ技術：Rust Edition 2024で学ぶ非同期処理入門]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/18/155416</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/18/155416</guid>
            <pubDate>Tue, 18 Nov 2025 06:54:16 GMT</pubDate>
            <content:encoded><![CDATA[はじめにプログラミングにおいて「待つ」処理は避けられません。サーバからのレスポンスを待つ、データベースの処理が終わるのを待つ、ファイルの読み込みが完了するのを待つ——この「待ち時間」の使い方が、プログラムの性能を大きく左右します。非同期処理とは、ある処理の完了を待たずに次の処理を開始する技術です。待っている間に他のタスクを処理することで、限られたリソースを効率的に活用できます。本記事では、以下の内容を解説します。Bashでの基本的な非同期処理Rust Edition 2024における非同期プログラミングの基礎実践的なコード例とパターン実際に動作するコード例を通じて、非同期処理の基礎から実践的なパターンまでを学んでいきましょう。Bashでの非同期処理シェルスクリプトでも基本的な非同期処理が可能です。まずは簡単な例から見ていきましょう。#!/bin/bash# バックグラウンドで実行echo "タスク1を開始..."sleep 3 &pid1=$!echo "タスク2を開始..."sleep 2 &pid2=$!echo "タスク3を開始..."sleep 4 &pid3=$!# すべてのタスクの完了を待つecho "すべてのタスクが完了するのを待っています..."wait $pid1echo "タスク1が完了しました"wait $pid2echo "タスク2が完了しました"wait $pid3echo "タスク3が完了しました"echo "すべて完了！"このスクリプトでは、& をコマンドの最後につけることで、そのコマンドをバックグラウンドで実行しています。wait コマンドで特定のプロセスの終了を待ちます。実用的な例：複数サーバの監視より実用的な例として、複数のサーバの死活監視を同時に行うスクリプトを見てみましょう。#!/bin/bashcheck_server() {    local server=$1    local start_time=$(date +%s)        if ping -c 1 -W 2 "$server" > /dev/null 2>&1; then        local end_time=$(date +%s)        local duration=$((end_time - start_time))        echo "$server: OK (${duration}秒)"    else        echo "$server: 到達不可"    fi}# 複数のサーバを同時にチェックservers=("google.com" "github.com" "stackoverflow.com" "rust-lang.org")for server in "${servers[@]}"; do    check_server "$server" &done# すべての完了を待つwaitecho "すべてのチェックが完了しました"順次実行すれば8秒かかるところを、並行実行で2秒程度に短縮できます。Bashの非同期処理の限界ただし、Bashの非同期処理には以下のような限界があります。プロセス単位での並行処理のため、オーバーヘッドが大きいエラーハンドリングが煩雑状態の共有が難しい細かい制御ができないより洗練された非同期処理には、プログラミング言語レベルでのサポートが必要となります。なぜ非同期処理が重要なのかハードウェアの性能向上の鈍化ハードウェアの性能向上は、2010年代以降、鈍化しています。NVIDIAのCEO Jensen Huangが2017年5月のCOMPUTEX TAIPEIで「ムーアの法則は死んだ」と述べました。単純にクロック速度を上げることで性能を向上させる時代は終わったのです。つまり、これ以上は単に「速いコンピュータを買えばいい」という解決策が使えなくなってきています。ソフトウェア要求の増大一方で、ソフトウェアに対する要求は増大し続けています。マイクロサービスアーキテクチャでは、システム内でのI/O呼び出しの回数が激増Webアプリケーションは、複数のAPIを並行して呼び出す必要があるモバイルアプリは、限られたリソースで複数のタスクを処理しなければならないここで重要になるのが非同期プログラミングです。並行処理・並列処理・非同期処理の違いと使い分けこれらの用語は混同されやすいですが、それぞれ異なる概念を指します。並行処理（Concurrency）複数のタスクが論理的に同時進行しているように見せる技術です。シングルコアCPUでも実現可能で、タスクを高速に切り替えながら実行します。例えば、レストランで一人のシェフが玉ねぎを炒めている間にトマトを切る動作が並行処理に相当します。参考：fastapi.tiangolo.com並列処理（Parallelism）複数のタスクが物理的に同時実行される技術です。マルチコアCPUを活用し、実際に複数の処理が同じ瞬間に実行されます。複数のシェフがそれぞれ別の料理を作る状態が並列処理に相当します。目的は処理速度の向上です。参考：freak-da.hatenablog.com非同期処理（Asynchronous）ある処理の完了を待たずに次の処理を開始する技術です。I/O操作のような「待ち時間」が多い処理に特に有効で、ネットワークリクエストのレスポンスを待っている間に他の処理を進められます。目的は待ち時間の有効活用です。まとめこれらは異なる次元の概念であり、組み合わせて使用できます。非同期処理を並行的に実行したり、並列に実行したりできます。CPUのコア数を増やさなくても、非同期プログラミングを用いれば性能を向上させることができます。サーバからのレスポンスを待っている時間があるなら、その間に他のタスクを処理すればよいのです。参考：qiita.comRust Edition 2024における非同期処理Rustの非同期処理は、2025年2月20日にリリースされたRust 1.85.0で、Edition 2024が安定化されました。これはRust史上最大規模のEditionとなりました。参考：blog.rust-lang.orgEdition 2024の哲学Rust Editionは、Rustの後方互換性を保ちながら破壊的変更を導入するための仕組みです。Rust 1.0がリリースされた際、チームは「1.xのどのバージョンでコンパイルできたコードは、将来の1.yバージョンでも問題なくコンパイルできる」という約束をしました。しかし、言語の進化には破壊的変更が必要な場合もあります。Editionはこの問題を解決します。参考：doc.rust-lang.orgEdition 2024は、多くの小さな改善の集合体です。大きな単一機能ではなく、言語全体の洗練を目指しています。言語は停滞せず、かつ急激な変化も避けるという健全な進化を示しています。参考：bertptrs.nlEdition 2024の主要な変更点1. Async Closures の段階的な導入これは非同期プログラミングにおける重要な機能の1つです。Edition 2024では基本的なasync closuresがサポートされました。ただし、async Fn()トレイト構文は2025年1月時点でまだunstableです。実際にはFn() -> impl Futureの形式で記述する必要があります。use std::time::Duration;// ついに可能になった！let async_closure = async || {    tokio::time::sleep(Duration::from_secs(1)).await;    "完了".to_string()};// AsyncFnトレイトを使った高階関数// 注：async Fn()構文はまだunstableのため、以下のように記述しますasync fn process_with_async_closure<F, Fut>(f: F) -> Stringwhere    F: Fn() -> Fut,    Fut: std::future::Future<Output = String>,{    f().await}#[tokio::main]async fn main() {    let result = async_closure().await;    println!("結果: {}", result);        let result = process_with_async_closure(async || {        "非同期クロージャ".to_string()    }).await;    println!("結果: {}", result);}これまでは、非同期のクロージャを書くために複雑な回避策が必要だったが、Edition 2024ではネイティブにサポートされるようになった。これにより、高階関数を使った非同期プログラミングが大幅に簡潔になる。参考：medium.com2. async fn in traits の完全サポートこれはRust 1.75.0で安定化された機能だが、Edition 2024の文脈で完全に統合された。use std::time::Duration;// これがついに標準機能に！trait AsyncService {    async fn process(&self, data: String) -> Result<String, Box<dyn std::error::Error>>;    async fn validate(&self, input: &str) -> bool;}struct MyService;impl AsyncService for MyService {    async fn process(&self, data: String) -> Result<String, Box<dyn std::error::Error>> {        tokio::time::sleep(Duration::from_secs(1)).await;        Ok(format!("処理完了: {}", data))    }        async fn validate(&self, input: &str) -> bool {        !input.is_empty()    }}// ジェネリックな非同期関数でも使えるasync fn use_service<T: AsyncService>(service: &T) {    match service.process("データ".to_string()).await {        Ok(result) => println!("{}", result),        Err(e) => eprintln!("エラー: {}", e),    }}この機能により、トレイトベースの抽象化が非同期コードでも自然に使えるようになった。Niko Matsakisは2024年の初めに「async fn in traitsはAsync Rustのハードモードを終わらせる基盤」と述べている。参考：smallcultfollowing.com3. PreludeへのFutureとIntoFutureの追加// もうuseステートメントが不要！// use std::future::Future; // ← 不要になったasync fn my_future() -> i32 {    42}// FutureもIntoFutureもpreludeに含まれているので// そのまま使えるfn process_future<F: Future<Output = i32>>(future: F) {    // ...}これは小さな変更に見えるが、非同期コードを書く際の摩擦を大幅に減らす。4. RPIT（Return Position impl Trait）のライフタイム捕捉ルールの改善// Edition 2021での問題fn old_way(x: &str) -> impl Future<Output = String> {    async move {        // xのライフタイムが正しく捕捉されない場合があった        x.to_string()    }}// Edition 2024での改善fn new_way(x: &str) -> impl Future<Output = String> {    async move {        // ライフタイムが適切に捕捉される        x.to_string()    }}// use<..> で明示的な制御も可能fn explicit_capture<'a>(x: &'a str) -> impl Future<Output = String> + use<'a> {    async move {        x.to_string()    }}この改善により、非同期関数から返されるimpl Future型のライフタイム推論がより直感的になった。参考：www.heise.de5. 一時変数のスコープ改善// if let での一時変数のドロップタイミングが改善async fn process() {    if let Some(data) = fetch_data().await {        // Edition 2024では、dataはこのブロック内でのみ有効        println!("{}", data);    } // ← dataはここでドロップされる}// tail expressionでの改善async fn compute() -> i32 {    let result = calculate().await;    result * 2  // この一時変数のスコープも改善された}これは非同期コードにおける「一時的な値がスコープ外になるまで保持される」問題を解決します。以前はコンパイルエラーになっていたコードが、Edition 2024では正しく動作します。6. unsafeの厳格化// Edition 2024では、extern blockはunsafeマーク必須unsafe extern "C" {    fn external_function();}// 環境変数の操作もunsafeにunsafe {    std::env::set_var("KEY", "value");}unsafeの範囲がより明確になり、安全でない操作がコード中で目立つようになった。これにより、コードレビュー時に安全性を検証しやすくなる。Edition 2024への移行[package]name = "my-async-app"version = "0.1.0"edition = "2024"  # ← ここを変更[dependencies]tokio = { version = "1", features = ["full"] }多くの場合、cargo fixで自動的に移行できる：cargo fix --edition実践例：Edition 2024の機能を使ったコードuse std::time::Duration;// Async closureを使った例async fn process_items<F, Fut>(items: Vec<String>, processor: F) -> Vec<String>where    F: Fn(String) -> Fut,    Fut: std::future::Future<Output = String>,{    let mut results = Vec::new();    for item in items {        results.push(processor(item).await);    }    results}// Async trait methodを使った例trait DataProcessor {    async fn process(&self, data: &str) -> String;}struct UppercaseProcessor;impl DataProcessor for UppercaseProcessor {    async fn process(&self, data: &str) -> String {        tokio::time::sleep(Duration::from_millis(100)).await;        data.to_uppercase()    }}#[tokio::main]async fn main() {    // Async closureの使用    let items = vec!["hello".to_string(), "world".to_string()];    let results = process_items(items, |item| async move {        format!("処理済み: {}", item)    })    .await;        println!("結果: {:?}", results);        // Async traitの使用    let processor = UppercaseProcessor;    let result = processor.process("rust 2024").await;    println!("変換結果: {}", result);}非同期Rustのベストプラクティス（2024-2025）1. ブロッキング操作を避ける// ❌ 悪い例#[tokio::main]async fn main() {    tokio::spawn(async {        // std::thread::sleepはスレッドをブロックする        std::thread::sleep(Duration::from_secs(5));    });}// ✅ 良い例#[tokio::main]async fn main() {    tokio::spawn(async {        // tokio::time::sleepは非同期        tokio::time::sleep(Duration::from_secs(5)).await;    });}Tokioのような非同期ランタイムでは、ブロッキング操作は他のタスクの実行を妨げる。常に非同期版の関数を使用すること。参考：blog.poespas.me2. CPU集約的な処理は別スレッドでuse tokio::task;fn cpu_intensive_work(n: u64) -> u64 {    // フィボナッチ数の計算など    (0..n).sum()}#[tokio::main]async fn main() {    // CPU集約的な処理はspawn_blockingで    let result = task::spawn_blocking(|| {        cpu_intensive_work(1_000_000)    }).await.unwrap();        println!("結果: {}", result);}非同期ランタイムはI/O待機に最適化されている。CPU集約的なタスクはspawn_blockingを使って別スレッドプールで実行します。参考：medium.com3. 適切なランタイム設定// シングルスレッドランタイム（軽量）#[tokio::main(flavor = "current_thread")]async fn main() {    // 単純なI/O処理に適している}// マルチスレッドランタイム（デフォルト）#[tokio::main(flavor = "multi_thread", worker_threads = 4)]async fn main() {    // 多数の並行タスクがある場合}アプリケーションの特性に応じてランタイムを選択します。4. エラーの適切な伝搬use anyhow::Result;async fn step1() -> Result<String> {    Ok("ステップ1完了".to_string())}async fn step2() -> Result<String> {    Ok("ステップ2完了".to_string())}async fn process() -> Result<()> {    let result1 = step1().await?;    let result2 = step2().await?;        println!("{}", result1);    println!("{}", result2);        Ok(())}#[tokio::main]async fn main() {    if let Err(e) = process().await {        eprintln!("エラー: {}", e);    }}?演算子を活用し、エラーを適切に伝搬させる。anyhowクレートは便利なエラーハンドリングを提供します。5. Send境界の理解use std::sync::Mutex;use tokio::sync::Mutex as TokioMutex;// ❌ 悪い例：std::sync::MutexGuardはSendではないasync fn bad_example() {    let data = Mutex::new(0);    let guard = data.lock().unwrap();        // これはコンパイルエラー    // some_async_function().await;}// ✅ 良い例：tokio::sync::Mutexを使用async fn good_example() {    let data = TokioMutex::new(0);    let guard = data.lock().await;        // これは問題ない    some_async_function().await;}マルチスレッドランタイムでは、awaitポイントを跨ぐデータはSendでなければならない。tokio::syncの型を使用すること。参考：www.shuttle.dev実践：非同期HTTPリクエストで性能を比較実際のコードで、非同期の威力を確認してみよう。依存関係の設定[dependencies]tokio = { version = "1", features = ["full"] }reqwest = { version = "0.11", features = ["json"] }同期的なアプローチuse std::time::Instant;use reqwest::Error;#[tokio::main]async fn main() -> Result<(), Error> {    let url = "https://3-shake.com/";    let start_time = Instant::now();    // 順番に4回リクエスト    for i in 1..=4 {        let response = reqwest::get(url).await?;        println!("リクエスト {} 完了: ステータス {}", i, response.status());    }    let elapsed = start_time.elapsed();    println!("合計時間: {} ms", elapsed.as_millis());    // 出力例: 合計時間は環境により変動（概ね数百ms程度）    Ok(())}非同期的なアプローチuse std::time::Instant;use reqwest::Error;#[tokio::main]async fn main() -> Result<(), Error> {    let url = "https://3-shake.com/";    let start_time = Instant::now();    // 4つのリクエストを同時に実行    let (r1, r2, r3, r4) = tokio::join!(        reqwest::get(url),        reqwest::get(url),        reqwest::get(url),        reqwest::get(url),    );    // 結果を確認    println!("リクエスト1: {:?}", r1.map(|r| r.status()));    println!("リクエスト2: {:?}", r2.map(|r| r.status()));    println!("リクエスト3: {:?}", r3.map(|r| r.status()));    println!("リクエスト4: {:?}", r4.map(|r| r.status()));        let elapsed = start_time.elapsed();    println!("合計時間: {} ms", elapsed.as_millis());    // 出力例: 合計時間は環境により変動    Ok(())}並行実行による性能改善ネットワークのレスポンスを待っている間、CPUは他のリクエストを処理できます。実際の性能改善はネットワーク状況、サーバーの同時接続制限、HTTP/2の利用状況などに依存しますが、理想的な条件下では、並行実行により順次実行と比べて大幅な時間短縮が可能です。より実践的な例：並行データ処理複数のAPIからデータを取得して統合する、よくあるシナリオを見てみましょう。use reqwest::Error;use serde::Deserialize;use std::time::Instant;#[derive(Deserialize, Debug)]struct User {    id: i32,    name: String,    email: String,}#[derive(Deserialize, Debug)]struct Post {    id: i32,    title: String,    body: String,}#[derive(Debug)]struct UserProfile {    user: User,    posts: Vec<Post>,    comments_count: usize,}async fn fetch_user(user_id: i32) -> Result<User, Error> {    let url = format!(        "https://jsonplaceholder.typicode.com/users/{}",         user_id    );    reqwest::get(&url)        .await?        .json::<User>()        .await}async fn fetch_user_posts(user_id: i32) -> Result<Vec<Post>, Error> {    let url = format!(        "https://jsonplaceholder.typicode.com/posts?userId={}",         user_id    );    reqwest::get(&url)        .await?        .json::<Vec<Post>>()        .await}async fn fetch_comments_count(user_id: i32) -> Result<usize, Error> {    let url = format!(        "https://jsonplaceholder.typicode.com/comments?postId={}",         user_id    );    let comments = reqwest::get(&url)        .await?        .json::<Vec<serde_json::Value>>()        .await?;    Ok(comments.len())}async fn get_user_profile(user_id: i32) -> Result<UserProfile, Error> {    // 3つのAPIを並行して呼び出す    let (user_result, posts_result, comments_result) = tokio::join!(        fetch_user(user_id),        fetch_user_posts(user_id),        fetch_comments_count(user_id),    );    Ok(UserProfile {        user: user_result?,        posts: posts_result?,        comments_count: comments_result?,    })}#[tokio::main]async fn main() -> Result<(), Error> {    let start = Instant::now();        let profile = get_user_profile(1).await?;        let duration = start.elapsed();        println!("ユーザー: {}", profile.user.name);    println!("投稿数: {}", profile.posts.len());    println!("コメント数: {}", profile.comments_count);    println!("\n処理時間: {} ms", duration.as_millis());        Ok(())}3つのAPI呼び出しを並行実行することで、順次実行する場合の3分の1程度の時間で完了します。Future とタスクの理解Rustの非同期処理の核心は Future トレイトです。pub trait Future {    type Output;        fn poll(self: Pin<&mut Self>, cx: &mut Context<'_>)         -> Poll<Self::Output>;}pub enum Poll<T> {    Ready(T),    Pending,}async関数は、Futureを返す関数に変換されます。// これを書くと...async fn example() -> i32 {    42}// コンパイラがこのように変換するfn example() -> impl Future<Output = i32> {    // 状態機械の実装}Futureは遅延評価されます。awaitされるまで実行されない。これにより、効率的なリソース管理が可能になる。参考：cosmicmeta.ioカスタムFutureの実装理解を深めるため、カウンターFutureを自作する例を示します。use std::future::Future;use std::pin::Pin;use std::task::{Context, Poll};use std::time::Duration;struct CounterFuture {    count: u32,    max: u32,}impl CounterFuture {    fn new(max: u32) -> Self {        Self { count: 0, max }    }}impl Future for CounterFuture {    type Output = u32;    fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>)         -> Poll<Self::Output>     {        self.count += 1;        println!("ポーリング #{}: カウント = {}", self.count, self.count);                // 実際の待機をシミュレート        std::thread::sleep(Duration::from_millis(100));                if self.count < self.max {            // まだ完了していない            cx.waker().wake_by_ref();            Poll::Pending        } else {            // 完了            Poll::Ready(self.count)        }    }}#[tokio::main]async fn main() {    let future1 = CounterFuture::new(3);    let future2 = CounterFuture::new(3);        let (result1, result2) = tokio::join!(future1, future2);        println!("\n結果1: {}", result1);    println!("結果2: {}", result2);}出力：ポーリング #1: カウント = 1ポーリング #1: カウント = 1ポーリング #2: カウント = 2ポーリング #2: カウント = 2ポーリング #3: カウント = 3ポーリング #3: カウント = 3結果1: 3結果2: 32つのFutureが交互にポーリングされている様子がわかる。エラーハンドリングとキャンセル安全性エラーハンドリングのベストプラクティスuse std::time::Duration;use tokio::time::timeout;async fn risky_operation() -> Result<String, &'static str> {    tokio::time::sleep(Duration::from_secs(2)).await;    Ok("成功".to_string())}async fn slow_operation() -> Result<String, &'static str> {    tokio::time::sleep(Duration::from_secs(10)).await;    Ok("遅い操作完了".to_string())}#[tokio::main]async fn main() {    // タイムアウト付き実行    let result = timeout(        Duration::from_secs(3),        slow_operation()    ).await;        match result {        Ok(Ok(value)) => println!("成功: {}", value),        Ok(Err(e)) => println!("操作エラー: {}", e),        Err(_) => println!("タイムアウト"),    }        // 複数の操作を並行実行し、エラーを適切に処理    let results = tokio::join!(        risky_operation(),        risky_operation(),        risky_operation(),    );        match results {        (Ok(r1), Ok(r2), Ok(r3)) => {            println!("すべて成功: {}, {}, {}", r1, r2, r3);        }        _ => {            println!("一部が失敗しました");        }    }}キャンセル安全性Tyler Mandryは「Making Async Rust Reliable」で、キャンセル安全性の重要性を強調している。参考：tmandry.gitlab.io。use tokio::sync::Mutex;use std::sync::Arc;// キャンセル安全でない例async fn unsafe_increment(counter: Arc<Mutex<i32>>) {    let mut guard = counter.lock().await;    *guard += 1;    // ここでキャンセルされると、ロックが保持されたまま    tokio::time::sleep(Duration::from_secs(1)).await;}// キャンセル安全な例async fn safe_increment(counter: Arc<Mutex<i32>>) {    let mut guard = counter.lock().await;    *guard += 1;    drop(guard); // 明示的にロックを解放        tokio::time::sleep(Duration::from_secs(1)).await;}実践的なパターン集パターン1: 並行実行で最初に完了したものを使うasync fn fetch_from_server_a() -> Result<String, Box<dyn std::error::Error>> {    tokio::time::sleep(Duration::from_secs(2)).await;    Ok("サーバーA".to_string())}async fn fetch_from_server_b() -> Result<String, Box<dyn std::error::Error>> {    tokio::time::sleep(Duration::from_secs(1)).await;    Ok("サーバーB".to_string())}#[tokio::main]async fn main() {    use tokio::select;        // 最初に完了したほうを使う    select! {        result_a = fetch_from_server_a() => {            println!("サーバーAから: {:?}", result_a);        }        result_b = fetch_from_server_b() => {            println!("サーバーBから: {:?}", result_b);        }    }}パターン2: 複数のタスクをスポーンして管理use tokio::task::JoinHandle;async fn worker(id: i32, duration: u64) -> String {    tokio::time::sleep(Duration::from_secs(duration)).await;    format!("ワーカー {} 完了", id)}#[tokio::main]async fn main() {    let mut handles: Vec<JoinHandle<String>> = Vec::new();        // 複数のワーカーをスポーン    for i in 0..5 {        let handle = tokio::spawn(worker(i, i as u64 + 1));        handles.push(handle);    }        // すべての完了を待つ    for handle in handles {        match handle.await {            Ok(result) => println!("{}", result),            Err(e) => eprintln!("エラー: {}", e),        }    }}パターン3: 共有状態の安全な管理use tokio::sync::RwLock;use std::sync::Arc;#[derive(Clone)]struct Counter {    value: Arc<RwLock<i32>>,}impl Counter {    fn new() -> Self {        Self {            value: Arc::new(RwLock::new(0)),        }    }        async fn increment(&self) {        let mut value = self.value.write().await;        *value += 1;    }        async fn get(&self) -> i32 {        let value = self.value.read().await;        *value    }}#[tokio::main]async fn main() {    let counter = Counter::new();    let mut handles = vec![];        // 10個のタスクで並行してインクリメント    for _ in 0..10 {        let counter_clone = counter.clone();        let handle = tokio::spawn(async move {            for _ in 0..100 {                counter_clone.increment().await;            }        });        handles.push(handle);    }        // すべて完了を待つ    for handle in handles {        handle.await.unwrap();    }        println!("最終カウント: {}", counter.get().await);    // 出力: 最終カウント: 1000}まとめ非同期処理における重要な概念を再確認しましょう。並行性（Concurrency）: 複数のタスクを論理的に同時進行させる技術効率性（Efficiency）: 待ち時間を無駄にせず他のタスクを処理することスケーラビリティ（Scalability）: リソースを効果的に使い多数のタスクを処理する能力応答性（Responsiveness）: ユーザーを待たせず素早く反応することRustの非同期処理は、型システムによる安全性保証と高い実行性能を両立しています。async/await構文はコードを簡潔に保ち、Futureトレイトは強力な抽象化を提供します。Tokioなどのランタイムは成熟しており、本番環境での使用実績も豊富です。Edition 2024以降も、Rustの非同期エコシステムは進化を続けています。async closures、send bound problem、async generatorsなど、さらなる改善が予定されています。参考：www.javacodegeeks.com非同期処理の本質非同期処理の本質は、待ち時間を効率的に活用することです。サーバからのレスポンスを待ちながら他のリクエストを処理し、ファイルの読み込みを待ちながら計算を行います。一つのタスクの完了を待たずに次のタスクを開始することで、限られたリソースを最大限に活用できます。Rustの非同期処理の特徴Rustの非同期処理は、型システムによる安全性保証と高い実行性能を両立しています。async/await構文はコードを簡潔に保ち、Futureトレイトは強力な抽象化を提供します。Edition 2024では、async closuresやasync fn in traitsのサポートが進み、より表現力の高い非同期コードが書けるようになりました。学習のポイント初学者にとって、SendとSyncのトレイト境界やライフタイムの扱いは困難に感じるかもしれません。しかし、これらの制約は、並行処理における安全性を保証するために必要なものです。Rustコンパイラのエラーメッセージは、タスクの依存関係やリソースの所有権について正しく考えるための指針となります。Edition 2024における改善は、単なる文法の追加ではありません。「複数の時間軸を同時に扱う」という非同期処理の考え方が、言語の中により深く統合されたことを意味したのかなぁって思います。おわり参考リンク公式ドキュメントRust公式非同期ブック: rust-lang.github.ioRust Edition 2024公式ガイド: doc.rust-lang.orgRust 1.85.0リリースノート（Edition 2024安定化）: blog.rust-lang.orgTokio公式ドキュメント: tokio.rs開発ロードマップAsync Rust 2024 Roadmap: smallcultfollowing.comRust Lang Team Roadmap 2024: lang-team.rust-lang.orgRust Project Goals 2024: rust-lang.github.ioコミュニティリソースMaking Async Rust Reliable - Tyler Mandry: tmandry.gitlab.ioAsync Rust in a Nutshell - Shuttle: www.shuttle.devRust Edition 2024 Annotated: bertptrs.nl]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Brev上でcuMLを使ってGPUが利用されることを確認してみた]]></title>
            <link>https://zenn.dev/akasan/articles/449fe4e1515f34</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/449fe4e1515f34</guid>
            <pubDate>Mon, 17 Nov 2025 11:50:00 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIA Brev上で機械学習モデルを学習させ、GPUメモリが利用されることを確認してみました。Brevについては以下の記事をご覧ください。https://zenn.dev/akasan/articles/686e4b3ef0b8ff 今回の検証内容今回は以前投稿した以下の記事と同じことをBrev上で構築し、GPUメモリが消費されることを確認してみました。以前はGoogle Colab城で実行しておりGPUメモリが消費される様子を確認できていなかったので、今回はBrev城で確認します。https://zenn.dev/akasan/articles/e7d9b92bf...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、つなげろ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/11/17/085207</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/11/17/085207</guid>
            <pubDate>Sun, 16 Nov 2025 23:52:07 GMT</pubDate>
            <content:encoded><![CDATA[はじめに数年前、私は大きなプロジェクトに取り組んでいました。SREとして、メール配信システムの大規模な障害に直面していました。毎日数百万通のメールを処理するシステムが、突然、配信遅延を起こし始めました。遅延は徐々に悪化し、やがてメールが数時間も届かなくなりました。ユーザーからの問い合わせが殺到しました。経営層からのプレッシャーも増していきました。いくら調べても原因が分かりません。データベースのクエリを最適化しました。キャッシュを増やしました。サーバーのスペックを上げました。でも、問題は解決しませんでした。設定を何度見直しても、どこがおかしいのか分かりません。数日間、問題と向き合いました。さまざまな知識を集めました。組み合わせを試しました。でも、決定的な答えは見つかりませんでした。疲れて、その日は諦めて寝ることにしました。ベッドに入って、目を閉じました。眠れませんでした。頭の中で、断片的な知識がつながり始めました。Webサービスで学んだバックプレッシャー。メールシステムのキューイング。DNSの問い合わせ。これらは別々の領域の知識でした。ところが根本的な構造、同じではないでしょうか。外部リソースへの依存。過剰な要求。システムの過負荷。そうか、と思った瞬間、はっきりと目が覚めました。メールシステムにバックプレッシャーを適用できます。キューが一定の長さを超えたら、新しいメールの受け入れを制限します。そうすれば、DNS問い合わせの数も自然と制御されます。眠れなくなりました。頭の中でアイデアが次々と展開されます。実装の方法。監視の設計。エラーハンドリング。そのまま朝まで考え続けました。朝になって、すぐに検証を始めました。シミュレーションを書きました。小規模な環境で試しました。うまくいきました。これが、私が異なる分野の知識をつなげる力を実感した瞬間でした。Webとメールという別々の領域を結びつけることで、新しい視点が生まれます。見えなかったものが見えます。できなかったことができます。しかし一年後、別のプロジェクトで私は再び行き詰まりました。今度は違う理由でした。あまりにも多くのアイデアを詰め込みすぎました。システムが複雑になりすぎたのです。新しい技術、最新のパターン、すべてを取り入れようとしました。つなげることへの夢中になりすぎて、何が本当に必要かを見失っていました。その時、私は気づきました。つなげることだけが答えではありません。断つことも同じくらい重要なのです。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、早速はじめていきます。問題解決の8つの段階あのメール配信システムの問題に戻りましょう。私はどうやって解決策を見つけたのでしょうか。振り返ってみると、問題解決とは、明確な段階を経るプロセスでした。このプロセスは8つの段階で構成されていました。問いを明確にする大きな問題を小さく分解する関係者の望みを理解する直接関係する分野とその外から知識を集める組み合わせを試しては捨てる意識を手放して無意識へ任せるふとした瞬間にアイデアが出現するのを待つ他者の視点で検証し現実で試すこの8つの段階は順番通りに進むわけではありません。行ったり来たりします。戻ることもあります。1つでも欠けると、あまり良い解決策は生まれません。ただし経験を積むと、いくつかの段階を素早く通過できます。場合によっては飛ばせることもあります。飛ばすことと欠けることは違います。例えば似たような問題を何度も解決していれば、問いの明確化や知識の収集は、ほぼ無意識にできるようになります。実際、私の問題解決では、これらの段階を何度も行き来しました。知識を集めながら組み合わせを試します。組み合わせを試しながら問いを見直します。他者による検証を受けて問題の分解へ戻ります。一度無意識へ任せた後、また知識を集め直します。第一段階から第八段階まで、順番に一度ずつ進むのではありません。螺旋を描くように、何度も同じ段階を通過します。しかし通過するたびに、理解が深まり、解決策が洗練されていきます。そして重要なことを言っておきたいです。これは私の問題解決のプロセスです。あなたにはあなたのプロセスがあります。人によって得意な段階も、時間のかけ方も、順序も違います。このプロセスを参考としつつ、自分へ合った形としてカスタマイズしてほしいです。これから、この8つの段階を1つずつ詳しく見ていきましょう。それぞれの段階で何が起きるのか。どんな落とし穴があるのか。どうすれば効果的に進められるのか。メール配信システムの具体例を使いながら、問題解決のプロセスを解き明かしていきます。では、第一段階から始めましょう。第一段階: 解くべき問いを明確にする最初、私は問題を「メールの配信が遅い」と捉えていました。でも、これは問いではありません。観察の記述です。問いとは、現在と望む未来の間に横たわる溝を、言葉で明確に描くことです。私は問いの形を変えました。「なぜメールの配信が遅いのか」ではなく、「どうすれば安定して配信できるのか」。そして、溝の幅を測定可能にしました。「99%のメールを5分以内に配信する」。なぜ測定可能でなければならないのでしょうか。測定できないものは、改善できないからです。「速くしたい」では、どこまで改善すればいいのか分かりません。「5分以内」なら、達成したかどうか判断できます。次に何をすべきか決められます。問いの輪郭を定めるということは、無限の可能性の空間に、一本の線を引くことです。この線の内側だけを探索し、外側は探索しません。そう決めることです。私は、速度の問題よりも、安定性の問題に意識を向けることを選びました。これは選択であり、同時に放棄でした。問いが曖昧なら、解もまた曖昧です。問いが明確なら、解の輪郭もまた明確になります。すべては最初の線の引き方で決まります。でも、ここで重要な認識があります。最適な問いは、最初から設定できません。だから、仮の問いを設定します。進めながら修正します。問い自体を、何度も見直します。問いを固定するのではなく、更新し続ける柔軟さが必要です。実際、私はこの問いを何度も修正しました。最初は「配信を速くする」でした。でも、関係者と話して（第三段階）、「安定性が重要だ」と気づきました。知識を集めて（第四段階）、「99%」という具体的な数字を設定しました。組み合わせを試して（第五段階）、「5分以内」という時間を決めました。第一段階は、最初に一度やって終わりではありません。他の段階を経ながら、何度も戻ってきて、問いを磨き続けます。測定可能な目標を設定するとき、私はいくつかの基準を考えました。まず、達成可能性。あまりに高い目標は、チームを疲弊させます。次に、意味のある改善。現状が「10分以内に90%」なら、「9分以内に91%」では意味がありません。そして、ビジネス価値。「5分以内に99%」は、来週のキャンペーンを成功させるのに十分な水準でした。問いの設定は、プロジェクト全体の方向性を定める基盤です。この段階を急いではいけません。時間をかけて、本当に解くべき問いは何かを考えます。関係者と対話します。現状を分析します。そして、測定可能で、達成可能で、意味のある目標を設定します。第二段階: 大きな問題を小さく分解する「安定して配信する」という問いは、まだ1つの塊でした。塊のままでは、手がつけられません。だから、私はそれを構成要素に分解しました。メール配信システムは、時間軸に沿って展開するプロセスです。受信、キューイング、処理、送信。この連鎖のどこで、時間が失われているのでしょうか。ログを読みました。メトリクスを見ました。そして発見しました。処理の段階で、時間が消えていました。さらに細かく見ました。処理とは何でしょうか。それは、内容の検証、宛先の解決、サーバーの選択という3つの行為でした。この中のどれが、時間を奪っているのでしょうか。測定しました。宛先の解決でした。外部のDNSサーバーへの問い合わせが、予想以上に多かったです。そして、その一部がタイムアウトしていました。分解するとは、全体を部分に還すことです。そして、部分の中に、真の問題を見つけることです。大きな問いは、答えられません。小さな問いは、答えられます。分解の精度が、解決の可能性を決めます。しかし、分解にも技術がいります。どの粒度で止めるか。細かくしすぎると、全体が見えなくなります。粗すぎると、具体的な行動につながりません。この判断は、経験によって磨かれます。最初は粗く分解します。必要なら、さらに細かく分解します。段階的に進めます。私は、問題を階層的に整理しました。第一階層：メール配信システム全体。第二階層：受信、キューイング、処理、送信という4つのフェーズ。第三階層：処理フェーズの中の検証、解決、選択という3つのステップ。そして第四階層：宛先解決の中のDNS問い合わせ。この階層構造を可視化することで、どこに焦点を当てるべきかが明確になりました。そして重要なのは、各階層での問題が、どう他の階層に影響するかを理解することです。分解した後、もう1つ重要な作業があります。それぞれの部分が、どう影響し合っているかを理解することです。部分を独立したものとして扱うのではなく、システムとして捉えます。宛先の解決が遅いと、キューが詰まります。キューが詰まると、受信も遅くなります。すべてはつながっています。DNS問い合わせの遅延が、なぜシステム全体の遅延につながるのでしょうか。それは、処理がブロックされるからです。1つのメールがDNS問い合わせを待っている間、次のメールは処理できません。キューに溜まっていきます。やがてキューが溢れます。新しいメールが受け入れられなくなります。この因果関係を理解することで、解決策の方向性が見えてきます。ただし、この分解も一度ではうまくできませんでした。最初は「処理が遅い」としか分かりませんでした。でも、知識を集めて（第四段階）、ログを詳しく読んで、「DNS問い合わせだ」と特定できました。そして、組み合わせを試す中で（第五段階）、「DNS以外の部分も見直すべきか」と再び分解に戻りました。分解は、一度やって終わりではなく、理解が深まるたびに、より精緻になっていきます。分解は、分析の技術です。全体を部分に分け、部分の関係を理解し、真の問題を特定します。この段階を丁寧に行うことで、次の段階での探索が効率的になります。第三段階: 関係者の望みを理解するここで重要な認識がありました。私が解決したい問題は、私の問題だけではありません。他者もまた、この問題に関わっています。そして、彼らはそれぞれ異なる視点から、異なる何かを望んでいます。開発チームは、変化を最小限にすることを望んでいました。なぜなら、大きな変更は、予測できないリスクを生むからです。彼らには、別のプロジェクトもあります。時間は限られています。彼らと話したとき、「システムの根幹を変えるような解決策は避けてほしい」という懸念を聞きました。彼らは安定性を重視していました。運用チームは、透明性を望んでいました。システムの状態が見えなければ、障害時に対応できません。複雑さは、透明性の敵です。彼らは、夜中に呼び出されることを恐れています。「何が起きているか分からないシステムは、運用できない」と彼らは言いました。彼らには、明確な監視とアラートが必要でした。ビジネス側は、速度を望んでいました。来週、重要なキャンペーンがあります。それまでに、問題を解決しなければなりません。予算も限られています。「理想的な解決策よりも、来週までに動く解決策が欲しい」と彼らは言いました。彼らには、時間が最も重要でした。これらの望みは、時に矛盾します。安定性と速度。シンプルさと機能性。理想と現実。でも、解決策とは、これらの矛盾する望みが交差する一点を見つけることです。すべての視点を無視すれば、解決策は使われません。技術的に優れていても、運用チームが理解できなければ、保守できません。ビジネスの期限に間に合わなければ、価値がありません。1つの視点だけを優先すれば、他の視点から拒絶されます。だから、私は時間をかけて、それぞれの望みを聞きました。対話しました。どこまで妥協できるか。何が絶対に譲れないか。この対話を通じて、解決策の制約条件が明確になりました。開発チームとの対話から：「既存のキューシステムを置き換えるのではなく、その上に制御層を追加する形なら受け入れられる」という妥協点が見つかりました。運用チームとの対話から：「キューの長さ、処理速度、DNS問い合わせ数の3つのメトリクスを可視化すれば、十分に監視できる」という具体的な要件が見えました。ビジネス側との対話から：「根本的な解決ではなく、段階的な改善でも良い。まず来週のキャンペーンを乗り切れる水準にして、その後さらに改善していく」という現実的なアプローチが決まりました。そして、制約条件こそが、創造性を引き出します。無限の可能性は、かえって選べません。制約があるから、選べます。「既存システムを大きく変えない」「一週間以内に実装できる」「監視可能な設計にする」という制約が、探索すべき解決策の空間を明確に定義しました。視点の交差点を見つけることが、実行可能な解決策への道です。そして、この交差点は、対話を通じてしか見つかりません。一人で考えていても、他者の視点は想像できません。実際に話を聞きます。実際に議論します。その過程で、初めて、すべての視点が満足できる解決策の輪郭が見えてきます。第四段階: 知識を集める - 直接関係する分野と、その外から問題が明確になりました。制約も理解しました。次は、知識を集める段階です。私は2つの方向から探索しました。問題に直接関係する分野と、その外です。問題に直接関係する分野とは、この問題そのものに関する知識です。メールシステムのアーキテクチャ。DNSの仕組み。キューの実装。過去の障害の記録。これらは、問題が存在する領域の全体像です。この全体像を把握することで、何が起きているかを理解できます。私はまず、過去の障害レポートを読みました。似たような問題は起きていないでしょうか。どう対処したでしょうか。一年前に、小規模な遅延問題がありました。その時はDNSキャッシュを増やして解決したと記録されていました。でも、今回の規模では、同じ解決策は通用しないと分かりました。次に、メールシステムの実装を読みました。どのライブラリを使っているでしょうか。どんな設定があるでしょうか。キューの実装はどうなっているでしょうか。コードを読むことで、システムの制約と可能性が見えてきました。しかし、問題に直接関係する分野だけを見ていても、新しい視点は生まれません。なぜなら、その分野の知識で解決できるなら、問題はとうに解決されているはずだからです。だから、その外を探索します。別の分野で、構造的に似た問題は、どう解決されたでしょうか。Webサービスでは、外部APIへのアクセスが過剰になった時、どう対処するのでしょうか。バックプレッシャーという概念を使います。流量を制御します。負荷が高い時、新しい要求を受け入れるのではなく、意図的に待機させます。そうすることで、システム全体の崩壊を防ぎます。私はバックプレッシャーについて、以前のプロジェクトで学んでいました。外部の決済APIが遅くなった時、同僚がその仕組みを実装するのを見ました。その時は「Webではこういう手法があるのか」と理解しました。でも、それはWebサービスの話だと思っていました。メールシステムには関係ない、と。でも、ある夜、ベッドの中で考えていて、気づきました。構造は同じです。Webの外部API呼び出しも、メールのDNS問い合わせも、外部リソースへの依存という点では変わりません。分野が違うだけで、本質的な問題は同じでした。データベースでは、書き込みが多すぎる時、どうするのでしょうか。バッチ処理を使います。個々の書き込みではなく、まとめて書き込みます。これも、私が以前のプロジェクトで学んだパターンでした。ネットワークでは、パケットが多すぎる時、どうするのでしょうか。キューイングとドロップを使います。これは、大学時代にネットワークの授業で習った内容でした。当時は理論として学んだだけで、実践で使うとは思っていませんでした。この原理は、分野を超えて適用できます。なぜなら根本的な構造が同じだからです。外部リソースへの依存。過剰な要求。システムの過負荷。これはWebやメール、データベース、ネットワークといった異なる領域においても、本質的には同じ問題です。別の分野から持ち帰った概念を、今の問題の文脈に翻訳します。これが、問題解決の核心です。既存の要素を、新しい形でつなげること。私はノートに書き出しました。左側に「メールシステムの問題」。右側に「別の分野の解決策」。そして、線でつないでいきました。DNS問い合わせの過剰とAPI呼び出しの過剰。キューの詰まりとネットワークの輻輳。処理の遅延とデータベースの書き込み遅延。ところがここで注意すべきことがあります。すべての情報を集めることはできません。無限の時間をかければ、無限の情報が集まります。ただし時間は有限です。だから選びます。何を集めるか。何を集めないか。私は、課題に直接関係する情報を優先しました。「面白いけど、今回は関係ない」情報は、後回しにしました。例えば、メールの暗号化技術についての記事を見つけました。興味深かったですが、今回の遅延問題には関係ありません。ブックマークはしましたが、深く読むのはやめました。サンクコストの罠へは陥らないようにしました。「ここまで読んだから最後まで読もう」ではなく、「関係ないと分かったから、ここで止める」という判断を何度も繰り返しました。情報収集には終わりがありません。だから、どこかで線を引きます。「これだけ集めれば十分」と判断します。この判断の基準は何でしょうか。それは、次の段階に進めるかどうかです。組み合わせを試すのに十分な要素が揃ったでしょうか。揃ったと感じたら、次に進みます。足りないと感じたら、もう少し集めます。私は、知識を集めながら、頭の中で組み合わせを試し始めていました。そして、組み合わせを試すうちに、「この情報が足りない」と気づいて、また知識を集めに戻ります。第四段階と第五段階を何度も行き来しました。三日間、この往復を繰り返しました。第五段階: 組み合わせを試しては捨てる知識が集まりました。次は、それらを組み合わせる段階です。私の前には、無数の可能性が広がっていました。A、B、C。それぞれ単独で使うこともできます。組み合わせることもできます。AとB。AとC。BとC。AとBとC。さらに、実装の詳細によって、それぞれの組み合わせは無限の変種を持ちます。可能性の空間は、想像を超えて広いです。すべてを試すことは不可能です。だから、歩く道を選ばなければなりません。A：DNSキャッシュの増強。以前の障害でうまくいった方法です。でも、今回の規模では不十分だと直感していました。B：バックプレッシャーの導入。キューが一定の長さを超えたら、新しいメールの受け入れを制限します。Webサービスで有効だったパターンです。C：非同期処理の最適化。DNS問い合わせを並列化して、待ち時間を減らします。私は頭の中で、1つずつ試しました。AとBをつなげます。どうなるでしょうか。DNSキャッシュを増やし、同時にバックプレッシャーで流量を制御します。効果はありそうです。でも、持続可能でしょうか。キャッシュは、メモリを消費します。無限に増やせません。そして、DNSの情報は変化します。キャッシュが古くなったら、間違った宛先に送ってしまいます。リスクが高いです。じゃあAとCは。DNSキャッシュを増やし、非同期処理を最適化します。処理は速くなります。でも、根本的な問題は解決しません。DNS問い合わせ自体の数は減りません。外部のDNSサーバーへの負荷は変わりません。一時的には改善するかもしれませんが、負荷が増えれば、また同じ問題が起きます。BとCは。バックプレッシャーで流量を制御し、非同期処理を最適化します。面白いです。流量が制御されれば、DNS問い合わせの数も自然と減ります。そして、非同期処理で、個々の問い合わせも速くなります。この組み合わせは、有望です。この探索の過程で、重要なことに気づきます。ほとんどの組み合わせは、うまくいきません。1つ試して、うまくいかないから捨てます。また1つ試して、やはりうまくいかないから捨てます。その時は「ちょっと試しただけ」に感じます。しかしこの小さな捨てるを繰り返していると、気づいたら膨大な数の組み合わせを試して捨てています。毎日コンビニで小さな買い物をしていたら、数ヶ月後に気づいたら20万円使っていたような感覚です。1つ1つは大したことないですが、積み重なると大きいです。でも、この失敗の積み重ねが、最終的な成功を導きます。なぜなら、失敗することで、何がうまくいかないかが分かるからです。そして、それは何がうまくいくかを知る手がかりになります。私は、さらに細部を検討しました。BとCの組み合わせが良さそうです。でも、どう実装するでしょうか。バックプレッシャーを、どのレベルで実装するでしょうか。受信の段階でしょうか。キューイングの段階でしょうか。処理の段階でしょうか。それぞれの選択肢を考えました。受信の段階での制限は、メール喪失のリスクを伴います。送信元へエラーを返すことになります。これは避けたいです。キューイングの段階を適切と判断しました。キューへ入れる前に、キューの長さをチェックします。長すぎたら一時的な受け入れ遅延を実施します。キューの長さの閾値を、どう設定するでしょうか。1000通でしょうか、5000通でしょうか、10000通でしょうか。この数字は、システムの処理能力とメモリ容量から決まります。メトリクスを見ました。通常時のキュー長は1000通以下でした。ピーク時で3000通程度でした。閾値を5000通に設定すれば、通常時は影響せず、異常時だけ制御できます。非同期処理を、どう最適化するでしょうか。DNS問い合わせを並列化します。しかし何個まで並列化できるでしょうか。並列度が高すぎると、DNSサーバーへ負荷をかけすぎます。最適なバランスを見つける必要があります。可能性の空間を歩くとは、ほとんどの道を捨てることです。残った一本の道を、さらに磨くことです。そして、この過程は、決して直線的ではありません。行ったり来たりします。戻って、別の道を試します。時には、最初の分岐点まで戻ります。迷路を歩くように、試行錯誤を繰り返します。BとCの組み合わせに絞りました。でも、まだ確信は持てません。頭の中でシミュレーションします。キューが5000通を超えます。新しいメールの受け入れが遅くなります。その間、処理は進みます。非同期処理が最適化されているから、処理は速いです。キューが減ります。また受け入れが再開します。うまくいきそうです。しかし本当にうまくいくかは、実装してみないと分かりません。実装してみたら新しい疑問が出てきます。「閾値は本当に5000で良いのか」。そう思って、また問いへ戻ります（第一段階）。「監視はどうするか」。そう考えて関係者と話します（第三段階）。組み合わせを試す段階は、あらゆる段階への入り口です。次の段階へ進む前に一度休憩します。疲れてきました。第六段階: 意識を手放して無意識に任せる探索を続けていると、疲労が蓄積します。思考が鈍ります。どの組み合わせを試しても、前に進んでいる気がしません。行き詰まります。このとき、最も逆説的で、最も効果的な行為があります。それは、問題を手放すことです。私は、その日の夜、問題を意識の外に置きました。夕食を作りました。ゆっくり食べました。映画を見ました。早めに寝ました。問題について考えないように、意識的に努力しました。「今日の自分には解けない。明日の朝の自分に任せよう」。そう決めました。深夜まで考え続けても、疲れた頭では良い答えは出ません。むしろ、間違った方向に固執してしまいます。だから、意識的に諦めます。今の自分ではなく、明日の自分に託します。なぜこれが重要なのでしょうか。意識は、強力ですが制約も多いです。意識は、一度に1つのことしか考えられません。逐次的です。線形です。そして、既存の思考パターンに縛られます。「こうあるべきだ」という規範に従います。「前回はこうだった」という経験に引きずられます。私が「DNSキャッシュを増やすべきだ」と一度考えると、その思考パターンから抜け出しにくくなります。意識は、その方向に固執します。別の可能性を見落とします。でも、無意識は、そういう制約を受けません。無意識は、並列に、複数の可能性を同時に探索できます。規範に縛られません。自由につながりを試せます。時には、意識が「不可能だ」と判断したつながりも試します。意識の支配を手放すとは、無意識という、より広大な処理能力に、問題を委ねることです。そして、無意識が何かを見つけたとき、それは閃きとして、意識に返されます。でも、誤解してほしくないのは、無意識に任せるためには、その前に十分な準備が必要だということです。知識を集めなければ、無意識は何も組み合わせられません。組み合わせを試さなければ、無意識は探索の方向が分かりません。意識的な努力の後に、初めて、無意識の並列処理が効果を発揮します。私は、三日間、問題と向き合いました。知識を集めました。組み合わせを試しました。そして、疲れました。その疲労が、意識を手放すサインでした。「ここまでやった。あとは明日の自分に任せる」。そう決めることで、心が軽くなりました。その日の夜、私は諦めて寝ることにしました。でも、実際には、諦めたのではありませんでした。意識的な努力を手放して、無意識に問題を委ねただけでした。そして、その無意識が、ベッドの中で答えを見つけることになります。休憩の仕方にも、技術があります。意識的に問題から離れます。仕事の話をしません。メールをチェックしません。コードを見ません。別のことに意識を向けます。料理をします。散歩をします。音楽を聴きます。体を動かします。睡眠も重要です。睡眠中、脳は情報を整理します。記憶を統合します。つながりを再構成します。十分な睡眠なしに、創造的な思考は生まれません。明日の朝の自分が答えを見つけるためには、今日の夜、しっかり眠ることが必要です。ただし、今回の私のように、眠ろうとした瞬間にアイデアが出現することもあります。それもまた、無意識の働きです。第七段階: ふとした瞬間にアイデアが出現するその日の夜、ベッドに入りました。疲れていました。早く眠りたかったです。でも、目を閉じた瞬間、それは起きました。突然、アイデアが浮かびました。というより、アイデアは常にそこにあって、ただ私がそれを認識していなかっただけだという感覚でした。メール処理のキューにバックプレッシャーを実装します。キューが一定の長さを超えたら、新しいメールの受け入れを制限します。同時に、非同期処理を最適化して、DNSキャッシュを効率的に使います。そうすれば、DNSへの問い合わせ数が自然と制御されます。これです。BとCの組み合わせです。Aは不要でした。DNSキャッシュを増やすのではなく、システム全体の流量を制御することで、結果的にDNSへの負荷を減らします。そして、もう1つ重要なことに気づきました。バックプレッシャーと非同期処理は、互いに補完し合います。バックプレッシャーが流量を制御します。その制御された流量の中で、非同期処理が効率的に動きます。並列度を上げすぎる心配がありません。なぜなら、そもそも流量が制限されているからです。はっきりと目が覚めました。もう眠れません。頭の中で、次々とアイデアが展開されます。監視の方法。アラートの設定。エラーハンドリング。実装の手順。キューの閾値は5000でしょうか。いや、動的に変えるべきでしょうか。運用チームには何を伝えるべきでしょうか。ベッドから出ました。ノートを開きました。すべて書き出しました。なぜなら、この種のアイデアは、すぐに忘れてしまうからです。夢のように、掴んだと思った瞬間に、すり抜けていきます。朝まで眠れませんでした。でも、それで良かったです。朝になったら、すぐに検証を始めました。アイデアの出現は、予測できません。意図して起こせるものでもありません。でも、条件を整えることはできます。知識を集めます。組み合わせを試します。疲れたら手放します。そして、無意識に任せます。この一連のプロセスを経ることで、アイデアが出現する確率は高まります。寝る前、散歩をしている時、眠りから覚める瞬間。これらの状態に共通するのは、意識が緩んでいることです。意識の統制が弱まっています。だから、無意識からのメッセージが、意識に届きやすくなります。つながりは、探すものではありません。出現するのを待つものです。そして、出現した時、それを逃さずに捕まえるものです。第八段階: 他者の視点で検証し、現実で試す朝になりました。一睡もしていませんでしたが、頭は冴えていました。アイデアが出現しました。でも、それは原石です。そのままでは使えません。研磨する必要があります。まず、自分で検証しました。シミュレーションを書きました。人工的に大量のメールを生成し、さまざまな閾値を試しました。3000通、5000通、10000通。それぞれの場合で、システムがどう振る舞うか観察しました。そして、他のエンジニアに説明しました。特に、メールシステムに詳しくない人を選びました。なぜなら、彼らは私の前提を共有していないからです。彼らの視点は、私の盲点を照らします。説明しながら、言葉に詰まりました。「ここで、バックプレッシャーが...」。どう説明すればいいのでしょうか。自分でも理解が曖昧だと気づきました。「なぜバックプレッシャーが必要なのか」と聞かれました。改めて考えました。根拠を整理しました。論理を組み立て直しました。「DNSの問い合わせが多すぎるから」ではありません。「システム全体の過負荷を防ぐため」だと理解し直しました。別のエンジニアが聞きました。「キューが5000通を超えたら制限するって言ったけど、その5000という数字はどこから来たの」良い質問でした。私は、朝のシミュレーションで決めたと説明しました。しかし彼は納得しませんでした。「シミュレーションを見たっていうけど、それは通常時のトラフィックでしょ。今回は異常時の話だから、通常時のデータだけで決めていいの」確かに。さらにデータを集めました。実際のトラフィックパターンで検証しました。5000通が適切だと確認できました。でも、さらに重要な発見がありました。閾値を固定するのではなく、動的に調整した方が良いということです。システムの処理能力は、時間帯によって変わります。夜間は処理能力が高いです。昼間は低いです。固定の閾値ではなく、処理能力に応じて変化する閾値の方が効果的です。これは、他者との対話から生まれた改善でした。一人で考えていたら、気づかなかったです。批評とは、他者の視点を通じて、自分の認識を修正するプロセスです。他者の問いが、自分の理解の穴を教えてくれます。他者の疑問が、自分の論理の弱点を示してくれます。そして、小規模な環境で実装しました。理論は現実と出会いました。新しい問題が見つかりました。キューが溢れた時の処理。監視メトリクスの設定。エラーハンドリング。ログの出力形式。アラートの閾値。1つずつ解決しました。理論的には正しくても、実装すると問題が出ます。だから、試します。問題が出たら、修正します。この反復を通じて、アイデアは研磨されます。原石は、使える形になります。実装の過程で、さらに気づいたことがあります。BとCの組み合わせだけでは不十分でした。監視（D）も必要でした。キューの長さを可視化しなければ、バックプレッシャーが機能しているか分かりません。アラート（E）も必要でした。問題が起きた時、すぐに気づけなければ意味がありません。運用チームと話しました（第三段階に戻りました）。「どんなメトリクスが必要か」と聞きました。彼らは、3つのグラフを要求しました。キューの長さの推移。処理速度の推移。DNS問い合わせ数の推移。そして、アラートの条件も具体的に提示してくれました。これらを実装するために、また知識を集め直しました（第四段階に戻りました）。監視ツールの使い方。メトリクスの設計。アラートの設定方法。そして、これらを組み合わせて（第五段階に戻りました）、全体の設計を修正しました。最初の設計は、実装を通じて進化しました。最終的な解決策は、最初のアイデアよりも複雑でした。しかしより現実的で堅牢でした。第一段階から第八段階まで、私は何度も行き来しました。その往復のたびに解決策は磨かれていきました。批評という研磨を経て、アイデアは現実で機能する解決策になります。そして、この研磨のプロセスこそが、問題解決の本質です。洗練されたアイデアが突然生まれるのではありません。粗いアイデアを、何度も磨いて、ようやく使える形になります。解決策の完成これらの段階を経て、私は解決策を実装しました。バックプレッシャーを導入し、非同期処理を最適化しました。結果、メールの配信遅延は解消されました。99%のメールを5分以内での配信が可能になりました。そして、来週のキャンペーンも、問題なく乗り切ることができました。振り返ってみると、私は8つの段階を何10回も行き来しました。問いを明確にして分解します。知識を集めて組み合わせを試します。そこで行き詰まり、問いへ戻ります。関係者と話して新しい制約へ気づきます。知識を集め直します。無意識へ任せてアイデアが出ます。検証して問題を発見し分解へ戻ります。この螺旋を描くような往復が、解決策を洗練させていきました。最初の問い「配信を速くしたい」は、最終的に「99%のメールを5分以内に安定して配信する」になりました。最初のアイデア「DNSキャッシュを増やす」は、最終的に「バックプレッシャーと非同期処理の組み合わせ」になりました。異なる分野の知識をつなげることで、問題は解決できます。でも、やみくみにつなげるだけでは、解決しません。問いの輪郭を定めます。全体を断片に還します。視点の交差点を見つけます。直接関係する分野とその外から知識を集めます。可能性の空間を歩きます。意識の支配を手放します。つながりの出現を待ちます。批評という研磨を経ます。これらの段階を何度も行き来して、初めて、本当に価値のある解決策が生まれます。問題解決とは、プロセスです。偶然ではなく、必然です。そして、このプロセスを理解し、意識的に実践することで、誰でも効果的な問題解決ができるようになります。なぜ私たちはつながりを見出すのか私自身の人生を振り返ると、つながりを見出すことは、喜びそのものでした。プログラミングを学び始めた頃、初めてループと配列をつなげて理解できた瞬間。「ああ、こうやって使うのか」と気づいた時の興奮。今でも覚えています。それまで、ループと配列は別々の概念でした。でも、ループで配列の要素を1つずつ処理できると理解した時、2つの概念がつながりました。霧が晴れるような感覚でした。データ構造とアルゴリズムをつなげて、効率的なコードが書けた時の達成感。最初、私はアルゴリズムを理論として学んでいました。でも、実際のコードで使ってみると、実行速度が劇的に改善しました。O(n²)からO(n log n)への変化を、体感として理解できました。理論と実践がつながった瞬間でした。別の言語を学んで、以前の言語との共通点を発見した時の「そういうことか」という驚き。Go言語からRustに移った時、最初は戸惑いました。でも、所有権やライフタイムといった概念を理解した時、メモリ管理の本質が見えました。Go言語でガベージコレクションに任せていたことを、Rustでは明示的に制御します。異なるアプローチですが、根本的な問題は同じだと気づきました。チーム開発で、エンジニアの視点とデザイナーの視点をつなげて、より良いユーザー体験を作れた時。私は、機能が動けば良いと思っていました。でも、デザイナーと一緒に仕事をして、ユーザーがどう使うかを考えるようになりました。技術的な実装とユーザー体験がつながりました。そして、より良いプロダクトが生まれました。ビジネスの要求と技術的な制約をつなげて、実現可能な解決策を見つけた時。最初、ビジネス側の要求は「無理だ」と判断することが多かったです。ところが対話を重ねるうちに、本当に必要なことが見えてきました。技術的な制約の中で、ビジネスの価値を最大化する方法を見つけられました。異なるバックグラウンドを持つ人たちと議論して、自分一人では思いつかなかった視点を得た時。インフラエンジニア、フロントエンドエンジニア、データサイエンティスト。それぞれが異なる視点を持っています。その視点をつなげることで、より包括的な解決策が生まれました。これらの瞬間は、純粋に楽しかったです。新しいつながりを見つけることは、謎が解けることです。霧が晴れることです。世界が少し明確になることです。そして、それは課題解決にも直結しました。問題に直面した時、別の分野の知識とつなげることで解決できた経験は数え切れません。インフラの問題を、Webの知見で解決しました。あのメール配信システムの問題がそうでした。パフォーマンスの問題を、データベース設計の知識で解決しました。遅いクエリを、インデックスの最適化で改善できました。チームの問題を、プロダクト開発の経験で解決しました。スプリントの進め方を、別のチームのやり方を参考に改善できました。つながりを見出すことは、私にとっての喜びであり、学びの源泉であり、課題解決の手段でした。この喜びが、私たちを新しい発見へと駆り立てます。課題解決の源泉になります。でも、生成AIの時代には、何が変わったのかしかし、生成AIが誕生した今、状況は変わりつつあります。ChatGPTやClaudeに問いを投げると、瞬時に答えが返ってきます。知識を集める段階が、数秒で終わります。組み合わせを試す段階も、AIが代わりにやってくれます。アイデアの出現を待つ必要もありません。すぐに解決策が提示されます。確かに、速いです。効率的です。でも、何かが失われています。それは単に「喜びを失う」という話ではありません。もっと本質的な問題があります。AIが提示するつながりは、AIの文脈でのつながりです。私の文脈でのつながりではありません。私がループと配列をつなげた時、それは私のコードの中で、私の問題を解決するために、つながりました。私の手を動かして、私のエラーを見て、私の頭で理解しました。だから、次に似た問題に出会った時、自分でつなげられます。でも、AIの答えをそのまま使うと、そのつながりは私のものになりません。AIがどうやってつなげたのか、なぜそうつなげたのか、私の文脈では本当に正しいのか、分かりません。そして、次に似た問題に出会った時、また同じようにAIに聞くしかありません。これは、この文章で語ってきた8つの段階との関係で考えると、より明確になります。第一段階の「問いを明確にする」。AIに曖昧な問いを投げても、それなりの答えが返ってきます。だから、問いを明確にする訓練ができません。でも、問いが曖昧なら、答えもまた曖昧です。AIが返した答えが、本当に自分が求めていた答えなのか、判断できません。第二段階の「問題を分解する」。AIは既に分解された答えを返します。だから、どう分解されたのか、なぜそう分解されたのか、自分の問題にとって適切な分解なのか、分かりません。第三段階の「関係者の望みを理解する」。これはAIには絶対にできません。私のチームの運用チームが何を恐れているか、ビジネス側が本当に求めているものは何か、AIは知りません。でも、AIの答えをそのまま使うと、この段階を飛ばしてしまいます。第四段階の「知識を集める」。AIは既に知識を持っています。だから、自分で知識を集める必要がありません。でも、自分で集めないと、どの知識が重要か、どの知識が自分の文脈に合うか、判断できません。第五段階の「組み合わせを試す」。AIは最適な組み合わせを提示します。でも、なぜ他の組み合わせがダメなのか、自分で試していないから分かりません。そして、断つべき組み合わせを自分で見極める能力が育ちません。第六段階の「無意識に任せる」。AIに聞けば瞬時に答えが出ます。だから、無意識が働く時間がありません。でも、無意識の並列処理こそが、意外なつながりを生み出します。第七段階の「アイデアが出現する」。AIがアイデアを提示します。でも、それは私のアイデアではありません。私の頭の中でつながりが出現する瞬間を、経験できません。第八段階の「検証する」。これが最も重要です。AIの答えを検証せずに使うと、間違った答えに気づけません。でも、検証するためには、前の7つの段階を理解している必要があります。プロセスが圧縮されすぎて、各段階で得られる学びが失われます。そして、最も危険なのは、つながりを断つ能力が育たないことです。AIの答えには、全てがつながっているように見えます。でも、実際には、自分の文脈に合わない部分があります。複雑すぎる部分があります。不要な部分があります。それらを断つ必要があります。でも、自分でつなげる経験がないと、何を断つべきか判断できません。この文章で語ってきたように、問題解決とは、つなげることと断つことの往復運動です。でも、AIに全てを任せると、つなげることだけが起きて、断つことが起きません。そして、つながりすぎた複雑な解決策を、そのまま実装してしまいます。あの失敗したプロジェクトと同じことが起きます。では、生成AIの時代に、どうすればいいのでしょうか。AIを、対話の相手として使います。答えを得るのではなく、自分の考えを確認するために使います。第一段階で、問いを明確にした後、AIに聞きます。「この問いは明確か」と。AIの答えを見て、自分の問いを修正します。第二段階で、問題を分解した後、AIに聞きます。「この分解は適切か」と。AIの分解と比較して、自分の分解を見直します。第四段階で、知識を集めた後、AIに聞きます。「他にどんな知識があるか」と。AIが提示した知識の中から、自分の文脈に合うものを選びます。合わないものは断ちます。第五段階で、組み合わせを試した後、AIに聞きます。「この組み合わせは有効か」と。AIの答えを見て、自分が見落としていた組み合わせに気づきます。でも、最終的には自分で判断します。第八段階で、検証する時、AIに聞きます。「この設計の問題点は何か」と。AIが指摘した問題を、自分で検証します。そして、必要なら修正します。重要なのは、AIの答えをそのまま使わないことです。AIの答えを、自分の文脈に翻訳します。自分の制約条件に合わせて修正します。不要な部分を断ちます。そして、自分の頭で理解してから、使います。ここで、もう1つ重要な洞察があります。AIと書籍では、知識の与え方が根本的に違います。AIは、私の質問に答えます。私が「ループとは何か」と聞けば、ループについて教えてくれます。私が「配列とは何か」と聞けば、配列について教えてくれます。でも、AIは「次にどういう質問をすべきか」を教えてくれません。私の文脈で、私の質問に、答えるだけです。一方、書籍は違います。著者が、入門者に対して、「この順番で学べば、つながりが見えてくる」という道筋を設計しています。最初にループを説明します。次に配列を説明します。そして、ループと配列を組み合わせる例を示します。この順番には、意味があります。著者が何年もかけて習得した知識を、どういう順序で、どういうつながりで学べば理解できるか、深く考えて構成されています。書籍は、知識そのものだけでなく、知識のつながりの構造を教えてくれます。AIに「ループと配列をどう組み合わせるか」と聞けば、答えは返ってきます。でも、なぜループの後に配列を学ぶべきなのか、なぜその逆ではないのか、この2つの概念がどう関連しているのか、その関連性を理解するためには何を知っておくべきか、そういう「メタ的なつながり」は教えてくれません。これは、この文章で語ってきた8つの段階との関係で、より深刻な問題になります。第一段階の「問いを明確にする」。書籍を読むと、著者が「こういう問いを立てると良い」という例を示してくれます。章立てそのものが、問いの構造を示しています。でも、AIに質問すると、自分が立てた問いにしか答えてくれません。「次にどういう問いを立てるべきか」は、自分で考えなければなりません。でも、初学者は、次にどういう問いを立てるべきか、分かりません。だから、同じような質問を繰り返したり、重要な問いを見逃したりします。書籍なら、著者が「この章の後は、こういう問いが生まれるはずだ。だから次の章でそれに答える」という構成を作っています。第二段階の「問題を分解する」。書籍は、複雑な問題をどう分解するかの例を示してくれます。章が進むごとに、徐々に複雑な問題に取り組んでいきます。その過程で、分解の技術を学べます。でも、AIに質問すると、既に分解された答えが返ってきます。分解のプロセスは見えません。第四段階の「知識を集める」。書籍は、どういう知識を、どういう順番で集めるべきか、道筋を示してくれます。関連する知識への参照を示してくれます。でも、AIは、質問された知識だけを返します。「この知識を理解するためには、先にあの知識を学ぶべきだ」という構造は見えません。AIは、点で答えます。書籍は、線で教えます。点だけを集めても、線にはなりません。自分で点をつなげなければなりません。でも、どう点をつなげるべきか、初学者には分かりません。だから、間違ったつなげ方をしたり、つなげるべき点を見逃したりします。書籍は、著者が既につないだ線を見せてくれます。その線をなぞることで、つなげ方を学べます。そして、次に別の点に出会った時、自分でつなげられるようになります。もちろん、AIにも利点はあります。自分の文脈に特化した答えが得られます。書籍にない最新の情報が得られます。対話的に質問を深掘りできます。でも、知識のつながりの構造を学ぶためには、書籍の方が優れています。だから、私は両方を使います。書籍で、知識のつながりの構造を学びます。どういう順番で学べば理解できるか、著者の道筋をたどります。そして、その構造を理解した上で、AIで具体的な疑問を解消します。自分の文脈に合わせた応用例を聞きます。書籍が線なら、AIは点です。線を理解してから、点を集めます。点だけを集めても、線は見えません。でも、線を理解していれば、点をどこに配置すべきか分かります。生成AIの時代だからこそ、書籍の価値が高まります。AIは答えを速く返してくれますが、答えに至る道筋は示してくれません。書籍は遅いですが、道筋を示してくれます。その道筋こそが、つながりを見出す能力を育てます。AIは、8つの段階を圧縮してしまいます。だから、意識的に8つの段階を経験する必要があります。AIを使いながらも、問いを明確にする時間を持ちます。知識を集める時間を持ちます。組み合わせを試す時間を持ちます。無意識に任せる時間を持ちます。そして、つながりを断つ訓練を、意識的に行います。AIの答えの中から、「これは自分の文脈には合わない」と判断して、断ちます。「これは複雑すぎる」と判断して、シンプルにします。「これは不要だ」と判断して、削除します。生成AIは、強力なツールです。うまく使えば、問題解決を加速できます。でも、全てを任せると、つながりを見出す能力も、つながりを断つ能力も、両方失います。だから、自分の頭でつなげます。AIに任せません。そして、自分の文脈で断ちます。AIの答えを鵜呑みにしません。速さだけを求めるのではなく、理解の深さを求めます。効率だけを求めるのではなく、没入する時間を確保します。AIを使いながらも、8つの段階を意識的に経験します。それが、生成AIの時代に、つながりを見出し続けるための道です。異なる領域を結びつけることで、新しい価値が生まれるプログラミングを始めた頃、私は1つの言語しか知りませんでした。それでコードを書いていました。でも、別の言語を学んだ時、視野が広がりました。「ああ、こういう書き方もあるのか」。そして、1つの言語で学んだパターンを、別の言語で応用できることに気づきました。チーム開発を始めた時、私はエンジニアしか知りませんでした。でも、デザイナーと働き始めた時、視点が変わりました。「なるほど、ユーザーはこう見ているのか」。ビジネス側の人と話した時、優先順位の付け方が変わりました。「そうか、これが重要なのか」。異なる視点をつなげることで、理解が深まります。問題の本質が見えます。解決策が生まれます。これは、つながりの本質です。アイデアとは、既存の要素の新しい組み合わせです。まったく新しいものなど、存在しません。すべては、既存の要素を、新しい方法でつなげたものです。でも、その組み合わせ方が新しければ、それは価値ある解決策になります。つなげてください。異なる知識を。異なる視点を。異なる人々を。つなげることで、世界は進歩します。でも、私は間違ったその大きなプロジェクトに戻りましょう。つながりの力を知った私は、すべてをつなげようとしました。最新の技術を学びました。新しいパターンを適用しました。異なる領域のベストプラクティスを取り入れました。マイクロサービス、イベント駆動、関数型プログラミング、リアクティブプログラミング。すべてを組み合わせました。設計は美しかったです。紙の上では理想的でした。でも、実装を始めると、問題が次々と出てきました。複雑すぎて、誰も理解できません。デバッグに膨大な時間がかかります。新機能の追加が困難になります。パフォーマンスは改善しましたが、開発速度は大幅に低下しました。チームは疲弊していきました。私は混乱しました。すべてを正しくつなげたはずでした。最適な技術を選び、最新のパターンを適用し、ベストプラクティスに従いました。なぜ、うまくいかないのでしょうか。数週間悩んだ後、私はある事実に気づきました。問題は、つなげすぎたことでした。必要ないものまでつなげました。複雑にする必要のないところを複雑にしました。そして何より、間違った前提を断てなかったことが問題でした。私は「最新の技術は優れている」という前提を疑いませんでした。「複雑なアーキテクチャは柔軟性をもたらす」と信じ込みました。でも、これらの前提は、私たちのプロジェクトには合っていませんでした。チームは小さく、変更は頻繁で、複雑さを管理するリソースはありませんでした。シンプルなアプローチの方が、遥かに適していました。つなげることに夢中になりすぎて、断つべきものを見逃していました。そして、もっと根本的な問題がありました。学んだことを、アンラーンできなかったのです。アンラーンとは、学習を解除することです。一度学んだ知識や信念を、意識的に手放すことです。これは、新しいことを学ぶよりも難しいです。なぜなら、学んだことは、自分の思考の一部になっているからです。それを疑うことは、自分自身を疑うことになります。私は、過去のプロジェクトで学んだパターンを持っていました。「大規模システムではマイクロサービスが有効だ」「イベント駆動は疎結合をもたらす」。これらは、確かに正しい状況もあります。でも、すべての状況で正しいわけではありません。過去の成功体験は、時に次の失敗の原因になります。以前うまくいったアプローチが、今回もうまくいくとは限りません。でも、人間は過去の成功を手放すことが難しいです。「これで成功したのだから、今回も使うべきだ」と考えてしまいます。アンラーンは、新しい知識を得る前に、古い知識を疑うことです。「この知識は、今の状況に本当に適用できるのか」と問うことです。そして、適用できないと分かったら、躊躇なく手放すことです。その時、私は理解しました。つなげることだけが答えではありません。もう半分は、断つことです。そして、断つためには、まずアンラーンすることが必要なのです。つながりを断つことは技術であるつなげることは本能ですが、断つことは技術です。一度見出したパターンを否定することは、本能に反します。「これとこれは関係がある」と信じているものを、「いや、関係ない」と認めることは、認知的な苦痛を伴います。既に投資した時間と労力が無駄になります。自分の判断が間違っていたと認めなければなりません。断つことは、本能ではありません。技術です。意識的に訓練しなければ、身につきません。コードを書いていて、ある実装に三時間かけたとします。でも、レビューで別のアプローチの方が良いと指摘されます。この時、人間の本能は「三時間を無駄にしたくない」と抵抗します。でも、優れたエンジニアは躊躇なく捨てます。三時間のサンクコストより、今後何年も保守されるコードの品質の方が重要だと知っているからです。つながりを断つ技術を持っていない人間は、一度つなげたものを手放せません。そして、つながりはどんどん増えていきます。最初は小さな勘違いだったものが、関連する情報を次々と取り込んで、巨大な信念体系になります。そして、その信念体系全体を否定することは、もはや不可能になります。この現象は、エンジニアリングの世界だけでなく、あらゆる分野で起きます。医療の診断、ビジネスの意思決定、人間関係の理解。そして、最も極端な形で現れるのが、陰謀論です。つながりを断つ技術がないと、どうなるでしょうか。陰謀論という極端な例を見れば、その危険性がよく分かります。物語に囚われるということ陰謀論や物語に深く囚われている人間を観察していて気づいたことがあります。彼らは新しいつながりを作ることが得意です。一見無関係な出来事から、驚くべき関連性を見出します。その発想力は、時に感心するほどです。問題は、彼らがつながりを断てないことです。普通の人間は、仮説を立てます。「AとBには関係があるかもしれない」。そして検証します。証拠を探します。反証も探します。もし関係がなさそうなら、その仮説を捨てます。つながりを断ちます。でも、物語に囚われた人間は違います。「AとBには関係がある」と一度信じたら、もう断ちません。反証が出てきても、別の解釈で説明します。証拠がなくても、証拠の不在を何らかの理由で正当化します。つながりを断つのではなく、さらに別のつながりを作って補強します。これは、つながりの創造性の問題ではありません。つながりの破棄能力の問題です。エンジニアがバグに遭遇したとします。「このエラーは、たぶんメモリリークが原因だ」と仮説を立てます。調査します。でも、メモリ使用量は正常でした。この時、優れたエンジニアはすぐに仮説を捨てます。「メモリリークではない」と認めて、別の原因を探します。でも、経験の浅いエンジニアは、最初の仮説に固執することがあります。「メモリ使用量が正常に見えるのは、測定方法が間違っているからだ」と考えます。「実は隠れたメモリリークがあるはずだ」と探し続けます。数時間を無駄にした後、ようやく別の原因に気づきます。つながりを断てないことが、探索を非効率にします。物語に囚われた人間も同じです。最初の仮説に固執して、それを支持する情報だけを集め続けます。反証する情報は、何らかの形で無効化されます。つながりは増え続けますが、決して減りません。そして最終的に、巨大で複雑で、誰にも検証不可能な信念体系ができあがります。もちろん、これは陰謀論だけの話ではありません。私たち全員が、程度の差こそあれ、この傾向を持っています。自分が信じたいことを信じ、信じたくないことを疑います。都合の良い情報を集め、都合の悪い情報を無視します。だからこそ、意識的につながりを断つ訓練が必要なのです。エコーチェンバーとは、つながりを断つ機会がない空間だSNSのエコーチェンバーが問題なのは、同じ意見ばかりが反響するからだと言われます。でも、本質はそこではありません。本質は、つながりを断つ機会がないことです。人間は誰でも、間違ったつながりを作ります。「これとこれは関係がある」と思い込みます。でも、通常はそのつながりを断つ機会があります。友人が「それ、違うんじゃない？」と指摘してくれます。本を読んでいて、自分の考えと矛盾する事実に出会います。議論の中で、自分の論理の穴に気づきます。これらの経験が、間違ったつながりを断つきっかけになります。でも、エコーチェンバーの中では、そのきっかけがありません。全員が同じつながりを信じています。だから、誰もそれを疑いません。間違ったつながりでも、誰も指摘しません。むしろ、そのつながりを補強する情報ばかりが流れてきます。つながりを作る機会は無限にありますが、つながりを断つ機会はゼロです。これは、情報の多様性の問題ではありません。つながりの新陳代謝の問題です。健全な思考には、つながりを作ることと断つことの両方が必要です。でも、エコーチェンバーの中では、作ることだけが起きて、断つことが起きません。だから、つながりは増殖し続けます。最初は小さな偏見だったものが、関連する情報を取り込んで、巨大な世界観になります。そして、その世界観を支えるつながりは、あまりに多く、あまりに複雑になって、もはや1つ1つを検証することすら不可能になります。私がエコーチェンバーから出た方がいいと思うのは、多様な意見を聞くためではありません。つながりを断つ機会を得るためです。自分が信じているつながりを、誰かに疑ってもらうためです。「それ、本当に関係あるの？」と聞かれて、立ち止まって考えるためです。検証とは、つながりを一度断つことだあのプロジェクトを一からやり直した時、私は新しいアプローチを取りました。つなげる前に、断つことから始めました。本当に必要な機能は何でしょうか。不要なものは何でしょうか。どの前提が正しく、どの前提が間違っているでしょうか。1つ1つ検証しました。そして、断つべきものを断ちました。検証とは、自分のつながりを一度断つことです。自分にとって自明なつながりを、疑ってみます。本当につながっているのでしょうか。それとも、自分がそう信じているだけでしょうか。アイデアが浮かんだら、それを他者の視点で見ます。自分一人で考えていると、自分のつながりが正しく見えます。でも、他人に説明しようとすると、論理の穴が見えます。「ここ、つながってないじゃん」と気づきます。他人は、あなたの思い込みを共有していません。だから、あなたが当然だと思っているつながりを疑います。「なぜAとBがつながるの？」と聞きます。その質問に答えられないとき、そのつながりは思い込みだったと分かります。実際に他人に説明する必要はありません。頭の中で、他人の視点を想像すればいいです。「このアイデアを、知識のない人に説明するとしたら、どう説明するか」。説明しようとすると、自分の理解が曖昧な部分が見えてきます。あなたが見ているものは、他人にも見えるでしょうか。この問いが、つながりの妥当性を確認します。自分だけに見えるつながりは、主観です。他人にも見えるつながりが、客観です。そして、実装します。頭の中でつながっていても、現実ではつながらないことがあります。理論的には正しくても、実装すると問題が出ます。だから、試します。そして、問題が出たら、そのつながりを断ちます。実装とは、つながりの淘汰プロセスです。無数のつながりを試して、ほとんどを捨てます。残ったわずかなつながりが、本当に機能するアイデアになります。ここで重要なのは、部分的に断つ能力です。アイデア全体を捨てるのではなく、うまくいかない部分だけを捨てます。AとBとCのつながりのうち、Bだけがうまくいかないなら、Bを断って、AとCのつながりを残します。そして、Bの代わりにDを試します。破棄とは、全体を捨てることではなく、不要な部分だけを切り離すことです。手術のように、病んだ部分を切除して、健康な部分を残します。問題解決とは、既存のつながりを断つことから始まるプロジェクトをやり直した結果、設計はシンプルになりました。理解しやすくなりました。開発速度は上がり、バグは減り、パフォーマンスも改善しました。そして何より、チーム全員が幸せになりました。何が変わったのでしょうか。つなげることを減らしました。断つことを増やしました。課題を設定する段階で、他のすべての課題を断ちました。収集する段階で、無関係な情報を断ちました。咀嚼する段階で、ほとんどの組み合わせを断ちました。実装する段階で、うまくいかない部分を断ちました。無数の可能性の中から、ほとんどを捨てました。残ったわずかなものを磨きました。それが、問題解決でした。彫刻家は、石を削ります。削ることで、形が生まれます。作家は、言葉を削ります。削ることで、文章が研ぎ澄まされます。エンジニアは、コードを削ります。削ることで、設計が明確になります。問題解決とは、加えることではなく、削ることです。つなげることだけではなく、断つことです。そして、断つことができて、初めて、本当に価値のあるつながりが残ります。断つ技術を身につけるでは、どうすればつながりを断てるようになるのでしょうか。私は新しい技術を学ぶとき、必ず反証を探します。「この技術は素晴らしい」という宣伝文句を読んだら、すぐに「この技術の欠点は何か」を探します。「どんな場合には向いていないか」を調べます。最初から反証を探すことで、技術と「素晴らしい」の間の安易なつながりを断ちます。そして、どんな文脈で、どんな問題に対して、この技術が有効なのか、正確に理解できます。コードを書いたら、一度捨てます。ゼロから書き直します。同じ機能を、別のアプローチで実装してみます。これは時間の無駄に見えるかもしれません。でも、最初の実装と「正しい」の間のつながりを断つ訓練になります。「動いたから正しい」と思い込みません。別のアプローチの方が、もっと良いかもしれません。実際に書き直してみると、最初の実装の問題点が見えてきます。一年前の自分と、今の自分で、考えが変わったことを書き出します。「以前はこう思っていたが、今はこう思う」。これは、過去の自分と現在の自分の間のつながりを断つ訓練になります。「過去の自分が信じていたことは、今の自分も信じるべきだ」という思い込みを捨てます。実際にやってみると、驚くほど多くのことが変わっていることに気づきます。時間をかけたものを、躊躇なく捨てます。三時間かけて書いたコードでも、より良いアプローチがあれば書き直します。一週間かけて調べた技術でも、プロジェクトに合わなければ採用しません。これは、努力と成果の間のつながりを断つ訓練になります。「時間をかけたから価値がある」という思い込みを捨てます。価値があるかどうかは、どれだけ時間をかけたかではなく、どれだけ問題を解決するかで決まります。自分が信じていることを、他人に説明します。特に、その分野に詳しくない人に説明します。説明しながら、「あれ、これ、うまく説明できないな」と気づくことがあります。それは、自分の理解と「正しい」の間のつながりが、実は曖昧だったということです。説明できないということは、本当は理解していないということです。つながりと断つことの往復運動つながりを断つことは、難しいです。認知的にも、感情的にも、社会的にも。一度見出したパターンを忘れることは、ほとんど不可能です。自分の判断が間違っていたと認めることは、苦痛です。周りの人間が信じているつながりを断つことは、孤立を意味します。それでも、断たなければなりません。なぜなら、断たなければ、成長できないからです。間違ったつながりを持ち続けている限り、正しいつながりは作れません。古い理解を手放さない限り、新しい理解は得られません。過去の自分に固執する限り、未来の自分にはなれません。断つことは、破壊ではありません。更新です。古いバージョンを削除して、新しいバージョンをインストールすることです。プログラムは、定期的に更新しなければ、脆弱性を抱えたまま動き続けます。思考も同じです。定期的につながりを見直して、間違ったつながりを断って、新しいつながりを作らなければ、脆弱なまま考え続けることになります。おわりにあのプロジェクトから数年が経ちました。今、私は別のプロジェクトに取り組んでいます。相変わらず、設計で悩むことはあります。アプローチで迷うことはあります。でも、以前とは違うことが1つあります。躊躇なく捨てられるようになりました。一週間かけて書いたコードでも、より良い方法があれば書き直します。チーム全員で決めた設計でも、問題があれば提案し直します。昨日まで正しいと思っていたことでも、今日は疑えます。これは能力の問題ではありませんでした。姿勢の問題でした。サンクコストを恐れない姿勢。過去の判断に縛られない姿勢。そして何より、間違いを認めることを恐れない姿勢。人間は、つながりを見出す生き物です。パターンを探します。関係性を発見します。意味を作り出します。これは本能です。でも、つながりを断つことは本能ではありません。意識して訓練しなければ、身につきません。プログラミングを学び始めたとき、私は「どうやってつなげるか」ばかり考えていました。データ構造とアルゴリズムをつなげます。フロントエンドとバックエンドをつなげます。理論と実装をつなげます。でも、本当に重要だったのは「どうやって断つか」でした。間違ったアプローチを断ちます。無駄な複雑性を断ちます。過去の判断を断ちます。そして、最も難しいのは、自分の思い込みを断つことでした。つながりを断つことは、否定ではありません。更新です。昨日の自分を否定するのではなく、今日の自分にアップデートします。古いバージョンを削除して、新しいバージョンをインストールします。でも、ここで誤解してほしくないことがあります。これは「つながるな」「つなげるな」という話ではありません。つながることは人間の本能であり、問題解決の源泉です。それを否定することは、人間であることを否定することに等しいです。私が言いたいのは、つなげたものを、多様な面で検証してほしいということです。「AとBは関係がある」と思ったとき、それを検証します。技術的に正しいでしょうか。論理的に整合しているでしょうか。他の事例でも成り立つでしょうか。他者から見ても妥当でしょうか。実装してみて機能するでしょうか。そして、検証した結果、うまくいかなかったら、そのつながりを保持しておいてよいか、もう一度考えます。もしかしたら、部分的には正しいかもしれません。ある条件下では有効かもしれません。別の文脈では使えるかもしれません。だから、すぐに断つ必要はないこともあります。でも、「常に正しい」「すべての状況で有効」と思い込むのは危険です。つながりには、適用範囲があります。前提条件があります。文脈があります。これらを無視して、つながりを普遍化しないこと。「この状況では有効だが、別の状況では違うかもしれない」と認識すること。この謙虚さが、つながりを適切に扱う技術の核心です。検証してダメだったつながりを、無理に保持し続けません。でも、すぐに捨てる必要もありません。保留にしておきます。別の角度から見直します。条件を変えて試してみます。そして、やはりダメだと分かったら、そこで初めて手放します。異なる知識をつなげます。異なる視点をつなげます。異なる人々をつなげます。そして、検証します。技術的に。論理的に。実践的に。多様な面から。そして、考え直します。このつながりは本当に有効でしょうか。どんな条件で成り立つでしょうか。どんな状況では成り立たないでしょうか。つなげることと検証すること。そして必要なら手放すこと。この往復運動ができて、初めて、本当の解決策が生まれます。そして、つながりに対して誠実であることが、この往復運動を可能にします。参考資料アイデアのつくり方作者:ジェームス W.ヤングCCC MEDIA HOUSEAmazon世界は認知バイアスが動かしている 情報社会を生きぬく武器と教養作者:栗山 直子SBクリエイティブAmazon情報を正しく選択するための認知バイアス事典作者:情報文化研究所フォレスト出版Amazon情報を正しく選択するための認知バイアス事典 行動経済学・統計学・情報学 編作者:情報文化研究所フォレスト出版AmazonTHINK BIGGER 「最高の発想」を生む方法：コロンビア大学ビジネススクール特別講義 (NewsPicksパブリッシング)作者:シーナ・アイエンガーニューズピックスAmazonTHINK AGAIN 発想を変える、思い込みを手放す (単行本)作者:アダム・グラント三笠書房Amazonリバース思考　超一流に学ぶ「成功を逆算」する方法作者:ロン・フリードマンかんき出版Amazon具体と抽象作者:細谷 功dZERO（インプレス）Amazon構想力が劇的に高まる アーキテクト思考――具体と抽象を行き来する問題発見・解決の新技法作者:細谷 功,坂田 幸樹ダイヤモンド社Amazon危険だからこそ知っておくべきカルトマーケティング作者:雨宮純ぱる出版Amazon増補改訂版 スマホ時代の哲学 なぜ不安や退屈をスマホで埋めてしまうのか (ディスカヴァー携書)作者:谷川嘉浩ディスカヴァー・トゥエンティワンAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud で目指す クラウド二刀流エンジニア講座第3回に登壇してきました。]]></title>
            <link>https://blog.masasuzu.net/entry/2025/11/16/203546</link>
            <guid isPermaLink="false">https://blog.masasuzu.net/entry/2025/11/16/203546</guid>
            <pubDate>Sun, 16 Nov 2025 11:35:46 GMT</pubDate>
            <content:encoded><![CDATA[11/14に Google Cloudで目指すクラウド二刀流エンジニア講座第3回 に登壇してきました。cloudonair.withgoogle.com第1回でパネルディスカッションに出てきたのに引き続き、今回は Fargate との差分で理解する、Cloud Run のシンプルな魅力 と題して登壇させていただきました。AWSをもうすでに使ってる方向けにECS Fargateと比較しつつCloud Runのシンプルな魅力を紹介するセッションとなっていました。視聴登録すればこちらから動画が見れるかと思います。資料も後ほどこちらに上がると思います。内容としては以下のような話をしました。Cloud Runの概要紹介ECS FargateとCloud Runのアプリケーションアーキテクチャ比較ネットワーク機能の紹介セキュリティ機能の紹介運用監視機能の紹介今回はCloud Runを使ってない人向けに概要紹介するセッションだったので、深入りできていない部分や説明不足、端折ったところがたくさんあります。これについてはどこかでエントリ書きたいと思ってます。とにかくCloud Runはいいぞ!ということが伝わっていれば何よりです。Cloud Runの良さを伝えたつもりですが、あくまでなんでも適材適所でFargateが向いているコンテキストに無理やり変える必要はなく、比較検討できる選択肢の一つとしてCloud Runを入れていただけたらと思います。スライドにもう少し図表を入れた方が伝わったかなとか反省点はありつつも、40分トークは初めてだったのですごくいい経験になりました。今後ももっと大きなところで登壇できるように精進していきます。]]></content:encoded>
        </item>
    </channel>
</rss>