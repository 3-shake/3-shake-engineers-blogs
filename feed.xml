<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sat, 13 Dec 2025 11:40:57 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[『おい、テックブログを書け』というタイトルで登壇しました]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/13/145159</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/13/145159</guid>
            <pubDate>Sat, 13 Dec 2025 05:51:59 GMT</pubDate>
            <content:encoded><![CDATA[はじめに正直に言うと、私はキャリアの序盤、破滅的な文章を書く人間だった。誰が読むのか考えていない文章を書きまくっていた。学生時代に読書感想文のコンクールで優勝したこともなければ、文章を褒められた経験もほとんどない。それでも書き続けて、今はこうして登壇の機会をいただいている。2025年12月5日、Forkwell Communityのイベント「おい、テックブログを書け」で登壇しました。forkwell.connpass.com発表資料はこちらです。 speakerdeck.com「おい、」シリーズがイベントになった私は「おい、」シリーズというブログを書いている。元々は書籍用に書き溜めていた文章を公開する場所として始めたものだが、ありがたいことに多くの反響をいただいている。syu-m-5151.hatenablog.com今回のイベントは、Forkwellのかわまたさんにお誘いいただいて実現した。かわまたさんには以前も「転職したらMCPサーバーだった件」というイベントでお声がけいただいた。 speakerdeck.com貴重な登壇の機会をいただいているが、結構変なことをさせてくれる。変な人だ（褒めている）。自分もこれぐらいふざけた企画をできるくらい組織で信用されたい。こうした機会をもらえるのは、発信を続けてきたからだ。私よりエンジニアとしても語り手や書き手としても才能のある人はたくさんいる。でも、その才能を発揮せずに誰からも見つからないままでいる人も多い。なぜ発信しないのか。まず、炎上が怖い。間違ったことを書いたら叩かれるんじゃないか。知識不足を晒して恥をかくんじゃないか。そう思うと、公開ボタンを押す手が止まる。次に、時間がない。業務が終わってから記事を書くのは大変だ。言いたいことを整理して、文章にまとめて、推敲して。そこまでの気力が残っていない日も多い。組織の問題もある。評価制度が発信を評価しない会社では、ブログを書いても給料は上がらない。それどころか「そんな暇があったらコードを書け」と言われることもある。発信は「業務外の趣味」として扱われる。こうした障壁は確かに存在する。でも、それらすべてを解決してから書き始める必要はない。まず書いてみることなら、今日からでもできる。炎上が怖いなら、小さな技術メモから始めればいい。時間がないなら、完璧を目指さず短い記事でいい。組織が変わらなくても、自分のブログは自分で始められる。書き始めるとき、人は出発点ばかり気にしがちだ。「自分には文章の才能がない」「最初からうまく書ける人には敵わない」──そう思って発信をためらう人がいる。でも、書く力は後天的になんとかなる。出発点が低くても、続けていれば追いつける。追い越せることだってある。見てくれた皆さんには、発信やアウトプットを通じて才能を発揮し、それに見合った評価や機会を得てほしい。そう思って今回の登壇資料を作った。書くときに大切にしていること資料では「どう書くか」の型を紹介したが、その前提にある考え方も書いておきたい。私が意識しているのは3つある。「なぜ」を問うこと、「変化」を描くこと、「ゆらぎ」を残すこと。この3つは独立しているようで、実は重なり合っている。「なぜ」を問い続ける単なる事実や記録ではなく、理由や背景を深掘りする姿勢が大切だ。たとえば「Aを使った」だけでなく「なぜAを選んだのか」「なぜBではダメだったのか」を書く。読者が最も知りたいのは「なぜ」の部分だ。選択の理由を言語化することで、自分の理解も深まる。ところが、技術ブログでありがちなのは、手順だけを淡々と書いてしまうこと。「この設定を入れます」「このコマンドを実行します」──それだけでは公式ドキュメントの劣化コピーになる。「なぜこの設定なのか」「なぜこの順番なのか」「なぜ他の方法ではダメだったのか」を書くことで、初めて読む価値が生まれる。「なぜ？」の部分が業務事情に抵触する場合もある。具体的な数値や社内の意思決定プロセスは書けない。そういうときは、一般的な観点に置き換える工夫をすればいい。「弊社の事情で」ではなく「〇〇のようなケースでは」と書く。具体的な比較ができないなら「一般的にAとBにはこういう違いがある」と整理する。工夫次第で、機密を守りながら「なぜ」を伝える方法はいくらでもある。「なぜ？」を問い続けると、自分の理解の浅さに気づくこともある。それでいい。書くことは、自分の理解を試す行為でもある。書けないということは、わかっていないということだ。その気づきこそが成長の起点になる。「行動」と「変化」のあるストーリーにする「なぜ」を問い続けていると、自然と「変化」が見えてくる。最初はこう思っていた、でも調べていくうちにこう変わった。その変化こそが、記事の核になる。人は変化の物語に心を動かされる。問題に出会い、試行錯誤し、解決に至る。その過程で自分の理解がどう変わったか。「わからない」から「わかった」への変化こそが、読者にとって価値のある情報だ。だから、静的な情報の羅列は退屈だ。「Kubernetesのリソース制限には以下の種類があります」と書くより、「OOMKilledで3時間溶かした。原因を調べていくうちに、リソース制限の仕組みが腹落ちした」と書く方が読まれる。同じ情報でも、変化の物語として語ることで、読者は追体験できる。行動と変化を意識すると、自然と時系列が生まれる。最初に何を思っていたか、何をしたか、何が起きたか、どう理解が変わったか。この流れがあるだけで、記事は格段に読みやすくなる。そして、変化には「失敗」も含まれる。むしろ失敗からの学びの方が読者には刺さる。「最初からうまくいきました」という記事より、「こう考えて失敗し、別のアプローチで解決した」という記事の方が、読者の記憶に残る。失敗を隠さず、そこから何を学んだかを書くことで、記事に深みが出る。「気持ちのゆらぎ」を素直に残す失敗を書くとき、その時の迷いや不安も一緒に残しておくといい。整いすぎた文章は、かえって心に響かない。なぜか。人間味が消えてしまうからだ。「最初は〇〇だと思っていたけど、実際は違った」「正直、これでいいのか迷った」「ここは今でも自信がない」──そうした揺れを正直に書くことで、読者との距離が縮まる。完璧を装う必要はない。技術ブログを書くとき、つい「わかっている人」として振る舞いたくなる。でも、読者が共感するのは「わかっていなかった人がわかるようになる過程」だ。迷い、間違え、遠回りした経験こそが、読者にとって価値がある。気持ちのゆらぎを残すことには、もう1つ意味がある。後から読み返したとき、その時の自分に出会える。「あの頃はこんなことで悩んでいたのか」と思えるのは、整いすぎていない文章だからこそだ。ゆらぎを残すことに抵抗がある人もいるだろう。弱く見えるのではないか、と。でも私の考えは違う。ゆらぎを残すことは、弱さを見せることではない。誠実さを見せることだ。「これが正解です」と断言する記事より、「私はこう考えてこうした、でも別の方法もあるかもしれない」と書く記事の方が、読者は信頼する。技術の世界に絶対の正解は少ない。その不確かさに正直であることが、かえって記事の信頼性を高める。おわりに技術ブログを書くことは、自分の成長のためだ。「なぜ？」を問い続け、変化の物語として語り、気持ちのゆらぎを素直に残す。結果として、それが誰かを救うこともあるかもしれない。私自身がそうだった。冒頭で書いたように、私は「破滅的な文章を書く人間」だった。それでも書き続けて、今がある。苦手から逃げても、その先にあるのはまた別の苦手だ「文章が苦手だから書かない」「人前で話すのが苦手だから発信しない」──そう言って避け続ける人は多い。気持ちはわかる。苦手なことに向き合うのは辛い。できない自分を直視するのは苦しい。でも、逃げた先に何があるだろうか。苦手なことを避け続けても、人生から苦手がなくなるわけではない。文章から逃げれば、別の場面でまた「苦手」にぶつかる。逃げ続けた結果、選択肢がどんどん狭まっていく。気づいたときには、逃げ場すらなくなっている。いま苦手であることと、将来成果を出せるかどうかには、何の因果関係もない。初期能力が高い人が最終的に優れた成果を出すとは限らない。むしろ、最初から得意な人は壁にぶつかったとき折れやすい。苦手だった人の方が、泥臭く続ける力を持っていたりする。私は明らかに後者だった。最初からうまく書けたわけではない。読み返すと恥ずかしい文章をたくさん書いた。それでも書き続けた結果、今がある。出発点の低さは、到達点を決めない。「自分探し」という名の逃避「自分に向いていることを見つければ、苦労せずに成果が出る」──そんな幻想がある。「自分探し」という言葉は、その幻想を正当化する。本当の自分を見つければ、努力なしに輝ける場所がある。そう信じたい気持ちはわかる。でも、多くの場合それは苦手や欠損から逃れるための言い訳でしかない。向いていないから別のことを探す。それも向いていないから、また別のことを探す。その繰り返しで時間だけが過ぎていく。本当の自分は、探すものではない。目の前のことに向き合い、苦手なことに取り組み続ける中で、少しずつ形作られていくものだ。「これが自分だ」と思えるようになるのは、何かをやり抜いた後だ。やる前からわかるものではない。向いていることを探すより、目の前のことに向き合う方が、よほど確実に成長できる。向いているかどうかは、やってみなければわからない。やり続けてみなければわからない。最初の苦手意識だけで判断するのは、あまりに早すぎる。正しい方向に努力すれば、必ず上達するとはいえ、漫然と続けるだけでは上達しない。書くことと、うまくなることは、自動的にはつながらない。読者の反応を見る。うまい人の文章を読んで、何が違うのか考える。自分の過去記事を読み返して、恥ずかしくなる。そうやってフィードバックを受け取り、意識的に改善しようとすることで、少しずつ書けるようになる。大事なのは、フィードバックループを回すことだ。書く→反応を見る→改善点を見つける→また書く。このサイクルを回し続ければ、必ず上達する。才能の有無ではなく、このループを回し続けられるかどうかが、成長を決める。こう書くと「それは書けた側の言い分だ」と思う人もいるかもしれない。生存者バイアスじゃないか、と。確かにそうだ。書けなかった人の声は届かない。でも、だからこそ書けた側が伝えるしかない。書けないと思っている人、文章に自信がない人に、「それでも書ける」と伝えられるのは、同じ場所から歩いてきた人間だけだ。だから、今日からでも始めてほしい。まずは今日学んだこと、ハマったこと、気づいたことを3行だけ書いてみる。下書きのまま放置している記事があるなら、不完全でもいいから公開してみる。完璧を待っていたら、いつまでも始まらない。苦手だと思っていることほど、始めてしまえば案外なんとかなる。登壇・技術顧問のご依頼について登壇依頼はいつでも募集しています。今回のようなちょっと変わった企画でも大歓迎です。気軽にDMしてください。また、技術顧問業もやっています。SRE、プラットフォームエンジニアリング、組織づくりなど、雑多な質問でもお待ちしております。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Professional Agentic AI編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_pro_agentic_ai</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_pro_agentic_ai</guid>
            <pubDate>Sat, 13 Dec 2025 05:21:28 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるProfessional Agentic AIを取得したので、その内容を共有しようと思います。 Professional Agentic AIとは？Professional Agentic AI（以下、NCP-AAI）は、マルチエージェントインタラクションや分散推論、スケーラビリティ、倫理的セーフガードに重点を置き、高度なエージェントAIソリューションを設計、開発、展開、管理する能力を試される試験です。エージェント開発だけなくそのサービングやモニタリングなど、DevOpsやMLOpsに関わるような内容が網羅的に出されるのが特徴です。https:/...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：ブログネタってどうやって探してる？お答えします]]></title>
            <link>https://zenn.dev/akasan/articles/blog_neta_howto</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/blog_neta_howto</guid>
            <pubDate>Fri, 12 Dec 2025 14:01:24 GMT</pubDate>
            <content:encoded><![CDATA[今回は短編です。のべ240日程度連日テックブログを書いている私ですが、どのようにネタを探しているのかを共有しようと思います。 そもそも何で毎日ブログ書いてるの？詳細は以下の記事にて共有していますが、今改めて毎日書いているモチベをまとめると以下になります。自分の技術に対する興味をせっかくなら発信したいそもそも三日坊主だったので、ちゃんと習慣化できるようになりたかった今更引けないw単純に楽しいhttps://zenn.dev/akasan/articles/4aba4d3a0616ce ネタの探し方私のブログではいくつかの要因によってネタが決まっています。選び方の順...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[専門家は話さないですよ(『専門家が「力」をセーブせずに全力で専門性を振り回してもリスペクトされる組織をつくりたい』を読んで)]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/12/163220</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/12/163220</guid>
            <pubDate>Fri, 12 Dec 2025 07:32:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめに正直に言う。この文章を書くかどうか、ずいぶん迷った。「専門家はもっと声を上げるべきだ」という意見に対して、「いや、話さないんですよ」と返すのは、なんだか後ろ向きに見えるかもしれない。諦めているように聞こえるかもしれない。そういう風に受け取られるのは、ちょっと嫌だな、と思った。でも、書くことにした。なぜなら、「話せばいいじゃん」「振りかざせばいいじゃん」という言葉に、ずっと違和感を抱えてきたからだ。その違和感の正体を、自分なりに言葉にしてみたかった。これは、専門家として組織の中で働いてきた、私個人の経験と考えだ。すべての人に当てはまるとは思わない。でも、同じような経験をしている人が、もしかしたらいるかもしれない。そういう人に届いたらいいな、と思いながら書いている。「専門性の刃で殴りかかってこい」への違和感「専門家が『力』をセーブせずに全力で専門性を振り回してもリスペクトされる組織をつくりたい」という記事を読んだ。専門家はプロらしく専門知識を振りかざしてほしい。そこに忖度はいらない。殺す気でかかってきていい。言っていることが難しくて良い。専門家ってのは、そういうもんだろ、と。痛快だし、気持ちはわかる。専門家がセーブせずに力を振るえる組織という理想像は、多くのエンジニアやデザイナーが心の底で望んでいることだろう。その理想を堂々と語る姿勢には敬意を覚える。フジイさんの記事は、専門家の側に立って「もっと力を発揮していいんだ」と背中を押してくれる。それ自体は素晴らしいことだ。でも、私はこの説明に違和感がある。そして、専門家としては、そういう組織を期待して待っていても仕方ないと思っている。専門家は話さない。振りかざす以前の問題として、そもそも話さない。fujii-yuji.net話すことの面倒くささ専門家が話さない理由は、実はとても単純だ。面倒くさいのだ。自分の中にある専門的な知見を言葉にして口から出した瞬間、それは相手の解釈に委ねられる。どれだけ正確に伝えようとしても、相手の受け取り方は相手次第だ。ずれが生じる。これはいかなるコミュニケーションにおいても不可避だ。そして、ずれた理解に基づいて余計なことを言われる。「それってつまりこういうことですよね」と、全然違う解釈を返される。「でもそれって現実的じゃないですよね」と、文脈を無視した反論が来る。そのたびに「いや、そういうことではなくて」と釈明しなければならない。これが、本当に面倒くさい。だから専門家は話さない。話しても伝わらないし、伝わらなかったときの釈明が面倒くさいから。専門家の言葉が届かなくなるとき他にも組織の中で、専門家の言葉が届かなくなる瞬間がある。エンジニアが「この設計だと将来困ります」と言っても、「今はそれどころではない」と退けられる。デザイナーが「このUIは使いにくい」と指摘しても、「ユーザーは慣れる」と押し切られる。セキュリティの専門家が「この実装は危険です」と警告しても、「リリースを優先して」と言われる。なぜこうなるのか。専門家の意見と非専門家の意見が、同じ重みで扱われるからだ。あるいは、声の大きさや立場の強さで、専門家の意見が上書きされるからだ。「それはあなたの感想ですよね」と言われる。「他の見方もある」と言われる。正しいことを言っているのに、「意見の違い」として処理される。これは単なる無関心ではない。専門知への拒絶だ。専門家が何か言うと、面倒くさがられる。「また難しいことを言っている」「理想論だ」「現場を知らない」と思われる。そのうち、専門家の発言自体が疎まれるようになる。私はこれを「専門家の言葉が死ぬ瞬間」だと思っている。言葉が発せられても、届かない。届いても、受け止められない。受け止められても、行動に変わらない。そういう組織では、専門家は黙るようになる。分業が生む視野の狭さなぜこうなるのか。私なりに考えてみた。こんな経験がある。セキュリティの観点から「この実装は危険だ」と2回警告した。2回とも「リリースを優先して」と言われた。3回目は言わなかった。そして半年後、その実装が遠因でインシデントが起きた。「なぜ発生した」と言われた。言ったんだけどな、と思った。組織が大きくなると、分業が進む。営業、開発、デザイン、マーケティング。それぞれが専門性を高め、効率よく仕事を回せるようになる。これ自体は正しい。でも、分業には副作用がある。自分の担当範囲だけを見るようになる。隣のチームが何をしているか、知らなくても仕事は回る。全体像が見えなくなる。自分の視野がどんどん狭くなっていることに、気づかない。視野が狭くなると、判断がずれる。自分の担当範囲では正しいことが、全体で見ると間違っていることがある。でも、全体が見えないから、そのずれに気づけない。そして、何かを変えようとしても、壁にぶつかる。「それは私の管轄じゃない」「そっちのチームに言ってくれ」「今はそれどころじゃない」。組織の境界線が、行動を阻む。やがて、組織全体が「どうもうまくいっていない」と感じるようになる。でも、何が問題なのかがわからない。みんなが自分の持ち場で懸命に働いているのに、全体としては空回りしている。これが、ある程度成熟した組織が陥る罠だ。誰かが悪いわけではない。構造がそうさせている。話しても届かない構造この構造の中で、専門家はどうなるか。まず、自分の視野が狭くなっていることに気づかなくなる。自分の担当領域のことしか見えない。全体像が見えないから、自分の懸念が組織全体にとってどれだけ重要か、判断できない。「言っても仕方ない」と思ってしまう。次に、何か言っても壁にぶつかる経験を重ねる。「それは私の管轄ではない」「今はそれどころではない」と言われる。何度かそういう経験をすると、言うこと自体をやめる。学習性無力感だ。そして、本質的な問題を指摘しても、表面的な対応で済まされる。「技術的負債を返済しないと」と言っても、「今月のリリースが優先だ」と返される。根本的な問題が見えない組織では、根本的な指摘は届かない。専門家が話さないのは、怠けているからではない。プロ意識が低いからでもない。話しても届かない構造の中に置かれているからだ。何度も壁にぶつかって、学習した結果だ。専門家と非専門家の間にある溝専門家の言葉が届かないのは、誰かが悪いからではない。専門家は自分の領域を深く知っている。だからこそ、何が重要で何が危険かがわかる。でも、その「わかる」が、相手に伝わるとは限らない。非専門家には、非専門家の世界がある。締め切りがあり、予算があり、上からのプレッシャーがある。彼らは彼らなりに合理的に判断している。専門家の言うことが理解できないとき、「今は優先度が低い」と判断するのは、彼らにとっては当然のことだ。つまり、どちらも自分の世界では正しいことを言っている。問題は、それぞれの世界が交差しないことだ。専門家の「危険です」と、非専門家の「今はそれどころじゃない」が、同じ言語で話されているようで、まったく違う文脈に立っている。この溝を埋めるには、お互いの世界を理解しようとする努力がいる。でも、その努力には時間がかかる。そして、時間をかける余裕がない組織では、溝は埋まらないまま放置される。専門知識を振りかざしても、この溝は埋まらない。むしろ、溝を広げてしまうことさえある。「振りかざす」だけでは何も変わらないフジイさんは「専門知識を振りかざせ」と言う。力をセーブするな、忖度するな、と。気持ちはわかる。でも、私の経験では、振りかざしてもうまくいかなかった。専門家が強く主張すればするほど、相手は身構える。「また難しいことを言い始めた」「仕事を遅らせるつもりか」と思われる。こちらは正しいことを言っているつもりなのに、「面倒くさい人」として扱われる。そして、一度そういう印象を持たれると、次から話を聞いてもらえなくなる。「あの人はいつも理想論ばかり言う」というレッテルが貼られる。正しいことを言っているのに、聞いてもらえない。悪循環だ。振りかざすという態度は、相手に「自分の世界を押し付けられている」と感じさせる。人は押し付けられると、反発する。これは自然な反応だ。だから、振りかざしても状況は良くならない。むしろ、悪くなることのほうが多い。対話とは何かじゃあ、どうすればいいのか。私は「対話」だと思っている。ただし、ここで言う対話は「話し合う」という単純なものではない。対話とは、相手の世界に入っていくことだ。相手が何を見ているのか。何を気にしているのか。何を恐れているのか。どういうプレッシャーの中にいるのか。それを理解しようとすること。そして、理解した上で、相手の言葉で、相手の文脈で、自分の知っていることを伝えること。これは「振りかざす」とは正反対の態度だ。振りかざすとは、自分の世界から一歩も出ないまま、相手に自分の言葉を投げつけること。相手が理解できないなら、相手が悪い。対話とは、自分の世界を一度脇に置いて、相手の世界に足を踏み入れること。相手の言葉で考え、相手の文脈で説明する。これは、とても難しい。そして、とても面倒くさい。対話のコストを払える組織対話には膨大なコストがかかる。相手の世界を理解するために時間をかける。相手の言葉で説明するために言葉を選ぶ。ずれが生じたら、丁寧に修正する。誤解が生まれたら、根気強く解きほぐす。このコストを、組織が払えるかどうか。「今月のリリースが優先だ」「そんな時間はない」「とにかく早く作れ」という組織では、対話のコストは払えない。対話に時間をかける人は「仕事が遅い人」として評価を下げられる。対話のコストを払える組織とは、どういう組織か。意思決定のプロセスに専門家を巻き込む組織。評価制度が「言われたものを早く作る」ではなく「価値あるものを作る」を評価する組織。専門家の意見が「めんどくさいこと」ではなく「必要なこと」として扱われる組織。そして、相手の世界を理解しようとする姿勢が、当たり前のこととして共有されている組織。対立を放っておかない対立は放っておくと腐る。私自身、何度も失敗した。相手の話を遮って、自分の正しさを主張して、結局何も変わらなかった。そのたびに学んだのは、急いで反応しないことの価値だ。対立を放っておくと気まずさが積み重なる。仕事の判断がぶれ、人が協力しにくくなる。でも、対立が起きた瞬間に「正しさ」で押し切ろうとすると、もっと悪くなる。私はそれを何度も経験した。だから今は、衝突の場面では一度立ち止まるようにしている。相手の話を最後まで聞く。相手が何を恐れているのか、何を守ろうとしているのかを理解しようとする。それだけで、相手の硬さがゆるむことがある。争点をはっきりさせると、不要な言い合いが減る。「ここは合意できる」「ここは意見が違う」と整理するだけで、議論が前に進む。小さな合意を積み上げると、相手への不信が弱まる。これは言うのは簡単だが、やるのは難しい。私も何度も失敗した。でも、やる価値はある。専門家が話せる組織を作るというのは、対立を避けることではない。対立が起きたときに、それを丁寧に扱える組織を作ることだ。「あなたになら話したい」専門家が話すのは、話しても大丈夫だと思える相手に対してだけだ。自分の言葉が曲解されない。余計な解釈を加えられない。「そういうことではない」と釈明する必要がない。相手が自分の世界を理解しようとしてくれている。そういう相手に対してだけ、専門家は話す。「あなたになら話したい」——この感覚が、専門家に話をさせる。話すことのリスクでも、「あなたになら話したい」と思える相手は、実はとても少ない。専門家が話さないのは、構造の問題だけではない。話すこと自体に、あまりにもリスクがある。曲解される。余計なことを言われる。釈明が必要になる。プライドを刺激する。張り合われる。情報を軽々しく扱われる。他の人に言いふらされる。これだけのリスクを負って、それでも話す価値があるか。多くの場合、ない。だから専門家は黙る。専門知識を振りかざすどころか、そもそも口を開かない。コミュニケーションの不可避的なずれどれだけ丁寧に対話しても、ずれは生じる。私が話したい出来事が言葉となって口から出た時点で、それは私のものではなくなる。相手がどう受け取るかは、相手次第だ。これは、いかなるコミュニケーションにおいても不可避だ。だからこそ、対話のコストを払う意志があるかどうかが重要になる。ずれが生じたときに、「そういうことではない」と切り捨てるのではなく、「どうずれているのか」を一緒に探る。誤解が生まれたときに、「わかってないですね」と責めるのではなく、「どう誤解されたのか」を一緒に確認する。そして、聞いたことを軽々しく他の人に話さない。他者の情報を、自分の優越感のために消費しない。そのコストを払う意志があり、その倫理観を共有できる組織に対してだけ、専門家は話す。組織に期待しても仕方ない専門家が専門家としてリスペクトされる組織。フジイさんが描く理想は、私も心から望んでいる。でも、そういう組織を期待して待っていても、来ない。組織が変わるのを待っていたら、専門家は永遠に黙ったままだ。「いつか理解してくれる組織に出会えるはず」「いつかリスペクトされる日が来るはず」。そう思って待っていても、その日は来ない。だから、専門家の側から動くしかない。振りかざすのではなく、対話する。相手の世界に入っていく。相手の言葉で、相手の文脈で、自分の知っていることを伝える。面倒くさいけど、そのコストを自分から払う。組織が対話のコストを払ってくれるのを待つのではなく、自分から払う。相手が自分の世界を理解してくれるのを待つのではなく、自分から相手の世界を理解しにいく。これは不公平だ。専門家の側だけが努力するのはおかしい。でも、待っていても状況は変わらない。専門家に「振りかざせ」と言うのは、順番が逆だ。でも、「組織が変われ」と言うのも、期待しすぎだ。組織は簡単には変わらない。変わるのを待っていたら、自分が消耗するだけだ。だから、自分から動く。対話のコストを、自分から払う。専門家は話さない、でも専門家は話さない。話しても届かないから。話しても曲解されるから。話しても釈明が面倒くさいから。話したことを軽々しく扱われるから。他人を嫌いになりたくないから。専門家の言葉が届かなくなった組織では、専門家は黙る。それは怠慢ではない。何度も壁にぶつかった結果の、合理的な適応だ。でも、黙ったままでいいのか。フジイさんの理想は素晴らしい。専門家がリスペクトされる組織。専門家が力をセーブせずに振るえる組織。私もそういう組織で働きたい。でも、そういう組織を待っていても来ない。だから、自分から動くしかない。振りかざすのではなく、対話する。面倒くさいけど、相手の世界に入っていく。組織が変わるのを待つのではなく、自分から対話のコストを払う。「あなたになら話したい」——そう思ってもらえる相手に、自分からなる。組織に期待するのではなく、自分がその一人目になる。振りかざせと言う前に、自分から対話のコストを払う。それが、専門家として生き残る唯一の方法だと、私は思っている。私もまだ道半ばだ。何度も失敗するし、面倒くさいと思うこともある。でも、やるしかない。一緒にやっていこう。おわりにここまで書いてきて、ふと思う。結局、私は何を言いたかったんだろう。「専門家は話さない」という事実を伝えたかったのか。「組織に期待するな」と言いたかったのか。「自分から動け」と説教したかったのか。たぶん、どれでもない。私が本当に言いたかったのは、「話さないことを選んでいる自分を、責めなくていい」ということかもしれない。黙っているのは怠慢じゃない。何度も壁にぶつかって、学習した結果だ。それは合理的な適応だ。でも同時に、「黙ったままでいいのか」という問いも、ずっと抱えている。この矛盾を、私はまだ解決できていない。だから、「自分から対話のコストを払う」という答えを、自分自身に言い聞かせているのかもしれない。フジイさんの記事に対する反論というより、自分への言い訳、あるいは自分への励まし。そういう側面もあると思う。この文章を読んで、「わかる」と思ってくれた人がいたら嬉しい。「違う」と思った人もいるだろう。それでいい。ただ、もし同じような経験をして、同じように黙ることを選んでいる人がいたら、伝えたい。あなたは間違っていない。でも、黙ったままでいいのか、という問いは、たぶん消えない。その問いと一緒に、私はこれからも対話のコストを払い続けるんだと思う。面倒くさいけど。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Workstations入門: 安全かつ再現可能な開発環境の作り方]]></title>
            <link>https://qiita.com/aminevg/items/b953ae647c81eef59e95</link>
            <guid isPermaLink="false">https://qiita.com/aminevg/items/b953ae647c81eef59e95</guid>
            <pubDate>Thu, 11 Dec 2025 22:08:14 GMT</pubDate>
            <content:encoded><![CDATA[この記事は 3-shake Advent Calendar 2025 (12 日目) の投稿です。こんにちは！ スリーシェイクのイリドリシ愛民 (@realaminevg) です。最近は主にクライアントワークを行なっているため、セキュリティやオンボーディングを徹底する必...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA NeMo Agent Toolkitを使ってみた]]></title>
            <link>https://sreake.com/blog/how-to-use-nvidia-nemo-agent-toolkit/</link>
            <guid isPermaLink="false">https://sreake.com/blog/how-to-use-nvidia-nemo-agent-toolkit/</guid>
            <pubDate>Thu, 11 Dec 2025 13:35:36 GMT</pubDate>
            <content:encoded><![CDATA[概要 こんにちは佐藤慧太@SatohJohnです。 NVIDIA NeMo Agent Toolkit（以下、この記事ではNATと呼ぶことにします）は生成AIに関する様々なツール・フレームワーク・言語モデルを組み合わせて […]The post NVIDIA NeMo Agent Toolkitを使ってみた first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[登壇記録：NVIDIA NIMとNVIDAI NeMo Guardrailsの紹介]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_nim_nemo_toudann</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_nim_nemo_toudann</guid>
            <pubDate>Thu, 11 Dec 2025 13:28:09 GMT</pubDate>
            <content:encoded><![CDATA[今回は本日以下のイベントで登壇しましたので、そちらの資料の共有と簡単な概要の共有になります。https://3-shake.connpass.com/event/373638/ 登壇資料の共有今回の登壇資料は以下のspeackerdeckにアップロードしておりますのでぜひご覧ください。 登壇内容今回の登壇では主に以下のトピックについて取り扱いました。NVIDIA NIMを用いた最適化された環境でのモデルのサービングについてgarakを用いたLLMの脆弱性診断NeMo Guardrailsを用いたLLMに対するガードレールの設定LLMをアプリケーションを組み込む...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年 俺が愛した本たち 技術書編]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/11/104143</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/11/104143</guid>
            <pubDate>Thu, 11 Dec 2025 01:41:43 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「今年読んで良かった本」という記事を書こうとしている自分に、ふと気づく。また書くのか。毎年書いている。誰に頼まれたわけでもないのに、12月になると決まってこの作業を始めてしまう。習慣なのか、義務感なのか、それとも単なる自己顕示欲なのか。たぶん、全部だ。100冊近く読んだ、と書こうとして手が止まる。この数字を出した瞬間、どこかで「すごいですね」と言われたい自分がいる。同時に、「いや、冊数なんて意味ないですから」と予防線を張りたがっている自分もいる。めんどくさい人間だ。でも正直に言えば、100冊読んだことより、1冊を血肉にできた人のほうがよほど偉いと本気で思っている。思っているのに、冊数を書いてしまう。そういう矛盾を抱えたまま、この文章を書いている。AIに聞けば答えは返ってくる。2025年はそういう年だった。コードを書いてもらい、設計を相談し、ドキュメントを要約させた。便利だ。本当に便利だ。では、なぜ本を読むのか。300ページもある本を、わざわざ最初から最後まで読む必要があるのか。たぶん、効率の悪さが必要なのだ。AIは正解を返してくれる。でも正解だけでは、何かが足りない。正解を得ることだけが目的なら、エンジニアをやっている意味がない。でも、そうじゃないはずだ。著者が失敗した話。遠回りした話。「今思えば、あれは間違いだった」という告白。そういう「ノイズ」が、不思議と頭に残る。正解は忘れる。でも、誰かの失敗談は覚えている。本を読んでいる時間、私は著者と対話している。いや、対話というより、ほとんど独り言だ。「それはそうだろう」と頷いたり、「いや、それは違うんじゃないか」と反発したりする。声には出さないけれど、頭の中ではずっと喋っている。その過程で、借り物の知識が少しずつ自分の言葉に変わっていく。検索では得られないもの。それを「身体性」と呼ぶのは大げさかもしれないけれど、他に適切な言葉が見つからない。読んだだけでは意味がない、と言われてきた。アウトプットしないと身につかない。実践しないと血肉にならない。わかっている。わかっているけれど、私は本を読むこと自体が好きなのだ。ページをめくる時間が好きだ。知らない概念や文脈に出会う瞬間が好きだ。だからブログを書き、登壇し、実務で試してきた。好きなことを正当化するために、アウトプットという免罪符を手に入れようとしていたのかもしれない。以下に紹介する26冊は、今年の「ベスト」ではない。そんな客観的な評価ができるほど、私は公平な人間ではない。単に「私に刺さった本」を並べただけだ。他の人には響かないかもしれない。でも、この26冊との出会いが、私の2025年を形作った。それだけは確かなことだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。はじめに昨年以前に紹介した本2025年に読んでよかった技術書Beyond Vibe CodingLLMOpsGenerative AI Design PatternsBuilding Applications with AI AgentsLearning GitHub CopilotTerraform in DepthArgo CD: Up and RunningEffective Platform EngineeringData Engineering Design Patternsソフトウェア設計の結合バランスFacilitating Software ArchitectureArchitecture ModernizationBuilding Event-Driven Microservices, 2nd EditionTaming Your Dragon: Addressing Your Technical DebtRefactoring to RustJust Use Postgres!The Software Engineer's Guidebookバックエンドエンジニアのためのインフラ・クラウド大全作る、試す、正す。アジャイルなモノづくりのための全体戦略良いコードの道しるべClean Code, 2nd Edition型システムのしくみFundamentals of Software EngineeringThe Product-Minded EngineerThe Engineering Leader"Looks Good to Me"おわりに昨年以前に紹介した本2022年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2023年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2023年 俺が愛した本たち 非技術書編 - じゃあ、おうちで学べる2024年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2024年 俺が愛した本たち 非技術書編(物語を除く) - じゃあ、おうちで学べる2025年に読んでよかった技術書Beyond Vibe Codinglearning.oreilly.comwww.oreilly.co.jp※日本語翻訳版が出版予定です。AIツールの導入が進む現場で、私が感じていた違和感がありました。生産性は上がっている。コードは早く書ける。しかし、チームメンバーがAI生成コードについて質問されたとき、「なぜこう書いたのか」を説明できない場面が増えている。Google ChromeチームのAddy Osmani氏による本書は、この違和感に「Vibe Coding」という名前を与えてくれました。Vibe Codingとは、AIが生成したコードを深く理解せずに受け入れてしまう傾向のことです。動くコードと、理解しているコードは違う——この区別は、個人の学習だけでなく、チームの品質管理にも直結します。レビューで「なぜこの実装なのか」と聞かれたとき、「AIがそう書いたから」では通らない。コードの責任は、書いた人間にある。著者は、AIを「単独で使うツール」ではなく「ペアプログラマ」として捉えることを提唱しています。この主張には同意するが、同時に違和感もある。ペアプログラマは、隣に座って一緒に考える存在だ。しかしAIは、こちらが何を求めているか察してくれない。問いを投げなければ答えは返ってこない。つまり、AIを「ペア」として機能させるためには、人間の側に「何を聞くべきか」を知っている力が必要になる。結局、AIを活かせるかどうかは、使う側の問いの質で決まる。ペアプログラマという比喩は美しいが、その美しさに甘えてはいけない。本書の終盤では、自律型コーディングエージェントがもたらす未来像が描かれています。テスト失敗時に自動で修正を試みたり、依存関係の更新PRを生成したりする世界。技術的には魅力的ですが、著者は冷静です。「AIが生成したコードの責任は、承認者にある」——この原則は変わらない。むしろ、エージェントが自律的に動くほど、人間による検証の重要性は高まる。この視点は、運用の現場を知っている人間には納得感があります。AIは相棒であって、魔法使いではない。本書は、その現実を直視しながら、AIとの協働をチームに根付かせるための実践的な指針を提供してくれます。Beyond Vibe Coding: From Coder to AI-Era Developer (English Edition)作者:Osmani, AddyO'Reilly MediaAmazonLLMOpslearning.oreilly.comLLM（Large Language Model、大規模言語モデル）を本番環境で運用し始めると、従来のMLOpsの知見だけでは対応できない課題に直面します。モデルの挙動が予測しにくい、コストが桁違いに高い、出力の品質をどう保証するか分からない——そんな困難にぶつかったとき、本書を手に取りました。著者が掲げるLLMOpsの4つの目標——信頼性、スケーラビリティ、堅牢性、セキュリティ——を見たとき、既視感がありました。これは、システムを運用する人間が長年追求してきた目標と重なる。新しい技術領域でも、運用の本質は変わらない。これまで培ってきた原則は、LLMにも適用できる——その確信を得られたことは、本書を読んだ大きな収穫でした。しかし、ここで私は立ち止まる。「従来の原則が適用できる」という安心感は、危うさも孕んでいる。LLMには従来のシステムにはない難しさがあるからだ。従来のMLモデルは、入力に対して比較的予測可能な出力を返す。しかしLLMは、同じプロンプトでも異なる応答を返すことがある。そもそも「正しい出力とは何か」が曖昧なのです。従来のシステムでは「期待する出力」を定義できた。LLMでは、それ自体が困難になる。この不確実性を前提に、どうSLO（Service Level Objective、サービスレベル目標）を設計し、どうモニタリングするか。本書はその実践的なアプローチを示してくれます。コスト管理の章も現実的で良かった。LLMのAPI呼び出しは、従来のマイクロサービス呼び出しと比較して桁違いにコストがかかる。機能要件を満たすことと、コストを現実的な範囲に収めること。このトレードオフを意識した設計と運用の知見は、実務で即座に役立つものばかりです。「正しい出力」が定義できないシステムを、どう運用するか。答えは、まだ業界全体で模索中です。正解がないから、難しい。正解がないから、面白い。本書はその議論の出発点として、LLMを本番で動かす人が押さえておくべき基盤を提供してくれます。LLMOps: Managing Large Language Models in Production (English Edition)作者:Aryan, AbiO'Reilly MediaAmazonGenerative AI Design Patternslearning.oreilly.comLLMを使ったアプリケーションを作り始めると、繰り返し同じような問題にぶつかります。ハルシネーション（AIが事実と異なる内容をもっともらしく生成してしまう現象）をどう防ぐか。長いコンテキストをどう扱うか。出力の品質をどう担保するか。これらの問題には、すでに先人たちが見つけた解決策がある。本書は、そうしたLLMの限界を克服するための32のデザインパターンを体系化した一冊です。RAG（Retrieval-Augmented Generation、検索拡張生成）、Chain of Thought（思考の連鎖）、Guardrails（安全装置）といったパターンは、今やLLMアプリケーション開発の共通言語になりつつあります。これらのパターンを知っているかどうかで、設計の議論がスムーズになるし、チーム内での認識合わせも早くなる。本書の価値は、単にパターンを列挙していることにあるのではありません。各パターンがなぜ必要か、どのような問題を解決するのか、そしてどのようなトレードオフがあるのか——その背景まで丁寧に解説している点にあります。例えば、RAGパターン。ハルシネーションの軽減策として有効なのは広く知られている。しかし本書は、RAGの導入がもたらす新たな課題も明確に指摘しています。ベクトルデータベースという新しいコンポーネントが加わり、監視対象と障害点が増える。検索の精度がLLMの出力品質を左右するため、検索システムの品質保証という新たな運用課題が生まれる。解決策は、新しい問題を連れてくる——技術選定の現場では、この現実を織り込んだ上で判断する必要があります。Chain of Thoughtパターンも同様です。複雑な推論を段階的に行わせることで出力精度が向上する。しかし、精度を上げれば、コストも上がる。APIコールが複数回になり、レイテンシーとコストが増加する。プロダクトとして許容できるコストとレイテンシーの範囲内で、どこまで精度を追求するか。このトレードオフは、技術だけでなくビジネス要件との兼ね合いで決まります。パターンを知っているかどうかで、設計の選択肢が変わる——本書は、パターンカタログとしても、チームでアーキテクチャを議論するための共通言語としても活用できます。Generative AI Design Patterns: Solutions to Common Challenges When Building GenAI Agents and Applications (English Edition)作者:Lakshmanan, Valliappa,Hapke, HannesO'Reilly MediaAmazonBuilding Applications with AI Agentslearning.oreilly.comLLMを使ったアプリケーションの次のステップとして、AIエージェントへの関心が高まっています。単に質問に答えるだけでなく、タスクを自律的に実行するシステム。しかし、エージェントを本番環境に投入しようとすると、従来のシステム運用とは異なる課題に直面します。従来のAPIは、リクエストを送れば決まった形式でレスポンスが返ってくる。処理時間もおおよそ予測できる。しかしエージェントは違う。どんな行動を取るか予測しにくい。タスクによって実行時間が大きく変わる。外部サービスへの呼び出しも、エージェント自身が判断して行う。従来のSLOの考え方が、そのままでは通用しない。では、どう運用設計するのか。本書を読んで改めて考えさせられたのは、ガードレールの設計です。エージェントは自律的に動く。自律的に動くからこそ、想定外の行動を取る可能性がある。どこまで自律性を許し、どこで人間が介入するか。この境界線を曖昧にしたまま本番投入すると、インシデント時の対応が混乱します。自律的に動くものを、どこまで信頼するか。その答えを、運用設計の段階で明確にしておく必要がある。信頼の境界線を引くのは、AIではなく人間の仕事だ。本書はその設計指針を与えてくれます。Learning GitHub Copilotlearning.oreilly.comGitHub Copilotを使い始めたころ、私はこれを「賢いオートコンプリート」だと思っていました。しかし、最近のCopilotは違います。コード補完だけでなく、チャットで質問に答え、コードの説明を生成し、テストまで書いてくれる。開発ワークフロー全体を変革する可能性を持っている。その進化に追いつくために、本書を手に取りました。インフラエンジニアとしても興味深い内容が多かった。IaC（Infrastructure as Code、インフラのコード化）の自動化、マニフェスト（Kubernetesなどの設定ファイル）の生成、パイプラインの構築。私自身、本書のテクニックが役立った場面は少なくありません。ただ、便利になればなるほど、新しい課題も生まれます。AIが生成したコードを、誰がどうレビューするのか。生成されたコードにバグがあったとき、責任は誰にあるのか。「AIがそう書いたから」では済まされない。コードの責任は、承認した人間にある。便利さの代償は、新しい責任——その両面を理解した上でCopilotを活用していきます。楽になった分だけ、考える責任が増えた。Building Applications with AI Agents: Designing and Implementing Multiagent Systems (English Edition)作者:Albada, MichaelO'Reilly MediaAmazonTerraform in Depthlearning.oreilly.comインフラをコードで管理する。Infrastructure as Code（IaC）は、もはや当たり前の実践になりました。その中でもTerraformは、クラウドを問わず広く使われている。しかし、基本的な使い方を覚えた後、どう深めていくか。本書は、TerraformとOpenTofuの両方をカバーしている点に惹かれて手に取りました。HashiCorpのライセンス変更以降、OpenTofuへの移行を検討している組織も多いでしょう。どちらを選んでも、基本的な概念やスキルは共通しています。ライセンスが変わっても、スキルは変わらない——その安心感は大きいと感じました。大規模環境でのTerraform運用では、ステート管理が最も頭を悩ませる課題の1つです。ステートとは、Terraformが管理するインフラの「現在の状態」を記録したファイルです。このファイルが壊れたり、実際のインフラと食い違ったりすると、意図しない変更が発生する危険がある。ステートが壊れたら、インフラが壊れる——この現実に正面から向き合う必要があります。インフラの信頼性を高めるためには、IaCの品質向上が不可欠です。アプリケーションコードにはテストを書くのが当たり前になっていますが、インフラコードはどうでしょうか。インフラコードも、テストなしには信頼できない——私はこの原則を実践に落とし込むために、本書を読みました。Terraform in Depth: Infrastructure as Code with Terraform and OpenTofu作者:Hafner, RobertManningAmazonArgo CD: Up and Runninglearning.oreilly.comIaCでインフラを定義できるようになったら、次はデプロイをどう自動化するか。GitOps（Gitリポジトリを中心にインフラやアプリケーションのデプロイを管理する手法）は、その答えの1つです。Gitリポジトリを唯一の真実の源とし、インフラの状態を宣言的に管理する。そのGitOpsの標準ツールとなったArgo CDを深く理解したくて手に取りました。公式ドキュメントには書かれていない設計判断の背景を知ることで、ツールの使い方だけでなく、思想を理解できると感じています。実践者が書いた本には、公式ドキュメントにはない「なぜ」がある——それが技術書を読む理由の1つです。大規模な環境でも管理可能なGitOpsワークフローを構築するためのテクニックを学べました。GitOpsの導入は、デプロイの信頼性を高めるだけではありません。変更管理の透明性が向上し、何か起きたときの原因追跡が容易になる。Gitを見れば、本番が分かる——宣言的なインフラ管理とGitによるバージョン管理の組み合わせは、チーム開発との相性が非常に良いと感じています。Argo CD: Up and Running: A Hands-On Guide to GitOps and Kubernetes (English Edition)作者:Block, Andrew,Hernandez, ChristianO'Reilly MediaAmazonEffective Platform Engineeringwww.manning.comIaCやGitOpsを導入し、インフラの自動化が進むと、次の課題が見えてきます。これらのツールやプラクティスを、どうやって開発チーム全体に展開するか。個人が使いこなしていても、チーム全体のものにならなければ意味がない。プラットフォームエンジニアリングは、その課題に対するアプローチです。しかし、技術的に優れたプラットフォームを作っても、開発者に使ってもらえなければ意味がない。使われないプラットフォームは、存在しないのと同じ——この現実は、プラットフォームチームにいると身に染みてわかります。本書が一貫して主張するのは、プラットフォームを「プロダクト」として扱うというマインドセットです。プラットフォームチームはインフラを提供するだけでなく、開発者体験を向上させる製品を開発している。開発者は顧客であり、彼らのフィードバックを受けて改善を続ける。インフラチームではなく、プロダクトチームである——この視点の転換は、チームの動き方を根本から変えます。この主張には強く共感する一方で、現実の難しさも感じている。「開発者は顧客」と言うのは簡単だ。しかし、顧客である開発者の要望をすべて聞いていたら、プラットフォームは一貫性を失う。標準化と柔軟性のバランス。セキュリティと利便性のトレードオフ。「顧客の声を聞く」と「顧客の言いなりになる」は違う。プロダクトチームとして振る舞うなら、時には「それはできません」と言う勇気も必要になる。本書はその難しさにも触れているが、私はもっと掘り下げてほしかった。開発者の認知負荷を下げながら、システムの信頼性を維持する。このバランスは、簡単ではありません。抽象化しすぎると、開発者がトラブルシューティングできなくなる。抽象化が足りないと、認知負荷が下がらない。プラットフォームの成功は、開発者の生産性で測る——この原則を軸に、どこまで抽象化するかを判断していく必要があります。Effective Platform Engineering (English Edition)作者:Chankramath, Ajay,Alvarez, Sean,Oliver, Bryan,Cheneweth, NicManningAmazonチームトポロジー　価値あるソフトウェアをすばやく届ける適応型組織設計作者:マシュー・スケルトン,マニュエル・パイス日本能率協会マネジメントセンターAmazonData Engineering Design Patternslearning.oreilly.comプラットフォームを運用していると、アプリケーションだけでなくデータパイプラインの信頼性も課題になってきます。データはシステムの血液のようなもので、流れが止まれば、ビジネスも止まる。データエンジニアリングにおけるデザインパターンを学びたくて手に取りました。デザインパターンとは、繰り返し現れる問題に対する定石のようなものです。先人たちが試行錯誤の末にたどり着いた解決策が、パターンとして整理されている。パターンには、先人の失敗が詰まっている——だから学ぶ価値がある。データパイプラインの信頼性、データ品質のモニタリング、レイテンシーの管理。これらの課題は、従来のアプリケーション開発とは異なるアプローチが必要です。たとえば、データパイプラインでエラーが発生したとき、どう対処するか。エラーを無視すればデータ品質が下がる。かといって、パイプライン全体を停止させれば、正常なデータまで届かなくなる。本書が紹介するパターンの1つは、問題のあるレコードを別の場所に退避させて後から対処する、というものです。1件のエラーで、100万件を止めるな——このパターンを知っているかどうかで、障害発生時の影響範囲が大きく変わります。また、データが届かないことも障害です。この視点も重要でした。アプリケーションの障害は目に見えやすいですが、データパイプラインの遅延や欠損は気づきにくい。データの品質をどう保証するか。本書から多くのヒントを得ました。Data Engineering Design Patterns: Recipes for Solving the Most Common Data Engineering Problems (English Edition)作者:Konieczny, BartoszO'Reilly MediaAmazonソフトウェア設計の結合バランスbook.impress.co.jpデータパイプラインでもアプリケーションでも、システムを構成する要素間の「結合」は避けて通れない課題です。疎結合が良い、密結合は悪い——そう教わってきたけれど、本当にそれだけで設計できるのか。この疑問に答えてくれるのが本書です。Vlad Khononov著『Balancing Coupling in Software Design: Universal Design Principles for Architecting Modular Software Systems』の翻訳本です。島田浩二さんの翻訳が秀逸で、原著の概念を自然な日本語で読めることに感謝しています。learning.oreilly.comしかし本書は、その固定観念を覆します。結合がなければ、ソフトウェアはシステムになれない。結合は悪ではない。結合は、システムを成り立たせる力だ。この主張を読んだとき、私は自分の設計判断を振り返った。「疎結合にしなければ」という呪縛に囚われて、過剰に分離したことはなかったか。分離した結果、かえって複雑になったことはなかったか。あった。確実にあった。マイクロサービスに分割したはいいが、サービス間の通信が増えて、障害の原因追跡が困難になった経験。本書の主張は、そうした失敗を言語化してくれた。本書の価値は、「結合」という概念を多次元で捉え直すところにあります。結合の強さだけでなく、結合の距離、結合の揮発性——複数の軸で分析することで、設計の判断基準が明確になる。これは手順書でもルールブックでもない。設計の意思決定に迷ったとき、インプットとして参照するための本です。どこまで結合を許容し、どこで切り離すか。その判断を支える思考の枠組みを、本書は与えてくれます。ある書評では「今後10年くらいの基礎知識になる」と評されていました。私も同感です。マイクロサービス、モジュラーモノリス、ドメイン駆動設計——どのアーキテクチャを選んでも、結合のバランスは避けて通れない。正解を教えてくれる本ではなく、正解を見つけるための視点をくれる本。そういう本こそ、長く手元に置いておきたい。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonFacilitating Software Architecturelearning.oreilly.comsyu-m-5151.hatenablog.com結合のバランスを考え、設計判断を重ねていく。しかし、その判断は誰がするのか。アーキテクトの役割が変わりつつあります。一人の天才が全てを決める時代から、チーム全体でアーキテクチャを育てていく時代へ。アーキテクトは、決める人から、決められるようにする人へ——この変化は、私自身の仕事のやり方にも影響を与えています。本書が提唱するのは、決定の権限を分散しつつ、責任の所在を明確にするアプローチです。誰でもアーキテクチャに関する決定を下せる。しかし、その前に適切な人々から助言を求めなければならない。権限は分散されるが、責任は決定者に残る。このバランスが、スピードと品質のトレードオフを緩和してくれます。実務で特に役立っているのは、ADR（Architecture Decision Records、アーキテクチャ決定記録）の考え方です。なぜその設計判断をしたのかを記録しておく。これは、将来のインシデント対応や技術的負債の評価において価値がある。なぜこのシステムはこうなっているのか。その説明ができる状態を維持することは、チームの意思決定の質を高め、運用の効率化にも直結する。決定を記録しないのは、忘れるためである——だから記録が重要なのです。Facilitating Software Architecture: Empowering Teams to Make Architectural Decisions (English Edition)作者:Harmel-Law, AndrewO'Reilly MediaAmazonArchitecture Modernizationlearning.oreilly.comsyu-m-5151.hatenablog.com設計判断を記録し、チームでアーキテクチャを育てる。しかし、既存のレガシーシステムはどうするのか。新規システムなら理想的なアーキテクチャを追求できるが、現実には10年、20年と動き続けているシステムがある。レガシーシステムのモダナイゼーションに関わった経験がある人なら、技術だけでは解決しない問題があることを知っているはずです。コードを書き直しても、組織構造や開発プロセスが同じままでは、また同じ問題が生まれる。コードだけを変えても、問題は戻ってくる——本書は、この現実を正面から扱っています。全てのシステムが同じ重要度ではない。競争優位の源泉となる部分と、汎用的な部分を区別し、限られたリソースをどこに集中すべきかを判断する。全部は直せない。だから、どこを直すか決める——この優先順位付けの考え方は、経営層との対話でも役立ちます。「なぜこのシステムを優先するのか」を説明できるようになる。システムだけを変えても、組織が変わらなければ意味がない——この全体像を把握することは、ソフトウェアに関わるすべての人にとって重要です。なぜこのシステムがこの設計になっているのか。なぜこのチームがこの範囲を担当しているのか。技術的な判断の背景には、組織の歴史や力学がある。それを理解することで、日々の判断もより適切になるし、関係者との対話もスムーズになります。Architecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazonBuilding Event-Driven Microservices, 2nd Editionlearning.oreilly.comマイクロサービスを設計するとき、私たちはつい「サービス間の通信をどうするか」という問いから始めてしまう。しかし本書を読んで、その問いの立て方自体が間違っていたのかもしれないと気づかされました。Adam Bellemare氏による本書の初版は2020年に出版され、イベント駆動型アーキテクチャの実践的な指針として多くのエンジニアに読まれてきました。この第2版では、その後の技術進化と実践知が大幅に加筆されています。本書が冒頭で引用するマクルーハンの「媒体はメッセージである」という言葉が象徴的です。私たちがどのような通信手段を選ぶかが、システムの設計だけでなく、組織構造やチーム間のコミュニケーションまで規定してしまう。リクエスト・レスポンス型の同期通信を選べば、サービス間の密結合が生まれる。イベントストリームを選べば、疎結合と自律性が生まれる。技術選択は、組織の形を決める選択でもある——コンウェイの法則を逆手に取るような視点が、本書には一貫して流れています。著者が強調するのは、データ通信構造（Data Communication Structure）という概念です。ビジネスコミュニケーション構造（チームの編成）と実装コミュニケーション構造（コードとAPI）は多くの組織で意識されている。しかし、データをどう流通させるかという構造は、往々にして後回しにされる。その結果、他チームのデータが必要になるたびに、場当たり的なAPI連携やデータコピーが生まれ、システムは複雑化していく。データ通信構造の欠如が、モノリスを肥大化させる——この指摘は、私自身の経験とも重なります。イベント駆動型マイクロサービスの本質は、データを「イベント」として永続化し、それを組織全体で共有可能にすることにあります。プロデューサーはイベントを発行する責任だけを負い、コンシューマーは必要なイベントを自分のペースで消費して独自のデータモデルを構築する。この分離によって、サービス間の依存関係が劇的に減少する。データは、実装に閉じ込めるものではなく、流れるものである——この発想の転換が、本書の核心です。ただし、私はこの主張を手放しで受け入れているわけではない。イベント駆動型アーキテクチャには、リクエスト・レスポンス型にはない複雑さがある。イベントの順序保証、べき等性の担保、結果整合性への対応。「疎結合になる」という美しい言葉の裏には、新たな運用課題が潜んでいる。本書はその課題にも誠実に向き合っているが、現場で直面する泥臭い問題——たとえば、イベントスキーマの進化をどう管理するか、障害時のリカバリをどう設計するか——については、もっと深掘りしてほしかった部分もある。本書の価値は、イベント駆動型アーキテクチャの「なぜ」を丁寧に解説している点にあります。単にKafkaの使い方を説明するのではなく、なぜイベントストリームが必要なのか、なぜ従来のアプローチでは限界があるのかを、組織論まで含めて論じている。リクエスト・レスポンス型マイクロサービスの欠点——ポイントツーポイント結合、依存スケーリング、分散モノリス化——を明確に言語化してくれたことで、私自身が過去に経験した失敗の原因が腑に落ちました。イベントは、サービス間の会話ではなく、組織の記憶である——本書を読んで、私はイベントストリームの捉え方が変わりました。データパイプラインやメッセージキューとしてではなく、ビジネスの出来事を永続化した「正典的な記録」として捉える。その視点があれば、新しいサービスを立ち上げるときも、過去のイベントを再生してデータモデルを構築できる。実装の寿命よりもデータの寿命のほうが長い——この現実を直視したアーキテクチャが、イベント駆動型マイクロサービスなのだと理解しました。Building Event-Driven Microservices: Leveraging Organizational Data at Scale (English Edition)作者:Bellemare, AdamO'Reilly MediaAmazonTaming Your Dragon: Addressing Your Technical Debtlearning.oreilly.comsyu-m-5151.hatenablog.comシステム開発で必ず直面するのが、技術的負債です。どこを優先的に直すかを判断するには、技術的負債の性質を理解する必要がある。技術的負債は「ドラゴン」のようなものです。放っておけば大きくなり、いつか手に負えなくなる。しかし、完全に倒すこともできない。なぜなら、技術的負債は開発を進める限り必ず生まれるものだからです。だから、敵として戦うのではなく、適切に付き合い、共存の道を探る。ドラゴンは殺せない。だから、飼い慣らす——この比喩が、私には刺さりました。本書を読んで、技術的負債を単なる技術的問題ではなく、トレードオフの問題、組織の問題、経済の問題として捉える視点を得ました。「技術的負債」という言葉は、金融の「負債」から借りてきた比喩です。しかし、両者には決定的な違いがあります。金融的負債は明確な金額があり、返済計画を立てられる。しかし技術的負債は、その量を正確に測定することが困難であり、返済のコストも不確実です。借金は金額がわかる。技術的負債は、わからない——このアナロジーの限界を、私たちはもっと意識すべきだと感じています。ここで著者の主張に、私は半分同意し、半分疑問を持つ。「ドラゴンを飼い慣らす」という比喩は美しい。しかし、飼い慣らせるドラゴンと、飼い慣らせないドラゴンがいるのではないか。ある種の技術的負債は、時間が経つほど返済コストが指数関数的に増大する。そういう負債は、早めに倒すべきだ。すべての負債を「共存する相手」として扱うのは、危険な楽観主義に陥る可能性がある。本書の比喩を鵜呑みにせず、「このドラゴンは飼い慣らせるのか、それとも早めに倒すべきなのか」を見極める目が必要だと、私は考える。技術的負債がなぜ蓄積していくのか、なぜ返済が後回しにされるのか。本書はその構造的な原因を可視化してくれます。原因がわかれば、より効果的な介入点を見つけることができる。技術的負債は倒すものではなく、飼い慣らすもの——「なぜこの改善が必要なのか」を経営層に説明するための理論的基盤を、本書から得ました。Taming Your Dragon: Addressing Your Technical Debt (English Edition)作者:Brown, Dr. Andrew RichardApressAmazonRefactoring to Rustlearning.oreilly.comsyu-m-5151.hatenablog.com技術的負債に対処する具体的な手法の1つとして、言語の移行があります。既存のコードベースを一から書き直すのではなく、段階的にRustに置き換えていくアプローチに興味があって手に取りました。全面的な書き直しはリスクが高い。だから、パフォーマンスクリティカルな部分から少しずつ置き換える。全部を書き直すな、一部を置き換えろ——この原則は、私の考え方にも合っています。「Rustを学ぶ」本ではなく、「Rustを実務で使う」本だと感じました。言語を学ぶのと、言語で仕事をするのは違う——その差を埋めてくれる本です。パフォーマンスクリティカルな部分や、メモリ安全性が重要な部分をRustに置き換えることで、システム全体の信頼性を向上させる。全面的な書き換えのリスクを避けながら、段階的に改善を進める方法論は、運用中のシステムを改善する際の参考になるでしょう。Refactoring to Rust (English Edition)作者:Mara, Lily,Holmes, JoelManningAmazonJust Use Postgres!learning.oreilly.comsyu-m-5151.hatenablog.com言語の選択、アーキテクチャの設計、技術的負債の返済——これまで見てきた本は、どれも「何を選ぶか」の判断を扱っていました。しかし、時には「選ばない」という選択が最良のこともある。「PostgreSQLだけで十分」という主張は、時に過激に聞こえるだろう。しかし本書を読んで、その主張にはしっかりとした根拠があることがわかりました。新しい技術スタックを追加することは、運用の複雑性を高める。だから、既存の技術でできることは、既存の技術で解決すべきです。新しいデータベースを導入する前に、Postgresでできないか考える。この姿勢が、私の技術選択の基準になっています。PostgreSQLは、リレーショナルデータベースとしての堅実な機能に加え、JSON処理、全文検索、地理空間データ、時系列データ、ベクトル検索まで対応しています。Postgresは、データベースではなく、プラットフォームである——この主張には説得力があります。ただし、この主張を額面通りに受け取るのは危険だとも思う。「Postgresで十分」という言葉が、技術的判断の放棄に使われることがある。本当にPostgresで十分なのか、それとも単に新しい技術を学ぶのが面倒なのか。その区別は、案外難しい。本書の価値は「Postgresを使え」という結論にあるのではなく、「なぜPostgresで十分なのか」を考えるフレームワークにある。シンプルさには価値がある。しかし、シンプルさを言い訳にして、必要な複雑さから逃げてはいけない。データベースの種類を減らすことで、運用の複雑性が下がるというメリットがあります。監視対象が減り、バックアップ戦略が統一され、チームが習得すべき技術スタックがシンプルになる。もちろん、PostgreSQLが適さないケースもあります。万能ではないことを認めた上で、どこまで対応できるかを知る。複雑さを減らすことも、エンジニアリングである——その境界線を理解することが、適切な技術選択には重要です。Just Use Postgres!: All the database you need (English Edition)作者:Magda, DenisManningAmazonThe Software Engineer's Guidebooklearning.oreilly.comここまで、技術的なトピックの本を紹介してきました。しかし、技術を身につけるだけでは、キャリアは作れない。ジュニアからシニア、そしてスタッフエンジニアへ。キャリアの各段階で求められるスキルは異なります。しかし、次の段階で何が必要になるかは、今の段階からは見えにくい。キャリアの次の段階で必要なスキルは、今の段階では見えない——本書は、その見通しを与えてくれます。技術的なスキルだけではキャリアは作れない。これは、ある程度経験を積むと実感することです。コードレビューの仕方、技術的な意思決定への関わり方、メンタリングの方法、組織への影響力の広げ方。コードを書く力と、キャリアを作る力は別物——両方を意識的に伸ばす必要があります。技術力は武器になる。しかし、武器だけでは戦場を選べない。ここで私は、本書の主張に対してある種の居心地の悪さを感じる。キャリアを「設計」するという発想自体に、違和感がある。私のキャリアは、計画通りに進んだことがない。偶然の出会い、予期せぬ異動、想定外のプロジェクト。そうした「偶然」の積み重ねが、今の自分を作っている。本書が示すロードマップは参考になる。しかし、ロードマップ通りに進むことが正解だとは思わない。計画を持つことと、計画に縛られることは違う。本書を読みながら、私は自分のキャリアを「設計」するのではなく、「振り返る」ことの方が多かった。ソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社Amazonバックエンドエンジニアのためのインフラ・クラウド大全www.shoeisha.co.jpキャリアを考えるとき、自分に影響を与えてくれた人の存在は大きい。尊敬するnetmarkjpさんの著書です。私がエンジニアとして仕事をする中で、netmarkjpさんから学んだことは数え切れません。その方が書いた本となれば、読まないわけにはいかなかった。本書は「基礎知識」と銘打たれた23章から構成されています。可用性、キャパシティ、パフォーマンス、監視、セキュリティ、DevOps、SRE——インフラに関わるエンジニアが押さえるべき領域を網羅的にカバーしている。しかし、この本の価値は網羅性だけではありません。各章に、実務経験に裏打ちされた「なぜそうするのか」が詰まっている。基礎とは、簡単という意味ではない。基礎とは、すべての基盤になるという意味だ。バックエンドエンジニアがインフラを理解することの意味は、年々大きくなっていると感じます。クラウドネイティブな環境では、アプリケーションとインフラの境界が曖昧になっている。コンテナ、Kubernetes、オブザーバビリティ——これらを理解せずに、本番環境で動くシステムは作れない。アプリだけ書けても、本番では動かせない。本書は、その橋渡しをしてくれる一冊です。 speakerdeck.comバックエンドエンジニアのためのインフラ・クラウド大全【リフロー型】作者:馬場 俊彰,株式会社X-Tech5翔泳社Amazon作る、試す、正す。アジャイルなモノづくりのための全体戦略作る、試す、正す。　アジャイルなモノづくりのための全体戦略bnn.co.jp技術の基礎を固め、システムを作る。しかし、作ったものが「正しいもの」かどうかは、また別の問題です。市谷聡啓さんの到達点とも言える一冊です。『カイゼン・ジャーニー』『正しいものを正しくつくる』を経て、20年以上の実践知が凝縮されています。note.com本書のタイトル「作る、試す、正す」は、ものづくりの本質を端的に表しています。作って終わりではない。試して、学んで、正す。その繰り返しの中で、少しずつ「正しさ」に近づいていく。完成形を目指すのではなく、動き続けることがゴールだという考え方です。私がこの本で最も考えさせられたのは、「正しさ」の捉え方でした。最初から正しいものを作ろうとすると、動けなくなる。かといって、何も考えずに作り始めると、迷子になる。本書が提示するのは、その中間にある姿勢です。「正しさ」は最初から存在するものではなく、作り、試し、正す過程で立ち現れてくるもの。だから、完璧な計画を立てることより、素早く試して学ぶ仕組みを整えることのほうが大事だと言う。この考え方は、ソフトウェア開発に限った話ではないと思います。仕事全般、もっと言えば生き方にも通じる。最初から「正解」を知っている人はいない。やってみて、失敗して、修正して——その繰り返しの中で、少しずつ「あるべき姿」が見えてくる。正しさを探すのではなく、正しくなる状況をつくる。本書のこの言葉は、私の仕事だけでなく、物事への向き合い方そのものを言語化してくれました。作る、試す、正す。　アジャイルなモノづくりのための全体戦略作者:市谷 聡啓ビー・エヌ・エヌAmazon良いコードの道しるべbook.mynavi.jp素早く適応しながら開発を進める。しかし、その過程で生まれるコードの品質はどう担保するか。この本を読んで、私は「説明の仕方」を学びました。動くコードを書くことは、実はさほど難しくない。大事なのは、書いたコードを他の人や将来の自分が読んで正しく理解できること——本書を通して伝えられる。本書の内容自体は、経験を積んだエンジニアにとって目新しいものではありません。命名、コメント、関数やクラスの分割、依存関係の整理、自動化テスト。どれも「基本」と呼ばれるものばかりです。しかし、この本の価値は内容の新しさではなく、説明の丁寧さにあります。なぜその原則が有用なのか、どうしてそう書くべきなのか——「なぜ」を省略せずに解説している。私がこの本を評価するのは、「人に説明するときの参考になる」からです。チームに若手が入ってきたとき、コードレビューで指摘するとき、「なぜこう書くべきか」を説明する必要がある。そのとき、自分の頭の中にある暗黙知を言語化するのは意外と難しい。本書は、その言語化の手本を見せてくれます。基本を、基本のまま、分かりやすく伝える。それは簡単なことではない。良いコードの道しるべ　変化に強いソフトウェアを作る原則と実践作者:森 篤史マイナビ出版AmazonClean Code, 2nd Editionlearning.oreilly.com良いコードの基本を学んだら、次はその原則を深く考えたい。Robert C. Martin（Uncle Bob）による『Clean Code』の第2版です。2008年に出版された初版から16年、全面的に書き直されました。初版を読んだのは何年も前のことです。その後、私のコードは変わったのか。正直に言えば、変わった部分もあれば、変わらなかった部分もある。だからこそ、第2版を手に取りました。自分がどこまで成長したのか、どこで止まっているのか、確認したかった。第2版で印象的だったのは、AI時代に対する著者の姿勢です。「コードはいずれなくなる」「AIがすべて書いてくれる」——そんな予測に対して、Uncle Bobは明確に反論しています。コードは要求の詳細を表現したものであり、その詳細は抽象化できない。AIがどれだけ賢くなっても、仕様を厳密に記述する行為——つまりプログラミング——はなくならない。コードは消えない。なぜなら、コードとは要求そのものだから。この主張に私は強く共感する。そして驚いたのは、第2版がここまで大幅にアップデートされていたことだ。16年という歳月は、ソフトウェア開発の世界では永遠に等しい。にもかかわらず、Uncle Bobは単なる改訂ではなく、現代の開発環境——AI、クラウド、分散システム——を踏まえた上で原則を再構築している。初版の「良いコードとは何か」という問いは変わらないが、その答え方が2025年の文脈に合わせて書き直されている。古典を現代に蘇らせるとは、こういうことなのだと思った。本書の核心は、タイトルの通り「クリーン」であることです。しかし、「クリーン」とは完璧を意味しない。住めない「ショーハウス」ではなく、住める「クリーンな家」を目指す。クリーンなコードとは、維持し、拡張し、進化させても、その住みやすさを損なわないコードのこと。完璧ではないが、手入れされている。クリーンとは、完璧ではなく、ケアされている状態だ。もう1つ、心に残った言葉があります。「私たちは書くよりも読む時間の方が圧倒的に長い」——だからこそ、読みやすいコードを書くことが、結果として書きやすさにつながる。速く行きたければ、うまくやれ（The only way to go fast is to go well）。この原則は、初版から変わらない。そして、16年経っても色褪せない。型システムのしくみ型システムのしくみ ― TypeScriptで実装しながら学ぶ型とプログラミング言語www.lambdanote.comクリーンなコードを書くための原則を学んだ。では、その原則を支える道具——型システム——はどう動いているのか。遠藤侑介さんの著書です。Rubyコミッタであり、TypeProfの開発者であり、『型システム入門』の訳者でもある。その方が「型システムを実装しながら学ぶ」本を書いた。読まないわけにはいかなかった。現代の開発環境では、コードを書いている最中にエラーが判明し、文脈に適した補完候補が提示される。当たり前のように使っているこの機能、その裏側で何が起きているのか。本書は、TypeScriptのサブ言語に対する型検査器を実装しながら、その「しくみ」を解き明かしていきます。型システムの理論を学ぶ方法は、数学的な教科書を読むことだけではない。実装を通じて理解する道がある——本書はその道を示してくれます。真偽値と数値の型から始まり、関数型、オブジェクト型、再帰型、ジェネリクスへと段階的に進んでいく構成が秀逸です。各章で型検査器を拡張しながら、「なぜこの機能が必要なのか」「どう実装するのか」を体験的に学べる。私がこの本を読んで得たのは、型システムへの「畏れ」と「親しみ」の両方でした。型システムは魔法ではない。人間が設計し、実装したものだ。しかし、その設計には深い思慮がある。エディタが「このコードは間違っている」と教えてくれるとき、その背後には型検査器の地道な仕事がある。その仕事の中身を知ることで、型に対する見方が変わりました。型は、プログラムを制約するものではなく、プログラムを守るものだ。Fundamentals of Software Engineeringlearning.oreilly.com型システム、クリーンコード、アーキテクチャ——ここまで個別の技術トピックを深掘りしてきました。しかし、それらを俯瞰的に捉える視点も必要です。ソフトウェアエンジニアリングの基礎を幅広くカバーしている一冊です。流行のフレームワークは数年で入れ替わる。しかし、基礎的な原則は変わらない。フレームワークは変わる。基礎は変わらない——長くこの業界にいると、この事実を繰り返し実感します。AIがコードを生成してくれる時代になって、基礎の重要性はむしろ高まっていると感じます。AIの出力をそのまま受け入れるのではなく、評価し、改善し、統合する。その判断ができるのは、基礎を理解している人間だけです。AIの出力を評価できるのは、基礎を知っている人だけ——特定の技術やフレームワークに依存しない普遍的な原則を、改めて確認するために本書を読みました。Fundamentals of Software Engineering: From Coder to Engineer (English Edition)作者:Schutta, Nathaniel,Vega, DanO'Reilly MediaAmazonThe Product-Minded Engineerlearning.oreilly.com基礎を学び、技術を深め、システムを作る。しかし、技術的に正しいものを作ることと、ユーザーに価値を届けることは、必ずしも同じではありません。エンジニアとして長く仕事をしていると、技術的に正しいことと、ビジネスとして正しいことが一致しない場面に何度も遭遇します。コードが動くだけでは十分ではない。そのコードが、ユーザーにどんな価値を届けているのか。コードを書くことと、価値を届けることは違う——この違いを理解することは、プロダクトに関わるエンジニアにとって必須のスキルです。エンジニアとして仕事をしていると、「ユーザーにとっての価値」と「技術的な正しさ」の間にギャップがあることに気づきます。たとえば、99.9%の可用性は技術者にとっては誇らしい成果でしょう。しかし、99.9%を裏返すと0.1%のダウンタイム。年間に換算すると約8時間の停止を意味する。ユーザーにとって、その8時間がどれだけ痛いか。99.9%は、ユーザーにとっては年間8時間の停止を意味する——技術的な数値をビジネスインパクトに翻訳できること。それがプロダクト思考の1つの形であり、本書はその視点を養う上で役立ちました。The Product-Minded Engineer: Building Impactful Software for Your Users (English Edition)作者:Hoskins, DrewO'Reilly MediaAmazonThe Engineering Leaderlearning.oreilly.comプロダクト思考を身につけ、技術とビジネスの両方を見られるようになる。すると、次に見えてくるのはリーダーシップの課題です。リーダーシップについて書かれた本は多いですが、本書は地に足のついた実践的なアドバイスが詰まっています。誰かがキャリアを設計してくれるわけではない。自分で考え、自分で動く必要がある。自分のキャリアの責任者は、自分である——ある程度経験を積むと、この現実を受け入れざるを得なくなります。本書は、その受け入れた後に何をすべきかを具体的に示してくれます。自分自身を導くこと、他者を導くこと、チームを導くこと、そしてチームを超えて導くこと。まず自分を導けないなら、他者は導けない——この順序は重要です。自己管理ができていない人間が、チームをまとめられるはずがない。「マネージャーになる」ことだけがリーダーシップではない。ポジションに関係なく、チームに良い影響を与えることはできる。リーダーシップは、ポジションではなく行動である——この考え方は、IC（Individual Contributor）としてのキャリアを続ける上でも指針になっています。The Engineering Leader: Strategies for Scaling Teams and Yourself (English Edition)作者:Huston, CateO'Reilly MediaAmazonエンジニアリングリーダー ―技術組織を育てるリーダーシップとセルフマネジメント作者:Cate Huston,岩瀬 義昌（翻訳）,岩瀬 迪子（翻訳）オーム社Amazon"Looks Good to Me"learning.oreilly.comリーダーシップを発揮する場面は、会議室だけではありません。日々の開発で最も頻繁に行われるコミュニケーションの1つが、コードレビューです。コードレビューは、品質保証の手段であると同時に、チームの学習機会でもある。バグを見つけるだけがレビューの役割ではない。知識を共有し、コードの意図を確認し、チーム全体の理解を揃える。レビューは、コードのためではなく、チームのためにある——この視点で見ると、レビューの仕方が変わってきます。コードレビューを「チームスポーツ」として捉える考え方に共感しました。個人の技術力を競う場ではなく、チーム全体の品質とスキルを向上させるための協働の場として位置づける。レビューコメントは、批判ではなく、贈り物である——この姿勢を持てるかどうかで、チームの雰囲気は大きく変わります。しかし、私はこの「贈り物」という表現に、少しだけ引っかかる。贈り物は、受け取る側が喜ぶものだ。しかしコードレビューのコメントは、時に厳しいことも言わなければならない。「ここは根本的に設計を見直すべきだ」と指摘することは、贈り物というより、苦い薬に近い。「贈り物」という美しい比喩に逃げて、言うべきことを言わなくなるのは本末転倒だ。本書の主張は正しいが、その比喩を鵜呑みにすると、レビューが馴れ合いになる危険がある。厳しさと敬意は両立できる。そのバランスこそが、本当の意味での「贈り物」なのだと思う。最後に「LGTM」と承認するのは人間です。その承認は、コードへの同意であると同時に、チームメンバーへの信頼の表明でもある。LGTMは、チームの信頼の証である——この認識を共有できているチームは、レビューが建設的になるし、心理的安全性も高まります。"Looks Good to Me": Constructive code reviews (English Edition)作者:Braganza, AdrienneManningAmazonLooks Good To Me作者:Adrienne Braganza秀和システムAmazonおわりに26冊。感想文を書き終えて、その数字を見つめている。多いのか少ないのか、正直わからない。まぁ多いか。「今年もたくさん読みましたね」と言われれば悪い気はしないし、「それだけ？」と言われればちょっとへこむ。結局、他人の評価を気にしている。読書量なんて自己満足だと言いながら、どこかで認めてほしがっている。振り返ると、今年の本には共通点があった。『Beyond Vibe Coding』は、AIに頼りすぎている自分を突きつけてきた。『LLMOps』は、正解が定義できないシステムの難しさを教えてくれた。『ソフトウェア設計の結合バランス』は、疎結合という呪縛から解放してくれた。『Taming Your Dragon』は、技術的負債と共存する道を示してくれた。どの本も、私に「それでいいのか」と問いかけてきた。『Terraform in Depth』を読んだ夜のことを思い出す。ステート管理のベストプラクティスなんて、AIに聞けば30秒で返ってくる。でも私は、著者が過去にやらかした失敗談のほうを覚えている。「これで痛い目を見た」という告白。公式ドキュメントには絶対に載らない、その生々しさ。なぜか、そっちのほうが頭に残る。正解より失敗のほうが記憶に焼きつくのは、私という人間の性質なのかもしれない。『Beyond Vibe Coding』を読んだとき、嫌な気持ちになった。自分のことを書かれている気がしたからだ。AIに聞いて、答えをもらって、なんとなくわかった気になる。その繰り返し。「なぜ」を考えなくなっていた。本を読むという行為は、その怠惰な自分への処方箋だったのかもしれない。ページをめくる時間だけ、「なぜ」を考え続けることができる。本は答えをくれない。くれるのは「そうだろうか」という違和感だ。著者の主張に首をかしげる。その違和感を言語化しようとする。そうやって、自分の考えが少しずつ形になっていく。AIは答えを返してくれる。でも「そうだろうか」とは返してくれない。たぶん、そこが決定的に違う。今年は、AI/LLMの運用が本格化した年だった。プラットフォームエンジニアリングが変わり、組織の話が増えた。技術だけ見ていればよかった時代は、とっくに終わっている。その変化に追いつこうとして、本を読んだ。読んで、ブログを書いて、登壇した。アウトプットしないと身につかない。言い聞かせるように、繰り返してきた。でも、本当のことを言えば、追いつこうとしていたわけではないのかもしれない。変化の中で、自分が何者であるかを確かめたかった。AIがコードを書いてくれる時代に、なぜ私はエンジニアをやっているのか。答えは出ていない。出ていないけれど、本を読むたびに、その輪郭が少しだけ見えてくる気がする。2025年はまだ3週間ほど残っている。年末年始に読んだ本は、来年の記事で。毎年同じことを書いている気がする。でも来年も、たぶんまた書くのだろう。誰に頼まれたわけでもないのに、12月になると、この作業を始めてしまう。本を読むことに意味があるのか。正直、わからない。わからないけれど、やめられない。AIがどれだけ賢くなっても、300ページを読み通した時間は消えない。その時間が、自分を少しだけ変えてくれたような気がする。気がするだけかもしれない。でも、その「気がする」を信じて、来年も本を開くのだと思う。正解を得ることだけが目的なら、エンジニアをやっている意味がない。はじめにで書いたこの言葉が、25冊の感想文を書き終えた今、少しだけ違って聞こえる。正解がないから難しい。正解がないから面白い。正解がないから、エンジニアを続ける価値がある。本を読む意味がある。来年もきっと、答えの出ない本を読み続けるのだろう。そして、また12月になったら、この記事を書く。それでいい。それがいい。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[とあるMLエンジニアの年末年始の予定の呟き]]></title>
            <link>https://zenn.dev/akasan/articles/2025_new_years_eve</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/2025_new_years_eve</guid>
            <pubDate>Wed, 10 Dec 2025 13:25:20 GMT</pubDate>
            <content:encoded><![CDATA[今回はMLエンジニアとしてひたすら精進を頑張っている私が今年の年末年始どのように過ごす予定か、誰得ではありますしこんなことをzennに書いている人がいるかわからないですが、まとめてみます。 まずはこのアドベントカレンダーについて2025/04/18に爆誕してすでに230日を超えていますが、まずは2025/12/25までは毎日投稿を続ける予定です！その後についてですが、年末年始は流石にちょっとお休みしようかなと思っており、2025/12/26から2026/1/4はお休みしようと思っています。もちろん、途中で急に書きたいことがあれば発信しますが、謎の義務感・使命感によって続けられている...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年版 私がAIエージェントと協働しながら集中する方法]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/10/092706</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/10/092706</guid>
            <pubDate>Wed, 10 Dec 2025 00:27:06 GMT</pubDate>
            <content:encoded><![CDATA[集中できなくなった何かがおかしい。AIエージェントを使い始めてから、自分が壊れていくのを感じていた。以前は4〜5時間ぶっ通しで集中できた。コードを書き始めたら、気づいたら夕方になっていた。あの没入感。あの充実感。それが、完全に消えた。30分も持たない。いや、10分だろうか。1つの作業に没頭しようとしても、すぐに別の作業に引き戻される。戻ってきたら、さっき何をしていたか忘れている。頭の中が常にざわついている。自分の脳が、自分のものではなくなっていく感覚があった。最初は自分を責めた。集中力が落ちたのは、体力のせいか。年齢のせいか。怠けているのか。スマホの見すぎか。でも違った。同じように苦しんでいる人が、周りにもいた。きっと、最初からうまく馴染める人もいるのだろう。複数のエージェントを同時に回しながら、涼しい顔で成果を出せる人。元々、全体を俯瞰しながら動くのが得意な司令官タイプ。私は違った。複数のエージェントが並行して動いている。1つのエージェントに指示を出して、出力を待っている間に別のエージェントの出力を確認する。確認が終わったら修正指示を出して、また別の作業に移る。案件も複数が同時に走っている。厄介だったのは、見せかけ上の効率は上がっていたことだ。タスクは消化されている。アウトプットも出ている。だから最初は原因に気づけなかった。でも、何かがおかしい。同じ時間、同じ環境で働いているのに、以前のように深く没入できない。達成感がない。自分は変わっていないはずなのに、なぜ？数字に現れない損失があった。タスクの消化数は増えた。しかし、1つ1つの仕事に対する理解の深さが落ちていた。コードをAIと共に書いているのに、なぜそう書いたのか説明できない。レビューを通しているのに、本当に良いコードなのか判断できていない。量は出ている。でも、自分の中に何も残らない。学習効率が落ちていた。成長している実感がなかった。品質の問題もあった。アウトプットは出ている。しかし、それは本当に良いアウトプットなのか。深く考える時間がないまま、次々とタスクを流していく。表面的には回っている。後から振り返ると「なぜこんな設計にしたんだ」と感じることが増えていた。速く走っているつもりが、同じ場所をぐるぐる回っていただけだった。そして何より、このペースや仕事のやり方が続くのかという不安があった。毎日、頭の中が騒がしい。仕事が終わっても、脳が休まらない。週末になっても回復しきれない。短期的には回っている。でも、1年後、3年後も同じように働けるのか。効率が上がったように見えて、実は遠回りしている道を走っていたり自分を前借りしているだけではないのか。ポモドーロ・テクニックを再稼働させた。25分の作業と5分の休憩を繰り返す方法だ。効果はあった。でもAIエージェントとの協働が始まってからは、25分の中での集中すら維持できなくなっていたというた25分のタスクというのを見積もれなくなった。通知を切った。効果なし。まとまった時間を確保した。効果なし。瞑想アプリを入れた。効果なし。何をやっても、うまくいかなかった。追い詰められていた。このままでは仕事にならない。でも、AIエージェントなしで働くという選択肢はもうなかった。環境を変えるのではなく、自分を変えるしかなかった。観察するという発見あるとき、ふと気づいた。作業中に自分の状態を観察していると、不思議と集中が続く。試しに意識的にやってみた。作業をしながら、自分の認知や感覚の微細な変化を、揺らぐ火を見るように観察し続ける。退屈になりかけている自分、焦り始めている自分、注意が逸れそうになっている自分。それをただ見ている。すると、集中が途切れにくくなった。途切れても、すぐに戻れるようになった。集中は「維持するもの」ではなく「戻るもの」だと気づいた。今では複数の案件を並行して回しながら、開発タスクを5本同時に進め、ブログも2〜3本並列で書けるようになった。ポモドーロの25分間、集中が途切れることはほとんどない。私はこの方法を「微観法」と呼ぶことにした。正しい名前がある時は教えてほしいです。「微」は微細なもの、「観」は観察すること。以前の集中と何が違うのか以前の私にとって、最良の集中状態とは湖の底に沈んでいくような感覚だった。体の感覚はどこか希薄になる。なぜ自分がキーボードを打っているのかわからなくなる。意識と作業の境界が溶けて、ただコードが生まれ、ただ文章が流れていく。水面の光が遠ざかり、静かな深みに降りていく。その状態に入れたとき、驚くほどの量と質の仕事ができた。あの深さを、私は愛していた。この「深く沈む」集中は、1つの大きなタスクに長時間取り組むときには最適だった。中断がなく、自分のペースで進められるソフトウェア開発や執筆の環境では、これ以上の方法はなかった。しかしAIエージェントと協働する環境では、この方法が通用しなくなった。深く沈もうとしても、エージェントの出力確認で水面に引き戻される。複数の案件を抱えていれば、1つに没入できない。深く沈むには、水面が静かでなければならない。でも今の水面は常に波立っている。そこで発想を変えることにした。深く沈むのではなく、水面近くに留まる。没入するのではなく、観察する。集中の「深さ」ではなく、「復帰の速さ」を重視する。ここで1つ、重要なことに気づいた。集中は、環境次第で形を変える。静かな水面なら深く沈む集中が最適だし、波立つ水面なら水面近くを泳ぐ集中が最適だ。どちらが優れているわけではない。環境に合った集中の持ち方がある。つまり、集中とは「自分の能力」ではなく「環境との関係」なのだ。同じ人間でも、環境が変われば最適な集中の形は変わる。集中できないのは能力の問題ではない。環境と方法のミスマッチだ。私は長い間、自分の集中力が落ちたと思っていた。でも違った。環境が変わったのに、方法を変えていなかっただけだった。なぜ観察すると集中できるのかここで疑問が生じる。作業に100%集中したほうが効率的なはずではないか。なぜ10〜20%を「自分の観察」に割くと、かえって集中できるのか。理由はおそらく「注意の逸脱」の仕組みにある。人間の注意は、放っておくと必ず逸れる。これは避けられない。問題は、逸れること自体ではなく、逸れたことに気づくまでの時間だ。普通は、気が逸れてから5分、10分経って「あ、逸れてた」と気づく。スマホを開いて、気づいたら15分経っていた。そういう経験は誰にでもある。この5分、10分、15分が積み重なって、1日の生産性を静かに破壊していく。微観法では、意識の一部を「自分を観察する視点」として常に確保しておく。すると、注意が逸れ始める瞬間を捉えられるようになる。「スマホを見ようかな」と思った瞬間。「積んである本を読みたいな」と思った瞬間。「コーヒーを淹れに行こうかな」と思った瞬間。「このタスク面倒だな」と感じた瞬間。逸れてから3秒で気づき、すぐ戻れる。5分後に気づくのと、3秒後に気づくのでは、累積の損失がまったく違う。100%集中しようとして5分ごとに逸れるより、90%の集中を安定して維持するほうが、結果的に多くの仕事ができる。もう1つ理由があると思っている。「退屈の無効化」だ。脳は刺激が足りないと退屈を感じ、新しい刺激を求める。SNSを見たくなるのはこのためだ。作業が単調になると、脳が「もっと刺激をくれ」と要求してくる。しかし自分の内面を観察対象にすると、そこには常に微細な変化がある。呼吸の深さ、肩の緊張、思考の流れ、感情の揺らぎ。これは揺らぐ炎のように、予測不能だが安定していて、見続けることができる。外部刺激に頼らなくても、脳が求める新規性は内側から供給できる。具体的なやり方方法は単純だ。ある日、疲れ果てて帰ってきた夜のことだった。だるい。本当にだるい。でも仕事が残っている。そのだるさを抱えたまま、仕方なくキーボードに向かった。そのとき、ふと気づいた。「だるいな」と感じている自分を、どこかで観察している。だるさはある。でも、だるさを見ている自分もいる。その「見ている自分」は、意外と冷静だった。不思議なことに、観察を続けていると作業が進んだ。だるさは消えない。でも、だるさに飲み込まれない。その感覚を忘れたくなくて、言語化しておくことにした。それが微観法の始まりだった。ポイントは、観察の「解像度」を下げることだ。「今、自分は何を考えているか」「なぜそう感じているか」と分析しようとすると、認知資源を食う。作業と同時にはできない。分析せず、ただ「ある」と気づくだけでいい。「退屈だな」と感じたら、なぜ退屈かは考えない。「退屈がある」とだけ認識する。それだけで十分だ。例えば、作業を始める前に5秒だけ自分の状態を確認する。呼吸は浅いか、深いか。肩に力が入っているか。頭の中は静かか、騒がしいか。答えを出す必要はない。ただ気づくだけでいい。これで観察モードが起動する。作業に入ったら、意識の10〜20%を「自分を観察する視点」に割り当てる。残りの80〜90%で作業しながら、バックグラウンドで自分の変化を捉え続ける。「今、少し退屈になってきた」「焦りが出てきた」「集中が浅くなっている」。この観察は論理的に行う必要はない。分析しなくていい。揺らぐ炎を眺めるように、ただ見ていればいい。観察を続けていると、注意が逸れ始める瞬間を捉えられるようになる。「スマホを見ようかな」という考えが浮かんだ瞬間に気づく。気づいたら、その考えを追いかけずに作業へ戻る。「このタスク面倒だな」と感じたら、その感覚を認めて、それでも続ける。別のことを考え始めたら、気づいた時点で戻る。それだけだ。シンプルだが、これが全てだ。作業の構造微観法と組み合わせて効果が上がった作業の構造がある。まず、案件は混ぜない。案件Aで開発をしていて、案件Bのメールに返信して、また案件Aに戻る。以前はこれを普通にやっていた。普通に効率の悪いマルチタスク。でもこれはAIエージェントと働いていても同じだった。案件を切り替えるとき、脳は多くのことを読み込み直している。関係者は誰か。この人にはどう接するべきか。過去にどんな経緯があったか。暗黙の制約は何か。自分はこの案件でどういう立ち位置か。これは単なる情報ではなく、人間関係のシミュレーションだ。技術的は話だけではない。だから重い。案件の「重さ」には差がある。関係者が多い案件は重い。長期で複雑な経緯がある案件は重い。緊張感のある関係を含む案件はより重い。これらを頻繁に切り替えると、作業そのものより切り替えで消耗する。だから案件単位で時間を区切っている。この2時間は案件A、次の2時間は案件B。案件の中で完結させる。次に、同一案件内ではモードを切り替える。開発モードではコーディングや設計、AIエージェントへの指示出しをする。執筆モードではドキュメントや企画書、翻訳に取り組む。準備モードでは開発や執筆を円滑に進めるための下調べ、環境構築、資料整理、Slackの確認などをする。Slackの通知は基本的に無視している。見るのは準備モードのときだけだ。開発中や執筆中にSlackへ戻っていたら、何も進まない。通知は他人の優先順位だ。自分の優先順位を守れ。ポモドーロの25分をモード単位で使っている。同じ種類の作業は並列で回す同じモード内であれば、複数の作業を並列で回せる。ブログを書くとき、1本だけを最初から最後まで書くのではなく、2〜3本を並列で進める。1本目の導入を書いて、詰まったら2本目に移る。2本目の本論を書いて、また1本目に戻る。開発でも同様で、5本程度のタスクを並列で回している。なぜこれができるのか。「書くモード」や「開発モード」を維持したまま、対象だけを切り替えているからだ。モードを起動するコストは高いが、一度起動してしまえば、対象を変えるコストは低い。しかし並列できる数には限界がある。開発は5本程度いけるが、ブログは2〜3本が限界だ。この差は「状態の外部化」で説明できる。開発はgit worktree（複数のブランチを同時に扱える開発ツール）やコード自体が「どこまでやったか」「何をしようとしていたか」を保持してくれる。見れば思い出せる。脳が状態を覚えておく必要がない。だから多くを並列にできる。ブログは違う。「この記事で何を言いたかったか」「どういう構成にするつもりだったか」が頭の中にしかない。外部化されていないから、並列の限界が低い。二重の飽き防止ここまで来て、自分が二重の飽き防止システムを走らせていることに気づいた。飽きは敵だ。でも飽きは設計で無効化できる。マクロレベルでは、同種作業の並列によって、外から新規性を供給している。ブログ1からブログ2へ、またブログ1へ。1つの記事を長時間書き続けると退屈になる。でも複数を回していれば、戻ってきたときに新鮮な目で見られる。ミクロレベルでは、微観法によって、内から新規性を生成している。自分自身の微細な変化を「見るもの」として扱っている。外部刺激がなくても退屈しない。この二重構造があるから、飽きによる集中力低下を防ぎながら、並列作業中に「自分がどこにいるか」を見失わずにいられる。実際、微観法がなければ並列作業は成立しない。複数の作業を回していると、「あれ、今どこにいたっけ」「何をしようとしてたんだっけ」となりやすい。微観法で自分の認知状態を観察し続けているから、位置感覚を保てる。迷子にならないから、遠くまで行ける。ようやく気づいたことここまで来て、ようやく気づいた。開発という仕事の性質そのものが変わっていたように思える。戦国無双と信長の野望というゲームがある。どちらも戦国時代を舞台にしているが、まったく別のゲームだ。戦国無双は自分が武将となって敵を斬りまくるアクションゲーム。信長の野望は君主となって複数の武将に指示を出し、国全体を動かすシミュレーションゲーム。自分で戦うか、全体を指揮するかの違いだ。AIエージェントとの協働は、仕事を戦国無双から信長の野望に変えた。プレイヤーから司令官へ。自分で剣を振るうのではなく、複数の部下に指示を出して全体を動かす。求められる集中の質が、根本から違う。私は最初、戦国無双の集中法で信長の野望をプレイしようとしていた。一人で深く没入しようとしていた。だからうまくいかなかった。私にとって微観法は、信長の野望のための集中法だったのだ。自分の状態を観察し続けることで、複数の部下（エージェント）の動きを把握し、全体を俯瞰する。深く沈むのではなく、広く見渡す。集中の形が変わったのではない。仕事の形が変わったのだ。深い集中が戻ってきた微観法を続けて数ヶ月、予想していなかった変化があった。諦めたはずのものが、形を変えて戻ってきた。以前の「湖に沈む」ような深い集中が、少しずつ戻ってきている。最初は水面近くを泳ぐだけだった。浅いけれど安定した集中。それはそれで十分に機能していた。でも続けているうちに、観察しながらでも深く入れる瞬間が出てきた。観察が自動化されてきたのだろう。最初は意識的に10〜20%を割り当てていた。それが習慣になり、無意識でも観察が走るようになった。すると、残りの意識をより深く作業へ向けられるようになった。意識して始めたことが、やがて無意識になる。それが習得だ。今は、水面近くで泳ぎながら、ときどき深く潜れる。潜っている間も、どこかで自分を観察している感覚がある。以前の「なぜキーボードを打っているかわからなくなる」状態とは少し違う。意識はあるのに、深い。完全に以前と同じではない。でも深さと柔軟さの両方を持てるようになりつつある。そして気づいた。あの「見せかけの効率」が消えていた。タスクは消化されている。でも今は、なぜそう書いたか説明できる。自分の中に残るものがある。量だけでなく、質も戻ってきた。集中の持ち方を変えたことで、仕事との向き合い方そのものが変わっていた。最近、もう1つ変化が起きている。案件Aの開発をしている待ち時間に、同じ案件の軽い調整作業ができるようになってきた。エージェントが処理している間の数十秒から数分の隙間で、ちょっとした修正や確認を挟める。自分がどこにいるかを常に把握できているから、短い寄り道をしても迷子にならない。進化は、まだ続いている。これからこれが2025年現在、AIエージェントと協働しながら働いている一人のソフトウェアエンジニアの集中法だ。完璧ではない。でも機能している。環境が変われば、集中の持ち方も変わる。以前の「深く沈む」集中法は、中断と再開が前提の環境には合わなくなった。代わりに見つけたのが、微観法だった。自分の微細な変化を観察し続けることで、注意の逸脱を早期に検知し、復帰を速くする。深さではなく、復帰の速さで勝負する。エージェントはこれからも進化する。集中の持ち方も、また変わるだろう。今の方法が最終形ではない。でも、変化に適応する方法は見つけた。微観法は才能ではなく方法だ。次の作業を始める前に、5秒だけ自分の呼吸を確認してみてほしい。5秒でいい。そこから全てが始まる。かつて愛した湖の深みに、今は違う形で戻れるようになった。水面近くを泳ぎながら、好きなときに深く潜れる。そして、いつでも水面に戻れる。参考書籍知性の未来―脳はいかに進化し、AIは何を変えるのか―作者:マックス・ベネット新潮社AmazonPLURALITY　対立を創造に変える、協働テクノロジーと民主主義の未来（サイボウズ式ブックス）作者:オードリー・タン,E・グレン・ワイルライツ社Amazon一点集中術――限られた時間で次々とやりたいことを実現できる作者:デボラ・ザックダイヤモンド社Amazon集中力がすべてを解決する　精神科医が教える「ゾーン」に入る方法作者:樺沢 紫苑SBクリエイティブAmazonイェール大学集中講義 思考の穴――わかっていても間違える全人類のための思考法作者:アン・ウーキョンダイヤモンド社Amazonヤバい集中力　1日ブッ通しでアタマが冴えわたる神ライフハック45作者:鈴木 祐SBクリエイティブAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[rustで非同期ランタイム実装してみた]]></title>
            <link>https://zenn.dev/sraku/articles/2e50371363cbaa</link>
            <guid isPermaLink="false">https://zenn.dev/sraku/articles/2e50371363cbaa</guid>
            <pubDate>Tue, 09 Dec 2025 15:00:05 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事はQiita 3-shake Advent Calendar 2025 シリーズ10日目の記事です。以前Rustのイベントに参加した時に非同期周りの話がでて、少し興味が湧いたので実装してみたというお話になります。リポジトリはこちらですhttps://github.com/sraku2159/async_runtimeはじめにRustにおける非同期処理の特徴を概説します。 Rustの非同期処理の特徴Rustはいわゆる協調的マルチタスクと呼ばれる機構によって非同期処理を実現しています。つまり、シグナルなどによってプリエンプトされるのではなく、async関...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[最近読んでいて興味深かった記事紹介 Vol.3]]></title>
            <link>https://zenn.dev/akasan/articles/interesting_tech_blogs_3</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/interesting_tech_blogs_3</guid>
            <pubDate>Tue, 09 Dec 2025 13:02:46 GMT</pubDate>
            <content:encoded><![CDATA[今回は読んでいて良かった記事を紹介するシリーズの第3弾になります。過去のシリーズは以下にまとめていますのでぜひご覧ください。https://zenn.dev/akasan/scraps/97b063540d2372 Open Source for DevelopersこちらはNVIDIAのエンジニアの方がコントリビュートしているOSSのリストが載っています。世界最高峰レベルのエンジニアがどのようなOSSに関わっておられるのか興味がありみていました。https://developer.nvidia.com/open-source?sortBy=open_source_projec...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[地方リモートエンジニア歴3年、やってよかった7つのこと]]></title>
            <link>https://zenn.dev/yuu0w0yuu/articles/f44cceadef5a53</link>
            <guid isPermaLink="false">https://zenn.dev/yuu0w0yuu/articles/f44cceadef5a53</guid>
            <pubDate>Tue, 09 Dec 2025 07:23:08 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、3-shake Advent Calendar 2025の11日目の記事です。おぼろげながら浮かんできたんです。7という数字が。 朝のラジオ体操私が住んでいる長野県の松本市は、晴天率が全国的に見ても高く、一年を通して晴れていることが多いです。ある朝、「こんな爽やかな朝、何かせねば」と思って始めたのがラジオ体操でした。第一だけなら約3分ほど。ほどよい負荷で爽やかな朝を迎えることができます。旅行先でも必ずやります。漫然とやるのではなく、お手本動画のように綺麗なフォームを意識することが重要です。肩周り・腰回りをブンブン回すので、デスクワークで姿勢が歪みがちなあなた、特...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[実力とは“最悪の自分”が決める]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/09/092256</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/09/092256</guid>
            <pubDate>Tue, 09 Dec 2025 00:22:56 GMT</pubDate>
            <content:encoded><![CDATA[はじめに私たちは「実力」という言葉を履き違えています。特に私がそうでした。様々な人の助力で得た結果、たまたま条件が揃って出せた最高到達点を「自分の実力」だと勘違いしていました。そして、その水準に届かない日々の自分を見て、「なんでもっとできないんだ」と追い込んでいました。結果は散々なものでした。心身ともに疲弊し、パフォーマンスはさらに落ち、悪循環に陥りました。しかし、その経験から大きな学びもありました。ゾーンに入り、神がかった速度でコードを書く自分。難解なバグを一瞬で特定する自分。私たちは、あの奇跡的な瞬間を「自分の実力」だと信じ、そうでない日を「調子が悪かった」と言い訳します。逆です。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その泥のような日に絞り出したアウトプット。それこそが、紛れもない私の「実力」です。 絶好調のときの成果は、再現性のない「運」や「上振れ」に過ぎません。この記事では、なぜそう言えるのか、そしてその認識がなぜ重要なのかを考えていきます。これは鬱屈とした日々を過ごしていたかつての自分に向けて書いています。「最高出力」という幻想まず、私たちが「実力」だと思い込んでいるものの正体を見てみましょう。過去半年を振り返ってみてください。「奇跡的にうまくいった日」は何日あったでしょうか。全てが噛み合い、コードがスラスラ書けて、レビューも一発で通り、障害対応も華麗にこなせた日。おそらく、片手で数えられる程度ではないでしょうか。なぜそんなに少ないのでしょうか。理由は単純です。「最高のパフォーマンス」を出すためには、無数の条件を揃える必要があります。十分な睡眠。適度なストレス。興味のある課題。邪魔の入らない環境。体調の良さ。プライベートの安定。これらすべてが揃う日は、人生において稀なのです。その稀な瞬間にしか出せないものを「実力」と呼ぶのは、ギャンブルで勝った日の収支を「年収」と呼ぶようなものです。 奇跡を前提にした人生設計は、破綻することが約束されています。それは本人にもコントロールできない「可能性の上限」であって、信頼できる「能力」ではありません。私自身の話をしましょう。このブログには、いわゆる「おい、」シリーズと呼ばれる記事があります。「おい、本を読め」「おい、スマホを置け」「おい、対話しろ」。ありがたいことに、これらの記事は多くの人に読まれています。はてなブックマークでもたくさんのコメントをいただきました。Xをフォローしてくれている人は知っていると思うのですが4冊紹介するフォーマットも実力以上にアウトプットを出せたと思います。syu-m-5151.hatenablog.comしかし、正直に言いましょう。あれは私の実力からかなり上振れしています。あの記事を書いたとき、たまたま言葉がスラスラと出てきました。たまたま自分の経験と文章のリズムが噛み合いました。たまたま読者の琴線に触れるタイミングでした。書籍レベルの何かを目指してブログとして色々出した。同じクオリティのものを、明日また書けるかと問われれば、自信を持ってイエスとは言えません。あれが私の「実力」だと思い込んでしまうと、危険です。次に書く記事が同じように読まれなかったとき、「調子が悪かった」「本来の力が出せなかった」と言い訳をしてしまいます。しかし実際には、「おい、」シリーズの方が例外なのです。私の本当の実力は、誰にも読まれない記事を淡々と書き続けられるかどうか、そちらの方にあります。では、なぜ私たちは「最高の自分」を実力だと思い込んでしまうのでしょうか。それは、そう思いたいからです。「あれが本当の自分だ」と信じることで、今の不甲斐ない自分を一時的なものとして処理できます。「今日は調子が悪いだけ」という言い訳は、私たちの自尊心を守ってくれます。しかし、その言い訳に甘えていると、現実を直視する機会を失ってしまいます。信頼は「下限」に支払われる「最高の自分」を実力だと思い込むのは、自分一人の問題なら、まだいいかもしれません。しかし、私たちは一人で働いているわけではありません。では、社会はどちらを評価するのでしょうか。「最高の自分」か、「最悪の自分」か。仕事を誰かに頼むとき、私ならどちらを選ぶでしょうか。「調子が良ければ神がかったコードを書くが、悪ければ全く動かないものを出してくる天才」か、「どんなに最悪の状況でも、必ずそこそこ動くものは持ってくる凡人」か。チームで働いていれば、答えは明らかです。前者はリスクであり、後者は計算できる資産です。天才は賭けられる。凡人は任せられる。 組織が求めているのは、後者です。なぜでしょうか。仕事には締め切りがあります。依存関係があります。他のメンバーのスケジュールがあります。私の成果物を待っている人がいます。私が遅れれば、その人も遅れます。その人が遅れれば、次の人も遅れます。「今日は調子が悪いので」という言葉は、その連鎖の中では通用しません。だから、周囲からの信頼とは、「最高の自分」ではなく「最悪の自分」に対して支払われます。あのシニアエンジニアが信頼されているのは、華麗なワンライナーを書けるからではありません。障害が起きたとき、体調が悪いときでも、最低限の品質で対応を完了させるからです。レビューが溜まっているとき、モチベーションが上がらないときでも、的確なコメントを返すからです。「この人に任せれば、最悪でもこのレベルは下回らない」という安心感。それが信頼の正体です。プロフェッショナルとは、派手なファインプレーをする人ではありません。どんな悪条件でも、期待された成果を淡々と、確実に納品できる人のことです。野球で言えば、たまにホームランを打つ選手ではなく、どんな状況でも確実にヒットを打てる選手。料理で言えば、たまに絶品を作る料理人ではなく、毎日安定して美味しいものを出せる料理人。派手さはありませんが、計算できます。それがプロです。なぜマニュアル本を読んでも「床」は上がらないのか「下限」が大事だということはわかった。では、どうすれば下限を上げられるのか。その答えを求めて、私たちは成功者の話に耳を傾けます。本、セミナー、SNS。「こうすればうまくいく」と教えてくれる人はたくさんいます。しかし、残念ながら、それらは役に立ちません。なぜでしょうか。成功とは「その人固有の条件」と「その時点での環境」が噛み合った結果であり、その組み合わせは二度と再現されないからです。10年のキャリアがあった人と、始めたばかりの人では前提が違います。たまたま有名な人にリツイートされた人と、そうでない人では運が違います。他人の成功パターンをコピーしても意味がありません。だから、私たちがやるべきことは、誰かの成功法則を学ぶことではありません。自分自身の「下限」を把握し、その下限を少しずつ上げていく仕組みを作ることです。 それは誰にも教えてもらえません。自分で試行錯誤するしかないのです。能力は文脈の中にしかない他人の成功パターンをコピーしても意味がない。自分の下限を自分で上げていくしかない。そう書きました。しかし、ここで少し立ち止まって考えたいことがあります。そもそも「能力」とは何なのでしょうか。私たちが上げようとしている「下限」とは、何の下限なのでしょうか。私たちは「能力」を、自分の中に固定的に存在するパラメータのように考えがちです。技術力がいくつ、コミュニケーション力がいくつ、というように。しかし、私はそうは思いません。能力は、環境によって大きく変わるものです。私は自分の技術力や業務遂行力を、完全に文脈依存だと思っています。ある環境では、私の思考パターンや働き方が完璧に噛み合い、高いパフォーマンスが出ます。しかし、別の環境では、私は無能になるでしょう。政治的な調整が最優先される組織や、レガシーな技術に固執する現場では、私の強みは発揮されません。あのプロジェクトがうまくいったのは、自分の技術力が高かったからでしょうか。それとも、チームメンバーが優秀だったからでしょうか。上司が適切にスコープを切ってくれたからでしょうか。インフラが安定していたからでしょうか。ドキュメントが整っていたからでしょうか。締め切りに余裕があったからでしょうか。その支えが消えた場合、同じクオリティを出せるでしょうか。「自分には能力がある」と過信するのは危険です。正しい認識はこうです。「この文脈において、これまでの経験と仕組みが噛み合って、たまたま価値が出せている」。この認識があれば、傲慢にはなれません。自分が成果を出せているのは、周囲の環境や、他者のサポートのおかげであるという事実が見えてきます。そして、その環境が変わったときに自分がどうなるかを、冷静に想像できるようになります。たとえるなら、魚と水の関係に似ています。魚は水の中では自由に泳げますが、陸に上がれば何もできません。 私たちは常に、自分の能力が機能する「水」の中にいます。その「水」がなくなったとき、私たちは何もできません。だからこそ、2つのことが必要だと私は思っています。1つは、自分に合った「水」を見つけること。自分の能力が活きる環境を選ぶこと。もう1つは、「水」がなくなったときにも最低限動けるように、自分の「下限」を上げておくことです。環境に恵まれなくても、最低限のアウトプットは出せる状態を作っておくこと。「頑張り」という免罪符能力は文脈に依存する。環境が変われば、同じ人間でも発揮できるパフォーマンスは変わる。だからこそ、自分に合った環境を見つけ、下限を上げる仕組みを作ることが大事だと書きました。ここまで読んで、こう思った人もいるかもしれません。「環境だの仕組みだの言っているけど、結局は頑張れば何とかなるのではないか」と。気持ちはわかります。私もそう思っていた時期がありました。しかし、残念ながら、そうではありません。多くの人は、能力の不足を「頑張り」で埋めようとします。環境が悪くても、仕組みがなくても、気合で乗り越えようとします。私もそうでした。しかし、「頑張り」は実力ではありません。なぜそう言えるのでしょうか。思い返してみてください。「頑張っています」という言葉を、どんなときに使ったでしょうか。私の場合、成果が出ていないときほど、その言葉を使っていました。深夜まで残業した。休日も勉強した。ドキュメントも読んだ。だから許してほしい。私も例外ではありません。締め切り前に焦って残業した経験は何度もあります。そのとき、「これだけやっているのだから」という気持ちが、どこかにありました。成果が出なくても、頑張った事実が自分を守ってくれるような気がしていました。しかし、「これだけ苦労したのだから」という免罪符は、プロの世界では通用しません。 専門的な仕事に対する報酬は、流した汗の量ではなく、生み出した価値に対して支払われるからです。私が「頑張ったのにできなかった」と最後に言ったのはいつだったでしょうか。その頑張りは、成果とどう結びついていたでしょうか。正直に振り返ると、「頑張り」と「成果」の間には、驚くほど相関がありませんでした。もう少し踏み込んで考えてみましょう。なぜ「頑張り」は実力にならないのでしょうか。人間の精神力や体力といった不安定なリソースに依存したシステムは、いずれ破綻するからです。徹夜で乗り切った。気合で押し切った。それは一時的には機能するでしょう。しかし、そのやり方は再現できません。翌週も同じことをやれと言われたら、身体が壊れます。翌月も同じことをやれと言われたら、心が壊れます。「頑張り」で出した成果は、「最高の自分」と同じです。再現性がありません。だから、実力とは呼べないのです。来月も同じことができないなら、それは実力ではありません。誤解しないでほしいのは、「頑張るな」と言いたいわけではないということです。踏ん張るべき時は、踏ん張らなければなりません。問題は、頑張ることそれ自体が目的化してしまうことです。方向を考えずにただ頑張る。成果ではなく、頑張っている姿勢で自分を守ろうとする。それは努力ではなく、努力のふりです。目指すべきは「頑張らなくても成果が出る状態」です。 怠けることではありません。頑張りに依存しなくても回る仕組みを作ることです。そうすれば、本当に踏ん張るべき時に、余力を残しておけます。環境構築という本当の能力「頑張り」に頼らない。では、具体的に何をすればいいのでしょうか。私なりの答えは、「最悪の自分でも動ける仕組みを作る」 ことです。気力ゼロの日でも実行できる仕組みを、私はいくつ持っているだろうか。この問いを自分に投げかけたとき、意外なほど少ないことに気づきました。エディタを開いたら自動でテストを走らせる。プルリクエストを出したら自動でレビュワーをアサインする。障害が起きたらアラートを飛ばし、対応手順書を自動で開く。毎朝同じ時間に、昨日のタスクの振り返りをSlackに届ける。毎週同じ曜日に、今週やるべきことをリストアップする。これはすべて、最悪の状態でも最低限の品質を担保するための仕組みです。私が目指しているのは、最悪の日でも自動的に手が動き、最低限のクオリティのものが出来上がってしまう状態を作ることです。意志の力で動くのではなく、意志がなくても動いてしまう仕組みを作る。これこそが「環境構築能力」であり、本当の意味での「実力」です。逆に、仕組み化されていない行動を見てみましょう。タスク管理ツールを開くのが面倒だから、頭の中で覚えておく。テストを書くのが面倒だから、動作確認は目視でやる。ドキュメントを書くのが面倒だから、後で誰かに聞けばいいと放置する。コードレビューを依頼するのが面倒だから、自分で何度も見直す。これはすべて、調子が良いときにしか機能しないシステムです。調子が悪くなった瞬間、すべてが崩壊します。頭の中のタスクは忘れます。目視の確認は見落とします。誰かに聞こうと思っていたことは、聞きそびれます。手を動かすまでのハードルはどこに潜んでいるでしょうか。それを仕組み化ではなく気合で乗り越えていないでしょうか。私はそう思って、少しずつ仕組みを増やしてきました。「人」を「環境」に合わせるな仕組みを作る話をしてきました。しかし、仕組みを作ろうとするとき、多くの人がある罠にはまります。「自分を変えなければ」という罠です。たとえば、こんなふうに自分を責めていないでしょうか。「なぜ自分はこんなに集中力がないのか」。「なぜ自分はこんなにやる気が出ないのか」。「なぜ自分は普通の人のように働けないのか」。その問いの立て方が、そもそも間違っています。「人」を「環境」に合わせようとするから苦しくなります。「自分を変えなければ」「自分が適応しなければ」と考えるから、うまくいかない自分を責めてしまいます。発想を逆転させるべきです。「集中力がなくても成果が出る環境を作れないか」と考える。「やる気がなくても手が動く仕組みを作れないか」と工夫する。「普通の働き方ができなくても、自分なりの働き方で成果を出せないか」と模索する。「障害」は人側にあるのではありません。環境側にあります。人を直すのではなく、環境を直す。それがエンジニアリングです。これは、私たちエンジニアにとっては馴染みのある考え方のはずです。ユーザーがシステムを使いこなせないとき、「ユーザーの能力が低い」とは言いません。「UIが悪い」と言います。システムがユーザーに合わせるべきであって、ユーザーがシステムに合わせるべきではありません。同じことが、自分自身にも言えます。自分という「ユーザー」が動きやすいように、自分の環境という「システム」を設計する。自分の弱点を克服しようとするのではなく、弱点があっても回るように環境を設計する。私たちは日々、他者のためにシステムを設計しています。そのシステムが、特定の「正常」を前提にしていないでしょうか。最高のコンディションの人間しか使えないように設計されていないでしょうか。最悪の状態の人間でも最低限動けるように設計されているでしょうか。自分自身の働き方も、同じように設計すべきです。「正常」な自分を前提にしない。「最悪」の自分でも回るように設計する。弱さこそが、堅牢なシステムを作る「人」を「環境」に合わせるのではなく、「環境」を「人」に合わせる。自分の弱点を克服しようとするのではなく、弱点があっても回るように環境を設計する。そう書くと、まるで弱さを隠すための工夫のように聞こえるかもしれません。弱い自分を誤魔化して、なんとかやり過ごすためのハックのように。しかし、私が言いたいのは、そういうことではありません。むしろ逆です。弱さは、隠すものではありません。弱さこそが、堅牢なシステムを作るための仕様書になります。私たちは誰でも、何かしら「苦手なこと」を抱えています。朝が弱い。人前で話すのが苦手。細かい作業が続かない。逆に、一度集中すると周りが見えなくなる。そういった、ごく普通の凸凹です。「このエラーメッセージは不親切だ」と感じるのは、かつて自分が同じような場面で困った経験があるからです。「このドキュメントはわかりにくい」と感じるのは、かつて自分がわからなくて苦しんだ経験があるからです。「このUIは使いにくい」と感じるのは、かつて自分が同じように躓いた経験があるからです。欠損は、視点を生みます。 困った経験は、問題を発見する能力になります。痛みを知っているからこそ、他者の痛みに気づけます。うまくいった人には、うまくいかない人の気持ちがわかりません。私自身、そうでした。「正常」に適応できていた頃の私には、「正常」の問題点が見えませんでした。システムにうまく乗れていた頃の私には、そのシステムから弾かれる人の存在が見えませんでした。自分が躓いて初めて、躓く人のための設計ができるようになりました。だから、過去の「苦手」を恥じる必要はありません。それは、視点の源泉です。「最悪の自分」を知っているからこそ、「最悪の状態でも動けるシステム」を設計できるのです。 自分のバグを知り尽くしているからこそ、バグに強いシステムを作れます。評価されるとは、下限が固定されることここまで、「下限を上げることが大事だ」と書いてきました。自分の苦手を知り、それを視点として活かし、最悪の状態でも動ける仕組みを作る。それが実力になると。ここまで読むと、「じゃあ下限を上げ続ければいいんだな」と思うかもしれません。しかし、話はそう単純ではありません。ここで1つ、厄介な問題について触れておかなければなりません。キャリアを積み、シニアになり、周囲から「できる人」として扱われるようになると、ある種の息苦しさが生まれます。「あの人ならこのレベル」という期待。それは信頼の証であると同時に、私たちを縛る鎖でもあります。評価されるということは、自分の「下限」が社会的に可視化され、固定されることを意味します。そして、ここに厄介な問題があります。下限が固定されると、それを下げることが許されなくなるのです。本来、下限を上げていくためには、一時的に下限を下げる必要があります。これは矛盾しているように聞こえるでしょうが、考えてみれば当然のことです。新しい領域に挑戦すれば、最初は当然うまくいきません。慣れない技術を使えば、普段の半分のクオリティしか出せません。未経験の役割を引き受ければ、しばらくは無能に見えます。たとえば、10年間Javaを書いてきたエンジニアがRustを学び始めたとします。最初の数ヶ月、その人の「下限」は確実に下がります。Javaなら寝ぼけていても書けたコードが、Rustでは何時間もかかります。しかし、その一時的な後退を経て、やがてRustでも安定した成果を出せるようになります。同様に、初めてチームリーダーを務める人は、最初は判断を誤り、メンバーとの関係構築に苦労するでしょう。しかし、その経験を経て、リーダーとしての「下限」が形成されていきます。これは成長のための必要なコストです。一時的に下がった下限は、経験を積むことで元の水準を超えていきます。しかし、「あの人ならこのレベル」という期待が固定されてしまうと、その期待を下回ることが許されなくなります。失敗が許されません。実験が許されません。成長のための一時的な後退が、信頼の毀損として記録されてしまいます。だから私は、仕事を選ぶようになりました。自分の下限が確実に通用する領域、自分のシステムが機能する文脈を選ばざるを得なくなりました。「結果を出す以外の選択肢がない」状況で、わざわざ未知の領域に踏み込むリスクを取れなくなりました。これは成長の鈍化を意味します。安全圏に留まり続けることで、下限は維持されますが、それ以上には上がりません。皮肉なことに、「信頼される」ことが「成長できなくなる」ことと表裏一体になっています。 評価されることの代償は、挑戦する自由を失うことです。期待に応え続けることと、成長し続けることは、両立しません。だからこそ、意識的に「失敗してもいい場所」を確保しておく必要があります。誰にも見せないプロジェクト。評価と切り離された実験。下限を一時的に下げることが許される、安全な砂場。それがなければ、私たちは自分の「実力」に閉じ込められてしまいます。余白がなければ成長できない評価されることで挑戦する自由を失う。「失敗してもいい場所」を意識的に確保しなければならない。ここまで書いてきて、気づいたことがあります。これは私個人の問題ではありません。もっと広い話です。「下限」の問題は、個人だけで解決できるものではありません。先ほど書いたように、下限を上げるためには、一時的に下限を下げる必要があります。新しいことに挑戦すれば、最初は失敗します。失敗が許されない環境では、挑戦ができません。挑戦ができなければ、成長もできません。つまり、成長には「余白」が必要なのです。「余白」とは何でしょうか。失敗しても致命傷にならない空間のことです。期待値を下回っても、信頼が毀損されない関係性のことです。最悪の状態を見せても、それを受け入れてもらえる場所のことです。エンジニアは強くなければならない。弱音を吐いてはいけない。立ち止まってはいけない。誰よりも速く学び、誰よりも多くのコードを書き、誰よりも深く技術を理解する。その強迫観念は、私たちを奮い立たせるガソリンであると同時に、余白を奪う呪いでもあります。常に100%を出し続けなければならない。そう思い込んでいる人は多いです。しかし、100%を出し続けることは、人間には不可能です。そして、100%を要求される環境では、人は80%の自分を見せることを恐れます。80%の自分を見せることを恐れるから、新しいことに挑戦できません。新しいことに挑戦できないから、100%のまま停滞します。完璧主義は、成長の敵です。「挑戦しろ」と背中を押す一方で、いざ失敗すれば「自己責任」の名の下に切り捨てる。そんな構造の中では、誰も本当の意味での挑戦ができません。みんな、自分の下限が確実に通用する範囲でしか動かなくなります。結果として、組織全体の成長が止まります。だから、「余白」は個人で確保するだけでなく、チームや組織として設計する必要があります。「最悪の日でも最低限の成果を出せる環境」を作るのは、個人の努力だけでは限界があります。チームとして、組織として、メンバーの「下限」を支える仕組みを作る。失敗を許容する文化を作る。一時的な後退を、成長のための投資として認める空気を作る。それが本当の意味での「強いチーム」です。 全員が常に100%を出し続けるチームではありません。誰かが50%しか出せない日があっても、チーム全体としては回るように設計されたチームです。個人の「下限」を上げる努力と、組織として「余白」を確保する努力。この両方が揃って初めて、持続的な成長が可能になります。床を1ミリずつ上げていくここまで、長々と書いてきました。「最高の自分」ではなく「最悪の自分」が実力である。信頼は下限に支払われる。能力は文脈に依存する。頑張りは実力ではない。環境を設計する。弱さを視点にする。評価は下限を固定する。成長には余白が必要。いろいろ書きましたが、言いたかったことはシンプルです。「能力」の定義を変えてほしい、ということです。「最高のときに出せるもの」から「最悪のときにも出せるもの」へ。自分の状態がベストであることを前提にしない。10分の1のコンディションでも形になるように設計する。緊張しても、失敗しても、体調が悪くてもいい。そのボロボロの状態から這いつくばって出したアウトプットだけを見る。それが、今の私の揺るぎない実力です。絶望する必要はありません。自分の「下限」、つまり床がどこにあるかを知っていれば、その床の上にレンガを積んでいくことができます。半年前の自分は、最悪の日に何ができていなかったでしょうか。タスク管理は頭の中だったでしょうか。テストは書いていなかったでしょうか。ドキュメントは後回しにしていたでしょうか。今はそのうち、どれだけ自動化・習慣化されているでしょうか。継続とは、平均値を上げることではありません。この床を1ミリずつ底上げしていく作業のことです。派手な成功は、運です。華々しい成果は、上振れです。 そんなものを基準にしてはいけません。運は二度来るとは限りませんが、仕組みは何度でも動きます。淡々と、最悪の日でも最低限のことをやる。その積み重ねだけが、誰にも奪われない実力になります。いつかその床の高さが、誰かの天井を超えたとき、私は誰からも信頼されるプロフェッショナルになっているはずです。おわりに最後に、この記事自体の話をさせてください。この記事を書きながら、私自身も自分の「下限」と向き合っています。正直に言えば、この文章を書いている今日も、絶好調とは言えません。頭がぼんやりして、言葉がすぐに出てきません。何度も書いては消し、消しては書いています。「おい、」シリーズのように言葉がスラスラ出てくる日ではありません。しかし、それでいいのです。この記事の価値は、私が絶好調のときに華麗な文章を書けることではありません。調子が悪い日でも、キーボードへ向かい、一文字ずつ積み上げ、最後まで形にできるかどうか。それこそが、私の「書く実力」です。冒頭で書いたように、私はかつて「最高の自分」を実力だと勘違いし、そこに届かない自分を責め続けていました。なんでもっとできないんだ、と。鬱屈とした日々を過ごし、心身ともに疲弊し、パフォーマンスはさらに落ちました。この記事は、あの頃の自分に向けて書きました。伝えたいのは、「基準が間違っている」ということです。私たちは、自分の「最高の瞬間」に執着しすぎています。あの日の自分、あのプロジェクトでの自分、あの輝いていた自分。しかし、その輝きは再現できません。再現できないものを基準にすれば、永遠に自分を肯定できません。だから、視点を変えました。最悪の日に、机に向かえるか。最悪の状態で、最低限のものを出せるか。その「下限」こそが、私の本当の実力です。そしてその下限は、仕組みと環境と、少しずつの積み重ねで、確実に上げていくことができます。派手な成功を追いかける必要はありません。ただ、最悪の日でも崩れない床を、1ミリずつ上げていけばいいのです。その床の高さが、いつか私を支えます。誰にも奪えない、揺るぎない実力として。そして、もう1つ。自分の弱さを恥じる必要はありません。その弱さがあったからこそ、私は「自分を助けるための仕組み」を発明できました。その仕組みは、いずれ同じ弱さを持つ誰かを救うことになります。私の「最悪の日」の対処法は、誰かにとっての「最高のノウハウ」になります。自分の下限を知ることは、諦めではありません。出発点です。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIに手順書を書かせよう! 手順書作成で向き合うAIの不確実性]]></title>
            <link>https://zenn.dev/kamos/articles/procedure_book_with_ai</link>
            <guid isPermaLink="false">https://zenn.dev/kamos/articles/procedure_book_with_ai</guid>
            <pubDate>Mon, 08 Dec 2025 15:23:49 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIに手順書を書かせてみよう! 手順書にはいくつか必要なポイントがあるね!明確な作業目的作業内容の確実性手順の網羅性影響範囲AIはここに書かれていること、結構苦手だよね。特に作業内容の確実性を担保することは苦手なんだよね!だから、AIに手順書を書かせるときは、AIが苦手なポイントを補うように工夫する必要があるよ!今回は、AIに｢災害時検証: CloudSQLリージョン移行｣の手順書を書かせてみて、一緒に工夫してみよう! 手順作成 まずはそのまままずは、AIにそのまま手順書を書かせてみよう! 以下のプロンプトを使用してみたよ!Cloud SQLの...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[メモリ安全なC言語実装「Fil-C」について紹介]]></title>
            <link>https://dev.mix64.com/2025/12/08/post-397/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/12/08/post-397/</guid>
            <pubDate>Mon, 08 Dec 2025 13:03:10 GMT</pubDate>
            <content:encoded><![CDATA[今回はメモリ安全なC言語実装を提供できる「Fil-C」について紹介します。既存のC言語プログラムに対しても互換性を持ち、再コンパイルすること...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[初心で挑むredis入門 ~Redis hashes編~]]></title>
            <link>https://zenn.dev/akasan/articles/redis_data_hash</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/redis_data_hash</guid>
            <pubDate>Mon, 08 Dec 2025 12:19:17 GMT</pubDate>
            <content:encoded><![CDATA[今回はredisで使えるhashesについてみていきます。昨日公開したStringsについてもぜひご覧ください。https://zenn.dev/akasan/articles/redis_datatypes 早速検証！！redisの環境構築については先日公開した以下の記事を参考にしてください。https://zenn.dev/akasan/articles/redis_quickstartRedis hashesのドキュメントは以下になります。https://redis.io/docs/latest/develop/data-types/hashes/ hashesに...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【初参加】CODE BLUE 2025レポート：体感したトレンドとAIの脅威]]></title>
            <link>https://qiita.com/yutaf11/items/239101da0bf5265b61df</link>
            <guid isPermaLink="false">https://qiita.com/yutaf11/items/239101da0bf5265b61df</guid>
            <pubDate>Mon, 08 Dec 2025 08:35:39 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先月、CODE BLUE 2025に参加してきました。私は普段、SRE兼セキュリティエンジニアとして働いています。過去、SREとして技術系のイベントにはいくつか参加してきましたが、セキュリティ特化のオフラインイベントは今回のCODE BLUEが初めてでした。こ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[技術広報はちゃんとなめてやれ（技術広報をなめるなを読んで）　]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/08/152614</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/08/152614</guid>
            <pubDate>Mon, 08 Dec 2025 06:26:14 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、whywaita Advent Calendar 2025 8日目のエントリ記事です。whywaita Advent Calendar 10周年ということで、自分もwhywaitaとの出会いと10年という節目を掛けて何か書きたいと考えたのですが、うまいネタが思いつかず。とはいえ、whywaitaと出会ったきっかけがお祭り的な技術イベントだったので、今回は技術イベントの「お祭り性」について語っていきます。思い返すと、技術コミュニティとの出会いは、いつもお祭りのようでした。見知らぬ人と技術の話で盛り上がり、気づいたらとんでもない深い時間になっていた懇親会。準備段階から当日まで、ワクワクしながら作り上げた勉強会。あの空気感こそが、私をエンジニアとして成長させてくれた原動力でもありました。そんな私が最近読んで、考えさせられた記事があります。はじめにSakutaroさんが書かれた「技術広報をなめるな」を読みました。note.comSakutaroさんの主張をこの記事で使うために要約すると、技術広報とは「技術に関する情報流通を最適化すること」であり、採用やブランディングにじわじわ効いてくる組織の筋肉である、ということです。片手間でやるものではなく、専門性を持って取り組むべき重要な機能だと。その主張には100%同意します。技術広報を軽視する組織への警鐘として、価値のある記事でした。詳しくは読んで下さい。ただ、読み終わったあと、ひとつ気になることがありました。「なめるな」と言われて、真面目に取り組んだ人は、どうなるだろう。技術広報の重要性を理解した。だから本気で取り組んだ。毎週ブログを書き、登壇の機会を作り、勉強会を企画した。でも、半年後、1年後、その人はまだ続けているだろうか。私が見てきた現実では、真面目に取り組んだ人ほど、燃え尽きていく。「技術広報は大事だ」と理解しているからこそ、手を抜けない。手を抜けないから、疲弊する。疲弊するから、続かない。続かないから、また新しい誰かが「大事だから」と引き継いで、同じサイクルを繰り返す。ここで断っておくと、私は専任のDevRelや技術広報をやっていたわけではありません。エンジニアとしてブログを書いたり、登壇したり、勉強会を企画したり、そういう活動に参加してきた側です。だから以下は、「現場で技術広報に関わってきたエンジニア」としての個人的な意見です。Sakutaroさんへの反論や批判ではなく、同じテーマを別の角度から眺めてみた、という試みです。Sakutaroさんが「技術広報の重要性」を語ったのなら、私は「技術広報の持続可能性」を語りたい。Sakutaroさんが「なめるな」と言ったのなら、私は「ちゃんとなめてやれ」と言いたい。「なめる」というのは、軽視することではありません。肩の力を抜いて、それでも真剣に向き合うこと。重く構えすぎず、軽やかに、本気で楽しむこと。そういう姿勢を指しています。この記事で言いたいのは、技術広報を「お祭り」として捉え直すことで、どう持続可能な形に設計できるか、という話です。「技術広報を続けられない」のは、個人の努力不足なのか技術広報が続かない。ブログの更新が止まる。勉強会の開催頻度が落ちる。登壇者が見つからない。こうした現象を見たとき、私たちはつい「担当者の努力が足りない」「モチベーションの問題だ」と考えがちです。でも、本当にそうでしょうか。私が見てきた限り、技術広報に関わる人は真面目な人が多い。「会社のためになる」「エンジニアの成長につながる」と信じているからこそ、時間を割いて取り組んでいる。努力が足りないのではなく、むしろ、努力しすぎて燃え尽きている。つまり、個人の努力ではなく、構造に原因があるのではないか。技術広報を「重要な業務」として位置づけるほど、プレッシャーは増す。「会社の顔としてふさわしい記事を」「PVやシェア数で成果を示さないと」「毎月コンスタントに発信を」。こうした期待は、真面目な人ほど重く受け止める。結果として、技術広報は「楽しいからやる」ものではなく「やらなければならない」ものになる。義務感で動く活動は、長くは続きません。だから私は、技術広報を「お祭り」として捉え直すことを提案したい。技術広報を「お祭り」として捉えたとき、何が変わるのか「お祭り」と「業務」の違いは何か。業務には、目標がある。KPIがある。期限がある。評価がある。達成できなければ、失敗になる。お祭りには、もちろん準備や段取りがある。でも、本質は違う。非日常性があって、ワクワクして、参加は自由で、失敗しても笑って済む。みんなで作り上げる。終わったあとに「楽しかったね」と言い合える。思い出してみてください。あなたが「楽しかった」と感じた技術イベントには、何がありましたか。KPIはなかったはずです。評価もなかった。ただ、技術の好きな人たちが集まって、ワイワイやっていた。それだけで、あの場は価値があった。技術広報を「業務」として捉えると、タスクになり、KPIになり、疲弊の原因になります。でも「お祭り」として捉えると、楽しみになり、創造性の源泉になり、持続可能な活動になる。もちろん、会社という組織なのでKPIは必要です。数字で語らないと理解されないこともある。大人ですから、建前として必要なものは必要です。でも本音の部分では、お祭りなんです。Sakutaroさんは技術広報を「技術に関する情報流通を最適化すること」と定義しました。私はその定義に異論はありません。ただ「情報流通の最適化」という言葉は正確ですが、人を動かす力は弱い。「今月の情報流通を最適化しよう」と言われても、イメージが湧かない。でも「お祭りを企画して盛り上げよう」と言い換えると、途端にイメージが湧きます。人は「最適化」という目標には動きにくいけど、「お祭り」という体験には参加したがるんです。そして面白いことに、良いお祭りを企画しようとすると、自然と「情報流通の最適化」が達成されます。読みたくなるブログは情報が届く。参加したくなる勉強会は知見が共有される。面白いカンファレンスブースはブランドが伝わる。お祭りが楽しいのは、予定調和じゃないからです。神輿が予想外の方向に進んだり、知らない人と急に仲良くなったり、思いもよらない出来事が起きる。その「意外性」がお祭りの醍醐味です。技術広報も同じで、完璧に計画されたブログより、思いつきで書いた記事がバズることもある。意外性こそが人の心を動かします。でも、意外性は余裕がないと生まれません。タスクに追われている人に、遊び心は出てこない。「やらなきゃいけない」という義務感からは、「やってみたら面白かった」という発見は生まれない。だから、技術広報には「精神的な遊び」が必要です。お祭りを「業務」として100%真面目にやると、それはもはやお祭りではなくなります。参加の形は、ひとつじゃないお祭りには色んな参加の仕方があります。神輿を担ぐ人もいれば、屋台で焼きそばを売る人もいる。踊る人もいれば、見ているだけの人もいる。写真を撮る人も、SNSで実況する人もいる。ゴミを拾う人も、場所取りをする人もいる。どの参加の仕方も、お祭りの一部です。技術広報も同じです。記事を書く人だけが貢献者ではない。レビューする人も貢献者です。アイデアを出す人も貢献者です。社内で記事をシェアする人も貢献者です。「この前のあの話、ブログにしたら面白そう」と声をかける人も貢献者です。登壇者の練習に付き合う人も貢献者です。「ブログを書いてもらえない」「登壇してもらえない」と悩んでいるなら、視点を変えてみてください。「書いてもらう」「登壇してもらう」以外の参加の形を、用意できているだろうか。神輿を担げる人は限られています。でも、お祭りを楽しむ方法は無数にある。担ぎ手だけがお祭りの参加者ではないんです。「ブログを書いてください」ではなく「先週のSlackでのやり取り、そのままブログにしませんか。私がタイトルと導入書きますよ」。「登壇してください」ではなく「5分のLTでいいので、この前の話をしてくれませんか」。義務ではなく、招待として。「ブログ書いてください」はお願い（義務感）。「ブログ書きませんか」は招待（選択肢）。この違いは大きいんです。あなた自身は、どうでしょうか。技術広報にどんな形でなら、無理なく関われそうですか。「怒られない範囲」は誰が決めているのかお祭りにも「やっていいこと」と「やってはいけないこと」がある。技術広報も同じです。失敗談を書け、人間臭さを出せ、と言われても、リスクが怖い。その懸念は正しいです。だからこそ、「怒られない範囲」を見極める力が必要になります。ただ、その「怒られない範囲」は、誰が決めているのでしょうか。明文化されたルールがあるのか、暗黙の了解なのか。上司が決めているのか、広報部門が決めているのか、法務が決めているのか。あるいは、なんとなく「空気」で決まっているのか。多くの組織では、「怒られない範囲」は明確に定義されていません。だから、発信する側は常に不安を抱えることになる。「これ、出していいのかな」「怒られないかな」。その不安が、発信のハードルを上げている。社内的にはOKだけど、社外的にNGになるケースがあります。「技術的には正しいけど、今その話題は炎上しやすい」という場合です。社外的にはOKだけど、社内的にNGになるケースもあります。「業界では普通の話題だけど、うちの会社ではタブー」という場合です。「怒られない範囲」を見極める能力とは、社内外の文脈を読む力です。これは経験を積むことでしか身につきません。小さく発信して、反応を見て、学んでいく。でも、もし組織として技術広報を続けたいなら、「怒られない範囲」を個人の判断に委ねるのではなく、組織として明確にする努力が必要ではないでしょうか。「ここまではOK」「これはNG」「迷ったらこの人に相談」。そういった指針があるだけで、発信のハードルはぐっと下がります。あなたの組織では、「怒られない範囲」はどのように決まっていますか。誰が決めていますか。それは明文化されていますか。持続可能にするために最後に、どうすれば技術広報を続けられるのか、という話をします。技術広報に関わる人が陥りがちな罠は、自分一人で全部やろうとすることです。ブログの企画、執筆依頼、レビュー、公開作業、SNSでの拡散。全部一人でやると、短期的には回ります。でも、長期的には崩壊します。お祭りは、主催者一人では成立しません。屋台を出す人、演奏する人、ゴミを拾う人、写真を撮る人、SNSで拡散する人。みんなが違う形で参加して、初めてお祭りは盛り上がります。「一人が100やる」のではなく、「10やる人、5やる人、1でも協力してくれる人を探す」これが持続の秘訣です。例えば、こんな工夫ができます。月に1回「ブログネタ出し会」を30分だけ開く。Slackに「こんな話をブログにしたい」と投げるだけのチャンネルを作る。「書けそうな人」ではなく「話が面白かった人」に声をかける。小さな仕組みを作っておくだけで、協力者は見つかりやすくなります。そして、もう1つ大事なこと。人間には波があるということです。10やれる時期もあれば、5しかやれない時期もある。1すらもやれない時期もある。プロジェクトが佳境に入っている時期。体調を崩している時期。家庭の事情がある時期。メンタルが落ちている時期。これは恥ずかしいことでも、甘えでもありません。人間だもの。「去年できたから、今年もできる」という思い込みこそが、燃え尽きの原因なんです。10やれる時は10やる。5しかやれない時は5でいい。やれない時は、休む。大事なのは、この「波」を組織として受け入れられているかどうかです。「先月は3本記事を出したのに、今月は1本もない。どうしたの」というプレッシャーがかかるなら、それは持続可能な仕組みとは言えません。「今月は厳しいので、来月がんばります」と言える文化があるかどうか。あなたのチームでは、パフォーマンスの波を受け入れられていますか。「今は無理」と言える空気がありますか。おわりに冒頭で書いた通り、私とwhywaitaの出会いは、お祭り的な技術イベントでした。あの場には「情報流通の最適化」なんて言葉はなかった。ただ、技術の好きな人たちが集まって、ワイワイやっていただけです。でも、今ならわかります。私がワイワイと参加していたあのイベントの裏側には、真面目に予算を集めてきた人がいた。色んなステークホルダーの合意をまとめてきた人がいた。会場を押さえ、スケジュールを調整し、トラブルに備えていた「ちゃんとした大人」がいた。私はその恩恵を受けて、楽しんでいただけだったんです。10年経って、そのことがようやくわかるようになりました。いずれ自分も、あの「ちゃんとした大人」の側に回らなければならない。恩返しをしなければならない。その自覚はあります。でも、それでも。いや、だからこそ。次の世代の人たちには、お祭り感を味わってほしい。「裏側の苦労」を見せずに、「楽しかったね」と言ってもらえるイベントを作りたい。真面目に準備しながら、参加者には「お祭り」として届ける。それが、私なりの恩返しの形だと思っています。技術広報に関わるすべての人へ。疲れたら、休んでください。無理したら、倒れます。真面目すぎたら、続きません。でも、楽しさだけでも続きません。楽しさと、仕組みと、仲間が必要です。もしあなたが今「何もやれない時期」にいるなら、それでいいんです。休んでください。お祭りは、また元気になってから参加すればいい。技術広報は、あなたがいないと回らないほど脆弱なものであってはいけない。でも、あなたがいると、もっと楽しくなる。それくらいの距離感がちょうどいい。10年前のあの日、技術イベントで会った人と、今もこうしてAdvent Calendarで繋がっている。これこそが、お祭り駆動の技術広報の成果です。どこかのカンファレンスの懇親会で会ったら、お祭りの話をしましょう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年AWS Community Builderの活動報告]]></title>
            <link>https://blog.masasuzu.net/entry/2025/12/08/100000</link>
            <guid isPermaLink="false">https://blog.masasuzu.net/entry/2025/12/08/100000</guid>
            <pubDate>Mon, 08 Dec 2025 01:00:00 GMT</pubDate>
            <content:encoded><![CDATA[今年はブログ4本(純粋なAWSの記事はなし)、登壇5本(内社内2本)という結果でした。勉強会参加自体はそこそこしてたんですが、アウトプットという点では少し物足りない結果になったなという感想です。要因としてはGoogle Cloud関連の活動が比較的多くて、AWS関連にリソースを割けなかったというのもあります。第二の理由としては今年後半が特に業務が多忙で身動きが取れない月があったのも事実です。とはいえ忙しいは言い訳に過ぎないので、なんとアウトプットする仕組み作りをしていきたいとことです。来年はもっとアウトプットを増やしていきたいです。そこで以下の数値を目標にやっていきたいと考えています。社外登壇: 月0.5本AWSテーマのブログ: 月1本以上やってくぞ。以下今年のアウトプットを置いておきます。ブログAWS関連なしクラウドニュートラルblog.masasuzu.netdiary.masasuzu.netdiary.masasuzu.netblog.masasuzu.net登壇AWS関連 speakerdeck.com speakerdeck.com speakerdeck.comクラウドニュートラル speakerdeck.com speakerdeck.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI時代のスキーマファースト開発 FastAPI × GitHub Packages で型安全なSDKを自動配布する]]></title>
            <link>https://zenn.dev/meziron/articles/32ac2241bbec38</link>
            <guid isPermaLink="false">https://zenn.dev/meziron/articles/32ac2241bbec38</guid>
            <pubDate>Sun, 07 Dec 2025 15:00:49 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事は 3-shake Advent Calendar 2025 の記事です。フロントエンド開発者とバックエンド開発者の間で「APIの仕様が違う」「ドキュメントが古い」といった問題に悩まされたことはありませんか？さらにAI時代になり、Claude CodeやCursorなどのAIコーディングツールを使う機会が増えてきました。しかし、AIにAPI呼び出しを実装させると、存在しないエンドポイントを「想像」で実装してしまったり、パラメータの型を間違えたりすることがあります。本記事では、FastAPIのOpenAPI自動生成機能を活用し、GitHub ActionsでTy...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[どこでも動くC言語プログラム「Cosmopolitan Libc」を触ってみた]]></title>
            <link>https://dev.mix64.com/2025/12/07/cosmopolitan-libc/</link>
            <guid isPermaLink="false">https://dev.mix64.com/2025/12/07/cosmopolitan-libc/</guid>
            <pubDate>Sun, 07 Dec 2025 09:16:04 GMT</pubDate>
            <content:encoded><![CDATA[今回はC言語でありながらbuild-anyware run-anywareを目指すプロジェクト「Cosmopolitan Libc」について...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[初心で挑むredis入門 ~Redis Strings編~]]></title>
            <link>https://zenn.dev/akasan/articles/redis_datatypes</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/redis_datatypes</guid>
            <pubDate>Sun, 07 Dec 2025 03:46:51 GMT</pubDate>
            <content:encoded><![CDATA[今回はredisで使えるStringsについてみていきます。 早速検証！！redisの環境構築については先日公開した以下の記事を参考にしてください。https://zenn.dev/akasan/articles/redis_quickstartRedis Stringsのドキュメントは以下になります。https://redis.io/docs/latest/develop/data-types/strings/ 単独の値の格納redisでは文字列やシリアライズされたデータをbytesデータとして格納します。早速keyとvalueを指定して文字列を格納してみましょう。...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[初心で挑むredis入門 ~サーバ起動とPythonからのアクセス~]]></title>
            <link>https://zenn.dev/akasan/articles/redis_quickstart</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/redis_quickstart</guid>
            <pubDate>Sat, 06 Dec 2025 13:56:14 GMT</pubDate>
            <content:encoded><![CDATA[今回は改めてredisに入門してみました。今まで使って経験はありつつ、ちゃんと調べて勉強しようということで使ってみました。まずはサーバの建て方とPythonからのアクセス方法をまとめてみます。 検証内容今回は以下の内容を実施しますredisサーバの起動Docker上でサーバを立てますポートは6379でポートフォワーディングによりローカル環境からアクセスできるようにしますpythonコードからのアクセスシンプルなデータの格納と取得を実施 早速検証！！ redisサーバの起動redisサーバをDocker上で立てます。以下のレポジトリを参考にたてます...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、類推するな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/06/060208</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/06/060208</guid>
            <pubDate>Fri, 05 Dec 2025 21:02:08 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、Rust Advent Calendar 2025 6日目のエントリ記事です。はじめに「それって、○○みたいなものですよね」私は、この言葉に何度救われてきただろう。新しい概念を理解するとき。誰かに説明するとき。問題を解決するとき。類推は、私の思考の基盤だった。いや、今でも基盤だ。ただ、その基盤が思ったほど頑丈ではなかったことを、私は何度も思い知らされてきた。Rustを学び始めた頃の話だ。Rustは、プログラミング言語の1つだ。安全で高速なプログラムを書けることで知られている。私はRustの公式教科書「The Rust Programming Language」を読んでいた。所有権の章に差し掛かったとき、こんな説明に出会った。Rustには「所有権（ownership）」という独特の概念がある。少し専門的な話になるが、プログラムを書くとき、データはコンピュータの「メモリ」という場所に保存される。メモリは有限だから、使い終わったデータは片付けなければならない。片付けを忘れると、メモリがいっぱいになって動かなくなる。逆に、まだ使っているデータを間違えて片付けてしまうと、プログラムが壊れる。多くのプログラミング言語では、この「いつ片付けるか」の管理をプログラマーに任せるか、自動で行うかのどちらかだ。Rustは第三の道を選んだ。「所有権」というルールで、コンパイル時（プログラムを実行する前）に安全性を保証する。ルールはシンプルだ。メモリ上のデータには、必ず1つの「所有者」となる変数が存在する。そして、その値を別の変数に渡すと、所有権が移動（move）する。移動した後は、元の変数からはアクセスできなくなる。所有者がいなくなったデータは、自動的に片付けられる。これがRustの基本ルールだ。（注：この先、コード例が続きます。プログラミングに詳しくない方は、コードの詳細を読み飛ばしても大丈夫です。「類推で理解したつもりになったが、実際は違った」という体験談として読んでいただければ、本記事の主旨は伝わります。）私は頭の中で、勝手に類推を作り上げた。「なるほど、本の貸し借りみたいなものか。本を誰かに貸したら、自分の手元にはない。返してもらうまで読めない」。教科書にそう書いてあったわけではない。私が勝手にそう解釈した。この類推で、所有権の基本は理解できた気がした。コンパイラが怒る理由もわかった。moveが起きる場面も予測できるようになった。私は満足した。「そういうことか」と納得して、次の章に進んだ。しかし、しばらくして困難に直面した。私がやりたかったのは、こういうことだ。本棚に本がある。本を誰かに貸す。貸した本が何かを覚えておきたい。現実世界では当たり前のことだ。これをコードで書こうとした。// 私が書こうとしたコード（コンパイルエラー）struct BookShelf {    books: Vec<String>,    lent_to: Option<&String>,  // 貸した本への参照を持ちたい}Rustでは、所有権を完全に移動させずに、一時的にデータを「見せる」だけの仕組みがある。これを「参照（reference）」や「借用（borrow）」と呼ぶ。&Stringは「Stringへの参照」を意味する。所有権は移動しない。ただ、一時的に覗き見できるだけだ。「本の貸し借り」の類推で考えれば、これは自然なはずだった。本棚には本がある。本を誰かに貸したら、貸した本への参照を持っておく。でも、Rustはこのコードを許さない。error[E0106]: missing lifetime specifier「ライフタイム」。また新しい概念だ。なぜライフタイムが必要なのか。参照は、データの「場所」を覚えている。でも、その場所にあったデータが消えてしまったらどうなるか。参照だけが残って、参照先には何もない。存在しないデータを指す参照。これは危険だ。だから、Rustは参照の「寿命」を追跡する。参照が有効な間は、参照先のデータも存在していなければならない。この寿命を明示するのが、ライフタイムだ。ライフタイムを指定すればいいのか。私は格闘した。// ライフタイムを追加してみるstruct BookShelf<'a> {    books: Vec<String>,    lent_to: Option<&'a String>,}コンパイルは通る。貸し出しもできる。let mut shelf = BookShelf {    books: vec![String::from("Rust Book"), String::from("Programming Rust")],    lent_to: None,};shelf.lent_to = Some(&shelf.books[0]);println!("貸し出し中: {:?}", shelf.lent_to);// => 貸し出し中: Some("Rust Book")でも、本棚に新しい本を追加しようとすると、地獄が始まる。shelf.books.push(String::from("New Book"));error[E0502]: cannot borrow `shelf.books` as mutable because it is also borrowed as immutable  --> src/main.rs:22:5   |18 |     shelf.lent_to = Some(&shelf.books[0]);   |                           ----------- immutable borrow occurs here...22 |     shelf.books.push(String::from("New Book"));   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ mutable borrow occurs here23 |     println!("新しい本を追加: {:?}", shelf.books);   |                                      ----------- immutable borrow later used here「本を貸している間は、本棚に新しい本を追加できない」。現実世界ではありえない制約だ。なぜこんなに難しいのか。私は「本の貸し借り」で考え続けた。貸している間も本棚にどの本があるかは覚えている。本棚に新しい本を追加することと、貸した本を追跡することは、まったく独立した操作のはずだ。なのに、なぜRustはそれを許さないのか。長い時間をかけて、やっと気づいた。私の類推が間違っていた。ここで「借用チェッカー（borrow checker）」の話をしなければならない。Rustには、コンパイル時にメモリ安全性を検証する仕組みがある。これが借用チェッカーだ。借用チェッカーの基本ルールはシンプルだ。「参照が有効な間は、参照先のデータを変更してはならない」。なぜこんなルールがあるのか。Vec（可変長配列）の仕組みを考えてみよう。Vecは、内部的には連続したメモリ領域にデータを格納している。本棚でいえば、横一列に並んだ棚だ。最初に5冊分のスペースを確保したとする。6冊目を追加したいとき、どうなるか。今の棚には入らない。だから、より大きな棚を用意して、5冊をすべて移動させる。そして6冊目を追加する。これがVecの動作だ。ここで問題が起きる。移動前の棚の位置を覚えている参照があったとする。本を移動した後、その参照はどこを指すのか。もう本がない場所だ。空っぽの棚を指している。これが「ダングリングポインタ」と呼ばれる危険な状態だ。存在しないデータへの参照。アクセスしたら、何が起きるかわからない。だから、Rustは「参照がある間は変更禁止」というルールを強制する。現実の本の貸し借りには、この問題は存在しない。本棚のサイズを変えても、貸した本が消えることはない。でも、コンピュータのメモリでは、Vecが成長するときにデータが移動する。「本」という類推が、私の理解を助けると同時に、私の理解を歪めていた。正しい設計は、参照ではなくインデックスや識別子を使うことだった。// アプローチ1: インデックスで管理struct BookShelf {    books: Vec<String>,    lent_index: Option<usize>,}let mut shelf = BookShelf {    books: vec![String::from("Rust Book")],    lent_index: None,};shelf.lent_index = Some(0);  // インデックスを記録shelf.books.push(String::from("New Book"));  // これは動く！println!("貸し出し中: {:?}", shelf.books.get(shelf.lent_index.unwrap()));// => 貸し出し中: Some("Rust Book")// アプローチ2: 所有権を完全に移動struct BookShelf {    books: Vec<String>,}struct LentBook {    book: String,        // 所有権ごと移動    borrower: String,}let mut shelf = BookShelf {    books: vec![String::from("Rust Book"), String::from("Programming Rust")],};let lent = LentBook {    book: shelf.books.remove(0),  // 本棚から取り出す    borrower: String::from("Alice"),};shelf.books.push(String::from("New Book"));  // 本棚は自由に変更できる「本の貸し借り」という類推は、入り口としては正しかった。でも、その類推を引きずりすぎた。Rustにおける「借用」は、現実世界の「貸し借り」とは違う。借用（&T）は「一時的に見せる」だけで、「貸した相手を追跡する」仕組みではない。そして、借用中はデータの変更ができない。この違いに気づくまでに、私は何週間も費やした。類推は、両刃の剣だ。思い返せば、これは初めての失敗ではなかった。非同期処理を学んだときも、同じ罠にはまった。（注：ここからも技術的な話が続きます。コードの詳細は読み飛ばしても、「料理の類推で考えたら、実際の挙動と違った」という話として理解できます。）まず、非同期処理とは何かを説明しておこう。日常生活で考えてみよう。洗濯機を回している間、あなたは洗濯機の前でじっと待っているだろうか。たぶん、その間に別のことをしているはずだ。掃除をしたり、料理を作ったり。洗濯機が終わったら、干しに行く。これが「非同期」の発想だ。プログラムも同じだ。通常のプログラムは、1つの処理が終わるまで次の処理に進めない。ファイルを読み込んでいる間、プログラムは待っている。ネットワークからデータを取得している間も、待っている。これでは効率が悪い。「待っている間に、別のことをやろう」。これが非同期処理だ。私は類推を作り上げた。「非同期処理は、料理を並行して作るようなものか」。パスタを茹でている間にソースを作る。オーブンで肉を焼いている間にサラダを準備する。待ち時間を有効活用して、全体の調理時間を短縮する。この類推で、Rustのasync/await構文の基本は理解できた。async fn cook_dinner() {    let pasta = boil_pasta();      // パスタを茹で始める    let sauce = make_sauce().await; // ソースを作る（待つ）    let pasta = pasta.await;        // パスタが茹で上がるのを待つ    serve(pasta, sauce);}問題は、「共有リソース」にアクセスするコードを書いたときだった。共有リソースとは何か。料理の例で考えよう。キッチンには、コンロが1つしかない。2人の料理人が、同時にそのコンロを使いたいとする。どうなるか。1人が使っている間、もう1人は待つしかない。プログラムでも同じことが起きる。データベース接続、ファイル、あるいはメモリ上のデータ構造。複数の処理が同時に1つのリソースにアクセスしようとすると、混乱が起きる。だから、「Mutex（ミューテックス）」という仕組みで順番を管理する。1つの処理がMutexを「ロック」したら、他の処理はロックが解除されるまで待たなければならない。use std::sync::Arc;use tokio::sync::Mutex;struct Kitchen {    stove: Arc<Mutex<Stove>>,  // コンロは1つしかない（Mutexで保護）}async fn cook_two_dishes(kitchen: &Kitchen) {    let stove = kitchen.stove.clone();    // 2つの料理を「並行して」作ろうとする    let dish1 = tokio::spawn({        let stove = stove.clone();        async move {            let mut s = stove.lock().await;  // コンロを確保            cook_on_stove(&mut s).await;     // 10分かかる        }    });    let dish2 = tokio::spawn({        let stove = stove.clone();        async move {            let mut s = stove.lock().await;  // コンロを確保しようとする            cook_on_stove(&mut s).await;     // ...が、dish1が終わるまで待つ        }    });    let _ = tokio::join!(dish1, dish2);}私は「並行して料理を作る」と思っていた。2つの料理を同時に調理して、時間を半分にできるはずだと。でも、コンロは1つしかない。片方がコンロを占有している間、もう片方は待っていた。実行してみると、こうなる。[メインコンロ] パスタ の調理を開始[メインコンロ] パスタ の調理が完了[メインコンロ] ソース の調理を開始[メインコンロ] ソース の調理が完了合計調理時間: 4.00秒各料理2秒なら、並行処理で2秒のはずだった。でも、4秒かかった。並行処理の意味がなかった。なぜこうなるのか。現実の料理で考えてみよう。実際のキッチンでは、Aさんがコンロの左側でパスタを茹でている間、Bさんが右側でソースを温められる。コンロには複数の口がある。だから、2人が同時に調理できる。でも、私のコードでは、コンロを「1つのもの」としてMutexで保護していた。「コンロ全体」をロックしていた。だから、1人がコンロを使っている間、もう1人はコンロの前で待つしかなかった。これが「排他制御」の現実だ。Mutexで保護された共有リソースは、一度に1つのタスクしかアクセスできない。「料理を並行して作る」という類推には、この排他制御の概念が含まれていなかった。私の頭の中のキッチンには、コンロの口がいくつもあった。でも、コードの中のキッチンには、コンロが1つしかなかった。より厄介な問題もあった。「デッドロック」だ。デッドロックとは何か。日常の例で説明しよう。AさんとBさんが、食事をしようとしている。テーブルには、ナイフとフォークが1本ずつしかない。食事をするには、両方が必要だ。Aさんは先にナイフを取った。Bさんは先にフォークを取った。Aさんは思う。「フォークがほしい。Bさんが手放すまで待とう」。Bさんも思う。「ナイフがほしい。Aさんが手放すまで待とう」。どちらも、自分が持っているものを手放さない。どちらも、相手が手放すのを待っている。永遠に。これがデッドロックだ。async fn prepare_meal(kitchen: &Kitchen) {    // タスク1: まずコンロを確保、次にオーブンを確保    let task1 = async {        let _stove = kitchen.stove.lock().await;        tokio::time::sleep(Duration::from_millis(10)).await;        let _oven = kitchen.oven.lock().await;  // オーブンを待つ        // ...    };    // タスク2: まずオーブンを確保、次にコンロを確保    let task2 = async {        let _oven = kitchen.oven.lock().await;        tokio::time::sleep(Duration::from_millis(10)).await;        let _stove = kitchen.stove.lock().await;  // コンロを待つ        // ...    };    tokio::join!(task1, task2);  // 永遠に終わらない}タスク1がコンロを持ってオーブンを待ち、タスク2がオーブンを持ってコンロを待つ。お互いが相手を待ち続けて、永遠に進まない。実行してみると、こうなる。[タスク1] コンロを確保しました！[タスク2] オーブンを確保しました！[タスク1] オーブンを確保しようとしています...[タスク2] コンロを確保しようとしています...⚠️  タイムアウト！デッドロックが発生しました。現実の料理では、こんなことは起きない。「ちょっとナイフ貸して」と声をかければ済む。あるいは、「先にフォーク使っていいよ」と譲り合える。人間には、コミュニケーションがある。でも、コンピュータのスレッドは声をかけない。ロックを取得したら、自分の処理が終わるまで手放さない。相手が待っていることすら知らない。だから、永遠に待ち続ける。この問題のデバッグに、私は丸一日を費やした。プログラムが動かない。エラーも出ない。ただ、止まっている。「なぜプログラムが止まるのかわからない」と頭を抱えた。料理の類推では、デッドロックという概念自体が存在しなかったからだ。キッチンで誰かと道具の取り合いになっても、最終的にはどちらかが譲る。でも、プログラムは譲らない。また同じ失敗をしている。私は少し落ち込んだ。でも、まだ終わりではなかった。データベースのトランザクションでも、同じ失敗をした。（注：ここでも技術的な話が続きます。「銀行振込の類推で考えたが、実際のシステムはもっと複雑だった」という話として読んでいただければ大丈夫です。）まず、トランザクションとは何かを説明しよう。日常生活で例えてみる。あなたがコンビニでおにぎりを買うとする。この「買い物」という行為は、2つのことが同時に起きなければ成立しない。「あなたがお金を払う」と「店があなたにおにぎりを渡す」。お金だけ払っておにぎりがもらえなかったら困る。おにぎりだけもらってお金を払わなかったら、それは万引きだ。両方が成功するか、両方が起きないか。どちらかでなければならない。データベースでも同じだ。銀行の振込を考えよう。Aさんの口座から1万円を引いて、Bさんの口座に1万円を足す。この2つの操作は、両方成功するか、両方失敗するか、どちらかでなければならない。Aさんから引いたのにBさんに足されなかったら、1万円が消えてしまう。このような「ひとまとまりの操作」を保証する仕組みがトランザクションだ。途中で失敗したら、最初の状態に戻す（ロールバック）。すべて成功したら、確定する（コミット）。私は類推を作り上げた。「トランザクションは、銀行の振込みたいなものか」。この類推で、データベースの基本的な特性は理解できた。BEGIN TRANSACTION;UPDATE accounts SET balance = balance - 10000 WHERE user_id = 'A';UPDATE accounts SET balance = balance + 10000 WHERE user_id = 'B';COMMIT;問題は、トランザクションが失敗したときの処理を書いたときだった。async fn transfer_money(    pool: &PgPool,    from: &str,    to: &str,    amount: i64,) -> Result<(), Error> {    let mut tx = pool.begin().await?;    // 送金元の残高を減らす    sqlx::query("UPDATE accounts SET balance = balance - $1 WHERE user_id = $2")        .bind(amount)        .bind(from)        .execute(&mut *tx)        .await?;    // 外部APIを呼び出して送金通知を送る（これが問題）    notify_transfer(from, to, amount).await?;    // 送金先の残高を増やす    sqlx::query("UPDATE accounts SET balance = balance + $1 WHERE user_id = $2")        .bind(amount)        .bind(to)        .execute(&mut *tx)        .await?;    tx.commit().await?;    Ok(())}外部APIの呼び出しが失敗したら、トランザクションはロールバックされる。データベースの状態は元に戻る。完璧だと思った。でも、ある日、こんなシナリオを考えた。外部APIの呼び出しが成功した後、2番目のUPDATE文が失敗したらどうなるか。順番を追ってみよう。まず、送金元の残高を減らす。成功。次に、送金通知を送る。成功。通知は、もう相手に届いている。最後に、送金先の残高を増やす。ここで失敗。アカウントが凍結されていた。トランザクションはロールバックされる。データベースの残高は元に戻る。でも、通知は？もう送ってしまった。取り消せない。実際にPostgreSQLで検証してみた。--- シナリオ: 外部API成功後にDB更新が失敗 ---（Alice → frozen_account: 5,000円 - 受取人アカウント凍結で失敗）  → 通知を送信しました（外部API呼び出し）送金失敗: 受取人のアカウントが凍結されています残高:  alice: Alice (90000円)  ← 変わっていない  bob: Bob (60000円)送金通知: 2件  alice → bob: 10000円  alice → frozen_account: 5000円  ← 通知は送信された！トランザクションはロールバックされる。データベースの残高は元に戻る。でも、送金通知はすでに送られている。「5,000円送金しました」という通知が届いているのに、実際には送金されていない。銀行の振込では、こんなことは起きない。なぜか。銀行では、振込処理と通知は同じシステムの中で一貫して管理されている。「お金を動かす」と「通知を送る」が、一体の操作として設計されている。でも、私が書いたコードはそうではなかった。データベースと、通知を送るサービスは、別々のシステムだった。データベースのトランザクションは、データベースの中だけを巻き戻せる。外部サービスへの呼び出しは、トランザクションの外にある。ロールバックしても、すでに送った通知は取り消せない。これが「分散システム」の難しさだ。複数のシステムにまたがる操作を、一貫して管理することは、想像以上に難しい。より厄介な問題もあった。ロールバック自体が失敗することがあるのだ。async fn complex_operation(pool: &PgPool) -> Result<(), Error> {    let mut tx = pool.begin().await?;    // 複数のテーブルを更新    update_table_a(&mut tx).await?;    update_table_b(&mut tx).await?;    update_table_c(&mut tx).await?;  // ここで失敗    tx.commit().await?;    Ok(())}// update_table_c()が失敗すると、txはドロップされてロールバックされる// ...はずだが、ネットワーク障害でロールバックも失敗したら？銀行の振込では、「振込を取り消す」という操作は確実に成功する。窓口で「やっぱりやめます」と言えば、それで終わりだ。でも、コンピュータの世界では、ロールバック自体がネットワーク障害やデータベースクラッシュで失敗することがある。「元に戻す」という操作が、途中で止まる。そうなると、データは中途半端な状態で残る。Aさんから引かれたのに、Bさんには足されていない。1万円が宙に浮いている。この問題に気づいたのは、本番環境で実際に起きてからだった。ユーザーからの問い合わせで発覚した。「送金したのにお金が届いていない」。調べてみると、ネットワーク障害でロールバックが完了していなかった。「銀行の振込みたいなもの」という類推が、分散システムの複雑さを覆い隠していた。銀行の振込は、何十年もかけて作り上げられた堅牢なシステムの上で動いている。私のコードは、そうではなかった。いつになったら学習するのだろう。私は自分に問いかけた。でも、失敗はまだ続いた。キャッシュでも、同じパターンだった。（注：最後の技術的な事例です。「辞書を手元に置いておく類推で考えたが、実際はもっとややこしかった」という話です。）まず、キャッシュとは何かを説明しよう。日常生活で考えてみる。あなたは仕事中、よく使うファイルをどこに置いているだろうか。毎回、会社の書庫まで取りに行くだろうか。たぶん、よく使うファイルは自分の机の上に置いているはずだ。すぐ手に取れるから。これがキャッシュの発想だ。プログラムの世界でも同じだ。データベースからデータを取得するのは、時間がかかる。ネットワーク越しに問い合わせて、データベースが検索して、結果を返す。毎回これをやると遅い。だから、一度取得したデータを「手元」に保存しておいて、次からはそれを使う。これがキャッシュだ。私は類推を作り上げた。「キャッシュは、よく使うものを手元に置いておくことか」。辞書を引くとき、毎回本棚まで行くのは面倒だ。よく使う辞書は、机の上に置いておく。机の上にあれば、すぐに引ける。この類推で、キャッシュの基本は理解できた。use std::collections::HashMap;use std::sync::RwLock;struct UserCache {    cache: RwLock<HashMap<UserId, User>>,}impl UserCache {    async fn get_user(&self, id: UserId, db: &Database) -> User {        // まずキャッシュを確認        if let Some(user) = self.cache.read().unwrap().get(&id) {            return user.clone();        }        // なければDBから取得        let user = db.fetch_user(id).await;        // キャッシュに保存        self.cache.write().unwrap().insert(id, user.clone());        user    }}問題は、データが更新されたときだった。async fn update_user_email(    cache: &UserCache,    db: &Database,    id: UserId,    new_email: String,) -> Result<(), Error> {    // DBを更新    db.update_email(id, &new_email).await?;    // キャッシュを無効化    cache.cache.write().unwrap().remove(&id);    Ok(())}これで十分だと思った。データを更新したら、キャッシュから削除する。次にアクセスしたときは、DBから最新のデータを取得する。シンプルで、正しいはずだった。でも、ある問題が起きた。「競合状態（race condition）」だ。競合状態とは何か。例え話で説明しよう。あなたと同僚が、同時に同じ辞書を使おうとしている。あなたは辞書で「apple」を調べている。その間に、同僚が辞書の「apple」の項目に付箋を貼った。あなたが辞書を閉じて、もう一度開くと、付箋が貼ってある。これは問題ない。でも、こういうケースはどうか。あなたが辞書の「apple」のページをコピーしている間に、同僚が辞書の「apple」の項目を書き換えた。そして、あなたがコピーを終えて、そのコピーを棚にしまった。棚にあるのは、古い情報のコピーだ。これが競合状態だ。複数の処理が同時に動いているとき、その「順番」によって結果が変わってしまう。どの処理が先に終わるかは、そのときの負荷やネットワーク状況で変わる。だから、結果が予測できない。時刻T1: リクエストAがget_user()を呼ぶ時刻T1: リクエストAがキャッシュを確認 → ない時刻T2: リクエストAがDBからuser(email="old@example.com")を取得時刻T3: リクエストBがupdate_user_email()を呼ぶ時刻T3: リクエストBがDBを更新(email="new@example.com")時刻T4: リクエストBがキャッシュを削除時刻T5: リクエストAがキャッシュに古いデータを保存(email="old@example.com")何が起きたのか、順番に見てみよう。リクエストAは、DBから古いデータを取得した。でも、キャッシュに保存する前に、一瞬待たされた。CPUが他の処理をしていたのかもしれない。ネットワークが混んでいたのかもしれない。その隙に、リクエストBがやってきた。リクエストBは、DBのデータを更新した。そして、キャッシュを削除した。「これで、次にアクセスしたときは最新のデータが取得される」と。でも、リクエストAはまだ終わっていなかった。リクエストAは、さっき取得した古いデータを、キャッシュに保存した。リクエストBが削除した後のキャッシュに。結果、キャッシュには古いデータが入った。DBには新しいデータがある。キャッシュとDBで、データが食い違っている。実行してみると、こうなる。[T1] リクエストA: get_user()開始  [キャッシュ] ミス[T2] リクエストA: DBから取得中...  [DB] 取得完了: email="old@example.com"[T3] リクエストB: update_user_email()開始  [DB] メール更新: old@example.com -> new@example.com[T4] リクエストB: キャッシュ無効化[T5] リクエストA: キャッシュに保存  [キャッシュ] 保存: email="old@example.com" ← 古いデータ！--- 結果確認 ---DBの値:        email=Some("new@example.com")キャッシュの値: email=Some("old@example.com")⚠️  キャッシュに古いデータが残っている！結果、キャッシュには古いデータが残り続ける。現実世界の辞書では、こんなことは起きない。なぜか。辞書の内容は、めったに変わらない。そして、辞書を使うのは通常1人だ。複数人が同時に同じ辞書を書き換えながら参照することは、まずない。でも、コンピュータのデータは違う。複数のプロセスが、同時に、同じデータを読み書きする。しかも、ネットワーク遅延やCPUスケジューリングで、処理の順序が予測できない。「Aが先に終わるはず」と思っても、実際にはBが先に終わることがある。この問題をデバッグするのに、3日かかった。「たまにデータが古いままになる」という報告を受けて、最初はDBの問題だと思った。DBを調べた。問題なかった。次にキャッシュの設定を調べた。問題なかった。ログを細かく分析して、やっと気づいた。タイミングの問題だった。特定の順番で処理が実行されたときだけ、問題が起きていた。「手元に置いておく」という類推は、キャッシュの無効化タイミングの複雑さを完全に見落としていた。机の上の辞書は、勝手に内容が変わらない。でも、キャッシュの中のデータは、いつ古くなるかわからない。Phil Karltonの有名な言葉がある。「コンピュータサイエンスで難しいことは2つしかない。キャッシュの無効化と、名前付けだ」。この言葉の意味を、私は身をもって理解した。どれも、類推としては間違っていない。でも、類推が示す以上のことを、私は類推から読み取ってしまっていた。類推は、理解を助ける。しかし、誤解も生む。類推は、新しい視点を与える。一方で、本質を見えなくもする。類推は、創造の源泉だ。同時に、思考停止の入り口でもある。これらの経験以来、私は類推について考え続けてきた。エンジニアとして、類推をどう使い分けるべきか。いつ類推すべきで、いつ類推を断つべきか。類推の力を活かしながら、その罠に落ちないためには、何が必要なのか。そして、もう1つ気づいたことがある。類推は、単なる思考ツールではない。それは、人間の知能の根幹だ。 われわれは、あまりにも無意識に類推的な考え方をしながら日々を過ごしている。だからこそ、類推の限界を知ることが、これほど重要なのだ。これは、類推に救われてきた人間が、類推に何度も裏切られた話だ。そして、それでもなお類推を手放せない人間が、類推とどう向き合うかを考えた記録だ。類推とは何かまず、類推とは何かを明確にしておきたい。類推（アナロジー）とは、2つの異なる領域の間に構造的な類似性を見出し、一方の知識を他方に適用する思考法だ。AとBは表面的には違うが、その関係性の構造は似ている。だから、Aで学んだことを、Bに応用できる。私は、類推こそが人間の思考の根幹だと考えている。論理的思考も、批判的思考も、創造的思考も、よく見ると類推が基盤にある。われわれは類推なしには、新しいことを考えることすらできない。ソフトウェアエンジニアなんて、類推だらけだ。コードを読んでいると、「あ、これ、あのコードと同じ構造だな」と気づく。設計を考えていると、「前のプロジェクトのあのパターンが使えそうだ」と気づく。バグを追っていると、「この挙動、前にも見たことがある」とピンとくる。私たちは、毎日、無意識に類推している。自分でも気づかないうちに。プログラミングを学ぶとき、類推を使っている。「変数は、ラベル付きの箱みたいなものだ」と教わる。値を入れて、取り出す。この類推があるから、抽象的な概念を具体的にイメージできる。新しいデータベースを学ぶとき、類推を使っている。「PostgreSQLのMVCCは、MySQLのInnoDBと似ているか」と考える。この類推があるから、ゼロから学ぶより速く理解できる。新しい言語を学ぶとき、類推を使っている。「Rustのtraitは、Goのinterfaceみたいなものか」と考える。完全に同じではないが、入り口にはなる。われわれの頭の中では、常に類推が働いている。既知の世界での関係づけから、未知の関係づけを推論している。物語を読むときも、私たちは類推している。登場人物の経験を自分の人生に重ね、フィクションの世界から現実への教訓を引き出す。主人公が困難を乗り越える姿を見て、自分の状況に当てはめる。異なる時代や文化を舞台にした物語から、普遍的な人間の営みを感じ取る。共感とは、つまり類推だ。「この人の気持ちは、あのときの自分の気持ちに似ている」。そう感じるから、私たちは物語に心を動かされる。類推がなければ、われわれは毎回ゼロから学ばなければならない。新しいフレームワークに出会うたび、過去の経験が役に立たない。累積的な学習ができない。技術も発展しない。だから、類推は人間の知能の基盤であり、思考の源泉だ。 これは疑いようがない。ここまで書いてきて、ふと気づいたことがある。私は今、類推について説明するために、言葉を使っている。では、言葉を使うとは、どういうことだろうか。目の前に、一冊の本がある。私はそれを見て、「本」と呼ぶ。でも、この「本」という言葉は、どこから来たのか。私がこれまでの人生で見てきた、無数の本。図書館で借りた本、書店で買った本、友人にもらった本。それらに共通する何かを抽出して、「本」というカテゴリを作った。目の前の物体を「本」と呼ぶとき、私はそれを、過去に見てきた本たちと「同じ仲間」だと判断している。これは、類推ではないか。「この物体は、私が知っている『本』に似ている。だから、これも『本』だ」。言葉を使うとは、目の前の具体的な現象を、過去に学んだカテゴリに当てはめることだ。当てはめるためには、類似性を見出さなければならない。つまり、言語化そのものが、類推なのだ。そう考えると、言葉の限界も見えてくる。目の前の本には、固有の特徴がある。紙の質感。インクの匂い。背表紙についた小さな傷。誰かが残した付箋。でも、「本」という言葉は、それらを捉えない。「本」という言葉が指すのは、無数の本に共通する抽象的な特徴だけだ。言葉にした瞬間、具体的な豊かさは零れ落ちる。だから、現状のすべてを完璧に表す言葉は、存在しない。 どんなに言葉を尽くしても、現実には追いつかない。言葉は常に近似だ。現実の一部を切り取っているだけだ。新しい経験をしたとき、私たちは「これは何だろう」と考える。既存の語彙の中から、「これに近い」言葉を探す。ぴったりの言葉が見つからなければ、複数の言葉を組み合わせる。それでも足りなければ、比喩を使う。「○○みたいなもの」と。でも、どれだけ工夫しても、言葉は現実を完全には捉えられない。類推は「AはBに似ている」という認識だ。言語化は「この現象は『X』という言葉に似ている」という認識だ。構造は同じだ。どちらも、目の前のものを、既知のものに当てはめる。そして、当てはめることで、何かを得る代わりに、何かを失う。私たちは、類推なしには思考できない。言葉なしには思考を伝えられない。でも、類推も言葉も、現実を完全には捉えられない。この記事を書いている今この瞬間も、私は類推と言葉の限界の中にいる。その限界を知りながら、それでも書くしかない。だからこそ、類推の限界を知ることが、これほど重要なのだ。しかし、だからこそ危険なのだ。類推はなぜ強力なのか類推の力を、もう少し詳しく見てみよう。抽象と具体の往復運動抽象的な概念は、そのままでは理解しにくい。人間の脳は、具体的なイメージを好む。抽象的な数学の公式より、具体的な例題の方が理解しやすい。抽象的な設計原則より、具体的なコード例の方が頭に入る。類推は、この抽象と具体を往復する運動だ。日常の例で説明しよう。カレーを作れる人は、シチューも作れる。なぜか。カレーとシチューは、表面的には違う料理だ。でも、「材料を切る → 炒める → 水を入れて煮る → ルーを溶かす」という構造は同じだ。カレーを作った経験から、この「構造」を抽出できれば、シチューに応用できる。これが抽象化であり、類推だ。プログラミングでも同じだ。具体的なもの（MySQL）を見て、抽象化（データを永続化するシステム）し、別の具体（PostgreSQL）に適用する。この往復が、類推の本質だ。ここで重要なのは、「抽象化」という能力だ。私の理解では、抽象化とは枝葉を切り捨てて幹を見ることだ。個別の事象から、本質的な構造だけを取り出す。MySQL、PostgreSQL、SQLiteはいずれも「SQLでデータを操作するシステム」という抽象に還元できる。Actix-web、Axum、Rocketはいずれも「HTTPリクエストを処理するRustのWebフレームワーク」という抽象に還元できる。この抽象化ができなければ、類推はできない。類推とは、2つの具体的な事象の間に共通の構造を見出すことだ。共通の構造を見出すには、まず具体から構造を抽出しなければならない。それが抽象化だ。私がこれまで見てきた限り、類推がうまい人は例外なく抽象化がうまい。 正しく抽象化できなければ、正しく類推できない。面白いことに、抽象の世界が見えている人には具体の世界も見える。でも、具体しか見えない人には抽象の世界が見えない。私はこれをマジックミラーのようなものだと思っている。抽象側からは両方見えるが、具体側からは向こう側が見えない。抽象を理解している人は、具体がその抽象の一例であることがわかる。「あ、これは○○の具体例だな」と。一方、具体しか見えない人は、それが何かの一例だとは気づかない。ただ、個別の事象として見るだけだ。だから、別の具体との共通点が見えない。多くの人は、この具体と抽象の往復運動を意識したことすらない。私自身、エンジニアになって何年も経ってから、やっと意識できるようになった。それまでは、類推を「なんとなく」やっていた。うまくいくこともあれば、失敗することもあった。でも、なぜ失敗するのかがわからなかった。抽象化を意識するようになってから、類推の成功率が上がった。「依存性の注入（DI）とは何か」。これはプログラムの設計手法の一つで、名前だけ聞くと難しそうに感じる。これを抽象的に説明すると、「オブジェクトが必要とする依存関係を外部から注入することで、結合度を下げてテスタビリティを高める設計パターン」となる。正確だが、初学者には意味不明だ。でも、「コンセントみたいなものだよ」と言えば、少し見えてくる。家電製品は、壁のコンセントに何が繋がっているか知らなくても動く。発電所が火力でも原子力でも太陽光でも、同じコンセントから電気が来る。DIも同じで、クラスは「何か」からデータベース接続を受け取るが、それが本番のMySQLなのかテスト用のモックなのかは知らなくていい。外部から「注入」される。類推によって、抽象が具体になる。見えなかったものが、見えるようになる。未知への橋渡し人間は、完全に未知のものを理解できない。新しい概念を学ぶとき、われわれは常に既知のものと関連づける。「これは、あれに似ている」。この関連づけがなければ、新しい知識は宙に浮いてしまう。既存の知識ネットワークに接続できない。類推は、未知と既知をつなぐ橋だ。Kubernetesを初めて学ぶとする。Kubernetesとは、たくさんのアプリケーションを複数のサーバーで効率よく動かすための管理システムだ。まったく新しい概念だ。でも、「Kubernetesは、コンテナのオーケストラ指揮者みたいなものだ。各コンテナ（アプリケーションを動かす小さな箱）がどこで動くべきか、いくつ動かすべきか、死んだら再起動すべきかを指示する」という類推があれば、入り口が見える。もちろん、この類推は不完全だ。Kubernetesの本質——宣言的な状態管理、コントロールループ、リコンシリエーション——を完全には捉えていない。でも、入り口にはなる。そこから、より正確な理解に進むことができる。類推は、足場だ。 建設現場の足場のように、本体を作るための仮の構造物だ。足場がなければ、高い建物は建てられない。類推がなければ、深い理解には到達できない。遠くから借りてくる力類推は、新しいアイデアを生む。異なる領域を結びつけることで、どちらの領域にも存在しなかった新しい視点が生まれる。ここで重要なのは、「どこから借りてくるか」だ。興味深いのは、同じ業界から持ってくるとパクりと言われるのに、違う業界からなら革命になることだ。なぜか。同じ業界の人は、同じものを見ている。だから、借りてきたことがすぐにバレる。でも、違う業界から借りてくると、誰も気づかない。そもそも、その業界を知らないからだ。他人が気づかないような遠くから借りてくる。そのために必要なのが、抽象化の力だ。遠い領域同士をつなげるには、それぞれの領域から本質的な構造を抽出しなければならない。表面的な違いを超えて、構造の類似を見抜く。これができる人だけが、革命を起こせる。生物の進化から、遺伝的アルゴリズムが生まれた。「自然選択と突然変異のプロセスを、最適化問題に適用したらどうだろう」。この類推が、新しい計算手法を生んだ。神経細胞のネットワークから、ニューラルネットワークが生まれた。「脳の情報処理を、コンピュータで模倣したらどうだろう」。この類推が、現在のAI革命の基盤を作った。私は、類推を創造の触媒だと思っている。異なる領域の知識を化学反応させて、新しいものを生む。遠くから借りてくるほど、その化学反応は激しくなる。近い領域から借りてくると、小さな改善にしかならない。遠い領域から借りてくると、パラダイムシフトが起きる。コミュニケーションの潤滑油類推は、相手にとって未知の概念を、既知の概念で説明することを可能にする。エンジニア同士でも、専門領域が違えば類推は有効だ。フロントエンドエンジニアにバックエンドの認証を説明するとき、JWT（JSON Web Token、ユーザーの認証情報を暗号化して持ち運ぶ仕組み）の説明をする機会がある。「JWTは、入場チケットみたいなものだよ」と言えば伝わる。一度発行されたら、チケット自体に情報が書いてある。だから毎回本部に問い合わせなくても、チケットを見せるだけで入れる。データベースのインデックス（データを高速に検索するための目次）を説明するときも同じだ。「本の索引みたいなものだよ。全ページをめくらなくても、索引を見れば目的の単語がどこにあるかすぐわかる」。チーム内でも類推は重要だ。リファクタリングとは、プログラムの動作を変えずに、コードの構造を整理・改善することだ。「このリファクタリングは、引っ越しみたいなものだ。荷物を新しい場所に移して、古い場所を片付ける。移行期間中は、両方にアクセスできるようにしておく」。こう言えば、作業のイメージが共有できる。類推は、異なる背景を持つ人々の間で、共通の理解を作る。類推はなぜ危険なのかここまで読むと、類推は素晴らしいものに思える。実際、素晴らしいのだ。でも、同時に危険でもある。なぜか。類推は「AとBは似ている」という前提に立っている。でも、この前提が正しいとは限らない。 似ているように見えて、実は違う。その違いが、致命的な判断ミスを生む。これは、ベストプラクティスが常に機能しないのと同じ構造だ。カンファレンスやブログで見たあの手法、あの技術、あの設計。「あの会社でうまくいったから、うちでもうまくいくはずだ」。こう考える。でも、これは類推だ。あの会社の文脈と、あなたの文脈は違う。あのチームと、あなたのチームは違う。ベストプラクティスが「ベスト」なのは、特定の文脈においてだけだ。 文脈が変われば、ベストではなくなる。デザインパターン（プログラム設計でよく使われる定番の解決策のカタログ）も同じだ。「このケースにはあのパターンが使える」と考える。でも、そのパターンが生まれた文脈と、今の文脈は違う。パターンを適用すれば解決するわけではない。パターンは出発点であって、答えではない。私が「何回説明しても伝わらない」と感じるとき、原因の多くは類推にある。類推は理解のショートカットとして強力だ。でも、相手と自分の「当たり前」が違うと、誤解を生む。なぜなら、類推は相手の頭の中にある既存の枠組みに接続するからだ。その枠組みが私と違えば、同じ言葉でも違う意味になる。冒頭の所有権の話を思い出してほしい。私は所有権を「本の貸し借りみたいなもの」と理解した。でも、「貸し借り」という言葉には、私が意識していなかった意味も含まれていた。「貸した相手との関係が続く」という意味だ。私は無意識にその意味も読み取っていた。だから、所有権を渡した後も「貸した先」を追跡できると思い込んでいた。類推が、私の思考を歪めていた。表面的類似と構造的類似の混同では、なぜ類推は失敗するのか。多くの場合、表面的な類似と構造的な類似を混同しているからだ。表面的な類似とは、見た目や印象の類似だ。「両方とも丸い」「両方とも赤い」「両方とも動く」。これは、誰でもすぐに気づく。構造的な類似とは、関係性のパターンの類似だ。「Aの中でXとYがこういう関係にあるのと同じように、Bの中でPとQもこういう関係にある」。これは、注意深く見ないと気づかない。類推が成立するためには、構造的な類似が必要だ。表面的な類似だけでは足りない。 問題は、人間が表面的な類似に騙されやすいことだ。見た目が似ていると、構造も似ていると思い込んでしまう。あるチームの話を聞いた。少し用語を説明しておこう。「モノリス」とは、1つの大きなプログラムとして構築されたシステムだ。「マイクロサービス」とは、機能ごとに小さなプログラムに分割し、それらを連携させるアーキテクチャだ。大企業が採用して成功したことで有名になった。そのチームは「マイクロサービスが成功しているから」という理由で、モノリスをマイクロサービスに分割しようとした。「あの有名企業がうまくいったんだから、うちもうまくいくはずだ」。表面的には似ている。「複雑なシステムを小さなサービスに分割する」という点で。しかし、構造は根本的に異なる。その有名企業には数千人のエンジニアがいる。専門のプラットフォームチームがいる。成熟した監視基盤がある。一方、そのチームは10人だった。運用の負荷が爆発的に増え、サービス間の通信障害のデバッグに追われ、結局モノリスに戻すことになった。彼らは、表面的な類似に騙されて、1年を失った。類推が思考を固定する類推には、もう1つ危険がある。思考を固定してしまうことだ。類推は、新しい視点を与える。「これはAみたいなものだ」と気づくと、Aの知識が使えるようになる。これは便利だ。でも同時に、Aの枠組みで考えるようになる。Aの論理で判断するようになる。Aで成立したことは、ここでも成立すると期待するようになる。ここに罠がある。BはAではない。Aにはない特性が、Bにはある。Bにはない特性が、Aにはある。類推によってAの枠組みを持ち込むと、Bの固有性が見えなくなる。Aとの共通点ばかりに目が行き、Aとの違いを見落とす。私はかつて、新しいチームのマネジメントで失敗した。前のチームで成功した方法を、そのまま適用しようとした。「前のチームと同じようにやればいい」と類推した。でも、チームが違えば、人が違う。カルチャーが違う。技術スタックが違う。ビジネスの文脈が違う。前のチームでうまくいった方法が、新しいチームでは逆効果だった。類推によって、私は新しいチームの固有性を見落としていた。「前のチームみたい」という枠組みが、目の前のチームを正確に見ることを妨げていた。これは、私だけの話ではない。世の中の「二番煎じ」は、すべてこの構造だ。表面的な成功パターンを真似る。でも、本質的な差異を見落としている。だから、同じ結果が得られない。独自性がないのではない。観察が浅いだけだ。類推が、観察を浅くしている。 成功事例を見て「うちも同じようにやろう」と考えるとき、私たちは無意識に類推している。でも、その類推が正しいかどうかを検証していない。表面的な類似に飛びついて、構造的な違いを無視している。類推は状況証拠であって物的証拠ではないここまでの話をまとめると、こうなる。類推は仮説であって、証明ではない。類推は、2つの領域の間に構造的な類似があるという仮定に基づいている。「AとBは似ているから、Aで成り立つことはBでも成り立つだろう」。これが類推の論理だ。でも、この仮定は、常に正しいとは限らない。似ているように見えて、実は違う。類推は状況証拠レベルであって、物的証拠レベルには至らない。ある領域で成功した法則が、別の領域でも通用する保証は、どこにもない。成功事例は、その文脈での成功を証明するだけだ。別の文脈での成功は、証明されていない。カンファレンスやブログで聞いた、あの会社の組織文化。あの会社でうまくいったからといって、すべての会社で同じ文化がうまくいくわけではない。あの有名な開発手法が成功したからといって、すべてのチームで同じ手法が成功するわけではない。成功事例から学ぶことは重要だ。でも、「あの会社みたいにやればいい」と単純に類推することは、危険だ。あの会社には、あの会社の文脈がある。業界。競合。人材市場。創業者の思想。歴史。規模。成長フェーズ。これらすべてが、あの文化を成立させている。あなたの会社には、あなたの会社の文脈がある。同じ文化を移植しても、機能するとは限らない。むしろ、害になることもある。より危険なのは、まったく新しい概念や技術を既存のものに無理やり当てはめることだ。ブロックチェーン（暗号技術を使って取引記録を改ざん困難な形で保存する技術）を「分散データベースみたいなもの」と類推すると、その本質的な違いを見落とす。信頼モデル（誰を信頼するか）、コンセンサスメカニズム（参加者間でどうやって合意を取るか）、イミュータビリティ（一度記録したら変更できないこと）——これらの特徴が、通常のデータベースとは根本的に異なる。結果的に間違った理解や過小評価につながる。類推を絶対視してはいけない。類推は仮説であって、証明ではない。類推がもたらす知的興奮ここまで、類推の危険性について書いてきた。でも、誤解しないでほしい。類推は危険だからといって、避けるべきものではない。類推には、代えがたい価値がある。類推は楽しい。類推は気持ちいい。 私は、類推が成功した瞬間の快感を、何度も味わってきた。まったく別のことに当てはまった時、頭の中で何かがつながる。あの瞬間——「あ、これって、あれと同じ構造だ」と気づく瞬間——には、独特の快感がある。世界の見え方がガラリと変わる。さっきまでバラバラだったものが、1つの構造で説明できるようになる。混沌が秩序になる。複雑が単純になる。なぜ、これが気持ちいいのか。人間は、わからないことに不安を感じる。新しい状況。未知の概念。複雑な問題。これらは、ストレスだ。脳は「これは何だ？」「どうすればいい？」と警戒モードに入る。でも、類推によって「あ、これは前に見たあれと同じだ」と気づくと、状況が一変する。未知が既知になる。複雑が単純になる。警戒モードが解除される。その瞬間、安堵とともに、快感が走る。これは、たぶん生存本能と関係している。予測できないものは危険だ。草むらで何かが動いた。あれは風か、それとも獲物か、それとも敵か。わからないと、逃げるべきか近づくべきか判断できない。でも、「あれは風だ」とわかれば、安心できる。予測できるものは安全だ。類推によって「これは、あれと同じだ」とわかると、予測ができるようになる。「あれ」のときはこうなった。だから、「これ」もそうなるだろう。予測ができると、安心する。安心は快感だ。しかも、類推は「遠くから借りてくる」ほど快感が大きい。 近い領域の類推——「MySQLはPostgreSQLに似ている」——は、驚きが少ない。当たり前だからだ。でも、遠い領域の類推——「ソフトウェアのリファクタリングは、文章の推敲と同じ構造だ」——は、発見の喜びが大きい。予想外のつながりだからだ。予想外であるほど、「わかった」瞬間のギャップが大きい。だから、快感も大きい。私がコードを書いていて、まったく関係ないはずの日常の出来事が当てはまることに気づいたとき。障害対応をしていて、これは以前経験した別の問題と同じ構造だと気づいたとき。設計を考えていて、過去に読んだ本の概念が使えると気づいたとき。そのたびに、ゾクっとする。「まさか、ここがつながるとは」という驚き。でも、よく考えると「なるほど、確かに同じだ」と納得できる。この驚きと納得の組み合わせが、最高に気持ちいい。類推の快感は、謎解きの快感に似ている。 バラバラだったピースが、カチッとはまる。見えなかった絵が、見えるようになる。あの瞬間の快感を知っている人は、類推をやめられない。日本では、この類推の喜びは昔から庶民の間で楽しまれていた。「○○と掛けて□□と解く。その心は△△である」という謎かけだ。まったく関係なさそうな2つのものが、ある抽象的な構造で結びつく。その発見の喜びが、笑いになる。漫才のツッコミも、類推と関係がある。ボケは、ある種の「間違った類推」だ。常識から逸脱したことを言う。ツッコミは、その逸脱を指摘する。「いや、それは違うやろ」と。ツッコミが面白いのは、観客が「そうそう、それはおかしいよね」と共感できるからだ。観客の頭の中にある「普通はこうだ」という枠組み——フレームと呼ぼう——に沿って、逸脱を指摘する。だから笑いが起きる。これは、類推の逆操作だ。類推が「AはBみたいなものだ」と結びつけるのに対して、ツッコミは「AはBではない」と切り離す。類推の破綻を、観客のフレームに沿って指摘する。ここで重要なのは、ツッコミが機能するためには、観客のフレームを理解していなければならないということだ。観客が「それはおかしい」と感じるポイントを、正確に捉えなければならない。これは、類推を使うすべての場面に通じる。私が所有権を「本の貸し借りみたいなもの」と理解したとき、私は「貸し借り」というフレームの中で考えていた。そのフレームの中では、貸し借りには「誰に貸したか」という追跡可能な関係が含まれていた。私は、そのフレームが当たり前だと思っていた。フレームの存在自体を意識していなかった。だから、フレームの限界が見えなかった。自分自身に対する「ツッコミ」——「いや、Rustの借用は、現実の貸し借りとは違うやろ」——ができなかった。自分がどんなフレームで類推しているかを意識しなければ、類推の限界が見えない。良い学習者は、類推を使うと同時に、自分自身でツッコミを入れる。「本の貸し借りみたいなものだけど、貸し借りと違って……」と。このツッコミができるかどうかが、類推で成功する人と失敗する人を分ける。創造性は異領域からの借用ソフトウェアエンジニアリングの歴史は、異なる領域からの借用の歴史でもある。Gitの分散型バージョン管理は、中央集権的なSVNの限界を、分散システムの発想で打破した。Git とは、プログラムの変更履歴を記録・管理するツールだ。SVNは「中央のサーバーにすべてを保存する」方式だったが、Gitは「全員が完全な履歴を持つ」方式を採用した。「すべてのリポジトリが対等なピアである」という考え方は、P2Pネットワークの構造と同じだ。Dockerのコンテナ技術は、仮想マシンの重さを、プロセス分離の軽さで置き換えた。Dockerとは、アプリケーションを「コンテナ」という小さな箱に詰めて、どこでも同じように動かせるツールだ。「OSレベルの仮想化ではなく、プロセスレベルの分離で十分ではないか」という発想が、コンテナ革命を起こした。MapReduceは、分散処理の複雑さを、関数型プログラミングの抽象で単純化した。これは大量のデータを複数のコンピュータで並列処理するための手法だ。「mapとreduceという2つの操作に分解すれば、並列処理が簡単になる」。この類推が、ビッグデータ処理の基盤を作った。類推は、新しい価値を生むための道具だ。既存の枠組みを超えるための、ジャンプ台だ。だから、類推を完全に否定できない。エンジニアリングにおける類推の両面性ここまで、類推の力と危険について見てきた。類推は強力だ。でも、危険でもある。では、エンジニアとして、類推をどう扱うべきか。答えは、場面によって使い分けることだ。 類推が有効な場面と、危険な場面がある。それを見極めることが重要だ。類推が有効な場面まず、類推が有効な場面を整理しよう。新しい技術を学ぶとき。 前に学んだ技術との類似点を見つけることで、学習が加速する。「Goのgoroutine（ゴルーチン）は、軽量なスレッドみたいなものか」。スレッドとは、プログラムの中で同時に動く処理の単位だ。goroutineはそれをより少ないメモリで実現する。この類推が、入り口になる。チームメンバーに説明するとき。 相手が知っている概念に置き換えることで、理解を助ける。「このアーキテクチャは、マイクロサービスというより、モジュラーモノリスに近いよ」。問題を発見するとき。 「これは前にやったあのプロジェクトに似ている」と気づくことで、早期に問題を予測できる。パターン認識だ。アイデアを発想するとき。 異なる領域の解決策を、目の前の問題に適用してみる。「他の業界ではどうやっているんだろう」。これらの場面では、類推は強力なツールだ。類推が危険な場面一方で、類推が危険な場面もある。共通点は、「判断」が伴う場面だ。設計判断を下すとき。 「あの有名な会社がこうやっているから」は、判断の根拠にならない。なぜか。あの会社にはあの会社の文脈がある。規模、チーム構成、ビジネス要件、技術的制約——すべてが違う。自分たちの文脈で、自分たちの制約を考慮して、判断しなければならない。パフォーマンス予測をするとき。 「前のプロジェクトではこのくらいのスループットだったから」は、予測の根拠にならない。ハードウェアが違う。データが違う。負荷パターンが違う。実測なしに類推で判断すると、本番環境で痛い目に遭う。チーム運営をするとき。 「前のチームではうまくいったから」は、根拠にならない。人が違う。状況が違う。目の前のチームを、目の前のチームとして見なければならない。ビジネス判断をするとき。 「あの会社がこうやって成功したから」は、根拠にならない。市場が違う。タイミングが違う。リソースが違う。「マイクロサービスが流行っているから、うちもマイクロサービスにしよう」。これも類推だ。でも、マイクロサービスが成功した会社と、あなたの会社は違う。チームの規模が違う。運用能力が違う。ビジネスの複雑さが違う。流行りのアーキテクチャは、流行っている理由があるが、あなたの問題を解決する保証はない。「TDD（テスト駆動開発：テストを先に書いてから本体コードを書く開発手法）がいいらしいから、TDDでやろう」。これも類推だ。TDDが有効だった文脈と、今の文脈は同じか。チームのスキルは。締め切りは。要件の安定度は。手法は、文脈とセットでしか評価できない。これらの場面では、類推に頼らず、具体を見なければならない。具体を見ろここまでの話から、私がたどり着いた結論はシンプルだ。類推は入り口として使う。でも、入ったら、具体を見る。どういうことか。「これはAみたいなものだ」と類推したら、まずはその類推で全体像を掴む。ここまでは類推の力だ。でも、判断を下す前に、次の問いを立てる。「Aとは何が違うんだろう」。違いを具体的に列挙する。その違いが、判断にどう影響するかを考える。つまり、抽象ではなく、具体を見る。パターンではなく、個別を見る。類似ではなく、差異を見る。これは、類推の否定ではない。類推の限界を知った上で、類推を使うということだ。類推は入り口として使い、判断は具体に基づいて行う。入り口と判断を、分離する。 これが、私の結論だ。類推を使い分ける技術「入り口と判断を分離する」と言った。では、具体的にどうすればいいのか。私が実践していることを、いくつか紹介する。類推のレベルを意識するまず、自分がどのレベルで類推しているかを意識することだ。類推には、レベルがある。表面的な類推：見た目や印象の類似。「両方とも丸い」「両方ともウェブサービスだ」。機能的な類推：役割や機能の類似。「両方ともユーザー認証する」「両方ともデータを永続化する」。構造的な類推：関係性のパターンの類似。「Aの中でXとYの関係が、Bの中でPとQの関係と同じだ」。原理的な類推：根底にある原理の類似。「両方とも、この物理法則に従う」「両方とも、この経済原理が働く」。レベルが深いほど、類推は有効だ。表面的な類推は危険だ。原理的な類推は強力だ。類推をするとき、自分がどのレベルで類推しているかを意識する。表面的な類推に気づいたら、警戒する。反例を積極的に探す類推が成立しない場面を、積極的に探す。「これはAみたいだ」と思ったら、「Aとは違う点は何か」を列挙する。「この類推が成立しない条件は何か」を考える。「Aでは成立したが、ここでは成立しないことは何か」を洗い出す。なぜ反例を探すのか。人間は、類推が成立する証拠ばかりを集める傾向がある。心理学では「確証バイアス」と呼ばれる現象だ。自分が信じたいことを裏付ける情報ばかりを無意識に集めてしまう。「似ている」と感じると、似ている点ばかり目につく。違う点は、無意識にスルーしてしまう。だから、意識的に反例を探さなければならない。反例は、自然には目に入ってこない。反例が見つかったら、類推の適用範囲を限定する。「この側面ではAに似ているが、この側面では違う」と認識する。反例を探すことは、類推を否定することではない。類推を精密にすることだ。どこまで使えて、どこから使えないのか。その境界線を引く作業だ。類推と実測を組み合わせる類推は仮説だ。仮説は検証しなければならない。私は何度も、類推を信じて痛い目を見てきた。「分かった」と思った瞬間が、一番危ない。類推は、分からないことを「分かったつもり」にさせてくれる。その確信が、検証を怠らせる。大切なのは、分かっていないことに確信を持たないことだ。類推で「たぶんこうだろう」と思っても、それは仮説でしかない。仮説に確信を持ってはいけない。確信を持った瞬間、検証しなくなる。検証しなければ、間違いに気づけない。だから、私は自分にこう言い聞かせている。類推したら、試せ。作ってみろ。動かしてみろ。「前のプロジェクトと同じくらいのパフォーマンスだろう」と類推したら、実測する。「このアーキテクチャパターンがうまくいくだろう」と類推したら、プロトタイプ（動作確認のための試作品）を作る。「このチーム運営方法が有効だろう」と類推したら、小さく試して観察する。試した結果、類推が外れることがある。むしろ、外れることの方が多い。でも、外れたときこそ、学びがある。なぜ外れたのか。どこが似ていて、どこが違ったのか。その差異を言語化できたとき、理解が一段深まる。私は、このサイクルを速く回すことを意識している。1回の大きな検証より、10回の小さな検証。外れることを恐れない。外れるたびに、類推が精密になっていく。類推は「似ている」という感覚に基づいている。でも、感覚は当てにならない。似ていると思っても、実際には違う。逆に、違うと思っても、実際には同じ。感覚を信じすぎると、現実を見誤る。実測は、感覚を現実に引き戻す。「本当にそうなのか？」を確認する。類推で仮説を立てて、実測で検証する。 類推は仮説生成の道具であって、証明の道具ではない。複数の類推を比較する1つの類推に固執しない。複数の類推を試す。「これはAみたいだ」と思ったら、「でも、Bみたいでもあるな」と考える。「Cという見方もできるな」と広げる。なぜ複数の類推を試すのか。最初に思いついた類推が、最適とは限らない。むしろ、最初の類推は表面的なことが多い。パッと見て似ているから、思いつく。でも、もう少し考えると、別の類推の方が本質を捉えていることがある。1つの類推に決め打ちすると、その視点でしか見えなくなる。複数の類推を並べると、それぞれの限界が見えてくる。そして、どの類推が最も適切かを吟味する。どの類推が、最も多くの側面を説明できるか。どの類推が、最も少ない反例を持つか。どの類推が、最も有用な洞察を与えるか。複数の類推を比較することで、1つの類推に囚われることを防ぐ。類推を言語化する類推を曖昧なまま使わない。明示的に言語化する。「これはAみたいだ」と思ったら、何がどうAに似ているのか、具体的に言葉にする。「Aのこの側面と、ここのこの側面が、この点で類似している」と。なぜ言語化が重要なのか。頭の中にある類推は、たいてい曖昧だ。「なんとなく似ている」という感覚で止まっている。でも、言葉にしようとすると、曖昧さが露呈する。「どこが似ているの？」と聞かれて、答えられない。言語化は、自分の思考を試すテストだ。 言葉にできないなら、実はわかっていない。言葉にできて初めて、本当に理解したと言える。言語化することで、類推が精密になる。曖昧な類推は、誤解を生む。精密な類推は、理解を深める。そして、言語化した類推を、他者に共有する。「私はこう類推しているが、どうだろうか」と問う。他者の視点で、類推の妥当性を検証する。類推力を鍛えるここまで、類推の力と限界について語ってきた。では、類推力を高めるには、どうすればいいのか。私が意識していることを3つ挙げる。1つ目は、遠い領域から引き出しを増やすことだ。私は、咀嚼しやすいものばかり読まないようにしている。数学や哲学、物語やSFなどの自分の仕事や語ることとは遠い世界のストーリーを読んで、自分の経験と照らし合わせる。実生活では役に立たないように見える抽象的な知識こそ、遠くから借りてくる力になる。なぜ遠い領域が大事なのか。近い領域の知識は、みんなが持っている。だから、そこから類推しても、みんなと同じ結論にしかたどり着かない。遠い領域の知識は、自分だけの武器になる。他の人が思いつかない類推ができる。2つ目は、常に「これは何かに使えないか」と考えることだ。映画を見ても、歴史を学んでも、スポーツを観戦しても、「これは自分の仕事にどう活かせるか」と考える。「関係ない」と決めつけず、「何か応用できないか」という視点で世界を見る。これを続けていると、頭の中に「類推のアンテナ」が立つ。普段の生活の中で、ふと「あ、これって、あれと同じだ」と気づくようになる。その瞬間が、類推力が育っている証拠だ。3つ目は、構造を2〜3つに絞って抽象化することだ。私の経験では、特徴や要点を2〜3つ挙げて、同じ構造を持つ事象を探すとうまくいく。1つだと何でも結びつけられてしまう。「両方とも存在する」では、類推にならない。4つ以上だと類推先が近くなりすぎて面白味がない。条件が厳しすぎて、同じ業界の似たようなものしか見つからない。2〜3つが、ちょうどいい。適度に絞られていて、適度に広い。この訓練を続けると、世界の見え方が変わる。一見無関係に見えるものの中に、共通の構造が見えてくる。私は、この感覚を得てから、仕事がずっと面白くなった。ニュースを読んでも、本を読んでも、人と話しても、「これは何かに使えるだろう」と思う。世界が、類推のネタの宝庫に見えてくる。類推を断つ勇気ここまで、類推を使い分ける技術について書いてきた。でも、もっと根本的なことがある。それは、類推を断つ勇気だ。一度つなげた類推を、必要なら断たなければならない。でも、これが難しい。なぜ難しいのか。類推は、理解の構造だ。「これはAみたいなものだ」という認識は、思考の足場になっている。その足場の上に、さらに理解を積み重ねている。足場を外すことは、その上に積み重ねたものも崩れることを意味する。一度「わかった」と思ったものを、「わからない」に戻すのは、心理的に辛い。人間は「わかった」状態を好む。「わからない」状態は不安だ。だから、間違った類推でも、手放したくない。間違っていると薄々気づいていても、「まあ、だいたい合っているだろう」と自分を納得させてしまう。認めたくない。また、類推は、コミュニケーションの基盤にもなる。チームで「これはAみたいなもの」と共有されていると、それを覆すことは、混乱を生む。「え、今までの説明は何だったの？」と言われる。自分の言ったことを訂正するのは、恥ずかしい。間違いを認めるのは、プライドが傷つく。だから、間違っているとわかっても、言い出せない。みんなが使っている類推に異を唱えるのは、勇気がいる。でも、間違った類推に固執し続けることの方が、はるかに有害だ。 間違った類推は、間違った判断を生む。間違った判断は、間違った設計を生む。間違った設計は、技術的負債を生む。技術的負債とは、急いで作った不完全なコードが後から修正コストとして跳ね返ってくることだ。借金のように、放置すればするほど利子が膨らんでいく。技術的負債は、チームを疲弊させる。最初の一歩で間違えると、その後のすべてがズレていく。早く気づいて修正するほど、傷は浅い。だから、類推が間違っていると気づいたら、勇気を持って断つ。「前にAみたいだと言ったけど、よく見たら違った。Bで考え直そう」と言う。これは、弱さではない。強さだ。現実を直視する強さだ。おわりに冒頭の話に戻ろう。私は、Rustの所有権を「本の貸し借りみたいなもの」と理解した。その類推で入り口は開けた。でも、その類推に縛られて、ライフタイムの本質を見誤った。非同期処理を料理に例えて、リソース競合を甘く見た。トランザクションを銀行振込に例えて、ロールバックの複雑さに気づかなかった。キャッシュを「手元に置く」と理解して、無効化の難しさを軽視した。私は、何度も同じ失敗を繰り返してきた。正直に言えば、私は今でも類推を使う。毎日のように使う。「これって、あれみたいだな」と考える癖は、もはや私の一部だ。類推なしに思考することなど、私にはできない。たぶん、誰にもできない。でも、これらの経験を経て、私は類推の使い方を変えた。類推は入り口として使う。入ったら、具体を見る。 「本の貸し借りみたいなもの」で入ったら、次に「でも、貸し借りと違って、所有権を渡したら元の変数からは完全にアクセスできなくなる。貸した相手を追跡する仕組みはない」と自分に言い聞かせる。類推と差異を、セットで意識する。そして、類推が成り立たない場面に出会ったら、類推を修正する勇気を持つ。類推は、人間の知能の基盤だ。われわれは類推なしには思考できない。だから、類推を否定するつもりはない。否定できるはずもない。でも、類推の限界を知らなければならない。類推は万能ではない。類推は常に成立するとは限らない。表面的な類推は、本質的な差異を見落とす。類推で入って、具体で判断する。類推は仮説であって、証明ではない。類推を絶対視せず、反例を探し、実測で検証する。そして、間違った類推は、勇気を持って断つ。これが、エンジニアとしての類推の使い方だ。「それって、○○みたいなものですよね」。この言葉を使うとき、私は今、一瞬立ち止まる。「本当にそうか？」と自問する。表面的な類似に惑わされていないか。本質的な差異を見落としていないか。先日、後輩にRustの所有権を説明する機会があった。私は「本の貸し借りみたいなものなんだけど」と言った後、こう続けた。「ただし、本と違って、Rustでは貸した先を追跡する仕組みはない。完全に手放すか、借用するかの二択なんだ」。あの頃の自分には、この補足ができなかった。類推は強力だ。だからこそ、慎重に扱わなければならない。おい、類推するな。いや、違う。類推しろ。でも、類推を疑え。類推で入って、具体で確かめろ。そして、間違っていたら、断つ勇気を持て。それが、類推に救われ、類推に何度も裏切られ、それでも類推を愛する人間からの、静かな呼びかけだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考書籍The Rust Programming Language, 3rd Edition (English Edition)作者:Klabnik, Steve,Nichols, Carol,Krycho, ChrisNo Starch PressAmazonプログラミングRust 第2版作者:Jim Blandy,Jason Orendorff,Leonora F.S. TindallオライリージャパンAmazonバックエンドエンジニアを目指す人のためのRust作者:安東 一慈,大西 諒,徳永 裕介,中村 謙弘,山中 雄大翔泳社Amazon類似と思考　改訂版 (ちくま学芸文庫)作者:鈴木宏昭筑摩書房Amazonアナロジー思考作者:細谷 功東洋経済新報社Amazon問題解決力を高める「推論」の技術作者:羽田康祐k_birdフォレスト出版Amazon新装版　アブダクション: 仮説と発見の論理作者:米盛 裕二勁草書房Amazon作る、試す、正す。　アジャイルなモノづくりのための全体戦略作者:市谷 聡啓ビー・エヌ・エヌAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでOWASP API Security Top 10を体験する（後編）：リソース制御と攻撃検知]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/06/055637</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/06/055637</guid>
            <pubDate>Fri, 05 Dec 2025 20:56:37 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、Rust Advent Calendar 2025 6日目のエントリ記事です。はじめに前編からの続き ← API1 (BOLA), API2 (Broken Authentication), API3 (Mass Assignment)の解説はこちら前編では認証・認可の基礎とデータ保護について解説した。後編では、リソース消費制御、機能レベルの認可、そしてサーバーサイド攻撃について体験していく。API4: Rate Limit - 総当たり攻撃対策パスワードクラッキング（パスワードを片っ端から試して突破する攻撃）の現実を体験できるデモ。owasp.orgなぜレート制限が重要なのかレート制限とは、「一定時間内に受け付けるリクエスト数を制限する」仕組みだ。レート制限がないAPIは「無限に試行できる」ことを意味する。 攻撃手法  被害  レート制限での防御  パスワード総当たり  アカウント乗っ取り  試行回数制限  クレデンシャルスタッフィング  流出パスワードでの不正ログイン  IPベースのブロック  OTPブルートフォース  2段階認証（SMS認証など）のバイパス  アカウントロック  APIの過剰呼び出し  サービス停止（DoS）  グローバルレート制限  スクレイピング  データの大量取得  リクエスト間隔の強制 パスワードクラッキングの数学4桁のPINコードを総当たりする時は以下のようになる。組み合わせ: 104 = 10,000通り毎秒10回の試行 → 約17分で全組み合わせを試行レート制限なし → 毎秒1000回で10秒8文字のパスワード（小文字+数字）の時は以下のようになる。組み合わせ: 368 ≒ 2.8兆通り毎秒1000回でも約89年かかるでも、辞書攻撃なら数万語 → 数分で完了レート制限は「総当たりを現実的に不可能にする」ための防御だ。cargo run --release --bin rate-limit-demoでは、実際にどうやってレート制限を実装するのか。単純に「1分間に10回まで」と制限すればいいように思えるが、攻撃者はそう甘くない。IPアドレスを変えながら攻撃したり、複数のアカウントを同時に狙ったりする。だから、防御も複数の観点から行う必要がある。二層の防御：IP追跡とアカウント追跡/// Tracks login attempts per IP address#[derive(Debug, Clone)]struct LoginAttemptTracker {    /// IP -> (attempt_count, first_attempt_time)    ip_attempts: Arc<RwLock<HashMap<String, (u32, Instant)>>>,    /// Email -> (attempt_count, first_attempt_time)    account_attempts: Arc<RwLock<HashMap<String, (u32, Instant)>>>,    /// Blocked IPs    blocked_ips: Arc<RwLock<Vec<String>>>,    /// Locked accounts    locked_accounts: Arc<RwLock<Vec<String>>>,}なぜ二層必要なのか。IP追跡のみだと、攻撃者がVPNやTorでIP変えながら攻撃できるアカウント追跡のみだと、1つのIPから多数のアカウントを攻撃できる両方で、どちらのパターンも防げるスライディングウィンドウの実装fn record_attempt(&self, ip: &str, email: &str) -> (u32, u32) {    let window = Duration::from_secs(300); // 5分間のウィンドウ    let now = Instant::now();    // Track IP attempts    let ip_count = {        let mut attempts = self.ip_attempts.write().unwrap();        let entry = attempts.entry(ip.to_string()).or_insert((0, now));        if now.duration_since(entry.1) > window {            // 5分経過したらリセット            *entry = (1, now);        } else {            entry.0 += 1;        }        entry.0    };    // Block IP after 10 attempts    if ip_count >= 10 {        let mut blocked = self.blocked_ips.write().unwrap();        if !blocked.contains(&ip.to_string()) {            blocked.push(ip.to_string());            tracing::warn!(ip = ip, "IP blocked due to too many attempts");        }    }    // Lock account after 5 attempts    if account_count >= 5 {        // ...    }    (ip_count, account_count)}governorクレートによるグローバルレート制限// Global rate limiter: 10 requests per secondlet rate_limiter = Arc::new(RateLimiter::direct(Quota::per_second(    NonZeroU32::new(10).unwrap(),)));governorはトークンバケットアルゴリズムを実装している。これは「バケツに水が溜まっていく」イメージだ。バケットに毎秒10トークン補充され、リクエストごとに1トークン消費。バケットが空になったら429（Too Many Requests）を返す。脆弱 vs 安全/// VULNERABLE: Login endpoint without rate limitingasync fn vulnerable_login(Json(req): Json<LoginRequest>) -> Result<Json<LoginResponse>, AppError> {    // 何回でも試行可能！    if req.email == "user@example.com" && req.password == "password123" {        Ok(Json(LoginResponse { /* ... */ }))    } else {        Err(AppError::Unauthorized)    }}/// SECURE: Login endpoint with rate limiting and lockoutasync fn secure_login(    State(state): State<AppState>,    ConnectInfo(addr): ConnectInfo<SocketAddr>,    Json(req): Json<LoginRequest>,) -> Result<Json<LoginResponse>, (StatusCode, Json<RateLimitError>)> {    let ip = addr.ip().to_string();    // 1. グローバルレート制限    if state.rate_limiter.check().is_err() {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 2. IPブロック確認    if state.tracker.is_ip_blocked(&ip) {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 3. アカウントロック確認    if state.tracker.is_account_locked(&req.email) {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 4. 認証処理    if req.email == "user@example.com" && req.password == "password123" {        state.tracker.reset_on_success(&ip, &req.email); // 成功したらカウンターリセット        Ok(Json(LoginResponse { /* ... */ }))    } else {        state.tracker.record_attempt(&ip, &req.email); // 失敗を記録        Err((StatusCode::UNAUTHORIZED, /* ... */))    }}微妙な脆弱性：レート制限のバイパス手法「レート制限を実装したから安全」と思っていないだろうか。残念ながら、レート制限にもバイパス手法がたくさんある。微妙な脆弱性 #1: X-Forwarded-Forを信用する/// 開発者の意図: 「ロードバランサーの後ろにいるから、X-Forwarded-Forを使わないと」/// 現実: 攻撃者もX-Forwarded-Forを設定できるasync fn subtle_xff_bypass(headers: HeaderMap, ...) -> Result<...> {    // BUG: X-Forwarded-Forを無条件に信用    let ip = headers        .get("X-Forwarded-For")        .and_then(|v| v.to_str().ok())        .and_then(|s| s.split(',').next())        .map(|s| s.trim().to_string())        .unwrap_or_else(|| addr.ip().to_string());    // 攻撃: curl -H "X-Forwarded-For: 1.2.3.4" ...    //       curl -H "X-Forwarded-For: 5.6.7.8" ...    // 毎回違うIPとしてカウントされる！    if state.tracker.is_ip_blocked(&ip) { /* ... */ }}X-Forwarded-Forは信頼できるプロキシ（ロードバランサーやCDNなど、自分たちが管理しているサーバー）からのみ受け入れるべきだ。信頼チェーンを確立せずにXFFを使うと、攻撃者がIPを自由に偽装できる。微妙な脆弱性 #2: 大文字小文字の不一致/// 開発者の意図: 「メールアドレスでアカウントロックを追跡」/// 現実: 大文字小文字で別アカウント扱いasync fn subtle_case_sensitivity(...) -> Result<...> {    // BUG: アカウントロックは大文字小文字を区別    if state.tracker.is_account_locked(&req.email) {        return Err(...);    }    // でも認証は大文字小文字を無視    let email_lower = req.email.to_lowercase();    if email_lower == "user@example.com" && req.password == "password123" {        // ...    }    // 攻撃:    // user@example.com で5回失敗 → ロック    // User@example.com で5回失敗 → 別カウント！    // USER@example.com で5回失敗 → また別カウント！    // 結果: 15回試行できる}アカウント識別子の正規化を一貫して行わないと、レート制限を回避される。微妙な脆弱性 #3: タイミングリーク/// 開発者の意図: 「ロックされたアカウントは早期リターン」/// 現実: レスポンス時間でアカウントの存在がわかるasync fn subtle_timing_leak(...) -> Result<...> {    // ロック済みアカウントは即座に拒否（速い！）    if state.tracker.is_account_locked(&req.email) {        return Err(/* 数マイクロ秒 */);    }    // パスワードハッシュ検証（遅い！）    tokio::time::sleep(Duration::from_millis(100)).await;    // 存在するアカウントは追加処理（もっと遅い！）    if account_exists(&req.email) {        tokio::time::sleep(Duration::from_millis(50)).await;    }    // 攻撃: レスポンス時間を測定    // 即座に返る → ロック済み（= 存在するアカウント）    // 100ms → 存在しないアカウント    // 150ms → 存在するが間違ったパスワード}レスポンス時間を均一にしないと、アカウント列挙攻撃に使われる。微妙な脆弱性 #4: TOCTOU競合/// 開発者の意図: 「カウンターを確認してから処理」/// 現実: 確認と更新の間に別のリクエストが入るasync fn subtle_race_condition(...) -> Result<...> {    // Step 1: カウンター読み取り（ロック解放）    let current_count = {        let attempts = state.tracker.ip_attempts.read().unwrap();        attempts.get(&ip).map(|(count, _)| *count).unwrap_or(0)    }; // ← ここでロック解放    // この間に並行リクエストが！    tokio::time::sleep(Duration::from_millis(10)).await;    // Step 2: 制限チェック（古い値で判断）    if current_count >= 10 {        return Err(...);    }    // Step 3: 処理後にカウンター更新    state.tracker.record_attempt(&ip, &req.email);    // 攻撃: 100並行リクエストを同時送信    // 全員が current_count = 0 で通過！}チェックと更新はアトミックに行うべき。RwLockではなくアトミック操作や、チェックと更新を1つのロック内で行う必要がある。API5: BFLA - 一般ユーザーが管理者になれてしまう問題前編でBOLA（Broken Object Level Authorization）を解説した。BOLAは「他人のデータにアクセスできてしまう」問題だった。では、「他人のデータ」ではなく「使えないはずの機能」にアクセスできてしまったら？それがBFLA（Broken Function Level Authorization）だ。owasp.orgBOLAが「他人のデータを見られる」なら、BFLAは「使えないはずの機能が使える」。例えば、一般ユーザーが管理者用のユーザー一覧APIを叩けてしまうケース。言ってみれば「平社員が社長の権限でシステムを操作できる」状態だ。BOLAとBFLAの違いを理解するこの2つは混同しやすいので、明確に区別しよう。 項目  BOLA  BFLA  何が壊れているか  オブジェクト（データ）へのアクセス制御  機能（エンドポイント）へのアクセス制御  攻撃例  BobがAliceの注文を見る  一般ユーザーが管理者APIを叩く  チェック対象  「このデータは誰のものか」  「この機能は誰が使えるか」  典型的な対策  リソースごとの所有者チェック  ロール/権限チェック 例えで言えばこうだ。BOLA = 他人のロッカーを開けられる（同じ権限レベル内での越境）BFLA = 社員証がないのに役員室に入れる（権限レベルの越境）この違いを理解すると、なぜBFLAが発生しやすいのかも見えてくる。なぜBFLAが発生するのかエンドポイントの「発見」 - /api/usersがあるなら/api/admin/usersもあるだろうと攻撃者は考えるフロントエンドによる隠蔽への過信 - 「管理メニューは管理者にしか見せてないから大丈夫」→ APIは直接叩ける認証と認可の混同（再び） - 「ログインしてるから管理APIも使えるはず」という誤った思い込みテスト不足 - 管理者機能は管理者アカウントでしかテストしないドキュメント化されていない管理API - 「隠しAPI」は攻撃者に見つかる実際の被害パターンBFLAによって可能になる攻撃を挙げる。ユーザー情報の一括取得 - 全ユーザーのメールアドレス、個人情報を抜き取る権限昇格 - 自分のアカウントに管理者権限を付与するシステム設定の変更 - APIキーの再生成、課金設定の変更データの一括削除 - 管理者用の一括削除機能を悪用監査ログの改ざん - 証拠隠滅のためにログを消去では、脆弱なコードと安全なコードを見比べてみよう。/// VULNERABLE: No role checkasync fn vulnerable_list_users(user: AuthenticatedUser) -> Result<Json<Vec<UserInfo>>, AppError> {    Ok(Json(vec![        UserInfo {            id: 1,            email: "admin@example.com".to_string(),            role: "admin".to_string(),            ssn: "123-45-6789".to_string(), // SSNまで露出        },        // ...    ]))}/// SECURE: Admin checkasync fn secure_list_users(user: AuthenticatedUser) -> Result<Json<Vec<SafeUserInfo>>, AppError> {    if !is_admin(&user.0) {        return Err(AppError::Forbidden("Admin permission required".to_string()));    }    // ...}is_adminのチェックは単純だ。pub fn is_admin(claims: &UserClaims) -> bool {    claims.permissions.iter().any(|p| p == "admin")}「これくらい誰でも書く」と考えるだろう。しかし、本番環境で「認証は通ってるから大丈夫」と言ってこのチェックを忘れる人が後を絶たない。微妙な脆弱性：一見正しく見えるBFLAのバグ「is_adminチェックさえ入れれば安全」と思っていないだろうか。残念ながら、そう単純ではない。微妙な脆弱性 #1: HTTPヘッダーを信用する/// 開発者の意図: 「フロントエンドが送るX-User-Roleヘッダーを信用しよう」/// 現実: curlでいくらでも偽装できるasync fn subtle_header_role_check(    user: AuthenticatedUser,    headers: HeaderMap,) -> Result<Json<AdminResponse>, AppError> {    // BUG: HTTPヘッダーを信用している！    let role = headers        .get("X-User-Role")        .and_then(|v| v.to_str().ok())        .unwrap_or("user");    if role != "admin" {        return Err(AppError::Forbidden("Admin role required".to_string()));    }    // 攻撃: curl -H "X-User-Role: admin" ...    Ok(Json(admin_data))}フロントエンドから「便利だから」とヘッダーでロール情報を送る設計を見たことがある。これはアウトだ。HTTPヘッダーはクライアントが自由に設定できる。JWTのペイロードのように署名で保護されていない限り、信用してはいけない。微妙な脆弱性 #2: JWTクレームをDBと照合しない/// 開発者の意図: 「JWTに権限が入っているから、それを使えばOK」/// 現実: トークン発行後にユーザーが降格されたら？async fn subtle_client_claims_check(    user: AuthenticatedUser,) -> Result<Json<AdminResponse>, AppError> {    // これ、一見正しそう    let has_admin = user.0.permissions.iter().any(|p| p == "admin");    if !has_admin {        return Err(AppError::Forbidden("Admin permission required".to_string()));    }    // 問題: ユーザーが管理者だったのは「トークン発行時」の話    // トークン発行後に降格されていても、トークンが有効な限りアクセスできてしまう    Ok(Json(admin_data))}JWT（JSON Web Token）は便利だが、「トークン発行時点のスナップショット」に過ぎない。JWTとは、ユーザー情報や権限を暗号化して埋め込んだトークンで、サーバーはDBを参照せずに認証できる。しかし、ユーザーの権限が変更されたら、古いトークンは無効にするか、DBで再確認する必要がある。微妙な脆弱性 #3: 大文字小文字の罠/// 開発者の意図: 「adminをチェックすれば安全」/// 現実: 「Admin」「ADMIN」「aDmIn」は？let has_admin = user.0.permissions.iter().any(|p| p == "admin");これ自体は問題ないが、トークン生成側で大文字小文字の統一が取れていないと問題になる。ある箇所では"admin"、別の箇所では"Admin"で権限が付与されていたら、チェックをすり抜けてしまう。// 安全な実装: 大文字小文字を無視let has_admin = user.0.permissions.iter()    .any(|p| p.eq_ignore_ascii_case("admin"));微妙な脆弱性 #4: キャッシュされた権限チェック/// 開発者の意図: 「ミドルウェアで権限チェック済みだから、エンドポイントでは確認不要」/// 現実: そのキャッシュ、どこから来た？async fn subtle_cached_permission_check(    user: AuthenticatedUser,    Query(query): Query<CachedCheckQuery>,) -> Result<Json<AdminResponse>, AppError> {    // BUG: クエリパラメータから「チェック済み」フラグを読んでいる！    let is_verified_admin = query.permission_verified.unwrap_or(false);    if is_verified_admin {        // 攻撃: ?permission_verified=true        return Ok(Json(admin_data));    }    // 本来のチェック    if !is_admin(&user.0) {        return Err(AppError::Forbidden("Admin permission required".to_string()));    }    Ok(Json(admin_data))}「ミドルウェアでチェック済み」というフラグをリクエストに含めるパターンは意外とある。でもそのフラグがクエリパラメータやヘッダーから来ていたら、攻撃者が自由に設定できる。API7: SSRF - サーバーを踏み台にするSSRF（Server-Side Request Forgery）は、サーバーに「代わりにリクエストを送らせる」攻撃だ。普通、攻撃者は外部から内部ネットワークにアクセスできない。でも、サーバーは内部ネットワークにアクセスできる。だから、サーバーを「踏み台」にして、内部ネットワークに攻撃を仕掛けるのがSSRFだ。owasp.orgたとえるなら、「社員に偽の指示書を渡して、機密書類を持ってこさせる」ようなものだ。社員（サーバー）は指示書が正当なものだと思い込んで、機密エリアにアクセスしてしまう。SSRFの危険性を理解するSSRFが特に危険な理由を説明する。ファイアウォールをバイパス - 外部からは遮断されていても、内部からのリクエストは通るクラウドメタデータにアクセス - AWS/GCPの169.254.169.254（クラウド環境で自動的に提供される情報サービス）から認証情報を取得可能内部サービスの探索 - ポートスキャンや内部APIの発見に悪用認証のバイパス - 「内部ネットワークからのアクセスは信頼」という設計を悪用特に2番目の「クラウドメタデータへのアクセス」は、現代のクラウド環境では致命的な被害につながる。なぜなら、メタデータサービスには一時的な認証情報が含まれているからだ。クラウド環境での致命的な被害クラウド環境でのSSRFは特に危険だ。2019年のCapital One事件では、SSRFを使ってAWSのメタデータサービスにアクセスし、1億人以上の顧客データが漏洩した。攻撃の流れを見てみよう。1. 攻撃者: http://169.254.169.254/latest/meta-data/iam/security-credentials/ にアクセスさせる2. サーバー: 内部からのリクエストなので通常通り処理3. AWSメタデータ: IAMロールの一時認証情報を返す4. 攻撃者: その認証情報でS3バケットにアクセス → 大量のデータを取得SSRFが発生しやすい機能この事件を見て「うちはそんな機能ないから大丈夫」と思うだろう。しかし、SSRFが発生する機能は意外と身近にある。以下のような機能はSSRFの温床になりやすい。URLプレビュー/OGP取得 - 「このURLのタイトルと画像を表示」Webhook送信 - 「指定されたURLにPOSTリクエストを送る」PDF生成 - 「このURLの内容をPDFにする」（ヘッドレスブラウザがURLを開く）画像のリサイズ/変換 - 「このURLの画像をサムネイルにする」インポート機能 - 「このURLからデータをインポート」どれも「ユーザーが指定したURLにアクセスする」という共通点がある。この「ユーザーが指定したURL」が問題だ。例えば、「URLを指定したらそのページの内容を取得する」機能があったとする。/// VULNERABLE: Fetches any URLasync fn vulnerable_fetch(Json(req): Json<FetchUrlRequest>) -> Result<String, AppError> {    let response = reqwest::get(&req.url).await?;    Ok(response.text().await?)}攻撃者は内部ネットワークのURLを指定する。curl -X POST http://localhost:8080/vulnerable/fetch \     -d '{"url":"http://localhost:8080/internal/secrets"}'/internal/secrets は本来、外部からアクセスできない内部APIだ。しかし、サーバー自身が「localhost」にアクセスするのは許可されている。結果、攻撃者はサーバーを経由して機密情報を引き出す。サーバーは「言われたことを忠実に実行する」だけだ。それが悪意あるリクエストだとは気づかない。対策: 許可リストとプロトコル制限では、どうやってSSRFを防ぐのか。基本的な考え方は「信頼できるURLだけを許可する」ことだ。async fn secure_fetch(Json(req): Json<FetchUrlRequest>) -> Result<String, AppError> {    let url = Url::parse(&req.url)        .map_err(|_| AppError::BadRequest("Invalid URL".to_string()))?;    // HTTPSのみ許可    if url.scheme() != "https" {        return Err(AppError::BadRequest("Only HTTPS URLs are allowed".to_string()));    }    // 許可されたドメインのみ    let allowed_domains = ["api.example.com", "cdn.example.com"];    let host = url.host_str()        .ok_or_else(|| AppError::BadRequest("Invalid host".to_string()))?;    if !allowed_domains.contains(&host) {        return Err(AppError::BadRequest("Domain not in allowlist".to_string()));    }    // 許可リストを通過したURLのみ処理    // ...}「なんでも取ってくる」から「許可されたものだけ取ってくる」へ。自由度は下がるが、セキュリティは上がる。微妙な脆弱性：SSRFの巧妙なバイパス手法「許可リストでドメインをチェックしているから安全」と思っていないだろうか。残念ながら、SSRFは想像以上に狡猾だ。攻撃者は、許可されたドメインを経由して、内部ネットワークにアクセスする方法を探す。微妙な脆弱性 #1: リダイレクトを追跡してしまう/// 開発者の意図: 「最初のURLを検証すればOK」/// 現実: リダイレクト先は検証されていないasync fn subtle_redirect_ssrf(Json(req): Json<FetchUrlRequest>) -> Result<String, AppError> {    let parsed_url = Url::parse(&req.url)?;    // 最初のURLは検証する    if !ALLOWED_DOMAINS.contains(&parsed_url.host_str().unwrap()) {        return Err(AppError::BadRequest("Domain not allowed".to_string()));    }    // BUG: リダイレクトを10回まで追跡する    let client = reqwest::Client::builder()        .redirect(reqwest::redirect::Policy::limited(10))        .build()?;    // 攻撃:    // 1. パートナーサイト webhook.partner.com を許可リストに追加    // 2. パートナーが webhook.partner.com/redirect?to=http://localhost/internal を設定    // 3. 最初は検証を通過、リダイレクトで内部サーバーにアクセス    let response = client.get(&req.url).send().await?;    Ok(response.text().await?)}パートナーサイトやCDNを許可リストに入れていて、そこにオープンリダイレクト（任意のURLにリダイレクトできる機能）があったら終わり。リダイレクト先も検証するか、リダイレクトを無効にするべきだ。微妙な脆弱性 #2: DNSリバインディング/// 開発者の意図: 「DNSで解決されたIPをチェックすれば内部アクセスを防げる」/// 現実: DNSの応答は変わりうるasync fn subtle_dns_rebinding(Json(req): Json<FetchUrlRequest>) -> Result<String, AppError> {    let host = Url::parse(&req.url)?.host_str().unwrap().to_string();    // 最初のDNS解決（ここでは外部IP）    let ips = tokio::net::lookup_host(format!("{}:80", host)).await?;    for ip in ips {        if ip.ip().to_string().starts_with("127.") {            return Err(AppError::BadRequest("Internal IP blocked".to_string()));        }    }    // BUG: 実際のリクエスト時には別のDNS解決が行われる可能性    // 攻撃者のDNSサーバー:    // 1回目のクエリ → 1.2.3.4（外部IP、チェック通過）    // 2回目のクエリ → 127.0.0.1（内部IP！）    tokio::time::sleep(Duration::from_millis(100)).await;  // この間にDNSが変わる    let response = reqwest::get(&req.url).await?;    Ok(response.text().await?)}DNSリバインディング攻撃は、DNSの応答を時間差で変えることで検証をすり抜ける。DNSとは、ドメイン名（例：example.com）をIPアドレス（例：93.184.216.34）に変換する仕組みだ。攻撃者は自分のDNSサーバーを用意し、最初は外部IPを返し、2回目のクエリでは内部IP（127.0.0.1）を返すようにする。対策は「解決したIPを直接使う」か「DNSピンニング」（一度解決したIPを再利用する）を実装すること。微妙な脆弱性 #3: URLパーサーの差異を悪用/// 開発者の意図: 「URLをパースしてホストを検証」/// 現実: 検証時と実際のリクエスト時でパーサーが違うasync fn subtle_parser_differential(Json(req): Json<FetchUrlRequest>) -> Result<String, AppError> {    // url クレートでパース    let parsed_url = Url::parse(&req.url)?;    let host = parsed_url.host_str().unwrap();    if !ALLOWED_DOMAINS.contains(&host) {        return Err(AppError::BadRequest("Domain not allowed".to_string()));    }    // BUG: reqwest内部のHTTPクライアントが別のパースをする可能性    // 攻撃例:    // "https://api.github.com@localhost/internal/secrets"    //   → url クレート: github.com がホスト    //   → 一部のHTTPクライアント: localhost がホスト    let response = reqwest::get(&req.url).await?;    Ok(response.text().await?)}URLの解釈は実装によって微妙に異なる。例えば、https://api.github.com@localhost/pathというURLを考えてみよう。あるパーサーはapi.github.comがホストだと解釈し、別のパーサーはlocalhostがホストだと解釈する。この差異を悪用して、検証をすり抜けることができる。微妙な脆弱性 #4: プロトコル/エンコーディングの罠/// 開発者の意図: 「エンコードされたURLもサポートしよう」/// 現実: 検証するURLとリクエストするURLが違うasync fn subtle_protocol_smuggling(Json(req): Json<EncodedUrlRequest>) -> Result<String, AppError> {    let url_to_validate = if req.decode_first.unwrap_or(false) {        // URLデコードしてから検証        naive_percent_decode(&req.url)    } else {        req.url.clone()    };    // デコード後のURLを検証    let parsed = Url::parse(&url_to_validate)?;    // ... validation ...    // BUG: オリジナルのURL（デコード前）でリクエスト！    let response = reqwest::get(&req.url).await?;  // ← url_to_validate じゃない！    Ok(response.text().await?)}検証に使うURLとリクエストに使うURLが一致していないと、検証をバイパスできる。「便利だから」と入力を加工するときは、必ず加工後の値を一貫して使うこと。動作確認：実際に脆弱性を突いてみるここまで、4つの脆弱性（API4: Rate Limit、API5: BFLA、API7: SSRF、そして前編で紹介したAPI1〜3）を解説してきた。でも、コードを読むだけでは「本当にこれで攻撃できるの？」という疑問が残るだろう。そこで、実際にcurlでリクエストを投げて、脆弱性が動作することを確認してみよう。「攻撃者の視点」を体験することで、防御の重要性が腑に落ちるはずだ。BOLA（API1）の動作確認# サーバー起動cargo run --release --bin bola-demo# Bobのトークンを取得BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# 脆弱なエンドポイント：BobがAliceの注文を見れてしまうcurl -H "Authorization: Bearer $BOB_TOKEN" http://localhost:8080/vulnerable/orders/1# 結果: {"id":1,"user_id":"alice","product":"Widget A","amount":100,...}# → BobがAliceの注文情報を取得できた！# セキュアなエンドポイント：適切に拒否されるcurl -H "Authorization: Bearer $BOB_TOKEN" http://localhost:8080/orders/1# 結果: {"error":"Order 1 not found or access denied"}# Subtle脆弱性：クエリパラメータでuser_idを上書きcurl -H "Authorization: Bearer $BOB_TOKEN" "http://localhost:8080/subtle/orders/1?user_id=alice"# 結果: {"id":1,"user_id":"alice","product":"Widget A",...}# → クエリパラメータでオーナーチェックをバイパス！Mass Assignment（API3）の動作確認# サーバー起動cargo run --release --bin mass-assignment-demo# 脆弱なエンドポイント：statusを注入curl -X POST http://localhost:8080/vulnerable/payments \  -H "Content-Type: application/json" \  -d '{"user_id":"attacker","amount":1000,"status":"approved"}'# 結果: {"id":"...","user_id":"attacker","amount":1000,"status":"approved",...}# → 攻撃者がstatusを"approved"に設定できた！# セキュアなエンドポイント：statusは無視されるcurl -X POST http://localhost:8080/payments \  -H "Content-Type: application/json" \  -d '{"user_id":"user","amount":1000,"status":"approved"}'# 結果: {"id":"...","user_id":"user","amount":1000,"status":"pending",...}# → statusはサーバー側で"pending"に設定される# Subtle脆弱性：serde(flatten)でHashMapに余分なフィールドが入るcurl -X POST http://localhost:8080/subtle/payments/flatten \  -H "Content-Type: application/json" \  -d '{"user_id":"user","amount":500,"status":"approved","id":"my-custom-id"}'# 結果: statusが"approved"、idも上書きされる可能性# → flatten + HashMapの危険性BFLA（API5）の動作確認# サーバー起動cargo run --release --bin bfla-demo# 一般ユーザーのトークンを取得USER_TOKEN=$(curl -s http://localhost:8080/token/user | jq -r .access_token)# 脆弱なエンドポイント：一般ユーザーでも管理者機能にアクセスcurl -H "Authorization: Bearer $USER_TOKEN" http://localhost:8080/vulnerable/admin# 結果: {"message":"Welcome to admin panel","admin_data":{"total_revenue":567890.12,...}}# → 一般ユーザーが管理者データを取得！# セキュアなエンドポイント：適切に拒否curl -H "Authorization: Bearer $USER_TOKEN" http://localhost:8080/admin# 結果: {"error":"Admin permission required"}# Subtle脆弱性1：HTTPヘッダーのロールを信頼curl -H "Authorization: Bearer $USER_TOKEN" \     -H "X-User-Role: admin" \     http://localhost:8080/subtle/admin/role-in-header# 結果: アクセス成功！# → ヘッダーを追加するだけでadminになれる# Subtle脆弱性2：キャッシュされた権限チェックを信頼curl -H "Authorization: Bearer $USER_TOKEN" \     "http://localhost:8080/subtle/admin/cached-check?permission_verified=true"# 結果: アクセス成功！# → クエリパラメータで権限チェックをバイパスSSRF（API7）の動作確認# サーバー起動cargo run --release --bin ssrf-demo# 脆弱なエンドポイント：内部サービスにアクセスcurl "http://localhost:8080/vulnerable/fetch?url=http://localhost:8080/internal/secrets"# 結果: {"secrets":["DATABASE_URL=postgres://admin:password@db:5432",...]}# → 内部の機密情報を取得！# セキュアなエンドポイント：localhost は拒否curl "http://localhost:8080/fetch?url=http://localhost:8080/internal/secrets"# 結果: {"error":"Access to internal addresses is not allowed"}# Subtle脆弱性：URLパーサーの差異を悪用curl "http://localhost:8080/subtle/fetch/parser-diff?url=http://localhost%2523@evil.com/"# → 異なるパーサーで解釈が変わり、バイパス可能Rate Limit（API4）の動作確認# サーバー起動cargo run --release --bin rate-limit-demo# 正常なレート制限：5回でロックfor i in {1..6}; do  curl -X POST http://localhost:8080/login \    -H "Content-Type: application/json" \    -d '{"email":"test@example.com","password":"wrong"}'  echo ""done# 6回目: {"error":"Account locked. Too many failed attempts."}# Subtle脆弱性1：X-Forwarded-For でIPを偽装for i in {1..10}; do  curl -X POST http://localhost:8080/subtle/login/xff \    -H "Content-Type: application/json" \    -H "X-Forwarded-For: 10.0.0.$i" \    -d '{"email":"victim@example.com","password":"attempt$i"}'done# → 毎回異なるIPとしてカウントされ、ロックされない！# Subtle脆弱性2：メールアドレスの大文字小文字curl -X POST http://localhost:8080/subtle/login/case \  -H "Content-Type: application/json" \  -d '{"email":"User@Example.COM","password":"wrong"}'# → user@example.com とは別のエントリとしてカウント# Subtle脆弱性3：タイミング攻撃# 存在するユーザー（高速レスポンス）time curl -X POST http://localhost:8080/subtle/login/timing \  -H "Content-Type: application/json" \  -d '{"email":"admin@example.com","password":"x"}'# → ~10ms# 存在しないユーザー（遅いレスポンス）time curl -X POST http://localhost:8080/subtle/login/timing \  -H "Content-Type: application/json" \  -d '{"email":"nobody@example.com","password":"x"}'# → ~110ms（意図的な遅延）# → レスポンス時間の差でユーザーの存在を推測可能！Broken Auth（API2）の動作確認# サーバー起動cargo run --release --bin broken-auth-demo# 期限切れトークンを取得EXPIRED_TOKEN=$(curl -s http://localhost:8080/token/expired | jq -r .access_token)# 脆弱なエンドポイント：期限切れトークンを受け入れるcurl -H "Authorization: Bearer $EXPIRED_TOKEN" \     http://localhost:8080/vulnerable/validate# 結果: {"message":"Token accepted","token_type":"expired"}# → 期限切れなのにアクセス成功！# セキュアなエンドポイント：適切に拒否curl -H "Authorization: Bearer $EXPIRED_TOKEN" \     http://localhost:8080/validate# 結果: {"error":"Token validation failed: ExpiredSignature"}# Subtle脆弱性：nbf（not before）をスキップFUTURE_TOKEN=$(curl -s http://localhost:8080/token/future | jq -r .access_token)curl -H "Authorization: Bearer $FUTURE_TOKEN" \     http://localhost:8080/subtle/validate/nbf-skip# 結果: まだ有効期間前なのにアクセス成功# → nbfのチェック漏れ動作確認のポイントこれらのテストで確認できる重要な点をまとめる。脆弱なエンドポイント vs セキュアなエンドポイント同じリクエストでも、実装によって結果が全く異なるセキュアな実装は「デフォルト拒否」の原則に従うSubtle脆弱性の危険性コードを見ただけでは問題に気づきにくい「動いているから大丈夫」では見逃すセキュリティテストで初めて発覚することが多い攻撃者の視点攻撃者は正常系だけでなく、エッジケースを狙うヘッダー追加、大文字小文字変換、URL エンコードなど「そんなリクエスト来ないでしょ」は通用しない全テストの実行20のセキュリティテストを一括で実行できる。./scripts/test_all.sh==========================================API Security Demo - Vulnerability TestsOWASP API Security Top 10==========================================[PASS] Vulnerable EP: Bob accessed Alice's order (HTTP 200)  ← 攻撃成功[PASS] Secure EP: Access denied (HTTP 404)                   ← 攻撃失敗...==========================================Test Results Summary==========================================PASS: 20FAIL: 0All security tests passed!「脆弱なエンドポイントで攻撃が成功すること」と「安全なエンドポイントで攻撃が失敗すること」の両方をテストしている。「攻撃が成功してPASS」というのは変な感じがするが、これは「脆弱性のデモとして正しく動作している」ことの確認だ。その他のデモobservability: 攻撃検知システムセキュリティ対策は「防ぐ」だけでは不十分だ。攻撃が起きたことを「検知する」仕組みも必要になる。なぜなら、完璧な防御は存在しないからだ。このデモでは、攻撃パターンを検知してログに記録する仕組みを体験できる。cargo run --release --bin observability-demoセキュリティメトリクス（攻撃の試行回数や種類などの統計情報）を収集し、攻撃パターン（SQLインジェクション、XSSなど）を検知してログ出力する。Prometheus（監視システム）等で収集して、ダッシュボードで監視する想定だ。security_test: 自動セキュリティテスト脆弱性の有無を自動的にテストするデモ。CI/CD（コードの変更があるたびに自動でテストやデプロイを行う仕組み）に組み込むイメージ。開発の早い段階でセキュリティ問題を発見できる。cargo run --release --bin security-test-democurl http://localhost:8080/test/run-allAPI6, 8, 9, 10を扱わない理由本記事ではOWASP API Security Top 10のうち、API6、API8、API9、API10を扱っていない。それぞれ理由がある。API6: Unrestricted Access to Sensitive Business Flowsowasp.orgビジネスロジックの悪用（大量購入、スパムアカウント作成など）に関する脆弱性。これは「コードの脆弱性」というより「ビジネスルールの実装漏れ」であり、汎用的なデモを作りにくい。実際のビジネス要件に依存するため、抽象的なサンプルコードでは本質を伝えにくい。API8: Security Misconfigurationowasp.org設定ミス（デバッグモードの本番有効化、不要なHTTPメソッド許可、CORSの過剰許可など）に関する脆弱性。これはコードではなくインフラ設定やデプロイ設定の問題であり、Rustのコードデモとして示すには適していない。設定ファイルやクラウド設定のベストプラクティス集として別途まとめる方が有用だろう。API9: Improper Inventory Managementowasp.orgAPIバージョン管理の不備（古いAPIの放置、ドキュメント化されていないエンドポイント）に関する脆弱性。これは運用・管理の問題であり、単一のコードデモでは再現しにくい。組織的なAPIガバナンスの話になる。API10: Unsafe Consumption of APIsowasp.orgサードパーティAPIからの応答を信頼しすぎる問題。外部APIとの連携をデモするには実際のサードパーティサービスが必要になり、自己完結型のデモとして構成しにくい。要するに、API1〜5とAPI7は「コードレベルで再現・修正できる脆弱性」であり、API6、8、9、10は「運用・設定・ビジネスロジックレベルの問題」という違いがある。本記事では前者に焦点を当てた。これらの脆弱性を学ぶにはAPI6、8、9、10を含む全ての脆弱性を体験したい場合は、以下の脆弱性学習プラットフォームを推奨する。OWASP Juice Shopowasp.org最も有名な脆弱性学習用Webアプリケーション。OWASP Top 10だけでなく、API Security Top 10の脆弱性も含む100以上のチャレンジがある。Dockerで簡単に起動でき、スコアボードで進捗を確認できる。crAPI (Completely Ridiculous API)owasp.orgAPI脆弱性に特化した学習プラットフォーム。Facebook、Uber、Shopifyなどで実際に発見された脆弱性をベースにしたチャレンジが含まれる。マイクロサービスアーキテクチャで構築されており、現代的なAPI構成を学べる。VAmPI (Vulnerable API)github.comFlaskで作られたシンプルな脆弱性API。OWASP API Top 10の脆弱性が含まれており、セキュリティツールのテストにも使える。Vulnerable REST API (2023 Edition)github.comNode.jsとReactで作られた脆弱性アプリケーション。OWASP API Security Top 10 2023版に対応しており、API6〜10を含む全ての脆弱性をカバーしている。APIsec Universitywww.apisecuniversity.comAPIセキュリティに特化した無料のオンライントレーニング。OWASP API Top 10の解説から実践的なペネトレーションテスト手法まで学べる。まとめ前編・後編を通じて、OWASP API Security Top 10のうち6つの脆弱性を体験してきた。セキュリティは「知っている」と「実感している」の間に大きな溝がある。このデモを作って、自分で攻撃を試して、初めて「あ、これ確かにヤバい」と腑に落ちた。ドキュメントを読むだけでは得られない理解だった。コードはGitHubで公開している。cargo run --release --bin bola-demoで起動して、実際に攻撃を試してみてほしい。最後に、冒頭の話に戻る。「認証してるから大丈夫でしょ」—この言葉を聞いたら、このデモのことを思い出してほしい。そして「認可は」と聞き返してほしい。認証は玄関のチェックに過ぎない。中に入った後、どの部屋に入れるかを制御するのが認可だ。参考リンクOWASP API Security Top 10 (2023)公式ドキュメント。owasp.orgOWASP API Security Projectプロジェクトのホームページ。owasp.org本記事のソースコードgithub.comAlice and Bob - WikipediaBobとAliceの歴史。en.wikipedia.orggovernor - Rust Rate Limiting Libraryレート制限の実装に使用。github.comCWE-918: Server-Side Request Forgery (SSRF)SSRFに関連するCWEエントリ。cwe.mitre.orgCWE-770: Allocation of Resources Without Limits or Throttlingレート制限不足に関連するCWEエントリ。cwe.mitre.orgCWE-285: Improper AuthorizationBFLAに関連するCWEエントリ。cwe.mitre.orgPortSwigger - Server-side request forgery (SSRF)SSRFの詳細な解説とラボ環境。portswigger.netOWASP Cheat Sheet - Authorization認可に関するベストプラクティス。cheatsheetseries.owasp.orgOWASP Cheat Sheet - Authentication認証に関するベストプラクティス。cheatsheetseries.owasp.orgCapital One Data Breach (2019)SSRFによる大規模情報漏洩事例。https://en.wikipedia.org/wiki/2019_Capital_One_data_breachen.wikipedia.orgAWS IMDSv2AWSメタデータサービスのセキュリティ強化。SSRF対策として重要。docs.aws.amazon.comSecurify弊社のプロダクトでもAPIセキュリティのチェックを一部行うことができるらしい。3-shake.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[NVIDIA 認定資格奮闘記 ~Associate Generative AI Multimodal編~]]></title>
            <link>https://zenn.dev/akasan/articles/nvidia_nca_genm</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/nvidia_nca_genm</guid>
            <pubDate>Fri, 05 Dec 2025 11:03:22 GMT</pubDate>
            <content:encoded><![CDATA[今回はNVIDIAの認定資格であるAssociate Generative AI Multimodalを取得したので、その内容を共有しようと思います。 Associate Generative AI Multimodalとは？Associate Generative AI Multimodal（以下、NCA-GENM）は、テキスト、画像、音声といった様々なモダリティからデータを合成・解釈するAIシステムの設計、実装、管理に必要な基礎スキルを検証するエントリーレベルの資格です。https://www.nvidia.com/en-us/learn/certification/gene...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでOWASP API Security Top 10を体験する（前編）：認証・認可の基礎とデータ保護]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/12/05/104919</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2025/12/05/104919</guid>
            <pubDate>Fri, 05 Dec 2025 01:49:19 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、Rust Advent Calendar 2025 5日目のエントリ記事です。はじめに先日、あるプロジェクトのコードレビューで「このエンドポイント、認証は通ってるけど認可は大丈夫か」と聞いたら、「認証してるから大丈夫でしょ」という返答が返ってきた。その瞬間、私の脳内では警報が鳴り響いた。これはあれだ。「鍵がかかってるから金庫は安全」と言いながら、金庫の中身を誰でも見られる状態にしているやつだ。認証（Authentication）と認可（Authorization）の違い。頭ではわかっていても、実際のコードでどう違うのか、どう危険なのかを体感したことがある人は意外と少ない。かくいう私も、セキュリティの本を読んで「ふーん」と思いながら、翌日には同じミスをやらかしていた口だ。そこで今回、OWASP API Security Top 10の脆弱性を実際に攻撃できる形でRustにより実装してみた。OWASPとは「Open Web Application Security Project」の略で、Webアプリケーションのセキュリティに関するオープンなコミュニティだ。彼らが発表する「Top 10」は、最も危険で頻繁に発生する脆弱性のランキングとして世界中の開発者に参照されている。「脆弱なエンドポイント」と「安全なエンドポイント」を並べて、攻撃がどう成功し、どう防げるのかを手を動かして確認できる。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜBobとAliceなのか初手で余談だがセキュリティの例でやたらと「BobがAliceのデータを〜」という話が出てくる。なぜこの2人なのか。これは1978年にRon Rivest、Adi Shamir、Leonard Adleman（RSA暗号のRSA）が書いた論文「A Method for Obtaining Digital Signatures and Public-Key Cryptosystems」に由来する。彼らは暗号通信の説明で「AさんがBさんにメッセージを送る」ではなく「AliceがBobにメッセージを送る」と書いた。AとBで始まる名前を選んだだけだが、これが定着した。その後、セキュリティの世界では登場人物が増えていった。AliceとBobは通信したい善良な2人（主人公）Eveは盗聴者（Eavesdropperから。悪役その1）Malloryは能動的攻撃者（Maliciousから。もっと悪い悪役）Trentは信頼できる第三者（Trustedから）CarolやCharlieは3人目の参加者が必要なときに登場つまり、BobとAliceは何十年も同じ役を演じ続けている。本記事でも、この伝統に従ってBobとAliceに登場してもらう。Bobには悪役を演じてもらうことになるが、本来のBobは悪い人ではない。「認可が不十分だと善良なBobでも悪いことができてしまう」というのが本質的な問題なのだ。en.wikipedia.orgなぜ「体験」が必要なのかセキュリティの勉強で一番難しいのは、「危険性を実感すること」だ。ドキュメントを読んで「BOLAは危険です」と書いてあっても、「へー、そうなんだ」で終わる。これは人間の性だ。交通事故のニュースを見ても「自分は大丈夫」と考えるのと同じで、実際にBobがAliceのデータを抜き取る瞬間を見ないと、その怖さは伝わらない。このデモを作った動機は単純で、自分が「あ、これ確かにヤバい」と冷や汗をかける教材が欲しかったからだ。本を読んで「なるほど」と思っても、3日後には忘れている。でも、自分の手で攻撃を成功させた経験は忘れない。ちなみに、このデモを作っている最中に「あれ、これ本番のコードにも似たようなのあったな...」と気づいて本当に冷や汗をかいた。勉強は大事。OWASP API Security Top 10 (2023) 一覧まず、OWASP API Security Top 10の全体像を把握しておこう。本記事では、このうち主要な脆弱性を実際にRustで実装して体験する。https://owasp.org/API-Security/editions/2023/en/0x11-t10/owasp.org リスク  説明  API1:2023 - Broken Object Level Authorization  APIはオブジェクト識別子を扱うエンドポイントを公開しがちで、オブジェクトレベルのアクセス制御の問題が広い攻撃対象となる。ユーザーからのIDを使ってデータソースにアクセスするすべての関数で、オブジェクトレベルの認可チェックを考慮すべき。  API2:2023 - Broken Authentication  認証メカニズムは不正に実装されることが多く、攻撃者が認証トークンを侵害したり、実装の欠陥を悪用して一時的または永続的に他のユーザーになりすますことを可能にする。  API3:2023 - Broken Object Property Level Authorization  このカテゴリはAPI3:2019の過度なデータ露出とAPI6:2019のMass Assignmentを統合し、根本原因であるオブジェクトプロパティレベルでの認可検証の欠如または不適切さに焦点を当てている。  API4:2023 - Unrestricted Resource Consumption  APIリクエストの処理にはネットワーク帯域、CPU、メモリ、ストレージなどのリソースが必要。成功した攻撃はサービス拒否や運用コストの増加につながる可能性がある。  API5:2023 - Broken Function Level Authorization  異なる階層、グループ、ロールを持つ複雑なアクセス制御ポリシーと、管理機能と通常機能の不明確な分離は、認可の欠陥につながりやすい。  API6:2023 - Unrestricted Access to Sensitive Business Flows  このリスクに脆弱なAPIは、自動化された方法で過度に使用された場合にビジネスを損なう可能性のある機能を補償せずにビジネスフローを公開している。  API7:2023 - Server Side Request Forgery  SSRFの欠陥は、APIがユーザー提供のURIを検証せずにリモートリソースを取得する際に発生する可能性がある。ファイアウォールやVPNで保護されていても、攻撃者がアプリケーションに細工されたリクエストを予期しない宛先に送信させることができる。  API8:2023 - Security Misconfiguration  APIとそれをサポートするシステムには通常、APIをよりカスタマイズ可能にするための複雑な構成が含まれている。ソフトウェアおよびDevOpsエンジニアがこれらの構成を見落としたり、セキュリティのベストプラクティスに従わない場合がある。  API9:2023 - Improper Inventory Management  APIは従来のWebアプリケーションよりも多くのエンドポイントを公開する傾向があり、適切で更新されたドキュメントが非常に重要。非推奨のAPIバージョンや公開されたデバッグエンドポイントなどの問題を軽減するために、ホストとデプロイされたAPIバージョンの適切なインベントリも重要。  API10:2023 - Unsafe Consumption of APIs  開発者はサードパーティAPIから受信したデータをユーザー入力よりも信頼する傾向があり、より弱いセキュリティ基準を採用しがち。APIを侵害するために、攻撃者はターゲットAPIを直接侵害しようとするのではなく、統合されたサードパーティサービスを狙う。 本記事で実際に体験できる脆弱性を挙げる。前編（本記事）ではAPI1 (BOLA)、API2 (Broken Authentication)、API3 (Mass Assignment)を扱う後編ではAPI4 (Rate Limit)、API5 (BFLA)、API7 (SSRF)を扱うデモの全体像このデモは9つのバイナリで構成されている。それぞれが独立したWebサーバーとして起動する。/token/{user_id} でテスト用JWTを取得（JWTとは「JSON Web Token」の略で、ユーザーの認証情報を安全にやり取りするためのトークン形式だ。ログイン後にサーバーから発行され、以降のリクエストで「私は認証済みのユーザーです」と証明するために使う）/vulnerable/... で脆弱なエンドポイントを叩く/... で安全なエンドポイントを叩くapi-security-demo/├── src/bin/│   ├── bola.rs              # BOLA: オブジェクトレベル認可の不備│   ├── bfla.rs              # BFLA: 機能レベル認可の不備│   ├── mass_assignment.rs   # Mass Assignment: 一括代入の脆弱性│   ├── broken_auth.rs       # Broken Auth: 認証の不備│   ├── rate_limit.rs        # Rate Limit: リソース消費制限の不備│   ├── ssrf.rs              # SSRF: サーバーサイドリクエストフォージェリ│   ├── jwt.rs               # JWT: トークン操作のデモ│   ├── observability.rs     # 攻撃検知システム│   └── security_test.rs     # 自動セキュリティテスト技術スタックはRust + axum（axumはRust用のWebフレームワークで、高速かつ型安全なAPIサーバーを構築できる）。Rust 2024エディションで書いている。前提条件試してみたい方は以下が必要だ。Rust 1.85以上（2024エディション対応）curl と jq（テスト用。curlはコマンドラインからHTTPリクエストを送るツール、jqはJSONデータを整形・抽出するツール）# リポジトリのクローンgit clone https://github.com/nwiizo/workspace_2025.gitcd workspace_2025/infrastructure/api-security-demo# ビルド（初回は依存関係のダウンロードで時間がかかる）cargo build --release実装アーキテクチャの詳細「デモを動かす」だけでなく「なぜこう実装したのか」を理解することで、自分のプロジェクトに応用できる。ここでは設計判断とその理由を詳しく説明する。プロジェクト構成api-security-demo/├── Cargo.toml              # Rust 2024エディション、依存関係定義├── src/│   ├── lib.rs              # ライブラリのエントリポイント│   ├── auth.rs             # JWT認証・認可ロジック│   ├── db.rs               # SQLiteデータベース操作│   ├── error.rs            # エラー型定義│   ├── models.rs           # データモデル定義│   └── bin/                # 各デモのバイナリ│       ├── bola.rs│       ├── bfla.rs│       └── ...└── scripts/    └── test_all.sh         # 全テスト実行スクリプト共通ロジックはsrc/配下にライブラリとして切り出し、各デモはsrc/bin/配下の独立したバイナリとして実装している。これにより以下のメリットがある。コードの再利用: 認証、DB操作、エラーハンドリングを全デモで共有単一責任: 各バイナリは1つの脆弱性カテゴリに集中独立した起動: cargo run --bin bola-demoで特定のデモだけ起動可能エラーハンドリング設計Rustらしいエラー設計を採用した。thiserrorクレートで列挙型エラーを定義し、axumのIntoResponseを実装した。use thiserror::Error;#[derive(Error, Debug)]pub enum AppError {    #[error("Authentication required")]    Unauthorized,    #[error("Access denied: {0}")]    Forbidden(String),    #[error("Resource not found: {0}")]    NotFound(String),    #[error("Invalid request: {0}")]    BadRequest(String),    #[error("Rate limit exceeded")]    RateLimitExceeded,    #[error("JWT error: {0}")]    JwtError(#[from] jsonwebtoken::errors::Error),    #[error("Database error: {0}")]    DatabaseError(String),}なぜanyhow::Errorではなく独自のエラー型なのか。HTTPステータスコードの制御。エラーの種類によって401、403、404、429などを返し分けたいクライアントへのメッセージ制御。内部エラーの詳細は隠し、クライアント向けのメッセージだけ返したいコンパイル時の網羅性チェック。matchで全ケースを処理しているか確認できるIntoResponseの実装を見てみよう。impl IntoResponse for AppError {    fn into_response(self) -> Response {        let (status, error_message) = match &self {            AppError::Unauthorized => (StatusCode::UNAUTHORIZED, self.to_string()),            AppError::Forbidden(msg) => (StatusCode::FORBIDDEN, msg.clone()),            AppError::NotFound(msg) => (StatusCode::NOT_FOUND, msg.clone()),            AppError::BadRequest(msg) => (StatusCode::BAD_REQUEST, msg.clone()),            AppError::RateLimitExceeded => (StatusCode::TOO_MANY_REQUESTS, "Rate limit exceeded".to_string()),            // ...        };        let body = Json(json!({ "error": error_message }));        (status, body).into_response()    }}これにより、ハンドラ関数で?演算子を使うだけで、エラーの種類に応じたHTTPレスポンスに変換される。認証・認可の実装パターンaxumのFromRequestPartsトレイトを実装したExtractorを使う。Extractorとは「抽出器」のことで、HTTPリクエストから必要な情報（ここでは認証情報）を自動的に取り出す仕組みだ。これがこのデモの核心部分だ。/// Extractor for authenticated user claims (secure version)#[derive(Debug, Clone)]pub struct AuthenticatedUser(pub UserClaims);impl<S> FromRequestParts<S> for AuthenticatedUserwhere    S: Send + Sync,{    type Rejection = AppError;    fn from_request_parts(        parts: &mut Parts,        _state: &S,    ) -> impl Future<Output = Result<Self, Self::Rejection>> + Send {        let result = extract_auth_from_parts(parts, false);        async move { result.map(AuthenticatedUser) }    }}Extractorパターンの利点を挙げる。宣言的: 関数シグネチャにAuthenticatedUserがあれば認証必須と一目でわかる再利用可能: 同じExtractorを全エンドポイントで使い回せるテスト容易: Extractorを差し替えてテスト可能失敗時の自動レスポンス: 認証失敗時は自動で401を返す「脆弱な」バージョンも用意している。/// Extractor for user claims WITHOUT proper validation (vulnerable version)#[derive(Debug, Clone)]pub struct VulnerableAuthUser(pub UserClaims);これは署名検証をスキップし、期限切れトークンも受け入れる。教育目的のみ。データベース層の設計SQLiteを使い、認可の有無でメソッドを分けている。/// Get order by ID (no authorization check - vulnerable)pub fn get_order_by_id(&self, id: i64) -> Result<Option<Order>, AppError> {    let conn = self.conn.lock().unwrap();    let mut stmt = conn.prepare(        "SELECT id, user, product, quantity FROM orders WHERE id = ?1"    )?;    // ...}/// Get order by ID with user check (secure)pub fn get_order_by_id_for_user(&self, id: i64, user: &str) -> Result<Option<Order>, AppError> {    let conn = self.conn.lock().unwrap();    let mut stmt = conn.prepare(        "SELECT id, user, product, quantity FROM orders WHERE id = ?1 AND user = ?2"    )?;    // ...}「なぜSQLで認可するのか。アプリケーション層でフィルタすればいいのでは」という疑問もあるだろう。アプリケーション層でも可能だが、DB層で認可する利点がある。パフォーマンス: 不要なデータをDBから取得しない防御の多層化: アプリ層のバグがあってもDB層で防げる一貫性: SQLで認可ロジックが一箇所に集約されるしかし、複雑な認可ルール（「自分のチームのデータ」など）はアプリ層で実装したほうが保守しやすい場合もある。依存関係の選定理由Cargo.tomlから主要な依存関係とその理由を説明する。# Web frameworkaxum = { version = "0.8", features = ["macros"] }axum: Tokioチームが開発、型安全、Extractorパターン。Actix-webより新しく、モダンな設計。# Authentication & Authorizationjsonwebtoken = "9"argon2 = "0.5"jsonwebtoken: Rustで最もポピュラーなJWTライブラリ。argon2: パスワードハッシュの現行推奨アルゴリズム。bcryptより新しく、メモリハード。# Error handlingthiserror = "2"thiserror: 派生マクロでボイラープレートを削減。#[error("...")]でDisplay実装が自動生成される。# Rate limitinggovernor = "0.8"governor: トークンバケットアルゴリズムの実装。非同期対応。# Databaserusqlite = { version = "0.32", features = ["bundled"] }rusqlite: SQLiteバインディング。bundledでSQLiteを同梱（環境依存を排除）。本番ではPostgreSQLやMySQLを推奨。テスト戦略各モジュールにユニットテストを配置している。#[cfg(test)]mod tests {    use super::*;    #[test]    fn test_order_authorization() {        let db = Database::new_in_memory().unwrap();        let order = db.create_order("alice", "Test Product", 5).unwrap();        // Alice can access her order        let result = db.get_order_by_id_for_user(order.id, "alice").unwrap();        assert!(result.is_some());        // Bob cannot access Alice's order        let result = db.get_order_by_id_for_user(order.id, "bob").unwrap();        assert!(result.is_none());    }}より、scripts/test_all.shでE2E的な統合テストを実行。各エンドポイントに実際にHTTPリクエストを送り、脆弱なエンドポイントで攻撃が成功すること、安全なエンドポイントで攻撃が失敗することを検証する。API1: BOLA - 最も危険で、最も見落とされやすい脆弱性OWASP API Security Top 10の堂々第1位がBOLA（Broken Object Level Authorization）だ。日本語では「オブジェクトレベル認可の不備」。https://owasp.org/API-Security/editions/2023/en/0xa1-broken-object-level-authorization/owasp.org名前が難しそうに見えるが、中身は簡単だ。要するに「BobがAliceのデータを見られてしまう」という、小学生でも「それダメでしょ」とわかる問題だ。しかし、驚くほど多くの本番システムにこれがある。人類は学ばない。なぜBOLAが最も危険なのかBOLAが1位である理由は明確だ。発生頻度が非常に高い - ほぼすべてのAPIがリソースIDを扱う。そのすべてで認可チェックが必要自動化しやすい - 攻撃者はIDを1, 2, 3...と順に試すだけ。スクリプト数行で全データを列挙できる検出が困難 - 正規のリクエストと見分けがつかない。WAFでは防げない影響が甚大 - 顧客データ、取引履歴、個人情報がすべて漏洩する可能性実際のインシデント事例BOLAによる情報漏洩は数え切れないほど発生している。2019年 First American Financial - 不動産の取引記録8億8500万件が流出。URLのIDを変えるだけで他人の書類にアクセス可能だった2018年 Facebook - View As機能の脆弱性で5000万アカウントのトークンが漏洩多数のモバイルアプリ - APIエンドポイントのID推測で他ユーザーのプロフィールにアクセス可能これらに共通するのは「認証はしていたが、認可が不十分だった」という点だ。ログインしているからといって、すべてのデータにアクセスできるわけではない。この当たり前のことを、コードで正しく実装するのは意外と難しい。なぜ開発者はBOLAを生み出してしまうのか認証と認可の混同 - 「ログインしてるからOK」という思い込みフレームワークの過信 - 「認証ミドルウェアを通ってるから安全」という誤解テストの盲点 - 機能テストは自分のデータでしか行わないIDの予測可能性 - 連番IDは攻撃を容易にする（でもUUIDでも根本解決にならない）開発速度優先 - 「認可は後で追加する」と言いながら忘れる脆弱なコード/// VULNERABLE: Returns any order by ID without checking ownershipasync fn vulnerable_get_order(    State(state): State<Arc<AppState>>,    _user: AuthenticatedUser, // 認証情報を受け取っているが...    Path(order_id): Path<i64>,) -> Result<Json<Order>, AppError> {    // 使っていない。アンダースコアプレフィックスがそれを物語っている    let order = state.db.get_order_by_id(order_id)?        .ok_or_else(|| AppError::NotFound(format!("Order {} not found", order_id)))?;    Ok(Json(order))}_userとしてわざわざ認証情報を受け取っているのに、アンダースコアつけて無視している。これは「セキュリティチェックしてますよ」というアリバイ作りにすらなっていない。むしろ「チェックしようとして忘れた」という証拠だ。安全なコード/// SECURE: Returns order only if it belongs to the authenticated userasync fn secure_get_order(    State(state): State<Arc<AppState>>,    user: AuthenticatedUser,  // アンダースコアなし    Path(order_id): Path<i64>,) -> Result<Json<Order>, AppError> {    let user_id = &user.0.sub;    // 「注文ID」と「ユーザーID」の両方でDBを検索    let order = state.db.get_order_by_id_for_user(order_id, user_id)?        .ok_or_else(|| AppError::NotFound(format!(            "Order {} not found or access denied", order_id        )))?;    Ok(Json(order))}違いは1行だけ。たった1行。でも、この1行が「情報漏洩インシデント発生」と「平穏な運用」の分かれ道だ。微妙な脆弱性：一見正しそうに見えるバグ本番環境で見つかる脆弱性の多くは、明らかな間違いではない。「一見正しそうに見える」コードに潜んでいる。このデモには3つの「微妙な脆弱性」エンドポイントを用意した。微妙な脆弱性 #1: クエリパラメータによる上書き#[derive(Deserialize)]struct UserIdQuery {    user_id: Option<String>,}/// 「デバッグ用にuser_idをクエリパラメータで指定できるようにしよう」/// という親切心から生まれた脆弱性async fn subtle_vulnerable_get_order(    State(state): State<Arc<AppState>>,    user: AuthenticatedUser,  // ちゃんと認証してる！    Path(order_id): Path<i64>,    Query(query): Query<UserIdQuery>,) -> Result<Json<Order>, AppError> {    // BUG: クエリパラメータが認証情報を上書きしてしまう    let user_id = query.user_id.unwrap_or_else(|| user.0.sub.clone());    let order = state        .db        .get_order_by_id_for_user(order_id, &user_id)?  // user_idが攻撃者の指定した値に！        .ok_or_else(|| AppError::NotFound("..."))?;    Ok(Json(order))}攻撃方法を見てみよう。# Bobとして認証BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# クエリパラメータでAliceになりすましcurl -H "Authorization: Bearer $BOB_TOKEN" \     "http://localhost:8080/subtle/orders/1?user_id=alice"このパターンは実際のコードレビューでよく見る。「管理画面でユーザーを切り替えて確認したい」「サポート担当がユーザーの代わりに操作する機能が必要」などの要件から生まれがち。対策は「そもそもこの機能は必要か」を問い直すことと、必要なら別の認証フローを用意すること。微妙な脆弱性 #2: TOCTOU（Time-of-Check-Time-of-Use）async fn race_condition_get_order(    State(state): State<Arc<AppState>>,    user: AuthenticatedUser,    Path(order_id): Path<i64>,) -> Result<Json<Order>, AppError> {    let user_id = &user.0.sub;    // Step 1: 注文を取得（全件から）    let order = state.db.get_order_by_id(order_id)?        .ok_or_else(|| AppError::NotFound(...))?;    // ↑ この時点で機密データがメモリに載っている！    // Step 2: 所有者をチェック    if order.user != *user_id {        // エラーメッセージが情報を漏らす        return Err(AppError::Forbidden(format!(            "Order {} belongs to another user",  // 存在することを教えてしまう            order_id        )));    }    Ok(Json(order))}何が問題なのか。データをフェッチしてから認可チェックしている。認可が通らなくても、データは既にメモリ上にあるエラーメッセージが情報を漏らす。「存在しない」と「アクセス権がない」が区別できるログに所有者情報が残る。認可失敗時のログにorder_owner = order.userを出力している正しい順序は「認可チェック → データフェッチ」だが、「IDだけでは認可チェックできない」という理由でこの順序になりがち。解決策はDB層でget_order_by_id_for_userのように、フェッチと認可を一体化すること。微妙な脆弱性 #3: 認可前のログ出力async fn logging_before_auth_get_order(    State(state): State<Arc<AppState>>,    user: AuthenticatedUser,    Path(order_id): Path<i64>,) -> Result<Json<Order>, AppError> {    // 「監査のために全リクエストをログに残す」という要件から    let order = state.db.get_order_by_id(order_id)?;    // 認可チェック前に詳細をログ出力    if let Some(ref o) = order {        tracing::info!(            order_id = o.id,            order_user = o.user,       // 誰の注文かログに残る            order_product = o.product, // 何を買ったかログに残る            requester = user.0.sub,            "Order access attempted"        );    }    // ここで認可チェック（でも遅い）    let order = order.ok_or_else(|| AppError::NotFound(...))?;    if order.user != user.0.sub {        return Err(AppError::Forbidden("Access denied".to_string()));    }    Ok(Json(order))}ログは「セキュリティのために残す」という意図だが、認可前にログを取ると攻撃者がアクセスできないデータがログに残る。これは情報漏洩だ。ログ収集基盤に脆弱性があった場合、このログから機密情報が漏れる。正しいパターンを示す。認可前のログは「誰が」「何にアクセスしようとしたか（IDのみ）」認可後のログは詳細情報を含めてOK実際に攻撃してみる# サーバー起動cargo run --release --bin bola-demo# Bobのトークンを取得BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# 脆弱なエンドポイント: BobがAliceの注文(ID=1)を取得curl -H "Authorization: Bearer $BOB_TOKEN" \     http://localhost:8080/vulnerable/orders/1結果を見てみよう。{  "id": 1,  "user": "alice",  "product": "Widget A",  "quantity": 5}Bobが、Aliceの注文データを取得できてしまった。 Aliceは知らない。Bobは黙っている。システムは何も気づいていない。これが現実のインシデントだったら、ニュースになるやつだ。安全なエンドポイントでは以下のようになる。curl -H "Authorization: Bearer $BOB_TOKEN" \     http://localhost:8080/orders/1結果はこうなる。{  "error": "Order 1 not found or access denied"}404を返している点もポイントだ。「なんで403（Forbidden）じゃないのか」という疑問があるだろう。403は「その注文は存在するよ。しかしお前には見せない」という意味である404は「何の話だ。そんな注文知らないが」という意味である403は「存在する」という情報を漏らしている。攻撃者にヒントを与えないためには404のほうが適切だ。API2: Broken Authentication - JWT検証の問題「署名さえ正しければOK」という誤解を打ち砕くデモ。https://owasp.org/API-Security/editions/2023/en/0xa2-broken-authentication/owasp.orgなぜJWT検証で失敗するのかJWTは「署名で改ざんを検出できる」という特性から、安全だと誤解されやすい。しかし、JWTのセキュリティは署名検証だけでは不十分だ。以下の検証がすべて必要だ。 検証項目  何をチェックするか  省略するとどうなるか  署名 (signature)  トークンが改ざんされていないか  偽造トークンが通る  有効期限 (exp)  トークンが期限内か  永久に使えるトークンが発生  発行者 (iss)  正当な発行者が作ったか  他システムのトークンが通る  オーディエンス (aud)  このAPIで使うべきか  別サービスのトークンが通る  Not Before (nbf)  まだ使用開始前ではないか  未来のトークンが先に使える JWTに関する危険な誤解「署名が正しければ安全」 → 署名は「改ざんされていない」だけで「使っていい」は別の話「JWTライブラリを使えば安全」 → デフォルト設定が安全とは限らない「短い有効期限だから大丈夫」 → expチェックを無効にしていたら意味がない「リフレッシュトークンで更新するから」 → 古いアクセストークンが使えたら問題cargo run --release --bin broken-auth-demo脆弱な実装：署名以外を検証しない/// VULNERABLE: Validates JWT signature but skips claim validationasync fn vulnerable_validate_token(headers: HeaderMap) -> Result<Json<TokenValidationResponse>, AppError> {    // ...    // VULNERABLE: Disable all validation except signature    let mut validation = Validation::new(Algorithm::HS256);    validation.validate_exp = false; // 有効期限チェックしない！    validation.validate_aud = false; // audience チェックしない！    validation.required_spec_claims.clear(); // 必須クレームなし！    let result = decode::<UserClaims>(        token,        &DecodingKey::from_secret(JWT_SECRET.as_bytes()),        &validation,    );    // ...}これが危険な理由：期限切れトークンが使い放題（退職した社員のトークンが永久に有効）別サービス用のトークンが使える（audがチェックされないため）なりすましトークンが通る（issがチェックされないため）安全な実装：全クレームを検証/// SECURE: Properly validates all JWT claimsasync fn secure_validate_token(headers: HeaderMap) -> Result<Json<TokenValidationResponse>, AppError> {    // ...    // SECURE: Enable all validation    let mut validation = Validation::new(Algorithm::HS256);    validation.set_audience(&[JWT_AUDIENCE]);  // この API 用か？    validation.set_issuer(&[JWT_ISSUER]);      // 正当な発行者か？    validation.validate_exp = true;             // 期限内か？    let result = decode::<UserClaims>(        token,        &DecodingKey::from_secret(JWT_SECRET.as_bytes()),        &validation,    );    // ...}テスト用トークン生成このデモでは4種類のトークンを生成できる。async fn generate_test_token(Path(token_type): Path<String>) -> Result<Json<TokenInfo>, AppError> {    let (claims, description) = match token_type.as_str() {        "valid" => {            // 有効なトークン（1時間後に期限切れ）            let claims = UserClaims {                exp: (Utc::now() + Duration::hours(1)).timestamp() as usize,                aud: Some(JWT_AUDIENCE.to_string()),                iss: Some(JWT_ISSUER.to_string()),                // ...            };            (claims, "Valid token - expires in 1 hour")        }        "expired" => {            // 期限切れトークン（1時間前に期限切れ）            let claims = UserClaims {                exp: (Utc::now() - Duration::hours(1)).timestamp() as usize, // 過去！                // ...            };            (claims, "Expired token - expired 1 hour ago")        }        "wrong-audience" => {            // 別サービス用のトークン            let claims = UserClaims {                aud: Some("https://wrong-audience.com".to_string()), // 別のサービス！                // ...            };            (claims, "Token with wrong audience")        }        "wrong-issuer" => {            // 不正な発行者のトークン            let claims = UserClaims {                iss: Some("https://malicious-issuer.com".to_string()), // 偽者！                // ...            };            (claims, "Token with wrong issuer")        }        // ...    };}攻撃シナリオを試してみよう。# 期限切れトークンを取得EXPIRED=$(curl -s http://localhost:8080/token/expired | jq -r .access_token)# 脆弱なエンドポイント → 通る！curl -H "Authorization: Bearer $EXPIRED" http://localhost:8080/vulnerable/validate# 安全なエンドポイント → 401 Unauthorizedcurl -H "Authorization: Bearer $EXPIRED" http://localhost:8080/validate微妙な脆弱性：JWT検証の巧妙なバイパス「全クレームを検証しているから安全」と思っていないだろうか。残念ながら、JWT検証にはもっと狡猾な問題がある。微妙な脆弱性 #1: アルゴリズム混同攻撃/// 開発者の意図: 「RS256もHS256もサポートして柔軟に」/// 現実: RS256の公開鍵をHS256の秘密鍵として使われるasync fn subtle_alg_confusion(headers: HeaderMap) -> Result<...> {    let header = jsonwebtoken::decode_header(token)?;    // BUG: トークンが主張するアルゴリズムを信用    let mut validation = Validation::new(header.alg);  // ← header.alg を信用！    validation.set_audience(&[JWT_AUDIENCE]);    validation.set_issuer(&[JWT_ISSUER]);    // 攻撃:    // 1. サーバーのRS256公開鍵を取得（公開されてる）    // 2. その公開鍵をHS256の秘密鍵として使ってトークン署名    // 3. {"alg": "HS256"} としてサーバーに送信    // 4. サーバーは公開鍵を「HS256の秘密鍵」として検証 → 成功！    let result = decode::<UserClaims>(        token,        &DecodingKey::from_secret(JWT_SECRET.as_bytes()),        &validation,    );}対策：アルゴリズムは固定値で指定。トークンのalgヘッダーを信用してはいけない。微妙な脆弱性 #2: Key ID (kid) インジェクション/// 開発者の意図: 「kidヘッダーで鍵を選択」/// 現実: kidに任意の値を入れられるasync fn subtle_kid_injection(headers: HeaderMap) -> Result<...> {    let header = jsonwebtoken::decode_header(token)?;    // BUG: kidを検証なしで使用    let kid = header.kid.unwrap_or_else(|| "default".to_string());    // 実際の脆弱なコード例：    // SQLインジェクション: kid = "key1' OR '1'='1"    // let key = db.query(f"SELECT key FROM keys WHERE id = '{kid}'");    // パストラバーサル: kid = "../../../etc/passwd"    // let key = fs::read(format!("/keys/{}.pem", kid));    // NULLキー: kid = "../../dev/null"    // 空のキーで署名検証 → 常に成功}kidは信頼できない入力。許可リスト方式でキーを選択するべき。微妙な脆弱性 #3: JKU (JWK Set URL) バイパス/// 開発者の意図: 「JKUヘッダーから公開鍵を取得」/// 現実: 攻撃者のサーバーから鍵を取得させられるasync fn subtle_jku_bypass(headers: HeaderMap) -> Result<...> {    let header = jsonwebtoken::decode_header(token)?;    if let Some(jku) = header.jku {        // BUG: 弱いチェック        let allowed_prefix = "https://auth.example.com";        if jku.starts_with(allowed_prefix) {            // 攻撃:            // jku = "https://auth.example.com.attacker.com/keys"            // jku = "https://auth.example.com@attacker.com/keys"            // jku = "https://auth.example.com%2F@attacker.com/keys"            // 全部 starts_with チェックを通過！            let keys = fetch_jwks_from_url(&jku).await?;            // 攻撃者の公開鍵を取得 → 攻撃者が署名したトークンが有効に        }    }}JKUは使わないか、完全一致でURLをチェックするべき。微妙な脆弱性 #4: Not-Before (nbf) 未検証/// 開発者の意図: 「expさえチェックすれば大丈夫」/// 現実: 未来用に発行されたトークンが今使えるasync fn subtle_nbf_skip(headers: HeaderMap) -> Result<...> {    let mut validation = Validation::new(Algorithm::HS256);    validation.set_audience(&[JWT_AUDIENCE]);    validation.set_issuer(&[JWT_ISSUER]);    validation.validate_exp = true;    validation.validate_nbf = false;  // BUG: nbfを検証しない    // 攻撃シナリオ:    // 1. 管理者が「来月1日から有効」なトークンを事前発行    // 2. そのトークンが漏洩    // 3. 攻撃者は今すぐそのトークンを使用 → nbf無視で成功    // または:    // 1. 内部犯行者が未来日付のトークンを大量に生成    // 2. 退職後にそれらを使用    // 3. expはチェックされるがnbfはスルー → アクセス成功}nbfクレームもexpと同様に重要。「まだ有効ではない」トークンを拒否しないと、事前発行されたトークンが悪用される。HS256 vs RS256JWT認証では2つの主要なアルゴリズムがある。// HS256: 同じ鍵で署名と検証（対称鍵）const HS256_SECRET: &str = "your-256-bit-secret-key-here-must-be-long-enough";// RS256: 秘密鍵で署名、公開鍵で検証（非対称鍵）const RS256_PRIVATE_KEY: &str = r#"-----BEGIN PRIVATE KEY-----MIIEvgIBADANBgkqhkiG9w0BAQEFAASC...-----END PRIVATE KEY-----"#;const RS256_PUBLIC_KEY: &str = r#"-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A...-----END PUBLIC KEY-----"#;なぜRS256が推奨されるのか。HS256は署名と検証に同じ鍵を使う。検証側にも秘密鍵が必要になり、漏洩リスクが高いRS256は署名に秘密鍵、検証に公開鍵を使う。公開鍵は配布しても安全なので、マイクロサービス向きAPI3: Mass Assignment - 見えないフィールドを操作されるこれは個人的に「一番やらかしやすい」脆弱性だ。そして「やらかしても気づきにくい」という意味で最も厄介だろう。https://owasp.org/API-Security/editions/2023/en/0xa3-broken-object-property-level-authorization/owasp.orgMass Assignmentとは何かMass Assignment（一括代入）は、クライアントから送られてきたデータを、サーバー側のオブジェクトにそのまま「一括で」割り当ててしまうことで発生する脆弱性だ。もともとはRuby on RailsやPHPのLaravelなど、「お手軽にCRUDを作れるフレームワーク」で頻発していた。これは「フォームのフィールドをそのままDBカラムにマッピング」する機能が便利すぎて、セキュリティを犠牲にしていた。Rustは型付けが厳格なので「安全」と思われがちだが、serdeでJSONをデシリアライズする際に同様の問題が発生しうる。serdeとはRustで最も広く使われているシリアライズ/デシリアライズ用のライブラリで、JSONなどのデータ形式とRustの構造体を相互変換できる。なぜ開発者はこのミスを犯すのか便利さの誘惑 - 「リクエストとモデルの型を同じにすればコードが減る」フィールド追加時の見落とし - DBにstatusカラムを追加 → Rustの構造体にも追加 → リクエスト型にも追加 → やらかし「デフォルト値があるから大丈夫」という誤解 - #[serde(default)]は「送られなかったら」デフォルト、「送られたら」その値テスト時の盲点 - 正常系では余分なフィールドを送らないので気づかない操作される可能性のあるフィールド攻撃者が狙う典型的なフィールドを挙げる。 フィールド  本来の用途  攻撃による悪用  status  処理状態管理  "pending" → "approved" で承認をバイパス  role  権限管理  "user" → "admin" で権限昇格  is_verified  検証フラグ  false → true で検証をスキップ  price  価格  1000 → 1 で値引き  user_id  所有者  他人のIDを指定してなりすまし  created_at  作成日時  過去の日付を指定して古いデータを偽装  id  主キー  既存IDを指定して上書き攻撃 例えば、支払い作成APIで、ユーザーが送ってきたJSONをそのまま使ってしまうケースを見てみよう。/// VULNERABLE: Accepts any fields from user input#[derive(Deserialize)]pub struct UnsafePaymentRequest {    pub amount: f64,    pub currency: String,    #[serde(default)]    pub status: Option<String>,  // ユーザーが設定可能になっている}async fn vulnerable_create_payment(    Json(req): Json<UnsafePaymentRequest>,) -> Json<Payment> {    let payment = Payment {        id: Uuid::new_v4().to_string(),        amount: req.amount,        currency: req.currency,        status: req.status.unwrap_or_else(|| "pending".to_string()),        // ↑ ユーザーが"approved"を送ってきたらそのまま使っちゃう    };    Json(payment)}攻撃してみよう。curl -X POST http://localhost:8080/vulnerable/payments \     -H "Content-Type: application/json" \     -d '{"amount": 100, "currency": "USD", "status": "approved"}'結果は"status": "approved"であり、未払いの支払いが承認済みになった。支払いステータスを「承認済み」に設定して、実際には支払いをしない。システムは何も気づかない。対策: DTOを分けるDTOとは「Data Transfer Object」の略で、データを受け渡すための専用オブジェクトだ。ここでは「ユーザーからの入力を受け取るための構造体」と「内部処理で使う構造体」を分けるという意味で使っている。/// SECURE: Only accepts allowed fields#[derive(Deserialize)]pub struct CreatePaymentRequest {    pub amount: f64,    pub currency: String,    // statusフィールドは存在しない}async fn secure_create_payment(    Json(req): Json<CreatePaymentRequest>,) -> Json<Payment> {    let payment = Payment::new(req.amount, req.currency);    // statusは常にサーバー側で"pending"に設定される    Json(payment)}入力用のDTOと内部用のモデルを分ける。コード量は増える。型定義は増える。でも、これが「自由度の高いAPI」と「セキュアなAPI」の違いだ。自由には責任が伴う。微妙なMass Assignment：serde flattenの罠「入力DTOを分けた」と言っても、実装の仕方次第で脆弱になる。微妙な脆弱性 #1: #[serde(flatten)]の問題#[derive(Deserialize, Serialize)]struct FlattenedPaymentRequest {    amount: f64,    currency: String,    // 「未知のフィールドをログに残したい」という意図    #[serde(flatten)]    extra_fields: HashMap<String, serde_json::Value>,}async fn subtle_flatten_payment(    State(state): State<Arc<AppState>>,    _user: AuthenticatedUser,    Json(req): Json<FlattenedPaymentRequest>,) -> Result<Json<Payment>, AppError> {    let mut payment = Payment::new(req.amount, req.currency.clone());    // 「extra_fieldsに有効なstatusがあれば使おう」    // 開発者の意図：「クライアントの便宜を図る」    // 現実：Mass Assignmentの再来    if let Some(status) = req.extra_fields.get("status") {        if let Some(s) = status.as_str() {            if ["pending", "approved", "rejected"].contains(&s) {                payment.status = s.to_string();  // approved も有効な値！            }        }    }    state.db.create_payment(&payment)?;    Ok(Json(payment))}#[serde(flatten)]とHashMapの組み合わせは便利だが、「未知のフィールドを捕捉する」という性質が裏目に出る。コードレビューでflattenを見たら警戒しよう。微妙な脆弱性 #2: 部分更新の罠PATCH（部分更新）エンドポイントは特に危険だ。#[derive(Deserialize)]struct PartialPaymentUpdate {    amount: Option<f64>,    currency: Option<String>,    // 「ユーザーが自分でキャンセルできるように」status を追加    #[serde(default)]    status: Option<String>,}async fn subtle_update_payment(    State(state): State<Arc<AppState>>,    _user: AuthenticatedUser,    Path(payment_id): Path<String>,    Json(update): Json<PartialPaymentUpdate>,) -> Result<Json<Payment>, AppError> {    let mut payment = state.db.get_payment_by_id(&payment_id)?        .ok_or_else(|| AppError::NotFound(...))?;    // 部分更新ロジック    if let Some(amount) = update.amount {        payment.amount = amount;    }    if let Some(currency) = update.currency {        payment.currency = currency;    }    // 「キャンセルは許可、でも承認は決済システム経由のみ」のつもり    if let Some(status) = update.status {        if payment.status == "pending" && status == "approved" {            // 開発者：「pendingからapprovedへの遷移だけ許可」            // 現実：これがまさに攻撃者がやりたいこと！            payment.status = status;        } else if payment.status == "pending" && status == "cancelled" {            payment.status = status;        }    }    Ok(Json(payment))}条件分岐で「許可する遷移」を書いたつもりが、攻撃者が欲しいものを許可している。ロジックが複雑になるほど、こういうミスは見つけにくくなる。攻撃方法を見てみよう。# 支払いを作成PAYMENT_ID=$(curl -s -X POST http://localhost:8080/payments \  -H "Authorization: Bearer $TOKEN" \  -H "Content-Type: application/json" \  -d '{"amount": 100, "currency": "USD"}' | jq -r .id)# 部分更新でステータスを承認済みにcurl -X POST "http://localhost:8080/subtle/payments/$PAYMENT_ID" \  -H "Authorization: Bearer $TOKEN" \  -H "Content-Type: application/json" \  -d '{"status": "approved"}'実装で学んだこと1. 認証と認可は別物これは何度言っても足りない。認証: 「あなたは誰か」 → 「私はBobです」認可: 「Bobさん、あなたはこれをしていいのか」 → 「...ダメです」JWTを検証して「このユーザーは本物だ」とわかっても、「このユーザーがこのリソースにアクセスしていいか」は全く別の問題だ。会社のビルで例えるとこうだ。認証 = 社員証を見せて入館する認可 = サーバールームに入れるかどうか社員証を持っていても、全員がサーバールームに入れるわけではない。当たり前だ。でも、APIでは「認証してるから大丈夫」と言ってしまいがちなのだ。2. 404 vs 403認可エラーの際に403を返すか404を返すか。403: リソースの存在を明かしつつアクセスを拒否404: リソースの存在自体を隠すセキュリティ的には404が安全だ。403は「存在する」という情報を漏らしている。しかし、デバッグは困難になる。「404なんだけど、本当に存在しないのか、権限がないのか」がわからない。本番環境では404、開発環境では403にするとか、ログには詳細を残すとか、工夫が必要だ。3. DTOの分離は面倒だが必要入力用の構造体と内部用の構造体を分けるのは、確かに面倒だ。同じようなものを2回書くことになる。しかし、Mass Assignment攻撃を防ぐには必要なコストだ。Rustの場合、コンパイル時に型チェックされるので、「うっかりユーザー入力をそのまま使ってしまう」ミスは起きにくい。CreatePaymentRequestにstatusフィールドがなければ、コンパイラが「そんなフィールドないよ」と教えてくれる。これはRustの強みだ。動的型付け言語だと、こうはいかない。続きは後編へ → API4 (Rate Limit), API5 (BFLA), API7 (SSRF), 動作確認、まとめ参考リンクOWASP API Security Top 10 (2023)公式ドキュメント。owasp.orgaxum - Rust Web Framework本デモで使用しているWebフレームワーク。github.comjsonwebtoken - Rust JWT LibraryJWT認証の実装に使用。github.comthiserror - Rust Error Handlingエラー型の定義に使用。github.comJWT.ioJWTのデバッグ・検証ツール。jwt.ioRFC 7519 - JSON Web Token (JWT)JWTの仕様。datatracker.ietf.orgCWE-639: Authorization Bypass Through User-Controlled KeyBOLAに関連するCWEエントリ。cwe.mitre.orgCWE-915: Improperly Controlled Modification of Dynamically-Determined Object AttributesMass Assignmentに関連するCWEエントリ。cwe.mitre.org]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AlloyDB と Cloud Spanner (スケーラビリティの境界)]]></title>
            <link>https://silasol.la/posts/2025-12-05-01_alloy-db-and-spanner/</link>
            <guid isPermaLink="false">https://silasol.la/posts/2025-12-05-01_alloy-db-and-spanner/</guid>
            <pubDate>Fri, 05 Dec 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AlloyDB と Cloud Spanner のアーキテクチャの違いやスケーラビリティの境界について解説します．]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PythonでIaCかきたい？それならPulumiだ！]]></title>
            <link>https://zenn.dev/akasan/articles/pulumi_quickstart_python</link>
            <guid isPermaLink="false">https://zenn.dev/akasan/articles/pulumi_quickstart_python</guid>
            <pubDate>Thu, 04 Dec 2025 11:45:46 GMT</pubDate>
            <content:encoded><![CDATA[今回はPulumiに入門して、PythonでIaCを実現する方法に触れてみました。今までTerraformしか使ったことがなかったですが、使い慣れたPythonでIaCが書けるということで使ってみました。 Pulumiとは？Pulumiは、Pythonなどのプログラミング言語を使ってクラウドインフラストラクチャを構築、デプロイ、管理するためのオープンソースプラットフォームです。あらゆるクラウドをまたいで、統一されたワークフローでインフラストラクチャ、シークレット、構成を管理できます。IaCといえばTerraformなどが有名かつよく利用されていると思いますが、普段使い慣れている言語...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、テックブログを書け]]></title>
            <link>https://speakerdeck.com/nwiizo/oi-tetukuburoguwoshu-ke</link>
            <guid isPermaLink="false">https://speakerdeck.com/nwiizo/oi-tetukuburoguwoshu-ke</guid>
            <pubDate>Thu, 04 Dec 2025 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年12月5日に「おい、テックブログを書け」という登壇をした。「おい」である。命令形である。30分間、人前に立って「書け」と言い続けるという、冷静に考えるとなかなか傲慢な振る舞いをしてきたわけだが、登壇資料を作っている最中、ふと気づいてしまった。書けと言っている自分は、なぜ書いているのだろうか、と。技術ブログを書くことについて語ろうとすると、それは私が「書いてきた」ことを晒すことに他ならず、AIとの付き合い方を語ろうとすると、それは私が「どう仕事をしているか」を開陳することと紙一重になる。そうなると聞いている側からすれば、こいつは結局、自分の話がしたいだけなのではないか、登壇という大義名分を得て気持ちよく自分語りをしているだけなのではないか、と思われても仕方がない。いや、実際そうなのかもしれない。そう見られることへの嫌悪感と、そう見られまいと振る舞う自分への嫌悪感が同時に存在していて、どちらに転んでも結局イヤなやつなのである。しかし登壇というのは厄介なもので、「書け」と命令するからには、自分がなぜ書いてきたのかを明かさなければ説得力がない。説得力のない登壇ほど空虚なものはない。空虚な登壇をする自分を想像して、それはそれで耐えられない。結局、自己開示から逃げられない構造になっている。なんという罠だろうか。身体性という言葉を使った。AIに記事を書かせることについて話した。私の答えは明確で、記事はほとんどAIに書かせている、しかし価値の源泉は私にある、と。私が素材を提供し、AIが構造化し、私がレビューして調整する。編集者としてのAI。この協働こそが現代の執筆だと、そう話した。話しながら、これは本当にそうだろうかと自分を疑う自分がいて、でもそういう迷いごと引き受けて喋るしかないのだった。まず自分のために書け、結果として、それが誰かを救う。そう締めくくった。https://forkwell.connpass.com/event/377267/https://syu-m-5151.hatenablog.com/archive/category/%E3%81%8A%E3%81%84%E3%80%81自宅からの昼登壇だったので、終わってから昼飯を食べに外に出た。参考書籍として紹介した本をもう一度読み返そうと思って、鞄に入れてきていた。店に向かう道すがら、本を開く。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Electronアプリで型安全なIPC通信を実現する electron-trpcという選択肢]]></title>
            <link>https://zenn.dev/meziron/articles/82dfa259c30bf8</link>
            <guid isPermaLink="false">https://zenn.dev/meziron/articles/82dfa259c30bf8</guid>
            <pubDate>Wed, 03 Dec 2025 15:00:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事は 3-shake Advent Calendar 2025 の記事です。Electronアプリケーションの開発において、Main ProcessとRenderer Process間の通信（IPC）を型安全に実装することは、開発体験と保守性を高める上で重要な課題です。本記事では、electron-trpcを用いて、IPC通信の型安全性を効率的に確保する方法について解説します。 従来の課題：型定義の分散とボイラープレートElectron標準のIPC通信（ipcMain / ipcRenderer）を使用する場合、型安全性を確保しようとすると、記述量が増大しが...]]></content:encoded>
        </item>
    </channel>
</rss>