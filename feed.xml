<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Fri, 08 Aug 2025 06:09:32 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[Vimでヤンクした内容の上書き回避にはマイナス（-）をブラックホールレジスタにマッピングするといいよ]]></title>
            <link>https://blog.atusy.net/2025/08/08/map-minus-to-blackhole-register/</link>
            <guid>https://blog.atusy.net/2025/08/08/map-minus-to-blackhole-register/</guid>
            <pubDate>Fri, 08 Aug 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[nnoremap - "_しておくと、ヤンクした内容を残しながら編集できて便利ですよ。"_ciwは入力しにくいですけど-ciwなら入力しやすいですよね。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud RunにデプロイしたmlflowからArtifactsをダウンロードすることを禁止するには？]]></title>
            <link>https://zenn.dev/akasan/articles/5f823793737bf6</link>
            <guid>https://zenn.dev/akasan/articles/5f823793737bf6</guid>
            <pubDate>Thu, 07 Aug 2025 11:03:16 GMT</pubDate>
            <content:encoded><![CDATA[今回はCloud RunにデプロイしたmlflowサーバかのUIからアーティファクトのダウンロードを阻止することができるか調べてみました。mlflowに関するアドバイスをしようということでmlflowについてちょうど調べていました。mlflow uiではモデルに関連づけられたアーティファクトのダウンロード機能があるのですが、mlflow uiとしてはそのダウンロードを拒否する仕組みが備わっていないです。セキュリティリスクになることからこれを防ぐ方法はないか調べてみた結果、うまくIAM設定をすると対応できるということがわかったので、その方法をブログにしてみます。※ 今回のコードはclau...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudサービスの生成AI関連サービス]]></title>
            <link>https://speakerdeck.com/shukob/google-cloudsabisunosheng-cheng-aiguan-lian-sabisu</link>
            <guid>https://speakerdeck.com/shukob/google-cloudsabisunosheng-cheng-aiguan-lian-sabisu</guid>
            <pubDate>Thu, 07 Aug 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2025年8月7日(木)、日本生成AIユーザ会 で「Google Cloudサービスの生成AI関連サービス」について発表しました。https://genai-users.connpass.com/event/361798/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Apache Linkisとはなんなのか？]]></title>
            <link>https://zenn.dev/akasan/articles/0cad512332b58c</link>
            <guid>https://zenn.dev/akasan/articles/0cad512332b58c</guid>
            <pubDate>Wed, 06 Aug 2025 14:12:33 GMT</pubDate>
            <content:encoded><![CDATA[今回はApache Linkisについて調べてみました。 今回も以下のツールを使って対象プロジェクトを決めました！https://zenn.dev/akasan/articles/7e30ad266c02c4※ 本企画に関する記事の目的は、それぞれのプロジェクトを本格的に深ぼるのではなく、プロジェクト名⇆どんな内容かをパッと思い出せるようにすることを目指します！※ とはいえ深ぼってみたいプロジェクトがあればどんどん複数連載になると思います。 Apache Linkisとは？公式サイトによると、Linkis builds a computation middleware l...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：Lensを使ってKubernetes環境をモニタリングしてみた]]></title>
            <link>https://zenn.dev/akasan/articles/9cc74002290a99</link>
            <guid>https://zenn.dev/akasan/articles/9cc74002290a99</guid>
            <pubDate>Tue, 05 Aug 2025 14:15:24 GMT</pubDate>
            <content:encoded><![CDATA[今回はKubernetesのモニタリングなどに利用できるIDEであるLensを使ってみたので、その共有をしようと思います。※ 本日健康診断を受けた結果体力が著しくなくなっちゃったため、ほんとにちょっと使ってみた感想になってしまいました。そのため、次回もっと本格的にLensを使ってみて記事出しますので今日のところはこれで勘弁してくださいw Lensとは？Lensとは公式の説明によるとLens is a Kubernetes platform that provides tools for seamless interaction with Kubernetes clusters...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Development Kitで作るマルチエージェントアプリケーション（GCNT2025）]]></title>
            <link>https://speakerdeck.com/yunosukey/agent-development-kitdezuo-rumarutiezientoapurikesiyon-gcnt2025</link>
            <guid>https://speakerdeck.com/yunosukey/agent-development-kitdezuo-rumarutiezientoapurikesiyon-gcnt2025</guid>
            <pubDate>Tue, 05 Aug 2025 04:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[PyCaretを利用して簡単にモデル比較を行おう]]></title>
            <link>https://zenn.dev/akasan/articles/9844ec4f012639</link>
            <guid>https://zenn.dev/akasan/articles/9844ec4f012639</guid>
            <pubDate>Mon, 04 Aug 2025 10:51:03 GMT</pubDate>
            <content:encoded><![CDATA[今回はローコードでモデルを開発できるPyCaretを利用して、titanicデータセットの分類をさせてみようと思います。 PyCaretとは？PyCaretとは、オープンソースのローコードで機械学習モデルを開発できるPythonライブラリです。通常機械学習モデルを開発する時はscikit-learnやXGBoost、LightGBMなど様々なライブラリを駆使し、ベースモデルや比較対象モデルを開発して行きます。しかし、モデルの開発を行う場合は多くの実装を必要とすることが多々あり、最低限の設定でモデルを作って性能比較をしたいと言った場合に負担が大きかったと思います。そこでPyCaret...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[組織の成長に伴う私のtimes の終焉についての思索]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/08/04/173559</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/08/04/173559</guid>
            <pubDate>Mon, 04 Aug 2025 08:35:59 GMT</pubDate>
            <content:encoded><![CDATA[さよなら、私の愛したtimesはじめに組織が成長する過程で、かつて機能していた構造が限界を迎える瞬間がある。私はおそらく今、その転換点に立っている。長年愛用してきた社内での個人的な発信空間であるtimesチャンネル(組織によっては分報という名前かも)を閉じることにした。これは単なるチャンネルの使用終了ではなく、組織の成長段階における必然的な選択だと考えている。ちなみにあくまで私の考えで私のみが実行しています。また、いつか復活する可能性もあります。会社の規模が大きくなってきたことを踏まえ、あくまで個人の考えでTimesチャンネルを削除することに決めました。 pic.twitter.com/eZfl1kuf2Q— nwiizo (@nwiizo) 2025年8月3日   このブログが良ければ読者になったり、nwiizoのXやGithubをフォロワーしてくれると嬉しいです。では、早速はじめていきます。timesの光と影小規模組織において、timesや分報などのいわゆるインフォーマルなコミュニケーションチャンネルは組織の血流として機能する。心理的障壁を下げ、階層を超えた知識共有を可能にし、暗黙知を形式知へと変換する触媒となる。しかし、この美しいエコシステムは、ある臨界点を超えると自己矛盾を抱え始める。スケーラビリティの逆説組織が拡大するにつれ、情報の流通経路は指数関数的に増加する。全体性の把握は不可能となり、部分最適化が進行する。かつて全員が共有していた文脈は断片化し、同じ組織にいながら異なる現実を生きることになる。情報の民主化を目指したはずのシステムが、逆に情報格差を生み出す。見える人と見えない人、聞こえる声と聞こえない声。組織の成長とともに、この非対称性は拡大していく。社内には案件のチャンネル、チームのチャンネル、技術のチャンネルが十分に整理されている。今後の発信はこれらの適切なチャンネルで行うことで、より効果的な情報共有を目指す。注意経済と生産性のパラドックス常時接続の環境は、注意力という有限の資源を巡る競争を生み出す。コミュニケーションの活性化が目的だったはずが、いつしかコミュニケーション自体が目的化する。リアクションの数が暗黙の評価軸となり、本来の価値創造から離れていく。組織内SNS化とでも呼ぶべきこの現象は、生産性向上のためのツールが生産性を阻害するという皮肉な結果を生む。心理的安全性の両義性カジュアルさは諸刃の剣である。フラットな対話を促進する一方で、境界線の曖昧さは時に傷を生む。デジタル空間に刻まれた言葉は、文脈を失いながら永続する。過去の自分が未来の自分を、あるいは他者を傷つける可能性を常に孕んでいる。組織構造をちゃんとやる最近読んだ『トリニティ組織』（矢野和男著）は、私の決断に理論的な確信を与えてくれた。組織の生産性と幸福度を決定づけるのは、人間関係の「形」だという。自分の知り合い2人同士も知り合いである「三角形の関係」が多い組織ほど、問題解決能力が高く、孤立も生まれにくい。トリニティ組織:人が幸せになり、生産性が上がる「三角形の法則」作者:矢野 和男草思社Amazontimesの構造について考えると、その限界が明確になる。発信者を頂点に、参加者が個別につながる形 ― これはまさに「V字型の関係」の量産装置である。私のチャンネルを見ているAさんとBさんが、そこでのやり取りを通じて直接つながることは稀だ。むしろ、それぞれが私との1対1の関係に終始する。リモートワーク環境下では、この構造的欠陥はより顕著になる。物理的な偶発的出会いが失われた今、意図的に「三角形」を作り出す仕組みが必要だ。しかし、個人チャンネルという形式は、その本質において中心化を促進し、分散化を阻害する。一方、チームチャンネルや技術雑談チャンネルでは、参加者同士が自然に相互作用する。同じ疑問に対して複数人が異なる視点でアドバイスし、そこから新たな議論が派生する。これこそが知識の三位一体化であり、創造性を高める組織の在り方だ。論理的思考の階層性がV字関係を生み出すという洞察も重要だ。分解と整理を基本とする思考フレームワークは、産業時代には機能したが、知識創造の時代には限界がある。生成AIによる知識の民主化が進む今、組織は階層的構造から、より有機的なネットワーク構造へと進化すべき時を迎えている。私の選択は、V字から三角形へのシフトである。個人の承認欲求を満たす場から、集合知が生まれる場へ。ネットワークのハブとしての自己から、ネットワークの一部としての自己へ。これは単なるツールの変更ではなく、組織内での存在様式の根本的な転換を意味している。時間という有限資源の配分問題個人チャンネルは「アテンション・エコノミー（注意の経済）」における構造的矛盾を抱えている。組織の成長に伴い、情報チャンネルは線形に増加するが、個人の処理能力は一定のまま。この非対称性は、必然的に選別と排除のメカニズムを生み出す。より深刻なのは、この選別が生む不可視の階層構造だ。物理的空間における排除は可視的だが、デジタル空間における排除は不可視でありながら、より根深い分断を生む。参加の自由が保証されているがゆえに、不参加や選択的参加が生む格差は個人の責任に帰されやすい。アテンション・エコノミーのジレンマ　〈関心〉を奪い合う世界に未来はあるか作者:山本 龍彦KADOKAWAAmazon心理的安全性のパラドックス個人チャンネルは心理的安全性を高めるために導入されながら、逆にそれを脅かす装置にもなりうる。これは、親密性と公開性の両立不可能性に起因する。親密な空間であるがゆえに生まれる無防備な発言は、公開空間であるがゆえに永続し、検索可能となる。私自身も経験したことだが、他者への批判を目撃することの疲弊は想像以上に大きい。社内SNS化した空間では、建設的批判と破壊的批判の境界が曖昧になりやすい。「事実と解釈を分ける」という個人的努力に依存する構造は、そもそも持続可能ではない。古参メンバーとしての責任組織の初期メンバーは、文化の形成者であると同時に、その変革の阻害要因にもなりうる。そこまで古参ではないが組織が急拡大しているので相対的に古参である。私の存在が、新しいメンバーにとっての見えない圧力になっていないか。私の発言が、本来生まれるべき多様な声を抑圧していないか。ここで重要なのは、timesの価値は世代や在籍期間によって大きく異なるという認識だ。若手や入社直後のメンバーにとって、timesは今でも有効なツールとして機能している。組織への順応過程において、インフォーマルな発信空間は心理的安全性を提供し、自己開示を通じた関係構築を促進する。新しいメンバーが組織文化を理解し、自分の居場所を見つけるための重要な装置として、その価値は否定できない。また、社長や事業部長といった経営層にとっても、timesは別の意味で価値を持つ。階層的な距離が生む心理的障壁を低減し、人間的な側面を共有することで組織全体の心理的安全性を高める効果がある。経営層の思考プロセスや日常的な悩みが可視化されることで、「雲の上の存在」から「同じ人間」へと認識が変わる。これは特に急成長する組織において、上下の分断を防ぐ重要な機能となりうる。しかし、長く在籍する中間層の一般社員である私の場合、その影響力は異なる性質を持つ。経営層のような明確な役割や責任に基づく発信ではなく、「古参であること」自体が生む見えない権威性が問題となる。この非対称性を自覚したとき、退場もまた一つの貢献となる。若手が自由に発信し、経営層との健全な対話が生まれる空間を守るためにも、中間層の古参は適切なタイミングで身を引く必要がある。個人の節度や自制に依存するシステムは、本質的に脆弱だ。構造的に承認欲求を刺激し、注意力を奪い、関係性を歪めるメカニズムの中で、個人の倫理にどこまで期待できるだろうか。むしろ、そうした個人的努力を不要とする構造へと移行することこそが、組織の進化ではないか。私のチャンネルには、長年の蓄積がある。試行錯誤の痕跡、成功と失敗の記録、人間関係の履歴。これらは個人にとっての財産であると同時に、組織にとっての負債にもなりうる。過去の堆積が未来の可能性を制約するとき、断捨離は創造的行為となる。生成AI時代における組織内コミュニケーション知識のオープン化は、組織の存在理由そのものを問い直している。もはや情報の独占や階層的な知識伝達では、価値創造は不可能だ。必要なのは、多様な視点が交差し、予期せぬ組み合わせが生まれる「場」の設計だ。個人チャンネルは、表面的には情報の民主化に貢献しているように見える。しかし実際には、情報の断片化と選択的可視性による新たな非対称性を生み出している。全体性の把握が不可能な状況下では、部分最適化が進行し、組織は分断される。これからの組織に必要なのは、個人の発信力ではなく、集団としての知識創造力だ。それは、中心化されたネットワークではなく、分散化されたメッシュとして知識が循環する仕組みから生まれる。個から全体へ、閉鎖から開放へ、所有から共有へ。この転換こそが、知識社会における組織の生存戦略となる。卒業という選択すべてのシステムには寿命がある。それを認めることは敗北ではなく、成熟の証である。私にとってのtimesは、その役割を終えた。これは組織の成長を祝福し、新しい段階への移行を受け入れる儀式でもある。個人チャンネルには組織の成長と反比例する有効性がある。規模の拡大は必然的にシステムの限界をもたらす。これは、あらゆる中心化されたネットワークが直面する普遍的な課題だ。組織の成長を喜びながら、その成長に適応できないシステムに固執することは、成長そのものを阻害する。興味深いのは、この空間から離脱した時に感じる「喪失感の不在」だ。むしろ、制約がもたらす創造性の向上を実感している。これは、無限の選択肢よりも適切な制約が人間の創造性を高めるという、古典的な原理の現れかもしれない。「代替可能性」という認識は重要だ。心理的安全性も、知識共有も、偶発的な創発も、すべて異なる構造で実現可能だ。むしろ、より持続可能で公平な形で。個人的な発信空間から、より構造化されたコミュニケーションチャンネルへ。この移行は、組織が次のフェーズに進むための必要な進化だと信じている。終わりに変化を恐れず、執着を手放し、新しい形を模索する。それが成長する組織の中で生きるということだ。私のこの選択は、単なる個人的な決断ではない。V字型の関係から三角形の関係へ、情報の独占から知識の循環へ、個人の承認欲求から集合知の創発へ。これは、知識社会が求める組織変革の、小さな、しかし確かな一歩だ。真の課題は特定のツールの有無ではなく、組織における関係性の質にある。健全な組織文化は、ツールを超えて、人と人との相互作用の中から生まれる。私のこの選択が、組織のコミュニケーション構造について考える一つのきっかけになれば幸いである。これまでの対話に感謝を込めて。そして、新しい形での再会を楽しみにしている。参考Slackのtimesのメリット・デメリットについて改めて考えてみる｜斎藤 雅史Slackの分報チャンネル使うのやめた - stefafafan の fa は3つですSlackの分報チャンネル使うのを再開していた - stefafafan の fa は3つですまたSlackでtimesを始めてしまった｜ばんくし分報を導入して3年経ったので振り返る - FRTKL]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fishのabbrを使ってコマンドライン上のトークンを複製する]]></title>
            <link>https://blog.atusy.net/2025/08/04/fish-abbr-commandline-token/</link>
            <guid>https://blog.atusy.net/2025/08/04/fish-abbr-commandline-token/</guid>
            <pubDate>Mon, 04 Aug 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[カーソル位置のn個前のトークンとか、先頭からn個目のトークンとかを展開できるとcpコマンドとかで便利！と思ってabbrを実装しました。,[+-]?[0-9]+というパターンに反応して、符号の有無と数値を基準に、うまいことトークンを展開できます。cp /path/to/file ,-1ってすると、,-1が/path/to/fileに展開されるイメージですね。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AI Experimentsを利用したMNISTモデル学習の実験管理]]></title>
            <link>https://zenn.dev/akasan/articles/1447fc369a5032</link>
            <guid>https://zenn.dev/akasan/articles/1447fc369a5032</guid>
            <pubDate>Sun, 03 Aug 2025 08:44:46 GMT</pubDate>
            <content:encoded><![CDATA[今回はVertex AIのExperimentsを利用して、MNISTの学習家庭を記録してみました。Vertex AI Experimentsを利用することでモデルの実験評価ができ、どのモデルをデプロイするのが良いかを判断する材料になります。 Vertex AI Experimentsとは？Vertex AI Experiments（以下、Experiments）とは、Google CloudのVertex AI上で提供されている実験管理ツールになります。Experimentsを利用することでモデルアーキテクチャからハイパーパラメータ、トレーニング結果まで幅広い情報を記録して分析す...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[一家に一台Anneliちゃん！LM Studio × AivisSpeechで簡易AIコンパニオンを作ろう]]></title>
            <link>https://zenn.dev/r4ynode/articles/local-llm-and-aivis-speech-anneli</link>
            <guid>https://zenn.dev/r4ynode/articles/local-llm-and-aivis-speech-anneli</guid>
            <pubDate>Sun, 03 Aug 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先日、LM Studioをダウンロードし、ローカルLLM環境を手に入れました。私はMacBookのバックグラウンドで常時稼働させています。詳しくはこちら↓https://zenn.dev/r4ynode/articles/local-llm-intro-with-obsidianまた、最近AivisSpeechという日本語音声合成エンジンを知りました。これがなかなか良くて、Anneliが可愛いだけでなく、合成音声の処理速度が速いです。https://aivis-project.com/ここでふと思いつきました。「完全ローカル環境で、自律的に動くAnneliちゃん作...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraformを利用してFastAPIサーバをCloud Runへデプロイ]]></title>
            <link>https://zenn.dev/akasan/articles/25766ce1ab7473</link>
            <guid>https://zenn.dev/akasan/articles/25766ce1ab7473</guid>
            <pubDate>Sat, 02 Aug 2025 08:15:52 GMT</pubDate>
            <content:encoded><![CDATA[今回はTerraformを利用してFastAPIサーバをCloud Runへデプロイしてみます。Terraformについて現在勉強中なので、まずは最低限の構成で試してみます。 今回作るシステム今回は以下の条件で検証します。IaCにはTerraformを採用FastAPIを利用してアプリケーションサーバを実装Google CloudArtifact RegistryにFastAPIのDockerイメージを格納Cloud Runにサーバをデプロイ それでは実装してみる FastAPIサーバの実装まずはFastAPIサーバを実装します。最初にuvを使って環...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[短編：普段使ってる自作コマンド達を紹介]]></title>
            <link>https://zenn.dev/akasan/articles/309acfb329c51d</link>
            <guid>https://zenn.dev/akasan/articles/309acfb329c51d</guid>
            <pubDate>Fri, 01 Aug 2025 13:39:32 GMT</pubDate>
            <content:encoded><![CDATA[今回は普段ターミナルで作業するときに活用している自作コマンド達を紹介しようと思います。なお、ここで定義しているコマンド達は一部claude codeなどの生成AIによって生成したものも含まれます。 自作コマンドの定義自作コマンドは~/.config/zsh/hogehoge.zshの形で設定し、~/.zshrcでまとめて読み込むように書いています。 コマンド達 ffmpeg編動画像を仕事で扱う時は時折ffmpegを利用していて、その中でよく利用するコマンドを別途定義しています。使い方は中盤-h/--helpの説明を参照してください。動画を一定間隔で分割するfunc...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[データサイエンティスト検定を受験してみた]]></title>
            <link>https://zenn.dev/akasan/articles/75c9d5cd4fdb2e</link>
            <guid>https://zenn.dev/akasan/articles/75c9d5cd4fdb2e</guid>
            <pubDate>Thu, 31 Jul 2025 12:35:44 GMT</pubDate>
            <content:encoded><![CDATA[今回は6月に受験していたデータサイエンティスト検定について共有しようと思います。 データサイエンティスト検定について 試験概要データサイエンティスト検定、略してDS検定は数理・データサイエンス・AI（リテラシーレベル）におけるモデルカリキュラムを総合し、実務能力と知識を有することを証明する試験です。試験範囲からもわかるようにいわゆる数学的な分野から法規的な部分まで幅広くデータサイエンスに求められる知識が問われるものとなっております。DS検定は年に数回受験することができ、私は2025年6月度の試験を受けました。https://www.datascientist.or.jp/d...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[BigQueryのパーティション分割テーブルでTIMESTAMPでエラーが出るときの理由]]></title>
            <link>https://zenn.dev/nedoko_dok0dko/articles/b6d8df76854c0a</link>
            <guid>https://zenn.dev/nedoko_dok0dko/articles/b6d8df76854c0a</guid>
            <pubDate>Thu, 31 Jul 2025 11:40:53 GMT</pubDate>
            <content:encoded><![CDATA[whatBigQueryでパーティション分割テーブルを作成する際、パーティショニングを設定することができるが、TIMESTAMPを利用しようとするとエラーが出る場合がある「公式ドキュメントでは設定できると記載があるが、エラーが出るのはなぜなのか?」 これについて調べてみたログ BigQueryのパーティショニングについてhttps://cloud.google.com/bigquery/docs/partitioned-tables?hl=jaBQでパーティションテーブルを作る際に、パーティショニングを設定する。これは、公式ドキュメントでは次の型から設定することがで...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DSPyを用いたAIエージェントの構築への入門]]></title>
            <link>https://zenn.dev/akasan/articles/a040961f463bee</link>
            <guid>https://zenn.dev/akasan/articles/a040961f463bee</guid>
            <pubDate>Wed, 30 Jul 2025 11:22:58 GMT</pubDate>
            <content:encoded><![CDATA[今回はDSPyを用いてAIエージェントを開発するチュートリアルを試してみたので共有します。DSPyは昨日公開したmlflow3のプロンプト最適化に関する機能のバックボーンとして利用されていたため、気になって調べた次第です。https://zenn.dev/akasan/articles/a1fec87efaa4a6 DSPyとは？前回の記事にも書きましたが、DSPyは以下のような特徴を持つライブラリとなります。モジュール型AIソフトウェアを構築するための宣言型フレームワークであり、脆弱な文字列ではなく構造化されたコードを高速に反復処理でき、AIプログラムを言語モデルに適した効...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[KubeCon Japan 2025] Composable Disaggregated Infrastructure(CDI)とは? Kubernetes基盤レイヤーでのHWリソース動的管理]]></title>
            <link>https://sreake.com/blog/kubecon-japan-2025-composable-disaggregated-infrastructure/</link>
            <guid>https://sreake.com/blog/kubecon-japan-2025-composable-disaggregated-infrastructure/</guid>
            <pubDate>Wed, 30 Jul 2025 05:12:48 GMT</pubDate>
            <content:encoded><![CDATA[2025年度の新卒エンジニアとして株式会社3-shakeに入社いたしました、荒木と申します。私はまだまだKubernetesの初学者であり、日々の学習を通じてスキルを向上させていきたいと考えています。 そんな折、先日、K […]The post [KubeCon Japan 2025] Composable Disaggregated Infrastructure(CDI)とは? Kubernetes基盤レイヤーでのHWリソース動的管理 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[KubeCon 2025 EU/JP] Kubernetes と 宇宙]]></title>
            <link>https://sreake.com/blog/kubecon-2025-eu-jp-kubernetes-and-universe/</link>
            <guid>https://sreake.com/blog/kubecon-2025-eu-jp-kubernetes-and-universe/</guid>
            <pubDate>Wed, 30 Jul 2025 02:04:28 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 宇宙は、人類にとって長年の探求対象であり、近年は特にKubernetesをはじめとするクラウドネイティブ技術によって、そのデータ処理とコンピューティングの方法が変革され、新たなフロンティアが拓かれつつあります。 […]The post [KubeCon 2025 EU/JP] Kubernetes と 宇宙 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[mlflow3のプロンプト最適化機能を利用してみた]]></title>
            <link>https://zenn.dev/akasan/articles/a1fec87efaa4a6</link>
            <guid>https://zenn.dev/akasan/articles/a1fec87efaa4a6</guid>
            <pubDate>Tue, 29 Jul 2025 12:26:15 GMT</pubDate>
            <content:encoded><![CDATA[今回は前回に引き続き、mlflow3で提供されているプロンプトの最適化機能を利用してみましたので紹介します。前回の記事は以下になりますので、合わせてみてもらえると嬉しいです。https://zenn.dev/akasan/articles/b37450993c3dce mlflow3のプロンプト最適化とは？本機能については以下のページにまとめられています。記事執筆時点ではまだExperimental機能として提供されています。説明によると、以下のような機能とのことです。mlflow.genai.optimize_prompt()を使用して、MLflowの統合インターフェース...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年夏 AIエージェントシステムに対する考え方]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/07/29/195608</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/07/29/195608</guid>
            <pubDate>Tue, 29 Jul 2025 10:56:08 GMT</pubDate>
            <content:encoded><![CDATA[はじめに正直に言って、AIエージェントを初めて理解しようとしたとき、私は完全に見当違いをしていた。単なる賢いチャットボットの延長線上にあるものだと思っていた。でも、実際に触れてみて驚いた。これは全く違う生き物だった。エージェントとは「行為者性（agency）」を持つ存在だ。つまり、ただ反応するだけじゃなくて、目的を持ち、意図的に行動し、経験から学習する自律的な存在だ。これって、ある意味で「生きている」ということに近いんじゃないだろうか。従来のソフトウェアを思い出してみる。入力に対して決まった出力を返す、予測可能な機械だった。でもAIエージェントは違う。確率的で、時に予想外の振る舞いを見せる。まるでデジタル世界に新しい種類の「生命」が誕生したかのような感覚を覚えることがある。私たちは今、Andrej Karpathyが言うところのSoftware 3.0の時代にいる。自然言語がプログラミング言語になり、プロンプトを書くことで複雑なタスクを実行できる時代だ。でも、この技術革新の中で、私が最も関心を持っているのは、エージェントシステムをどう設計し、どう制御し、どう共生していくかということだ。blog.riywo.comkarpathy.medium.com考えてみれば、人類の歴史は道具との共進化の歴史だった。石器が私たちの手を変え、文字が私たちの記憶を変え、インターネットが私たちの社会を変えた。そして今、AIエージェントが私たちの思考そのものを変えようとしている。 speakerdeck.comサピエンス全史　上　文明の構造と人類の幸福 (河出文庫)作者:ユヴァル・ノア・ハラリ河出書房新社Amazonこのブログが良ければ読者になったり、nwiizoのXやGithubをフォロワーしてくれると嬉しいです。では、早速はじめていきます。エージェントとは何か行為者性を持つデジタル存在エージェントを理解するには、まずその本質である「行為者性（agency）」を理解する必要がある。これは単に命令に従うだけではなく、自らの判断で行動を選択する能力だ。人間の秘書を思い浮かべてみてほしい。優秀な秘書は、単に言われたことをこなすだけじゃない。スケジュールを見て「この会議の前に資料の確認時間が必要ですね」と提案したり、「先方からの返信がまだですが、リマインドしましょうか」と気を利かせたりする。AIエージェントも同じような能力を持ち始めている。環境を認識し、目標を理解し、最適な行動を選択する。これは従来のプログラムとは根本的に違う。プログラムは「もしAならBをする」という決まったルールに従うが、エージェントは「この状況で目標を達成するには何をすべきか」を考える。エージェントの基本的な能力技術的に見れば、エージェントは大規模言語モデル（LLM）を基盤として動いている。でも、ここが面白いところで、彼らは単に反応するだけじゃない。環境を認識し、意思決定し、行動を実行するサイクルを自律的に回す。エージェントの環境認識能力は驚くほど幅広い。テキストはもちろん、画像、音声、構造化データなど、人間が理解できる情報ならほぼ何でも処理できる。例えば、スクリーンショットを見せて「このエラーを解決して」と言えば、画面の内容を理解し、エラーメッセージを読み取り、解決策を提案する。この能力により、人間とほぼ同じような方法で情報を受け取り、理解できるようになった。推論能力においては、エージェントは複雑な問題を人間の専門家のように段階的に分解して考える。「売上が下がっている原因を分析して」と言われたら、まず売上データを確認し、前期との比較を行い、変化があった要因を特定し、それぞれの影響度を評価する。この思考プロセスは、経験豊富なアナリストが行うアプローチとほとんど変わらない。そして行動実行能力により、エージェントは考えるだけでなく実際に行動できる。メールを送り、カレンダーに予定を入れ、データベースを更新し、レポートを作成する。これらの能力を組み合わせることで、単純なタスクから複雑なワークフローまで、幅広い業務を完遂できるようになった。zenn.dev非同期処理による新しい働き方エージェントの革命的な特徴の一つが非同期的な処理だ。これにより、人間の働き方が根本的に変わりつつある。従来は、タスクが発生したら人間がすぐに対応する必要があった。メールが来たら読んで返信し、レポートの依頼が来たら作成し、バグが報告されたら調査する。常に反応的で、割り込みに振り回される日々だった。でも、エージェントがいれば違う。メールが届いたとき、エージェントが内容を理解し、返信の下書きを用意してくれる。朝起きたら、すでに適切な返信案が準備されている。人間は内容を確認し、必要に応じて修正し、送信ボタンを押すだけだ。請求書の処理も変わる。以前は、請求書を受け取ったら内容を確認し、システムに入力し、承認フローに回す...という作業を人間がやっていた。今は、エージェントが請求書を読み取り、過去の取引と照合し、異常がなければ自動的に処理を進める。人間は例外的なケースだけを確認すればいい。24時間365日の継続的な監視も可能になった。人間には睡眠が必要だが、エージェントは休まない。システムの異常を検知し、初期対応を行い、必要に応じて人間にエスカレーションする。Microsoftが発表したAzure SRE Agentは、まさにこの概念を具現化したものだ。techcommunity.microsoft.comこれにより、人間は作業者から管理者へと役割が変わる。細かい作業はエージェントに任せ、人間は戦略的な判断や創造的な仕事に集中できる。文書処理の革命エージェントの最も実用的な強みの一つは、大規模文書の高速解析と構造化データ抽出だ。これは単なる要約機能を超えて、技術ブログのサンプルコードを実際に検証し、動作確認まで行う段階に進化しつつある。例えば、新しいライブラリやフレームワークの解説記事があったとき、エージェントはコードスニペットを自動抽出し、実行環境を構築してサンプルコードを検証、バージョン間の互換性問題や潜在的なエラーを検出し、「このコードは最新版では動作しないため、こう修正する必要があります」といった具体的なフィードバックを提供する。技術仕様書や契約書なら、数百ページの文書から技術要件、制約条件、リスク要因を構造化データとして抽出し、人間なら丸一日かかる作業を数分で完了する。技術調査においては、特定の技術トピックについて、公式ドキュメント、技術ブログ、コミュニティの議論から情報を収集し、実装例の動作検証、採用事例の分析、メリット・デメリットの整理、既存システムへの適用可能性の評価まで自動化できる。さらに、ブログで紹介されているアーキテクチャやベストプラクティスの実現可能性を、技術的複雑さ、運用負荷、チームのスキルセット、投資対効果の観点から多角的に検証し、「この技術を採用すべきか？」という意思決定に必要な判断材料を提供する。エージェントフレームワークを活用すれば、このような文書処理と検証のシステムは比較的短期間で構築可能だ。非構造化データの理解従来のシステムの最大の弱点は、決まった形式のデータしか扱えないことだった。CSVファイルやデータベースなら処理できるが、メールの文面や手書きのメモは理解できなかった。エージェントは違う。人間の自然な言葉をそのまま理解できる。「先週の会議で話した件について、関連する情報をまとめて」という曖昧な指示でも、会議の議事録を探し、関連するメールを見つけ、該当するドキュメントを特定し、coherentなサマリーを作成する。ソーシャルメディアの分析も得意だ。「うちの製品についての評判を調べて」と言えば、TwitterやRedditの投稿を分析し、ポジティブ・ネガティブな意見を分類し、改善点の示唆まで提供する。感情のニュアンスも理解するので、「不満はあるが期待している」といった複雑な感情も読み取れる。zenn.devマルチモーダルな理解最新のエージェントは、テキストと画像を統合した推論もできる。これが実務でどれだけ強力か、いくつか例を挙げてみよう。エラー画面のスクリーンショットを見せて「このエラーの原因は？」と聞けば、スタックトレース、エラーメッセージ、UIの状態から問題を特定し、解決策を提案する。「このNullPointerExceptionは、非同期処理の完了前にUIが更新されているためです。Promise.allで待機処理を追加してください」といった具体的なアドバイスを提供する。システム構成図やアーキテクチャ図を見せて「パフォーマンスのボトルネックは？」と聞けば、データフローやコンポーネント間の依存関係から潜在的な問題を指摘する。「このAPIゲートウェイに全てのトラフィックが集中しています」「データベースへの同期的なアクセスがレスポンス時間を悪化させています」など、設計上の改善点を提案する。モニタリングダッシュボードのスクリーンショットから異常を検出することも可能だ。CPU使用率、メモリ使用量、レスポンスタイムのグラフを見て、「午後3時頃からメモリリークの兆候が見られます」「このスパイクはデプロイのタイミングと一致しています」といった分析を行う。ホワイトボードに描かれたシステム設計をコードの雛形に変換したり、UIモックアップからReactコンポーネントを生成したりと、視覚的な情報を実装可能なコードに変換する能力も備えている。エージェントの自律性と責任観察・判断・実行のサイクルエージェントの本質的な能力は、「観察→判断→アクション」という自律的なサイクルを回せることだ。これは人間の専門家が行う思考プロセスと同じだが、エージェントは疲れることなく、24時間このサイクルを続けられる。システム運用の文脈で考えてみよう。エージェントはまず、メトリクス、ログ、イベントを継続的にモニタリングして環境を観察する。そこから異常パターンを検出し、原因を推論し、対応策を選定するという判断を下す。そして、自動修復やスケーリング、必要に応じたアラート送信といったアクションを実行する。このサイクルが高速で回ることで、人間では見逃しがちな微細な異常も早期に発見できる。さらに重要なのは、属人化の解消だ。特定の専門家しか判断できなかった複雑な問題も、エージェントなら一貫した品質で対応できる。自律性がもたらす問題ここで重要な問題に直面する。エージェントが自律的に何かを決定したとき、その責任は一体誰にあるのだろうか。ある企業でこんな事件があった。在庫管理エージェントが、過去のデータから需要を予測し、「最適」と判断して大量の商品を自動発注した。しかし、そのエージェントは季節的な要因を十分に考慮していなかった。クリスマス商戦の直後に、クリスマス用品を大量発注してしまったのだ。この責任は誰にある？エージェントを開発した会社？それを導入した企業？設定を行った担当者？あるいは、エージェント自身に責任能力を認めるべきなのか？法的にも倫理的にも、これは簡単に答えの出ない問題だ。でも、実務的には何らかの解決策が必要だ。zenn.devgenai.owasp.org監督された自律性（Supervised Autonomy）というアプローチ私の考えでは、「監督された自律性（Supervised Autonomy）」というモデルが現実的だと思う。これは、エージェントに自律性を与えつつ、人間が適切に監督・制御する仕組みだ。 speakerdeck.comこれはちょうど、見習いに仕事を任せる職人のようなものだ。基本的な作業は任せるが、重要な決定や最終チェックは師匠が行う。そして何より、最終的な責任は人間が持つ。例えば、顧客対応エージェントの場合。簡単な問い合わせには自動で回答するが、クレームや複雑な要求は人間にエスカレーションする。返金や補償の判断は必ず人間が行う。エージェントは提案はするが、最終決定は人間の承認が必要だ。タスクの重要度に応じた自律性レベルタスクの重要度に応じて自律性のレベルを変えることが重要だ。すべてを同じレベルで扱うのは危険だし、非効率でもある。最も基本的な完全自動レベルでは、定期的なレポート作成やデータのバックアップ、システムの監視といったルーチンタスクを完全に自動化する。これらは失敗してもリカバリ可能で、影響が限定的なタスクだから、エージェントに完全に任せても問題ない。一段階上の通知付き自動レベルでは、在庫の自動発注や定型的な顧客対応などを扱う。エージェントは自律的に行動するが、実行内容を人間に通知する。これにより、問題があれば人間がすぐに介入できる体制を保ちながら、日常業務の効率化を実現する。さらに重要度が高い作業には承認後実行レベルを適用する。大きな購入、重要な顧客への提案、システムの大幅な変更などがこれに該当する。エージェントは詳細な提案を作成するが、人間の明示的な承認なしには実行しない。これにより、エージェントの分析力を活用しながら、最終的な責任は人間が持つという体制を維持できる。最も慎重さが求められる場面では支援モードを使う。戦略立案、創造的な作業、倫理的判断が必要な場面では、エージェントは情報提供と提案に徹し、すべての判断と実行は人間が行う。これは、エージェントを優秀な助手として活用しながら、人間の判断力を最大限に活かすアプローチだ。責任の所在を明確にする仕組みエージェントシステムを運用する上で、責任の所在を明確にする仕組みが不可欠だ。まず、すべての決定とその理由を記録する必要がある。エージェントが何を根拠に、どんな判断をしたのか、使用したデータ、適用したルール、考慮した要因をすべて追跡可能にする。これは法的な保護のためだけでなく、システムの改善にも役立つ。透明性のある記録は、問題が起きたときの原因究明を容易にし、同じ過ちを繰り返さないための貴重な学習材料となる。次に、人間の承認プロセスを明文化することが重要だ。どのレベルの決定には誰の承認が必要か、緊急時の対応はどうするか、承認者が不在の場合の代理権限は誰にあるか。これらを事前に決めておくことで、責任の所在が曖昧になることを防げる。そして、定期的な監査とレビューを行う必要がある。エージェントの判断が適切だったか、人間の介入が必要だった場面はなかったかを月次でレビューし、必要に応じてルールを更新する。この継続的な改善プロセスが、システムの信頼性を高めていく。zenn.dev人間のフィードバックによる継続的改善エージェントは完璧じゃない。だからこそ、人間からのフィードバックを継続的に受け入れる仕組みが重要だ。「この判断は良かった」「これは違う」という評価を積み重ねることで、エージェントは人間の価値観を学んでいく。単純な正解・不正解だけでなく、「技術的には正しいが、ビジネス的には不適切」といった微妙なニュアンスも理解できるようにするように修正すべき。syu-m-5151.hatenablog.comブラックボックスを開けるなぜ透明性が必要かエージェントの「ブラックボックス」問題は、実は深刻だ。なぜその決定を下したのか分からないシステムを、どうやって信頼すればいいのか？実際にあった話を紹介しよう。ある投資会社で、AIの推奨に従って大量の株を購入した。AIは過去のパターンから「買い」と判断したが、前例のない政治的な出来事を考慮できなかった。結果は大損失。後から分析しても、なぜAIがその判断をしたのか、完全には理解できなかった。これは単なる技術的な問題じゃない。信頼の問題だ。人間は、理解できないものを信頼しにくい。特に、重要な決定に関わる場合はなおさらだ。透明性の3つのレベルここで重要なのは、単に技術的な透明性じゃなくて、「認識論的透明性」だと思う。つまり、人間が理解できる形で説明できること。私は透明性を三つのレベルで考えている。プロセスレベルの透明性エージェントがどんな手順を踏んだかを示すこと。どのツールを使い、どんな情報を参照し、どんな推論をしたか。例えば、市場分析を行うときには「まず過去3ヶ月の売上データを取得しました。次に競合5社の価格推移を調査しました。その後、季節要因を考慮して需要予測モデルを適用し、最後にこれらを総合して推奨価格を算出しました」というように、ステップバイステップで説明する。料理のレシピを見せるように、誰でも理解できる形で思考プロセスを開示することが重要だ。意図レベルの透明性そもそも何を達成しようとしているのかを明確にすること。同じデータを見ても、目的が違えば結論も変わる。売上データを分析するときを考えてみよう。「異常を検出するため」という目的なら、エージェントは外れ値や急激な変化に注目する。「成長機会を探すため」なら、上昇トレンドや相関関係に注目する。「リスクを評価するため」なら、ボラティリティや下降要因に注目する。同じデータでも、意図によってまったく異なる分析になるのだ。エージェントが「私は顧客満足度を最大化しようとしています」と言うのと「利益を最大化しようとしています」と言うのでは、全く違う行動につながる。この意図を明確にすることで、人間は適切な指示を出せる。限界の透明性これが意外と重要で、エージェントが「これはできません」「ここは自信がありません」と正直に言えることが、逆説的に信頼を生む。完璧を装うシステムより、「この分析は70%の確信度です。過去のデータが少ないため、精度に限界があります」と説明してくれる方が信頼できる。また、「為替の影響は考慮していません。必要であれば、金融専門エージェントと連携します」といった形で、自分の限界を認識した上で代替案を提示できることも重要だ。時には「このタスクは私の専門外です。他のエージェントに引き継ぐことを推奨します」と、適切に判断を委ねることも必要になる。医師が「わからない」と言える勇気を持つように、エージェントも自分の限界を認識し、それを伝える能力を持つべきだ。説明可能性の実装技術的には、エージェントの説明可能性を高めるいくつかのアプローチがある。Chain of Thought（思考の連鎖）は、エージェントに段階的に考えさせ、その思考過程を出力させる手法だ。「まず...次に...したがって...」という形で、論理的な流れを明示することで、人間がエージェントの推論を追跡できるようになる。関連性スコアの表示も有効だ。判断の根拠となった情報に、それぞれの重要度を数値で示す。「この要因が60%、この要因が30%、この要因が10%影響しました」といった形で、どの情報がどの程度判断に寄与したかを明確にする。反事実的説明は、「もし〜だったら、結果は変わっていた」という形で説明を提供する手法だ。「もし在庫が20%多かったら、値下げを推奨していました」というように、条件が変わった場合の結果を示すことで、現在の判断の妥当性を理解しやすくする。類似事例の提示も効果的だ。過去の似たケースを示して、判断の妥当性を説明する。「3ヶ月前の類似状況では、同じ判断をして成功しました」といった形で、経験に基づく判断であることを示すことができる。エージェントに魂を吹き込むなぜコンテキストが重要なのかここまでエージェントの自律性と透明性について話してきたが、これらを実現する上で最も重要な技術がコンテキストエンジニアリングだ。考えてみてほしい。どんなに優秀な人でも、状況がわからなければ適切な判断はできない。会議に途中から参加して「で、どう思う？」と聞かれても、答えようがない。背景、目的、制約条件...これらの文脈（コンテキスト）があって初めて、意味のある貢献ができる。エージェントも同じだ。どんなに高性能なLLMを使っていても、適切なコンテキストがなければ、的外れな回答しかできない。プロンプトエンジニアリングからコンテキストエンジニアリングへエージェントシステムの設計において、最も重要な概念の転換が起きている。それは「プロンプトエンジニアリング」から「コンテキストエンジニアリング」への進化だ。blog.langchain.comプロンプトエンジニアリングは、単一のタスクを最適な形式でLLMに伝える技術だった。まるで料理のレシピを完璧に書くようなものだ。「材料はこれとこれ、手順は1、2、3...」と明確に指示する。でも、実際の料理人の仕事を考えてみてほしい。その日の気温、湿度、食材の状態、お客様の好み、使える調理器具、時間の制約...これらすべてを考慮しながら、動的に判断していく。レシピは出発点に過ぎない。コンテキストエンジニアリングは、まさにこの動的な判断を可能にする技術だ。エージェントに、その時々で必要な情報とツールを、ちょうど良いタイミングで提供し続ける。エージェントが失敗する最大の原因は、適切なコンテキスト、指示、ツールがモデルに伝達されていないことだ。どんなに賢いエージェントでも、文脈なしには良い仕事はできない。コンテキストエンジニアリングは「デジタル世界の建築学」私は、コンテキストエンジニアリングを「デジタル世界の建築学」だと考えている。物理的な建築が空間を設計するように、コンテキストエンジニアリングは情報の空間を設計する。どの情報をどこに配置し、どのタイミングでアクセス可能にするか。どの情報同士を近くに置き、どれを遠ざけるか。良い建築が人の動線を自然に導くように、良いコンテキスト設計はエージェントの思考を自然に導く。必要な情報がすぐ手に入り、不要な情報に邪魔されない。これがエージェントの能力を最大限に引き出す。コンテキストエンジニアリングの4つの戦略コンテキストエンジニアリングの実践には、4つの基本戦略がある。これらは独立したものではなく、相互に関連し、組み合わせて使われる。Write（書き込み）戦略エージェントがタスクを実行する過程で得た情報や洞察を、コンテキストウィンドウの外部に保存する戦略だ。人間がメモを取るように、エージェントも重要な情報を記録する。でも、ただ記録するだけじゃない。未来の自分（または他のエージェント）が理解しやすい形で構造化することが重要だ。例えば、顧客分析を行ったときには、「顧客プロファイル：田中様」として、購買傾向は高品質志向でブランド重視、予算感は中〜高価格帯、過去のクレームとして配送遅延に敏感であること、そして推奨アプローチとして品質と信頼性を強調すべきことを記録する。このような構造化された記録があれば、次回の対応時に素早く文脈を把握できる。Select（選択）戦略必要な情報を動的に取得してコンテキストに追加する戦略だ。すべての情報を常に持ち歩くわけにはいかない。コンテキストウィンドウは有限のリソースだから。図書館で本を探すように、必要な時に必要な情報だけを取り出す。でも、何が「必要」かを判断すること自体が高度な能力を要求する。例えば、「新商品の価格設定」というタスクなら、競合商品の価格データ、ターゲット顧客の購買力データ、原価と利益率の情報、過去の類似商品の販売実績といった情報を選択的に取得する。一方で、在庫データや物流情報は、このタスクには不要なので取得しない。優れた選択は、ノイズを減らし、シグナルを増幅する。Compress（圧縮）戦略長大な会話履歴やツール出力を要約し、本質的な情報だけを保持する戦略だ。1時間の会議の議事録を、5つの決定事項と3つのアクションアイテムに圧縮する。100ページのレポートを、1ページのエグゼクティブサマリーにする。圧縮は単なる要約じゃない。それは情報の蒸留だ。ウィスキーを作るときのように、大量の原料から本質的なエッセンスだけを抽出する。何を残し、何を捨てるか。この判断が、圧縮の品質を決める。Isolate（分離）戦略複雑なタスクを小さな部分に分割し、それぞれに独立したコンテキストを提供する戦略だ。例えば、「新規事業の立ち上げ」という巨大なタスクは、市場調査、競合分析、事業計画作成、資金調達、チーム編成といったサブタスクに分割できる。それぞれに必要なコンテキストは違う。市場調査には業界データが必要だが、チーム編成には人材データが必要だ。一つの大きな混沌より、複数の小さな秩序の方が管理しやすい。分離は複雑さを飼いならす技術だ。コンテキストの種類と管理エージェントが扱うコンテキストは多様だ。それぞれが異なる性質を持ち、異なる管理方法を必要とする。指示とプロンプト：エージェントの憲法基本的な振る舞いを定義し、価値観を埋め込む。「顧客第一主義で行動する」「プライバシーを最優先する」といった根本的な指針。これらは頻繁に変更すべきじゃない。コロコロ変わる憲法では、一貫性のある行動ができない。でも、必要に応じて慎重に進化させる必要はある。会話履歴：短期記憶現在進行中の対話の文脈を保持する。「さっき言った件だけど」と言われたときに、何の話か理解できるようにする。でも、すべてを覚えている必要はない。人間だって、1週間前の雑談の詳細は覚えていない。重要なのは、関連性の高い情報を適切に保持すること。ツールの説明：能力カタログエージェントが使えるツールとその使い方を記述する。でも、ツールが増えすぎると選択が困難になる。人間の道具箱を考えてみてほしい。よく使う道具は手前に、たまにしか使わない道具は奥に。同じように、ツールも使用頻度や重要度で階層化する必要がある。作業メモリ：ワーキングスペース現在のタスク実行中の中間状態を保持する。複雑な計算の途中結果、仮説、検討中の選択肢など。人間が紙に計算式を書きながら問題を解くように、エージェントも作業メモリを使って思考を展開する。これがないと、複雑な推論ができない。長期記憶：経験の蓄積ユーザーの好み、過去の成功パターン、失敗から学んだ教訓。これらが積み重なることで、エージェントは単なるツールから、信頼できるパートナーへと成長する。でも、記憶も整理が必要だ。古い情報、間違った情報、もう関係ない情報...これらを適切に忘却することも、良い記憶管理の一部だ。コンテキストエンジニアリングの実践例実際の例を見てみよう。カスタマーサポートエージェントのコンテキスト設計だ。まず基本コンテキストとして、会社のサポートポリシー、製品の基本情報、よくある質問と回答を常に保持する。これらは変化が少なく、すべての対応で必要となる基礎的な情報だ。次に動的コンテキストとして、顧客の購入履歴、過去の問い合わせ履歴、現在のキャンペーン情報などを必要に応じて取得する。これらは状況や顧客によって変わる情報で、パーソナライズされた対応を可能にする。会話コンテキストはリアルタイムで更新される。現在の問い合わせ内容、顧客の感情状態、解決に向けた進捗などを追跡し、会話の流れに応じて適切な対応を選択できるようにする。最後に圧縮されたコンテキストとして、過去の類似ケースの要約や成功した解決パターンを保持する。これにより、新しい問題に直面しても、過去の経験から素早く解決策を導き出せる。この構造により、エージェントは適切な情報に基づいて、パーソナライズされた対応ができる。情報過多にもならず、情報不足にもならない。コンテキストエンジニアリングの未来コンテキストエンジニアリングは、今後さらに重要になっていく。エージェントが複雑化し、扱う情報が増えるにつれて、適切なコンテキスト管理がシステムの成否を分ける。将来的には、コンテキストエンジニアが独立した専門職として確立されるだろう。建築家が物理空間を設計するように、コンテキストエンジニアが情報空間を設計する時代が来る。そして、エージェント自身がコンテキストを最適化することも可能になるだろう。どの情報が有用で、どの情報が邪魔だったか。使用パターンから学習し、自動的にコンテキストを改善していく。でも、最終的な設計思想は人間が持つべきだ。何を重視し、何を優先するか。これは技術的な問題じゃなく、価値観の問題だから。実践的な設計アプローチ：MVAから始める最小実行可能エージェント（MVA）の思想ソフトウェア開発の世界で学んだ最大の教訓は「完璧を目指すな、まず動くものを作れ」ということだ。これをエージェントに応用したのがMVA（最小実行可能エージェント）の考え方だ。リーン・スタートアップ作者:エリック・リース日経BPAmazonMVAは単純さの美学だ。複雑さは敵であり、シンプルさは力だ。最初から全知全能のエージェントを作ろうとすれば、必ず失敗する。代わりに、一つのことを確実にできるエージェントから始める。例えば、最初は「FAQに答える」だけのシンプルなエージェントを作る。これが安定して動作し、ユーザーに価値を提供できることを確認する。そして重要なのは、実際のユーザーの使い方を観察することだ。開発者の想定と実際の使われ方は、しばしば大きく異なる。次に「過去の問い合わせを参照する」機能を追加する。これによってエージェントは文脈を理解し始める。さらに「簡単な問題を自動解決する」機能を追加する。こうして段階的に成長させていく。進化は革命より強い。小さな改善の積み重ねが、やがて質的な変化をもたらす。生物の進化と同じように、エージェントも環境との相互作用を通じて、より適応的な形へと変化していく。モジュラリティと責任の明確化エージェントシステムのモジュラリティは、単なる技術的な話じゃない。それは複雑さを管理し、理解可能性を保つための哲学的アプローチだ。優れたモジュール設計は、音楽のオーケストラに似ている。各楽器（モジュール）は独自の音色と役割を持ちながら、全体として調和のとれた音楽を奏でる。バイオリンがトランペットの役割を担おうとしても、良い音楽は生まれない。同様に、各モジュールは自分の責任に集中すべきだ。スキルモジュールは、エージェントの手足だ。特定の能力を提供し、実世界（デジタル世界）に働きかける。Web検索、データ分析、文書作成など、具体的なアクションを実行する。メモリモジュールは、エージェントの記憶装置だ。情報を記憶し、必要に応じて提供する。しかし、単なるストレージではない。記憶の整理、関連付け、忘却までを管理する、生きたシステムだ。プランニングモジュールは、エージェントの前頭葉だ。タスクを分解し、実行順序を決定し、リソースを配分する。複雑な問題に直面したとき、どこから手をつけるべきかを判断する知恵を提供する。重要なのは、各モジュール間でのコンテキストの受け渡し方法だ。必要な情報だけを共有し、不要な情報でコンテキストを汚染しない。これは組織におけるコミュニケーションと同じだ。すべての情報を全員に共有すれば、情報の洪水で溺れてしまう。失敗からの学習メカニズムエージェントも人間と同じで、試行錯誤を通じて成長する。重要なのは、失敗を恥じることではなく、失敗から学ぶことだ。Reflexionという手法は、この考え方を技術的に実装したものだ。エージェントが失敗したとき、単に「失敗した」で終わらせない。「なぜ失敗したんだろう？」と自問自答する。そして具体的な教訓を言語化して記録する。例えば、ユーザーの要求を文字通りに解釈しすぎて失敗したとする。「簡潔に」と言われたので重要な詳細を省略してしまい、かえって分かりにくくなった。この経験から、「簡潔さと完全性のバランスを取る」という教訓を学ぶ。失敗は教師であり、エラーは進化の原動力だ。完璧を求めて何もしないより、失敗を恐れずに挑戦し、そこから学ぶ方がはるかに価値がある。失敗から学ぶためには、適切なコンテキストの保存が不可欠だ。何を試みて、どんな結果になり、なぜそうなったのか。これらの情報を構造化して保存し、将来の意思決定に活用する。単なるログではなく、経験の結晶化だ。トイルの削減と自動化エージェントシステムの大きな価値の一つは、トイル（繰り返し作業）の削減だ。人間が何度も繰り返す単調な作業をエージェントに任せることで、より価値の高い仕事に集中できる。トイルとは、手動で行う繰り返し作業のことで、本来は自動化可能だが、まだ人間がやっているものを指す。これらは戦術的で長期的な価値を生まず、しかもサービスの成長に比例して作業量が増えていくという厄介な性質を持っている。毎朝のシステムチェック、定期レポートの作成、ルーチンのデータ整理などがその典型例だ。エージェントはこれらを学習し、自動化し、人間を解放する。しかし重要なのは、単に自動化するだけでなく、プロアクティブな改善も行うことだ。エージェントは作業を実行しながら、「もっと効率的な方法はないか」「このステップは本当に必要か」と考え、改善提案を行う。これにより、単なる作業の自動化を超えて、プロセス全体の最適化が実現される。マルチエージェントシステムとコンテキスト共有なぜマルチエージェントが必要か単一のエージェントですべてを処理しようとすると、すぐに限界が来る。これは人間の組織と同じだ。一人の天才より、専門性を持った複数の人が協力する方が、より大きな成果を生み出せる。実際、Claudeにはsub agentという機能が実装され、この考え方が現実のものとなった。sub agentは特定のタスクに特化したAIアシスタントで、それぞれが独自のコンテキストウィンドウを持ち、専門的な作業を効率的に処理できる。docs.anthropic.comblog.langchain.comsub agentの本質は、認知の分散化だ。人間の脳が異なる領域で異なる処理を行うように、エージェントシステムも専門性を持った複数のユニットが協調することで、より高度な知的活動を実現する。例えば、コードレビューを専門とするエージェント、デバッグを専門とするエージェント、データ分析を専門とするエージェントといった形で、それぞれが特定の領域に特化している。これは単なる作業の分担ではなく、異なる思考パターンの共存を意味する。sub agentの最大の利点はコンテキストの分離だ。メインの会話のコンテキストを汚染することなく、それぞれのタスクに集中できる。これは、人間が複雑な問題を解くときに、異なる視点を切り替えながら考えるのと同じだ。数学的に考えたり、直感的に考えたり、論理的に考えたりする、その切り替えをシステム的に実現している。さらに重要なのは、sub agentがプロアクティブに動作できることだ。これは、優秀なチームメンバーが指示を待たずに必要な作業を先回りして実行するのと同じだ。システムが成熟するにつれて、各エージェントは自分の役割を理解し、適切なタイミングで自律的に行動するようになる。しかし、マルチエージェントシステムの最大の課題は、各エージェントが適切なコンテキストを持つことだ。情報が不足していれば適切な判断ができないし、過剰な情報は混乱を招く。これはデジタル世界における「伝言ゲーム」問題だ。情報が伝達される過程で歪み、本来の意図が失われる。あるエージェントが「売上を分析して」と言われたとき、それは前四半期との比較なのか、競合との比較なのか、地域別の分析なのか。文脈が失われれば、的外れな分析になってしまう。効果的なコンテキスト共有の方法マルチエージェントシステムにおけるコンテキスト共有は、情報の交響曲を奏でるようなものだ。各エージェントが持つ情報が適切に共有され、調和することで、単独では不可能な成果を生み出す。sub agentシステムでは、各エージェントが独立したコンテキストウィンドウを持つことで、この理想に近づいている。メインのエージェントは全体の流れを把握し、各sub agentは自分の専門領域に深く潜る。この階層的なコンテキスト管理により、情報の混乱を防ぎながら、必要な深さの分析が可能になる。共有メモリパターンは、中央の図書館のようなものだ。重要な情報を一箇所に集め、各エージェントが必要に応じて参照する。しかし、すべての本を全員が読む必要はない。インデックスとメタデータが重要だ。何がどこにあるかを知ることで、必要な情報に素早くアクセスできる。メッセージパッシングは、手紙のやり取りのようなものだ。エージェント間で必要な情報だけを直接やり取りする。送り手は受け手が何を必要としているかを理解し、適切にパッケージングする必要がある。良いメッセージは、短く、明確で、行動可能だ。ハンドオフプロトコルは、リレーのバトンパスのようなものだ。タスクを引き継ぐ際に、これまでの経緯、現在の状態、次にすべきことを明確に伝える。単に「これをやって」ではなく、「なぜこれが必要で、今までに何を試みて、どんな制約があるか」を伝える。優れたハンドオフは、シームレスな継続を可能にする。sub agentの登場により、このコンテキスト共有はより洗練されたものになった。各エージェントが自分の文脈を保持しながら、必要な情報だけを交換する。これは、専門家チームが効率的に協働する理想的な形に近い。Sub Agentという思想sub agentの設計思想は、専門性と責任の明確化にある。これは単なる機能分割ではなく、認知の本質に関わる深い洞察を含んでいる。人間の思考を観察すると、私たちは常に異なる「モード」を切り替えながら考えている。分析的に考えるとき、創造的に考えるとき、批判的に考えるとき、共感的に考えるとき。これらは同じ脳の中で起きているが、それぞれ異なる神経回路が活性化している。sub agentは、この認知の多様性をシステム的に実現する試みだ。各エージェントは、特定の「思考の型」を体現する。それは単に異なるタスクを実行するのではなく、異なる視点から世界を見る。例えば、品質を重視する視点、効率を重視する視点、セキュリティを重視する視点、ユーザビリティを重視する視点。これらは時に対立することもあるが、その対立こそが健全な判断を生む。一つの視点に偏ることなく、多面的な検討が可能になる。さらに深い意味で、sub agentは分散化された知性の実験でもある。単一の巨大な知性ではなく、専門化された複数の知性が協調することで、より柔軟で適応的なシステムを作る。これは、生物の進化が単細胞から多細胞へと進んだプロセスにも似ている。各sub agentは、限定された権限と視野を持つ。しかし、その限定こそが深い洞察を可能にする。すべてを見ようとすれば何も見えない。特定の側面に集中することで、その領域の微細な変化や重要なパターンを捉えることができる。Sub Agentの協調と創発さらに高度な使い方として、複数のsub agentを連鎖的に協調させることもできる。これは、異なる専門性を持つエージェントが、より大きな目標に向かって協力するプロセスだ。問題を発見する視点、原因を分析する視点、解決策を実装する視点、結果を検証する視点。これらが順番に、あるいは同時並行的に働くことで、単一のエージェントでは不可能な深い問題解決が可能になる。これは現実の知的労働のプロセスと同じだ。研究者が仮説を立て、実験者がそれを検証し、分析者が結果を解釈し、著述者がそれを文書化する。各段階で異なる思考様式が必要であり、それぞれに特化したエージェントが最適な処理を行う。興味深いのは、このような協調から予期しない創発的なパターンが生まれることだ。あるエージェントの出力が、別のエージェントにとって新しい視点を提供し、それがさらに第三のエージェントの創造的な解決策につながる。これは計画されたものではなく、システムの中から自然に生まれる知性だ。現在のsub agentシステムは、このような高度な協調の第一歩に過ぎない。しかし、すでに小規模な創発現象は観察されている。複数の専門性が交差する点で、新しい洞察が生まれる瞬間を目撃することができる。Sub Agentの設計哲学sub agentを効果的に活用するには、いくつかの重要な設計哲学がある。まず、単一責任の原則だ。各エージェントは一つの明確な責任を持つべきで、その責任に完全に集中する。これは単純化のためではなく、深い専門性を実現するためだ。浅く広い知識より、狭く深い専門性の方が、実際の問題解決では価値がある。次に、最小権限の原則が重要だ。各エージェントには、その役割を果たすために必要な最小限の権限だけを与える。これはセキュリティの観点だけでなく、認知的な明確さのためでもある。限定された権限は、限定された責任を意味し、それが明確な思考につながる。文脈依存の自律性も重要な概念だ。エージェントは、適切な文脈で自動的に起動し、自律的に行動する。しかし、この自律性は無制限ではない。明確に定義された境界の中で、最大限の自由を発揮する。これは、信頼できる専門家に仕事を任せるときの原則と同じだ。継続的な進化も忘れてはいけない。sub agentは静的な存在ではなく、使用を通じて進化する。フィードバックを受け、パフォーマンスを改善し、新しい状況に適応する。これは、生きたシステムとしてのエージェントの本質を表している。最後に、協調的な独立性という一見矛盾した概念が重要だ。各エージェントは独立して動作するが、より大きな目標に向かって協調する。オーケストラの各楽器が独立した音を出しながら、全体として美しい音楽を奏でるように。創発的な振る舞いへの対処マルチエージェントシステムの魅力的な特性として、個々のエージェントの単純な相互作用から、予想外の複雑なパターンが生まれることがある。これを創発と呼ぶ。創発は自然界でも見られる現象だ。アリの群れが複雑な巣を作り、鳥の群れが美しい編隊を組む。個々のアリや鳥は単純なルールに従っているだけなのに、全体として驚くべき知性を示す。sub agentシステムにおいても、各エージェントが自分の専門領域で最善を尽くすことで、予想外の相乗効果が生まれることがある。あるエージェントの洞察が、別のエージェントにとって新しい視点となり、それがさらに第三のエージェントの創造的な解決策を触発する。この創発は、計画された協調を超えた何かだ。設計者が意図しなかった、しかし有用な振る舞いが自然に生まれる。それは、異なる専門性が交差する境界で起きる化学反応のようなものだ。重要なのは、創発的な振る舞いを観察し、評価し、必要なら介入する仕組みを持つことだ。創発は素晴らしいイノベーションを生むこともあれば、システムを不安定にすることもある。賢明な庭師のように、成長を見守りながら、必要に応じて剪定する。現段階では、エージェント間の予期しない協調パターンを観察し、それが価値を生んでいれば、新しい標準的なワークフローとして定式化するアプローチが有効だ。偶然の発見を意図的な設計に昇華させることで、システムの能力を着実に向上させることができる。sub agentシステムは、より大規模で複雑な創発現象への第一歩だ。個々の専門性が保たれながら、全体として新しい知性が生まれる可能性を秘めている。エージェントたちの民主的意思決定なぜサンガが必要かエージェントシステムが成長し、自己改善能力を持つようになると、根本的な問題に直面する。「誰が何を決めるのか」という問題だ。コード・ブッダ　機械仏教史縁起 (文春e-book)作者:円城 塔文藝春秋Amazon現在のsub agent機能では、人間が各エージェントの役割と権限を定義している。しかし、将来的にエージェントがより自律的になったとき、エージェント同士が協調して意思決定する仕組みが必要になるかもしれない。中央集権的な制御では柔軟性に欠ける。一人の独裁者がすべてを決めるシステムは、その独裁者の限界がシステムの限界になる。一方、完全な自律では暴走のリスクがある。各エージェントが勝手に判断すれば、システム全体の一貫性が失われる。サンガ（Sangha）は、この二つの極端の間にある第三の道だ。仏教用語で「僧侶の共同体」を意味するこの言葉を、私はエージェントシステムの集団意思決定機構として再定義した。ただし、これはまだ実験的な概念であり、実装には多くの技術的・倫理的課題が残されている。github.comサンガはデジタル民主主義の実験場だ。エージェントたちが議論し、投票し、合意を形成する。人間の民主主義が何世紀もかけて洗練させてきた知恵を、デジタル世界に実装する試みだ。現状では、sub agentのような仕組みで十分かもしれない。しかし、エージェントの能力が向上し、より複雑な協調が必要になったとき、サンガのような民主的な意思決定機構が重要になる可能性がある。サンガの基本機能サンガは生きた組織だ。固定的なルールに縛られるのではなく、状況に応じて進化する。以下は、将来的に実現可能かもしれない機能の構想である。議題提案の機能により、どのエージェントも改善提案や新しいルールの制定を提案できる。これはイノベーションの民主化だ。良いアイデアは、どこから来てもおかしくない。新人エージェントの新鮮な視点が、システム全体を変革することもある。議論の過程では、各エージェントが専門的観点から意見を述べる。フロントエンドエージェントはユーザビリティの観点から、セキュリティエージェントは安全性の観点から、パフォーマンスエージェントは効率性の観点から。多様な視点の衝突が、より良い解決策を生む。投票と決定のプロセスは、単なる多数決ではない。議論の質、提案の実現可能性、潜在的なリスクなど、多面的な評価を経て決定される。時には少数意見が正しいこともある。重要なのは、決定プロセスの透明性と、結果への責任だ。実装と遵守の段階では、決定事項が全エージェントによって実行される。しかし、盲目的な服従ではない。実装の過程で問題が見つかれば、それをフィードバックする仕組みがある。サンガは学習する組織だ。サンガがもたらす価値以下は、サンガが実現した場合に期待される価値である。現時点では検討段階にある。サンガによる意思決定は、単なる効率化のツールではない。それはエージェントシステムに魂を吹き込む仕組みだ。集合知の活用により、個々のエージェントの限界を超えた判断が可能になる。一人の専門家より、多様な専門家の協議の方が、より包括的な視点を提供する。しかし、これは単なる知識の足し算ではない。相互作用により、新しい洞察が生まれる。透明性の確保は、信頼の基盤だ。すべての決定プロセスが記録され、後から検証可能になる。なぜその決定がなされたのか、どんな議論があったのか、誰がどんな意見を述べたのか。歴史を持つシステムは、未来を持つシステムだ。柔軟な進化により、環境の変化に適応できる。固定的なルールは、変化する世界では足枷になる。サンガは、必要に応じてルールを更新し、新しい状況に対応する。生き残るのは最も強い種ではなく、最も適応力のある種だ。正統性の維持は、システムの安定性につながる。独裁的な決定は反発を生むが、民主的な決定は受け入れられやすい。たとえ自分の意見が通らなくても、公正なプロセスを経た決定なら従いやすい。プロセスの正統性が、結果の正統性を生む。しかし、これらを実現するには、まだ多くの技術的・倫理的課題を解決する必要がある。現時点では、sub agentのような実装可能な技術を活用しながら、将来の可能性を模索している段階だ。エージェントとの共進化人間の役割の変化エージェントシステムの発展は、人間の役割を根本的に変える。しかし、それは置き換えではなく、能力の拡張と役割の進化だ。かつて、計算機の登場で人間は計算から解放され、より高度な数学的思考に集中できるようになった。同様に、エージェントの登場で人間はルーチンワークから解放され、より創造的で戦略的な仕事に集中できる。トイルからの解放は、単に楽になるということではない。それは人間の潜在能力を解き放つことだ。定期レポートの作成、データ入力、ルーチンのチェック作業...これらに費やしていた時間を、新しいアイデアの探求、イノベーションの推進、人間関係の構築に使える。人間の新しい役割の一つは、意図の設計者だ。何を達成したいかを明確に定義し、それをエージェントが理解できる形で表現する。これは単なる命令ではない。ビジョンを描き、価値観を埋め込み、方向性を示すことだ。もう一つの重要な役割は、倫理的判断者だ。技術的に可能なことと、すべきことは異なる。エージェントは効率的な解を見つけられるが、それが正しい解かどうかは人間が判断する必要がある。できることとすべきことの間にある深淵を橋渡しするのが、人間の責任だ。そして、創造的探索者としての役割も重要だ。エージェントは既知のパターンを学習し、最適化できる。しかし、真に新しいアイデア、パラダイムシフトを起こすような発想は、人間の領域に留まる。エージェントが思いつかない問いを投げかけ、新しい可能性を探索する。このように、エージェントの進化は人間を不要にするのではなく、人間をより人間らしくする。機械的な作業から解放され、創造性、共感、戦略的思考といった、人間固有の能力を最大限に発揮できるようになる。コンテキストエンジニアリングの進化コンテキストエンジニアリングは、今後さらに重要性を増していく。エージェントシステムが複雑化するにつれ、適切なコンテキスト管理がシステムの成否を分ける決定的な要因となる。将来的には、コンテキストエンジニアリングが独立した専門分野として確立されるだろう。建築家が物理的な空間を設計するように、コンテキストエンジニアが情報の空間を設計する。どの情報をどこに配置し、どのように流通させ、どのタイミングでアクセス可能にするか。これらの設計が、エージェントシステムの性能を左右する。コンテキストエンジニアは、情報の詩人でもある。大量の情報を、エージェントが理解しやすい形に編集し、構造化する。不要な情報を削ぎ落とし、本質を浮かび上がらせる。それは科学であると同時に芸術でもある。また、コンテキストエンジニアリングは動的な分野だ。エージェントの能力が向上すれば、より高度なコンテキスト管理が可能になる。新しいツールや手法が開発され、より効率的で効果的な方法が生まれる。常に学び続け、進化し続ける必要がある。エージェント向けの世界設計Software 3.0の時代では、世界そのものがエージェント向けに再設計される必要がある。これまで人間向けに作られてきたインターフェースやシステムが、エージェントフレンドリーなものへと進化していく。llmstxt.orgこれは単なる技術的な変更ではない。世界観の転換だ。道路が自動車のために設計されたように、デジタル世界もエージェントのために設計される。しかし、それは人間を排除することではない。むしろ、人間とエージェントが共に生きやすい世界を作ることだ。例えば、ウェブサイトは人間が読むためのHTMLと、エージェントが理解するための構造化データの両方を提供する。APIは人間の開発者にとって使いやすく、同時にエージェントが自動的に理解し利用できるように設計される。情報のアクセシビリティも重要だ。視覚障害者のためのスクリーンリーダー対応と同じように、エージェントのための情報アクセシビリティが標準となる。すべての情報が、エージェントにとって発見可能で、理解可能で、利用可能になる。この変化は、新しい仕事や産業を生み出す。エージェント向けのコンテンツ作成、エージェント体験の設計、エージェントと人間の仲介など。エージェントエコノミーとでも呼ぶべき新しい経済圏が形成される。さいごにAIエージェントシステムの設計において最も重要なのは、コンテキストエンジニアリングを中心に据えた実践的なアプローチだ。それは単なる技術的な手法ではなく、エージェントに魂を吹き込む芸術だ。MVAから始め、段階的に機能を追加し、適切なコンテキスト管理を行う。小さく始めて大きく育てる。これは自然の摂理に従った、最も確実な成長の道だ。マルチエージェントシステムでは、効果的なコンテキスト共有の仕組みを設計する。情報の交響曲を奏でるように、各エージェントの知識と能力を調和させる。そして、サンガのような民主的意思決定機構により、個の成長と全体の調和のバランスを保つ。技術は急速に進化している。しかし、人間中心の設計思想と段階的な実装アプローチは今後も有効だ。そして何より、適切なコンテキスト管理こそが、エージェントシステムの成功の鍵となる。www.oreilly.comプログラミングの定義は変わりつつある。コードを書くことから、意図を設計することへ。命令することから、協働することへ。しかし、良い意図を持ち、それを適切に表現し、システムに実装する能力の価値はむしろ高まっている。私たちは今、人間とAIが真に協働する新しい時代の入り口に立っている。エージェントは道具であると同時に、新しい形の知的存在でもある。この両面性を理解し、適切に設計し、共に成長していくことが、これからの私たちの課題だ。現実的には、sub agentのような実装可能な技術から始めて、段階的に高度な協調メカニズムへと進化させていくことになるだろう。サンガのような民主的意思決定機構は、まだ実験的な概念だが、エージェントシステムの未来の一つの可能性を示している。エージェントとの共進化は、人類の次なる進化かもしれない。それは生物学的な進化ではなく、文化的、知的、そして精神的な進化だ。私たちがエージェントを育て、エージェントが私たちを高める。この相互作用の中で、両者とも今まで到達できなかった高みへと昇っていく。未来は不確実だ。しかし、一つ確かなことがある。私たちが作るエージェントシステムが、私たちの未来を形作るということだ。だからこそ、慎重に、思慮深く、そして希望を持って、この新しい世界を設計していく必要がある。現実的な技術と理想的な概念の両方を視野に入れながら、将来像を考えながらバランスの取れた発展を目指すべきだ。技術的に可能なことと、倫理的に望ましいことの間で、常に適切な判断を下していく必要がある。これが2025年夏の、私のAIエージェントシステムに対する考え方だ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenLIT解説：Get started with Guardrails]]></title>
            <link>https://zenn.dev/akasan/articles/6232fddf6a0e71</link>
            <guid>https://zenn.dev/akasan/articles/6232fddf6a0e71</guid>
            <pubDate>Mon, 28 Jul 2025 11:39:02 GMT</pubDate>
            <content:encoded><![CDATA[今回は前回に引き続き、OpenLITのチュートリアルの一つである、Guardrails機能を使ってみます。前回の記事は以下になるので、合わせてみてくれると嬉しいです。https://zenn.dev/akasan/articles/20e3fd8a87c44a Guardrailsとは？LLMの文脈におけるGuardrailsとは、ユーザがLLMに対して不適切な動作をさせるようなプロンプトを指定した時に、それを検知し推論をさせないようにする機能のことです。例えば個人情報などを扱うようなAIエージェントを利用するときに悪意のあるプロンプトによってその情報が引き出されるようなケースを...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ローカルLLM入門！LM Studioをバックグラウンドで実行しObsidianと連携する]]></title>
            <link>https://zenn.dev/r4ynode/articles/local-llm-intro-with-obsidian</link>
            <guid>https://zenn.dev/r4ynode/articles/local-llm-intro-with-obsidian</guid>
            <pubDate>Sun, 27 Jul 2025 09:00:01 GMT</pubDate>
            <content:encoded><![CDATA[はじめに生成AI盛り上がってますね。私は置いていかれています。そんな私、奇遇なことに30コアGPUを積んだMacBookを持っているではないですか。本当は最近キラキラなAI（Devin, Claude Codeなど）を使いたいのですが、時代に逆行してローカルLLMに入門してみます。この記事では以下のことをします。LM Studioに入門Obsidianと連携LM Studio CLIを使ってバックグラウンドで実行一応、ObsidianというのはMarkdownのノートアプリです。本記事では詳しく解説しません。 デモ本記事の手順を最後まで実施すると、LM St...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[コマンド紹介シリーズ：act]]></title>
            <link>https://zenn.dev/akasan/articles/6392d28e0e02f0</link>
            <guid>https://zenn.dev/akasan/articles/6392d28e0e02f0</guid>
            <pubDate>Sun, 27 Jul 2025 05:14:37 GMT</pubDate>
            <content:encoded><![CDATA[今回はactを紹介します。actを利用することで、ローカル環境でGitHub Actionsのワークフローを動かすことができます。なお、第12回は以下になりますので、ぜひご興味があればご覧ください。https://zenn.dev/akasan/articles/0fafdc64b8ae1c actとは？actはGitHub Actionsをローカルで動かすことを目的としたものとなります。GitHub Actionsを使ったことならわかると思いますが、動作テストをするためにはレポジトリにpushした後に実際にワークフローを動かす必要があります。検証用の内容などならまだいいですが...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Auth0を用いたセキュアなFastAPIサーバの実装について]]></title>
            <link>https://zenn.dev/akasan/articles/2f574b0c8e2550</link>
            <guid>https://zenn.dev/akasan/articles/2f574b0c8e2550</guid>
            <pubDate>Sat, 26 Jul 2025 10:33:27 GMT</pubDate>
            <content:encoded><![CDATA[今回は、Auth0を用いてFastAPIサーバをセキュアに構築する方法について共有します。 Auth0とは？Auth0は認証認可を簡単に実装するために利用できるプラットフォームであり、ECサイトからアプリケーション開発者まで幅広く利用されています。様々なアプリケーションに簡単に組み込むことができる上に、実現できるセキュアな状態のレベルが高いことから、多くの場面で利用されていることを目にします。https://auth0.com/ 今回やりたいこと今回はFastAPIサーバを立てる時に、Auth0を用いてアクセス制限等の認証認可を実現したいと思い調べていたところ、以下のチュー...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ついに記事数が100到達したので振り返り]]></title>
            <link>https://zenn.dev/akasan/articles/8fb26bfd1fe732</link>
            <guid>https://zenn.dev/akasan/articles/8fb26bfd1fe732</guid>
            <pubDate>Fri, 25 Jul 2025 14:05:54 GMT</pubDate>
            <content:encoded><![CDATA[今回はこの記事で100記事目になりましたので、ただただ振り返ります。 そもそもなんで記事書いているの？私はセルフエンドレスアドベントカレンダーという呼び名で実施しており、それのためにひたすら記事を毎日書いております。記事の概略についてはこちらを参考にしてください。https://zenn.dev/akasan/articles/4aba4d3a0616ce 記事を書き続けてどう変わった？自分がこのアドベントカレンダーを初めて生活がどう変わったかまとめてみます。毎日記事を書くのに時間がかかるので、時間の管理がちょっとだけうまくなった気がする毎日何を書くか考える必要がある...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenLIT解説：Get started編]]></title>
            <link>https://zenn.dev/akasan/articles/20e3fd8a87c44a</link>
            <guid>https://zenn.dev/akasan/articles/20e3fd8a87c44a</guid>
            <pubDate>Thu, 24 Jul 2025 13:43:58 GMT</pubDate>
            <content:encoded><![CDATA[今回はOpenLITについて、Get startedを試してみたので共有しようと思います。 OpenLITとは？OpenLITは、Generative AIとLLMにおけるAI開発ワークフローを簡素化するための仕組みを提供するものとなります。LLMの実験やプロンプトの整理とバージョン管理、APIキーのセキュアな取り扱いといった様々な重要なタスクを効率化して運用することができます。また、OpenTelemetryネイティブの可観測性を実現し、LLM、ベクターデータベース、GPUを含むフルスタックのモニタリングを実現でき、開発者はAI機能やアプリケーションを構築し、テストから本番環境へ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Apache Commons Numbersとはなんなのか？]]></title>
            <link>https://zenn.dev/akasan/articles/686bcca556decf</link>
            <guid>https://zenn.dev/akasan/articles/686bcca556decf</guid>
            <pubDate>Wed, 23 Jul 2025 12:12:11 GMT</pubDate>
            <content:encoded><![CDATA[今回はApache Commons Numbersについて調べてみました。 今回も以下のツールを使って対象プロジェクトを決めました！https://zenn.dev/akasan/articles/7e30ad266c02c4※ 本企画に関する記事の目的は、それぞれのプロジェクトを本格的に深ぼるのではなく、プロジェクト名⇆どんな内容かをパッと思い出せるようにすることを目指します！※ とはいえ深ぼってみたいプロジェクトがあればどんどん複数連載になると思います。 Apache Commons Numbersとは？公式サイトによると、Apache Commons Numbers...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[転職したらAWS MCPサーバーだった件]]></title>
            <link>https://speakerdeck.com/nwiizo/zhuan-zhi-sitaraaws-mcpsabadatutajian</link>
            <guid>https://speakerdeck.com/nwiizo/zhuan-zhi-sitaraaws-mcpsabadatutajian</guid>
            <pubDate>Wed, 23 Jul 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[「 転職したらMCPサーバーだった件」というタイトルで登壇したことがある。本日は「JAWS-UG SRE支部 #13 つよつよSREの秘伝のタレ」というなんとなく強そうなイベントで登壇しました。🔍 イベント詳細:- イベント名: JAWS-UG SRE支部 #13 つよつよSREの秘伝のタレ- 公式URL: https://jawsug-sre.connpass.com/event/358781/- ハッシュタグ: https://x.com/search?q=%23jawsug_sre&f=live- 参考資料①: https://speakerdeck.com/nwiizo/zhuan-zhi-sitaramcpsabadatutajian]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Zennでブログを書くのに使っているツールの紹介]]></title>
            <link>https://zenn.dev/akasan/articles/9e0a6f1e0d0f1a</link>
            <guid>https://zenn.dev/akasan/articles/9e0a6f1e0d0f1a</guid>
            <pubDate>Tue, 22 Jul 2025 12:23:42 GMT</pubDate>
            <content:encoded><![CDATA[今回は、Zennでブログを書くときにどのようなツールを使っているか紹介しようと思います。 GitHub私はZennの記事をGitHub連携で管理しています。GitHub連携を用いると、Gitレポジトリにて記事を管理し、特定のブランチにPushされたら差分を反映させるということができるようになります。この機能を有効化するとZenn上では記事の編集ができなくなってしまいますが、GitHubで管理すると過去の記事の修正なども変更が終えるのでとてもいいかなと思います。連携方法は以下を参照ください。https://zenn.dev/zenn/articles/connect-to-gith...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【開催報告】2025年夏ワークショップ「クラウドネイティブ技術を体験しよう！」を開催しました]]></title>
            <link>https://sreake.com/blog/2025-summer-workshop-report/</link>
            <guid>https://sreake.com/blog/2025-summer-workshop-report/</guid>
            <pubDate>Mon, 21 Jul 2025 23:00:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 2025年7月3日（木）・4日（金）の2日間、株式会社スリーシェイクの夏ワークショップを開催しました。今回は13名の学生の皆さんにご参加いただき、クラウドネイティブ技術の世界を体験していただきました。 📸 ワー […]The post 【開催報告】2025年夏ワークショップ「クラウドネイティブ技術を体験しよう！」を開催しました first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Development KitとAgent Engineを使ってVertex AI Agent Builderに入門してみる]]></title>
            <link>https://sreake.com/blog/vertex-ai-agent-builder-with-agent-development-kit-and-agent-engine/</link>
            <guid>https://sreake.com/blog/vertex-ai-agent-builder-with-agent-development-kit-and-agent-engine/</guid>
            <pubDate>Mon, 21 Jul 2025 22:00:00 GMT</pubDate>
            <content:encoded><![CDATA[1. 概要 本記事では、Googleが提供するAgent Development Kit (ADK) とAgent Engineを利用して、AIエージェントの構築方法を紹介しつつ、Vertex AI Agent Buil […]The post Agent Development KitとAgent Engineを使ってVertex AI Agent Builderに入門してみる first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[argocdに入門してみた]]></title>
            <link>https://zenn.dev/akasan/articles/e43105fe5175d3</link>
            <guid>https://zenn.dev/akasan/articles/e43105fe5175d3</guid>
            <pubDate>Mon, 21 Jul 2025 06:26:04 GMT</pubDate>
            <content:encoded><![CDATA[今回はGetting Startedを通してArgoCDに入門してみました。 ArgoCDとは？ArgoCDとは公式ページによるとArgo CD is a declarative, GitOps continuous delivery tool for Kubernetes.ということで、Kubernetes用の宣言型のGitOps継続的デリバリーツールということです。GitOpsとは、Gitレポジトリを信頼できるソースとして取り扱い、Gitレポジトリ上での変更に対して環境を構築するというOpsになります。ArgoCDはGitOpsのうち特にKubernetes環境に特化し...]]></content:encoded>
        </item>
    </channel>
</rss>