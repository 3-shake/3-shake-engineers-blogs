<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sun, 08 Dec 2024 22:34:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[ KubeCon NA 2024: The Future of DBaaS on Kubernetesのセッションレポート]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2024/12/08/kubecon_na_the_future_of_dbaas_ob_kubernetes</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2024/12/08/kubecon_na_the_future_of_dbaas_ob_kubernetes</guid>
            <pubDate>Sun, 08 Dec 2024 03:00:00 GMT</pubDate>
            <content:encoded><![CDATA[この記事は以下アドベントカレンダー8日目の記事です。3-shake Advent Calendar 2024 シリーズ2Kubernetes Advent Calendar 2024 シリーズ2The Future of DBaaS on Kubernetesのセッションレポートセッション概要:https://kccncna2024.sched.com/event/1i7kL/the-future-of-dbaas-on-kubernetes-melissa-logan-constantia-sergey-pronin-percona-deepthi-sigireddi-planetscale-gabriele-bartolini-edbセッション動画:https://www.youtube.com/watch?v=Z35SlsYd1ds「The Future of DBaaS on Kubernetes」は、Data on Kubernetes Communityのメンバーによるパネルディスカッション形式で、Kubernetes上で動作するDBaaSの将来について議論されました。ここ数年でデータベースをKubernetes上で動かすにあたりどう便利になったか？セッションでは、Kubernetesにおけるストレージとネットワーキングの進化が、データベース運用を大きく改善した点が強調されました。Volume Snapshotなどのストレージ関連機能の向上は、バックアップとリカバリといったDay 2 Operationを効率化し、Local Persistent Volumeの導入と改善は、データベースの高可用性とディザスタリカバリ構成をシンプルに実現可能にしました。また、Cilium Network PolicyやIngress/Egressといったネットワーキング機能は、マルチテナントサービスにおけるアクセス制御を容易にし、セキュリティ強化に貢献しています。これらの改善により、増加するデータベースと、優秀なデータベースエンジニア不足という課題に対し、Kubernetesは少ない人員でデータベースをスケールさせる有効な手段となっています。数年前に比べ、Kubernetes上でのデータベース運用はより現実的になり、エンタープライズグレードの運用にも耐えうるレベルに達しています。これは、Kubernetesがステートレスなアプリケーションだけでなく、ステートフルなデータベースにも適したプラットフォームへと進化したことを示しています。私がKubernetesを触り始めた時点ではここで紹介されているほとんどの機能はサポートされており、なぜKubernetesでデータベースを運用することが難しいのかを理解しきれない面がありました。このセクションによる直近のデータベース観点でのKubernetesのアップデートの紹介により、何が障壁でそれがどのように解決されたのかの理解が深まりました。Kubernetes上でデータベースを動かしている顧客についてシェアできる事例はあるか？セッションでは、Nokia、Broadcom、HubSpot、Shopify、IBMなど、様々な企業がKubernetes上でデータベースを運用している事例が紹介されました。これらの事例は、マイクロサービスアーキテクチャの普及と密接に関連しています。マイクロサービス化されたアプリケーションでは、単一のモノリシックなデータベースではなく、サービスごとにデータベースを持つ傾向があり、Kubernetesはそのような分散データベース環境の構築と管理を容易にします。特に、開発者がデータベースを所有し、インフラ管理者がDBaaSをインターフェイスとしてデータベースを払い出すという新しい運用モデルは、今後の主流となる可能性を示唆しています。これは、DevOpsの原則をデータベース運用に取り入れることで、開発速度と運用効率を向上させるアプローチと言えるでしょう。セクション内で紹介されている開発者がデータベースを所有し、インフラ管理者がデータベースを払い出すという体制はパブリッククラウドで運用されるマイクロサービスアーキテクチャでは当たり前のように実践されており、Kubernetesでも今後の主流となると考えることは不思議ではないでしょう。そしてそれは従来のVMやベアメタルベースのDBAがデータベース管理を行うには多すぎるデータベースが運用され、限界を迎えることは想像に難くなく、KubernetesとOperatorによる運用の簡略化は必須と言えるかもしれません。Kubernetes上でデータベースを動かすにあたりベストプラクティスはなにか？ベストプラクティスとして、クラウド中立性、クラウドレディネス、セルフサービス、セキュリティ、アーキテクチャ設計などが挙げられました。Operatorの活用は、クラウドベンダーに依存しない運用を実現する上で重要であり、UI/APIの整備やArgoCDなどのツールとの連携により、データベースのプロビジョニングと管理を自動化できます。また、開発者が容易にスケーリングやテスト環境構築を行えるセルフサービス環境も重要です。セキュリティについては、業界標準やコンプライアンス要件に合わせたポリシー設定が不可欠です。アーキテクチャ設計では、PostgreSQLを例に、Kubernetesの機能を活用した高可用性構成や、複数のアベイラビリティゾーンを考慮した設計が重要となります。さらに、Kubernetesの標準APIを活用することで、オブザーバビリティやセキュリティ、証明書の管理を簡素化し、他のコンポーネントとの統合を容易にすることが推奨されています。VMからの移行時には、ストレージを分離することでリソース管理の予測精度を高めることが重要です。ここではベストプラクティスとしてユーザーがセルフサービスでデータベースを立ち上げる方法としてGUIとAPIとツール連携による自動化二つの観点が出ていました。個人的にはパブリッククラウドとIaCの流れを見るにGUIベースよりAPIによる自動化が主流になっていくのではないかと考えます。またデータベースではないですがオンプレミスのVMベースシステムからKubernetesのコンテナベースに移行するプロジェクトに関わった時は独自のプロトコルによる通信をVMで実装しており、その方法をコンテナの世界に持ち込もうとした結果非常に複雑になっていた事例を見たことがあります。そのため、ここで紹介されているKubernetesとそのエコシステムに合わせることは不可欠ではないかと感じます。データベースをKubenetesで動かす場合の課題や落とし穴はあるか？セッションでは、VM環境での運用とKubernetes環境での運用を混同してしまうこと、マイグレーション計画の不足、リソースの過剰確保、そして人材育成の課題が議論されました。既存のVM向けスクリプトをそのままKubernetesに適用しようとするのではなく、クラウドネイティブな考え方を取り入れ、スケーラビリティと信頼性の向上に焦点を当てるべきです。マイグレーションにおいては、全てのワークロードの移行と、ダウンタイム最小化を両立するための綿密な計画が必要です。リソース管理においては、Kubernetesの柔軟性を活かし、適切なリソース割り当てを行うための実験と調整が重要です。さらに、DBAがKubernetesの基礎知識を習得し、データベース運用における新たなパラダイムシフトに対応できるよう、人材育成に力を入れる必要があります。このセッションを通して一番に感じたのはオンプレからパブリッククラウドへの移行と気にするところは同じだということと、DBAとKubernetesの距離を近づけることはやはり大事だということでした。特にDBAとKubernetesについてはより簡単なソリューションとして存在してしまっているマネージドデータベースが、Kubernetesを利用することから目を背けさせてしまう要因になっていると感じます。しかしDBAがより求められるのはデータベースをセルフホストする場合で、今後DBAとして活躍していくにはLinuxに適応してきたようにKubernetesに適応していく日強うがあると考えています。DBaaSの将来はどのように変わっていくと考えるか？将来のDBaaSは、Kubernetesとの統合がさらに深まり、データベースとKubernetesの境界が曖昧になっていくと予測されています。PostgreSQLの例では、Kubernetesとの親和性を高めるためのパッチ適用や、拡張機能のコンテナ化などが進んでいます。また、プライベートDBaaSだけでなく、商用DBaaSのKubernetes上での提供も増加し、データベースサービスの利用がさらに容易になると考えられます。Google Cloudなどのクラウドプロバイダーも、将来的にKubernetes上でマネージドデータベースサービスを提供する可能性があり、これにより、数千規模のデータベース管理が容易になるでしょう。Kubernetesの普及と成熟に伴い、Helm ChartやYAML以外の、より洗練されたUXも期待されます。セッション内ではGoogle CloudではCloud SQLがKubenetes1で運用される未来があるかもしれないと言及していましたが、すでにSpannerはKubernetesで動いています。商用DBaaSがKubernetesで動くことについてはよくある構成ですが、プライベートDBaaSがKubernetes上で動き、さまざまなエコシステムと組み合わせてAPIベースなど自動化に適したUXが提供されていくことには非常に注目しています。まとめ「The Future of DBaaS on Kubernetes」セッションは、Kubernetes上でのデータベース運用が成熟期を迎えていることを示しました。ストレージとネットワーキングの進化、Operatorの普及、そして様々な企業での成功事例は、Kubernetesがデータベース運用のための堅牢でスケーラブルなプラットフォームであることを証明しています。クラウドネイティブなアプローチ、セルフサービス化、セキュリティ強化、そして適切なアーキテクチャ設計は、Kubernetes上でのデータベース運用を成功させるための鍵となります。同時に、VM環境からの移行、リソース管理、人材育成といった課題にも適切に対処する必要があります。今後のDBaaSは、Kubernetesとの統合がさらに進み、データベースサービスの利用がより容易になると期待されます。このセッションで得られた知見は、今後のデータベース運用戦略策定に役立つ貴重な情報源となるでしょう。特に、オンプレミスでマイクロサービスアーキテクチャを採用する組織にとって、Kubernetesはデータベース運用における重要な選択肢となるでしょう。↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[バッチ処理をCloud RunからCloud Run jobsに変更してみた話]]></title>
            <link>https://qiita.com/bayobayo0324/items/71f7e19a051261d1adfc</link>
            <guid>https://qiita.com/bayobayo0324/items/71f7e19a051261d1adfc</guid>
            <pubDate>Sat, 07 Dec 2024 22:06:20 GMT</pubDate>
            <content:encoded><![CDATA[この記事は3-shake Advent Calendar 2024 シリーズ1の8日目の記事ですはじめましてあるいはこんにちは、@bayobayo0324 です。株式会社スリーシェイクでクラウド…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CUDAを利用したプログラムの高速化とNvidia Container Toolkit]]></title>
            <link>https://sreake.com/blog/cuda-nvidia-container-toolkit/</link>
            <guid>https://sreake.com/blog/cuda-nvidia-container-toolkit/</guid>
            <pubDate>Fri, 06 Dec 2024 01:51:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake事業部インターン生の高島陸斗です。インターン生としてSRE技術の調査・検証を行っています。私は、情報系の大学院生で、普段は数値解析に関する研究をしています。学部時代は、今回のブログ内容とも関係する並 […]The post CUDAを利用したプログラムの高速化とNvidia Container Toolkit first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[「SRE Kaigi 2025」にスリーシェイクのエンジニアが登壇]]></title>
            <link>https://sreake.com/blog/sre_kaigi_2025/</link>
            <guid>https://sreake.com/blog/sre_kaigi_2025/</guid>
            <pubDate>Thu, 05 Dec 2024 01:00:00 GMT</pubDate>
            <content:encoded><![CDATA[株式会社スリーシェイク（本社：東京都新宿区、代表取締役社長：吉田 拓真、以下スリーシェイク）に在籍するエンジニアが、2025年1月26日（日）に開催される「SRE Kaigi 2025」にセッション登壇することをお知らせします。The post 「SRE Kaigi 2025」にスリーシェイクのエンジニアが登壇 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rustによる郵便番号検索API (yubin_api) の技術解説]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/12/04/233641</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/12/04/233641</guid>
            <pubDate>Wed, 04 Dec 2024 14:36:41 GMT</pubDate>
            <content:encoded><![CDATA[こちらの記事は Rust Advent Calendar 2024 シリーズ 3 7日目の記事です！qiita.comはじめにRustを使用したWebアプリケーション開発は、高いパフォーマンスと堅牢性を両立させる方法として注目を集めています。本記事では、日本の郵便番号システムにアクセスするRESTful API「yubin_api」の実装を通じて、Rustの実践的な開発手法を解説します。workspace_2024/yubin_api at main · nwiizo/workspace_2024 · GitHubこのプロジェクトでは、axumを使用したWebサーバーの構築、非同期プログラミング（async/await）、構造化されたエラーハンドリングを実装しています。また、プロダクション環境を想定したメトリクス収集とモニタリング、型安全なAPIデザインにも焦点を当てています。1. プロジェクトの構成まず、Cargo.tomlの依存関係から見ていきましょう：[dependencies]# Webフレームワーク関連axum = { version = "0.7", features = ["macros"] }  # Webフレームワークtokio = { version = "1.0", features = ["full"] }   # 非同期ランタイムtower = { version = "0.4", features = ["full"] }   # HTTPサービス抽象化tower-http = { version = "0.5", features = ["cors", "trace", "limit", "request-id"] }# ロギングと監視tracing = "0.1"                # ログ出力tracing-subscriber = "0.3"     # ログ設定metrics = "0.21"              # メトリクス収集metrics-exporter-prometheus = "0.12"  # Prometheus形式出力# シリアライズ/デシリアライズserde = { version = "1.0", features = ["derive"] }serde_json = "1.0"# ユーティリティthiserror = "1.0"   # エラー定義uuid = { version = "1.0", features = ["v4"] }  # ユニークID生成utoipa = { version = "4.1", features = ["uuid"] }  # OpenAPI生成# 郵便番号データベースjpostcode_rs = "0.1.3"2. エラー処理の実装（error.rs）エラー処理は、APIの信頼性を確保する重要な部分です：use axum::{    http::StatusCode,    response::{IntoResponse, Response},    Json,};use thiserror::Error;use tracing::warn;// APIのエラー型を定義#[derive(Debug, Error)]pub enum ApiError {    #[error("Invalid postal code format")]    InvalidPostalCode,    #[error("Address not found")]    NotFound,    #[error("Internal server error: {0}")]    Internal(String),}// エラーをHTTPレスポンスに変換する実装impl IntoResponse for ApiError {    fn into_response(self) -> Response {        // エラーの種類に応じてステータスコードを設定        let (status, error_message) = match self {            ApiError::InvalidPostalCode => (StatusCode::BAD_REQUEST, self.to_string()),            ApiError::NotFound => (StatusCode::NOT_FOUND, self.to_string()),            ApiError::Internal(ref e) => {                // 内部エラーはログに記録                warn!("Internal server error: {}", e);                (                    StatusCode::INTERNAL_SERVER_ERROR,                    "Internal server error".to_string(),                )            }        };        // JSONレスポンスの構築        let body = Json(serde_json::json!({            "error": error_message,            "status": status.as_u16(),            // エラー追跡用のユニークID            "request_id": uuid::Uuid::new_v4().to_string()        }));        (status, body).into_response()    }}3. データモデルの定義（models.rs）APIで使用するデータ構造を定義します：use serde::{Deserialize, Serialize};// 住所情報のレスポンス構造体#[derive(Debug, Serialize, Deserialize, utoipa::ToSchema)]pub struct AddressResponse {    pub postal_code: String,    pub prefecture: String,    pub prefecture_kana: String,    pub prefecture_code: i32,    pub city: String,    pub city_kana: String,    pub town: String,    pub town_kana: String,    pub street: Option<String>,    pub office_name: Option<String>,    pub office_name_kana: Option<String>,}// jpostcode_rsのAddress型からの変換を実装impl From<jpostcode_rs::Address> for AddressResponse {    fn from(addr: jpostcode_rs::Address) -> Self {        AddressResponse {            postal_code: addr.postcode,            prefecture: addr.prefecture,            prefecture_kana: addr.prefecture_kana,            prefecture_code: addr.prefecture_code,            city: addr.city,            city_kana: addr.city_kana,            town: addr.town,            town_kana: addr.town_kana,            street: addr.street,            office_name: addr.office_name,            office_name_kana: addr.office_name_kana,        }    }}// 住所検索用のクエリ構造体#[derive(Debug, Deserialize, utoipa::ToSchema)]pub struct AddressQuery {    pub query: String,    #[serde(default = "default_limit")]    pub limit: usize,}// デフォルトの検索結果制限数fn default_limit() -> usize {    10}4. メトリクス収集の設定（metrics.rs）アプリケーションのパフォーマンスを監視するためのメトリクス設定：use metrics::{describe_counter, describe_histogram, register_counter, register_histogram};use metrics_exporter_prometheus::PrometheusBuilder;pub fn setup_metrics() {    // リクエスト数のカウンター    describe_counter!(        "yubin_api_postal_lookups_total",        "Total number of postal code lookups"    );    describe_counter!(        "yubin_api_address_searches_total",        "Total number of address searches"    );    // レスポンス時間のヒストグラム    describe_histogram!(        "yubin_api_postal_lookup_duration_seconds",        "Duration of postal code lookups in seconds"    );    describe_histogram!(        "yubin_api_address_search_duration_seconds",        "Duration of address searches in seconds"    );    // メトリクスの登録    register_counter!("yubin_api_postal_lookups_total");    register_counter!("yubin_api_address_searches_total");    register_histogram!("yubin_api_postal_lookup_duration_seconds");    register_histogram!("yubin_api_address_search_duration_seconds");    // Prometheusレコーダーの設定    PrometheusBuilder::new()        .install()        .expect("Failed to install Prometheus recorder");}Rustの知っておいたほうがいいポイント解説(前編)属性マクロの使用#[derive(...)]: 自動実装の導入#[error(...)]: エラーメッセージの定義#[serde(...)]: シリアライズ設定トレイトの実装From<T>: 型変換の実装IntoResponse: HTTPレスポンスへの変換Error: カスタムエラー型の定義ジェネリクスとライフタイムOption<T>: 省略可能な値の表現Result<T, E>: エラーハンドリングVec<T>: 可変長配列の使用型システムの活用カスタム構造体の定義列挙型によるエラー表現デフォルト値の実装Rust初学者のためのyubin_api実装解説 - 後編5. APIルートの実装（routes.rs）APIの実際のエンドポイントを実装します：use axum::{extract::Path, http::StatusCode, response::IntoResponse, Json};use metrics::{counter, histogram};use tracing::info;// ヘルスチェックエンドポイントpub async fn health_check() -> impl IntoResponse {    StatusCode::OK}// 郵便番号検索エンドポイントpub async fn lookup_by_postal_code(    Path(code): Path<String>,  // URLパスからパラメータを取得) -> Result<Json<Vec<AddressResponse>>, ApiError> {    // リクエストのログ記録    info!("Looking up postal code: {}", code);        // メトリクスのカウントアップ    counter!("yubin_api_postal_lookups_total", 1);        // 処理時間の計測開始    let start = std::time::Instant::now();    // 郵便番号検索の実行    let result = jpostcode_rs::lookup_address(&code).map_err(|e| match e {        jpostcode_rs::JPostError::InvalidFormat => ApiError::InvalidPostalCode,        jpostcode_rs::JPostError::NotFound => ApiError::NotFound,    })?;    // 処理時間の計測と記録    let duration = start.elapsed().as_secs_f64();    histogram!("yubin_api_postal_lookup_duration_seconds", duration);    // 結果の返却    Ok(Json(result.into_iter().map(Into::into).collect()))}// 住所検索エンドポイントpub async fn search_by_address(    Json(query): Json<AddressQuery>,  // リクエストボディからのJSONパース) -> Result<Json<Vec<AddressResponse>>, ApiError> {    info!("Searching address with query: {}", query.query);        // 入力値の検証    if query.query.trim().is_empty() {        return Err(ApiError::InvalidPostalCode);    }    counter!("yubin_api_address_searches_total", 1);    let start = std::time::Instant::now();    // 住所検索の実行    let mut results: Vec<AddressResponse> = jpostcode_rs::search_by_address(&query.query)        .into_iter()        .map(Into::into)        .collect();    // 結果数の制限適用    results.truncate(query.limit);    let duration = start.elapsed().as_secs_f64();    histogram!("yubin_api_address_search_duration_seconds", duration);    Ok(Json(results))}6. メインアプリケーションの実装（main.rs）アプリケーションのエントリーポイントとサーバーの設定：use axum::{routing::{get, post}, Router};use std::net::SocketAddr;use tower::ServiceBuilder;use tower_http::{    cors::{Any, CorsLayer},    trace::{DefaultMakeSpan, DefaultOnResponse, TraceLayer},};use tracing::info;#[tokio::main]async fn main() {    // ロギングの初期化    tracing_subscriber::fmt()        .with_env_filter(            tracing_subscriber::EnvFilter::try_from_default_env()                .unwrap_or_else(|_| "yubin_api=debug,tower_http=debug".into()),        )        .init();    // メトリクス収集の初期化    setup_metrics();    // リクエストトレース設定    let trace_layer = TraceLayer::new_for_http()        .make_span_with(DefaultMakeSpan::new().include_headers(true))        .on_response(DefaultOnResponse::new().include_headers(true));    // CORS設定    let cors = CorsLayer::new()        .allow_methods(Any)        .allow_headers(Any)        .allow_origin(Any);    // ルーターの設定    let app = Router::new()        .route("/health", get(health_check))        .route("/postal/:code", get(lookup_by_postal_code))        .route("/address/search", post(search_by_address))        .layer(ServiceBuilder::new()            .layer(trace_layer)            .layer(cors));    // サーバーアドレスの設定    let addr = SocketAddr::from(([127, 0, 0, 1], 3000));    info!("Server listening on {}", addr);    // サーバーの起動    let listener = tokio::net::TcpListener::bind(addr).await.unwrap();    axum::serve(listener, app).await.unwrap();}7. 重要な実装パターンの解説非同期処理// 非同期関数の定義pub async fn lookup_by_postal_code(...) -> Result<...> {    // 非同期処理の実行    let result = jpostcode_rs::lookup_address(&code)?;    // ...}// 非同期ランタイムの設定#[tokio::main]async fn main() {    // ...}エラーハンドリング// Result型を使用したエラー処理let result = jpostcode_rs::lookup_address(&code).map_err(|e| match e {    JPostError::InvalidFormat => ApiError::InvalidPostalCode,    JPostError::NotFound => ApiError::NotFound,})?;ミドルウェアの構成let app = Router::new()    .route(...)    .layer(ServiceBuilder::new()        .layer(trace_layer)        .layer(cors));8. API使用例郵便番号による検索curl http://localhost:3000/postal/1000001レスポンス例：[  {    "postal_code": "1000001",    "prefecture": "東京都",    "city": "千代田区",    "town": "千代田",    ...  }]住所による検索curl -X POST http://localhost:3000/address/search \  -H "Content-Type: application/json" \  -d '{"query": "東京都千代田区", "limit": 10}'9. Rustの知っておいたほうがいいポイント解説(後編)非同期プログラミングasync/awaitの使用方法tokioランタイムの理解非同期関数の定義と呼び出しエラーハンドリングパターンResult型の活用エラー変換のベストプラクティスエラーの伝播（?演算子）HTTPサーバーの実装ルーティング設定ミドルウェアの活用リクエスト/レスポンスの処理テスト可能な設計モジュール分割依存性の分離エラー処理の一貫性おわりにyubin_apiの実装を通じて、Rustによる実践的なWeb API開発の全体像を見てきました。このプロジェクトでは、カスタムエラー型の定義や型安全なデータ変換、トレイトの実装といった堅牢な型システムの活用を行いました。また、tokioによる非同期ランタイムやasync/awaitの効果的な使用、エラーハンドリングとの統合などの非同期プログラミングの実践も重要な要素となっています。さらに、メトリクス収集や構造化ログ、エラートラッキングといった運用面の考慮など、重要な概念と技術を学ぶことができました。このプロジェクトは、単なる郵便番号検索APIの実装を超えて、Rustの実践的な使用方法と、プロダクション品質のWebサービス開発の基本を学ぶ良い例となっています。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[構造化ログのスキーマを考えてみる]]></title>
            <link>https://blog.atusy.net/2024/12/04/log-schema/</link>
            <guid>https://blog.atusy.net/2024/12/04/log-schema/</guid>
            <pubDate>Wed, 04 Dec 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[ログ設計初心者なりに、分析しやすいログってなにかなと考えてみた。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[argocd コマンドで別ブランチとの差分を確認する]]></title>
            <link>https://qiita.com/yteraoka/items/aea03d50288375f85183</link>
            <guid>https://qiita.com/yteraoka/items/aea03d50288375f85183</guid>
            <pubDate>Tue, 03 Dec 2024 15:14:17 GMT</pubDate>
            <content:encoded><![CDATA[ArgoCD の GitOps で Merge 前に manifest の差分を見たいArgoCD は Application リソースで source に指定した Git などの定義と実際に K…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LLMのモデル更新や廃止による影響を考える]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/12/03/232856</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/12/03/232856</guid>
            <pubDate>Tue, 03 Dec 2024 14:28:56 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、MLOps（LLMOps、生成AIOps） Advent Calendar 2024 4日目の記事です。生成AIの普及により、アプリケーションに組み込んで実運用を始めた方も増えてきたと思います。LLMOpsをする中で気をつけたいことを考えてみました。モデルの更新まず、思い浮かぶのがモデルの更新よる影響です。モデルの更新によって性能が上がるなどのメリットを享受できる反面、挙動変更によって、困ることもあります。私の場合、システムの実運用では無いですが、LLM技術書のサンプルコードが動かなくなる事態がありました。06_agent/agent_5.py で2回目の実行結果が正しく表示されません · Issue #3 · harukaxq/langchain-book · GitHubgpt-3.5-turboをAgentとして使用したときの挙動が変わったという内容です。アプリに組み込んでいたら、機能が使えなくなる可能性があり、使えなくなった場合の代替案も用意しておく必要があると考えました。また、LLMのリリース情報もウォッチしておく必要があるでしょう。Geminiはリリースの最新情報を日本語で提供しています。gemini.google.comChatGPTはリリースノートを英語のみですが提供しています。ChatGPT — Release Notes | OpenAI Help CenterAnthropic製品（Claude）のリリースノートは日本語で提供されています。docs.anthropic.comモデルの廃止モデルの廃止もウォッチする必要があるでしょう。GPT-3.5 Turbo終了はニュースになりました。xtech.nikkei.com↑日経クロステックの有料会員記事ですが、会員でなくても1ページ目で内容は把握できます。learn.microsoft.comAzure OpenAIでは、GPTの各種マイナーバージョンが提供されていますが、適宜廃止になるので注意が必要です。廃止になる場合、モデルのVersion UPが必要なので、早めに開発環境でVersion UPしたモデルの挙動確認をする必要があるでしょう。Version UPしたモデルだと、LLMの利用料が高くなることも念頭に置いて、コスト試算しましょう。まとめモデル更新や廃止を早く知るために、LLM公式サイトのリリースノートなどのウォッチをして、早めに対策をしましょう。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rustで郵便番号・住所検索TUIツールを開発した - jposta]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/12/03/230030</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/12/03/230030</guid>
            <pubDate>Tue, 03 Dec 2024 14:00:30 GMT</pubDate>
            <content:encoded><![CDATA[こちらの記事は Rust Advent Calendar 2024 シリーズ 3 5日目の記事です！qiita.comはじめにこんにちは！jposta を紹介させてください。jpostaは、日本の郵便番号・住所をターミナルから手軽に検索できるTUIツール 🔍 です。Rustで書かれており ⚡、使いやすさを重視してリアルタイム検索を実装しました 🖥️。jposta の動作イメージ機能紹介この小さなツールでは、郵便番号から住所の簡単検索 🏠 はもちろん、住所からの郵便番号検索 🔢 もラクラクできます。入力しながらサクサク表示されるリアルタイム検索 ⚡ や、キーボードだけでスイスイ操作 ⌨️ が可能で、スクロールもサクサク動き 📜、もちろん日本語もバッチリ対応 🗾 しています。ぜひGitHubをチェックしてみてください！github.comインストールcargo install --git https://github.com/nwiizo/jpostaもしくはcargo install jpostaこちら、みんなだいすきcrate.ioにちゃんとあげました。https://crates.io/crates/jposta基本操作Tab: 郵便番号/住所検索モード切替↑↓: 結果スクロールEsc: 終了検索モード郵便番号検索数字を入力すると自動で該当する住所を表示部分一致対応（"100"で始まる郵便番号すべて等）住所検索漢字やかなで住所を入力部分一致対応（"渋谷"等）Rustでの実装解説1. 基本構造の定義#[derive(Clone)]enum InputMode {    Postal,   // 郵便番号検索    Address,  // 住所検索}struct App {    input: String,    results: Vec<String>,    input_mode: InputMode,    scroll_state: ScrollbarState,    scroll_position: u16,    search_tx: mpsc::Sender<String>,    result_rx: mpsc::Receiver<Vec<String>>,}InputModeは検索モードを表す列挙型です。Cloneトレイトを導出することで、値のコピーが可能になります。App構造体はアプリケーションの状態を管理します。input: 現在の入力文字列results: 検索結果の配列input_mode: 現在の検索モードscroll_stateとscroll_position: スクロール状態の管理search_txとresult_rx: スレッド間通信用のチャンネル2. アプリケーションの初期化impl App {    fn new() -> App {        let (search_tx, search_rx) = mpsc::channel::<String>();        let (result_tx, result_rx) = mpsc::channel();        thread::spawn(move || {            let mut last_query = String::new();            let mut input_mode = InputMode::Postal;                        while let Ok(query) = search_rx.recv() {                // 検索処理（後述）            }        });        App {            input: String::new(),            results: Vec::new(),            input_mode: InputMode::Postal,            scroll_state: ScrollbarState::default(),            scroll_position: 0,            search_tx,            result_rx,        }    }}new()関数では、2つのチャンネルを作成（検索クエリ用と結果用）検索処理を行うワーカースレッドを起動初期状態のAppインスタンスを返す3. 検索処理の実装// 検索スレッド内の処理if query.starts_with("MODE_CHANGE:") {    input_mode = match &query[11..] {        "postal" => InputMode::Postal,        _ => InputMode::Address,    };    continue;}if query == last_query { continue; }last_query = query.clone();if query.is_empty() {    let _ = result_tx.send(Vec::new());    continue;}thread::sleep(Duration::from_millis(100));let results = match input_mode {    InputMode::Postal => lookup_addresses(&query)        .map(|addresses| {            addresses                .into_iter()                .map(|addr| addr.formatted_with_kana())                .collect()        })        .unwrap_or_default(),    InputMode::Address => search_by_address(&query)        .into_iter()        .map(|addr| addr.formatted_with_kana())        .collect(),};let _ = result_tx.send(results);検索処理では、モード変更メッセージの確認と処理重複クエリのスキップ空クエリの即時処理ディバウンス処理（100ms）モードに応じた検索実行結果の送信4. UIとイベント処理fn main() -> io::Result<()> {    enable_raw_mode()?;    let mut stdout = stdout();    execute!(stdout, EnterAlternateScreen)?;    let backend = CrosstermBackend::new(stdout);    let mut terminal = Terminal::new(backend)?;    let mut app = App::new();    loop {        app.check_results();        terminal.draw(|f| {            let chunks = Layout::default()                .direction(Direction::Vertical)                .constraints([                    Constraint::Length(3),                    Constraint::Min(0)                ])                .split(f.size());            // 入力欄の描画            let input_block = Block::default()                .title(match app.input_mode {                    InputMode::Postal => "郵便番号検索",                    InputMode::Address => "住所検索",                })                .borders(Borders::ALL);                        let input = Paragraph::new(app.input.as_str())                .block(input_block)                .style(Style::default().fg(Color::Yellow));            f.render_widget(input, chunks[0]);            // 結果表示の描画            let results_block = Block::default()                .title(format!("検索結果 ({} 件)", app.results.len()))                .borders(Borders::ALL);                        let results = Paragraph::new(app.results.join("\n"))                .block(results_block)                .scroll((app.scroll_position, 0));            f.render_widget(results, chunks[1]);        })?;        // キー入力処理        if let Event::Key(key) = event::read()? {            match key.code {                KeyCode::Char(c) => {                    app.input.push(c);                    app.search();                }                KeyCode::Backspace => {                    app.input.pop();                    app.search();                }                KeyCode::Up => app.scroll_up(),                KeyCode::Down => app.scroll_down(),                KeyCode::Tab => app.change_mode(match app.input_mode {                    InputMode::Postal => InputMode::Address,                    InputMode::Address => InputMode::Postal,                }),                KeyCode::Esc => break,                _ => {}            }        }    }    // 終了処理    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;    disable_raw_mode()?;    Ok(())}UIとイベント処理では、ターミナルの初期化メインループ検索結果の確認画面描画キー入力処理終了時のクリーンアップ5. 補助機能の実装impl App {    fn search(&mut self) {        let _ = self.search_tx.send(self.input.clone());    }    fn check_results(&mut self) {        if let Ok(new_results) = self.result_rx.try_recv() {            self.results = new_results;            self.scroll_position = 0;            self.scroll_state = ScrollbarState::new(self.results.len());        }    }    fn scroll_up(&mut self) {        self.scroll_position = self.scroll_position.saturating_sub(1);    }    fn scroll_down(&mut self) {        if !self.results.is_empty() {            self.scroll_position = self                .scroll_position                .saturating_add(1)                .min((self.results.len() as u16).saturating_sub(1));        }    }    fn change_mode(&mut self, mode: InputMode) {        self.input_mode = mode;        let mode_str = match self.input_mode {            InputMode::Postal => "postal",            InputMode::Address => "address",        };        let _ = self.search_tx.send(format!("MODE_CHANGE:{}", mode_str));        self.input.clear();        self.results.clear();    }}補助機能として、1. 検索リクエストの送信2. 検索結果の確認と更新3. スクロール処理4. モード切替処理これらの機能により、スムーズな検索体験を実現しています。使用ライブラリratatui: TUI（テキストユーザーインターフェース）フレームワークcrossterm: ターミナル操作ライブラリjpostcode_rs: 郵便番号データ処理ライブラリRust学習リソース1. 基礎学習The Rust Programming Language - 公式ガイドブックRust by Example - 実例で学ぶRustRustlings - 対話型学習ツール2. 基本概念構造体（Structs）列挙型（Enums）メソッド実装3. メモリ管理所有権システム参照と借用4. 言語機能パターンマッチングクロージャ5. エラー処理と型システムエラー処理Result型境界チェック演算子さいごにこのプロジェクトは、Rustの実践的な学習と日本の住所システムへの理解を深める良い機会となりました 📚。非同期処理やTUIの実装を通じて、Rustの強力な型システムと安全性を活かしたコーディングを実践できました ⚡。ぜひ使ってみて、フィードバックをいただければ幸いです 🙏。プルリクエストも大歓迎です 🎉！ソースコード🦀GitHub - jposta]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[退屈なことはRust Build Scripts にやらせよう]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/12/03/143149</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/12/03/143149</guid>
            <pubDate>Tue, 03 Dec 2024 05:31:49 GMT</pubDate>
            <content:encoded><![CDATA[こちらの記事は Rust Advent Calendar 2024 シリーズ 3 3日目の記事です！qiita.comはじめにRustのビルドスクリプト（build.rs）は、コンパイル前のデータ処理や環境設定を自動化する強力なツールです。しかし、大いなる力には、大いなる責任が伴います。コードの生成、リソースの最適化、プラットフォーム固有の設定管理など、ビルド時の様々なタスクを効率的に処理できます。今回は、そのユースケースの1つとして、郵便番号データを処理するビルドスクリプトの実装を詳しく解説します。この例を通じて、build.rsの基本的な使い方から実践的な活用方法まで、段階的に理解を深めていきましょう。doc.rust-lang.orgはじめにユースケース：郵便番号データの処理実装の全体像実装の詳細解説1. ファイル変更の監視設定2. パスの設定3. データの処理4. 結果の出力生成したデータの利用方法1. アプリケーションでのデータ読み込み2. 検索機能の実装build.rsの主要な機能1. 環境変数の設定2. リンカ設定3. コードの生成実践的な利用シーン1. 設定ファイルの統合と生成2. プロトコル定義ファイルの生成3. アセットファイルの埋め込み4. データベースマイグレーションファイルの統合参考資料まとめユースケース：郵便番号データの処理このビルドスクリプトは、複数のJSONファイルに分散された郵便番号データを1つのファイルにマージする処理を行います。github.com実装の全体像use serde_json::Value;use std::collections::HashMap;use std::fs;use std::path::Path;use walkdir::WalkDir;fn main() {    println!("cargo:rerun-if-changed=jpostcode-data/data/json");    let json_dir = Path::new("jpostcode-data/data/json");    let out_dir = std::env::var("OUT_DIR").unwrap();    let dest_path = Path::new(&out_dir).join("address_data.json");    let mut merged_data = HashMap::new();    // ... データ処理ロジック ...}実装の詳細解説1. ファイル変更の監視設定println!("cargo:rerun-if-changed=jpostcode-data/data/json");この行は、指定したディレクトリ内のファイルが変更された場合にのみビルドスクリプトを再実行するように設定します。これにより、不必要なビルド時間を削減できます。2. パスの設定let json_dir = Path::new("jpostcode-data/data/json");let out_dir = std::env::var("OUT_DIR").unwrap();let dest_path = Path::new(&out_dir).join("address_data.json");json_dir: 入力となるJSONファイルが格納されているディレクトリout_dir: Cargoが提供するビルド出力ディレクトリdest_path: 生成されるファイルの出力先3. データの処理for entry in WalkDir::new(json_dir).into_iter().filter_map(|e| e.ok()) {    if entry.file_type().is_file()        && entry.path().extension().map_or(false, |ext| ext == "json")    {        let content = fs::read_to_string(entry.path()).unwrap();        let file_data: HashMap<String, Value> = serde_json::from_str(&content).unwrap();        let prefix = entry.path().file_stem().unwrap().to_str().unwrap();        for (suffix, data) in file_data {            let full_postcode = format!("{}{}", prefix, suffix);            merged_data.insert(full_postcode, data);        }    }}このコードブロックでは以下の処理を行っています。WalkDirを使用してディレクトリを再帰的に走査JSONファイルのみを対象にフィルタリング各ファイルの内容を読み込みとパースファイル名とデータを組み合わせて完全な郵便番号を生成マージされたデータに追加4. 結果の出力fs::write(dest_path, serde_json::to_string(&merged_data).unwrap()).unwrap();処理したデータを1つのJSONファイルとして出力します。生成したデータの利用方法1. アプリケーションでのデータ読み込みuse once_cell::sync::Lazy;use serde::{Deserialize, Serialize};use std::collections::HashMap;#[derive(Debug, Serialize, Deserialize)]struct Address {    postcode: String,    prefecture: String,    city: String,    // ... 他のフィールド}static ADDRESS_MAP: Lazy<HashMap<String, Vec<Address>>> = Lazy::new(|| {    let data = include_str!(concat!(env!("OUT_DIR"), "/address_data.json"));    serde_json::from_str(data).expect("Failed to parse address data")});2. 検索機能の実装fn lookup_address(postal_code: &str) -> Option<&Vec<Address>> {    ADDRESS_MAP.get(postal_code)}fn search_by_prefecture(prefecture: &str) -> Vec<&Address> {    ADDRESS_MAP        .values()        .flat_map(|addresses| addresses.iter())        .filter(|addr| addr.prefecture == prefecture)        .collect()}build.rsの主要な機能1. 環境変数の設定// コンパイル時の条件設定println!("cargo:rustc-cfg=feature=\"custom_feature\"");// 環境変数の設定println!("cargo:rustc-env=APP_VERSION=1.0.0");2. リンカ設定// 外部ライブラリのリンクprintln!("cargo:rustc-link-lib=sqlite3");println!("cargo:rustc-link-search=native=/usr/local/lib");3. コードの生成// バージョン情報の生成let version_code = format!(    "pub const VERSION: &str = \"{}\";\n",    env!("CARGO_PKG_VERSION"));fs::write("version.rs", version_code)?;実践的な利用シーン1. 設定ファイルの統合と生成複数の環境向けの設定ファイルを1つに統合する例：use std::collections::HashMap;use serde_json::Value;fn main() {    println!("cargo:rerun-if-changed=config/");        let environments = ["development", "staging", "production"];    let mut merged_config = HashMap::new();        for env in environments {        let config_path = format!("config/{}.json", env);        let config_content = std::fs::read_to_string(&config_path).unwrap();        let config: Value = serde_json::from_str(&config_content).unwrap();                merged_config.insert(env, config);    }        let out_dir = std::env::var("OUT_DIR").unwrap();    let dest_path = Path::new(&out_dir).join("config.rs");        // 設定をRustのコードとして出力    let config_code = format!(        "pub static CONFIG: Lazy<HashMap<&str, Value>> = Lazy::new(|| {{            serde_json::from_str({}).unwrap()        }});",        serde_json::to_string(&merged_config).unwrap()    );        std::fs::write(dest_path, config_code).unwrap();}使用例：// main.rsuse once_cell::sync::Lazy;include!(concat!(env!("OUT_DIR"), "/config.rs"));fn get_database_url(env: &str) -> String {    CONFIG[env]["database"]["url"].as_str().unwrap().to_string()}2. プロトコル定義ファイルの生成Protocol Buffersの定義ファイルからRustコードを生成する例：use std::process::Command;fn main() {    println!("cargo:rerun-if-changed=proto/");        // protoファイルのコンパイル    let status = Command::new("protoc")        .args(&[            "--rust_out=src/generated",            "--proto_path=proto",            "service.proto"        ])        .status()        .unwrap();            if !status.success() {        panic!("Failed to compile proto files");    }        // 生成されたコードをモジュールとして登録    let mod_content = r#"        pub mod generated {            include!("generated/service.rs");        }    "#;        std::fs::write("src/proto_mod.rs", mod_content).unwrap();}使用例：// lib.rsmod proto_mod;use proto_mod::generated::{UserRequest, UserResponse};pub async fn handle_user_request(req: UserRequest) -> UserResponse {    // プロトコル定義に基づいた処理}3. アセットファイルの埋め込み画像やテキストファイルをバイナリに埋め込む例：use std::collections::HashMap;use base64;fn main() {    println!("cargo:rerun-if-changed=assets/");        let mut assets = HashMap::new();        // 画像ファイルの埋め込み    for entry in std::fs::read_dir("assets").unwrap() {        let entry = entry.unwrap();        let path = entry.path();                if path.extension().map_or(false, |ext| ext == "png" || ext == "jpg") {            let content = std::fs::read(&path).unwrap();            let encoded = base64::encode(&content);                        let asset_name = path.file_name().unwrap().to_str().unwrap();            assets.insert(asset_name.to_string(), encoded);        }    }        // アセットデータをRustコードとして出力    let out_dir = std::env::var("OUT_DIR").unwrap();    let dest_path = Path::new(&out_dir).join("assets.rs");        let assets_code = format!(        "pub static ASSETS: Lazy<HashMap<String, String>> = Lazy::new(|| {{            let mut m = HashMap::new();            {}            m        }});",        assets.iter().map(|(k, v)| {            format!("m.insert(\"{}\".to_string(), \"{}\".to_string());", k, v)        }).collect::<Vec<_>>().join("\n")    );        std::fs::write(dest_path, assets_code).unwrap();}使用例：// lib.rsuse once_cell::sync::Lazy;include!(concat!(env!("OUT_DIR"), "/assets.rs"));pub fn get_image_data(name: &str) -> Option<Vec<u8>> {    ASSETS.get(name)        .map(|encoded| base64::decode(encoded).unwrap())}4. データベースマイグレーションファイルの統合SQLマイグレーションファイルを1つのモジュールにまとめる例：fn main() {    println!("cargo:rerun-if-changed=migrations/");        let mut migrations = Vec::new();        // マイグレーションファイルの収集    for entry in std::fs::read_dir("migrations").unwrap() {        let entry = entry.unwrap();        let path = entry.path();                if path.extension().map_or(false, |ext| ext == "sql") {            let version = path.file_stem().unwrap().to_str().unwrap()                .split('_').next().unwrap();            let content = std::fs::read_to_string(&path).unwrap();                        migrations.push((version.to_string(), content));        }    }        // マイグレーションをRustコードとして出力    let migrations_code = format!(        "pub static MIGRATIONS: &[(&str, &str)] = &[{}];",        migrations.iter()            .map(|(ver, sql)| format!("(\"{}\", \"{}\")", ver, sql.replace("\"", "\\\"")))            .collect::<Vec<_>>()            .join(",\n")    );        let out_dir = std::env::var("OUT_DIR").unwrap();    let dest_path = Path::new(&out_dir).join("migrations.rs");    std::fs::write(dest_path, migrations_code).unwrap();}使用例：// database.rsinclude!(concat!(env!("OUT_DIR"), "/migrations.rs"));pub async fn run_migrations(db: &SqlitePool) -> Result<()> {    for (version, sql) in MIGRATIONS {        db.execute(sql).await?;        println!("Applied migration version {}", version);    }    Ok(())}これらの例は、build.rsの実践的な使用方法を示しています。各例で以下のような利点があります。コンパイル時のリソース最適化開発時の利便性向上ランタイムパフォーマンスの改善コードの保守性向上実際のプロジェクトでは、これらの手法を組み合わせたり、プロジェクトの要件に合わせてカスタマイズしたりすることで、より効率的な開発環境を構築できます。しかし、魔環境もしくはビルド地獄を顕現させることもできるので注意が必要だと思いました。参考資料The Cargo Book - Build ScriptsRust By Example - Build Scriptsまとめこのビルドスクリプトの実装例を通じて、build.rsの有用性が明確になりました。コンパイル時のデータ最適化や複数ファイルの統合処理、動的なコード生成、そしてプラットフォーム固有の設定管理など、多岐にわたる機能を提供します。実際のプロジェクトでは、これらの機能を組み合わせることで、効率的な開発環境とビルドプロセスを実現できます。build.rsを活用することで、コンパイル時に必要なリソースの最適化や設定の自動化が可能となり、開発効率の向上とコードの保守性改善に大きく貢献します。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Transformer論文を生成AIで要約してみた]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/12/02/233507</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/12/02/233507</guid>
            <pubDate>Mon, 02 Dec 2024 14:35:07 GMT</pubDate>
            <content:encoded><![CDATA[この記事は生成AI Advent Calendar 2024の3日目の記事です。AIの歴史は60年ほどで、「冬の時代」と呼ばれる停滞期もありましたが、昨今の生成AIの発展は「Transformer」と呼ばれる論文から始まりました。この「Transformer」論文をわかりやすいように生成AIに要約させてみました。arxiv.org「Transformer」論文は上記サイトで見ることができます。論文の正式名称は「Attention Is All You Need」で、現在 v7。右側のAccess Paper:から閲覧することができます。なお、PDFの直リンクはこちらGoogle CloudのVertex AIでGeminiに要約させた結果は以下です。プロンプト「この論文を日本語で高校3年生レベルにもわかるように要約してください。」この論文は、「Transformer」という、機械翻訳などのシーケンス変換タスクに革新をもたらした新しいニューラルネットワークアーキテクチャを提案しています。高校3年生にもわかるように要約すると、以下のようになります。従来の機械翻訳の問題点:従来の機械翻訳は、RNN（再帰型ニューラルネットワーク）という技術を主に使っていました。RNNは文章を単語ごとに順番に処理していくため、長い文章だと処理に時間がかかります。また、並列処理が難しいため、計算速度を上げるのが困難でした。Transformerの革新的な点:Transformerは、「Attention（注意機構）」という仕組みに完全に基づいており、RNNのような順番に処理する仕組みを完全に排除しました。Attention機構は、翻訳したい単語を生成する際に、入力文全体の中でどの単語に注目すべきかを判断する仕組みです。人間の翻訳作業で例えると、翻訳する際に原文の関連する部分に注目するのと同じような働きをします。この仕組みにより、以下の利点が生まれました。並列処理が可能: 単語を順番に処理する必要がないため、並列処理が可能になり、計算速度が大幅に向上しました。長い文章への対応力向上: RNNでは、文章が長くなるほど精度が落ちやすいという問題がありましたが、Attention機構は入力文全体を一度に見ることができるため、長い文章でも高い精度を維持できます。Transformerの成果:Transformerは、従来のモデルよりも高い精度を達成し、さらに学習時間も大幅に短縮しました。具体的には、英語からドイツ語への翻訳と英語からフランス語への翻訳タスクで、当時の最高精度を更新しました。Transformerの仕組みを簡単に説明すると:Transformerは、エンコーダとデコーダという2つの部分から構成されています。エンコーダ: 入力文を解析し、各単語の意味をベクトル表現に変換します。デコーダ: エンコーダの出力とAttention機構を利用して、翻訳文を生成します。エンコーダとデコーダは、それぞれ複数の層が積み重なった構造になっています。各層は、Attention機構と全結合層という2つのサブ層から構成されています。Transformerの今後の展望:Transformerは、テキスト以外のデータ（画像、音声、動画など）にも応用できる可能性があり、様々な分野での活用が期待されています。また、文章生成をより効率的に行うための研究も進められています。簡単に言うと、Transformerは、人間の「注意」の仕組みに似た仕組みを使って、従来よりも高速で高精度な機械翻訳を実現した画期的な技術です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年の崖から考える我々のパーパス（序章）]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/12/01/232229</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/12/01/232229</guid>
            <pubDate>Sun, 01 Dec 2024 14:22:29 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事は3-shake Advent Calendar 2024 シリーズ1の2日目の記事です。1日目はシリーズ1がYutaro ShirayamaさんのポストCloud9？クラウドIDE CoderでPlatform Engineeringを実践する2日目はシリーズ2がYoshinori Teraokaさんのvector で kubernetes の container log を CloudWatch Logs に転送するでした。なお、シリーズ2の2日目はshingo919さんの 九州旅行記（ドライブでの九州一週旅行は大変だった！）です。2025年の崖今回は「2025年の崖」について軽くご紹介したいと思います。いよいよ2025年になりますが、ITでは「2025年の崖」という言葉が存在します。2025年の崖がある中で、スリーシェイクのSreake事業部が果たす役割を考えていきたいと思います。「2025年の崖」をググったら色々出てきますが、経済産業省のレポートが1次情報源的かつわかりやすいでしょう。www.meti.go.jpなお、DXレポート ～ITシステム「2025年の崖」の克服とDXの本格的な展開～（サマリー）はスライド5枚にまとまっており、さっと読みやすいです。「2025年の崖」は要するに何なのかというと、IT人材が不足しており、レガシーシステムを保守するのに限界が来ている。DXも推進しないといけない。何とかしないともう後が無い。という状況。2015年時点で、IT人材の不足が約17万人とされていたところ、2025年には約43万人にまで上ります。既存のレガシーシステムの保守がブラックボックス、属人的になっており、DX化の足枷に → デジタル競争の敗者に技術的負債が溜まる一方保守運用の担い手不足で、サイバーセキュリティ事故が起きやすくこんな厳しい状況を打破するには、ユーザとベンダーそれぞれで対策していく必要があります。ユーザは人材・資金を保守からDXにシフトベンダーも同様に人材・資金を保守からDXにシフトベンダーはAI、アジャイル、マイクロサービス等最新技術を用いたビジネスにシフトやることはわかっていても、そう簡単にはいきません。ただし、スリーシェイクのSreake事業では、内製化支援も行っており、これまで数々の企業様の支援を行ってまいりました。Sreakeという商材は難しく、入社して1年が経った私もストンと腹落ちできる説明ができないままでしたが、「2025年の崖」をどう克服するかが我々のパーパスだと感じました。私は生成AIアプリケーション開発支援というDXを担当しておりますが、案件の推進を通して、「DX推進」を語れるようになっていきたいと思います。今回は、序章のような形で今後も2025年の崖について書いていければと思います。次の3-shake Advent Calendar 2024はシリーズ1がkechigonさんの「Google Cloud monitoringのアラートをGitHub issueに通知する」シリーズ2がtryu___さんの「kubebuilder使ってpodの監視してみた」です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[vector で kubernetes の container log を CloudWatch Logs に転送する]]></title>
            <link>https://qiita.com/yteraoka/items/df0777cdcb403a7af750</link>
            <guid>https://qiita.com/yteraoka/items/df0777cdcb403a7af750</guid>
            <pubDate>Sun, 01 Dec 2024 12:20:46 GMT</pubDate>
            <content:encoded><![CDATA[Vector とはvector は timber とともに買収され datadog がメンテナンスしているオープンソースプロジェクトのようです。(Datadog acquires Timber T…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[週間アトデ 2024-12-01]]></title>
            <link>https://blog.atusy.net/2024/12/01/atodeyomanakata/</link>
            <guid>https://blog.atusy.net/2024/12/01/atodeyomanakata/</guid>
            <pubDate>Sun, 01 Dec 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[アトデヨム、ウソジャナイ、ヨムノタノシー]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Geminiのビジネス利用でのメリットを語る]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/11/30/233039</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/11/30/233039</guid>
            <pubDate>Sat, 30 Nov 2024 14:30:39 GMT</pubDate>
            <content:encoded><![CDATA[この記事はGCP(Google Cloud Platform) Advent Calendar 2024 1日目の記事です。2024年はIT業界にとって、最も話題に上がったトピックは生成AIだったのではないでしょうか？2023年までは生成AIと家は、ChatGPTでしたが、2024年はGoogleがBardをリブランディングして、Gemini（ジェミニ）とし、しのぎを削っています。私はGoogle Cloudのパートナー企業である株式会社スリーシェイク Sreake事業部にて、Geminiを用いた生成AIアプリケーション開発に携わっており、Geminiのビジネス利用でのメリットを語りたいと思います。Gemini-1.5-Proは最大200万トークンの読み込みが可能Geminiの強みの中で、最も他の生成AIモデルと差別化できているのが、トークン数の長さです。これにより、動画解析などへの利用もしやすくなりました。Geminiはマルチモーダルなので、音声、画像、動画なども処理可能です。量の目安としては以下になります。書籍15〜20冊程度の分量動画約2時間音声約22時間BigQueryで容易にデータ分析基盤を構築可能他のクラウドには同様のサービスがなく、同じ機能を実現するためには複数のサービスを組み合わせる必要があります。AzureやAWS、オンプレのデータはそのままで読み込みだけ行う機能もあります。今お使いのシステム構成はほぼ変えず、追加構築可能となります。Geminiは他のモデルと比較してトークンあたりの利用料が安いGoogle Cloud上で稼働させるのに最適化しているためです。他社のクラウドで使える生成AIモデルは別会社のものなので、クラウドも生成AIもGoogleのGeminiによって、この点も強みです！もしもGeminiの出力結果が著作権侵害で係争が発生してもGoogle Cloudがサポート他クラウドにはないサービスです。こちらも、クラウドも生成AIも会社が揃っている強みと言えるでしょう。真実性1位！Gemini 1.5 ProがNIKKEI Digital Governanceが調査した真実性のスコアで1位となりました！以下の記事は最初日経で見れていたと思うのですが、今はNIKKEI Digital Governanceに登録しないと見れないようです。博識のGoogle､主観強いMeta　生成AIの｢真実性｣を検証上記画像は下記記事から引用させていただきました。note.com2024年もあと少し。2025年もGeminiとともに生成AIを盛り上げていきたいと思います！GCP(Google Cloud Platform) Advent Calendar 2024 次の記事はknak72さんによる企業のセキュリティ強化に！ Chrome Enterprise Premium のURLフィルタリングとマルウェアスキャン機能です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[3-shake Advent Calendar 2024 やっていきます #3SHAKE]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/30/142710</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/30/142710</guid>
            <pubDate>Sat, 30 Nov 2024 05:27:10 GMT</pubDate>
            <content:encoded><![CDATA[こんにちは、nwiizoです。晩秋の肌寒さが身にしみるこの11月も今日で終わりですね。ついこの前、昨年のAdvent Calendarで記事埋めを依頼され、慌ただしく準備した記憶が鮮明です。まったく、時の流れとは不思議なものです。今年もスリーシェイクのAdvent Calendarを開催することができます。この企画が実現したのは、ひとえに社内の方々の温かいご協力の賜物であり、その事実に深い感謝の念を抱いております。qiita.comスリーシェイクは「インフラをシンプルにしてイノベーションを起こす」というビジョンのもと、クラウド、セキュリティ、データ連携、HR領域で4つのサービスを展開しているテクノロジーカンパニーです。3-shake.com先日、シリーズB追加ラウンドとしてNTTデータ、SCSKから10億円の資金調達を実施し、資本業務提携を締結するニュースが出るなど、着実に成長を続けています。prtimes.jp今年のAdvent Calendarでも、エンジニアの技術的な記事だけでなく、スリーシェイクで働く様々な職種のメンバーによる記事をお届けする予定です。エンジニア以外にも営業、カスタマーサクセス、広報、経営企画など、多様なバックグラウンドを持つメンバーたちが、それぞれの視点からスリーシェイクでの経験や日々の発見を共有していきます。なぜ技術的な記事に限定しないのか。それは、私たちが目指すイノベーションには、技術だけでなく、様々な専門性や視点が必要だと考えているからです。このAdvent Calendarを通じて、スリーシェイクがどのような会社で、どんな人たちが働いているのか、より深く知っていただければ幸いです。記事の更新情報は、スリーシェイクの公式Xアカウント（@3shake_Inc）でお知らせしていきますので、ぜひフォローをお願いします！また、Advent Calendarも合わせてチェックしていただければと思います。成長を続けるスリーシェイクの「今」を知るきっかけとして、どうぞお楽しみください！jobs-3-shake.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[スリーシェイク、Google Cloud Japan の「 Google Cloud Partner Top Engineer 2025 」にて3名のエンジニアが受賞]]></title>
            <link>https://sreake.com/blog/%e3%82%b9%e3%83%aa%e3%83%bc%e3%82%b7%e3%82%a7%e3%82%a4%e3%82%af%e3%80%81google-cloud-japan-%e3%81%ae%e3%80%8c-google-cloud-partner-top-engineer-2025-%e3%80%8d%e3%81%ab%e3%81%a63%e5%90%8d%e3%81%ae/</link>
            <guid>https://sreake.com/blog/%e3%82%b9%e3%83%aa%e3%83%bc%e3%82%b7%e3%82%a7%e3%82%a4%e3%82%af%e3%80%81google-cloud-japan-%e3%81%ae%e3%80%8c-google-cloud-partner-top-engineer-2025-%e3%80%8d%e3%81%ab%e3%81%a63%e5%90%8d%e3%81%ae/</guid>
            <pubDate>Thu, 28 Nov 2024 06:03:47 GMT</pubDate>
            <content:encoded><![CDATA[株式会社スリーシェイク（本社：東京都新宿区、代表取締役社長：吉田 拓真、以下スリーシェイク）のエンジニア3名が、Google Cloud Japan が高い技術力を持ったエンジニアを表彰するプログラムである「 Google Cloud Partner Top Engineer 2025 」に選出されたことをお知らせします。The post スリーシェイク、Google Cloud Japan の「 Google Cloud Partner Top Engineer 2025 」にて3名のエンジニアが受賞 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[3-shake における組織的な Google Cloud Partner Top Engineer 推進について]]></title>
            <link>https://sreake.com/blog/google-cloud-partner-top-engineer-2025/</link>
            <guid>https://sreake.com/blog/google-cloud-partner-top-engineer-2025/</guid>
            <pubDate>Thu, 28 Nov 2024 06:00:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 3-shakeで、Engineering Team Lead / SRE をやっている横尾（@866mfs）です 今回、3-shake では、佐藤 慧太(@SatohJohn), 横尾 杏之介(@866mfs) […]The post 3-shake における組織的な Google Cloud Partner Top Engineer 推進について first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[コミュニティ紹介: Kubernetes Meetup Novice]]></title>
            <link>https://speakerdeck.com/bells17/komiyuniteishao-jie-kubernetes-meetup-novice</link>
            <guid>https://speakerdeck.com/bells17/komiyuniteishao-jie-kubernetes-meetup-novice</guid>
            <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Cloud Native Days Winter 2024のCommunity & Beginner LTでお話した資料です。https://pfem.notion.site/CNDW2024-Community-Beginner-LT-13821b0141e0800cb403c880cb4d2738]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[メインテーマはKubernetes]]></title>
            <link>https://speakerdeck.com/nwiizo/meintemahakubernetes</link>
            <guid>https://speakerdeck.com/nwiizo/meintemahakubernetes</guid>
            <pubDate>Thu, 28 Nov 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[2024年16:20-17:00（Track A）にて「メインテーマはKubernetes」というタイトルで登壇します。セッションURL:https://event.cloudnativedays.jp/cndw2024/talks/2373公式URL:https://event.cloudnativedays.jp/cndw2024/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KubeCon + Cloud NativeCon North America 参加レポート]]></title>
            <link>https://sreake.com/blog/kubecon-cloud-nativecon-north-america-2024-report/</link>
            <guid>https://sreake.com/blog/kubecon-cloud-nativecon-north-america-2024-report/</guid>
            <pubDate>Wed, 27 Nov 2024 00:28:01 GMT</pubDate>
            <content:encoded><![CDATA[はじめに こんにちは！3-shak inc, で SRE をやっている横尾(@866mfs)です。 2024/11/12 ~ 2024/11/15 に開催された、 KubeCon + CloudNativeCo […]The post KubeCon + Cloud NativeCon North America 参加レポート first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[mini.aiで日本語の括弧の中身をrepeatableに編集する]]></title>
            <link>https://blog.atusy.net/2024/11/27/mini-ai-ja/</link>
            <guid>https://blog.atusy.net/2024/11/27/mini-ai-ja/</guid>
            <pubDate>Wed, 27 Nov 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[mini.aiとremapをうまく活用するとaj[で「foo」のような日本語の括弧に対して発動するテキストオブジェクトをdot-repeatableに実装できることに気付きました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neovimのイベントタイミングガイド]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/27/023303</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/27/023303</guid>
            <pubDate>Tue, 26 Nov 2024 17:33:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめにNeovimでの設定やプラグイン開発において、適切なタイミングでコードを実行することは非常に重要です。このガイドでは、Neovimの主要なイベントについて、実用的な例を交えながら解説します。1. 起動時のイベント系統Neovimの起動プロセスで最も重要なイベントはVimEnterです。これは全ての初期化処理（vimrcの読み込み、プラグインの初期化など）が完了した後に発火します：vim.api.nvim_create_autocmd("VimEnter", {  callback = function()    -- プラグインの初期化    -- カラースキームの設定    -- ステータスラインの設定など  end,})2. バッファ操作のイベント系統バッファの作成から読み込みまでの主要なイベント：BufNew: バッファ作成直後BufAdd: バッファリストへの追加時BufReadPre: ファイル読み込み前BufReadPost: ファイル読み込み後BufEnter: バッファアクティブ化時vim.api.nvim_create_autocmd("BufReadPost", {  pattern = "*",  callback = function()    -- ファイル読み込み後の処理    -- 最後のカーソル位置の復元など  end,})3. 編集モードのイベント系統テキスト編集に関連する主要なイベント：InsertEnter: 挿入モード開始時TextChangedI: 挿入モードでテキスト変更時InsertLeave: 挿入モード終了時TextChanged: ノーマルモードでテキスト変更時vim.api.nvim_create_autocmd("InsertEnter", {  pattern = "*",  callback = function()    -- 挿入モード開始時の設定    -- 相対行番号の無効化など  end,})4. ファイル保存のイベント系統ファイル保存時の処理フロー：BufWritePre: 保存前BufWrite: 保存処理中BufWritePost: 保存後vim.api.nvim_create_autocmd("BufWritePre", {  pattern = "*",  callback = function()    -- 保存前の自動整形    -- 末尾の空白除去など  end,})5. 終了時のイベント系統Neovim終了時の処理順序：QuitPre: 終了コマンド実行時VimLeavePre: 終了処理開始前VimLeave: 最終終了処理時vim.api.nvim_create_autocmd("VimLeavePre", {  callback = function()    -- セッション保存    -- 未保存バッファの保存など  end,})実践的なサンプルコード以下は、よくある設定パターンの例です：-- ファイルタイプ別の設定vim.api.nvim_create_autocmd("FileType", {  pattern = {"python", "lua", "rust"},  callback = function()    local settings = {      python = { indent = 4, expandtab = true },      lua = { indent = 2, expandtab = true },      rust = { indent = 4, expandtab = true }    }    local ft = vim.bo.filetype    if settings[ft] then      vim.bo.shiftwidth = settings[ft].indent      vim.bo.expandtab = settings[ft].expandtab    end  end,})-- 自動保存の設定vim.api.nvim_create_autocmd({"InsertLeave", "TextChanged"}, {  pattern = "*",  callback = function()    if vim.bo.modified and vim.bo.buftype == "" then      vim.cmd("silent! write")    end  end,})-- 最後のカーソル位置を復元vim.api.nvim_create_autocmd("BufReadPost", {  pattern = "*",  callback = function()    local last_pos = vim.fn.line("'\"")    if last_pos > 0 and last_pos <= vim.fn.line("$") then      vim.cmd('normal! g`"')    end  end,})注意点イベントは適切な順序で処理される必要があります重い処理は非同期で行うことを推奨しますパターンマッチングを活用して、必要なファイルタイプのみで実行するようにしますvim.schedule()を使用して、UIブロッキングを避けます参考文献Neovim オートコマンドドキュメントAutocmd | 5. eventsNeovim Lua API リファレンスnvim_create_autocmd())]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rustでterraform plan/apply のターゲット指定を簡単にするツールを作ってみた - tfocusの仕組みと使い方]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/27/004309</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/27/004309</guid>
            <pubDate>Tue, 26 Nov 2024 15:43:09 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめにこんにちは、nwiizoです。本記事では、Terraformで特定のリソースだけをplan/applyするためのインタラクティブCLIツール「tfocus」の設計と実装について、Rustの学習という観点も交えながら詳しく解説していきます。github.comまた、良さそうであればGithub Starsをいただきたいです。2. 背景と動機2.1 開発の契機大規模なTerraformコードベースでの作業において、様々な課題に直面することがあります。本番環境で特定リソースにトラブルが発生した際の調査や、開発中の変更を検証する場合、また大規模な変更を段階的に適用する必要がある場合などが典型的な例です。従来のTerraform CLIでも-targetオプションでリソースを指定できますが、正確なリソースパスを記述する必要があり、緊急時の運用には適していません。特に本番環境でのインシデント対応時には、迅速かつ正確なリソース指定が求められます。developer.hashicorp.com2.2 解決したい問題ツールの開発にあたり、複数の課題解決を目指しています。まずリソース選択を直感的に行えるようにすることで、運用者の負担を軽減します。同時に操作ミスを未然に防ぐ仕組みを導入し、安全性を確保します。また、緊急時にも迅速な対応ができるインターフェースを実現し、効率的なデバッグ作業を可能にすることで、運用効率の向上を図ります。3. 技術スタックの選定3.1 Rustを選んだ理由Rustを採用した理由は複数あります。まず、ゼロコスト抽象化による高いパフォーマンスを実現できることが挙げられます。また、強力な型システムと所有権モデルにより、メモリ安全性を確保できます。さらに、様々なOS向けにネイティブバイナリを生成できるクロスプラットフォーム対応も重要な選定理由となりました。豊富なクレートが利用可能な充実したエコシステムも、開発効率を高める要因となっています。最後に、純粋な学習目的として、小規模なツール開発を通じてRustの理解を深めることも目指しています。何かを引用するために書籍を貼ったが何を引用したいか忘れてしまった(がぎりぎりでこのブログを書いている為に調べることができない)。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazon3.2 主要な依存クレート[dependencies]walkdir = "2.3"      # ファイルシステム走査regex = "1.5"        # パターンマッチングclap = "4.4"         # CLIパーサーthiserror = "1.0"    # エラー型colored = "2.0"      # カラー出力crossterm = "0.27"   # TUIfuzzy-matcher = "0.3" # あいまい検索doc.rust-lang.org各クレートの選定理由：walkdir: 効率的な再帰的ファイル走査を提供regex: 高速で柔軟なパターンマッチングが可能clap: 型安全なCLI引数パーサーthiserror: エラー型の簡潔な定義crossterm: プラットフォーム独立なTUI実装fuzzy-matcher: 使いやすいあいまい検索機能4. 実装の詳細4.1 アーキテクチャ設計プロジェクトは機能ごとに明確に分離された以下のモジュール構成を採用しています：src/├── cli.rs        # CLIインターフェース├── display.rs    # 表示処理├── error.rs      # エラー型├── executor.rs   # Terraform実行├── input.rs      # 入力処理├── main.rs       # エントリーポイント├── project.rs    # プロジェクト解析├── selector.rs   # リソース選択UI└── types.rs      # 共通型定義各モジュールの責務：cli.rs: コマンドライン引数の定義と解析#[derive(Parser)]#[command(author, version, about)]pub struct Cli {    /// Terraformディレクトリのパス    #[arg(short, long, default_value = ".")]    pub path: PathBuf,    /// 実行する操作    #[arg(short, long)]    pub operation: Option<Operation>,    /// 詳細出力の有効化    #[arg(short, long)]    pub verbose: bool,}project.rs: Terraformファイルの解析impl TerraformProject {    pub fn parse_directory(path: &Path) -> Result<Self> {        let mut project = TerraformProject::new();        for file_path in Self::find_terraform_files(path)? {            project.parse_file(&file_path)?;        }        Ok(project)    }    fn parse_file(&mut self, path: &Path) -> Result<()> {        let content = fs::read_to_string(path)?;        self.parse_resources(&content, path)?;        self.parse_modules(&content, path)?;        Ok(())    }}4.2 エラーハンドリング型安全なエラーハンドリングを実現するため、カスタムエラー型を定義：#[derive(Error, Debug)]pub enum TfocusError {    #[error("IO error: {0}")]    Io(#[from] std::io::Error),    #[error("Failed to parse terraform file: {0}")]    ParseError(String),    #[error("Invalid target selection")]    InvalidTargetSelection,    #[error("Terraform command failed: {0}")]    TerraformError(String),    #[error("No terraform files found")]    NoTerraformFiles,}4.3 リソース選択UIの実装fuzzy検索を活用した効率的なリソース選択：impl Selector {    fn filter_items(&mut self) {        let query = self.query.to_lowercase();        let mut matches: Vec<(usize, i64)> = self            .items            .iter()            .enumerate()            .filter_map(|(index, item)| {                self.matcher                    .fuzzy_match(&item.search_text.to_lowercase(), &query)                    .map(|score| (index, score))            })            .collect();                // スコアでソート        matches.sort_by_key(|&(_, score)| -score);        self.filtered_items = matches.into_iter()            .map(|(index, _)| index)            .collect();    }    fn render_screen(&mut self) -> Result<()> {        let mut stdout = stdout();        execute!(            stdout,            terminal::Clear(ClearType::All),            cursor::MoveTo(0, 0)        )?;                self.render_search_box()?;        self.render_items()?;        self.render_status_line()?;                stdout.flush()?;        Ok(())    }}4.4 パフォーマンス最適化実行速度とメモリ使用量の最適化：[profile.release]opt-level = 3        # 最高レベルの最適化lto = true          # リンク時最適化codegen-units = 1   # 単一コード生成ユニットstrip = true        # バイナリサイズ削減5. Rustから学ぶシステム設計tfocusの実装を通じて学べるRustの重要概念プログラミングRust 第2版作者:Jim Blandy,Jason Orendorff,Leonora F. S. TindallオライリージャパンAmazon5.1 所有権とライフタイムリソースの効率的な管理：impl Resource {    pub fn full_name(&self) -> String {        if self.is_module {            format!("module.{}", self.name)        } else {            format!("{}.{}", self.resource_type, self.name)        }    }}5.2 エラー伝播?演算子を使用した簡潔なエラーハンドリング：pub fn execute_terraform_command(    operation: &Operation,    target_options: &[String],) -> Result<()> {    let mut command = Command::new("terraform");    command.arg(operation.to_string());        for target in target_options {        command.arg(target);    }        let status = command.spawn()?.wait()?;        if status.success() {        Ok(())    } else {        Err(TfocusError::TerraformError(            "Command execution failed".to_string()        ))    }}5.3 トレイトの活用共通インターフェースの定義：pub trait Display {    fn render(&self) -> Result<()>;    fn update(&mut self) -> Result<()>;}6. まとめ6.1 現在の成果このプロジェクトは現在、直感的なリソース選択UIを実現し、クロスプラットフォームでの利用を可能にしています。また、効率的なメモリ使用を実現するとともに、型安全なエラーハンドリングを導入することで、安定性の向上にも成功しています。6.2 今後の展開使われるようになったらやっていきたいこと。機能拡張の面では、依存関係の可視化機能を導入し、リソース状態をより詳細に表示できるようにしたいと考えています。さらに、バッチ処理のサポートを追加することで、大規模な処理にも対応できるようにしていきます。品質向上については、テストカバレッジを拡大し、システム全体のパフォーマンスを最適化していく予定です。また、エラーメッセージをより分かりやすく改善することで、ユーザー体験の向上を図ります。ドキュメント整備においては、API文書を充実させ、初心者向けのチュートリアルを作成していきます。さらに、実際の使用シーンを想定したユースケース集を整備することで、ユーザーの理解促進を支援していきたいと考えています。おわりにtfocusの開発を通じて、RustとTerraformの実践的な活用方法を示しました。このツールが皆様のインフラ運用の一助となれば幸いです。コードはGitHubで公開しています：nwiizo/tfocusフィードバックやコントリビューションをお待ちしています。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでJSONを扱いたいのでSerde入門します]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/26/141035</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/26/141035</guid>
            <pubDate>Tue, 26 Nov 2024 05:10:35 GMT</pubDate>
            <content:encoded><![CDATA[はじめにRustでデータのシリアライズ/デシリアライズを扱う際、最も広く使われているのがserdeクレートです。特にWeb APIやファイル入出力でよく使用されるJSONとの相互変換において、非常に重宝するツールです。今回は、serdeの基本的な使い方と、開発効率を上げるためのツールについて解説します。SerdeとはSerdeは"Serialize"と"Deserialize"を組み合わせた造語で、データ構造の変換を担当するRustのフレームワークです。多様なデータフォーマットに対応（JSON、YAML(アーカイブされている)、TOML等）高性能で型安全な実装カスタマイズ可能な属性システムコード生成による簡単な実装docs.rsプロジェクトのセットアップまず、Cargo.tomlに必要な依存関係を追加します。[dependencies]serde = { version = "1.0", features = ["derive"] }serde_json = "1.0"基本的な使い方1. 構造体の定義use serde::{Serialize, Deserialize};#[derive(Serialize, Deserialize, Debug)]struct User {    name: String,    age: u32,    email: String,    is_active: bool,}2. JSONからRustへの変換（デシリアライズ）fn main() {    let json_str = r#"    {        "name": "John Doe",        "age": 30,        "email": "john@example.com",        "is_active": true    }    "#;    let user: User = serde_json::from_str(json_str).unwrap();    println!("Deserialized user: {:?}", user);}3. RustからJSONへの変換（シリアライズ）fn main() {    let user = User {        name: "Jane Doe".to_string(),        age: 25,        email: "jane@example.com".to_string(),        is_active: true,    };    let json = serde_json::to_string_pretty(&user).unwrap();    println!("Serialized JSON:\n{}", json);}JSON to Rust ツールの活用開発効率を大幅に向上させるツールとして、「JSON to Rust」があります。このツールは、JSONデータからRustの構造体定義を自動生成してくれます。JSON to Rustの使い方https://jsonformatter.org/json-to-rust にアクセス左側のペインにJSONデータを貼り付け自動的に右側にRustの構造体定義が生成される例えば、以下のようなJSONデータがあった場合{    "user_profile": {        "id": 123,        "username": "rust_lover",        "settings": {            "theme": "dark",            "notifications": true        },        "tags": ["rust", "programming"]    }}以下のようなRust構造体が生成されます。// Example code that deserializes and serializes the model.// extern crate serde;// #[macro_use]// extern crate serde_derive;// extern crate serde_json;//// use generated_module::[object Object];//// fn main() {//     let json = r#"{"answer": 42}"#;//     let model: [object Object] = serde_json::from_str(&json).unwrap();// }extern crate serde_derive;#[derive(Serialize, Deserialize)]pub struct Welcome3 {    #[serde(rename = "user_profile")]    user_profile: UserProfile,}#[derive(Serialize, Deserialize)]pub struct UserProfile {    #[serde(rename = "id")]    id: i64,    #[serde(rename = "username")]    username: String,    #[serde(rename = "settings")]    settings: Settings,    #[serde(rename = "tags")]    tags: Vec<String>,}#[derive(Serialize, Deserialize)]pub struct Settings {    #[serde(rename = "theme")]    theme: String,    #[serde(rename = "notifications")]    notifications: bool,}高度な使い方カスタム属性の活用Serdeは様々な属性を提供して、シリアライズ/デシリアライズの挙動をカスタマイズできます。#[derive(Serialize, Deserialize, Debug)]struct Configuration {    #[serde(rename = "apiKey")]    api_key: String,        #[serde(default)]    timeout_seconds: u32,        #[serde(skip_serializing_if = "Option::is_none")]    optional_field: Option<String>,}エラーハンドリング実際のアプリケーションでは、適切なエラーハンドリングが重要です。use serde::{Serialize, Deserialize};use std::error::Error;use std::fs;use std::io;use std::collections::HashMap;// ユーザーの基本構造体#[derive(Serialize, Deserialize, Debug)]struct User {    id: u32,    name: String,    age: u32,    email: String,    is_active: bool,    // オプショナルなフィールド    #[serde(skip_serializing_if = "Option::is_none")]    metadata: Option<HashMap<String, String>>,}// カスタムエラー型の定義#[derive(Debug)]enum UserError {    ParseError(serde_json::Error),    // JSONパースエラー    ValidationError(String),          // バリデーションエラー    DatabaseError(String),            // DB操作エラー    IoError(io::Error),              // ファイル操作エラー}// serde_json::ErrorからUserErrorへの変換を実装impl From<serde_json::Error> for UserError {    fn from(err: serde_json::Error) -> UserError {        UserError::ParseError(err)    }}// io::ErrorからUserErrorへの変換を実装impl From<io::Error> for UserError {    fn from(err: io::Error) -> UserError {        UserError::IoError(err)    }}// std::error::Errorトレイトの実装impl std::fmt::Display for UserError {    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {        match self {            UserError::ParseError(e) => write!(f, "Parse error: {}", e),            UserError::ValidationError(msg) => write!(f, "Validation error: {}", msg),            UserError::DatabaseError(msg) => write!(f, "Database error: {}", msg),            UserError::IoError(e) => write!(f, "IO error: {}", e),        }    }}impl Error for UserError {}// Userの実装impl User {    // バリデーションメソッド    fn validate(&self) -> Result<(), UserError> {        if self.name.is_empty() {            return Err(UserError::ValidationError("Name cannot be empty".to_string()));        }        if self.age > 150 {            return Err(UserError::ValidationError("Invalid age".to_string()));        }        if !self.email.contains('@') {            return Err(UserError::ValidationError("Invalid email format".to_string()));        }        Ok(())    }}// 基本的なJSONパース関数fn parse_user(json_str: &str) -> Result<User, serde_json::Error> {    // map_errを使用してエラーをログ出力    serde_json::from_str(json_str).map_err(|e| {        println!("Error parsing JSON: {}", e);        e  // 元のエラーを返す    })}// より詳細なエラーハンドリングを行う関数fn process_user_data(json_str: &str) -> Result<User, UserError> {    // JSONのパース    let user: User = serde_json::from_str(json_str)?;  // ?演算子でエラーを伝播        // バリデーション    user.validate()?;  // ?演算子でエラーを伝播        Ok(user)}// 複数ユーザーからの検索（Option型との組み合わせ）fn find_user_by_id(json_str: &str, target_id: u32) -> Result<Option<User>, UserError> {    // JSONから複数ユーザーをパース    let users: Vec<User> = serde_json::from_str(json_str)?;        // 指定されたIDのユーザーを探す    Ok(users.into_iter().find(|user| user.id == target_id))}// ファイル操作を含むエラーハンドリングfn load_user_from_file(path: &str) -> Result<User, UserError> {    // ファイルを読み込み    let content = fs::read_to_string(path).map_err(|e| {        eprintln!("Failed to read file {}: {}", path, e);        UserError::IoError(e)    })?;    // JSONをパースしてUserを返す    process_user_data(&content)}// ファイルへの保存fn save_user_to_file(user: &User, path: &str) -> Result<(), UserError> {    // UserをJSONに変換    let json = serde_json::to_string_pretty(user).map_err(|e| {        eprintln!("Failed to serialize user: {}", e);        UserError::ParseError(e)    })?;    // ファイルに書き込み    fs::write(path, json).map_err(|e| {        eprintln!("Failed to write to file {}: {}", path, e);        UserError::IoError(e)    })?;    Ok(())}fn main() {    // 1. 有効なJSONの例    let valid_json = r#"        {            "id": 1,            "name": "John Doe",            "age": 30,            "email": "john@example.com",            "is_active": true,            "metadata": {                "last_login": "2024-01-01",                "location": "Tokyo"            }        }    "#;    // 2. 無効なJSONの例（バリデーションエラー）    let invalid_json = r#"        {            "id": 2,            "name": "",            "age": 200,            "email": "invalid-email",            "is_active": true        }    "#;    // 3. 複数ユーザーのJSONの例    let users_json = r#"[        {            "id": 1,            "name": "John Doe",            "age": 30,            "email": "john@example.com",            "is_active": true        },        {            "id": 2,            "name": "Jane Doe",            "age": 25,            "email": "jane@example.com",            "is_active": true        }    ]"#;    // 4. 各種エラーハンドリングの実演    println!("1. 基本的なパース:");    match parse_user(valid_json) {        Ok(user) => println!("成功: {:?}", user),        Err(e) => println!("エラー: {}", e),    }    println!("\n2. バリデーション付きパース:");    match process_user_data(invalid_json) {        Ok(user) => println!("成功: {:?}", user),        Err(e) => println!("エラー: {}", e),    }    println!("\n3. ユーザー検索:");    match find_user_by_id(users_json, 1) {        Ok(Some(user)) => println!("ユーザーが見つかりました: {:?}", user),        Ok(None) => println!("ユーザーが見つかりません"),        Err(e) => println!("エラー: {}", e),    }    println!("\n4. ファイル操作:");    // 有効なユーザーをファイルに保存    if let Ok(user) = parse_user(valid_json) {        match save_user_to_file(&user, "user.json") {            Ok(()) => println!("ユーザーを保存しました"),            Err(e) => println!("保存エラー: {}", e),        }        // 保存したファイルから読み込み        match load_user_from_file("user.json") {            Ok(loaded_user) => println!("ロードしたユーザー: {:?}", loaded_user),            Err(e) => println!("ロードエラー: {}", e),        }    }}ベストプラクティス型の使い分け必須フィールドは通常の型オプショナルフィールドはOption<T>配列はVec<T>を使用エラーハンドリングunwrap()は開発時のみ使用本番コードではResultを適切に処理カスタム属性の活用#[serde(rename)]でフィールド名の変換#[serde(default)]でデフォルト値の設定#[serde(skip_serializing_if)]で条件付きスキップまず、Cargo.tomlにchronoの依存関係を追加します。use chrono;use serde::{Deserialize, Serialize};use std::collections::HashMap;use std::error::Error as StdError;use std::fmt;use std::fs; // chronoクレートのインポート// ベストプラクティスに基づいた構造体の定義#[derive(Serialize, Deserialize, Debug)]struct UserProfile {    // 1. 必須フィールド（通常の型）    id: u64,    username: String,    email: String,    // 2. オプショナルフィールド（Option<T>の使用）    #[serde(skip_serializing_if = "Option::is_none")]    phone_number: Option<String>,    #[serde(skip_serializing_if = "Option::is_none")]    biography: Option<String>,    // 3. 配列（Vec<T>の使用）    #[serde(skip_serializing_if = "Vec::is_empty")]    interests: Vec<String>,    // 4. カスタム属性の活用    // JSONでは"lastLoginTime"として表示    #[serde(rename = "lastLoginTime")]    last_login_time: String,    // デフォルト値の設定    #[serde(default)]    is_active: bool,    // 動的なキーバリューペア    #[serde(default, skip_serializing_if = "HashMap::is_empty")]    metadata: HashMap<String, String>,}// カスタムエラー型の定義#[derive(Debug)]enum ProfileError {    JsonError(serde_json::Error),    ValidationError(String),    IoError(std::io::Error),}// ProfileErrorにDisplayトレイトを実装impl fmt::Display for ProfileError {    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {        match self {            ProfileError::JsonError(e) => write!(f, "JSON error: {}", e),            ProfileError::ValidationError(e) => write!(f, "Validation error: {}", e),            ProfileError::IoError(e) => write!(f, "IO error: {}", e),        }    }}// ProfileErrorにErrorトレイトを実装impl StdError for ProfileError {    fn source(&self) -> Option<&(dyn StdError + 'static)> {        match self {            ProfileError::JsonError(e) => Some(e),            ProfileError::ValidationError(_) => None,            ProfileError::IoError(e) => Some(e),        }    }}// エラー変換の実装impl From<serde_json::Error> for ProfileError {    fn from(err: serde_json::Error) -> Self {        ProfileError::JsonError(err)    }}impl From<std::io::Error> for ProfileError {    fn from(err: std::io::Error) -> Self {        ProfileError::IoError(err)    }}// UserProfileの実装impl UserProfile {    // コンストラクタ    fn new(id: u64, username: String, email: String) -> Self {        UserProfile {            id,            username,            email,            phone_number: None,            biography: None,            interests: Vec::new(),            last_login_time: chrono::Utc::now().to_rfc3339(),            is_active: true,            metadata: HashMap::new(),        }    }    // バリデーション    fn validate(&self) -> Result<(), ProfileError> {        if self.username.is_empty() {            return Err(ProfileError::ValidationError(                "Username cannot be empty".to_string(),            ));        }        if !self.email.contains('@') {            return Err(ProfileError::ValidationError(                "Invalid email format".to_string(),            ));        }        Ok(())    }    // メタデータの追加    fn add_metadata(&mut self, key: &str, value: &str) {        self.metadata.insert(key.to_string(), value.to_string());    }    // 興味・関心の追加    fn add_interest(&mut self, interest: &str) {        self.interests.push(interest.to_string());    }}// プロファイル処理関数fn process_profile(json_str: &str) -> Result<UserProfile, ProfileError> {    // JSONからプロファイルを作成    let profile: UserProfile = serde_json::from_str(json_str)?;    // バリデーション    profile.validate()?;    Ok(profile)}// ファイル操作を含むプロファイル保存fn save_profile(profile: &UserProfile, path: &str) -> Result<(), ProfileError> {    // バリデーション    profile.validate()?;    // JSON文字列に変換（整形付き）    let json = serde_json::to_string_pretty(profile)?;    // ファイルに保存    fs::write(path, json)?;    Ok(())}fn main() -> Result<(), Box<dyn StdError>> {    // 1. プロファイルの作成    let mut profile = UserProfile::new(1, "john_doe".to_string(), "john@example.com".to_string());    // オプショナルフィールドの設定    profile.phone_number = Some("123-456-7890".to_string());    profile.biography = Some("Tech enthusiast and developer".to_string());    // 興味・関心の追加    profile.add_interest("Programming");    profile.add_interest("Open Source");    // メタデータの追加    profile.add_metadata("location", "Tokyo");    profile.add_metadata("timezone", "UTC+9");    // 2. JSONへの変換と保存    println!("保存するプロファイル:");    println!("{:#?}", profile);    save_profile(&profile, "profile.json").map_err(|e| Box::new(e) as Box<dyn StdError>)?;    println!("\nプロファイルを保存しました");    // 3. JSONからの読み込みとバリデーション    let json_str = r#"{        "id": 2,        "username": "jane_doe",        "email": "jane@example.com",        "phone_number": "098-765-4321",        "biography": "Software Engineer",        "interests": ["AI", "Machine Learning"],        "lastLoginTime": "2024-01-01T00:00:00Z",        "metadata": {            "location": "Osaka",            "language": "ja"        }    }"#;    match process_profile(json_str) {        Ok(loaded_profile) => {            println!("\n読み込んだプロファイル:");            println!("{:#?}", loaded_profile);        }        Err(e) => match e {            ProfileError::JsonError(e) => println!("JSONエラー: {}", e),            ProfileError::ValidationError(e) => println!("バリデーションエラー: {}", e),            ProfileError::IoError(e) => println!("I/Oエラー: {}", e),        },    }    // 4. 無効なデータの例    let invalid_json = r#"{        "id": 3,        "username": "",        "email": "invalid-email"    }"#;    match process_profile(invalid_json) {        Ok(_) => println!("予期せぬ成功"),        Err(e) => match e {            ProfileError::ValidationError(msg) => {                println!("\nバリデーションエラー（期待通り）: {}", msg)            }            _ => println!("予期せぬエラー"),        },    }    Ok(())}まとめSerdeは、RustでJSONを扱う際の強力なツールです。JSON to Rustのようなツールと組み合わせることで、より効率的な開発が可能になります。基本的な使い方を押さえた上で、プロジェクトの要件に応じて高度な機能を活用していくことをお勧めします。参考リンクSerde公式ドキュメントJSON to Rust Converterserde_json クレートドキュメント]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[社内活動の取り組み紹介~ スリーシェイクでこんな取り組みしてます ~]]></title>
            <link>https://speakerdeck.com/bells17/she-nei-huo-dong-noqu-rizu-mishao-jie-surisieikudekonnaqu-rizu-misitemasu</link>
            <guid>https://speakerdeck.com/bells17/she-nei-huo-dong-noqu-rizu-mishao-jie-surisieikudekonnaqu-rizu-misitemasu</guid>
            <pubDate>Tue, 26 Nov 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[CloudNative Days Winter 2024 船上LT会 小さな一歩、大きな飛躍〜クラウドネイティブを継続する〜 で発表したLT資料です。https://cloudnativedays.connpass.com/event/334620/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【ISUCON14対策】private-isuチャレンジ記]]></title>
            <link>https://zenn.dev/melanmeg/articles/a8ece09570279f</link>
            <guid>https://zenn.dev/melanmeg/articles/a8ece09570279f</guid>
            <pubDate>Mon, 25 Nov 2024 09:01:34 GMT</pubDate>
            <content:encoded><![CDATA[はじめに2024/12/8(日)に開催のISUCON14へ向けて練習した記録です。お題は private-isu と呼ばれるISUCON練習問題の中でも、特にメジャーなものを解くことにしました。結論として、最終スコアは44,2852点でした。ここに辿り着く道のりは険しかったです💦練習リポジトリ：https://github.com/melanmeg/private-isu-challengeまた、パフォーマンス改善タスクを求められる現場の方々にも、実際に役立つテクニックも恐らくあるはずで、興味のある方、色んな方に読んでいただけると嬉しいです。改善したことだけ知りたい方...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私の為のNvChadのキーマッピングガイド]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/24/171651</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/24/171651</guid>
            <pubDate>Sun, 24 Nov 2024 08:16:51 GMT</pubDate>
            <content:encoded><![CDATA[はじめに私は定期的に必要なことを忘れてしまう。子ども時代に水を口に入れて水の飲み方を忘れてしまったことがある。大切なことを今まで普通にできたことが急にできなくなることがある。学習もそう、定期的に復習して思い出すことが大切だと感じているが突然忘れてしまうことがある。突然忘れてしまうと探す必要があるが毎回探すのが面倒になってきたのでNvChadのキーマッピングをまとめてみた。基本的なショートカット表記<C> = Ctrlキー<leader> = スペースキー（デフォルト）<A> = Altキー<S> = Shiftキーよく使う機能とそのキーマッピング1. ファイル操作で必須のコマンド<C-s>    - 保存（これだけは絶対覚える。:w なんてやっているとvsCodeを使っている人にバカにされる）<C-c>    - ファイル全体をコピー（便利）<leader>fm - フォーマット（コードを整形してくれる）<leader>n  - 行番号の表示/非表示<leader>rn - 相対行番号の切り替え2. 検索系（Telescope）検索系は本当によく使うので、最優先で覚えたいです。<leader>ff - ファイル検索（最重要）<leader>fw - プロジェクト内のテキスト検索（grep）<leader>fb - 開いているバッファを検索<leader>fo - 最近開いたファイルを検索<leader>fz - 現在のバッファ内をあいまい検索<leader>cm - Gitコミットを検索<leader>gt - Gitのステータスを表示github.com3. LSP関連（コードジャンプ・リファレンス）コードリーディングする時に本当に助かる機能たちです。gd - 定義へジャンプ（最も使う）gr - 参照を探す（変数やメソッドの使用箇所を探せる）K  - ドキュメントを表示（カーソル位置の要素の説明を表示）gi - 実装へジャンプ（インターフェースから実装を探せる）<leader>ds - 診断情報をloclistに表示github.com4. 画面分割とウィンドウ移動複数のファイルを同時に見たい時に使います。<C-h> - 左のウィンドウへ<C-l> - 右のウィンドウへ<C-j> - 下のウィンドウへ<C-k> - 上のウィンドウへ5. バッファ操作<leader>b - 新しいバッファを開く<tab> - 次のバッファへ<S-tab> - 前のバッファへ<leader>x - バッファを閉じる6. ターミナル操作ターミナルは必要に応じて呼び出せます。<A-i> - フローティングターミナル（これが一番便利）<A-h> - 水平分割のターミナル<A-v> - 垂直分割のターミナル<C-x> - ターミナルモードを抜ける7. その他の便利機能<leader>ch - チートシート表示（キーマッピングを忘れた時用）<leader>/  - コメントアウトのトグル<C-n>     - ファイルツリーの表示/非表示<leader>e  - ファイルツリーにフォーカス<Esc>     - ハイライトをクリアなぜこれらのキーマッピングを覚える必要があるのか私の経験上、以下の機能は開発効率を大きく向上させてくれます。ファイル検索（Telescope）プロジェクト内のファイルを素早く見つけられるコードベースの把握が容易になるGit操作との連携で変更管理がしやすいLSP機能コードの定義や参照を素早く調べられるリファクタリングが楽になるコードの理解が深まるエラー診断が即座にわかるRust を書いていると 1 箇所書き換えると芋づる式に修正が発生するのでどうしても必要になる。ターミナル統合エディタを離れずにコマンドを実行できるgit操作やビルドが快適フローティング表示で作業の邪魔にならないバッファ管理複数ファイルの編集がスムーズ必要なファイルをすぐに切り替えられるなぜNvChadを選んだのか実は、私のエディタ遍歴は長い。最初はVimから始まり、その後SpaceVim、AstroNvim、LunarVimなど、様々なNeovim系のディストリビューションを試してきた。VSCodeやIntelliJ IDEAのVimプラグインも使っていた時期がある。その過程で、Vimのキーバインドの快適さと、モダンなIDEの便利さ、その両方の良さを実感していた。ただ、どれも何かが違った。なんとなくしっくりこない。そんな中で出会ったのがNvChadだった。そんな中でNvChadに出会い、決め手となったのは開発体制の健全さだった。リポジトリは定期的に更新され、ドキュメントも整備されている。破壊的な変更がある場合も、きちんとアナウンスされ、移行のガイドラインが提供される。コミュニティも活発で、問題が起きた時のサポートも期待できる。nvchad.comさらに、NvChadの設計思想も気に入った。必要最小限の機能を高速に動作させることを重視し、その上で必要な機能を追加できる拡張性を持っている。プラグインマネージャーにlazy.nvimを採用し、起動時間の最適化もされている。LSPやTreeSitterの統合も洗練されており、快適なコーディング環境を提供してくれる。結果として、NvChadは私の理想とするエディタ環境に最も近かった。Vimの哲学を大切にしながら、モダンな開発環境を実現している。もちろん、完璧なエディタは存在しないし、NvChadにも改善の余地はきっとある。しかし、現時点で最も信頼できる選択肢の一つであることは間違いない。Vimを学ぶために通常のVimを学ぶ場合は、「実践Vim 思考のスピードで編集しよう！」がおすすめだ。Vimの基本から応用までを体系的に学べ、実践的な例も豊富に掲載されている。実践Vim　思考のスピードで編集しよう！ (アスキー書籍)作者:Ｄｒｅｗ Ｎｅｉｌ,新丈 径角川アスキー総合研究所Amazonまた、Vim Adventuresというゲームも面白い。ゲーム感覚でVimのキー操作を学べ、楽しみながら基本的なコマンドが身につく。初心者にも優しい学習カーブで、Vimの世界に入るきっかけとして最適だ。vim-adventures.comしかし、NvChadはこれらの基本的なVimの知識に加えて、モダンなIDE的機能を提供してくれる。このガイドでは、特にNvChad特有の機能に焦点を当てて説明しました。私自身、日々の開発作業でNvChadの恩恵を受けており、その便利さを多くの人と共有したいと考えている。まとめ私はVimを使い始めて数年経つが、今でも新しい発見がある。NvChadも同様で、日々の作業の中で「こんな機能があったのか」と驚かされることが多い。最初は覚えることの多さに圧倒されるかもしれないが、焦る必要はない。私の経験では、まずは基本的なファイル操作から始めるのが良い。保存やコピーといった最低限の操作を確実に覚えることで、日常的な編集作業に支障がなくなる。次に、Telescopeによるファイル検索を習得すると、作業効率が格段に上がる。プロジェクト内のファイルを瞬時に探せるようになり、コードベースの把握も容易になる。その後、LSPの基本機能を学んでいくと良いだろう。定義ジャンプやドキュメント表示は、コードリーディングの強力な味方となる。ウィンドウ操作とバッファ管理、ターミナル操作は、これらの基本操作に慣れてから徐々に取り入れていけば良い。結局のところ、エディタは道具でしかない。完璧に使いこなす必要はなく、自分の作業をサポートしてくれる程度に理解していれば十分だ。このガイドも、そんな私のような「忘れっぽいプログラマー」のための備忘録として活用してもらえれば幸いだ。少しずつでも確実に、自分なりのNvChadの使い方を見つけていってほしい。参考リンクNvChad公式ドキュメントGitHub - NvChad/NvChadNeovim LSP Documentation]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[週間アトデ 2024-11-22]]></title>
            <link>https://blog.atusy.net/2024/11/22/atodeyomanakata/</link>
            <guid>https://blog.atusy.net/2024/11/22/atodeyomanakata/</guid>
            <pubDate>Fri, 22 Nov 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[アトデヨム、ウソジャナイ、ヨムノタノシー]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reckoner における Datadog Browser Test の活用事例 / Datadog Browser Test at Reckoner]]></title>
            <link>https://speakerdeck.com/nomadblacky/datadog-browser-test-at-reckoner</link>
            <guid>https://speakerdeck.com/nomadblacky/datadog-browser-test-at-reckoner</guid>
            <pubDate>Wed, 20 Nov 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Japan Datadog User Group Meetup#6https://datadog-jp.connpass.com/event/334594/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[先人の知見から学ぶ、その経験則]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/11/20/122114</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/11/20/122114</guid>
            <pubDate>Wed, 20 Nov 2024 03:21:14 GMT</pubDate>
            <content:encoded><![CDATA[この度、Cloud Native における最新の機能やベストプラクティスにおいての学びについて、登壇させていただくことになりました。このテーマについて私なりに取り留めのない思考を整理した考えを共有させていただきます。event.cloudnativedays.jpソフトウェアエンジニアリングの型についてソフトウェアの世界には、プログラミング言語における変数やデータの「型」とは別に、長年の経験と知恵から生まれた様々な型が存在します。ここでいう「型」とは、開発者の思考や行動のパターンを体系化したものを指します。これらの型は、プログラマーが日々直面する問題に対する体系的な解決策を提供します。こうした型は、文脈や状況によって様々な呼び方をします。例えば、同じような問題解決のアプローチでも、ある文脈では「パターン」、別の文脈では「ベストプラクティス」と呼ばれることがあります。また、同じような設計手法でも、技術スタックやチームの文化によって異なる名前で知られていることもあります。このように、型の呼び方は多様ですが、その本質は問題解決のための知恵の結晶であることは変わりません。そのため、このブログでは意図的に「定石」「パターン」「手法」「アプローチ」「作法」「ベストプラクティス」など、様々な呼び方を用いて型を説明していきます。これは、同じような概念や手法が異なる文脈で別の名前で呼ばれている実態を反映させるためです。それぞれの呼び方が持つニュアンスの違いを理解することで、型に対するより深い理解が得られると考えています。いろんな名前の型の種類と特徴まず「定石」は、特定の状況下での最適な対処方法を示します。例えば、データベースにおけるN+1問題の解決方法やメモリリーク対策の手順など、具体的な技術的課題に対する確立された解決策です。次に「パターン」は、一般的な設計上の問題に対する標準的な解決策を提供します。いくつかの文脈で登場しますがコードやソフトウェアの構造化と再利用性を高めます。「手法」は開発プロセスを改善するための具体的な方法論を指します。テスト駆動開発（TDD）、リファクタリング、継続的インテグレーションなどが該当し、より体系的な開発アプローチを可能にします。「アプローチ」は問題解決への基本的な考え方や戦略を示し、ドメイン駆動設計（DDD）やマイクロサービスアーキテクチャなどが含まれます。また、「作法」はコードの品質と保守性を高めるための慣習を表します。SOLID原則、クリーンコード、命名規則などがこれにあたり、チーム開発における共通理解を促進します。「ベストプラクティス」は実践で効果が実証された推奨される方法であり、セキュリティ対策、パフォーマンスチューニング、エラー処理などの具体的な実装手法を含みます。他にも同じような文脈なのにいろんな言い方の「型」があります。aws.amazon.comlearn.microsoft.comcloud.google.com型の重要な特性これらの型には、いくつかの重要な特性があります。まず状況依存性があり、プロジェクトの規模や要件、チームの習熟度、ビジネスドメインによって最適な型が変化します。また、進化と適応の性質も持ち合わせており、新しい技術の登場により型自体が進化したり、既存の型が新しい文脈で再解釈されたり、チームの経験を通じて洗練されていきます。さらに、相互補完性も重要な特性です。複数の型を組み合わせることで相乗効果が生まれ、異なる型が互いの弱点を補完し合います。状況に応じて型を柔軟に組み合わせることが、効果的な問題解決には不可欠です。このように、ソフトウェアにおける「型」は、単なる規則や制約ではなく、効果的な問題解決のための知識体系として機能しています。これらの型を理解し、適切に活用することで、より効率的で品質の高い開発が可能になります。型の存在を認識し、その本質を理解することは、プログラマーとしての成長において重要な要素となるでしょう。プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則作者:上田勲秀和システムAmazonどの巨人の型に乗るのか？ソフトウェアの世界で「定石」を学ぶことは、ある種の賭けのような性質を持っています。最初は論理的な理解が難しい概念や方法論を受け入れる必要があるにもかかわらず、その価値は実践してみないとわからないという矛盾を抱えているためです。多くの場合、「きっと将来役立つはず」という信念に基づいて学習を進める必要があります。この学習における矛盾は、特に高度な開発手法を習得する際に顕著に現れます。例えば、テスト駆動開発（TDD）の習得では、最初はテストを先に書くという一見非効率に思える手法に違和感を覚えるでしょう。しかし、この手法の真価は、実際にプロジェクトで実践し、コードの品質向上や保守性の改善を体験してはじめて理解できます。同様に、アーキテクチャ設計原則の導入においても、初期段階では過度に複雑に感じられる設計パターンや抽象化の価値を理解することは困難です。デザインパターンの学習や関数型プログラミングの考え方も、習得には相当な時間と労力を要します。これらの知識は、直接的な効果が見えにくい一方で、長期的には開発効率と品質を大きく向上させる可能性を秘めています。このジレンマを乗り越えるためには、段階的な学習アプローチと実践を通じた検証が重要になります。小規模なプロジェクトや個人的な開発で新しい手法を試し、その効果を実感することから始めることで、より大きなプロジェクトでの適用に向けた確信と経験を積むことができます。ルールズ・オブ・プログラミング ―より良いコードを書くための21のルール作者:Chris Zimmermanオーム社Amazon作法の習得における難しさ確立された手法（パターン）の習得には、独特の困難さが伴います。その中でも特に重要な課題として、習得前後のジレンマと成長段階による最適解の変化が挙げられます。まず、習得前後のジレンマについて考えてみましょう。体得するまでは本当の価値がわからないという特徴は、多くの開発手法に共通しています。例えば、ある設計パターンを学び始めた時点では、それがどのような状況でどれほどの効果を発揮するのか、具体的にイメージすることが困難です。さらに厄介なことに、体得してしまうと、その影響を客観的に評価しづらくなるという逆説的な問題も存在します。手法が無意識のうちに身についてしまうと、その手法を使わない場合との比較が難しくなり、問題が発生した際に、その原因がパターンの適用にあるのか、それとも他の要因によるものなのか、判断が困難になってしまいます。次に、成長段階による最適解の変化について着目する必要があります。プログラマーとしての習熟度によって最適な手法が変わるというのは、多くの現場で観察される現象です。例えば、初級者の段階では、まずはシンプルな実装手法に焦点を当て、基本的なプログラミングスキルを確実に身につけることが重要です。中級者になると、設計パターンの理解と適切な適用が課題となり、コードの構造化や再利用性を意識した開発が求められるようになります。上級者では、さらに進んで、パターンの取捨選択や状況に応じた最適化が必要となります。また、チームの規模や製品の成熟度によっても適切なアプローチは変化します。小規模なチームでは比較的シンプルな設計で十分な場合でも、チームが大きくなるにつれて、より体系的なアプローチが必要となることがあります。同様に、プロダクトの初期段階では迅速な開発を優先し、成熟期に入ってからより洗練された設計パターンを導入するなど、状況に応じた柔軟な対応が求められます。このように、作法の習得プロセスは単純な知識の蓄積ではなく、様々な要因を考慮しながら、継続的に改善と適応を行っていく必要のある複雑な取り組みと言えます。私たちはどう学んでいるのか　――創発から見る認知の変化 (ちくまプリマー新書)作者:鈴木宏昭筑摩書房Amazon不適切なパターンを見分けるための3つの条件複雑さという落とし穴不適切なパターンの最も顕著な特徴は、シンプルさの欠如です。優れたパターンには、核となる概念がシンプルで説明が簡潔であり、様々な状況への応用が柔軟に可能という特徴があります。このシンプルさは、単なる実装の簡素さだけでなく、パターンが解決しようとする問題と解決方法の関係性が明確であることを意味します。一方で、複雑な条件分岐が多い実装手法や、例外処理が複雑に絡み合ったエラーハンドリング、過度に抽象化された設計パターンなどは、保守性を低下させる要因となりかねません。特に、抽象化の層が必要以上に深くなると、コードの見通しが悪くなり、バグの温床となる可能性があります。シンプルさを欠いたパターンは、チームメンバー間での共有や理解を困難にし、結果として開発効率の低下やメンテナンスコストの増大を招くことがあります。批判を許さない教条主義検証がタブー視されている状況は、不適切なパターンの存在を示す重要な指標です。「それが会社の方針だから」という説明やレガシーコードの無批判な踏襲、特定の実装パターンへの過度な信仰は、危険な兆候と言えます。このような状況では、パターンの有効性や適用範囲について、客観的な評価や建設的な議論が行われにくくなります。定石の効果は常に検証可能であるべきであり、新しい技術やアプローチとの比較検討を行える環境が必要です。また、チーム内で改善提案が歓迎される雰囲気を醸成することも、健全なパターン活用には不可欠です。例えば、定期的なコードレビューやアーキテクチャ検討会での議論、実装パターンの効果測定など、具体的な検証の機会を設けることが重要です。パターンの効果や適用方法について、オープンな議論と継続的な改善が可能な環境を整えることで、より適切なパターンの選択と進化が促進されます。また、新しいチームメンバーからの質問や疑問を歓迎する文化を作ることで、既存のパターンの妥当性を定期的に見直すきっかけにもなります。魔法の解決策という幻想パターンに対する過度な期待は、不適切な適用を引き起こす大きな要因です。特定のアーキテクチャやパターンへの過度な期待や、新しいフレームワークやツールへの盲目的な信仰は、実装の複雑化や運用コストの増大を引き起こす可能性があります。特に、「銀の弾丸」を求める姿勢は、現実的な問題解決を見失わせる原因となりかねません。どんなパターンにも適用範囲や限界があることを認識し、状況に応じた適切な選択を行うことが重要です。例えば、マイクロサービスアーキテクチャは分散システムの柔軟性を高める可能性がありますが、運用の複雑さやネットワークの信頼性など、新たな課題も同時にもたらします。期待と現実のギャップを冷静に評価し、パターンの適用による実際の効果を慎重に見極める必要があります。これには、パターン導入前後での定量的な指標の比較や、チームメンバーからのフィードバック収集、実際のユーザーへの影響分析など、多角的な評価アプローチが求められます。また、パターンの導入は段階的に行い、各段階での効果を確認しながら進めることで、リスクを最小限に抑えることができます。仮説思考―ＢＣＧ流　問題発見・解決の発想法 内田和成の思考作者:内田 和成東洋経済新報社Amazon定石の進化と検証確立された手法は、暫定的な真実としての性質を持っています。これは、定石が先人の経験則の集大成として形成されながらも、常に改善の余地があるという特徴を示しています。時代とともに技術は進化し、新しい方法論が生まれることで、既存の定石が見直されたり置き換わったりすることは珍しくありません。この変化を受け入れ、柔軟に適応していく姿勢が重要です。また、定石の適用には段階的な最適化が必要です。プロジェクトの初期段階では、迅速な開発とフィードバックループの確立を重視した手法が有効です。その後、サービスがスケールしていく段階では、パフォーマンスや保守性を考慮したパターンの導入が必要となってきます。さらに、プロダクトが成熟期に入ったメンテナンスフェーズでは、長期的な運用を見据えた定石の適用が求められます。このように、プロジェクトのライフサイクルに応じて、適切な手法を選択し組み合わせていくことが重要です。そして、これらの手法の有効性を担保するためには、継続的な検証が不可欠です。具体的には、パフォーマンス指標による定量的な評価や、実際のユーザーからのフィードバックの収集、さらにはチーム内での定期的な振り返りを通じて、採用している手法の効果を多角的に検証する必要があります。この検証プロセスを通じて、チームは定石の適用方法を改善し、より効果的な開発プラクティスを確立することができます。このような進化と検証のサイクルを通じて、定石は単なる形式的なルールではなく、実践的で価値のある知識体系として発展していきます。重要なのは、定石を固定的なものとして捉えるのではなく、常に改善と適応を繰り返す生きた知識として扱うことです。それによって、チームは変化する要求や技術環境に柔軟に対応しながら、より効果的な開発プロセスを実現することができます。論点思考作者:内田 和成東洋経済新報社Amazonおわりにそもそも、Kubernetesは型の集大成とも言える存在です。PodやDeployment、Service、Operatorなど、その設計思想には分散システム開発における長年の経験と知恵が型として結晶化されています。Kubernetesの各機能は、それぞれが独立した型でありながら、組み合わさることでより大きな価値を生み出しており、まさにここで議論してきた型の相互補完性を体現していると言えるでしょう。ソフトウェアにおける定石やパターンとの付き合い方は、プログラマーとしての成長において重要な要素となります。ここで重要なのは、バランスの取れたアプローチです。定石を完全に否定せず、かといって盲目的にも従わないという姿勢を保ちながら、常に検証と改善を心がけることが大切です。チームや製品の成長に合わせて手法を進化させていくことで、より効果的な開発プロセスを確立することができます。また、開発手法の習得には継続的な学習のサイクルが不可欠です。まずは基本的なパターンを学び実践するところから始め、経験を積みながら定石の本質を理解していきます。その過程で、状況に応じて手法を適応させたり改善したりすることで、より深い理解と実践的なスキルを身につけることができます。さらに、未来への視点を持つことも重要です。現在の課題解決だけでなく、将来の拡張性も考慮に入れた選択を心がけます。新しい技術やアプローチに対してオープンな姿勢を保つことで、より良い解決策を見出す可能性を広げることができます。また、チーム全体での知識と経験の共有を促進することで、組織としての成長も期待できます。定石やパターンは確かに重要な指針となりますが、それは絶対的な真理ではありません。状況や文脈に応じて、柔軟に解釈し適用していく必要があります。プログラマーとして成長するには、確立された手法を理解し、適切に活用しながら、常に改善と進化を続けることが重要です。この継続的な学習と適応のプロセスこそが、真に効果的な開発手法の確立につながるのです。このような姿勢で開発に取り組むことで、個人としての技術力向上だけでなく、チーム全体の生産性と品質の向上にも貢献することができます。ソフトウェアの世界は常に進化し続けており、その中で成長し続けるためには、確かな基礎と柔軟な思考を併せ持つことが不可欠なのです。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[プロンプトエンジニアリングハンズオン]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/11/19/172452</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/11/19/172452</guid>
            <pubDate>Tue, 19 Nov 2024 08:24:52 GMT</pubDate>
            <content:encoded><![CDATA[genai-users.connpass.com↑上記勉強会の資料です。shu-kob.hateblo.jp↑こちらの記事の「プロンプトエンジニアリング」もご覧ください。（上記記事はChatGPT）先ずは復習してみましょうGeminiにアクセスgemini.google.comお好みで、ChatGPTやClaudeを使用しても構いません。プロンプトエンジニアリングJSON形式で出力カレーのレシピを教えてください。出力は以下のJSON形式にしてください。==============={  "材料": ["材料1", "材料2"],  "手順": ["手順1", "手順2"]}==============={  "材料": [    "玉ねぎ 1個",    "にんじん 1本",    "じゃがいも 2個",    "牛肉 300g",    "カレールー 2箱",    "サラダ油 大さじ2",    "水 800ml",    "塩 少々",    "こしょう 少々"  ],  "手順": [    "玉ねぎ、にんじん、じゃがいもは食べやすい大きさに切る。",    "鍋にサラダ油を熱し、牛肉を炒め、色が変わったら玉ねぎを加えて炒める。",    "にんじん、じゃがいもを加えてさらに炒める。",    "水を加え、アクを取りながら煮込む。",    "野菜が柔らかくなったらカレールーを加えて溶かす。",    "塩こしょうで味を調え、5分ほど煮込む。"  ]}出力結果をJSON Formatterで確認JSON Formatter & ValidatorZero-shotプロンプティングZero-Shotプロンプティングとは、大規模言語モデル（LLM）に対して、事前に具体的な例を示すことなく、一度の指示でタスクを実行させるプロンプトエンジニアリング手法です。以下はニュース記事のタイトルです。「政治」「経済」「芸能」「スポーツ」「科学」「その他」のうち1つに分類してください。回答だけ一言で出力してください===========================紅白出場歌手の選考基準 NHK公開芸能Few-shotプロンプティングFew-Shotプロンプティングは、大規模言語モデル（LLM）に対して、タスクを実行するための具体的な例をいくつか示すことで、モデルの性能を向上させるプロンプトエンジニアリングの手法です。Zero-Shotプロンプティングとの違いZero-Shotプロンプティングが、一切の例を示さずにタスクを実行させるのに対し、Few-Shotプロンプティングでは、少数の例を提示することで、モデルがより効率的にタスクを学習できるようにします。以下はニュース記事のタイトルです。「政治」「経済」「芸能」「スポーツ」「科学」「その他」のうち1つに分類してください。回答だけ一言で出力してください===========================「紅白出場歌手の選考基準 NHK公開」===========================以下は例です「G20 バイデン氏不在で集合写真」：政治「岡田将生&高畑充希結婚 SNS反応」：芸能芸能Zero-shot Chain-of-ThoughtプロンプティングZero-shot Chain-of-Thought（CoT）プロンプティングは、大規模言語モデル（LLM）に対して、具体的な例を示すことなく、段階的な推論プロセスを踏ませるように指示するプロンプトエンジニアリングの手法です。回答だけ一言で出力してください。10 + 2 * 3 - 4 * 28ステップバイステップで考えてください。10 + 2 * 3 - 4 * 2## ステップバイステップで解いてみましょう！**与えられた式:** 10 + 2 * 3 - 4 * 2**解き方:**数学では、**掛け算と割り算**は、**足し算と引き算**よりも先に計算します。これを**優先順位**と言います。1.  **掛け算を計算する:**    * 2 * 3 = 6    * 4 * 2 = 8    式は、10 + 6 - 8 に変わります。2.  **残りの計算を行う:**    * 10 + 6 = 16    * 16 - 8 = 8**答え:** 8**まとめ:**与えられた式をステップバイステップで解くと、答えは**8**になります。**ポイント:*** **優先順位:** 掛け算と割り算は、足し算と引き算よりも先に計算します。* **計算順序:** 左から右へ計算していきます。**補足:**もし、計算の順番を変えたい場合は、**かっこ**を使います。例えば、(10 + 2) * 3 - 4 * 2 のようにかっこでくくると、最初に10と2を足してから3をかけることになります。**この説明で理解できましたか？** もし、まだ疑問な点があれば、遠慮なく質問してくださいね。Few-shot Chain-of-ThoughtプロンプティングFew-shot CoTプロンプティングは、大規模言語モデル（LLM）に、いくつかの例を示しながら、段階的な推論プロセスを踏ませることで、より複雑な問題を解かせる手法です。このグループの奇数を合計すると偶数になります。: 4、8、9、15、12、2、1。A: 奇数を全て加えると(9, 15, 1)25になります。答えはFalseです。このグループの奇数を合計すると偶数になります。: 17、10、19、4、8、12、24。A: 奇数を全て加えると(17, 19)36になります。答えはTrueです。このグループの奇数を合計すると偶数になります。: 16、11、14、4、8、13、24。A: 奇数を全て加えると(11, 13)24になります。答えはTrueです。このグループの奇数を合計すると偶数になります。: 17、9、10、12、13、4、2。A: 奇数を全て加えると(17, 9, 13)39になります。答えはFalseです。このグループの奇数を合計すると偶数になります。: 15、32、5、13、82、7、1。A:参考文献LangChainとLangGraphによるRAG・AIエージェント［実践］入門10倍速で成果が出る！ChatGPTスゴ技大全]]></content:encoded>
        </item>
    </channel>
</rss>