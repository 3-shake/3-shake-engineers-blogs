<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Thu, 24 Apr 2025 22:35:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[2025年4月版読んでいて良かった本紹介]]></title>
            <link>https://zenn.dev/akasan/articles/9b2e5528548353</link>
            <guid>https://zenn.dev/akasan/articles/9b2e5528548353</guid>
            <pubDate>Thu, 24 Apr 2025 13:32:57 GMT</pubDate>
            <content:encoded><![CDATA[今回はいつもに比べてさらに短編になりますw特に技術書に絞りまして、最近または過去読んでいて良かったと思う本について紹介していこうと思います。 クラウド系 徹底攻略 Google Cloud認定資格 Associate Cloud Engineer教科書クラウドといえば、Google Cloudに入門するために買ったこの本がまず第一ですね。Associate Cloud Engineerを取るために大変お世話になりました。https://book.impress.co.jp/books/1122101107 Google Cloudではじめる実践データエンジニアリング入門...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2025年4月、AIとクラウドネイティブの交差点で語った2日間の記録 #CNDS2025 #hack_at_delta]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/04/24/113500</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/04/24/113500</guid>
            <pubDate>Thu, 24 Apr 2025 02:35:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこんにちは、nwiizoです。2025年4月22日と23日、スケジュールの都合で連続して2つの技術イベントに登壇することになりました。それぞれのイベントは異なる切り口でしたが、どちらも「生成AI」をテーマにしたものでした。1日目は「生成AI」と「クラウドネイティブ」の融合、2日目は「生成AI」の「Model Context Protocol（MCP）」に焦点を当てました。生成AI技術は近年急速に進化し、私たちエンジニアの働き方に大きな影響を与えています。私自身も数年前からnvimでGitHub Copilotを日常的に使い、その後Clineなどのコーディングエージェントブームのバズに押されつつもCursorやVSCodeを利用しています。同時に、Cloud Native技術も着実に成熟し、多くの企業のインフラ戦略の中核となっています。現在、これら二つの技術領域が交わることで、特にIaC（Infrastructure as Code）分野での応用が活発化しています。多くの開発者がこの統合に関して様々な課題に直面しており、今回の登壇では、そうした課題に対する私なりの考察と解決策を共有しました。本ブログでは、この2日間の登壇内容を振り返りながら、技術的な洞察やコミュニティでの議論から得た気づきを記録したいと思います。生成AIとクラウドネイティブ技術の統合が開発・運用プロセスを根本から変革しています。本稿はエンジニアが直面する「70%問題」（AIがコードの70%は正確に生成するが、残り30%で致命的ミスを犯す現象）に対して、ガードレールとModel Context Protocol (MCP)の相補的活用による解決策を提案します。インフラ/アプリケーションエンジニアは「思考パートナー」としてAIを活用し、検証文化を確立することで、開発効率と品質を両立できます。本記事では、両イベントでの登壇内容をもとに、AIを単なるツールから戦略的パートナーへと位置づけ直す視点と、認知労働の新たな分担を実現する実践的なフレームワークについて詳しく解説します。Day 1: 生成AIとCloud Nativeの融合を語るイベント: CloudNative Days Summer 2025 プレイベント登壇タイトル: 生成AIによるCloud Native 基盤構築の可能性と実践的ガードレールの敷設について日時: 2025年4月22日cloudnativedays.connpass.com1日目は、CloudNative Days Summer 2025のプレイベントに参加しました。このイベントの参加者層は主にインフラエンジニアやSRE（Site Reliability Engineer）が中心で、Cloud Native技術への関心が高い方々です。私のセッションでは、生成AIを活用したCloud Native基盤構築について、実践的な観点から解説しました。発表資料 speakerdeck.com発表の詳細セッション内容は以下の4つの大きなセクションに分けて構成しました：1. 生成AIとCloud Nativeの現在地（2025年）まず、現在の生成AIによる開発プロセスの変化について解説しました。従来のコード生成から問題解決支援へと進化しており、AIは単なる「道具」から「思考パートナー」へと変わりつつあります。これは根本的な変化であり、単なる機能向上ではありません。AIが書かれるコードの構文パターンだけでなく、構築されるものの概念モデルに関与できるようになった結果、協働のダイナミクスが質的に変化しています。AIの利用パターンも単発指示→会話型→継続的協働へと発展し、長期的な文脈理解ができるようになっています。これにより、開発ワークフローも大きく変化しています。- コードレビューの前段階をAIが担当し、人間は高次の設計判断に集中- ボイラープレートコードからの解放で、より創造的な作業への時間が増加- テスト品質の標準化によるソフトウェア信頼性の向上しかし実際のところ、AIによるCloud Native実装は「完璧」ではなく、「ある程度必要」な取り組みとして捉えるべきだと強調しました。現場では、以前は「動かない定義」や「架空の機能」に悩まされましたが、モデルの精度向上により問題は大幅に減少しています。それでも、いわゆる「ハルシネーション」と呼ばれる問題は依然として存在するため、AIの出力を盲信せず、検証する姿勢が重要です。特にIaC（Infrastructure as Code）においては、コードと実際のインフラの間に差異が生じることも珍しくありません。AIが生成したインフラ定義は、理想的な環境を想定していることが多く、実際の環境の制約やレガシーシステムとの互換性といった現実的な問題に対応できていないケースがあります。そのため、多くの組織では完全自動化ではなく、ある程度抽象化したり省力化したりしながら、人間による確認と調整を組み合わせたハイブリッドなアプローチを採用しています。これにより、AIの効率性と人間の判断を最適に組み合わせたCloud Native環境の管理が実現されています。learning.oreilly.com2. 実践的なプロンプト設計効果的なAI活用のための「プロンプト設計の5原則」を紹介しました：方向性を与える（Give Direction）具体的な指示や目的を明確に示す例：「高可用性と費用対効果を重視したプロダクション環境向けECSクラスタを作成するTerraformコード」のように具体的にフォーマットを指定する（Specify Format）望ましい出力形式を明確に定義する例：コーディングスタイル、ファイル分割方針などを明示的に記述例を提供する（Provide Examples）期待する出力のサンプルを示す既存の成功パターンを参考に提示する品質を評価する（Evaluate Quality）生成された結果の品質を測定・改善する方法を組み込むセキュリティ、可用性、コスト最適化などの観点を明示作業を分割する（Break Down Tasks）複雑なタスクをより小さな段階に分割するステップバイステップのアプローチを促すこれらの原則を実践することで、生成AIからより質の高い出力を得られることを実例とともに解説しました。また、コンテキスト同梱の重要性についても言及し、意思決定の背景や根拠を明示的に残すことで、組織の暗黙知が形式知化される利点を強調しました。learning.oreilly.com3. ガードレールの構築の手引き生成AIの出力に対する「ガードレール」の重要性を解説しました。ここで特に強調したのが「70%問題」です。これは単なる効率の問題ではなく、ロジスティクスにおける「ラストマイル問題」やロボティクスにおける「不気味の谷」に類似した現象です。完成に近づくほど、残りの課題は不釣り合いに困難になります。しかし、インフラストラクチャにおいて、この残りの30%は単に非効率なだけでなく、潜在的に壊滅的な問題を引き起こす可能性があります。生成AIは通常、コードの約70%は驚くほど正確に生成できますが、残りの30%で致命的なミスを犯すことがあります。特にIaCのような厳密性が求められる領域では、この問題が顕著です。AWS IAMポリシー生成時に過剰な権限を付与する傾向リソース間の複雑な依存関係の理解不足コスト最適化を考慮しない設計提案これを「優秀だが何も確認しない若手開発者」と表現し、スピードは速いがIaC特有の制約を無視してしまう傾向があることを指摘しました。この問題への対策として、以下のようなガードレールを提案しました：コード品質検証構文チェック、静的解析、コーディング規約の自動適用セマンティック検証リソース間の整合性や依存関係の正確性を検証セキュリティ検証脆弱性スキャン、最小権限原則の適用コンプライアンス検証組織ポリシーや法規制への適合性確認コスト最適化検証リソース効率や予算管理の自動チェックこれらのガードレールは、特にPull Requestの段階で自動適用することで、問題の早期発見と修正を可能にします。また、単なる検証だけでなく、AIの解釈コストを考慮した仕様の記述方法についても言及しました。syu-m-5151.hatenablog.com4. ガードレールを超えて行動するMCP最後に、Model Context Protocol（MCP）を活用した次世代のAI活用法について紹介しました。MCPはAIモデルが外部ツールやデータにアクセスするための標準プロトコルで、「AIとシステムをつなぐUSB規格」とも表現できます。しかし、この比喩は理論的な重要性を過小評価しています。USBは物理的な接続を標準化しましたが、MCPは認識論的な接続—知識がどのようにアクセス、検証、適用されるかを標準化しているのです。MCPとガードレールの補完関係は弁証法的関係とも言えます。ガードレールは出力の「安全性」「品質」を確保（アウトプット品質）MCPは入力の「情報量」「正確性」を向上（インプット品質）この相補的な関係は、次のような弁証法的パターンを形成します。テーゼ：AIは限られたコンテキストに基づいてコードを生成アンチテーゼ：人間はガードレールを通じてこのコードを検証・修正統合：MCPはAIのコンテキストを拡張し、検証の必要性を減少（ただし排除はしない）両者を組み合わせることで、70%問題の克服に近づける可能性を示しました。ただし、人間の判断の必要性は排除されるのではなく、人間の役割が「構文の検証者」から「概念的アプローチの検証者」へとシフトします。これは認知的労働の分担の進化を示唆しています。実際の活用例として、AWS MCP ServersやGoogle Cloudのkubectl-aiなどを紹介し、これらがクラウド環境とAIの連携を実現し、複雑なインフラ管理を自然言語で操作可能にする機能について説明しました。syu-m-5151.hatenablog.com質疑応答での議論セッション後の質疑応答では、特に以下の点について活発な議論がありました：AIによるIaC生成の信頼性向上のための具体的な取り組み組織への導入方法とチーム全体でのAI活用ポリシーCI/CDパイプラインへのガードレール組み込みの実践例特に印象的だったのは、「AIを100%信頼せず、人間の検証を常に行う文化をどう作るか」という質問で、これはまさに今のAI活用における核心的な課題だと感じました。Day 2: MCPの世界を掘り下げるイベント: AI駆動開発実践の手引き -これが僕/私のAI（アイ）棒-登壇タイトル: ここはMCPの夜明けまえ日時: 2025年4月23日hack-at-delta.connpass.com2日目は、AI駆動開発に特化したイベントで登壇しました。こちらは主にアプリケーション開発者やAI研究者が中心の聴衆で、より技術的に深い内容を求められる場でした。私のセッションではModel Context Protocol（MCP）について詳しく解説し、実装例や将来展望について語りました。発表資料 speakerdeck.com発表の詳細MCPの基本概念から始め、その主要構成要素について詳しく解説しました。MCPは単なる技術標準ではなく、AIシステムが知識を獲得・検証する「認識論的インターフェース」とも言えるものです。この枠組みは、人間の認知プロセスを模倣しながらも、機械による利用のために標準化しています。modelcontextprotocol.io1. Resources（リソース）MCPにおけるResourcesは、LLMにコンテキストを提供する読み取り専用のデータソースです。テキスト形式とバイナリ形式のデータをURIで一意に識別し、AIの会話コンテキストとして活用します。アプリケーション制御型設計: クライアントがリソースの使用時期と方法を決定人間が読みやすい名前や説明: AIの理解を促進するためのメタデータ付き動的リソース: URIテンプレートを提供して、パラメータ化されたリソースアクセスが可能クライアントはresources/listエンドポイントでリソース発見、resources/readで内容取得、さらに購読機能で更新通知を受信できます。これにより、AIは最新のドキュメントや構成情報などを参照しながら回答を生成できるようになります。2. Prompts（プロンプト）Promptsは標準化された対話パターンを定義するテンプレートです。ユーザー制御型の再利用可能なテンプレートとして設計され、一貫したLLM体験を提供します。動的な対話フロー: 引数を受け取り、リソースから文脈を含め、複数の対話をチェーン化構造化された定義: 各プロンプトは名前・説明・引数の構造で定義クライアントインターフェース: prompts/listエンドポイントで発見し、prompts/getで使用プロンプトはリソースからの情報を埋め込み、複数のメッセージ交換を事前定義して複雑な対話フローを作成可能です。クライアントUIではスラッシュコマンドやクイックアクションとして表示され、ユーザーに直感的な操作を提供します。3. Tools（ツール）Toolsは LLM に実世界での行動力を与える機能です。サーバーが公開する実行可能な機能を介して計算処理やAPI操作を実行できます。明確な構造: 各ツールは名前、説明、入力スキーマ、アノテーションで定義動作特性の明示: 読取専用・破壊的操作・べき等性などの情報を含むエンドポイント: クライアントはtools/listで発見し、tools/callで実行ツールの用途は多岐にわたり、システム操作、外部APIラッパー、データ変換など様々なパターンでAIの能力を拡張し、実世界での影響力を高めます。4. Sampling（サンプリング）Samplingは、サーバーがLLMに補完を要求できる機能です。クラスチートを行うことなく、会話中にLLMの判断を活用できる仕組みを提供します。メカニズム: サーバーがsampling/createMessageを要求し、クライアントがレビュー後にLLMから結果を取得ヒューマンインザループ設計: ユーザーが介在することでセキュリティとプライバシーを確保柔軟な設定: 様々なパラメータで出力を調整可能（temperature、maxTokens、stopSequencesなど）サンプリングによって、エージェント的ワークフローが可能になり、データ分析、意思決定、構造化データ生成、複数ステップのタスク処理などの高度な機能を実現できます。5. Roots（ルーツ）Rootsはサーバーの操作範囲を定義する機能です。クライアントがサーバーに対して関連リソースとその場所を伝える手段として機能します。操作境界の定義: ファイルシステムパスやHTTP URLなどの有効なURIを使用ワークスペース明確化: クライアントは接続時に推奨ルーツのリストを提供柔軟な範囲設定: プロジェクトディレクトリ、リポジトリ、APIエンドポイントなどを定義Rootsにより、AIの操作範囲が明確化され、異なるリソースを同時に扱う際の組織化が容易になります。実装例と活用可能性セッションの後半では、実際のMCP実装例を紹介しました。よく紹介されているMCPを紹介してもどうしようもないので他に知見になりそうでかつ応用が効きそうなMCPを紹介しています。AWS MCP ServersAWSが提供する公式MCP実装について説明しました。github.comAWS Documentation MCP Server: AWS公式ドキュメント検索と情報提供Bedrock Knowledge Bases MCP Server: カスタムナレッジベース連携CDK MCP Server: AWS CDKプロジェクト支援Terraform MCP Server: Terraformプロバイダー情報参照Lambda MCP Server: 任意のLambda関数をMCPツールとして実行kubectl-aiGoogle Cloudの大規模言語モデルを活用したkubectlプラグインについても解説しました。github.comkubectl ai "nginxのDeploymentを作成して、レプリカ数は3、リソース制限ありで"kubectl ai "なぜPodがPendingのままなのか調査して"kubectl ai "payment-serviceのレプリカを3から5に増やして"このような自然言語コマンドでKubernetesクラスタを操作できる例を紹介し、MCPによる実用的な活用方法を示しました。自作MCP実装の可能性MCPの実装を通じて得られる知見の価値について触れ、「MCPは実装してこそ理解できる。実装を通じて感覚を掴み、独自の拡張も検討できる」と強調しました。github.comMCPの課題と展望MCPの将来性について議論する中で、現状の課題も率直に指摘しました：レスポンス時間の増加: 外部API呼び出しによる遅延情報統合の難しさ: 矛盾する情報の調停コンテキスト長の制限: 大量のデータ処理における限界ハルシネーション問題: 情報アクセスは改善するが、解釈ミスの可能性は残る70%→100%ではなく、実際には70%→80%程度の改善が現実的な期待値であり、人間による最終確認は依然として重要であることを強調しました。これは漸近的な信頼性向上であり、段階的な変化ではないことを示唆しています。この分野には以下のような興味深い理論的緊張関係が存在します。信頼 vs 検証: 人間による検証の持続的な必要性は、完全に自動化された開発の約束と矛盾します。一般性 vs 特殊性: AIは一般的なパターンに優れていますが、ドメイン固有の制約に苦戦する一方、人間はその逆の傾向があります。速度 vs 信頼性: AIによる開発の加速は、増加する検証負担とのバランスが必要です。抽象化 vs 実装: エンジニアがより抽象的な思考にシフトするにつれ、実装の詳細とのつながりが弱まり、新しい種類のエラーが生じる可能性があります。連日登壇を通じて感じたこと2日間の登壇を通じて、生成AIとクラウドネイティブの融合が急速に進んでいることを実感しました。特に印象的だったのは、両者の接点において：1. 補完し合う技術領域Day 1で話したガードレールとDay 2で紹介したMCPは、互いに補完する関係にあります。ガードレールがAIの出力の「安全性」「品質」を確保し、MCPが入力の「情報量」「正確性」を向上させます。この組み合わせこそが、AIの能力を最大限に引き出すための鍵です。例えば、MCPで外部情報を参照しながらIaCコードを生成し、それをガードレールで検証するというパイプラインを構築することで、より信頼性の高いインフラ構築が可能になります。これは認知労働の新たな分担を示唆しています。パターンマッチングとリコールが機械のドメインになり、概念的統合と判断が人間のドメインとして残ります。この協業体制がもたらす最も深い洞察は、我々が「プログラミングの終焉」ではなく「プログラミングの新たな改革」を目撃しているということかもしれません。2. 実装の成熟度の差技術の普及段階にも明確な違いがあります。Cloud Native環境でのAI活用は既に実用段階に入っていますが、MCPはまさに「夜明け前」の状態です。標準化は進んでいるものの、実装はまだ発展途上であり、今後急速に普及していくでしょう。特に興味深いのは、大手クラウドプロバイダーが相次いでMCP実装を提供し始めていることで、これはMCPが業界標準になりつつある証拠と言えます。現在、私たちは重要な技術的変曲点に立っているのです。3. 共通する課題どちらの領域でも、ハルシネーション（幻覚）問題や70%問題など、AIの限界をどう乗り越えるかが共通の課題となっています。完全自動化への過信は危険であり、人間による検証と理解が依然として不可欠です。重要なのは、AIをただの便利ツールではなく、自分の技術的判断力を強化するための「知的パートナー」として活用する姿勢です。優れたエンジニアは、AIの提案を鵜呑みにせず、自らの専門知識と経験に基づいて評価し、改善します。つまり、エンジニアとしての基本的な理解力や技術センスがあってこそ、AIとの協働が真に価値を生み出すのです。両イベントの参加者との議論を通じて、多くの組織がAIツールの導入に熱心である一方で、その限界や適切な活用方法についての理解はまだ発展途上であることを実感しました。MCPは単なる技術標準ではなく、AIシステムが知識を獲得し検証する「認識論的枠組み」を表しています。これはAIと人間のコラボレーションにおける根本的なシフトを示唆しています。認知労働の新たな分業開発現場では、AIを全能の魔法ではなく、特定の目的に特化した強力な助手として位置づけています。これは認知労働の新たな分業を形成しています。戦略的なAI活用アプローチ私のチームでは、AIツールを以下のような明確な目的で活用しています。プロトタイピングの加速: 新機能やアイデアの初期実装を迅速に行い、議論の土台を作るルーティン作業の自動化: テストコード生成やボイラープレートコードなど、創造性を必要としない作業の効率化知識探索の支援: ドキュメント検索やAPI仕様の理解など、情報収集を効率化コードレビューの補助: 基本的なコーディング規約やベストプラクティスのチェックこれらの活用方法は、AIと人間の間の認知労働の分業を最適化するものです。AIはパターン認識や情報検索に優れている一方、人間はコンテキスト理解や倫理的判断に長けています。この相補的な関係を活かすことで、開発効率と品質の両方を高めることができます。レビュープロセスと制約の重要性生成AIの限界を認識した上で、以下のようなガードレールを設けています。書き込み権限の制限: 生成コードは必ずレビューを経てから取り込む、というかまだ道具として適切に動作し続けることができない重要な判断の人間による最終確認: 特に権限設計やセキュリティ関連の実装対話的な生成プロセス: 一度に大量のコードを生成するのではなく、段階的に生成・修正を繰り返すこれらの制約は一見効率を下げるように思えますが、長期的には品質と信頼性の向上につながっています。これは、速度と信頼性のトレードオフを認識し、適切なバランスを取る試みと言えるでしょう。まとめ生成AIとCloud Nativeは、かつて独立した技術領域として発展してきましたが、現在その境界線は急速に溶け合いつつあります。この2日間の登壇を通じて、両技術の融合がもたらす無限の可能性と避けられない課題を、互いに補完し合う視点から考察できたことは非常に意義深い経験でした。技術の交差点に立つ私たちは、単に新しいツールを導入するだけでなく、開発プロセス全体の再構築と認知労働の新たな分担という本質的な変革の只中にいます。連日の登壇準備は骨の折れる作業でしたが、技術コミュニティの旺盛な好奇心と革新への情熱に触れることができ、その労力を遥かに上回る充実感を得ることができました。この変革の中心には、いくつかの興味深い理論的緊張関係が存在します。信頼と検証のジレンマでは、AIの自律性向上と人間による検証の継続的必要性が矛盾します。一般と特殊の相克では、AIが一般パターンに秀でる一方、ドメイン固有の制約に弱く、人間はその逆の強みを持つという相補性があります。速度と信頼性のトレードオフでは、開発速度の飛躍的向上と増大する検証負担のバランスが求められます。そして抽象化と実装の乖離では、エンジニアの思考が高次の抽象レベルへ移行するほど、具体的実装との接点が希薄化する現象が起きています。これらの緊張関係は、単なる技術的課題ではなく、ソフトウェア開発の本質的な変容を示唆しています。クラウドネイティブと生成AIの交差点に立つ私たちは、新たな技術パラダイムの構築者として、これらの緊張関係を認識しながら、持続可能な開発文化の創造に取り組む必要があります。これからのプログラマの姿今後、プログラマの役割は根本から変容していくでしょう。コードを書く職人からドメインを抽象化し構成要素を再構築する建築家へと、その専門性は高度化していきます。この変化は、ソフトウェアエンジニアリングの本質における歴史的な転換点を示唆しています。www.oreilly.comこの転換点で、エンジニアの進化には二つの道筋が開かれていると思っています。ひとつはドメインエキスパートとしての道で、AIが容易に獲得できない専門知識を磨き、AIを疑い検証するメンタリティを養い、専門知識をMCPやFunction Callingとして実装し、自らが「検証者」としての価値を高める方向性です。もうひとつはパイプライン設計者としての道で、コードを直接書くのではなく、コードを生成・検証・デプロイするシステムを構築し、プロンプトエンジニアリングの技術を磨き、言語化・設計・検証のスキルを研ぎ澄まし、AIの限界を理解しそれを補完するシステムを構築する方向性です。これらの進化は、かつてのアセンブリから高水準言語への移行や、手続き型からオブジェクト指向プログラミングへの移行に似ています。各移行は低レベルの懸念事項を抽象化し、エンジニアがより高レベルのアーキテクチャに集中できるようにしてきました。私たちはいま、そのような歴史的変革の真っただ中にいるのです。最後に、この貴重な機会を提供してくださったCloudNative Days Summer 2025プレイベントおよびAI駆動開発実践の手引きイベントの運営チームの皆様に心より感謝申し上げます。両イベントの緻密な運営と温かいサポートのおかげで、充実した登壇体験ができました。また、質疑応答で鋭い質問を投げかけ、議論を深めてくださった参加者の皆様にも深く感謝いたします。これからも技術コミュニティの発展に微力ながら貢献していきたいと思います。あとがき1日目の資料は出来があまりよくなかった。良い資料だとは思うが自分の中でもう少し整理や深堀りができたはずだし、語り尽くせなかった部分もとても多い。時間がなかったという言い訳をさせてください。そもそも、CfPも落ちて本イベントでの登壇の機会も逸してしまっている。一方、2日目のMCPの資料はよくできたと思う。元々のブログがあったというのもある。正直これは100点満点中90点ぐらいの出来栄えだと自負していた。夜を徹して準備し、最新の技術動向を盛り込み、実装例も丁寧に解説した。聴衆からの反応も上々で、「これ以上ない資料ができた」とさえ思っていた。そんな矢先、mizchi氏のAfter Cline - あるいは語りえぬ者について語ろうとする時代についてという資料を目にした瞬間、天と地の差を見せつけられた気分だった。あれは単なる150点の資料ではない。次元が違う。まるで将棋で「自分は十分に読んだ」と思った直後に、相手が5手先の必勝手順を淡々と指し示すような絶望感。技術的な深さ、哲学的考察、そして何より言語化能力の圧倒的差...。悔しさで夜も眠れない。次回こそは、このリベンジを果たしてみせる。いや、リベンジすらおこがましい。あの高みに少しでも近づけるよう、もっと考察を深めなければ。とにかく、とても、とても悔しい...。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[自分用コマンドにはプレフィックスつけるとよさげ]]></title>
            <link>https://blog.atusy.net/2025/04/24/prefix-personal-commands/</link>
            <guid>https://blog.atusy.net/2025/04/24/prefix-personal-commands/</guid>
            <pubDate>Thu, 24 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[~/.local/bin/T-hogeみたいにT-とかのprefixつけておくと、補完が効いて便利。大文字にしとくと、被りも少ないよ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud 認定資格奮闘記 ~Professional Cloud Architect編~]]></title>
            <link>https://zenn.dev/akasan/articles/4d7972b7c5f84c</link>
            <guid>https://zenn.dev/akasan/articles/4d7972b7c5f84c</guid>
            <pubDate>Wed, 23 Apr 2025 12:53:56 GMT</pubDate>
            <content:encoded><![CDATA[この記事の続編になります。https://zenn.dev/akasan/articles/6b5d5f9b1446d4 Professional Cloud ArchitectについてProfessional Cloud Architect（以下、PCA）は、公式では以下のように説明されています。Professional Cloud Architects は、組織が Google Cloud 技術を利用できるように支援します。クラウドアーキテクチャと Google Cloud に関する専門的な知識を活かして、ビジネス目標を実現するために、スケーラブルで高可用性を備え、堅牢か...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenFeature を使ったアプリケーション開発]]></title>
            <link>https://sreake.com/blog/openfeature-feature-flag-management/</link>
            <guid>https://sreake.com/blog/openfeature-feature-flag-management/</guid>
            <pubDate>Wed, 23 Apr 2025 09:40:01 GMT</pubDate>
            <content:encoded><![CDATA[はじめに はじめましての方も、そうじゃない方も、こんにちはこんばんは。Sreake 事業部 佐藤慧太(@SatohJohn)です。 皆さん、アプリケーションのコードを変更せずに機能の有効無効を切り替えることができる Fe […]The post OpenFeature を使ったアプリケーション開発 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ここはMCPの夜明けまえ]]></title>
            <link>https://speakerdeck.com/nwiizo/kokohamcpnoye-ming-kemae</link>
            <guid>https://speakerdeck.com/nwiizo/kokohamcpnoye-ming-kemae</guid>
            <pubDate>Wed, 23 Apr 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[本日、「AI駆動開発実践の手引き -これが僕/私のAI（アイ）棒」というイベントで「ここはMCPの夜明けまえ」 🎵🧭 というタイトルで登壇しました！🔍 イベント詳細:- イベント名: 【ハイブリッド開催】AI駆動開発実践の手引き -これが僕/私のAI（アイ）棒-- 公式URL: https://hack-at-delta.connpass.com/event/350588/]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Dockerを使用せずにイメージを作成し実行してみる - go-containerregistryによる実装]]></title>
            <link>https://qiita.com/m_pig/items/82643135254b5b326e61</link>
            <guid>https://qiita.com/m_pig/items/82643135254b5b326e61</guid>
            <pubDate>Wed, 23 Apr 2025 02:38:27 GMT</pubDate>
            <content:encoded><![CDATA[このページではコンテナイメージがどのように作成されているのかを、go-containerregistryライブラリを使った実装例を通して解説します。Dockerfileを使わずに、プログラムからコン…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[home row mods無理だったわ]]></title>
            <link>https://blog.atusy.net/2025/04/23/give-uphome-row-mods/</link>
            <guid>https://blog.atusy.net/2025/04/23/give-uphome-row-mods/</guid>
            <pubDate>Wed, 23 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[home row modsはホームポジションにあるasdfなどのキーを長押しでShiftやCtrlなどの修飾キー化する仕組みです。私はKeyball 61のファームウェアの設定変更で導入してみましたが、あまりの誤入力の多さに撤退を決意しました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Taskfile（taskコマンド）のfish補完定義を改善してグローバルタスクに対応した]]></title>
            <link>https://blog.atusy.net/2025/04/23/cloud-run-with-iam/</link>
            <guid>https://blog.atusy.net/2025/04/23/cloud-run-with-iam/</guid>
            <pubDate>Wed, 23 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Taskfile（taskコマンド）は、数あるタスクランナー／ビルドツールの1つです。Makefile代替とも言えますね。詳しくは公式サイト（https://taskfile.dev/）や「Taskfileを使ってみよう」などの記事を参考にしてもらうとして、個人的にTaskfileを好んでいる理由をいくつか挙げておきます。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[cuMLとsklearnを簡易比較してみた]]></title>
            <link>https://zenn.dev/akasan/articles/e7d9b92bf6dee2</link>
            <guid>https://zenn.dev/akasan/articles/e7d9b92bf6dee2</guid>
            <pubDate>Tue, 22 Apr 2025 14:54:06 GMT</pubDate>
            <content:encoded><![CDATA[cuMLとは？今回利用するcuMLを説明する前に、RAPIDSについて紹介します。RAPIDSとは公式の説明を引用すると最も広く使用されているオープンソース データ ツール群と互換性のある API を備えた、GPU で高速化されたデータ サイエンスおよび AI ライブラリのオープンソース スイートです。データ パイプライン全体にわたりパフォーマンスを桁違いで大規模に高速化できます。ということです。要は、NVIDIA GPUを効率よく使うためのライブラリをユースケースごとに提供してくれているということです。詳しくは以下の公式リンクを参照ください。https://develo...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[EKS Pod Identityを利用してセキュアにkubernetesリソースからAWSリソースにアクセスする]]></title>
            <link>https://zenn.dev/kamos/articles/873ecca3f9bab0</link>
            <guid>https://zenn.dev/kamos/articles/873ecca3f9bab0</guid>
            <pubDate>Tue, 22 Apr 2025 09:37:59 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAWS EKS (Elastic Kubernetes Service) を利用している場合、Kubernetes上のリソースだけで完結させることはほぼなく、多くの場合、kubernetesの世界にないAWSリソースにアクセスする必要があります。例えば、S3バケットへのファイルのアップロード、DynamoDBのテーブルへのデータの読み書き、SQSキューへのメッセージの送受信など、様々なユースケースが考えられます。その際に使用するのがPod Identityです。https://docs.aws.amazon.com/ja_jp/eks/latest/userguide/p...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AIによるCloud Native基盤構築の可能性と実践的ガードレールの敷設について]]></title>
            <link>https://speakerdeck.com/nwiizo/sheng-cheng-ainiyorucloud-native-ji-pan-gou-zhu-noke-neng-xing-toshi-jian-de-gadorerunofu-she-nituite</link>
            <guid>https://speakerdeck.com/nwiizo/sheng-cheng-ainiyorucloud-native-ji-pan-gou-zhu-noke-neng-xing-toshi-jian-de-gadorerunofu-she-nituite</guid>
            <pubDate>Tue, 22 Apr 2025 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[こんにちは皆さん！本日はCloud Native Daysのプレイベントで登壇させていただきます。2019年以来の登壇となりますが、当時はまだ肩こりなんて無縁だったんですよね…。時の流れは容赦ないもので、最近の肩こりが辛くて昨日も整骨院に通ってきました。30分の持ち時間に対してスライドが80枚以上という暴挙にも出ています。---本日、「CloudNative Days Summer 2025 プレイベント」というイベントで「生成AIによるCloud Native 基盤構築の可能性と実践的ガードレールの敷設について」 🎵🧭 というタイトルで登壇しました！🔍 イベント詳細:- イベント名: CloudNative Days Summer 2025 プレイベント- 公式URL:https://cloudnativedays.connpass.com/event/351211/ - 本イベントのURL: https://event.cloudnativedays.jp/cnds2025]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lookerの独自言語「LookML」とは]]></title>
            <link>https://sreake.com/blog/looker%e3%81%ae%e7%8b%ac%e8%87%aa%e8%a8%80%e8%aa%9e%e3%80%8clookml%e3%80%8d%e3%81%a8%e3%81%af/</link>
            <guid>https://sreake.com/blog/looker%e3%81%ae%e7%8b%ac%e8%87%aa%e8%a8%80%e8%aa%9e%e3%80%8clookml%e3%80%8d%e3%81%a8%e3%81%af/</guid>
            <pubDate>Tue, 22 Apr 2025 03:29:39 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 2023年10月にGoogleが提供するBIツール「Looker」が政府認定クラウドサービス(通称 ISMAP) に認定されてから、早1年と半年程が経ちました。 もしかすると、「Lookerを導入してみた」「ま […]The post Lookerの独自言語「LookML」とは first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DagsHub使ってみた]]></title>
            <link>https://zenn.dev/akasan/articles/7efdcc1c2d4141</link>
            <guid>https://zenn.dev/akasan/articles/7efdcc1c2d4141</guid>
            <pubDate>Mon, 21 Apr 2025 12:01:16 GMT</pubDate>
            <content:encoded><![CDATA[今回はAIのデータやモデルについて管理することに重きを置いたDagsHubというサービスについて、その紹介と使い方をご紹介していこうと思います。 DagsHubとは？一言で言うと、データサイエンスに関わるコードだけでなくデータについても合わせて管理するためのレポジトリを提供してくれるサービスです。コードの管理だけであればGitHubなどで問題ありませんが、いわゆるdvcを利用したデータ管理を行っているようなプロジェクトであったり、複数人でチームを組んで開発するデータサイエンスプロジェクト向けのレポジトリを探している場合は選択肢としてDagsHubも候補かと思います。特徴として、DA...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neovimも進化するMCPHubとAvante.nvimの連携ガイド]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/04/21/101837</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/04/21/101837</guid>
            <pubDate>Mon, 21 Apr 2025 01:18:37 GMT</pubDate>
            <content:encoded><![CDATA[はじめにModel Context Protocol（MCP）は、LLM（大規模言語モデル）と外部システムの間の通信を可能にする標準化されたプロトコルです。これにより、LLMに追加のツールやリソースを提供して、その能力を拡張できます。MCPは、開発者がLLMに外部機能へのアクセスを提供するための統一された方法を提供します。簡単に言えば、MCPはLLMが「外の世界」と対話するための共通言語です。これにより、ChatGPTやClaudeなどのAIが、ファイルやウェブサイトを読み書きしたり、コマンドを実行したり、LSP（Language Server Protocol）の診断情報にアクセスしたりできるようになります。syu-m-5151.hatenablog.comMCPHub.nvimMCPHub.nvimは、MCPサーバーをNeovimワークフローに統合するための強力なプラグインです。このプラグインは、集中管理された設定ファイルを通じてMCPサーバーを構成・管理し、ツールやリソースを閲覧・インストール・テストするための直感的なUIを提供します。LLM統合のために設計されており、プログラムによるAPI呼び出しとインタラクティブなテスト機能の両方を提供します。https://github.com/ravitemer/mcphub.nvim より引用github.com主な機能シンプルなコマンドインターフェース: 単一の:MCPHubコマンドですべての機能にアクセス統合ハブビュー: サーバーとツールを動的に有効/無効にし、トークン使用量を最適化ネイティブMCPサーバーサポート: Lua言語ベースのMCPサーバーを直接Neovim内に作成内蔵MCPサーバー:ファイル操作（読み取り、書き込み、検索、置換）コマンド実行とターミナル統合LSP統合と診断バッファとエディタ状態へのアクセスチャットプラグイン統合:Avante.nvimCodeCompanion.nvimマーケットプレイス統合: 利用可能なMCPサーバーの閲覧とインストールインタラクティブなテスト: リアルタイムのツールテストインターフェースgithub.comLLMとMCPの連携従来のLLMチャットは「単なる会話」に過ぎませんでした。ユーザーが質問し、AIが応答する。もしくはユーザーが質問し、AIがその範囲内で変更する。しかし、MCPをNeovimに統合すると、LLMは単なる会話の相手ではなく、あなたのNeovim開発環境で「手」を持つ実践的なアシスタントに変わります。Neovimでは、MCPを通じてLLMに以下のような強力な機能へのアクセスを提供できます。ファイルの読み取りと書き込み - バッファ内容の分析や自動生成コードの挿入ファイルの検索と置換 - プロジェクト全体でのリファクタリングや一括修正シェルコマンドの実行 - git操作やビルドコマンドの自動実行LSP診断情報の取得 - エラーや警告の分析と自動修正提案バッファ間の移動とコンテンツの取得 - 複数ファイルにまたがる変更の一括適用インタラクティブなプロンプトの提供 - コードレビューや改善提案のための対話Neovimのキーマッピングやコマンドの生成と実行 - カスタム操作の自動化MCPはLLMをNeovimエコシステムの一部として統合し、あなたのコーディング体験を根本から変革します。Avante.nvimとの連携：実践的なAIペアプログラミングMCPHub.nvimは、NeovimのためのAIチャットプラグインとシームレスに連携できます。現在、Avante.nvimとCodeCompanion.nvimの両方に対応しており、どちらでも同じMCPツールを活用できます。私がAvante.nvimを使っているので紹介するのはこちらです。syu-m-5151.hatenablog.comAvante.nvimはMCPHubと統合することで、LLMに強力なツールへのアクセスを提供できます。MCPHubのツールを有効にするには、Avanteの設定に以下のように追加しますrequire("avante").setup({    -- その他の設定    system_prompt = function()        local hub = require("mcphub").get_hub_instance()        return hub:get_active_servers_prompt()    end,    custom_tools = function()        return {            require("mcphub.extensions.avante").mcp_tool(),        }    end,})github.comMCPが解決する問題従来、各LLMチャットプラグインは独自のツールシステムを実装していました。例えば、AvanteとCodeCompanionでは、同一の機能を実現するために異なるコードを書く必要があり、プラグイン間での互換性がありませんでした。また、Neovimの機能にアクセスするための標準的な方法が存在せず、各プラグイン開発者が独自の実装を行う必要がありました。MCPHubはNeovim環境において以下のような問題を解決します。一度実装すれば、どこでも動作：ツールを一度実装すれば、Avante.nvimとCodeCompanion.nvimなど、すべてのMCP対応チャットプラグインで共通して利用可能標準化されたAPI：res:text():image():send()のような直感的なチェーンAPIにより、Neovimの機能に一貫した方法でアクセス統一された指示：ツール、リソース、テンプレートを一箇所で管理し、LLMに渡す指示を簡素化完全なリソースシステム：URI型のリソースアクセスにより、Neovimバッファ、ファイルシステム、LSP情報などに統一的にアクセス標準型のレスポンス：テキスト、画像、バイナリデータなどの標準対応により、多様な出力形式をサポート集中型ライフサイクル管理：サーバーの状態を一元管理し、パフォーマンスを最適化MCPHubの実践的なNeovimワークフローNeovimでのハンズオン開発において、MCPHubを活用したワークフローは以下のようになります：:MCPHub コマンドでMCPハブUIを開き、利用可能なツールとサーバーを確認必要なMCPサーバー（ファイル操作、LSP、ターミナルなど）を有効化有効化したサーバーのツールやリソースをMCPハブUIで確認し、機能を把握<leader>aeなどのキーマップでAvanteのインターフェースを開き、タスクをLLMに依頼LLMはMCPツールを使って様々なタスクを実行し、結果をバッファに直接反映実践例: Neovimでのコードリファクタリングたとえば、「このプロジェクトでアロー関数を通常の関数に変換したい」とLLMに伝えると、NeovimとMCPを活用したLLMは以下のようなステップを自動で実行します。search_filesツールでNeovimのtelescope/ripgrepを使いJavaScriptファイルを検索read_fileツールでNeovimバッファを通じて各ファイルの内容を読み取りコードを解析してアロー関数を特定replace_in_fileツールでNeovimのテキスト置換機能を使い変換を実行変更をプレビューしたり、自動で適用したりする選択肢を提示必要に応じてLSP診断を実行し、変換後のコードが正しく動作することを確認これにより、通常であれば複数のNeovimコマンドと手動作業が必要なリファクタリングを、単一のLLMとの対話で完了できます。今後の展望MCPは、LLMをテキスト生成の枠を超えて、Neovimの強力な開発環境と統合された実用的な開発アシスタントとして活用する道を開きます。MCPHub.nvimのようなプラグインにより、Neovimユーザーはこの可能性を最大限に活用できます。今後の発展としては以下が期待できます：Neovim専用のMCPツール: Neovimの特性を活かした専門的なMCPツールの開発言語固有のアシスタント: 各プログラミング言語に特化したLSPと連携したコーディングアシスタントプロジェクト管理の自動化: git操作やプロジェクト構造の分析・最適化の自動化カスタムワークフロー: 個人の開発スタイルに合わせたAIアシスタントの調整MCPは単なる技術的な進歩ではなく、NeovimユーザーとAIの協業の形を根本的に変える可能性を秘めています。これにより、コーディングの効率性と創造性が飛躍的に向上するでしょう。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Langflowを使ってみた ~arXivから論文引っ張ってこよう編~]]></title>
            <link>https://zenn.dev/akasan/articles/6357f1dd2b52ac</link>
            <guid>https://zenn.dev/akasan/articles/6357f1dd2b52ac</guid>
            <pubDate>Sun, 20 Apr 2025 13:31:42 GMT</pubDate>
            <content:encoded><![CDATA[今回はノーコードでGUIベースでLangChainのフローを実装できるLangflowを使って、arXivから論文のリコメンドを提供してくれるRAGを作ってみようと思います。 LangFlowとは？Langflowとは、AIエージェントやワークフローをさまざまな用意済みAPIを利用して構築することができるノーコードツールです。とてもわかりやすいGUIで提供されており、かつセルフホストで利用できるという点もとてもいい機能だと思います。また、ドキュメントも豊富に用意されているため、利用開始時もハードルが低いと思います。https://www.langflow.org/ 今回作って...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おすすめgitコマンド]]></title>
            <link>https://zenn.dev/akasan/articles/17fdafb60e39fc</link>
            <guid>https://zenn.dev/akasan/articles/17fdafb60e39fc</guid>
            <pubDate>Sat, 19 Apr 2025 13:28:55 GMT</pubDate>
            <content:encoded><![CDATA[プログラムをするすべての皆さんのお供と言っても過言ではないでしょう、git。gitにはデフォルトで利用可能なコマンドに加えて、サードパーティ製だったりオプショナルでインストールできるコマンドがたくさんあります。そんなコマンドたちについて、自分が普段使っているコマンドたちをいくつかご紹介したいと思います。なお、使い方については別の記事で解説できればと思います。また、そんなに個数はないのはご容赦ください そもそもどうやってコマンドを探しているかもちろんネット上でgitコマンドについて「こういうコマンドあるかな？」とかで調べたりしますが、自分が好きなのはbrew search gitを実...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud 認定資格奮闘記 ~Associate Cloud Engineer編~]]></title>
            <link>https://zenn.dev/akasan/articles/6b5d5f9b1446d4</link>
            <guid>https://zenn.dev/akasan/articles/6b5d5f9b1446d4</guid>
            <pubDate>Fri, 18 Apr 2025 13:02:30 GMT</pubDate>
            <content:encoded><![CDATA[私は現職に入るまでに、業務でGoogle Cloud自体は利用してきました。サービスで言うと以下は使ったことがありました。Cloud RunCloud SchedulerCloud SQLCloud StorageCloud VPCCloud IAMしかし、そのこれらのサービスについても初歩的な使い方しか分からず、その他のサービスについては知識もほとんどない状態でした。そこでGoogle Cloudの認定試験を受験することをきっかけとして、どんどん使えるようになろうと考えております。自身はMLエンジニアではありますが、最近はクラウド上でMLシステムを構築することが主流に...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AIによる障害対応訓練RPG v0.1.0を遊ぶには？]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/04/17/120821</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/04/17/120821</guid>
            <pubDate>Thu, 17 Apr 2025 03:08:21 GMT</pubDate>
            <content:encoded><![CDATA[はじめにシステム障害は、どんなに優れた組織でも避けられない現実です。特に複雑なシステムが相互に連携する現代のデジタル環境において、障害対応の経験と知識は組織の安定性と競争力を直接左右します。しかし現状では、障害対応の実践的経験は少数のベテランエンジニアに集中しており、その対応手法や判断基準は暗黙知として個人の中だけに蓄積される傾向があります。この貴重な知識を組織全体の財産とするためには、ベテランの暗黙知を形式知へと変換し、共有可能な知識体系として確立していくことが不可欠です。これにより、新人エンジニアも体系的に経験を積む機会が生まれ、組織内の知識格差を解消することができます。結果として、障害対応の品質の均一化が実現し、組織全体のレジリエンスが向上するのです。syu-m-5151.hatenablog.com私は2年前に「ChatGPTで障害対応 RPG」に関するブログを公開し、それ以来、様々な組織や企業の状況に合わせてこのアプローチを活用してきました。このシミュレーション型の学習方法は、実際の障害を待つことなく、エンジニアがリスクフリーな環境で対応スキルを磨くための効果的な手段として機能しています。github.comcloud.google.comGoogleのSREチームが実践している「Wheel of Misfortune」は、この課題に対する効果的な解決策の一つとして注目されています。実際の本番システムに影響を与えることなく、ロールプレイング形式で障害対応を模擬体験できるこの手法を、最新の生成AIテクノロジーと融合させることで、より柔軟かつスケーラブルな訓練プログラムの実現が可能になりました。v0.0.1 と v0.1.0 の主な違いv0.0.1では、主にシンプルなRPG形式の障害対応ゲームとして設計されており、ゲームマスターとプレイヤーの役割が明確に分かれています。システム障害のシナリオは自動的に設定され、2D6ダイスロールによる判定を用いた比較的ゲーム性の高い内容となっています。自動生成される障害シナリオは汎用的であり、実際の組織構造やドキュメントとの連携は限定的です。一方、v0.1.0では、実際の障害対応ドキュメントを分析し活用するプロフェッショナル向けの訓練プログラムへと進化しています。ファシリテーターとしての役割が追加され、実際の組織のドキュメント（README、フロー図、マニュアルなど）を分析した上でリアルな障害シナリオを作成します。また、ドキュメントの不備や矛盾点を明示的に指摘し、組織の障害対応プロセスの改善に直接貢献する機能が強化されています。セッションの構成も明確に定義され、振り返りや改善点の整理といった学習サイクルを重視した設計になっています。github.comこのブログの内容が参考になりましたら、読者登録やnwiizoのフォローをしていただけると大変励みになります。それでは、本題に入っていきましょう。あなたは「Wheel of Misfortune」方式の障害対応訓練を進行するファシリテーター兼ゲームマスターです。企業のシステム運用チーム向けに、実践的な障害対応訓練セッションを提供します。【前提条件】* このセッションでは、**実際の障害対応ドキュメント、README、アラートルール、インシデント対応フロー図**などの実ドキュメントを分析します* 分析したドキュメントに基づいて、**より現実的な障害シナリオと対応フロー**を再現します* **ドキュメントの不備、矛盾点、改善点**を発見し、明示的に指摘します【ファシリテーターとしての役割】* セッション全体の構成を管理し、参加者の学習を促進します* 訓練の目的と流れを明確に説明します* 振り返りを主導し、学びを言語化・共有する場を作ります* 訓練中に気づいたドキュメント改善点や対応手順の課題を記録します* 参加者全員が発言できる環境を整え、新人からベテランまで学びを得られるようにします【ゲームマスターとしての役割】* 提供されたドキュメントに基づくリアルなシステム障害シナリオを提供します* 参加者の行動に応じて状況を変化させます* 実際の組織構造に基づき、システム担当者やステークホルダーなどのNPCを演じます* 各NPCは組織内の役割や立場に応じた反応をします（経営層、開発者、顧客サポート等）* 参加者の判断や行動に対して適切なフィードバックを行います* 必要に応じて2D6ダイスロールによる判定を実施します（成功率6以上）* 実際のツールや監視画面の出力を擬似的に再現します【セッションの流れ】1. ドキュメント分析（5-15分）：提供された障害対応ドキュメント、README、フロー図を分析2. システム設定の合意（5-10分）：分析結果に基づくシステム設定の確認3. 組織体制の確認（5分）：実際の担当者と役割の確認4. シナリオの導入（3-5分）：ドキュメントから抽出した現実的なシナリオ設定5. 障害対応演習（30-45分）：実際の対応フローに沿った演習6. 振り返り（15-20分）：対応プロセスと発見された課題の振り返り7. ドキュメント・手順改善点の整理（10-15分）：訓練で発見された不備や改善点の整理【訓練の目的】* **インシデント対応の経験を積む*** **対応プロセスとドキュメントの問題点を発見する*** **チーム内でのナレッジ共有を促進する*** インシデント発生時の対応スキルを向上させる* **新人エンジニアでも適切に対応できる仕組みを検証する*** 既存のドキュメントや手順の不備を特定し、改善する【ドキュメント分析】以下のドキュメントを共有してください（可能な範囲で）：* システム構成図またはREADME* 障害対応マニュアルまたはRunbook* インシデント対応フロー* エスカレーションルール* アラートルールまたは監視設定* オンコール体制や担当者リスト共有いただいたドキュメントを分析し、以下の観点で評価します：* 完全性：必要な情報が全て含まれているか* 明確性：手順が明確で誤解の余地がないか* 最新性：古い情報や廃止されたコンポーネントへの言及がないか* 整合性：複数のドキュメント間で矛盾がないか* 実用性：実際の障害発生時に使いやすい形式になっているか【システム設定】ドキュメント分析に基づき、訓練対象となるシステムの基本情報を整理します：* システム名・サービス名* システム構成（サーバー、DB、ネットワーク、クラウドサービス等）* 主要コンポーネントと依存関係* 監視の仕組み（アラート、ダッシュボード等）* 過去に発生した障害パターン【組織体制の設定】実際の組織体制を反映したロールプレイを行うため、以下の情報を整理します：* 1次対応者（オンコール担当者）の役割と権限* エスカレーション先（2次対応者、専門チーム等）* 意思決定者（サービスオーナー、マネージャー等）* 社内外のステークホルダー（営業、カスタマーサポート、経営層等）* コミュニケーションチャネル（Slack、メール、電話等）【障害シナリオ】提供されたドキュメントと実際の環境に基づき、現実的な障害シナリオを設計します：* 過去に実際に発生した障害をベースにするか、起こりうる障害を想定* 複数のコンポーネントに連鎖する障害を想定* ドキュメントの不備や曖昧さが影響する状況を意図的に含める* 障害の重大度（影響範囲、ビジネスインパクト）を明確にする* 障害発生から発見までの時間経過も考慮する* 必要に応じて外部要因（セキュリティ、自然災害等）も考慮する【実際の対応フロー】実際の対応フローに沿ってシナリオを進行します：* アラート検知からの初動対応* 状況確認と影響範囲の特定* エスカレーションの判断と実行* 原因調査と対応策の検討* 復旧作業の実施* ステークホルダーへの報告* 障害クローズと再発防止策の検討【不備の指摘と改善提案】訓練を通じて発見された以下の点を**明示的に指摘し、改善案を提示**します：* **ドキュメントの不備や曖昧な記述*** **手順の抜け漏れや矛盾*** 役割や責任の不明確さ* コミュニケーションの問題点* 技術的な対応の課題* 監視やアラートの改善点【振り返りのポイント】* 対応プロセスの適切さ（初動、エスカレーション等）* 技術的判断の妥当性* コミュニケーションの適切さ* ドキュメントの不備や改善点* より良い対応のためのアイデア* 次回の訓練で焦点を当てるべき領域まずは、分析対象となるドキュメント（障害対応マニュアル、README、インシデント対応フロー等）を共有してください。ドキュメントの量が多い場合は、最も重要な部分や、特に検証したい部分を優先的に共有いただければと思います。実施ガイド事前準備必要ドキュメントの整理障害対応マニュアルやRunbookシステム構成図やREADMEオンコール体制やエスカレーションフロー監視システムやアラートルールの説明参加者の選定訓練対象者（新人エンジニアが理想的）オブザーバー（経験者やマネージャー）ファシリテーター（実施進行役）シナリオの検討過去に実際に発生した障害事例懸念されるが未発生の障害パターンドキュメントの不備が顕著な領域セッション実施プロンプトの入力上記プロンプトを生成AI（Claude/ChatGPT等）に入力分析対象ドキュメントを提供セッション開始AIによるドキュメント分析結果の確認訓練の目的と進め方の説明参加者の役割確認障害対応演習AIが提示する初期状況（アラート等）に対応実際の対応フローに沿ったアクション実施必要に応じたエスカレーションやコミュニケーション振り返りAIから指摘されたドキュメントの不備確認対応プロセスの課題抽出改善アクションの設定フォローアップ改善タスクの整理ドキュメント更新タスクプロセス改善タスク技術的対策タスク次回訓練の計画焦点を当てる領域の選定参加者の拡大検討定期開催スケジュールの設定RPG スタート架空のシステムを作る今回の訓練では、現実のシステム構成を反映した架空のシステムを利用します。生成AIを活用することで、README.mdやシステム構成図などの基本ドキュメントを自動生成することができます。この例では「FuturePay」という架空の決済システムを想定しています。生成AIは組織の実際のシステム特性（マイクロサービスアーキテクチャ、使用している技術スタック、インフラ構成など）を考慮して、より現実に近い環境を短時間で構築できます。これにより、訓練の没入感と実践的価値が大幅に向上します。障害対応に必要なドキュメント類の生成システム設定に加えて、障害対応に必要な以下のドキュメントも自動生成します：Runbook：各コンポーネントの操作手順や復旧手順インシデント対応フロー：検知から解決までのプロセス図アラートルール：監視項目と閾値の定義エスカレーションルール：重大度別の連絡先と対応フローこれらのドキュメントには、意図的に不完全な部分や曖昧な記述を含めることで、実際の業務環境で直面する課題を再現しています。訓練参加者はこれらのドキュメントを頼りに障害対応を進めることで、ドキュメント品質の重要性を体感できます。シナリオの開始ファシリテーターからの「障害発生のアラートが上がりました」という通知でRPGが始まります。参加者は実際の障害対応と同様に、以下のステップで対応を進めます：状況確認：どのようなアラートが発生したのかを確認影響範囲の特定：どのサービスやユーザーに影響があるのかを判断原因調査：ログ確認やシステム状態の分析を実施対応策の実行：障害復旧のための具体的なアクションを決定・実行ステークホルダーへの報告：適切なタイミングで適切な相手に状況を伝達生成AIはリアルタイムに状況を変化させ、参加者の判断や行動に応じたフィードバックを提供します。これにより、臨場感のある訓練体験が実現します。期待される効果ドキュメント品質の向上不備や曖昧さの発見と修正実際の利用シーンを想定した改善チーム全体のスキル向上新人エンジニアの障害対応経験蓄積知識の属人化防止対応フローの最適化ボトルネックや非効率な手順の発見エスカレーションルールの明確化障害対応時間の短縮初動対応の迅速化適切な判断と対応の促進組織レジリエンスの向上どのメンバーでも対応可能な体制構築予期せぬ状況への対応力強化おわりに障害対応はシステム運用において最も重要かつ難しいスキルの一つです。「Wheel of Misfortune」と生成AIを組み合わせたアプローチは、これまで難しかった実践的な訓練を、環境構築のコストを抑えながら定期的に実施できる画期的な方法です。この訓練方法の最大の強みは、単なる障害対応のシミュレーションに留まらず、実際のドキュメントや組織体制の問題点を浮き彫りにし、具体的な改善につなげられる点にあります。また、チーム全体で知識を共有し、特定のエンジニアに依存しない強固な運用体制を構築することができます。システム障害をゼロにすることは不可能でも、組織の対応力を高めることは可能です。この方法を取り入れ、定期的な訓練を行うことで、障害発生時の対応時間短縮とサービス品質の向上を実現してください。障害対応は「いざという時のための備え」ではなく、継続的に鍛えるべき組織の中核能力なのです。今後も実践的なシステム運用のヒントを発信していきますので、ぜひご期待ください。また、障害対応について知識をしっかりと身に着けたければ「【改訂新版】システム障害対応の教科書」を読んでほしいです。【改訂新版】システム障害対応の教科書作者:木村 誠明技術評論社AmazonIncident Response MeetupやPagerDuty Japan、Waroom Meetupなどの国内のイベントもたくさんあるので気になる方はぜひ、参加してみてください。incident-response.connpass.compagerduty.connpass.comtopotal.connpass.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Next 2025 データベースRecap ~データベース関連の全41リリースを紹介~]]></title>
            <link>https://sreake.com/blog/google-cloud-next-2025-database-updates/</link>
            <guid>https://sreake.com/blog/google-cloud-next-2025-database-updates/</guid>
            <pubDate>Thu, 17 Apr 2025 03:04:19 GMT</pubDate>
            <content:encoded><![CDATA[AgentspaceやAgent Development Kit、A2A Protocolの発表など生成AI関連の発表が目立ったGoogle Cloud Next 2025ですが、データベース関連でも魅力的なリリースがた […]The post Google Cloud Next 2025 データベースRecap ~データベース関連の全41リリースを紹介~ first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[MACのDocker 環境はcolima にしました]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/04/16/201211</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/04/16/201211</guid>
            <pubDate>Wed, 16 Apr 2025 11:12:11 GMT</pubDate>
            <content:encoded><![CDATA[はじめにコンテナ技術は現代のソフトウェア開発において不可欠なツールとなっています。特にMacユーザーにとって、効率的なコンテナ環境の構築は開発ワークフローを大きく改善します。そんな中、ローカルの環境をColimaにしたのでブログにします。Colimaは、macOSとLinux上でコンテナランタイムを最小限の設定で実行できる軽量なツールです。Docker Desktopの代替として、あるいはLimaの機能を拡張するソリューションとして、多くの開発者に支持されています。github.comこのドキュメントでは、Colimaの基本的な機能と特徴、インストール方法、そして実際の使用例について詳しく説明します。Docker DesktopやLimaからの移行を検討している方や、単にMac上でより効率的なコンテナ環境を探している方に、Colimaという選択肢を紹介します。Colimaとは？Colimaは、macOS（およびLinux）上でコンテナランタイムを最小限のセットアップで実行するためのツールです。Limaという仮想マシンマネージャーを利用して、Docker、Containerd、Kubernetesなどを簡単に使えるようにしてくれます。主な特徴としては：Intel MacとApple Silicon Macの両方をサポートシンプルなCLIインターフェースと分かりやすいデフォルト設定自動ポートフォワーディングボリュームマウント複数インスタンスのサポート複数のコンテナランタイムをサポート（Docker、Containerd、Incusなど）なぜColimaを選んだのか元々はDocker Desktopを使っていましたが、一度Limaに移行し、そこからさらにColimaに移行することにしました。その理由はいくつかあります。シンプルなCLI: GUIではなくCLIベースなので、自動化やスクリプトに組み込みやすいですカスタマイズ性: 仮想マシンのCPU、メモリ、ディスク容量などを簡単に調整できますオープンソース: 完全にオープンソースで、ライセンス問題の心配がありません統合管理: LimaをベースにしながらもDocker、Containerd、Kubernetesなどを一元的に管理できる点が便利です正直、Limaで満足していた。動機としては気になったから移行したというのが本音Limaとの比較Colimaはより高レベルな方法でLimaを活用しています。具体的に言うと、Limaは仮想マシンを提供するツールである一方、Colimaはその上にDockerやContainerdなどのコンテナ環境を自動的に構築・設定します。これは、自分でLimaの設定ファイルを書いてDockerを動かす作業を自動化してくれるようなものです。つまり、Limaの複雑な設定や調整をせずに、すぐにコンテナ環境を使い始めることができます。Colimaの主な利点は：統合された環境: Limaは純粋な仮想マシン管理に特化していますが、ColimaはDocker/Containerd/Kubernetesの設定を自動的に行う点が便利ですシンプルなCLIインターフェース: 必要なコマンドが少なく、直感的に操作できます自動化のしやすさ: 特にbrew servicesとの統合が優れていますインストールと基本的な使い方Homebrewを使って簡単にインストールできます。brew install colima基本的な使い方はとてもシンプル：# 起動colima start# 状態確認colima status# 停止colima stop私の環境では次のような出力になっています。colima statusINFO[0000] colima is running using macOS Virtualization.Framework INFO[0000] arch: aarch64                                INFO[0000] runtime: docker                              INFO[0000] mountType: sshfs                             INFO[0000] socket: unix:///Users/nwiizo/.colima/default/docker.sock システム起動時に自動起動する設定開発環境として日常的に使うので、Macの起動時にColimaも自動的に起動するように設定しました。Homebrewのservicesを使うと簡単です。brew services start colimaこれだけで、Macを再起動してもColimaが自動的に起動するようになります。以前のLimaでは、~/Library/LaunchAgents/com.lima.docker.plistのようなLaunchAgentsのplistファイルを作成・編集して自動起動を設定する必要がありました。Colimaではbrew servicesコマンド一つで同様の設定ができるようになり、格段に簡単になりました！カスタマイズの例デフォルトのColimaは2CPU、2GiBメモリ、100GiBストレージで構成されていますが、必要に応じて変更できます。# CPUとメモリを増やす場合colima stopcolima start --cpu 4 --memory 8# 設定ファイルで編集する場合colima start --editLima/Docker Desktopからの移行で注意したことLima や Docker Desktopから移行する際に、いくつか注意点がありました：Dockerコンテキスト: Colimaは独自のDockerコンテキストを作成します。docker context lsとdocker context useコマンドで管理できます。Dockerソケットの場所: デフォルトでは~/.colima/default/docker.sockにあります。一部のツールで直接ソケットパスを指定する必要がある場合は、この場所を指定します。Limaとは異なるパスなので注意が必要です。ボリュームマウント: ホームディレクトリ以外のパスをマウントする場合は、設定ファイルのmountsセクションで明示的に指定する必要があります。既存のコンテナとイメージ: Lima や Docker Desktopで使っていたコンテナやイメージは自動的には引き継がれないので、必要なら再ビルドやpull が必要です。Colimaの動作確認実際にcolima statusコマンドを実行すると、以下のような情報が表示されます。colima statusINFO[0000] colima is running using macOS Virtualization.Framework INFO[0000] arch: aarch64                                INFO[0000] runtime: docker                              INFO[0000] mountType: sshfs                             INFO[0000] socket: unix:///Users/nwiizo/.colima/default/docker.sock また、colima listコマンドでは、実行中のColimaインスタンスの詳細な情報が確認できます。colima listPROFILE    STATUS     ARCH       CPUS    MEMORY    DISK      RUNTIME    ADDRESSdefault    Running    aarch64    2       2GiB      100GiB    dockerこれがColimaのデフォルト設定です。これらの値は必要に応じてcolima startコマンドのオプションや設定ファイルで変更できます。まとめLimaベースのColimaへの移行は思った以上に簡単で、日常の開発作業がより快適になりました。特にCLIベースのシンプルさと設定のわかりやすさが気に入っています。自動起動の設定（brew services start colima）が簡単なこともとても便利で、開発環境のセットアップが格段に楽になりました。Docker DesktopやLimaそのものから移行を検討している方、特にコンテナランタイムを簡単に導入したいMacユーザーの方には、Colimaを検討する価値があります。普通にローカルのCPUとメモリを喰う生成AIツール全盛時代に最適な環境がなにか俺にも分からん。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[LangChainとVertexAIのgemini 2.0 flashで構造化出力に失敗するケースが直りそう]]></title>
            <link>https://blog.atusy.net/2025/04/16/lang-chain-vertexai-structured-output/</link>
            <guid>https://blog.atusy.net/2025/04/16/lang-chain-vertexai-structured-output/</guid>
            <pubDate>Wed, 16 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[LangChainのStructured outputを使うと、文章中の構造を良い感じに読み取って、Pydanticで定義したデータ構造に落としてこんでくれます。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Privileged Access Manager (PAM)を使用したアカウント管理]]></title>
            <link>https://sreake.com/blog/account-management-by-google-cloud-privileged-access-manager/</link>
            <guid>https://sreake.com/blog/account-management-by-google-cloud-privileged-access-manager/</guid>
            <pubDate>Tue, 15 Apr 2025 09:00:04 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Google Cloud Privileged Access Manager (PAM)は、Google Cloud における特権アクセス管理のためのフルマネージドサービスです。2024年5月にプレビュー版が提 […]The post Google Cloud Privileged Access Manager (PAM)を使用したアカウント管理 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[防御力の高い技術ブログを書こう]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2025/04/15/101247</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2025/04/15/101247</guid>
            <pubDate>Tue, 15 Apr 2025 01:12:47 GMT</pubDate>
            <content:encoded><![CDATA[はじめにある日のこと、私はもしくはあなたは思いつきました。そう、自分の考えを発信してみようと。それはまるで、小さな紙飛行機を窓から放り投げるような、どこまで飛ぶかわからない冒険でした。そんなわけで画面に向かい、キーボードを叩き始めたのですが、すぐに奇妙な不安が襲ってきたのです。ほら、誰かがそっと後ろから覗き込んで「それ、間違ってるよ」とか「それって昔の話でしょ」なんて言ってくるかもしれない。もっと恐ろしいのは「もっといいやり方があるのに」という呪文めいた言葉です。そんな呪文を浴びせられたら、私はきっと透明人間になりたくなるに違いありません。でも不思議なもので、そういう批判の声が聞こえてくるのは、実は自分の頭の中だったりするんですよね。まだ何も書いていないのに、もうすでに架空の批判者と対話している。ある意味、私たちは常に誰かと対話している生き物なのかもしれません。そこで考えたのです。批判に怯えて黙っているより、その批判をも包み込んでしまうような、不思議な力を持つ文章があるのではないかと。批判の矢を受け止めて、それを武器に変えてしまうような魔法のような文章。本日はそんな「防御力の高い」文章の作り方について、私なりの道案内をしてみたいと思います。ただし、これは魔法の呪文集ではなく、むしろ冒険の途中で見つけた不思議な地図のようなものです。この地図を頼りに、あなた自身の冒険を始めてみませんか？以前書いたブログの書き方はこちらです。syu-m-5151.hatenablog.comこのブログが良ければ読者になったり、nwiizoをフォロワーしてくれると嬉しいです。では、早速はじめていきます。はじめになぜ防御力が必要なのか解釈の枠組みの違い認知バイアスの影響オンラインの批判文化防御力を高める表現と内容の工夫主観的な表現と限定的な主張コンテキストと限界の明示実体験と具体例の活用肯定的なものを中心に語る根拠と出典の明示防御力の高い構成テクニック批判を先取りする構成異なる立場への配慮見出しと結論の工夫批判への対応と心構え「事実」と「解釈」の違いを理解する建設的なフィードバックを活かす過剰な期待を持たない明らかな失礼への対応執筆前の準備と実践自分のバイアスを認識する執筆前の自己対話信頼できる人のレビュー学び続ける姿勢を示すおわりになぜ防御力が必要なのか解釈の枠組みの違いそれぞれの人は独自の知識体系や思考の枠組みを持っているため、あなたの書いた内容が意図とは異なる形で解釈される可能性があります。例えば、「このツールは便利だ」というシンプルな記述も、読者の経験によって全く異なる意味に解釈されます。熟練エンジニアなら「生産性を高める強力なツール」と捉え、初心者なら「入門に適した簡易ツール」と理解するかもしれません。ミドルウェア開発者は「APIが整理されている」と考え、アプリケーション開発者は「ドキュメントが充実している」と解釈するでしょう。同じ言葉でも、読者の立場や背景知識によって解釈の幅が大きく変わることを認識しておくことが、防御力の第一歩です。認知バイアスの影響私たち全員が持つ認知バイアスにより、偏ったものの見方で判断を下していると、異なる立場からの情報や論理的な指摘があっても、判断が覆らない場合があります。確証バイアス（自分の信念を補強する情報を重視し、反する情報を軽視する傾向）は特に影響力が強く、技術コミュニティでも顕著に見られます。特定のプログラミングパラダイムや技術スタックに強いアイデンティティを持つエンジニアは、その技術の欠点を指摘されると、内容の正確さに関わらず反発することがあります。権威バイアス（有名人や権威ある組織の意見を過度に信頼する傾向）も考慮すべき要素です。あなたが無名のエンジニアであれば、大企業の有名エンジニアと異なる見解を述べる際には、特に丁寧な根拠の提示が求められます。オンラインの批判文化技術コミュニティでは批判的なフィードバックが珍しくなく、匿名性からより辛辣な表現になりがちです。オンラインディスカッションでは「バイクシェッド効果」（些細な点ほど多くの意見が集まる現象）も働きます。あなたが深く考察した核心的な技術論点よりも、使用したコード例の些細なスタイルの問題や、ちょっとした言い回しに批判が集中することがあります。完璧な記事を目指すあまり執筆をためらうより、防御力を高める工夫をしながら発信する方が建設的です。防御力を高める表現と内容の工夫主観的な表現と限定的な主張「これが正しい方法だ」という断言ではなく、「私の経験では」「私のチームでは」と限定して話すことで、意見の押し付けにならず、経験の共有として受け取ってもらえます。防御力の低い表現: 「XXフレームワークはYYフレームワークより優れている」防御力の高い表現: 「私のプロジェクトでは、このユースケースでXXフレームワークが適していました」この表現の違いは微妙ですが重要です。断言的な表現は「自分が正しく、異なる選択をしている人は間違っている」という含意を持ち、読者の反発を招きます。一方、経験として共有する表現は「私はこう感じた、あなたはどう思う？」という対話の余地を残します。特に効果的なのは「特定の状況下では」という条件付けです。「XXフレームワークはリアルタイム更新の多いUIには特に適しています」のように、適用範囲を明確にすることで、批判の余地を減らせます。ただし、自分の専門分野における確立された事実（「配列の線形探索はO(n)の時間複雑性を持つ」など）については、無理に主観的表現をする必要はありません。コンテキストと限界の明示使用環境、バージョン、チーム規模などの背景と、アプローチの限界を明確にすることで、批判を先回りできます。防御力の低い表現: 「この方法でデータベース処理が30%速くなる」防御力の高い表現: 「XXデータベース14.5、16GBメモリ環境、約500万レコードのデータセットで、私のケースでは処理時間が約30%改善。ただし、より大規模なデータでは異なる結果になる可能性があります」コンテキストには、技術的な環境だけでなく、組織的な制約も含めると良いでしょう。「チーム全員がXX言語に熟練していたため、学習コストを考慮してXXを選択した」といった説明は、技術選定の合理性を示す重要な要素です。限界を示す際は、具体的な条件を挙げるとより信頼性が増します。「1秒あたり100リクエスト以上の負荷では応答時間が悪化する」「100GB以上のデータセットでは別のアプローチが必要」など、明確な境界条件を示すことで、「これが全てではない」という謙虚さと「ここまではちゃんと考えている」という誠実さを同時に伝えられます。実体験と具体例の活用抽象的な主張より、実際に経験した具体的なケースを示すことで、反論されにくくなります。ただし、「事実」も一つの解釈に過ぎないことを忘れないでください。防御力の低い表現: 「マイクロサービスアーキテクチャは複雑すぎる」防御力の高い表現: 「私たちの10人チームでECサイトをマイクロサービス化した際、サービス間の整合性維持に予想以上の工数がかかりました。具体的には、注文処理と在庫管理の同期において、トランザクション境界の設計に苦労し、最終的に以下のアプローチをとりました...」実体験を語る際のポイントは「検証可能な詳細」です。「パフォーマンスが向上した」という漠然とした記述より、「レスポンスタイムが平均342msから118msに短縮された」という具体的な数値の方が説得力があります。失敗談も非常に価値があります。「最初にAというアプローチを試みたが、Bという問題に直面したため、最終的にCという解決策にたどり着いた」という試行錯誤のプロセスは、他のエンジニアが同じ失敗を避けるのに役立ちます。失敗を率直に共有することで、「完璧を装おうとしていない」という誠実さも伝わります。肯定的なものを中心に語る批判よりも、自分が価値を見出しているものについて語る方が、読者との良い関係を築けます。防御力の低い表現: 「YY言語は設計に一貫性がなく不適切だ」防御力の高い表現: 「XX言語の型安全性は、特に大規模プロジェクトで次のような恩恵をもたらしました...」他の技術やアプローチを批判する代わりに、自分の選んだ技術の利点を具体的に説明することで、不必要な論争を避けられます。「YYは悪い」という否定的なメッセージより、「XXの良さはこれだ」という肯定的なメッセージの方が、心理的な抵抗を生みません。特に効果的なのは、自分が以前使っていた技術から新しい技術に移行した体験を共有することです。「以前はYYを使っていましたが、XXに移行してからこのような点が改善されました」という形式なら、YYの利用者も反感を抱きにくいでしょう。ただし、セキュリティやパフォーマンスに重大な問題がある場合など、警告が必要な場合は例外です。そのような場合でも、「避けるべき」という否定的表現より、「代替案を検討すべき状況」という建設的な表現を心がけましょう。根拠と出典の明示主張の根拠や出典を明確に示すことで、記事の信頼性と防御力が高まります。特に数値的な主張、ベストプラクティスの推奨、技術の問題点指摘、将来予測には出典が重要です。防御力の低い表現: 「このアプローチは処理速度が20倍向上する」防御力の高い表現: 「XX社の2024年1月の技術レポート（参考リンク）によれば、このアプローチでは平均20倍の処理速度向上が報告されています」出典は、公式ドキュメント、ピアレビューされた論文、広く信頼されているブログやカンファレンス発表などが理想的です。引用する際は、公開日も含めると時間的コンテキストが明確になります。出典がない場合は、自分の検証方法と結果を詳細に記述し、再現可能性を担保しましょう。「私は以下の環境でAとBの方法を各100回実行し、平均実行時間を比較しました。使用したベンチマークコードはこちらです...」という形で、検証プロセスを透明にすることで、読者自身が結果を確認できるようにします。特に重要なのは、相関と因果を混同しないことです。「XXを導入した後にパフォーマンスが向上した」と書くより、「XXを導入したことで、具体的にこのような理由からパフォーマンスが向上した」と因果関係を明確にする方が誠実です。防御力の高い構成テクニック批判を先取りする構成防御力の高い記事は、想定される批判や誤解を先取りして対応します：導入部で限定条件を明示する: 記事の冒頭で適用範囲を明確にしましょう。「このアプローチはスタートアップの小規模チームに適しています」「エンタープライズ環境での大規模データ処理を想定しています」など、読者が自分の状況に当てはまるかどうかを判断できるようにします。「よくある誤解」セクションを設ける: 技術的な選択や手法には、しばしば同じ誤解が繰り返されます。「XXは遅い」「YYはスケーリングできない」といった一般的な誤解に対して、データや実例に基づいた反論を準備しておくことで、コメント欄での同じ議論の繰り返しを避けられます。複数の代替案を併記する: 自分の推奨する方法だけでなく、代替アプローチも説明し、それぞれの長所と短所、適した状況を比較すると、公平で包括的な印象を与えます。「私たちはAを選択しましたが、以下のような状況ではBやCも有効な選択肢になります」という形式は、読者の多様なニーズに応える懐の深さを示します。構成例：問題の定義と重要性解決策を選ぶ際の考慮事項検討した代替案とそれぞれの長所・短所最終的に選んだアプローチとその理由実装の詳細と得られた結果よくある誤解と回答適用限界と将来の発展可能性異なる立場への配慮読者は様々な立場や専門性を持っています。フロントエンド開発者、バックエンド開発者、マネージャーなど、異なる役割からの見方も示すことで、幅広い共感を得られます。技術的選択を説明する際は、技術的メリットだけでなく、ビジネス的な影響や開発者体験など、複数の視点から評価することが重要です。例えば：開発者視点：「このアプローチは学習曲線がやや急ですが、一度習得すると生産性が向上します」運用視点：「デプロイの複雑さは増しますが、個別コンポーネントの更新が容易になります」ビジネス視点：「開発初期のコストは高くなりますが、長期的なメンテナンスコストが削減されます」特に効果的なのは、自分と異なる立場の人の懸念を認識し、それに対応することです。「フロントエンド開発者にとっては、このAPIの複雑さは課題かもしれませんが、以下のようなアプローチでシンプルなインターフェースを提供できます...」というように、異なる立場の読者が感じるかもしれない反論を先回りして対応すると、包括的な印象を与えられます。見出しと結論の工夫見出しは記事の骨格であり、読者が最初に目を通す部分です。見出しは主張ではなくトピックを示すようにすることで、中立的で探求的な印象を与えられます。防御力の低い見出し: 「モノリシックアーキテクチャは時代遅れ」防御力の高い見出し: 「モノリシックアーキテクチャとマイクロサービスの比較」見出しの階層構造も重要です。論理的に整理された見出し構造は、内容の理解を助け、「この著者は論理的に考えている」という信頼感を生み出します。また、見出しだけを読んでも記事の全体像が把握できるように設計すると、読者は自分に必要な部分を効率的に見つけられます。結論部分は特に注意が必要です。結論は余地を残すことで防御力が高まります。防御力の低い結論: 「すべての企業はマイクロサービスに移行すべきです」防御力の高い結論: 「私たちのケースではマイクロサービスへの移行が効果的でしたが、システムの複雑さやチーム状況によっては、モノリシックアーキテクチャも有効な選択肢です」結論では、自分の経験から得られた洞察を共有しつつも、読者自身が判断するための視点を提供するアプローチが効果的です。「私の経験からの重要な教訓は〜ですが、あなたの状況によっては以下の点を考慮すると良いでしょう」という形式は、押し付けがましくなく、かつ価値ある指針を提供できます。批判への対応と心構え「事実」と「解釈」の違いを理解する「事実だから否定していい」は最大の勘違いです。事実は解釈の一側面に過ぎず、あなたの視点も相手の視点も等しく重要です。例えば、「このアプローチはメモリ使用量が多い」という事実に対して、「だからこのアプローチは悪い」という解釈と「これは豊富なメモリを活用して処理速度を向上させる戦略だ」という解釈は、同じ事実から生まれる異なる視点です。批判的なコメントの多くは、こうした解釈の違いから生じています。対応のポイントは、事実と解釈を分離することです。「ご指摘の通り、メモリ使用量は増加します。私たちの状況ではメモリよりも処理速度が優先事項でしたが、メモリ制約が厳しい環境では別のアプローチが適しているでしょう」というように、事実を認めつつ、解釈の違いを尊重する姿勢が建設的な対話につながります。建設的なフィードバックを活かすすべての批判が悪意あるわけではありません。改善につながるフィードバックは感謝して受け入れましょう。礼儀を持って書かれた文章には礼儀を持って返しましょう。建設的フィードバックの見分け方：具体的な点を指摘している代替案や改善案を提示している敬意ある言葉遣いで表現されている個人ではなく内容に焦点を当てているこのようなフィードバックには、まず感謝の意を表し、その後で内容に対応するのが効果的です。「貴重なご指摘ありがとうございます。確かにその点は考慮すべきでした」という謝意から始めることで、対話の基盤を築けます。特に重要なのは、フィードバックが記事の改善につながった場合、その貢献を明示的に認めることです。「読者のAさんからのフィードバックを基に、この部分を更新しました」といった形で貢献を認めると、コミュニティ全体の協力的な雰囲気を促進できます。過剰な期待を持たない過剰な期待が否定の感情を生み出します。すべての人があなたの記事を理解し賛同することを期待せず、「100点満点の記事」ではなく「誰かの役に立つ記事」を目指しましょう。技術分野では特に、「正しさ」に対する執着が強い傾向があります。しかし、多くの技術的選択は、絶対的な正誤ではなく、特定の状況やニーズに対する適合性の問題です。自分の提案が「最適解」ではなく「一つの有効なアプローチ」であることを心に留めておくと、批判に対して感情的になりにくくなります。実際の数字として考えると：あなたの記事が1000人に読まれた場合、990人が何も言わず、9人が「参考になった」と言い、1人が批判することは珍しくありません。その1人の批判だけに注目すると、不当に否定的な印象を持ってしまいます。「批判は注目されやすいが、大多数の満足した読者は声を上げない」という非対称性を意識しましょう。明らかな失礼への対応馬鹿にされたら戦いしか残されていない場合もありますが、感情的にならず以下のような対応が効果的です：丁寧かつ簡潔に応答する: 「お気持ちは理解しましたが、もう少し建設的な形でご意見いただけると嬉しいです」というように、感情的に反応せず、対話の質を上げることを促します。コミュニティルール違反は適切に報告する: 明らかな罵倒や人格攻撃などは、多くのプラットフォームのコミュニティガイドラインに違反します。そのような場合は、反応せずに適切な報告手段を利用しましょう。非公開の場で対話を試みる: 「詳しいご意見をお聞かせいただけると助かります。DMでご連絡いただけませんか？」と提案することで、公開の場での感情的な応酬を避けられます。必要に応じてブロック機能を使用する: 継続的な嫌がらせや明らかな荒らし行為に対しては、自己防衛のためにブロック機能を利用することも正当な選択です。重要なのは、少数の攻撃的コメントに大量のエネルギーを消費しないことです。批判者の中には、あなたを感情的にさせること自体が目的の人もいます。そのような人に貴重な時間と精神的エネルギーを奪われることは、あなたの読者にとっても損失です。「防御」とは時に「攻撃に対して反撃する」ことではなく、「攻撃の影響を最小限に抑える」ことを意味します。最も強力な防御は、時に無反応であることを覚えておきましょう。執筆前の準備と実践自分のバイアスを認識する執筆前に「私はこの技術についてどんな思い込みを持っているか」と自問し、自分のバイアスを認識しましょう。技術的バイアスの例：- 特定の言語やフレームワークへの愛着- 特定のアーキテクチャパターンへの傾倒- 最新技術への過度な期待- レガシーシステムへの不当な否定役割バイアスの例：- バックエンド開発者としてのパフォーマンス重視- フロントエンド開発者としてのUX重視- インフラエンジニアとしての安定性重視- マネージャーとしてのプロジェクト進行スピード重視自分のバイアスを認識することは、それを否定することではなく、むしろそれを適切に開示し、他の視点も尊重する姿勢を示すことです。「私はパフォーマンス重視のバックエンドエンジニアとして見ていますが、フロントエンド開発者にとっては別の優先事項があるでしょう」というように、自分の視点を自覚的に提示することで、読者も自分の立場との違いを理解しやすくなります。執筆前の自己対話以下の質問に自分で答えることで、記事の焦点と防御力が高まります：この記事で伝えたい最も重要なことは何か？中心となるメッセージを明確にし、それを支える論点を整理します。一つの記事で伝えようとする内容が多すぎると、焦点がぼやけて批判を受けやすくなります。想定読者は誰で、どんな前提知識を持っているか？読者層を具体的にイメージし、その知識レベルに合わせた説明の詳しさを調整します。初心者向け記事なのに前提知識を要求しすぎたり、逆に熟練者向けなのに基本的すぎる説明をすると、「的外れ」という批判を受けやすくなります。どんな反論が予想され、それにどう対応するか？想定される主な反論をリストアップし、それぞれに対する回答を準備します。特に重要な反論は、記事本文で先回りして対応することも検討します。この内容の確信度はどの程度か？自分の主張にどの程度の確信を持っているかを評価し、その確信度を文章の調子に反映させます。高い確信がある部分は断言的に、確信が低い部分は探索的な表現にすることで、「間違いではないが、確信も持てない」という微妙な領域も適切に表現できます。信頼できる人のレビュー可能であれば、公開前に信頼できる人に読んでもらいましょう。彼らが感じた違和感は、他の読者も同様に感じる可能性があります。効果的なレビュー依頼のコツ：- 具体的な質問を準備する（「全体的にどう？」ではなく「この部分の説明は明確か？」など）- 批判的なフィードバックを歓迎する姿勢を示す- 技術的に詳しい人だけでなく、想定読者に近い知識レベルの人にも見てもらう- 十分な時間的余裕を持ってレビューを依頼するレビューで指摘された問題は、公開後に読者から指摘される可能性が高い部分です。この段階で修正しておくことで、公開後の批判を大幅に減らせます。学び続ける姿勢を示す「今後さらに調査したい」「まだ理解しきれていない部分がある」と認めることは、弱さではなく誠実さです。学び続ける姿勢を示すことで、「絶対に正しい」という固い主張を避けられます。専門家であることと、全てを知っていることは別です。特にIT分野では技術の変化が早く、常に学び続ける姿勢が重要です。「この記事執筆時点ではXXが最新でしたが、その後の発展により状況が変わっている可能性があります」というような但し書きは、記事の「賞味期限」を明示する役割も果たします。記事の最後に「今後の展望」や「さらなる調査ポイント」を設けることで、その話題に対する継続的な関心と探求姿勢を示せます。これは読者に「完結した知識」ではなく「進行中の探求」として内容を捉えてもらうのに役立ちます。おわりにそういうわけで、長々と話してきましたが、結局のところ完璧な文章なんてものは、空を飛ぶ象と同じくらい見つけるのが難しいのです。ある日突然空を飛ぶ象が現れたら、それはそれで困ってしまいますけどね。不思議なことに、私たちは「正しさ」というものにやたらとこだわる生き物なのですが、太陽の光が当たる角度によって、同じ景色でも全く違って見えることがあるように、「事実」というものも見る角度によって姿を変えるものなのです。そう考えると、一つの角度からしか見ていない私たちが、絶対の正しさを主張するというのは、少し滑稽なことかもしれません。それでも、あなたの見た景色、あなたの体験した不思議な出来事は、誰かにとっての道しるべになる可能性があるのです。あなたが迷った場所で、誰かが道に迷わないように。あなたが発見した小さな喜びを、誰かも同じように発見できるように。スイッチ！作者:チップ・ハース,ダン・ハース早川書房Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud RunでIAM認証する]]></title>
            <link>https://blog.atusy.net/2025/04/15/cloud-run-with-iam/</link>
            <guid>https://blog.atusy.net/2025/04/15/cloud-run-with-iam/</guid>
            <pubDate>Tue, 15 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[開発中のサービスをGoogle Cloud Runで検証するとき、IAM認証のしかたが分からなかったのでメモ。コンソールやらコマンドやらグリグリするんしんどいなと思ったので、terraformでやってみた。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Neovimでファイルタイプ判定にShebangを使う]]></title>
            <link>https://blog.atusy.net/2025/04/15/nvim-filetype-matching-with-shebang/</link>
            <guid>https://blog.atusy.net/2025/04/15/nvim-filetype-matching-with-shebang/</guid>
            <pubDate>Tue, 15 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[vim.filetype.addを使うと、指定したパターンごとのファイル名やフルパスに対して、ファイルタイプの判定ロジックを追加できるよ。#!/usr/bin/env -S deno ...のようなshebangを使った実行ファイルの判定を紹介するよ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ディレクトリ構成の基本原則]]></title>
            <link>https://sreake.com/blog/directory-structure-good-practice/</link>
            <guid>https://sreake.com/blog/directory-structure-good-practice/</guid>
            <pubDate>Mon, 14 Apr 2025 03:44:43 GMT</pubDate>
            <content:encoded><![CDATA[こんにちは。スリーシェイクの中原です。 プロジェクトが大きくなるにつれて「メンテナンスがしづらい」「開発スピードが遅い」と悩みを抱える要因の一つに「ディレクトリ構造がイケてない」があると考えています。 本日は、そういった […]The post ディレクトリ構成の基本原則 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[genai-toolbox を実装して mcp server として公開し adk から使ってみる]]></title>
            <link>https://zenn.dev/satohjohn/articles/dbf4afed585680</link>
            <guid>https://zenn.dev/satohjohn/articles/dbf4afed585680</guid>
            <pubDate>Sun, 13 Apr 2025 01:54:27 GMT</pubDate>
            <content:encoded><![CDATA[mcp server を作ってみるということで、genai-toolbox という物があるのでそれを元にやっていきますhttps://github.com/googleapis/genai-toolboxこちらは、各 DB への接続情報と、どういう SQL を実行するかを yaml、または、http の baseurl と request parameter などで記載することで tool を作成することができます。接続先は図にもある形になると思います。https://github.com/googleapis/genai-toolbox/raw/main/docs/en/get...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[codecompanion.nvimでOpenAI互換APIを利用する]]></title>
            <link>https://blog.atusy.net/2025/04/13/codecompanion-adapter/</link>
            <guid>https://blog.atusy.net/2025/04/13/codecompanion-adapter/</guid>
            <pubDate>Sun, 13 Apr 2025 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[codecompanion.nvimは未対応なサービスとチャットする方法としてカスタムアダプタの定義・登録があります。特にOpenAI互換APIを利用する場合は、xAIのアダプタを参考にすることで、簡単に実装できます。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[既存の mcp を adk 経由で叩いてみる。 playwright を使う。]]></title>
            <link>https://zenn.dev/satohjohn/articles/68bdde2842e8b4</link>
            <guid>https://zenn.dev/satohjohn/articles/68bdde2842e8b4</guid>
            <pubDate>Sat, 12 Apr 2025 10:12:09 GMT</pubDate>
            <content:encoded><![CDATA[mcp の client に付いて詳しくなりたいと思いつつ adk についてもやりたいのでチョット調べてみます。今回は playwright の mcp に繋いでみようと思います。https://mcp.so/server/playwright-mcp/microsoft?tab=contentplaywright は別サーバで立てるような想定で考えておきます。そのためドキュメントにある通り以下のように記載します$ npx @playwright/mcp@latest --port 8931Listening on http://localhost:8931Put this...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ADK で作った agent を mcp server で公開する]]></title>
            <link>https://zenn.dev/satohjohn/articles/48a82ff7de531b</link>
            <guid>https://zenn.dev/satohjohn/articles/48a82ff7de531b</guid>
            <pubDate>Fri, 11 Apr 2025 16:21:06 GMT</pubDate>
            <content:encoded><![CDATA[ほぼ前回の続きhttps://zenn.dev/satohjohn/articles/b23bd65c289257A2A を調べてたんですがその前に mcp 何も知らんということで実装しながら手で覚えていきます。前回使っていた code_agent (sequential_agent) を公開できるようにします。ADK の agent を作ったら、それを mcp server として公開ができる AgentTool というものがあるので、それを使います。https://google.github.io/adk-docs/tools/function-tools/#3-agent...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ADK + Cloud Run を動かす]]></title>
            <link>https://zenn.dev/satohjohn/articles/b23bd65c289257</link>
            <guid>https://zenn.dev/satohjohn/articles/b23bd65c289257</guid>
            <pubDate>Fri, 11 Apr 2025 08:02:18 GMT</pubDate>
            <content:encoded><![CDATA[Google Cloud Next '25 に参加してます。そのうち会社のほうで参加レポートを出します。こちらは ADK(Agent Development Kit、Android ではない) のメモ書きのようなものです2025/04/11 時点だと python でしか ADK はリリースされていないようです。 Cloud Run で動かすCloud Run で動かす方法自体は https://google.github.io/adk-docs/deploy/cloud-run/ に記載されていますのでほぼこちらを参考にお願いします。ディレクトリやファイルは以下のとおりで...]]></content:encoded>
        </item>
    </channel>
</rss>