<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Mon, 25 Mar 2024 18:31:13 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[AWS EKSのNetwork Policyの動作と実装を確認してみる]]></title>
            <link>https://zenn.dev/satoken/articles/eks-network-policy</link>
            <guid>https://zenn.dev/satoken/articles/eks-network-policy</guid>
            <pubDate>Sat, 23 Mar 2024 15:00:47 GMT</pubDate>
            <content:encoded><![CDATA[はじめに2023年の9月にAWS EKSのCNIがNetwork Policyをサポートしました。ここで興味深いのが、Network Policyの実装にeBPFを使用していることです。今回は環境を構築して動作を確認しつつ、コントローラとeBPFの実装を見てみます。https://aws.amazon.com/jp/blogs/news/amazon-vpc-cni-now-supports-kubernetes-network-policies/ 環境構築と動作確認環境構築のためにAWSのblogに書かれているyamlファイルとeksctlでクラスタを作りました。c...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[はてなブログの記事をGitHubに自動でPushする方法]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/03/23/194702</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/03/23/194702</guid>
            <pubDate>Sat, 23 Mar 2024 10:47:02 GMT</pubDate>
            <content:encoded><![CDATA[ツールに感謝。コミュニティに感謝。github.comxではてなブログで更新する時にDiffが見れるととても助かるのだけど有料版だと可能とかありますか？みたいなこと聞いてたらwhywaita さんが教えてくれた!!!blogsyncどうでしょう https://t.co/Duh31GJGrV— why/橘和板 (@whywaita) 2024年3月23日   この記事では、blogsyncを用いてはてなブログの記事をGitHubに自動的に同期する方法について説明します。GitHub Actionsを使用して、はてなブログの記事を定期的にプルし、GitHubリポジトリに反映させることができます。当初はブログを更新する際に、記事の変更点（Diff）を確認できるようにしたいと考えました。しかし、NeoVimを使用してブログを書いているわけではないので、単に日付単位のDiffを取得できれば十分だと思ったため、この構成にしました準備1. はてなブログのAPIキーを取得はてなブログの設定ページ（https://blog.hatena.ne.jp/-/config）にアクセスし、「詳細設定」タブの「APIキー」セクションでAPIキーを取得します。2. GitHub Actionsのワークフローを設定.github/workflows/hatena-blog-pull.yamlに以下の内容を配置します。name: Blogsync Pullon:  schedule:    - cron: '0 0 * * *'jobs:  blogsync_pull:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - name: Set up Blogsync        uses: x-motemen/blogsync@v0        with:          args: --version      - name: Create blogsync.yaml        run: |          cat << EOF > blogsync.yaml          your_hatena_blog_id.hatenablog.com:            username: your_hatena_blog_id            password: ${{ secrets.HATENA_API_KEY }}          default:            local_root: .          EOF        shell: bash      - name: Pull articles from Hatena Blog        run: |          blogsync pull --no-drafts      - name: Commit changes        run: |          git config --local user.email "action@github.com"          git config --local user.name "GitHub Action"          git add .          git reset -- blogsync.yaml          git commit -m "Pull articles from Hatena Blog" || echo "No changes to commit"      - name: Push changes        uses: ad-m/github-push-action@v0.6.0        with:          github_token: ${{ secrets.GITHUB_TOKEN }}          branch: mainこのワークフローは、毎日0時（UTC）に実行されるようにスケジュールされています。3. ワークフローの権限を設定GitHub リポジトリの設定ページ（https://github.com/ユーザー名/リポジトリ名/settings/actions）にアクセスし、「Workflow permissions」セクションで「Read and write permissions」を選択します。これにより、ワークフローがリポジトリに変更を書き込むことができるようになります。4. はてなブログのAPIキーを設定GitHub リポジトリの設定ページ（https://github.com/ユーザー名/リポジトリ名/settings/secrets/actions）にアクセスし、「Repository secrets」セクションで「New repository secret」をクリックします。「Name」にHATENA_API_KEYと入力し、「Value」に手順1で取得したはてなブログのAPIキーを入力します。カスタマイズblogsync.yamlファイルの設定を必要に応じて書き換えてください。以下は設定例です。your_hatena_blog_id.hatenablog.com:  username: your_hatena_blog_id  password: ${{ secrets.HATENA_API_KEY }}default:  local_root: .your_hatena_blog_idの部分を実際のはてなブログIDに置き換えてください。また、このファイルは秘密にしなければいけないので基本的には.gitignoreに入れておいてください。blogsync.yaml使い方.github/workflows/hatena-blog-pull.yamlファイルをリポジトリに追加します。ワークフローは毎日0時（UTC）に自動的に実行されます。はてなブログの記事がGitHubリポジトリにプルされ、変更がコミットされます。以上で、はてなブログの記事をGitHubで自動的に管理できるようになります。ワークフローを設定したら、あとは記事を書くだけです。記事の変更が毎日GitHubリポジトリに自動的に反映されます。参考URLGitHub Actions でのシークレットの使用push-to-hatenablogを使い，はてなブログへの投稿記事をGitHubで管理したら最高だった！はてなブログ作成から投稿までを自動化したGitHub Actionsのワークフロー]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ビットコイン・ブロックチェーン入門]]></title>
            <link>https://speakerdeck.com/shukob/hitutokoinhurotukutienru-men</link>
            <guid>https://speakerdeck.com/shukob/hitutokoinhurotukutienru-men</guid>
            <pubDate>Fri, 22 Mar 2024 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[初学者の方向けにビットコイン・ブロックチェーン技術の全体像をお話ししました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[nwiizoはなぜSpeaker Deckに上げた資料をブログにするのか？]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/03/22/122847</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/03/22/122847</guid>
            <pubDate>Fri, 22 Mar 2024 03:28:47 GMT</pubDate>
            <content:encoded><![CDATA[はじめに私(nwiizo)は、ソフトウェアエンジニアとして日々の開発で得た知見やノウハウを、勉強会などで作成したプレゼンテーション資料としてSpeaker Deckに公開しています。Speaker Deckは、スライド共有サービスの一つで、スライドを簡単に公開・共有できる素晴らしいプラットフォームです。しかし、Speaker Deckに資料を公開するだけでは、いくつかの課題があります。スライドは情報を凝縮して伝えるために作られているため、詳細な説明や補足情報が不足しがちです。また、スライドだけでは、多くの人に情報が届きにくいという問題もあります。これらの課題を解決するために、私は Speaker Deck に上げた資料の内容を、改めてブログ記事として書くことにしています。本記事では、Speaker Deckに上げた資料をブログ記事化する意義について、詳しく解説していきます。詳細な説明と補足情報の追加スライドは、限られた紙面に要点を簡潔にまとめる必要があるため、情報を凝縮して伝えることに重点が置かれています。しかし、これだけでは聴衆の理解が表面的なものにとどまってしまう可能性があります。一方、ブログ記事では、スライドの内容を詳細に説明することができます。例えば、スライドではコードのスニペットを示すだけで終わってしまうことがありますが、ブログ記事ではそのコードの解説を丁寧に行うことができます。また、スライドでは伝えきれなかった背景情報や、補足説明を加えることで、より深い理解を促すことができるでしょう。さらに、スライドで紹介した技術や手法が、他の分野とどのように関連しているかを説明することもできます。これにより、読者は新たな観点からの問題解決のヒントを得ることができるかもしれません。Speaker Deck のスライドをブログ記事化することで、内容をより詳細に、より多面的に説明することができるのです。これは、情報を正確に伝え、読者の理解を深めるために非常に重要なことだと言えるでしょう。アウトプットによる技術力の向上ソフトウェアエンジニアにとって、新しい技術を学ぶことは重要ですが、学んだことをアウトプットすることも同様に重要です。登壇もそうですがブログ記事を書くためには、自分の知識を整理し、体系的に説明する必要があります。この過程で、自分の理解が深まり、技術力の向上につながります。ブログ記事を書く際には、自分が当たり前だと思っていたことを改めて見直すことになります。その際、自分の理解が不十分だったところや、説明が難しい部分に気づくことがあるでしょう。これは、さらなる学習のモチベーションにつながります。また、ブログ記事を公開することで、読者からのフィードバックを受けることができます。読者の質問や指摘は、自分では気づかなかった視点を提供してくれるかもしれません。このようなフィードバックから学ぶことで、さらなる技術力の向上が期待できます。ここで注目すべきは、登壇資料とブログ記事の違いです。登壇資料は、聴衆の反応を見ながら、その場で説明を調整することができます。また、質問に答えることで、理解が不十分な部分を補うこともできます。一方、ブログ記事は、書いた内容がそのまま読者に伝わります。誤りや不十分な説明があれば、それがダイレクトに読者に伝わってしまうのです。つまり、ブログ記事を書くことは、自分の知識や理解をより厳密に見直す機会になります。誤魔化しが効かない分、自分の理解の甘さが露呈するリスクがあるのです。しかし、だからこそ、ブログ記事を書くことは、技術力向上により大きな効果をもたらすと言えるでしょう。自分の知識のギャップに気づき、それを埋めていく過程こそが、真の成長につながるのです。Speaker Deckの資料をブログ記事にすることは、自己の知識と真摯に向き合う機会を提供してくれます。これは、技術力向上のための素晴らしい機会だと言えるでしょう。継続的な学習習慣の確立技術の進歩が速いソフトウェア開発の世界では、常に新しいことを学び続ける必要があります。しかし、日々の業務に追われていると、学習の時間を確保することが難しく感じることもあるでしょう。そんな中で、Speaker Deckの資料をブログ記事化することは、継続的な学習習慣を確立するための良い方法だと言えます。ブログ記事を書くためには、Speaker Deckの資料で扱ったトピックについて、さらに深く調査・研究する必要があります。この過程自体が、学習のプロセスの一部となります。また、ブログ記事を書くことを習慣化することで、学習のための時間を確保することが自然とできるようになるでしょう。さらに、自分の学習の成果をブログ記事としてアウトプットすることで、学習へのモチベーションを維持することもできます。自分の成長を可視化することは、さらなる学習への原動力になるはずです。加えて、ブログが増えて充実してくると、ブログを書くこと自体が楽しくなってくるものです。自分の知識や経験が、記事という形で蓄積されていくことに喜びを感じるようになります。また、読者からのフィードバックや反響が、さらなるブログ記事を書くモチベーションにつながります。こうして、Speaker Deckの資料をブログ記事化することと学習が、正のフィードバックループを形成するのです。学習した内容をブログ記事にすることで、学習が促進され、ブログ記事が充実します。充実したブログは、さらにブログを書く意欲を高めます。この好循環が、継続的な学習習慣を確立し、維持することにつながるのです。Speaker Deckの資料をブログ記事化することは、継続的な学習習慣を確立するための素晴らしい方法なのです。技術の進歩に遅れないためにも、この習慣を身につけることをおすすめします。そして、この習慣が、エンジニアとしての成長を加速させる良いサイクルを生み出すことを期待しています。エンジニアとしての認知度向上とアイデンティティの確立ソフトウェアエンジニアにとって、自分の専門性や技術力を示すことは、キャリアを積み重ねる上で非常に重要です。Speaker Deckに資料を公開することは、自分の知見を共有する良い方法ですが、それだけでは限界があります。一方、ブログ記事を通じて、自分の知見やスキルを広くアピールすることができます。質の高い技術情報を継続的に発信することで、徐々に読者がついてくるでしょう。これは、エンジニアとしての認知度の向上につながります。認知度が高まれば、仕事の依頼や、登壇の機会なども増えるかもしれません。これは、キャリアアップのチャンスにもなるでしょう。また、企業のエンジニアとして働いている場合は、社外での認知度の向上が、社内での評価にもつながる可能性があります。さらに、ブログ記事を書くことは、エンジニアとしてのアイデンティティの確立にも役立ちます。自分の考えや経験を言葉にすることで、エンジニアとしての自分の立ち位置が明確になります。これは、自分のキャリアの方向性を考える上でも重要なことだと言えるでしょう。Speaker Deckの資料をブログ記事化して発信することは、エンジニアとしてのキャリア形成において非常に有益なのです。認知度の向上とアイデンティティの確立は、長期的な視点で見たときに、大きな意味を持つはずです。登壇への動機づけエンジニアにとって、カンファレンスや勉強会での登壇は、自分の知見を共有し、人脈を広げるための素晴らしい機会です。しかし、登壇することへの不安や、ネタが思いつかないといった理由で、なかなか一歩を踏み出せないエンジニアも多いのではないでしょうか。Speaker Deckの資料をブログ記事化することは、登壇への良い動機づけになります。すでにSpeaker Deckで発表した内容をベースにブログ記事を書くことで、徐々に自信がつくでしょう。また、ブログ記事への反響を見ることで、自分の知見に対する需要や、興味を持ってくれる人の存在を実感することができます。これは、登壇へのモチベーションにつながるはずです。また、ブログ記事は、登壇の良い練習の場にもなります。ブログ記事を書く際には、自分の考えを明確に言葉にする必要があります。これは、登壇の際にも求められるスキルです。ブログ記事を書くことで、プレゼンテーションスキルの向上も期待できるでしょう。さらに、ブログで築いた信頼関係が、登壇の機会につながることもあります。ブログを読んだ人から、登壇の依頼を受けるケースも珍しくありません。登壇は、エンジニアとしてのさらなる成長と、人脈の拡大に役立つはずです。Speaker Deckの資料をブログ記事化することは、その第一歩を踏み出すための素晴らしい動機づけになるのです。技術情報の発信と共有ソフトウェアエンジニアにとって、自分の知見やノウハウを共有することは重要な責務の一つです。新しい技術や手法を学んだら、それを他のエンジニアにも伝えることで、エンジニアコミュニティ全体の知識レベルの向上に貢献することができます。Speaker Deckに公開した資料をブログ記事として再構成することで、技術情報をより詳細かつ体系的に発信することができます。スライドだけでは伝えきれなかった詳細な説明や、実際のコード例などを交えることで、より深い理解を促すことができるでしょう。さらに、ブログ記事にはコメント欄を設けることができます。読者からの質問や意見を受け付けることで、インタラクティブなコミュニケーションが生まれます。これは、さらなる知識の共有や、新たな発見につながる可能性を秘めています。ソフトウェアエンジニアが持つ知識は、利用するか共有されてこそ価値があります。Speaker Deckの資料をブログ記事化し、積極的に情報を発信することは、エンジニアコミュニティ全体の発展に寄与する素晴らしい取り組みだと言えるでしょう。ブログの方があとから見返しやすいSpeaker Deckの資料を読むと、その時は内容を理解した気になれます。特に、登壇を聞いている時は、登壇者の説明を聞きながら資料を見ることができるので、理解が深まった感覚を得られるでしょう。しかし、時間が経つと、資料の内容を忘れてしまうことが多いのではないでしょうか。資料だけでは、詳細な説明が不足していることが多いため、あとから見返しても、内容を思い出すことが難しいのです。一方、ブログ記事は、詳細な説明と補足情報が含まれているため、あとから見返した時にも内容を理解しやすいという利点があります。つまり、Speaker Deckの資料だけでは一時的な理解にとどまってしまいますが、ブログ記事であれば、長期的な理解と知識の定着に役立つのです。また、ブログ記事は検索しやすいというメリットもあります。特定の話題や技術について調べたい時に、関連するブログ記事を探すことができます。これは、自分が過去に学んだ内容を振り返る時にも役立ちます。さらに、ブログという文字のフォーマットを使うことで、登壇に比べて主張そのものに注意を向けさせることができます。人は身振り、声質、表情、顔といった外見や肩書に惑わされて主張を歪めて解釈してしまうことがありますが、ブログではそういった先入観をなるべく排除し、あくまで中身に集中させることができるのです。Speaker Deckの資料をブログ記事化することは、知識を長期的に活用するために非常に有効な方法だと言えるでしょう。おわりにSpeaker Deckに上げた資料をブログ記事として再構成することには、多くの意義があります。詳細な説明と補足情報の追加、アウトプットによる技術力の向上、継続的な学習習慣の確立、エンジニアとしての認知度向上とアイデンティティの確立、登壇への動機づけ、技術情報の発信と共有など、個人の成長とエンジニアコミュニティ全体の発展に寄与する様々なメリットがあるのです。また、Speaker Deckの資料は初見では理解した気になれますが、時間が経つと内容を忘れてしまいがちです。一方、ブログ記事は詳細な説明と補足情報が含まれているため、あとから見返した時にも内容を理解しやすいという利点があります。つまり、Speaker Deckの資料だけでは一時的な理解にとどまってしまいますが、ブログ記事であれば、長期的な理解と知識の定着に役立つのです。Speaker Deckは、スライドを公開・共有するための素晴らしいプラットフォームですが、それだけでは情報共有の手段としては限界があります。一方、ブログは、より詳細で探索しやすい情報を提供することができます。Speaker Deckとブログを組み合わせることで、より効果的な技術情報の発信が可能になるのです。皆さんも、自分の知見を共有するためにこの方法を活用してみてはいかがでしょうか？参考資料木村政彦はなぜ力道山を殺さなかったのか作者:増田俊也新潮社Amazon人生は、運よりも実力よりも「勘違いさせる力」で決まっている作者:ふろむだダイヤモンド社Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[新人SREが0から始めるGKE上でのArgoCDによるWordPressデプロイ]]></title>
            <link>https://sreake.com/blog/deploy-wordpress-with-argocd-on-gke/</link>
            <guid>https://sreake.com/blog/deploy-wordpress-with-argocd-on-gke/</guid>
            <pubDate>Thu, 21 Mar 2024 23:34:40 GMT</pubDate>
            <content:encoded><![CDATA[はじめに はじめまして。Sreake事業部インターン生の高島です。2023年10月から長期インターン生としてKubernetes関連技術の習得とSRE技術の調査・検証を行っています。私は、情報系の大学院生で、普段は数値解 […]The post 新人SREが0から始めるGKE上でのArgoCDによるWordPressデプロイ first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[データエンジニアリングの要諦の後ろ髪を掴む - Fundamentals of Data Engineeringを読んで]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/03/20/164434</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/03/20/164434</guid>
            <pubDate>Wed, 20 Mar 2024 07:44:34 GMT</pubDate>
            <content:encoded><![CDATA[最強なデータ分析基盤は何か⁉︎多種多様なデータ分析基盤が、制約のない環境で競合した時… ビジネス用途に限らず、あらゆるシナリオで使用可能な「データ分析」で比較した時、最強なデータ分析基盤は何か⁉︎ 今現在最強のデータ分析基盤は決まっていないデータ分析基盤まとめ（随時更新） などもあり大変参考にさせていただきました。ありがとうございます。はじめにデータエンジニアリングは、データの収集、処理、保存、そして提供を行う技術やプロセスを扱う複雑な分野です。この分野の全容を系統的に把握することは決して容易なことではありません。このような状況の中で、『Fundamentals of Data Engineering』という書籍に出会いました。この本は、著者たちの豊富な実務経験に基づいて書かれており、データエンジニアリングの基本概念とそのライフサイクルに焦点を当てています。さらに、これらの概念を現実の問題解決に応用する方法についても詳しく説明しています。全624ページに及ぶこの書籍は、その分厚さが示す通り、読破するには相当な時間と努力が必要ですが、その価値は十分にあると確信しています。learning.oreilly.com本書の特徴は、特定のツールや技術ではなく、データエンジニアリングの根幹をなす原則に焦点を当てている点です。著者らは、データ生成、ストレージ、取り込み、変換、提供といったライフサイクルの各段階を丁寧に解説し、それらを支える重要な概念を、具体的な技術選択やアーキテクチャ設計と関連付けて説明しています。また、クラウド技術を効果的に組み合わせて、下流のデータ消費者のニーズに応えるための方法論も提示しています。本書は、データエンジニアリングの理論と実践を見事に融合させ、この分野の要諦を掴むための羅針盤となります。著者らの豊富な知見と経験が随所に活かされ、ベストプラクティスのフレームワークを用いた問題の評価方法、市場の誇大広告を見抜く視点、堅牢なアーキテクチャの設計・構築手法などが解説されています。これらの知識は、データエンジニアリングの要諦を理解し、実践に活かすために不可欠な要素です。また、本書は、データエンジニアリングを取り巻く環境の変化についても言及しています。特に、クラウドファーストのアプローチを取ることで、オンプレミスからクラウドへのシフトを見据えた議論を展開しています。加えて、セキュリティとプライバシーの重要性についても強調しており、データエンジニアリングの現在と未来を見据えた内容となっています。本書を通じて、データエンジニアリングの全体像を俯瞰し、実践的な知識を得ることができました。データエンジニアリングの原則を自らの役割に取り入れ、クラウド技術を駆使して問題解決に取り組む方法を学べた点は、特に有益でした。本書は、データエンジニアリングの要諦を掴むための一助となる、貴重な一冊であると言えます。本稿はそんな書籍の読書感想文である。あくまで、私の感想なので指摘はそれぞれのSNSに書き散らしてください。『Fundamentals of Data Engineering』の構成本書は4つのパートで構成されています。パートIでは、第1章でデータエンジニアリングを定義し、第2章でデータエンジニアリングのライフサイクルを示します。第3章ではよいアーキテクチャについて議論し、第4章では適切な技術を選択するためのフレームワークを紹介します。パートIIは、第2章を基にデータエンジニアリングのライフサイクルを深く掘り下げています。データ生成、ストレージ、取り込み、変換、提供の各段階が独立した章で扱われます。パートIIは本書の中核をなす部分であり、他の章はここで扱われる核心的なアイデアをサポートするために存在しています。パートIIIでは、追加のトピックスとして、第10章でセキュリティとプライバシーについて議論しています。これらは常にデータエンジニアリングにおいて重要な部分でしたが、営利目的のハッキングや国家支援のサイバー攻撃の増加に伴い、さらに重要性が増しています。また、GDPRやCCPAなどの規制の出現により、個人データの不注意な取り扱いは重大な法的影響を及ぼす可能性があります。第11章では、データエンジニアリングの近未来について、著者らの大胆な予測を概説しています。付録では、データエンジニアリングの日々の実践に非常に関連性が高いものの、本文の主要部分には収まらなかった技術的トピックスを取り上げています。具体的には、シリアル化と圧縮（付録A）、クラウドネットワーキング（付録B）です。はじめに『Fundamentals of Data Engineering』の構成Part I. Foundation and Building BlocksChapter 1. Data Engineering DescribedChapter 2. The Data Engineering LifecycleChapter 3. Designing Good Data ArchitectureChapter 4. Choosing Technologies Across the Data Engineering LifecyclePart II. The Data Engineering Lifecycle in DepthChapter 5. Data Generation in Source SystemsChapter 6. StorageChapter 7. IngestionChapter 8. Queries, Modeling, and TransformationChapter 9. Serving Data for Analytics, Machine Learning, and Reverse ETLPart III. Security, Privacy, and the Future of Data EngineeringChapter 10. Security and PrivacyChapter 11. The Future of Data EngineeringさいごにPart I. Foundation and Building BlocksChapter 1. Data Engineering Describedデータエンジニアリングを「raw dataを取り込み、高品質で一貫性のある情報を生成するシステムとプロセスの開発、実装、維持」と定義しています。データエンジニアは、セキュリティ、データ管理、DataOps、データアーキテクチャ、オーケストレーション、ソフトウェアエンジニアリングの交差点に位置し、データのソースシステムから始まり、分析や機械学習などのユースケースにデータを提供するまでのライフサイクル全体を管理します。Figure 1-1. The data engineering lifecycle よりまた、データエンジニアリングの歴史的な発展についても触れられており、データウェアハウジングから始まり、ビッグデータ時代を経て、現在はデータのライフサイクル全体を管理するフェーズに入っていることが分かります。データエンジニアは、データサイエンティストの上流に位置し、分析やモデリングに必要な基盤を提供する重要な役割を担っています。さらに、本章では、企業のデータ成熟度に応じたデータエンジニアの役割の変化や、他の技術的役割（ソフトウェアエンジニア、データアーキテクト、DevOpsエンジニアなど）およびビジネスリーダーとの関わりについても説明されています。データエンジニアは、技術的スキルだけでなく、コミュニケーション能力やビジネス理解も求められる、組織内の重要な接点となる存在であることが強調されています。本章を通じて、データエンジニアリングが急速に発展し、組織内で不可欠な役割を担うようになってきたことを実感しました。データドリブンな意思決定が求められる現代において、データエンジニアは、データの価値を最大限に引き出すための鍵を握っています。今後もデータエンジニアリングの動向に注目し、自身のスキルを磨いていく大切さを学びました。Chapter 2. The Data Engineering Lifecycleデータエンジニアリングのライフサイクルについて詳細に解説されています。データエンジニアリングのライフサイクルとは、raw dataを有用な最終製品に変換するための一連のプロセスを指します。本章では、データエンジニアリングのライフサイクルを5つのステージ（生成、ストレージ、取り込み、変換、提供）に分類し、各ステージの役割と考慮事項を丁寧に説明しています。また、ライフサイクル全体を支える重要な要素として、セキュリティ、データ管理、DataOps、データアーキテクチャ、オーケストレーション、ソフトウェアエンジニアリングの6つの「潮流」を紹介しています。特に印象的だったのは、データ管理の重要性についての議論です。著者らは、データガバナンス、データモデリング、データの系統、データ統合、ライフサイクル管理など、企業のデータ管理における様々なベストプラクティスを紹介し、これらがデータエンジニアリングにどのように関連するかを明確に示しています。データエンジニアは、単なる技術者ではなく、組織全体のデータ活用を戦略的に促進する役割を担っているのだと実感しました。また、DataOpsの概念も興味深かったです。DataOpsは、アジャイル開発、DevOps、統計的プロセス管理の手法をデータに適用したものであり、自動化、モニタリング、インシデント対応の3つの要素から成ります。データエンジニアリングにおいてDataOpsを実践することで、データ製品の迅速な開発と高品質な運用が可能になるとのことです。本章を通じて、データエンジニアリングが、単なるデータ処理の技術にとどまらず、組織のデータ活用を支える総合的な取り組みであることを学びました。データエンジニアは、ライフサイクルの各ステージにおける技術的な選択と、セキュリティ、データ管理、アーキテクチャなどの戦略的な考慮事項のバランスを取ることが求められます。本書で提示されたデータエンジニアリングのライフサイクルのフレームワークは、この複雑な領域を体系的に理解するための強力なツールになると感じました。Chapter 3. Designing Good Data Architectureデータエンジニアリングにおける良いアーキテクチャ設計について詳細に解説されています。本章では、まず、データアーキテクチャを「企業のデータニーズの進化を支えるシステムの設計であり、柔軟で可逆的な意思決定により、トレードオフを慎重に評価して達成されるもの」と定義しています。そして、良いデータアーキテクチャの原則として、共通コンポーネントの賢明な選択、障害への対策、スケーラビリティの確保、リーダーシップ、継続的なアーキテクト活動、疎結合システムの構築、可逆的な意思決定、セキュリティの優先、FinOpsの採用の9つを挙げています。また、本章では、分散システム、スケーラビリティ、障害対策、密結合と疎結合、シングルテナントとマルチテナント、イベント駆動アーキテクチャ、ブラウンフィールドとグリーンフィールドプロジェクトなど、データアーキテクチャ設計に関連する主要な概念について説明しています。さらに、データウェアハウス、データレイク、モダンデータスタック、ラムダアーキテクチャ、カッパアーキテクチャ、IoTアーキテクチャ、データメッシュなど、具体的なデータアーキテクチャの例や種類についても紹介されています。これらの例を通じて、データエンジニアがビジネスの要件に合わせて適切なアーキテクチャを選択し、設計するための知見が提供されています。本章を読んで、データアーキテクチャ設計の重要性と複雑さを改めて認識しました。データエンジニアは、技術的な知識だけでなく、ビジネスの文脈を理解し、ステークホルダーとのコミュニケーションを通じて要件を把握する必要があります。そして、セキュリティ、データ管理、アーキテクチャなどの戦略的な考慮事項とのバランスを取りながら、柔軟で進化可能なアーキテクチャを設計していかなければなりません。この辺はソフトウェアアーキテクチャの基礎を思い出した。ソフトウェアアーキテクチャの基礎 ―エンジニアリングに基づく体系的アプローチ作者:Mark Richards,Neal FordオライリージャパンAmazon本書で提示された良いデータアーキテクチャの原則や、様々なアーキテクチャパターンの知識は、この難しい課題に取り組むための強力な助けになると感じました。データエンジニアとして、これらの知見を活かし、組織のデータニーズに合ったアーキテクチャを設計していきたいと思います。詳細に知りたい場合には『データ指向アプリケーションデザイン』あたりを読むと良さそうデータ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin KleppmannオライリージャパンAmazonChapter 4. Choosing Technologies Across the Data Engineering Lifecycleデータエンジニアリングのライフサイクル全体にわたる適切な技術選択のための考え方と基準について詳細に説明されています。この章では、アーキテクチャが戦略的な設計であることに対し、ツールはその実現を目指す戦術的な選択肢であるという点が強調されています。 技術選択時に考慮すべき要素として、チームの規模と能力、市場投入までのスピード、相互運用性、コスト最適化とビジネス価値、技術トレンドの変化、デプロイ環境、ビルドかバイの選択、モノリシックかモジュール化か、サーバーレスかサーバーか、性能最適化などが挙げられています。あまりにも「ソフトウェアアーキテクチャの基礎」すぎてデータ基盤もソフトウェアアーキテクチャなのだと分からせをくらいました。加えて、クラウドのコスト効率とクラウドネイティブアーキテクチャのコスト最適化の重要性が説明されており、オンプレミス、クラウド、ハイブリッドクラウド、マルチクラウドなどの配置オプションとその特性についても詳述されています。オープンソースソフトウェア（コミュニティ型と商用型）とプロプライエタリーソフトウェアの選択、モノリシックとマイクロサービスアーキテクチャの比較、サーバーレスと従来型サーバーの検討など、具体的な技術選択のシナリオにおける検討が提示されています。技術選択の複雑さとその重要性を理解する上で、この章は大いに役立ちます。データの世界は常に進化しているため、最適な選択肢は状況に応じて変わります。適切なトレードオフを評価し、柔軟かつ可逆的な意思決定を行うことが重要です。この辺はソフトウェアアーキテクチャメトリクスみがあって良かった。ソフトウェアアーキテクチャメトリクス ―アーキテクチャ品質を改善する10のアドバイス作者:Christian Ciceri,Dave Farley,Neal Ford,Andrew Harmel-Law,Michael Keeling,Carola Lilienthal,João Rosa,Alexander von Zitzewitz,Rene Weiss,Eoin Woodsオーム社Amazonセキュリティ、データ管理、DataOps、オーケストレーションなどの現代のトレンドが技術選択に与える影響も大きいことが認識されています。これらを総合的に考慮し、ビジネス価値を最大化する技術スタックを構築することが、SREとしての責任であると捉えられます。本章で提供される原則とガイドラインは、DXの推進と共に増大する複雑な意思決定の指針となります。組織のニーズに沿いながら、これらの洞察を活用していくことが推奨されています。Part II. The Data Engineering Lifecycle in DepthChapter 5. Data Generation in Source Systems本章では、データエンジニアリングのライフサイクルの初期段階であるソースシステムにおけるデータの生成プロセスについての詳細な解説が展開されています。 ここで、データエンジニアがソースシステムからのデータの特性と生成プロセスを理解することの重要性が強調されており、これは非常に重要な点です。特に印象深かったのは、ソースシステムのオーナーやステークホルダーとの関係構築の必要性です。データエンジニアリングはチーム単独ではなく、関係者全員の協力が必須であり、上流システムで問題が生じた際に迅速な対応が可能な信頼関係の構築が不可欠です。データ品質の維持に関する言及もあり、これは特に重要です。ソースシステムの設計や運用に直接影響を与えることは困難かもしれませんが、期待されるデータ品質について上流チームと合意を形成し、定期的な品質チェックを行うことが必要です。これは、SREとしての役割とも重なる側面があります。セキュリティ、可用性、信頼性を考慮したソースシステムのアーキテクチャへの理解も、障害発生時に影響を最小限に抑え、迅速に復旧する設計を実現する上で重要です。さらに、データ管理、DataOps、オーケストレーションといったデータエンジニアリングの新しい動向とソースシステムとの関連性についても触れられており、これらの原則を上流工程に適用し、エンド・ツー・エンドでの高品質なデータパイプライン構築が目標です。リバースETLやイベントストリーミングプラットフォームの活用による、データエンジニアとソースシステムとの連携強化の可能性についての言及もあり、これはアプリケーション開発チームとのWin-Winの関係構築、及びユーザー向けデータ製品の共創へと繋がるでしょう。本章を通じて、SREとデータエンジニアの役割が密接に関連しており、両者の協力が不可欠であることが明確になりました。 上流から下流への一貫した高品質なデータフローを実現するためには、両分野の専門知識を統合し、継続的な改善を図る必要があります。得られた知見を活用し、開発チームと協力しながら、より堅牢なデータインフラを構築していくことが目指されています。Chapter 6. Storageデータエンジニアリングのライフサイクルにおけるストレージの重要性と、その設計・運用に関する考慮事項について詳しく解説されています。本章では、まず、ハードディスク、SSD、システムメモリなど、ストレージシステムを構成する基本的な要素について説明しています。データエンジニアは、これらの物理的ストレージコンポーネントの特性を理解することで、パフォーマンス、耐久性、コストのトレードオフを適切に評価できるようになります。次に、ファイルストレージ、ブロックストレージ、オブジェクトストレージ、ストリーミングストレージなど、主要なストレージシステムの種類と特徴を紹介しています。特に、クラウドにおけるオブジェクトストレージの重要性が強調されており、その柔軟性とスケーラビリティが、データレイクやクラウドデータウェアハウスの基盤となっていることが分かります。さらに、データウェアハウス、データレイク、データレイクハウス、データプラットフォームなど、データエンジニアリングで用いられる主要なストレージの抽象化についても言及されています。これらの抽象化は、ストレージシステムの上に構築され、データの取り込み、変換、提供といったライフサイクルの各段階をサポートします。本章では、ストレージに関する重要なトレンドや考え方についても触れられています。例えば、コンピュートとストレージの分離、ゼロコピークローニング、データカタログ、データ共有などは、現代のデータアーキテクチャにおいて欠かせない要素だと指摘されています。また、データのライフサイクルと保持期間の管理、シングルテナントとマルチテナントのストレージ設計の違いなど、運用面での考慮事項についても説明されています。データエンジニアは、これらの要素を総合的に判断し、組織のニーズに合ったストレージ戦略を立てる必要があります。本章を通じて、ストレージがデータエンジニアリングのあらゆる段階で重要な役割を果たしていることを再認識しました。生のデータを価値あるインサイトに変えるためには、適切なストレージの選択と設計が不可欠です。また、セキュリティ、データ管理、DataOps、オーケストレーションなどの「潮流」を常に意識しながら、ストレージシステムを進化させていく必要があります。本書で得られた知見を活かし、自社のデータアーキテクチャにおけるストレージの最適化に取り組んでいきたいと思います。特に、コストとパフォーマンスのバランスを取りつつ、将来の拡張性も考慮した設計を心がけたいと考えています。ストレージの話は『パタ&へネ』などを読むとしっかりと分かるので読み直す機会があれば読み返したい。しかし、人生の時間は有限なので悲しい。コンピュータの構成と設計　MIPS Edition　第6版　上作者:David Patterson,John Hennessy日経BPAmazonコンピュータの構成と設計 MIPS Editoin 第6版 下作者:David Patterson,John Hennessy日経BPAmazonChapter 7. Ingestionデータエンジニアリングにおけるデータ取り込みの重要性と複雑さを再認識しました。本章では、データ取り込みを「データを一つの場所から別の場所へ移動するプロセス」と定義しています。その主要な考慮事項として、ユースケース、再利用性、データ量、データフォーマット、データ品質などが挙げられています。さらに、バッチ処理とストリーミング処理の違い、同期型と非同期型のデータ取り込み、シリアル化とデシリアル化、スループットとスケーラビリティといった、設計上の重要な概念について詳しく説明されています。また、Otelなどのオブザーバビリティ情報の取得については言及されていないのですが、この章を通じて現代の監視基盤が実際にはデータエンジニアリングに大きく依存してるものなのだと思いはじめました。特に印象的だったのは、データ取り込みの方法の多様性です。データベースへの直接接続、CDC、API、メッセージキュー、ファイルエクスポートなど、様々な手段があり、それぞれにメリットとデメリットがあります。状況に応じて適切な方法を選択し、組み合わせることが求められます。また、データ取り込みにおけるデータ品質の確保の重要性も強調されていました。スキーマの変更や遅延データへの対応、エラーハンドリングなど、様々な課題に直面します。上流のシステムとの緊密なコミュニケーションと、ロバストなモニタリングの仕組みが不可欠だと感じました。本章では、データ取り込みに関わる様々なステークホルダーとの協力についても言及されています。特にソフトウェアエンジニアとのコラボレーションは、データ品質の向上と、よりリアルタイムなデータ活用につながる可能性があります。組織のサイロを超えて、Win-Winの関係を築いていくことが重要だと分かりました。さらに、セキュリティ、データ管理、DataOps、オーケストレーション、ソフトウェアエンジニアリングといった「潮流」が、データ取り込みにどのように影響するかについても議論されていました。これらの原則を常に意識しながら、エンドツーエンドのデータパイプラインを設計していく必要があります。データ取り込みは、地味な作業に見えるかもしれません。しかし、それは分析やMLなどのエキサイティングなアプリケーションを支える重要な基盤です。本章で得られた知見を活かし、より信頼性が高く、価値あるデータを提供できるよう、日々精進していきたいと思います。Chapter 8. Queries, Modeling, and Transformationデータエンジニアリングにおけるクエリ、モデリング、変換の重要性と技術的な考慮事項について理解を深めることができました。本章では、まず、クエリの仕組みと最適化の手法について解説されています。クエリオプティマイザの役割や、結合戦略の最適化、説明プランの活用など、パフォーマンス向上のための具体的なアドバイスが提示されており、大変参考になりました。また、ストリーミングデータに対するクエリの特殊性についても言及されていました。次に、データモデリングの重要性と主要な手法が紹介されています。概念モデル、論理モデル、物理モデルの違いや、正規化、スター・スキーマ、Data Vaultなどのバッチデータのモデリング手法、ストリーミングデータのモデリングの考え方など、幅広いトピックがカバーされています。ビジネスロジックをデータモデルに反映させることの重要性が強調されていました。そして、変換の役割と主要なパターンについて解説されています。単純なクエリとは異なり、変換では結果を永続化し、ダウンストリームで利用できるようにすることが目的だと説明されています。バッチ処理とストリーミング処理それぞれの変換パターンや、更新パターン、データラングリングなどの具体的な手法が紹介されていました。また、マテリアライズドビュー、フェデレーションクエリ、データ仮想化など、クエリ結果を仮想的なテーブルとして提示する手法についても言及されていました。これらの手法は、複雑なデータパイプラインの一部として活用できる可能性があります。本章では、クエリ、モデリング、変換に関わる様々なステークホルダーとの協力についても議論されています。ビジネスロジックを理解し、上流のシステムへの影響を最小限に抑えつつ、下流のユーザーにとって価値のあるデータを提供することが求められます。また、セキュリティ、データ管理、DataOps、オーケストレーション、ソフトウェアエンジニアリングといった「潮流」が、この段階でも重要な役割を果たすことが指摘されていました。データ変換は、データパイプラインの中核をなす工程です。単に最新の技術を追求するのではなく、ステークホルダーにとっての価値を常に意識することが重要だと感じました。本章で得られた知見を活かし、ビジネスの目標達成に貢献できるデータ変換プロセスを設計していきたいと思います。Chapter 9. Serving Data for Analytics, Machine Learning, and Reverse ETL本章では、データエンジニアリングのライフサイクルの最終段階である、データの提供について解説されていた。データエンジニアが直面する主要な3つのユースケース - 分析、機械学習、リバースETLについて、どのようにデータを提供するかが述べられていた。データを提供する際の重要な考慮点として、エンドユーザーがデータを信頼できるようにすることが何より大切だと強調されていた。データへの信頼がないと、いくら高度なアーキテクチャやシステムを構築しても意味がない。信頼を得るためには、データの検証とオブザーバビリティのプロセスを活用し、ステークホルダーと協力してデータの有効性を確認する必要がある。また、ユースケースとユーザーを理解し、提供するデータプロダクトを明確にし、セルフサービスかどうかを検討し、データの定義やロジックを確立することが重要だと述べられている。データメッシュのコンセプトにも触れられ、データ提供の方法が大きく変化しつつあることがわかった。分析や機械学習のためのデータ提供方法としては、ファイル、データベース、クエリエンジン、データ共有などがあげられていた。セマンティック層やメトリクス層の活用も有効とのことだった。また、ノートブックを使ったデータサイエンスのワークフローについても解説があり参考になった。リバースETLは、処理されたデータをOLAPシステムからソースシステムにロードすることだが、フィードバックループを作り出すリスクがあるので注意が必要だと指摘されていた。本章を読んで、データ提供において信頼性とセキュリティが非常に重要であり、様々な方法や最新のトレンドを理解しておく必要性を感じた。生のデータを渡すのではなく、匿名化などの工夫も必要だ。データプロダクトを通じてビジネスに貢献するという視点を常に持ちながら、品質の高いデータを提供できるよう、日々研鑽していきたい。Part III. Security, Privacy, and the Future of Data EngineeringChapter 10. Security and Privacyセキュリティとプライバシーは、データエンジニアリングにおいて非常に重要な側面であり、後回しにしてはならないということが本章で強調されていました。データ漏洩や流出は、企業に壊滅的な結果をもたらす可能性があります。GDPR、FERPA、HIPAAなど、データプライバシーに関する法的要件が増えており、違反すると多額の罰金が科せられる可能性があります。データエンジニアは、このような法規制を理解し、遵守する必要があります。セキュリティの最大の弱点は人間であるため、データエンジニアは常に防御的な姿勢で行動し、認証情報の扱いには細心の注意を払い、倫理的な懸念があれば提起しなければなりません。セキュリティプロセスはシンプルで習慣的なものでなければならず、単なるコンプライアンスのためのセキュリティ・シアターであってはいけません。最小権限の原則を適用し、必要最小限のアクセス権のみを付与すべきです。きめ細かいアクセス制御を実装することが重要です。クラウドにおけるセキュリティは、プロバイダーとユーザーの共同責任であり、ユーザー側の設定ミスが原因で流出が起こることが多いのです。定期的なデータバックアップは、災害復旧やランサムウェア対策に欠かせません。リストアのテストも定期的に行うべきでしょう。技術面では、脆弱性を修正するためのシステムのパッチ適用と更新、保存中と通信中の両方でのデータの暗号化、アクセス・リソース・コストのログ記録・監視・アラート、ネットワークアクセスの厳重な制御、CPUなどの低レベルでのセキュリティ考慮などが重要な実践項目として挙げられていました。すべてのエンジニアが自分の領域で潜在的なセキュリティ問題を探し出すという能動的な姿勢が重要であり、軽減策を積極的に展開すべきだと述べられています。本章を通して、セキュリティとプライバシーは企業文化・プロセス・技術のすみずみまで浸透させる必要があり、関係者全員が常に警戒心を持ち、積極的な対策を講じることが機密データ資産を守るために不可欠だということを実感しました。法的にも評判的にも、その重要性は非常に高いのです。データエンジニアとして、**セキュリティとプライバシーを常に最優先事項と位置づけ、ベストプラクティスを実践していきます。Chapter 11. The Future of Data Engineering本章では、データエンジニアリングの将来について著者の考察が述べられていました。データエンジニアリングの分野は急速に変化しているため、本書の執筆は挑戦的な作業だったと思います。しかし、変化の中にも不変の本質を見出し、ライフサイクルという形で体系化したことは意義深いと感じました。著者は、データエンジニアリングのライフサイクルは今後も変わらず重要であり続けると予測しています。一方で、ツールの簡素化が進み、より高度な作業にフォーカスできるようになるでしょう。クラウドスケールの「データOS」の出現によって、相互運用性が向上することも期待されます。また、従来の「モダンデータスタック」を超えて、リアルタイムのデータ処理と機械学習を融合させた「ライブデータスタック」へと進化するとの展望も示されていました。ストリーミングパイプラインとリアルタイム分析データベースの発展によって、アプリケーションとデータ、機械学習の間のフィードバックループが短くなり、より洗練されたユーザー体験が実現するというビジョンは興味深いです。一方で意外だったのは、スプレッドシートの重要性への言及でした。確かに、現場ではExcelが分析ツールとして依然大きな役割を果たしています。クラウドのOLAPシステムとスプレッドシートの使い勝手を兼ね備えた新しいツールの登場にも注目したいと思います。全体を通して、技術トレンドは複雑な技術と文化の相互作用の中で生まれるものであり、予測は難しいというのが著者の率直な見解だと感じました。私たち一人一人がデータエンジニアリングの発展に関わっていく中で、ツールの採用と活用を通じて、ビジネス価値の創出という大きな目標を見失わないようにしたいと思います。本書で得た知見をもとに、コミュニティに参加し、専門家と対話しながら、自分なりの探求を続けていきたいと思います。データエンジニアリングは奥深く、やりがいのある分野だと改めて感じました。さいごにこの本を通じて、私はデータエンジニアリングの幅広さと深さを理解する機会を得ました。『Fundamentals of Data Engineering』は、データエンジニアリングの基礎から応用に至るまで、その様々な側面を包括的に解説しており、データエンジニアとしての技術や知識の向上に寄与する貴重なリソースです。データライフサイクルの各段階に対する詳細な説明は、実務で直面するさまざまな問題への理解を深めるのに非常に有用です。また、セキュリティとプライバシーの章では、技術の理解だけでなく、倫理的な視点から物事を考えることの重要性が強調されていることが特に印象的でした。データエンジニアは技術者であると同時に、データを取り扱う上での社会的責任を有する存在であり、この点を再確認させられます。データエンジニアリングの将来に関する展望を含めて、この書籍は、データエンジニアリングの現状理解と将来に向けた方向性を示す貴重な指南書です。技術の進歩は早く、今日学んだことが明日には旧式になる可能性がありますが、本書で得られる原則や考え方は、変わることのない有用な知識。データエンジニアリングの基盤となります。syu-m-5151.hatenablog.com最後に、この本を読むことで得られる最大の利点は、データエンジニアリングに対する深い理解と共に、学び続け、成長し続けることの重要性を再認識できることです。技術変遷に適応しつつ、データエンジニアリングの核心を見失わないよう努めることが、私たちには求められています。この旅は続きますが、『Fundamentals of Data Engineering』は、その道中で頼りになる羅針盤となるでしょう。日本語訳の出版が待ち遠しいですね。そして、付録「A. Serialization And Compression Technical Details」と「B. Cloud Networking」に関しては、ぜひ自身で読んでみていただきたいです。これらのセクションは、データエンジニアリングの深い理解に不可欠なテクニカルな洞察を提供しており、実務での適用に役立つ知見が満載です。Fundamentals of Data Engineering (English Edition)作者:Reis, Joe,Housley, MattO'Reilly MediaAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ECSのタグ付け認可とアカウント単位のオプトアウトの廃止]]></title>
            <link>https://blog.masasuzu.net/entry/2024/03/20/121151</link>
            <guid>https://blog.masasuzu.net/entry/2024/03/20/121151</guid>
            <pubDate>Wed, 20 Mar 2024 03:11:51 GMT</pubDate>
            <content:encoded><![CDATA[ECSのタグ付け認可とはアカウント単位のオプトアウトの廃止確認影響がある例対応まとめ関連リソースECSのタグ付け認可とはECS関連のリソース作成時にリソースタグを付けることができます。その際 ecs:tagResource の権限が必要となります。なお、リソースタグを設定しないECSリソース作成の際は権限不要です。この権限の有無のチェックをタグ付け認可と言います。具体的にECSリソースの作成のアクションは以下の通りです。CreateCapacityProviderCreateClusterCreateServiceCreateTaskSetRegisterContainerInstanceRegisterTaskDefinitionRunTaskStartTaskタグ付け認可の仕組みは2023年4月18日に導入されました。しかしながら従来からECSリソースを作成する際にタグ付けしていたAWSアカウントに関しては影響があるため、アカウントレベルでタグ付け認可の機能を無効(オプトアウト)することができました。つまりアカウントレベルで無効にしていれば ecs:tagResource の権限がなくてもタグ付けをすることが可能でした。しかしながらアカウント単位のオプトアウト設定は2024年3月9日に廃止されます。アカウント単位のオプトアウトの廃止タグ付け認可におけるタイムラインは以下のとおりです2023年4月18日 タグ付け認可の導入とアカウント単位での有効化設定の導入2024年2月9日- 2月28日 新規アカウントおよび影響を受けないアカウントに関してデフォルトでタグ付け認可の有効化が行われる2024年2月29日 アカウント単位で有効にしている場合、無効に変更できなくなる2024年3月29日 すべてのアカウントでタグ付け認可が有効になり、アカウント単位での設定が不可能になる現時点(2024/03/20)であまり時間がありません。現在タグ付け認可に影響あるAWSアカウントに関しては、Personal Health Dashboadに以下のような通知が来ているはずです。▼ElasticContainerService security notification (クリックで展開)▼English follows Japanese | 英語のメッセージは日本語の後にございますお客様のアカウントにて過去 1 年以内に ecs:TagResource の許可無しに ECS リソースの作成時にタグを付けていることが判明したため、ご連絡差し上げます。Amazon ECS は、2023 年 4 月 18 日にリソース作成のタグ付け認証を導入しました [1]。新規および既存のお客様は、ECS Console または API の ECS アカウント設定ページを使用して、この新機能の使用をオプトインする必要があります。このセキュリティ制御により、ECS リソースの作成時にタグをつけることをユーザーに拒否または許可できます。2024 年 3 月 29 日以降もお客様の IAM プリンシパルが新しく作成された ECS リソースに引き続きタグを適用できるように、IAM ポリシーを更新して ecs:TagResource アクションを明示的に許可することを強くお勧めします。2024 年 2 月 9 日以降、AWS コンソール の ECS アカウント設定ページにて tagResourceAuthorization アカウント設定を明示的に off に設定していないすべてのお客様のアカウントは、自動的にこの設定にオプトインされました。お客様の AWS アカウントは一時的に許可リストに載せているため、2024 年 3 月 29 日まではタグリソース認証の off の動作が継続されます。2024 年 3 月 8 日、現在オプトインしているアカウントが tagResourceAuthorization をオプトアウトする機能を削除し、タグをサポートするすべての ECS リソースの作成に際して ecs:TagResource IAM 権限の使用を強制するようにしました。最終的に 2024 年 3 月 29 日をもってお客様のアカウントを許可リストから削除し、tagResourceAuthorization を有効化します。呼び出し元のプリンシパルの IAM ポリシーに ecs:TagResource アクションを含めずにタグをつけて ECS リソースを作成しようとすると、「AccessDenied」メッセージが表示されます。この変更は CreateCapacityProvider, CreateCluster, CreateService, CreateTaskSet, RegisterContainerInstance, RunTask, StartTask, および RegisterTaskDefinition の API に影響を及ぼします。ecs:TagResource を使用しない拒否レスポンスの例以下は、ecs:CreateCluster アクションを付与している IAM ポリシーの一部です。ecs:TagResource アクションは含まれていません。tagResourceAuthorization アカウント設定がオンの場合、リクエスト例では以下の AccessDenied 例外が返されます。# IAM ポリシー“Statement”: [{“Sid”: “AllowCreateCluster”,“Effect”: “Allow”,“Action”: [“ecs:CreateCluster”],“Resource”: “*”}]# クラスター作成のリクエストaws ecs create-cluster --cluster-name MyCluster --tags key=key1,value=value1# タグ付けの拒否されたレスポンスAn error occurred (AccessDeniedException) when calling the CreateCluster operation:User: is not authorized to perform: ecs:TagResource on resource: cluster/MyCluster because no identity-based policy allows the ecs:TagResource action必要なアクション:IAM プリンシパルが 2024 年 3 月 29 日以降も新しく作成された ECS リソースに引き続きタグを適用できるように、IAM ポリシーに次のステートメントを追加することを強くお勧めします。すべての ECS リソースの作成時にタグ付けを許可以下の説明に従って ecs:TagResource アクションを追加すると、ECS リソースの作成中にタグ付けが可能になります [2]。“Statement”: [{“Sid”: “AllowTagging”,“Effect”: “Allow”,“Action”: [“ecs:TagResource”],“Resource”: “*”}]単一の ECS リソースタイプ (ECS クラスタ) の作成時にタグ付けを許可条件ステートメント ecs:CreateAction を使用すると、タグ付けを特定の ECS API に制限できます。以下の例では、ECS CreateCluster API でのみタグ付けへのアクセスを許可します。タグ付きの ECS RunTask API へのリクエストは、拒否判定になります [2]。“Statement”: [{“Sid”: “AllowClusterTagging”,“Effect”: “Allow”,“Action”: [“ecs:TagResource”],“Resource”: “*”,“Condition”: {“StringEquals”: {“ecs:CreateAction” : “CreateCluster”}}}]タイムライン:2024 年 2 月 9 日（完了）- タグ付け認証はデフォルトで on になっています。これには、ホワイトリストに登録されているアカウントは含まれません。tagResourceAuthorization アカウント設定の on/off を切り替えることも可能であり、ポリシーへの準拠をテストいただけます。2024 年 3 月 8 日 - タグ付け認証を on にすると、off にすることはできなくなります。この日まではアカウント設定を切り替えることができますので、その間に IAM ポリシーをテストすることをお勧めします。2024 年 3 月 29 日 - すべての AWS アカウントでタグ付け認証が有効になります。アカウントレベルの設定は使用されなくなり、AWS コンソールの ECS アカウント設定ページから削除されます。ご質問やご不明点等ございましたら、AWS サポート [3] までお問い合わせください。[1] https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#tag-resources-setting[2] https://docs.aws.amazon.com/AmazonECS/latest/developerguide/supported-iam-actions-tagging.html[3] https://aws.amazon.com/support---We are contacting you because we identified that your account has tagged ECS resources upon creation, within the past year, without the ecs:TagResource permission. Amazon ECS introduced tagging authorization for resource creation on April 18, 2023 [1]. New and existing customers must opt-in to use this new feature by using the ECS Account Settings page in the ECS Console or API. This security control allows users to deny or allow tagging ECS resources when they are created. We strongly recommend you update your IAM policies to explicitly allow the ecs:TagResource action so that your IAM principals continue applying tags to newly created ECS resources on or after March 29, 2024.From February 9, 2024, all customer accounts which have not explicitly set the tagResourceAuthorization account setting to “off” in the ECS Account Settings page in the AWS Console were automatically opted into the setting. We have temporarily allow-listed your AWS account so you will continue to have the “off” behavior for tagResourceAuthorization until March 29, 2024.On March 8, 2024, we removed the ability for currently opted-in accounts to opt-out of tagging authorization and enforced the creation of all ECS resources that support tags to use the ecs:TagResource IAM permission.Finally on March 29, 2024, we will remove your account from the allow-list and activate tagResourceAuthorization. You will experience an "AccessDenied" message if you attempt to create tagged ECS resources without including the ecs:TagResource action in the IAM policy of the calling principal. This change will affect the following APIs: CreateCapacityProvider, CreateCluster, CreateService, CreateTaskSet, RegisterContainerInstance, RunTask, StartTask, and RegisterTaskDefinition.Example Deny Response without ecs:TagResourceThe following is part of an IAM policy that is granting the ecs:CreateCluster Action. It does not include the ecs:TagResource Action. When tagResourceAuthorization Account setting is on, the example request would return the AccessDeniedException below.# IAM Policy“Statement”: [{“Sid”: “AllowCreateCluster”,“Effect”: “Allow”,“Action”: [“ecs:CreateCluster”],“Resource”: “*”}]# Create Cluster Requestaws ecs create-cluster --cluster-name MyCluster --tags key=key1,value=value1# Tagging Denied ResponseAn error occurred (AccessDeniedException) when calling the CreateCluster operation:User: is not authorized to perform: ecs:TagResource on resource: cluster/MyCluster because no identity-based policy allows the ecs:TagResource actionRequired Action:To ensure your IAM principals continue applying tags to newly created ECS resources on or after March 29, 2024, we strongly recommend adding the following statement(s) to your IAM policies:Allow Tagging during creation for all ECS ResourcesAdding the ecs:TagResource Action as described below would Allow tagging during ECS resource creation [2].“Statement”: [{“Sid”: “AllowTagging”,“Effect”: “Allow”,“Action”: [“ecs:TagResource”],“Resource”: “*”}]Allow Tagging during creation for single ECS Resource Type (ECS Cluster)Using the Conditional statement ecs:CreateAction allow you to limit the tagging to a specific ECS API. The example below grants access to tagging only on the ECS create-cluster API. A request to the ECS API run-task with tags would result in a Deny decision [2].“Statement”: [{“Sid”: “AllowClusterTagging”,“Effect”: “Allow”,“Action”: [“ecs:TagResource”],“Resource”: “*”,“Condition”: {“StringEquals”: {“ecs:CreateAction” : “CreateCluster”}}}]Timeline:February 9, 2024 (Completed) - Tagging Authorization is “on” by default. This excludes your account which is allowlisted. The tagResourceAuthorization account setting can be turned on/off to help test your policy compliance.March 8, 2024 - Tagging Authorization can no longer be turned “off” once it is turned “on”. It is recommended that you test your IAM policies before this date while you are able to toggle the account setting.March 29, 2024 - Tagging Authorization will be turned on for all AWS accounts. The account level setting will no longer be used and will be removed from the ECS Account Settings page in the AWS Console.If you have any questions, please contact AWS Support [3].[1] https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-account-settings.html#tag-resources-setting[2] https://docs.aws.amazon.com/AmazonECS/latest/developerguide/supported-iam-actions-tagging.html[3] https://aws.amazon.com/support通知が来ているアカウントは29日までに対応する必要があります。確認aws ecs list-account-settings --effective-settings --name tagResourceAuthorization を実行すると以下のような表示になると思います。ここがonであれば、すでにアカウント単位で有効になってるので影響がありません。(ただし、タグ付きのリソースを新規作成する際には権限が足りないとエラーになる可能性はあります)ここがoffになっている場合、タグ付け認可が無効になってるので3月29日以降影響を受ける可能性があります。% aws ecs list-account-settings --effective-settings --name tagResourceAuthorization{    "settings": [        {            "name": "tagResourceAuthorization",            "value": "on",            "principalArn": "arn:aws:iam::xxxxxxxxxxxx:root"        }    ]}影響がある例ユースケースにもよりますが、タグ付け認可に関連する操作は以下のようなものが考えられるかと思いますインフラ担当者によるECSリソース構築開発担当者(またはCI/CD)によるECSサービスのデプロイ前者に関しては、PowerUser相当の強い権限を付与されていることが多くここが問題になることはほとんどど無いかとは思います。後者の特にCI/CDによるデプロイに問題となることがありえます。一般的に非人間ユーザで目的が明確であれば、最小権限の原則に則り、 ecs:TagResource が付与されていない可能性があります。トライアンドエラーで権限を付与した場合、過去にうまく動いたためそのままの権限で使い続けている可能性もあります。その場合影響がある可能性あります。デプロイ時のタスク定義登録の際、タスク定義内に従来なかったtagsの記述を新規追加した際にResgisterTaskDefinitionでエラーになるという事例を私は経験しました。タスク定義にtagsがないときはタグ付け認可は実行されないのでそのまま成功していたため、ecs:TagResource が必要なことに気づいていませんでした。エラーとしては以下のような記述になるので、タグ付け認可の機能の存在を知っていて冷静に読み解けば、ecs:TagResource が足りていないことに気づけると思います。An error occurred (AccessDeniedException) when calling the RegisterTaskDefinition operation: User: arn:aws:sts::xxxx:assumed-role/deploy-github-actions/GitHubActions is not authorized to perform: ecs:TagResource on resource: arn:aws:ecs:ap-northeast-1:xxxx:task-definition/ecs-service because no identity-based policy allows the ecs:TagResource action対応まずECSサービスを利用しているIAM RoleとIAM Policyを洗い出します。その上でそれらが以下のアクションを許可している場合、ecs:TagResource を追加してあげます。CreateCapacityProviderCreateClusterCreateServiceCreateTaskSetRegisterContainerInstanceRegisterTaskDefinitionRunTaskStartTask私の場合は、ECSサービスデプロイ用のポリシーに以下のStatementを追加しました。それぞれ適切な記述を足していただけたらと思います。この場合タスク定義を登録する際にタグ付け認可を通すような許可を追加しています。        {            "Action": "ecs:TagResource",            "Condition": {                "StringEquals": {                    "ecs:CreateAction": "RegisterTaskDefinition"                }            },            "Effect": "Allow",            "Resource": "arn:aws:ecs:ap-northeast-1:xxxxxx:task-definition/yyyyyyyyyyyyyyy",            "Sid": "RegisterTaskDefinitionWithTag"        },まとめタグ付け認可について説明しました。タグ付け認可は2024年3月29日に強制的に全アカウントで有効になります。時間が少ないですが、影響受ける可能性があるかどうかチェックしてハマらないようにしましょう。また、これまでタグ付けしてなかったリソースにタグ付けする際にタグ付け認可に引っかかる可能性があります。デプロイやリソース作成の際にnot authorized to perform: ecs:TagResource と言われたらこの記事を思い出していただけたらと思います。それでは良いECSライフを!関連リソースアカウント設定による Amazon ECS 機能へのアクセス - Amazon Elastic Container Service タグ付け認可リソース作成時にタグ付けするための許可を付与する - Amazon Elastic Container Service]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Skaffoldの良さを色々語る]]></title>
            <link>https://zenn.dev/kojake_300/articles/11945f2047b22b</link>
            <guid>https://zenn.dev/kojake_300/articles/11945f2047b22b</guid>
            <pubDate>Mon, 18 Mar 2024 11:24:43 GMT</pubDate>
            <content:encoded><![CDATA[この記事は、2024/3/15に登壇したJagu'e'r クラウドネイティブ分科会　俺の考える最強のCI/CDのリマスターになります。 k8sアプリケーション開発の悩み突然ですが皆さん、k8sでアプリを動かす時にこんな悩み、イライラはありませんか？k8sで検証する時には必ず通る道だと思います。効率よく検証するにはどうしたものか、、Skaffoldはそんな悩みを解決してくれます😄 Skaffoldとは？ 概要Skaffold[1]は、コンテナベース及びKubernetesアプリケーションの継続的開発(Continuous Development = CD)を容易...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tagpr で tag trigger の workflow が実行されなくてハマった話]]></title>
            <link>https://blog.1q77.com/2024/03/tagpr/</link>
            <guid>https://blog.1q77.com/2024/03/tagpr/</guid>
            <pubDate>Fri, 15 Mar 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[最近 tagpr という便利ツールの存在を知って試していたのですが、使い方が悪くてハマったのでメモ。 tagpr とは 作者さまの記事を参照ください。 リリース用のpu]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Helm chart を GitHub Container Registry に host する]]></title>
            <link>https://blog.1q77.com/2024/03/helm-push-to-ghcr/</link>
            <guid>https://blog.1q77.com/2024/03/helm-push-to-ghcr/</guid>
            <pubDate>Thu, 14 Mar 2024 15:13:39 GMT</pubDate>
            <content:encoded><![CDATA[背景 最近は書いたアプリを Kubernetes に deploy することも多い。 その際に helm で簡単に deploy できるようになっていると便利ということで Helm chart を Git に入れておいても良いのだけ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Docker Desktop のアンインストールと Lima の導入]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/03/14/083605</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/03/14/083605</guid>
            <pubDate>Wed, 13 Mar 2024 23:36:05 GMT</pubDate>
            <content:encoded><![CDATA[はじめにDocker Desktop は多くの開発者にとって便利なツールですが、さまざまな理由で Lima への移行を検討するケースもあります。この記事では、MAC でDocker Desktop をアンインストールし、Lima を導入する過程を説明します。参考文献LimaInstallation | LimaUsage | LimaDocker Desktop のアンインストールDocker Desktop のアンインストールは、公式ドキュメントの指示に従って行うことができます。アンインストールはシステムの設定やリソースの解放に役立ち、Lima の導入の準備を整えます。# CLI から Docker Desktop をアンインストールすることもできます。さようなら。/Applications/Docker.app/Contents/MacOS/uninstall# Docker Desktop をアンインストールした後、削除できるファイルがいくつか残るので合わせて削除rm -rf ~/Library/Group\ Containers/group.com.dockerrm -rf ~/Library/Containers/com.docker.dockerrm -rf ~/.docker参照: Docker Desktop アンインストール方法Lima のインストールLima は、macOS で Linux 仮想マシンを容易に管理するためのツールです。Docker コンテナの実行環境として Lima を使用することで、Docker Desktop と同等の機能を低リソースで利用できます。Lima のインストールプロセスは以下のコマンドで行います。# Lima インスタンスの作成:# `docker` という名前の Lima インスタンスを Docker のテンプレートを使って作成します。limactl create --name=docker template://docker# Lima インスタンスの起動:# 作成した `docker` インスタンスを起動します。limactl start docker# 稼働中の Lima インスタンスの一覧表示:# 現在稼働中の Lima インスタンスの状態を表示します。limactl lsNAME       STATUS     SSH                VMTYPE    ARCH       CPUS    MEMORY    DISK      DIRdocker     Running    127.0.0.1:65015    qemu      aarch64    4       4GiB      100GiB    ~/.lima/dockerDocker CLI のインストールLima がインストールされた後、Docker コマンドラインインターフェース (CLI) をインストールする必要があります。以下のコマンドを使用して、macOS 用の Docker CLI をダウンロードし、インストールします。今回はdocker-25.0.4.tgzをダウンロードしますがこちらを参考に最新版をinstallしてください。# Docker CLI バイナリのダウンロード:# macOS 用の Docker CLI バイナリをダウンロードします。curl -L -O https://download.docker.com/mac/static/stable/aarch64/docker-25.0.4.tgz# ダウンロードしたアーカイブの展開:# ダウンロードした tar.gz アーカイブを展開します。# 毎回、調べているので悲しいtar -xvzf docker-25.0.4.tgz# Docker CLI の移動:# 展開した Docker CLI をシステムの PATH の一部である /usr/local/bin に移動します。パスはどこでもいいけどブログなので...。mv docker/docker /usr/local/bin/参照: macOS でのクライアントバイナリのインストール修正: brew install でのdockerのインストール勝手に--caskとか付けて全部入るなーって思っていたのですがbrew install dockerのみの場合にはDocker CLIのみをインストールすることができますbrew install docker我らがteraoka 師匠から教えていただきました。参照: Lima で vz + rosetta を使って ARM VM 上で x86_64 バイナリを実行する #Docker - QiitaLima-Docker の設定Lima と Docker CLI がセットアップされたら、Lima ベースの Docker 環境を利用するための設定を行います。以下のコマンドで Docker コンテキストを作成し、利用を開始します。# Docker コンテキストの作成:# `lima-docker` という名前の Docker コンテキストを作成し、Lima インスタンス上の Docker デーモンに接続します。docker context create lima-docker --docker "host=unix:///Users/<username>/.lima/docker/sock/docker.sock"# 作成した Docker コンテキストの使用:# `lima-docker` コンテキストをアクティブにして、以降の `docker` コマンドが Lima インスタンスを対象に実行されるようにします。docker context use lima-docker# Docker コンテナの実行 (テスト):# Docker 環境が正しく設定されているかを確認するため、hello-world イメージを実行します。docker run hello-world# Docker での nginx コンテナの実行:# nginx イメージをバックグラウンドで実行し、ポート 8181 をコンテナのポート 80 にフォワードします。docker run --name lima-test-nginx -d -p 8181:80 nginxこれらのステップを完了することで、Lima 上で Docker コンテナを実行し、管理することができるようになります。おまけ:KIND (Kubernetes IN Docker) の利用KIND (Kubernetes IN Docker) は、Docker 上に軽量な Kubernetes クラスタを構築するためのツールです。Lima 環境上で Docker を利用している場合でも、KIND を使用して Kubernetes のテスト環境を簡単にセットアップできます。# KIND クラスタの作成:# 新しい Kubernetes クラスタを作成します。このクラスタは Docker コンテナ内に構築されます。$ kind create cluster# 作成されたクラスタの一覧表示:# 現在 KIND によって作成されたクラスタの一覧を表示します。$ kind get clusterskind# クラスタ情報の取得:# 作成した KIND クラスタのコントロールプレーンやサービスの情報を取得します。$ kubectl cluster-info --context kind-kindKubernetes control plane is running at https://127.0.0.1:51050CoreDNS is running at https://127.0.0.1:51050/api/v1/namespaces/kube-system/services/kube-dns:dns/proxyTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.# 全てのネームスペースで動作している Pod の一覧表示:# クラスタ内の全ネームスペースにわたる Pod の状態を確認します。$ kubectl get pod -ANAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGEkube-system          coredns-76f75df574-5glm8                     1/1     Running   0          87skube-system          coredns-76f75df574-jwn6z                     1/1     Running   0          87skube-system          etcd-kind-control-plane                      1/1     Running   0          103skube-system          kindnet-qlftc                                1/1     Running   0          86skube-system          kube-apiserver-kind-control-plane            1/1     Running   0          102skube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          100skube-system          kube-proxy-6nwnv                             1/1     Running   0          86skube-system          kube-scheduler-kind-control-plane            1/1     Running   0          100slocal-path-storage   local-path-provisioner-7577fdbbfb-vd28d      1/1     Running   0          87sおわり。結論Docker Desktop のアンインストールと Lima の導入に焦点を当てました。本記事で紹介した手順を通じて、開発環境を効率的に管理し、Docker コンテナの実行を最適化することが可能です。参考文献Docker Desktop アンインストール方法macOS でのクライアントバイナリのインストール]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[『読書とは、能力、知識ではなく 問いを獲得するための行為』みたいな内容で登壇しました。]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/03/13/164951</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/03/13/164951</guid>
            <pubDate>Wed, 13 Mar 2024 07:49:51 GMT</pubDate>
            <content:encoded><![CDATA[問題を解決する能力は確かに重要ですが、それ以上に、何が本当に重要な問題なのかを見極め、それを明確に設定する能力が不可欠です。問いを適切に定義できなければ、どんなに高度な解決技術を持っていても、その力は十分に発揮されません。また、誰にとって適切な問いなのかも考える必要があります。問題解決の過程において、問題そのものの本質を正確に把握し、適切な問いを立てることは重要です。イシューからはじめよ――知的生産の「シンプルな本質」作者:安宅和人英治出版Amazon概要SREたちの廊下〜あなたの現場での悩み、あの本にヒントがあるかも〜にて「書を捨てよ、現場へ出よう - このSRE本がすごい！2024年 LT版」 というテーマで登壇しました。のイベントは2024年1月末に注目を集めた『このSRE本がすごい！2024年版』をテーマにしたもので、多くの参加者とパネルディスカッションのスピーカーであるTopotal のnari_exさん、kenta_hiさんと共に、その内容を深掘りして議論することができ、イベントも無事成功し、大変充実した時間を過ごすことができました。findy.connpass.comイベントを引き起こしたきっかけとなったツイートは以下のものです。この経験から、積極的に意見を発信することの大切さを実感しました。時には思いがけない展開をもたらすこともあるなぁって思いました(小並)。モチベーションになるのでブログの読者登録おねがいします。強い下心を持っているため、Findyさんなどからこれらの本に関する解説をする勉強会の依頼が来ることを期待しています。 https://t.co/amL2de5qFI— nwiizo (@nwiizo) 2024年1月31日   資料この資料は思いの外、感情的な要素が強くエンジニアなのに技術の話を全くしないポエムっぽさが反映されてしまいました。当初は技術的な内容と本の紹介を避ける方針でしたが、認知科学のような専門分野に深く踏み込む知識は持ち合わせていないため、このような方向性になってしまいました。それにもかかわらず、受け取り手からはそこそこに好評を得られたことが非常に嬉しく思います。 speakerdeck.comXでのポストはこちらです。内省の話運用技術者組織の設計と運用 / Design and operation of operational engineer organizationを読んで勝手に憧れていたnari_exさんとのイベントでそのnari_exさんから内省の大切さの話が出てきていた。明日の朝から読んでみようと思う。リフレクション（REFLECTION） 自分とチームの成長を加速させる内省の技術 (オリジナルフレームワークPPT・PDF特典付き)作者:熊平美香ディスカヴァー・トゥエンティワンAmazon読書とは、能力、知識ではなく 問いを獲得するための行為資料を作る前のアウトラインと文章をブログでも公開しておきます。このような内容が気になった方は参考文献を読んでいただければと思います。能力のイメージ能力の抽象性と具体化の必要性日常生活において、私たちは「コミュニケーション能力」、「問題解決能力」、「技術力」などの様々な「能力」について語ります。これらは教育や仕事、プライベートにわたって使われますが、深く考えると、これらの「能力」が具体的に指すものは何か、どう解釈すべきか疑問が生じます。能力に関する理解を深めるには、背後にある原因や要素、その行動や成果への影響を分析することが不可欠です。能力という概念は抽象的であるがゆえに、その実態を把握するには具体的な文脈における観察と分析が欠かせません。能力解釈におけるメタファーの限界と可能性能力の解釈は、しばしばメタファーを通じて行われます。「力」という言葉自体が、物理的な力や潜在的な特性を想起させます。しかし、これらのメタファーは、能力が一貫して同じ効果をもたらすという誤解を生むことがあります。例えば、「コミュニケーション能力」を「言葉の力」と表現することで、言葉さえ巧みに使えば常に良好なコミュニケーションが取れるという誤った印象を与えかねません。能力についての理解を深めるには、メタファーが示すイメージを超えて、実際の文脈での能力の現れ方を丁寧に探ることが重要です。メタファーは理解の出発点としては有用ですが、それに留まらず、具体的な事例や経験から能力の本質を捉えていく必要があります。能力は文脈依存で時と場合次第能力の文脈依存性人間の能力は、状況に応じて異なる形で表れます。ある特定の文脈において顕著な能力が発揮される一方で、他の状況ではまったく異なる影響を持つかもしれません。例えば、プレゼンテーションの場で優れたコミュニケーション能力を発揮する人物が、親密な人間関係の中では十分にその能力を活かせないということもあり得ます。この文脈依存性は、能力が単純な属性ではなく、状況や環境、それに伴う要求に対する応答の結果として理解すべきであることを示唆しています。つまり、能力とは、特定の文脈において、その状況に適した行動を取ることができる力なのです。文脈に応じた問いの形成問いは、私たちが直面する特定の文脈における能力の発揮や理解を深めるのに重要な役割を果たします。そのため、問いは文脈に応じて形成される必要があります。適切な問いを立てることで、その状況における最適な行動や能力の発展につながります。例えば、プレゼンテーションの場面では、「どのようにすれば聴衆の関心を引き付けられるか」、「効果的な情報伝達のために何が必要か」といった問いが重要になります。一方、親密な人間関係の中では、「相手の感情を適切に理解するにはどうすればよいか」、「信頼関係を築くために何ができるか」といった問いが求められます。能力を最大限に活かすためには、その能力をどのように、いつ、どのような状況で使うべきかを考える問いが不可欠なのです。知識の非伝達性と構成主義知識の非伝達性多くの人々は、知識や技能が他者から伝達できるものだと考えがちです。しかし、実際には、知識は伝達されるのではなく、各個人が自身の経験や環境から創発するものなのです。教育や読書を通じて提供されるのは情報のみであり、それを個人が内面化し、自らの知識として再構築するプロセスが必要不可欠です。つまり、知識は受け取るものではなく、自ら作り上げていくものなのです。この視点は、知識獲得を受動的な受け入れではなく、能動的な創造過程として捉えるべきであることを示唆しています。知識の構成主義知識は個人の認知的リソースと環境から提供される情報を結合させて創発されます。このプロセスでは、経験や環境からの情報を基に、個人が能動的に知識を構築します。構成主義の視点から、知識は静的なものではなく、個々の経験や文脈に応じて動的に形成されると捉えられます。これは、知識を単に受け入れるだけでなく、自分自身の行動や内省を通じて深める過程です。知識の構成主義は、学習者の能動性と主体性を重視し、知識の個人的な意味づけを重要視する立場だと言えます。知識の応用と実践道を知っていることと実際に歩くことは違う理論から実践への移行は知識の本質的な価値を明らかにします。教室や書籍で得た知識が、実際の体験や応用を経て深化し、真に生きた知識へと変わります。このプロセスは、抽象的な概念を具体的な行動や体験に結び付け、それによって得られる新しい理解や洞察がさらなる学びのモチベーションを高めます。知識から行動への変換知識を実際の行動に転換することは、それを社会や日常生活に応用し、問題解決や創造的な活動に活かすプロセスです。この実践を通じて、知識は単なる情報の蓄積を超え、個人の体験と統合され、生きたものへと変化します。実践から得られる新たな体験は、知識の内面化を促し、持続可能な知的成長の重要な要因となります。知識と行動の相互作用は、知的な営みの本質であり、知識の実践的な価値を示すものだと言えるでしょう。プログラミング言語の文法や設計パターンを学んだだけでは、実際のソフトウェア開発で成功することは難しいでしょう。理論を実践に活かし、試行錯誤を重ねることで、本当に生きたプログラミングスキルが身につくのです。知っているだけでは不十分で、実際にコードを書き、動かしてみて、時間が経って発生する問題を観測することが重要なのです。読書は、見えなかったものを見えるようにすること問いの形成と知識の活用適切な問いを立てることは、文脈に依存する能力の理解と、個々に構成される知識の活用を促進します。問いは、特定の状況で何が必要であり、どのように行動すべきかを明らかにし、その過程で深い知識の構築と適用が可能になります。読書は、私たちの内面に新たな問いを生み出し、その問いを深めるための知識を提供してくれます。読書と問いの形成は、知識の活用と探究を促す相補的な営みなのです。問いに基づく学習の進展問いは、学習過程において重要な役割を果たします。それにより、私たちは受動的な知識の受け手から、能動的な学習者へと変化し、知識をより深く、文脈に応じて理解し、活用する能力を高めます。これは、個人の成長と発展にとって不可欠なプロセスです。読書は、問いを生み出し、その問いに答えるための知識を与えてくれる営みです。読書と問いに基づく学習は、知的な探究心を育み、生涯にわたる学びの基盤となるのです。問いを深める読書と知的好奇心の拡大読書は、単なる知識の蓄積以上に、私たちの内面的な問いを掘り下げ、それらを広げる活動です。異なる分野や視点からの本を読むことで、従来の枠組みを超えた新しい問いが生まれ、これが知的好奇心を刺激し、さらなる探究へと促します。こうしたプロセスは、私たちの知的な地平を拡げ、より複雑な問題に対する洞察力を高めます。読書の継続と習慣化読書を継続的に行うことは、知識の深化と問いの発展に不可欠です。習慣としての読書は、長期的に見て自己成長を促し、知識をより深く理解し活用する能力を養います。習慣化による読書は、日々の小さな努力を積み重ねることで、大きな学びへと繋がる基礎を築きます。読書習慣は、知的な探究心を持続させ、生涯学習の基盤を形成する上で欠かせない要素だと言えるでしょう。さいごに能力と知識と実践の相互関係能力と知識は、読書を通じて理解し、実践に活かすことができます。読書は、能力の文脈依存性と知識の非伝達性に光を当て、私たちを新たな理解へと導きます。実践を通じて得られる経験は、学んだことを確かなものにし、問いを通じてさらに深い洞察を得ることができます。能力と知識、そして実践は、相互に影響を与え合い、螺旋状に発展していくのです。これらの要素の有機的な結びつきが、私たちの知的成長を支える基盤となります。知識を深めるための継続の意義継続は、知識を蓄積し、それを活用するうえでの基礎を築きます。読書の習慣は、日々の積み重ねによって、知識を内面化し、問いを深め、思考を拡張する重要なプロセスです。知識を深め、問いを追求し続けることで、私たちは自己の成長と進化を遂げることができます。継続的な読書と学びは、私たちを知的な探究者へと導く、生涯にわたる営みなのです。それは、私たちの内なる知的世界を豊かにし、より深い理解と洞察へと導いてくれるでしょう。参考資料学びとは何か――〈探究人〉になるために (岩波新書) 言語の本質-ことばはどう生まれ、進化したかジェームズ・クリアー式 複利で伸びる1つの習慣私たちはどう学んでいるのか　─創発から見る認知の変化達人プログラマー(第2版): 熟達に向けたあなたの旅プログラマー脳 ～優れたプログラマーになるための認知科学に基づくアプローチ]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[書を捨てよ、現場へ出よう]]></title>
            <link>https://speakerdeck.com/nwiizo/shu-woshe-teyo-xian-chang-hechu-you</link>
            <guid>https://speakerdeck.com/nwiizo/shu-woshe-teyo-xian-chang-hechu-you</guid>
            <pubDate>Tue, 12 Mar 2024 04:00:00 GMT</pubDate>
            <content:encoded><![CDATA[書を捨てよ、現場へ出よう このSRE本がすごい！2024年 LT版というタイトルで登壇してきました。SREたちの廊下〜あなたの現場での悩み、あの本にヒントがあるかも〜https://findy.connpass.com/event/311323/元ブログはこちらこのSRE本がすごい！2024年版https://syu-m-5151.hatenablog.com/entry/2024/01/26/165255登壇ブログはこちら『読書とは、能力、知識ではなく 問いを獲得するための行為』みたいな内容で登壇しました。https://syu-m-5151.hatenablog.com/entry/2024/03/13/164951]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud Modern App Summit Tokyo'24 に登壇しました！ ~オススメのセッションを添えて~]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/488f1aa442444f</link>
            <guid>https://zenn.dev/yokoo_an209/articles/488f1aa442444f</guid>
            <pubDate>Tue, 12 Mar 2024 00:13:04 GMT</pubDate>
            <content:encoded><![CDATA[はじめに3/1（金）に Google Cloud Modern App Summit Tokyo’24 が開催されましたhttps://cloudonair.withgoogle.com/events/modern-app-summit-24#私はスポンサーセッションのSpeakerとして参加しましたが、他のセッションもすごくおもしろいものが多かったのでゆる〜く内容も合わせてご紹介しますこの方の参加レポートがすごく良いのでぜひ！https://iret.media/94801 登壇内容私は、以下の内容で登壇しました。これだけ規模の大きいイベントに登壇するのは初め...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OSC(Open Source Conference) 2024 Online/Springでセミナー発表とTokyo/Springでブース出展]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/03/10/225747</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/03/10/225747</guid>
            <pubDate>Sun, 10 Mar 2024 13:57:47 GMT</pubDate>
            <content:encoded><![CDATA[2024年3月2日(土)、Open Source Conference 2024 Online/Spring でセミナー発表し、event.ospn.jp同年3月10日(日)、Open Source Conference 2024 Tokyo/Spring でブース出展しました。event.ospn.jpOnline/Springのセミナー（以下、オンラインセミナー）では、日本生成AIユーザ会として、生成AI入門というテーマで発表させていただきました。日本生成AIユーザ会のデビュー戦でした。これまで日本暗号通貨ユーザ会などでブロックチェーン関連の発表はたくさんしてきましたが、生成AIの発表は初めて。無事発表できてよかったです。 speakerdeck.comyoutu.beなお、オンラインセミナーでは日本暗号通貨ユーザ会の発表は今回しておりません。日本生成AIユーザ会と日本暗号通貨ユーザ会の2つの発表をやることはしんどいので、1つだけにしました。一方、3月10日OSC東京はブース展示会でした。会場は東京都立産業貿易センター台東館で、最寄りは浅草駅。天気もよく、浅草駅前の吾妻橋交差点から見える東京スカイツリーとアサヒビール本社屋上の金色のオブジェが綺麗に見えました！浅草からの風景↓金色のオブジェは聖火台の炎を表現していたのですね！今知りましたwwww.asahibeer.co.jpブース展示会では、日本生成AIユーザ会と日本暗号通貨ユーザ会の合同出展という形にしました。出展しております！#osc24tk#genai_users#ccstudy pic.twitter.com/O0vcEGWkd0— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月10日   ブースにはたくさんの方にきていただき、生成AI、暗号通貨(ブロックチェーン)ともに興味を持っていただけました。OSC東京の休憩で、坦々麺 一龍 浅草本店へ。昨年春の浅草でのOSC東京にてここで食べて美味しかったので、再来店。かなりの細麺、正方形の器になり、進化してました。美味しかった！#osc24tk#genai_users#ccstudy pic.twitter.com/wvscqgtHHj— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月10日   日本生成AIユーザ会と日本暗号通貨ユーザ会のブースに、リナックくんが遊びに来たで！#Linuc#リナックくん#osc24tk#genai_users#ccstudy pic.twitter.com/R9CLB7FG4g— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月10日   皆様お疲れ様でした！運営の皆様、出展させていただきありがとうございました。ブースに来ていただいた皆様ありがとうございました。これまでOSCのブース出展は、日本暗号通貨ユーザ会だけで行ってきましたが、今回、日本生成AIユーザ会でも出展することで新たな気付きもたくさん得られました。秋のOSC東京ではもっとブースをパワーアップして挑みたいと思います！日本暗号通貨ユーザ会は次回、3月22日(金)20:00〜「ビットコイン・ブロックチェーン入門」という講義形式(※)のオンライン勉強会を開催します。※講義形式というのは、ハンズオンや輪読ではないということです。発表者が説明して、質問を受け付けます。当てたりしませんので、ご安心をw！cryptocurrency.connpass.com日本生成AIユーザ会は次回、4月5日(金)20:00〜「生成AIアプリケーション開発入門ハンズオン」というハンズオン形式のオンライン勉強会を開催します。ハンズオンですが、難しくなく皆さんがついてこられて、かつ楽しい内容にします！genai-users.connpass.comどちらもぜひご参加ください！これからも両コミュニティをよろしくお願いいたしますm(._.)m]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloud 主催 Generative AI Summit Tokyo '24に参加]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/03/08/095543</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/03/08/095543</guid>
            <pubDate>Fri, 08 Mar 2024 00:55:43 GMT</pubDate>
            <content:encoded><![CDATA[2024年3月7日、GOOGLE CLOUD 主催 Generative AI Summit Tokyo '24 に参加しました！cloudonair.withgoogle.com株式会社スリーシェイクはスポンサーを務めておりました。基調講演を除き、観たセッションはXにてスレッドで軽く実況を書いています。Google Cloud Generative AI Summit「生成 AI 時代に Google Kubernetes Engine は何を解決してくれるのか」を聞きます！https://t.co/c0HW6f9xVP#gc_genai— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   Google Cloud Generative AI Summit「Chugai DX × GenAI の裏側 : 製薬業界の IT エンジニアが挑む次世代の業務最適化」https://t.co/fbqXCTorEq#gc_genai  #track1— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   弊社の発表！「SRE@scale: 生成 AI で実現するスリーシェイクが考える拡張性のある SRE の取り組み」https://t.co/lv5Rd15qLb#gc_genai #track4— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   Google Cloud Generative AI Summit Tokyo '24住友ゴム工業株式会社さん「製造業における生成 AI を使った業務効率化への取り組み」https://t.co/XGSFieexyF#gc_genai #track1— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   Google Cloud Generative AI Summit Tokyo '24株式会社トップゲートさん「文書管理の未来：Document AI と Vertex Search を利用した紙文書管理」https://t.co/SIHbwTcJPb#gc_genai #track3— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   Google Cloud Generative AI Summit Tokyo '24株式会社サテライトオフィスさん「2000 社導入、生成 AI 活用最前線　サテライト AI のご紹介」https://t.co/brrAlihedB#gc_genai #track3— Shu Kobuchi(こぶシュー) (@shu_kob) 2024年3月7日   全体を通しての感想は、キラーアプリはまだ出てきていないものの、早期に生成AIを触り始めた企業さんは、事例をどんどん作っていると言う印象でした。私も生成AIに携わるものとして、モチベーションが上がったので、日々の業務に邁進していきたいと思います！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[君たちはどう記事を管理しているか]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/c0ccd3bd241ad6</link>
            <guid>https://zenn.dev/yokoo_an209/articles/c0ccd3bd241ad6</guid>
            <pubDate>Thu, 07 Mar 2024 13:49:49 GMT</pubDate>
            <content:encoded><![CDATA[はじめにみなさんは良いなと思った記事や保存しておきたいなと思った記事をどのように溜めていますか？また、どのように引き出していますか？身近に「こんな資料・記事あったな、参考になると思うのでどうぞ 」で ｼｭﾂ と出してくる猛者エンジニアの方がいたりしますよね...ここでは、頭のストレージ容量が皆無な私が、そんなことできる人になりてぇなぁ〜と思いながら、考えた方法をご紹介します。ブラウザのブックマークや、Qiita・Zennに搭載のネイティブのストック機能、Slackの自分だけのチャンネルに貼る、様々な方法があると思います。もしくは、特に溜めていなく引き出したいときに思い出せ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[生成AI入門]]></title>
            <link>https://speakerdeck.com/shukob/sheng-cheng-airu-men</link>
            <guid>https://speakerdeck.com/shukob/sheng-cheng-airu-men</guid>
            <pubDate>Sat, 02 Mar 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[今話題の生成AIについて簡単に技術概要をお話ししたのち、LangChain、プロンプトエンジニアリング、RAG（Retrieval Augmented Generation）、Embedding、グラウンディングなどを実装の手法などを紹介しました。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[酸いも甘いもある Shared VPC（共有VPC） ~ GKE をShared VPC で構築する際の苦悩~]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/1818360b9c7821</link>
            <guid>https://zenn.dev/yokoo_an209/articles/1818360b9c7821</guid>
            <pubDate>Thu, 29 Feb 2024 07:23:09 GMT</pubDate>
            <content:encoded><![CDATA[このブログは、【Google Cloud】GDG Tokyo Monthly Online Tech Talksにて発表した内容を元にしています。登壇時の資料はこちらになります。https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao はじめにGoogle Cloud Shared VPC（共有VPC）はネットワークリソースの集中管理ができる一方で、リソース利用に関して制約があったり、エンプラ企業...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[迅速に叶える、GKE Autopilot によるユニバーサルモダンアーキテクチャの実践]]></title>
            <link>https://speakerdeck.com/parupappa2929/xun-su-nixie-eru-gke-autopilotniyoruyunibasarumodanakitekutiyanoshi-jian</link>
            <guid>https://speakerdeck.com/parupappa2929/xun-su-nixie-eru-gke-autopilotniyoruyunibasarumodanakitekutiyanoshi-jian</guid>
            <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Google Cloud Modern App Summit Tokyo '24 にて登壇した内容です。https://cloudonair.withgoogle.com/events/modern-app-summit-24?talk=session-c1]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Astroでブログを作ってる]]></title>
            <link>https://abnoumaru.com/tech/2024-02-27-create-blog-from-astro-template/</link>
            <guid>https://abnoumaru.com/tech/2024-02-27-create-blog-from-astro-template/</guid>
            <pubDate>Tue, 27 Feb 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AstroのBlog templateをベースに自分が欲しい機能を検討してブログ作ってる]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Azure Container Apps Jobs を Self-hosted GitHub Actions Runner として使う]]></title>
            <link>https://blog.1q77.com/2024/02/container-apps-jobs-self-hosted-github-actions-runner/</link>
            <guid>https://blog.1q77.com/2024/02/container-apps-jobs-self-hosted-github-actions-runner/</guid>
            <pubDate>Fri, 23 Feb 2024 10:05:41 GMT</pubDate>
            <content:encoded><![CDATA[GitHub Actions の Self-hosted Runner を安く用意する方法を探していたところ、 Azure の Container Apps Jobs というのが便利に使えるらしいというのを見つけたので試してみる。 チュートリアル:Az]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[酸いも甘いもある Shared VPC（共有VPC） ~ GKE を Shared VPC で構築する際の苦悩 ~]]></title>
            <link>https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao</link>
            <guid>https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao</guid>
            <pubDate>Thu, 22 Feb 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Google Cloud Digital Leader合格！]]></title>
            <link>https://shu-kob.hateblo.jp/entry/2024/02/21/225145</link>
            <guid>https://shu-kob.hateblo.jp/entry/2024/02/21/225145</guid>
            <pubDate>Wed, 21 Feb 2024 13:51:45 GMT</pubDate>
            <content:encoded><![CDATA[Google Cloud Platdorm(GCP)認定資格の第一歩、Cloud Digital Leaderに合格しました！ちなみに、これまでの私の保有IT資格は、基本情報、応用情報、AWSクラウドプラクティショナーです。AWS資格の第一歩、クラウドプラクティショナーはUdemyの模擬試験を1周したら合格できたので、同様にGoogle Cloud Digital LeaderもUdemyの模擬試験を1周以上したら合格できました。なお、AWSは最初、ソリューションアーキテクトアソシエイト受けたものの勉強不足で落ち、クラウドプラクティショナーならなんとかなるのでは？と思い、その2週間後に受けたら受かりました。Udemyの模擬試験は途中保存もでき、採点、不正解のみ見直しなどができて便利ですよね。次はGCPのAssociate Cloud Engineerに挑戦したいと思います！皆さんもUdemyで勉強してIT資格をとりましょう！AWSやGCPの資格をとるならUdemyで学ぼう！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform workspace使って思ったこと]]></title>
            <link>https://sreake.com/blog/terraform-workspace/</link>
            <guid>https://sreake.com/blog/terraform-workspace/</guid>
            <pubDate>Sun, 18 Feb 2024 14:28:59 GMT</pubDate>
            <content:encoded><![CDATA[背景 そこまで大きな案件でもなく、 環境間の差分もあまりなさそうだったため 何より使ったことないから試してみようっていう好奇心 ある案件にて上記の理由から、Terraform workspaceを採用しました。 今回は、 […]The post Terraform workspace使って思ったこと first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[個人開発でWebアプリの開発とデプロイの流れ]]></title>
            <link>https://kechigon.hatenablog.com/entry/2024/02/13/125853</link>
            <guid>https://kechigon.hatenablog.com/entry/2024/02/13/125853</guid>
            <pubDate>Tue, 13 Feb 2024 03:58:53 GMT</pubDate>
            <content:encoded><![CDATA[個人でWebサービスを開発したいけど、どのような流れで作っていけばいいのかわからない方向けです。個人開発でWebアプリを開発、デプロイをしたのでその流れを共有したいと思います。作ったもの麻雀戦績管理アプリ名付けて「PungPals」。雀荘などのオフラインでの対戦結果を残し、個人成績やランキングを確認できます。pungpals-service-xstpolfd4q-an.a.run.app開発とデプロイの流れ1.要件定義、設計実装がスムーズに進むために、しっかりとしておきましょう。以前記事を書いたので、参考にしてください。kechigon.hatenablog.com2.技術選定今回作ったアプリケーションはDjangoで開発し、Cloud Runにデプロイしています。選定理由は、Django: 経験があるから。Cloud Run: Djangoアプリのデプロイ方法の公式ドキュメントがあった(後ほど説明します)、マネージドな部分とカスタムできる部分のバランスがちょうどよかったから。でした。以下これらの技術を使って、開発デプロイまでの流れを説明していきます。3.Djangoを使ってアプリケーションを作成Djangoにはチュートリアルがあり、はじめての Django アプリ作成、その 1 | Django ドキュメント | Djangoはじめての Django アプリ作成、その2 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 3 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 4 | Django ドキュメント | Djangoを読めば開発方法がわかると思います。環境構築をし、実装し、ローカルで動作確認をしながら開発していきます。4.Cloud run へのデプロイDjangoアプリのCloud runへのデプロイ方法は公式ドキュメントにまとめられているので、これを見ながら進めます。cloud.google.comDjangoアプリケーションを環境に合わせて設定した後コンテナ化し、Cloud Runに載せます。それに伴い、Cloud SQL(データベース)、Secret Manager(シークレット管理)、Cloud Storage(静的アセットの保存など)、Cloud Build(CI/CD)、Artifact Registry(コンテナレジストリ)の作成、設定も行います。ドキュメントではGCRを使っていますが、現在非推奨なので、Artifact Registryをコンテナレジストリとして使用します。cloud.google.comオプションですが、GCPへのリソースの作成はTerraformを利用すると、構成管理ができ便利です。作成するインフラの図以上のことを行った後のGitHubリポジトリPungPalsのコードは公開しているので、参考にしていただければと思います。github.comこれから今後は、運用面の課題解決や集客などを行っていく予定なので、ブログにしていくつもりです！]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudの管理を楽にする、 トイルを減らすクラウドガバナンス]]></title>
            <link>https://speakerdeck.com/parupappa2929/google-cloudnoguan-li-wole-nisuru-toiruwojian-rasukuraudogabanansu</link>
            <guid>https://speakerdeck.com/parupappa2929/google-cloudnoguan-li-wole-nisuru-toiruwojian-rasukuraudogabanansu</guid>
            <pubDate>Sun, 11 Feb 2024 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Tech Blogが生えました]]></title>
            <link>https://abnoumaru.com/tech/2024-02-10-start-tech-blog/</link>
            <guid>https://abnoumaru.com/tech/2024-02-10-start-tech-blog/</guid>
            <pubDate>Sat, 10 Feb 2024 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Astroのほぼテンプレで作ったブログにTech Blogを生やした]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私のメンターがくれた初めてのターミナル管理、それはtmuxで私は新卒でした。]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2024/02/06/110341</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2024/02/06/110341</guid>
            <pubDate>Tue, 06 Feb 2024 02:03:41 GMT</pubDate>
            <content:encoded><![CDATA[はじめに2024年2月5日夜の東京 外は雪が降っている。tmuxとの出会いは、新卒としての初めての職場でした。メンターがターミナルの管理において最初に紹介してくれたのがtmuxで、この出会いが私の開発効率と作業環境を大きく変革しました。tmuxの基礎を学んだ後、私は自分の開発環境をさらにカスタマイズし、tmuxを日々の作業効率化のために積極的に使い始めました。ここでは、私が実際に使っているtmuxの設定と、日常的に使うコマンドを紹介します。これらは、より快適なターミナル操作環境を実現するために役立ちます。この過程で、tmuxはただのツール以上のものになり、私の開発作業における最適なパートナーになりました。tmuxを使いこなすことで、複数のプロジェクトを同時に管理する能力が向上し、長時間の作業も中断せずに続けられるようになりました。リモートワークが増えた今では、tmuxのセッション管理機能が特に役立っています。サーバーに接続した状態で作業を行い、一時的に他のタスクに切り替えても、再びtmuxセッションに戻れば瞬時に作業を再開できます。tmuxを通じて、私はターミナル操作に関してプロフェッショナルな開発者としての成長していると実感しています。あとやっている感がとても出ているので好きです。tmuxとはtmuxは「ターミナルマルチプレクサ（Terminal Multiplexer）」の略称で、Linux系OSを中心に利用されています。このツールを使うと、一つのターミナルウィンドウ内で複数のセッション、ウィンドウ、ペインを効率的に管理することが可能になります。github.comセッションの管理： 一つのターミナルで複数のセッションを持ち、それぞれ独立した作業スペースとして利用できます。仕事とプライベート、複数のプロジェクト間でセッションを分けることができるため、タスクの切り替えがスムーズになります。ウィンドウとペイン： 一つのセッション内で、複数のウィンドウを開くことができ、さらにウィンドウをペインと呼ばれる小分けにすることが可能です。これにより、同一画面内で複数の作業を並行して行うことができ、効率的なマルチタスクが実現します。セッションの維持： tmuxの最大の特徴の一つは、ターミナルを終了してもセッションが維持されることです。これにより、長時間かかるコマンドを実行中にログアウトしてしまったり、接続が切れてしまったりしても、作業が中断される心配がありません。tmux設定のカスタマイズ私の.tmux.confファイルには、効率的なターミナル操作を可能にするための様々なカスタマイズが施されています。これらの設定を通じて、tmuxを自分にとって最適な作業環境に変えることができました。github.comプレフィックスキーの変更: デフォルト設定のCtrl+bをCtrl+qに変更しました。これは、より操作性の良いキーバインドに変更することで、他のショートカットキーとの競合を避け、操作のスムーズさを向上させるためです。キーバインドのカスタマイズ: vimを頻繁に使用することから、ペインの移動やリサイズをvim風に設定しています。これにより、キーボード操作の一貫性を保ちながら、直感的で迅速なウィンドウ管理を実現しています。ペインの分割: よく使用する|キーでペインを縦に分割し、-キーでペインを横に分割するように設定しました。これにより、柔軟かつ迅速に作業スペースをカスタマイズすることが可能になります。ステータスバーのカスタマイズ: ステータスバーには、現在のセッションの状態や時刻など、必要な情報を表示するよう設定しています。これにより、作業中に一目で状況を確認できるようになり、生産性の向上に貢献しています。プラグインの利用: tmux-resurrectやtmux-continuumなどのプラグインを導入しています。これらのプラグインは、セッションの自動保存や復元を可能にし、長時間にわたる作業や一時的な中断からのスムーズな再開を支援します。セッションの保存と復元は、プレフィックスキーに続けてCtrl+sで保存、Ctrl+rで復元することができます。これにより、突然のシステム停止や作業の中断が発生しても、簡単に前の状態に戻ることができます。さらに、tmuxのプラグインエコシステムは非常に豊富で、tmux-pluginsのリストからは、あらゆるニーズに応える特別なプラグインを見つけることができます。自分の作業フローに合わせて、最適なプラグインを選択し、tmux環境をさらにパワフルで柔軟なものにカスタマイズすることが可能です。よく利用するtmuxコマンドtmuxを効率的に使用するためには、日常的に役立つコマンドを知っておくことが重要です。ここでは、特に重宝するコマンドを紹介します。どんな時にでも味方になってくれるチートシートです。ちなみにチートシートには入れてないのですがprefix + e で全てのペインの操作、prefix + Eでそれらの解除などもインフラエンジニアとしては非常に重宝します。github.com以下は、日々の作業で特に役立つコマンドです。新規セッションの開始: tmux new -s <セッション名>コマンドで、特定の名前を持つ新規セッションを開始します。この機能を活用することで、プロジェクトやタスクごとにセッションを分け、作業を整理しやすくなります。セッションの一覧表示と切り替え: tmux lsコマンドで現在のセッション一覧を表示し、tmux attach -t <セッション名>またはtmux a -t <セッション名>で特定のセッションにアタッチします。これにより、複数のプロジェクトや作業を効率的に管理し、スムーズに切り替えることができます。ペインとウインドウの操作: tmuxでは、ペインの分割やウインドウの作成、移動、リサイズを柔軟に行うことができます。これらの操作をカスタマイズしたキーバインドで行うことで、必要に応じて作業スペースを自由に調整し、マルチタスク作業を効率的に進めることができます。マウス操作の有効化: set-option -g mouse onコマンドにより、tmux内でのマウス操作を有効にすることができます。マウスでペインを選択したり、サイズを調整したりすることが可能になり、キーボードとマウスを組み合わせた直感的な操作が実現します。これらのコマンドは、tmuxを使ってターミナル操作を効率化し、生産性を高めるための基本となります。tmuxをより深く理解し、活用することで、開発作業をより快適に、より効率的に行うことができるでしょう。さいごにtmuxとの出会いは、私の開発効率と作業環境を大きく変えただけでなく、インフラエンジニアとしての成長にも大きく寄与しました。日々の作業でtmuxを使いこなすことで、システムの監視、ログの分析、複数のサーバーへの同時操作など、インフラ管理の幅広いタスクを効率的にこなすスキルを身につけることができました。また、セッションの持続性は、長時間実行するプロセスの管理や、中断された作業の再開といった面で、インフラ作業の品質を向上させるのに役立ちました。tmuxのカスタマイズ性と拡張性を活かして、個人の作業環境を最適化することは、単に作業を効率化するだけではなく、技術者としての視野を広げ、問題解決能力を養う機会となりました。tmuxを深く理解し活用することで、インフラエンジニアリングの知識を実践的に拡張し、より複雑なシステムと効果的に向き合う力を養うことができました。tmux は3.4 がリリースされており今でも進化を続けている。愛している。github.comtmuxは、単なるツール以上の存在となり、私の技術的な成長を支えてくれる貴重なパートナーです。これからもtmuxを活用し続けることでしょう。しかし、人は変わる。実はZellijやターミナルもMAC標準なものを利用しているがAlacrittyが気になっているので検証と導入を進めている。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OWASP Top 10 for Large Language Model Applications をまとめる]]></title>
            <link>https://sreake.com/blog/owasp-top-10-for-llm-application/</link>
            <guid>https://sreake.com/blog/owasp-top-10-for-llm-application/</guid>
            <pubDate>Mon, 05 Feb 2024 09:29:32 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake 事業部インターン生の中林です。私は、Sreake 事業部長期インターン生として SRE 技術の技術検証を行っています。 今回は、Sreake 事業部で作成している LLM アプリケーションに対する […]The post OWASP Top 10 for Large Language Model Applications をまとめる first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDB ManagedのAlways Free枠を試そう]]></title>
            <link>https://zenn.dev/nnaka2992/articles/play_with_yugabytedb_managed_sandbox</link>
            <guid>https://zenn.dev/nnaka2992/articles/play_with_yugabytedb_managed_sandbox</guid>
            <pubDate>Sun, 04 Feb 2024 15:02:28 GMT</pubDate>
            <content:encoded><![CDATA[YugabyteDB Managedにフリートライアルがあるのは知っていたのですが、期間が限られたものしか無いと思っていました。YugabyteDBについて調べごとをしていたら機能制限はあるもののSandboxクラスターというクレジットカード登録すら不要でAlways Freeな利用枠があることを知りました。いままでローカルでYugabyteDBを建てたりminikube上で遊んでいたのですが、簡単な検証であればSandboxクラスターで十分です。この記事ではそんなYugabyteDB ManagedのSandboxクラスターを紹介します。 Sandbox Clusterの制限...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google CloudのObservability : Personalized Service Health]]></title>
            <link>https://zenn.dev/yokoo_an209/articles/8cc96cb8acb4bb</link>
            <guid>https://zenn.dev/yokoo_an209/articles/8cc96cb8acb4bb</guid>
            <pubDate>Sun, 04 Feb 2024 11:02:42 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先日、これまでプレビュー版だった、Google Cloudの稼働監視サービスのPersonalized Service HealthがGAになりました！https://cloud.google.com/blog/products/devops-sre/personalized-service-health-is-now-generally-available/?hl=en基本的な使用方法や設定はこちらのブログにて詳しく解説されていますので、ここでは運用面の機能を中心にPersonalized Service Healthを見ていきます。Google Cloudの障害情...]]></content:encoded>
        </item>
    </channel>
</rss>