<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Fri, 30 Jan 2026 11:48:05 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[AIを使うのは当たり前になったけど、AIを使いたくないときがある]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</link>
            <guid isPermaLink="false">https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</guid>
            <pubDate>Thu, 29 Jan 2026 15:55:21 GMT</pubDate>
            <content:encoded><![CDATA[ChatGPTが最初に発表されたときに比べ、生成AIはいわゆるハルシネーションも減ればコーディングも得意になり、情報の調査から整理まであらゆる用途で実用的になりました。普段のくだらない疑問をSNSに垂れ流していたのが、雑にChatGPTに聞くようになりました。少し真剣に考えたい仕事のトピックや会社の情報を入れる時には、取り敢えずGeminiに意見を求めることも少なくありません。コーディングにいたっては速さも質も私ではClaudeに勝てなくなりました。そんな便利な生成AIですが、自身のキャリアや目標、解釈ではなく理解したい技術を学ぶときは安易に生成AIを利用したくないと思っています。生成AIを利用することでインスタントにそれらしい出力を得ることができますが、よく語られるようにその質は現時点のインプットに従属します。キャリアや目標のような自分の中にしか答えがなく、向き合い磨くことでしか良くならいものがあります。こういったトピックに生成AIを利用すると短期的にそれらしい出力を得られますが、頭の片隅にしかない小さなアイデアの種は簡単に押しのけられてしまいます。うわべではなく、真に理解したい論文や技術を生成AIに頼ると、理解ではなく解釈に逃げてしまい、要約の周りにある削ぎ落とされた情報を自分のものにすることができません。生成AIが幅を利かせるようになるまでは、アウトプットこそ至高でアウトプットこそ正義と考えていました。最近は生成AIを使って言葉することで消えてしまう考えや、言葉にすることでこぼれ落ちてしまうアイデアの輪郭にこそ価値があるのではないかと思うようになりました。言葉にすれば消えちゃう関係なら言葉を消せばいいやって思ってた 恐れてただけど あれ? なんかちがうかも曲名 恋愛サーキュレーション作詞 meg rock作曲・編曲 神前 暁 (MONACA)歌 千石撫子(CV:花澤香菜)一昔前のオタクが大好きな恋愛サーキュレーションの真逆だなと思った冬の日です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIでやれる時代に、それでも誰かと何かをやるということ]]></title>
            <link>https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</link>
            <guid isPermaLink="false">https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</guid>
            <pubDate>Thu, 29 Jan 2026 11:43:18 GMT</pubDate>
            <content:encoded><![CDATA[「AIを触り、感動する」という体験が個人的に一巡した感覚があるので、2026年を走り出すにあたり、思っていることを記録する。 ソロ登頂よりも、チーム敗退「うまくいかなくても、誰かと何かをやる」ということの価値が相対的に上がっているように感じる。ここでいう「誰か」は、当然GeminiでもChatGPTでもないし、「何かをやる」とは、Vibe Codingでソフトウェアを作ることでも、NotebookLMでそれっぽいスライドを生成することでもない。過去の自分の仕事の記憶。「どういうものをイメージしてるのか全く分からん」と内心上司にキレながらオフィスで過ごしたスライドレビューのワッペ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ 2026年1月 Neovim の Rust 環境を見直した]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</guid>
            <pubDate>Thu, 29 Jan 2026 04:07:42 GMT</pubDate>
            <content:encoded><![CDATA[はじめにgithub.com先週、エージェントが書いた200行のコードを開いた。move |ctx| { ... } というクロージャがあった。この ctx は何をキャプチャしているのか。所有権は移動しているのか、借用なのか。コードを読んでも分からない。コンパイルしてエラーが出るまで待つか、エージェントに「このクロージャは何をキャプチャしてる？」と聞くか。どちらも面倒だった。エージェントにすべてを任せれば楽になる、と思っていた時期がある。しかし1ヶ月ほど使って気づいた。大きな変更はエージェントが得意だ。ファイルを跨いだリファクタリング、新機能の実装、テストの追加。これらは確かにエージェントに任せた方が速い。しかし、生成されたコードの一部だけを直したいとき、エージェントに再度依頼するのは効率が悪い。「この unwrap() を ? に変えたい」「このクロージャの引数名を直したい」「この変数を別のスコープに移動したい」といった微修正は、手で直した方が速い。私は1日の開発時間のうち、7割をエージェントとの対話に、3割をNeovimでの直接作業に使っている。この3割のほとんどはコードリーディングとコードベースの理解で、実際に手で修正するのは5%程度だ。それでも、コードを「読む」環境が貧弱だと、全体の生産性が落ちる。Neovimを使う理由は、思考のスピードで編集できるからだ。ciw で単語を置換し、. で繰り返し、/ で検索して n で次へ。この一連の操作が指に染み付いていると、「直したい」と思った瞬間に直せる。エージェントに依頼を書いて、結果を待って、差分を確認する時間がない。私は Claude Code も Neovim から起動している。ターミナルで claude を叩き、エージェントと対話し、生成されたコードを Neovim で開いて確認する。すべてが同じ環境で完結する。エージェントが書いたコードを「読む」環境と、細部を「直す」環境。この両方が揃って初めて、エージェント時代の開発は快適になる。この記事では、Neovim 0.11+ と Rust の開発環境を見直した結果を紹介する。冒頭で困っていた「クロージャのキャプチャが分からない」問題を解決する Inlay Hints と、素早い修正を可能にするキーマップに重点を置いている。構成の概要私の開発環境全体については以下の記事で紹介している。本記事では Rust 関連の設定に絞って解説する。syu-m-5151.hatenablog.com私の Neovim 環境は NvChad をベースにしている。プラグイン管理には lazy.nvim を使い、Rust 関連は以下の構成だ。 コンポーネント  役割  rust-analyzer  LSP（言語サーバー）、nvim-lspconfig 経由で設定  rustaceanvim  Rust専用の拡張機能（テスト実行、マクロ展開など）  crates.nvim  Cargo.toml の依存関係管理  nvim-dap  デバッグサポート（LLDB連携）  conform.nvim  フォーマッタ（rustfmt）  mason.nvim  LSP/DAP のインストール管理 エージェント連携として claudecode.nvim も入れている。Neovim から Claude Code を起動し、生成されたコードをそのまま編集できる。以下では、この構成を「コードを読む」→「テスト・実行する」→「細部を直す」の順で紹介する。rust-analyzer と Inlay Hints でコードを理解し、rustaceanvim でテストを回し、conform.nvim でフォーマットを整える。この流れが、エージェントが書いたコードを確認・修正するワークフローと対応している。rust-analyzer の設定github.comgithub.comrust-analyzer は Rust の公式 LSP（Language Server Protocol）実装だ。LSP とは、エディタに補完、定義ジャンプ、エラー表示などの機能を提供するプロトコルで、VSCode でも Neovim でも同じ言語サーバーを使える。エージェントが生成したコードを読む際、型やライフタイム（Rust特有のメモリ管理の仕組みで、参照がいつまで有効かを示す）の情報がインラインで表示されると理解が格段に速くなる。私は nvim-lspconfig 経由で設定している。Neovim 0.11+ では vim.lsp.config API が使えるようになり、設定がシンプルになった。-- lua/configs/lspconfig.luarust_analyzer = {  settings = {    ["rust-analyzer"] = {      checkOnSave = {        command = "clippy",        extraArgs = { "--all", "--", "-W", "clippy::all" },      },      cargo = {        allFeatures = true,        loadOutDirsFromCheck = true,        buildScripts = { enable = true },      },      procMacro = {        enable = true,        attributes = { enable = true },      },      inlayHints = {        enable = true,        chainingHints = { enable = true },        typeHints = { enable = true, hideClosureInitialization = true },        parameterHints = { enable = true },        closureReturnTypeHints = { enable = "with_block" },        lifetimeElisionHints = { enable = "skip_trivial", useParameterNames = true },        maxLength = 25,        bindingModeHints = { enable = true },        closureCaptureHints = { enable = true },        discriminantHints = { enable = "fieldless" },        expressionAdjustmentHints = { enable = "reborrow" },        rangeExclusiveHints = { enable = true },      },      completion = {        autoimport = { enable = true },        postfix = { enable = true },        callable = { snippets = "fill_arguments" },        fullFunctionSignatures = { enable = true },        privateEditable = { enable = true },      },      imports = {        granularity = { group = "module" },        prefix = "self",      },      diagnostics = {        enable = true,        experimental = { enable = true },        styleLints = { enable = true },      },      semanticHighlighting = {        operator = { specialization = { enable = true } },        punctuation = { enable = true, specialization = { enable = true } },        strings = { enable = true },      },      hover = {        actions = {          enable = true,          references = { enable = true },          run = { enable = true },          debug = { enable = true },          gotoTypeDef = { enable = true },          implementations = { enable = true },        },        documentation = { enable = true, keywords = { enable = true } },        links = { enable = true },      },      typing = {        autoClosingAngleBrackets = { enable = true },      },      lens = {        enable = true,        references = { enable = true, adt = { enable = true }, enumVariant = { enable = true }, method = { enable = true }, trait = { enable = true } },        implementations = { enable = true },        run = { enable = true },        debug = { enable = true },      },      workspace = {        symbol = { search = { kind = "all_symbols" } },      },    },  },}-- Neovim 0.11+ の新しい API を使用vim.lsp.config("rust_analyzer", config)vim.lsp.enable("rust_analyzer")主な設定項目inlayHints - エージェントが生成したコードを読むとき、最も役立つのがこれだ。エディタ内にインラインで型情報やパラメータ名が表示される。特に closureCaptureHints は重宝している。Rustではクロージャ（|x| x + 1 のような無名関数）が外部の変数を使うとき、その変数を「キャプチャ」する。move |data| { ... } と書くと、data の所有権がクロージャに移動（ムーブ）する。この「何がキャプチャされているか」がエディタ上に [move: data] と表示されるようになる。エージェントが書いたクロージャを理解するのに、コンパイルエラーを待つ必要がなくなった。rangeExclusiveHints も地味に便利で、0..len が排他的（len を含まない）であることを .. の横に明示してくれる。diagnostics.experimental - 実験的な診断機能を有効化する。styleLints を有効にすると、Clippy のスタイル系リントも保存時に表示される。エージェントが生成したコードは動くが、慣用的でないことがある。この設定で「動くけど直したほうがいい」箇所が分かる。semanticHighlighting - 演算子や句読点に対してセマンティックハイライトを適用する。*self.data のような式で * が参照外しとして色付けされると、複雑な式の構造が視覚的に分かる。ただし、色数を増やしすぎるとノイズになるので、私は operator と punctuation のみ有効にしている。hover.actions - ホバー時に「Run」「Debug」「Go to Type Definition」などのアクションを表示する。エージェントが追加したテストを実行したいとき、テスト関数にカーソルを合わせて K を押すだけで実行できる。typing.autoClosingAngleBrackets - Vec< と入力すると自動的に > が補完される。エージェントが書いた型を微修正するとき、> の数を数えなくて済む。rustaceanvim の設定github.comrust-analyzer がコードを「読む」ための機能を提供するのに対し、rustaceanvim は「テスト・実行・デバッグ」のための機能を提供する。両者は補完関係にあり、rust-analyzer の LSP 機能に加えて、Rust 特有の操作（マクロ展開、テスト実行など）を追加する。rustaceanvim は rust-tools.nvim の後継だが、単なるメンテナンス引き継ぎではない。最大の違いは遅延読み込みへの対応で、.rs ファイルを開くまで何も読み込まない。私の環境では Neovim の起動時間が 120ms から 45ms に短縮された。エージェント時代に rustaceanvim が重要な理由は、「素早い確認と修正」のワークフローを支えることにある。エージェントがコードを生成したら、テストを実行し、エラーがあれば修正し、また実行する。このサイクルを <leader>rt（テスト実行）と <leader>re（エラー説明）で高速に回せる。-- lua/plugins/lang.lua{  "mrcjkb/rustaceanvim",  version = "^5",  lazy = false,  init = function()    vim.g.rustaceanvim = {      tools = {        hover_actions = { replace_builtin_hover = false },        float_win_config = { border = "rounded" },        inlay_hints = { auto = true },        code_actions = { ui_select_fallback = true },      },      server = {        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ra", function() vim.cmd.RustLsp "codeAction" end, vim.tbl_extend("force", opts, { desc = "Rust code action" }))          vim.keymap.set("n", "<leader>rd", function() vim.cmd.RustLsp "debuggables" end, vim.tbl_extend("force", opts, { desc = "Rust debuggables" }))          vim.keymap.set("n", "<leader>rr", function() vim.cmd.RustLsp "runnables" end, vim.tbl_extend("force", opts, { desc = "Rust runnables" }))          vim.keymap.set("n", "<leader>rt", function() vim.cmd.RustLsp "testables" end, vim.tbl_extend("force", opts, { desc = "Rust testables" }))          vim.keymap.set("n", "<leader>rm", function() vim.cmd.RustLsp "expandMacro" end, vim.tbl_extend("force", opts, { desc = "Expand macro" }))          vim.keymap.set("n", "<leader>rc", function() vim.cmd.RustLsp "openCargo" end, vim.tbl_extend("force", opts, { desc = "Open Cargo.toml" }))          vim.keymap.set("n", "<leader>rp", function() vim.cmd.RustLsp "parentModule" end, vim.tbl_extend("force", opts, { desc = "Parent module" }))          vim.keymap.set("n", "<leader>rj", function() vim.cmd.RustLsp "joinLines" end, vim.tbl_extend("force", opts, { desc = "Join lines" }))          vim.keymap.set("n", "<leader>rs", function() vim.cmd.RustLsp "ssr" end, vim.tbl_extend("force", opts, { desc = "Structural search replace" }))          vim.keymap.set("n", "<leader>re", function() vim.cmd.RustLsp "explainError" end, vim.tbl_extend("force", opts, { desc = "Explain error" }))          vim.keymap.set("n", "<leader>rD", function() vim.cmd.RustLsp "renderDiagnostic" end, vim.tbl_extend("force", opts, { desc = "Render diagnostic" }))          vim.keymap.set("n", "K", function() vim.cmd.RustLsp { "hover", "actions" } end, vim.tbl_extend("force", opts, { desc = "Rust hover actions" }))        end,        default_settings = {          ["rust-analyzer"] = {            cargo = { allFeatures = true },            checkOnSave = { command = "clippy" },          },        },      },      dap = {        adapter = {          type = "executable",          command = "lldb-dap",          name = "rt_lldb",        },      },    }  end,},キーマップ一覧 キー  機能  <leader>ra  コードアクション  <leader>rr  実行可能ターゲットを選択して実行  <leader>rt  テストを選択して実行  <leader>rd  デバッグ実行  <leader>rm  カーソル位置のマクロを展開  <leader>re  エラーの詳細説明を表示  <leader>rD  診断をレンダリング  <leader>rs  構造的検索置換（SSR）  <leader>rp  親モジュールに移動  <leader>rj  行を結合  <leader>rc  Cargo.toml を開く  K  ホバーアクション付きドキュメント crates.nvim の設定github.comRustでは Cargo.toml というファイルで依存ライブラリ（クレートと呼ぶ）を管理する。Node.js の package.json、Python の requirements.txt に相当するものだ。クレートには「フィーチャー」という機能のオン・オフがあり、必要な機能だけを有効にしてビルドサイズを抑えることができる。エージェントに「tokio を追加して」と頼むと、だいたい最新版を入れてくれる。しかし、フィーチャーの選択は雑なことが多い。tokio = { version = "1", features = ["full"] } と書かれていて、「full はオーバーキルだな、macros と rt-multi-thread だけでいいのに」と思うことがある。crates.nvim があれば、Cargo.toml 上でクレートにカーソルを合わせて <leader>cf を押すだけで、フィーチャー一覧がポップアップする。必要なものだけ選んで、不要なものは外す。この微調整はエージェントに頼むより、手でやった方が速い。-- lua/plugins/lang.lua{  "saecki/crates.nvim",  tag = "stable",  event = { "BufRead Cargo.toml" },  dependencies = { "nvim-lua/plenary.nvim" },  config = function()    local crates = require "crates"    crates.setup {      completion = {        cmp = { enabled = true },        crates = { enabled = true, max_results = 8, min_chars = 3 },      },      lsp = {        enabled = true,        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ct", crates.toggle, vim.tbl_extend("force", opts, { desc = "Toggle crates" }))          vim.keymap.set("n", "<leader>cr", crates.reload, vim.tbl_extend("force", opts, { desc = "Reload crates" }))          vim.keymap.set("n", "<leader>cv", crates.show_versions_popup, vim.tbl_extend("force", opts, { desc = "Show versions" }))          vim.keymap.set("n", "<leader>cf", crates.show_features_popup, vim.tbl_extend("force", opts, { desc = "Show features" }))          vim.keymap.set("n", "<leader>cd", crates.show_dependencies_popup, vim.tbl_extend("force", opts, { desc = "Show dependencies" }))          vim.keymap.set("n", "<leader>cu", crates.update_crate, vim.tbl_extend("force", opts, { desc = "Update crate" }))          vim.keymap.set("v", "<leader>cu", crates.update_crates, vim.tbl_extend("force", opts, { desc = "Update crates" }))          vim.keymap.set("n", "<leader>cU", crates.upgrade_crate, vim.tbl_extend("force", opts, { desc = "Upgrade crate" }))          vim.keymap.set("v", "<leader>cU", crates.upgrade_crates, vim.tbl_extend("force", opts, { desc = "Upgrade crates" }))          vim.keymap.set("n", "<leader>cA", crates.upgrade_all_crates, vim.tbl_extend("force", opts, { desc = "Upgrade all crates" }))          vim.keymap.set("n", "<leader>cH", crates.open_homepage, vim.tbl_extend("force", opts, { desc = "Open homepage" }))          vim.keymap.set("n", "<leader>cR", crates.open_repository, vim.tbl_extend("force", opts, { desc = "Open repository" }))          vim.keymap.set("n", "<leader>cD", crates.open_documentation, vim.tbl_extend("force", opts, { desc = "Open docs.rs" }))          vim.keymap.set("n", "<leader>cC", crates.open_crates_io, vim.tbl_extend("force", opts, { desc = "Open crates.io" }))        end,        actions = true,        completion = true,        hover = true,      },      popup = {        border = "rounded",        show_version_date = true,        max_height = 30,        min_width = 20,      },    }  end,},キーマップ一覧 キー  機能  <leader>ct  crates.nvim の表示切り替え  <leader>cr  クレート情報を再読み込み  <leader>cv  バージョン一覧をポップアップ表示  <leader>cf  フィーチャー一覧を表示  <leader>cd  依存関係を表示  <leader>cu  クレートを最新パッチバージョンに更新（ビジュアルモードで複数選択可）  <leader>cU  クレートを最新バージョンにアップグレード  <leader>cA  すべてのクレートをアップグレード  <leader>cH  クレートのホームページを開く  <leader>cR  クレートの GitHub リポジトリを開く  <leader>cD  docs.rs を開く  <leader>cC  crates.io を開く nvim-dap によるデバッグgithub.comgithub.comgithub.comgithub.comエージェントが生成したコードで「なぜこの値になるのか分からない」という場面がある。println デバッグで済むこともあるが、複雑なロジックでは変数の変化を追いたくなる。Rust のデバッグには LLDB を使う。LLDB は C/C++/Rust などのコンパイル言語向けのデバッガで、ブレークポイント（プログラムを一時停止する地点）を設定し、変数の中身を確認しながらステップ実行できる。macOS の場合は Homebrew で LLVM をインストールし、その中に含まれる lldb-dap（DAP = Debug Adapter Protocol）を使う。-- lua/plugins/lang.lua{  "mfussenegger/nvim-dap",  lazy = true,  dependencies = {    "rcarriga/nvim-dap-ui",    "nvim-neotest/nvim-nio",    "theHamsta/nvim-dap-virtual-text",  },  keys = {    { "<leader>db", function() require("dap").toggle_breakpoint() end, desc = "Toggle breakpoint" },    { "<leader>dB", function() require("dap").set_breakpoint(vim.fn.input "Breakpoint condition: ") end, desc = "Conditional breakpoint" },    { "<leader>dc", function() require("dap").continue() end, desc = "Continue" },    { "<leader>dC", function() require("dap").run_to_cursor() end, desc = "Run to cursor" },    { "<leader>di", function() require("dap").step_into() end, desc = "Step into" },    { "<leader>do", function() require("dap").step_over() end, desc = "Step over" },    { "<leader>dO", function() require("dap").step_out() end, desc = "Step out" },    { "<leader>dp", function() require("dap").pause() end, desc = "Pause" },    { "<leader>dr", function() require("dap").repl.toggle() end, desc = "Toggle REPL" },    { "<leader>dt", function() require("dap").terminate() end, desc = "Terminate" },    { "<leader>du", function() require("dapui").toggle() end, desc = "Toggle DAP UI" },    { "<leader>de", function() require("dapui").eval() end, desc = "Eval", mode = { "n", "v" } },  },  config = function()    local dap = require "dap"    local dapui = require "dapui"    -- DAP UI setup with custom layout    dapui.setup {      icons = { expanded = "▾", collapsed = "▸", current_frame = "▸" },      layouts = {        {          elements = {            { id = "scopes", size = 0.25 },            { id = "breakpoints", size = 0.25 },            { id = "stacks", size = 0.25 },            { id = "watches", size = 0.25 },          },          size = 40,          position = "left",        },        {          elements = {            { id = "repl", size = 0.5 },            { id = "console", size = 0.5 },          },          size = 10,          position = "bottom",        },      },    }    -- Virtual text for debugging    require("nvim-dap-virtual-text").setup { enabled = true, commented = true }    -- LLDB adapter    dap.adapters.lldb = {      type = "executable",      command = "/opt/homebrew/opt/llvm/bin/lldb-dap",      name = "lldb",    }    dap.configurations.rust = {      {        name = "Launch",        type = "lldb",        request = "launch",        program = function()          return vim.fn.input("Path to executable: ", vim.fn.getcwd() .. "/target/debug/", "file")        end,        cwd = "${workspaceFolder}",        stopOnEntry = false,        args = {},        runInTerminal = false,      },    }    -- Auto open/close DAP UI    dap.listeners.after.event_initialized["dapui_config"] = function() dapui.open() end    dap.listeners.before.event_terminated["dapui_config"] = function() dapui.close() end    dap.listeners.before.event_exited["dapui_config"] = function() dapui.close() end    -- Signs    vim.fn.sign_define("DapBreakpoint", { text = "●", texthl = "DapBreakpoint" })    vim.fn.sign_define("DapBreakpointCondition", { text = "●", texthl = "DapBreakpointCondition" })    vim.fn.sign_define("DapStopped", { text = "▶", texthl = "DapStopped", linehl = "DapStoppedLine" })  end,},キーマップ一覧 キー  機能  <leader>db  ブレークポイントの切り替え  <leader>dB  条件付きブレークポイント  <leader>dc  続行  <leader>dC  カーソル位置まで実行  <leader>di  ステップイン  <leader>do  ステップオーバー  <leader>dO  ステップアウト  <leader>dp  一時停止  <leader>dr  REPL の切り替え  <leader>dt  終了  <leader>du  DAP UI の切り替え  <leader>de  カーソル位置の式を評価（ビジュアルモードでも使用可） conform.nvim でのフォーマットgithub.com保存時に rustfmt を自動実行する。エージェントが生成したコードはフォーマットが崩れていることがあるので、保存するだけで整形されるのは便利だ。-- lua/plugins/lsp.lua{  "stevearc/conform.nvim",  event = "BufWritePre",  config = function()    require("conform").setup {      formatters_by_ft = {        rust = { "rustfmt", lsp_format = "fallback" },        -- 他の言語も設定可能      },      format_on_save = { timeout_ms = 500, lsp_fallback = true },    }  end,},ここまでで「コードを読む」「テスト・実行する」「細部を直す」の設定が揃った。最後に、これらのツール自体をどうインストールするかを紹介する。Mason でのツールインストールgithub.comLSP サーバーやデバッグアダプタは Mason で管理する。:MasonInstall で個別にインストールすることもできるが、ensure_installed に書いておけば自動でインストールされる。-- lua/plugins/lsp.lua{  "williamboman/mason.nvim",  opts = {    ensure_installed = {      "rust-analyzer",      "codelldb",  -- DAP adapter      -- 他の言語のツールも同様に追加    },  },},まとめこの設定を入れた翌日、またエージェントが書いたコードを開いた。move |ctx| { ... } というクロージャがある。今度は違った。クロージャの横に [move: ctx, config] と表示されている。何がキャプチャされているか、一目で分かる。コンパイルを待つ必要も、エージェントに聞く必要もない。エージェントに任せる7割と、自分で読む3割。この3割を Neovim で快適にすることが、全体の生産性につながる。rust-analyzer の Inlay Hints でコードを読み、rustaceanvim のキーマップでテストを回し、思考のスピードで細部を直す。エージェント時代だからこそ、手元のエディタは大事だ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[
		Binance Referral Bonus より		]]></title>
            <link>https://sreake.com/blog/chatgpt-slack-integration/#comment-4744</link>
            <guid isPermaLink="false">https://sreake.com/blog/chatgpt-slack-integration/#comment-4744</guid>
            <pubDate>Thu, 29 Jan 2026 02:20:31 GMT</pubDate>
            <content:encoded><![CDATA[I don't think the title of your article matches the content lol. Just kidding, mainly because I had some doubts after reading the article. https://accounts.binance.info/es-MX/register?ref=GJY4VW8W]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（後編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</guid>
            <pubDate>Thu, 29 Jan 2026 00:20:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前編を書き終えたあと、エディタを閉じて、しばらくターミナルを眺めていた。Zellijのペインが3つ並んでいる。左でVimが開き、右上でテストが走り、右下にシェルが待機している。何も起きていない。何も起きていないのに、裏では6つのスレッドが動いている。チャネルを介してメッセージが流れ、PTYがカーネルとやり取りし、VTEパーサがバイト列を解釈している。前編では設計パターンを抽出した。cat huge_log_file.logで200万行を流し込んだとき、Zellijが固まらない理由——境界付きチャネルによるバックプレッシャー。その仕組みを概念として説明した。後編では、その実装の中に入る。正直に言うと、後編は地味だ。WASMプラグインの通信プロトコル、ANSIエスケープシーケンスのパース、KDL形式のセッション永続化。どれも「知っていると便利」だが「知らなくても困らない」話かもしれない。華やかさはない。ただ、Rustで本格的なアプリケーションを書こうとしたとき、こういう地味な部分でつまずく。つまずいてから調べるか、先に知っておくか。その違いは、たぶん小さくない。Cargo Workspace構成の深掘り前編でCargo Workspaceの構造を見た。後編では、なぜこの分割になっているのかを考える。zellij-utils/を開くと、IPCの定義やエラー処理、設定ファイルのパーサーが入っている。default-plugins/* → zellij-tile → zellij-tile-utilsclient ↓ server       ↓    zellij-utils ← 共有型定義（IPC契約）zellij-utilsが双方向依存を防いでいる。clientもserverもutilsに依存するが、utilsはどちらにも依存しない。これにより、clientを変更してもserverの再コンパイルは不要になる。10万行超のコードベースでは、このビルド時間の差が開発体験に直結する。zellij-tileをSDKとして独立させた意図も見える。プラグイン開発者はサーバー実装への依存なしにビルドできる。これは「プラグインエコシステムの成長」を設計段階で意識した判断だ。後からSDKを切り出すより、最初から分けておく方が遥かに低コストになる。後編で必要な追加知識前編でPTY、チャネル、Actorモデル、WASMの基礎を説明した。後編では、さらに低レベルな概念が登場する。ここで整理しておこう。termios構造体とターミナルモードUnixのターミナルはtermios構造体で制御される。この構造体には、ターミナルの振る舞いを決めるフラグが数十個含まれている。// nixクレートでの操作let mut tio = termios::tcgetattr(fd)?;  // 現在の設定を取得termios::cfmakeraw(&mut tio);            // Raw Modeに設定termios::tcsetattr(fd, SetArg::TCSANOW, &tio)?;  // 即座に適用ターミナルには2つの主要なモードがある。Cooked Mode（カノニカルモード）- カーネルが行編集を処理する（バックスペース、Ctrl+Wなど）- Enterを押すまで入力がバッファされる- Ctrl+CでSIGINTが自動送信されるRaw Mode- すべてのキー入力がそのままアプリケーションに届く- 行編集もシグナル生成もアプリケーションの責任- ターミナルマルチプレクサには必須Zellijは起動時にRaw Modeに入り、終了時に元のモードに戻す。これを忘れると、ターミナルが「壊れた」状態になる。主要なシグナルターミナルアプリケーションが扱う主なシグナルは以下の通り。 シグナル  発生条件  用途  SIGWINCH  ウィンドウリサイズ  ターミナルサイズの再取得  SIGINT  Ctrl+C  プロセスの中断  SIGTSTP  Ctrl+Z  プロセスの一時停止  SIGTERM  killコマンド  正常終了の要求  SIGKILL  kill -9  強制終了（捕捉不可）  SIGHUP  端末切断  セッション終了  SIGCHLD  子プロセス終了  子プロセスの状態変化 ZellijはSIGWINCHを特に注意深く扱う。ウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のSIGWINCHが発生する。すべてに反応するとパフォーマンスが悪化するため、スロットリング（間引き）が必要だ。ioctl：デバイス制御ioctl（I/O Control）は、デバイスに対する特殊な操作を行うシステムコールだ。ターミナル関連では以下が重要。// ウィンドウサイズの取得ioctl(fd, TIOCGWINSZ, &mut winsize);  // Get WINdow SiZe// ウィンドウサイズの設定ioctl(fd, TIOCSWINSZ, &winsize);      // Set WINdow SiZeWinsize構造体は4つのフィールドを持つ。struct Winsize {    ws_row: u16,      // 行数    ws_col: u16,      // 列数    ws_xpixel: u16,   // ピクセル幅（Sixel画像用）    ws_ypixel: u16,   // ピクセル高さ（Sixel画像用）}TIOCSWINSZでPTYのサイズを変更すると、カーネルは子プロセスにSIGWINCHを送る。シェルはこのシグナルを受けて画面を再描画する。ANSIエスケープシーケンスの詳細前編で触れたが、後編ではより詳しく見る。CSI（Control Sequence Introducer）\x1b[  → CSI開始CSIの後にパラメータとコマンドが続く。\x1b[31m      → 前景色を赤に（SGR: Select Graphic Rendition）\x1b[10;5H   → カーソルを10行5列に移動（CUP: Cursor Position）\x1b[2J      → 画面全体をクリア（ED: Erase in Display）\x1b[?25h    → カーソルを表示（DECTCEM）\x1b[?25l    → カーソルを非表示OSC（Operating System Command）\x1b]0;title\x07  → ウィンドウタイトルを設定\x1b]8;;URL\x07   → ハイパーリンク開始DCS（Device Control String）\x1bP...ST  → 同期出力、Sixel画像などZellijはvteクレートでこれらをパースし、Performトレイトの各メソッドに振り分ける。ファイルディスクリプタとPTYUnixでは「すべてがファイル」だ。PTYもファイルディスクリプタ（FD）で表現される。let OpenptyResult { master, slave } = openpty(None, &termios)?;// master: RawFd (例: 3)// slave:  RawFd (例: 4)子プロセス（シェル）は、login_tty()で以下の処理を行う。新しいセッションを作成（setsid()）slave PTYを制御端末に設定FD 0, 1, 2（stdin, stdout, stderr）をslave PTYに接続これにより、シェルの入出力はすべてPTY経由になる。tcdrain：出力の完了待ちtcdrainは、書き込んだデータがすべて送信されるまでブロックするシステムコールだ。write(fd, bytes)?;   // バッファに書き込むtcdrain(fd)?;        // 送信完了を待つなぜ必要か。write()はカーネルのバッファに書き込んだ時点で返る。相手がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。Zellijでは、PTYへの書き込み後にtcdrain()を呼ぶ。これにより、入力が確実にシェルに届いてから次の処理に進む。これらの概念の関係[termios] ─── Raw/Cooked Mode を制御    ↓[PTY Master] ←── ioctl(TIOCGWINSZ/TIOCSWINSZ) でサイズ制御    │    │ write() + tcdrain()    ↓[PTY Slave] ─── シェルの stdin/stdout/stderr    │    │ SIGWINCH, SIGCHLD など    ↓[シグナルハンドラ] ─── スロットリング、グレースフル終了後編では、これらの概念がZellijの実装にどう現れるかを見ていく。境界付きチャネルの実装前編で「バッファサイズ50」と書いた。実際のコードを見てみよう。zellij-client/src/lib.rsを開く。// zellij-client/src/lib.rslet (send_client_instructions, receive_client_instructions): ChannelWithContext<    ClientInstruction,> = channels::bounded(50);  // バッファサイズ: 50サーバー側も同様だ。zellij-server/src/lib.rsを開く。// zellij-server/src/lib.rslet (to_server, server_receiver): ChannelWithContext<ServerInstruction> =    channels::bounded(50);なぜ50なのか。開発者ブログやissue #525を調査したが、この値を選んだ明確な理由は記載されていなかった。ただし、技術的な背景は理解できる。境界付きチャネル導入の発端はissue #525だ。PTYスレッドがプログラム出力を読み取り、無制限チャネル経由でScreenスレッドに送る。出力生成がレンダリング速度を超えると、キューが無限に成長し、メモリ使用量と入力遅延が悪化する。単純に境界付きチャネルに変えるとデッドロックのリスクがある。PTYがキューを満杯にする→WASMスレッドがレンダリング命令を送ろうとしてブロック→Screenスレッド（キューを空にすべき側）がWASMスレッドの応答待ちでブロック——この連鎖だ。解決策として、crossbeamのselect!マクロを使った選択的ルーティングが採用された。PTY→Screen間のみ境界付きチャネルでバックプレッシャーをかけ、他のコンポーネント間は無制限チャネルを維持する。50という数字は「小さすぎてスループットを落とさず、大きすぎてメモリを圧迫しない」経験的なバランス点だろう。開発者ブログによると、境界付きチャネルの導入だけでベンチマークは19秒から9秒に改善された。WASMランタイムの移行Zellijのコードを読んでいて、最も意外だったのがこの部分かもしれない。WASMランタイムを「遅い方」に移行している。普通は逆だ。前編では「wasmiを使っている」と書いた。しかし、Zellijの歴史を調べると、WASMランタイムは2度移行している。初期はWasmer、0.40.0でWasmtime、そして最新版ではWasmiに移行した（PR #4449）。zellij-server/Cargo.tomlを開く。# zellij-server/Cargo.toml[dependencies.wasmi]version = "0.51.3"default-features = falsefeatures = ["std"]なぜWasmtimeからWasmiへ移行したのか。PR #4449のディスカッションを読むと、理由が明確になる。コンパイルからインタプリタへ。Wasmtimeは.wasmファイルをJITコンパイルする。これには秒単位の時間がかかっていた。Wasmiはインタプリタ方式で、ミリ秒単位（一桁）で実行を開始できる。キャッシュ管理の排除。Wasmtime時代はコンパイル済みコードをキャッシュしていた。プラグイン開発時に「キャッシュバスティング」が必要で、これがコードの複雑さを増していた。Wasmiならキャッシュ不要だ。バイナリサイズとメモリ削減。WasmtimeはCraneliftコンパイラを含むため、バイナリサイズが大きい。Wasmiは純粋なRust実装のインタプリタで、依存関係がシンプルだ。Debianパッケージングでも、Wasmtimeの依存関係（wiggle等）がブロッカーになる可能性があったが、Wasmiなら問題ない。性能面のトレードオフ。PRテスターの報告によると、Debugビルドでは一部プラグイン（Zjstatus）の起動に約1秒の遅延が観察された。ただし、Releaseビルドでは顕著な影響がなかった。コンパイルプロファイルの調整で改善も報告されている。ステータスバーの更新に1msかかるか0.1msかかるか——ユーザーには分からない。「最速のランタイム」より「最もメンテナンスしやすいランタイム」を選んだ判断は、オープンソースプロジェクトとして合理的だ。Protocol Buffersによるプラグイン通信WASMランタイムがプラグインの「実行環境」なら、Protocol Buffersはプラグインの「通信手段」だ。プラグインとホスト間の通信はProtocol Buffersで実現されている。zellij-utils/src/plugin_api/plugin_command.protoを開く。// zellij-utils/src/plugin_api/plugin_command.protoenum CommandName {  Subscribe = 0;  Unsubscribe = 1;  SetSelectable = 2;  GetPluginIds = 3;  OpenFile = 9;  OpenTerminal = 14;  // ... 150以上のコマンド}150以上のコマンド。前編で見たScreenInstructionの100バリアントを超えている。プラグインはUI操作、ファイル操作、ネットワーク、他プラグインとの通信など、ホストの機能に広くアクセスできるからだ。なぜProtocol Buffersなのか。WASMとホストの間でデータを受け渡すには、シリアライズが必要だ。JSONでもMessagePackでも良いが、Protocol Buffersには以下の利点がある。スキーマがドキュメントになる: .protoファイルを見れば、プラグインAPIの全体像が分かる後方互換性: フィールドの追加・削除が安全にできる型安全: コード生成により、シリアライズ/デシリアライズのミスを防げるzellij-tile/src/lib.rsのマクロを見ると、Protocol Buffersがどう使われているか分かる。#[no_mangle]fn load() {    STATE.with(|state| {        let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();        // Protocol Buffersをデシリアライズして設定を取得    });}プラグインは標準入力からProtocol Buffersを読み、標準出力に書き込む。WASM境界を越えるのは単なるバイト列だ。シンプルだが、型安全性は失われない。パーミッションシステムの設計思想プラグインは16種類のパーミッションから必要なものを要求する。zellij-utils/src/plugin_api/plugin_permission.protoを開く。// zellij-utils/src/plugin_api/plugin_permission.protoenum PermissionType {  ReadApplicationState = 0;      // ペイン・タブ・UI状態の読み取り  ChangeApplicationState = 1;    // ペイン・タブ・UIの変更  OpenFiles = 2;                 // ファイルを開く  RunCommands = 3;               // コマンド実行  OpenTerminalsOrPlugins = 4;    // ターミナル/プラグインを開く  WriteToStdin = 5;              // ペインへの入力  WebAccess = 6;                 // HTTPリクエスト  ReadCliPipes = 7;              // CLIパイプの読み取り  MessageAndLaunchOtherPlugins = 8;  // 他プラグインとの通信  Reconfigure = 9;               // 設定変更  FullHdAccess = 10;             // ファイルシステム完全アクセス  StartWebServer = 11;           // Webサーバー起動  InterceptInput = 12;           // 入力のインターセプト  ReadPaneContents = 13;         // ペイン内容の読み取り  RunActionsAsUser = 14;         // ユーザーとしてアクション実行  WriteToClipboard = 15;         // クリップボードへの書き込み}このパーミッションモデルは「悪意あるプラグイン」より「バグのあるプラグイン」を想定している——と私は読んだ。考えてみてほしい。悪意あるプラグインを防ぎたいなら、ユーザーに許可を求めるUIは逆効果だ。ユーザーは深く考えずに「許可」を押す。AndroidやiOSの経験から、我々はそれを知っている。Zellijのパーミッションモデルが防いでいるのは、むしろ「うっかりファイルを消してしまうバグ」や「意図せずネットワークにアクセスしてしまう問題」だ。FullHdAccessを持つプラグインがファイルを誤削除するリスクを、ユーザーが明示的に受け入れる——そういう設計だと理解している。許可されたパーミッションはPermissionCacheにプラグイン名ごとに保存され、次回起動時は再確認されない。これも「毎回聞かれると面倒」という実用性を優先した判断だ。ANSIエスケープシーケンスのパースここまでプラグインの実行環境（WASM）、通信手段（Protocol Buffers）、安全性（パーミッション）を見てきた。ここからはサーバー側の話に移る。シェルの出力をどう画面に変換するか——その起点がANSIエスケープシーケンスのパースだ。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。use vte::{Params, Perform};impl Perform for Grid {    // 通常文字の描画    fn print(&mut self, c: char) {        self.add_character(c);    }    // C0/C1制御文字（改行、タブなど）    fn execute(&mut self, byte: u8) {        match byte {            b'\n' => self.move_cursor_down(1),            b'\r' => self.move_cursor_to_beginning_of_line(),            b'\t' => self.advance_to_next_tabstop(),            _ => {}        }    }    // CSIシーケンス（カーソル移動、色設定など）    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    _ignore: bool, action: char) {        // \x1b[10;2H → カーソル移動        // \x1b[36m → 色設定    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。Alacrittyと同じクレートを使っている点も興味深い。ターミナルエミュレータの世界では、vteがデファクトスタンダードになりつつある。差分レンダリングの実装VTEパーサがANSIエスケープシーケンスを解釈し、Gridが更新される。次の問題は、そのGridをどう効率的に画面へ反映するかだ。全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。// zellij-server/src/output/mod.rspub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更行インデックス    pub should_update_all_lines: bool,    styled_underlines: bool,}impl Default for OutputBuffer {    fn default() -> Self {        OutputBuffer {            changed_lines: HashSet::new(),            should_update_all_lines: true,  // 初回は全画面レンダリング            styled_underlines: true,        }    }}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn clear(&mut self) {        self.changed_lines.clear();        self.should_update_all_lines = false;    }}なぜ「行レベル」であり「セル単位」ではないのか。セル単位の差分追跡も技術的には可能だ。しかし、ターミナルの出力は行単位で更新されることが多く、1文字だけ変わるケースは稀だ。セル単位にすると、追跡のオーバーヘッドが差分レンダリングの利点を上回る可能性がある。HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。シンプルだが効果的だ。パフォーマンス最適化の成果ここまで見てきた境界付きチャネルと差分レンダリングは、個別には小さな改善に見える。しかし、組み合わせると効果は大きい。開発者ブログによると、以下の最適化によりcat bigfileのベンチマークが大幅に改善された。ベンチマーク条件:- 測定コマンド: hyperfine --show-output "cat /tmp/bigfile"（10回実行の平均）- ファイルサイズ: 200万行- ペインサイズ: 59行 × 104列 段階  時間  最適化前  19.175秒 ± 0.347秒  境界付きチャネル導入後  9.658秒 ± 0.095秒  全最適化後  5.270秒 ± 0.027秒  tmux（参考）  5.593秒 このベンチマークではtmuxと同等以上のパフォーマンスを達成している。ただし、マシンスペックやtmuxのバージョン・設定は記載されていない。実環境での性能はワークロードや設定に依存するため、「Zellijの方が常に速い」とは言えない。重要なのは、適切な最適化によってRust製の新参者が30年の歴史を持つtmuxと同等のパフォーマンスを達成できた点だ。主な最適化は以下の4つだ。境界付きチャネルによるバックプレッシャー: 19秒→9秒の最大の貢献Vecの事前確保: Vec::with_capacity()で再確保を削減Unicode幅のキャッシュ: 絵文字などの幅計算を毎回やらない行レベル差分追跡: 変更行のみを再描画どれも「当たり前」の最適化だ。しかし、当たり前のことを愚直にやるのは難しい。ターミナル特有の問題への対処ここまで、チャネル、WASM、Protocol Buffers、ANSIパーサー、差分レンダリングと見てきた。どれも汎用的なパターンの応用だ。ここからは違う。ターミナルエミュレータでなければ出会わない問題ばかりだ。ソースコードを読んでいて、一番面白かったのはこのあたりだった。RcCharacterStyles: 16バイトに収めるメモリ効率化ターミナルの文字列バッファは数百万の要素を持つ。1文字あたりのメモリサイズがパフォーマンスに直結する。zellij-server/src/panes/terminal_character.rsを開く。// Enum Niche Optimization: 2つのvariantしかないため、ポインタサイズと同じ8バイトに収まる#[derive(Clone, Debug, PartialEq)]pub enum RcCharacterStyles {    Reset,    Rc(Rc<CharacterStyles>),}// compile-time assertionでメモリサイズを保証#[cfg(target_arch = "x86_64")]const _: [(); 8] = [(); std::mem::size_of::<RcCharacterStyles>()];// TerminalCharacter全体も16バイト#[cfg(target_arch = "x86_64")]const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];// thread_local!でデフォルトスタイルをキャッシュし、メモリ再利用thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}impl Default for RcCharacterStyles {    fn default() -> Self {        RC_DEFAULT_STYLES.with(|s| s.clone())  // thread_localから共有参照を取得    }}compile-time assertionが面白い。const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];は、TerminalCharacterのサイズが16バイトでなければコンパイルエラーになる。将来フィールドを追加したとき、意図せずメモリサイズが増えることを防ぐ。Enum Niche Optimization + Reference Counting + thread_localの組み合わせで、リセット状態の文字スタイルをメモリ効率的に管理している。型安全性を失わずに、大規模なパフォーマンス最適化を実現しているのが印象的だ。PaneResizer: Cassowary制約ソルバーによるペイン配置ペインのレイアウト計算は、意外と難しい。「固定サイズのペイン」と「パーセンテージ指定のペイン」が混在し、ウィンドウリサイズ時に全体を再計算する必要がある。zellij-server/src/panes/tiled_panes/pane_resizer.rsを開く。use cassowary::{    strength::{REQUIRED, STRONG},    Expression, Solver, Variable,    WeightedRelation::EQ,};pub struct PaneResizer<'a> {    panes: Rc<RefCell<HashMap<PaneId, &'a mut Box<dyn Pane>>>>,    vars: HashMap<PaneId, Variable>,    solver: Solver,}// 制約を設定: 「固定サイズペイン」と「パーセンテージペイン」の両方に対応fn constrain_spans(space: usize, spans: &[Span]) -> HashSet<cassowary::Constraint> {    let mut constraints = HashSet::new();    // 全ペインの合計サイズは、利用可能なスペースと等しい（REQUIRED強度）    let full_size = spans        .iter()        .fold(Expression::from_constant(0.0), |acc, s| acc + s.size_var);    constraints.insert(full_size.clone() | EQ(REQUIRED) | space as f64);    // 固定サイズはREQUIRED、パーセンテージはSTRONGで制約    for span in spans {        match span.size.constraint {            Constraint::Fixed(s) => constraints.insert(span.size_var | EQ(REQUIRED) | s as f64),            Constraint::Percent(p) => constraints                .insert((span.size_var / new_flex_space as f64) | EQ(STRONG) | (p / 100.0)),        };    }    constraints}// 丸め誤差の分配: error.signum()で±1ずつペインサイズを調整for span in flex_spans {    rounded_sizes        .entry(span.size_var)        .and_modify(|s| *s += error.signum());    error -= error.signum();}Cassowaryは線形計画法を使った制約ソルバーだ。元々はmacOSのAuto Layoutに使われていたアルゴリズムで、それをペインレイアウトに応用している。REQUIREDとSTRONGの強度で優先度を管理するのが賢い。固定サイズのペインは絶対に守られ、パーセンテージ指定のペインは「できるだけ守る」という柔軟性を持つ。error.signum()で丸め誤差を1ピクセルずつ分配するのも秀逸だ。浮動小数点の計算結果を整数に変換すると、どうしても誤差が出る。その誤差を均等にばらまくことで、ギャップやオーバーラップを回避している。HyperlinkTracker: カーソルジャンプ検出によるURL追跡ターミナルでURLをクリック可能にするには、「文字列がURLかどうか」を検出する必要がある。しかし、ターミナルは1文字ずつ出力されるため、URLの開始と終了を正確に把握するのは難しい。zellij-server/src/panes/hyperlink_tracker.rsを開く。pub struct HyperlinkTracker {    buffer: String,    cursor_positions: Vec<HyperlinkPosition>,  // 各文字のカーソル位置を記録    start_position: Option<HyperlinkPosition>,    last_cursor: Option<HyperlinkPosition>,    // カーソルジャンプ検出用}// カーソルが「連続的に移動していない」ことを検出fn should_reset_due_to_cursor_jump(&self, current_pos: &HyperlinkPosition) -> bool {    if let Some(last_pos) = &self.last_cursor {        let is_contiguous =            // 同一行の隣（通常の文字出力）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x + 1) ||            // 改行（行の折り返し）            (current_pos.y == last_pos.y + 1 && current_pos.x == 0) ||            // 同じ位置（上書き）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x);        !is_contiguous    } else {        false    }}カーソルジャンプ = URLの中断と判定するのが面白い。例えば、https://example.comと出力される途中で、プロンプトに戻るためにカーソルが左上にジャンプしたら、URLは完了したと見なす。複数行にまたがるURLや、ターミナルの折り返しにも対応している。Sixel画像: 負の座標とオーバーラップ判定Sixelは、ターミナル内に画像を表示するための古い規格だ。Zellijはこれをサポートしているが、スクロール時の挙動が複雑になる。zellij-server/src/panes/sixel.rsを開く。#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Hash)]pub struct PixelRect {    pub x: usize,    pub y: isize,  // 負の値対応！スクロールバッファの上部に消えた画像    pub width: usize,    pub height: usize,}// 新しい画像が古い画像を完全に覆った場合、古い画像を削除for (image_id, pixel_rect) in &self.sixel_image_locations {    if let Some(intersecting_rect) = pixel_rect.intersecting_rect(&image_size_and_coordinates) {        if intersecting_rect.x == pixel_rect.x            && intersecting_rect.y == pixel_rect.y            && intersecting_rect.height == pixel_rect.height            && intersecting_rect.width == pixel_rect.width        {            self.image_ids_to_reap.push(*image_id);  // 完全に覆われた→削除予定        }    }}y: isizeが興味深い。スクロールで画像がバッファの上部に消えると、yが負の値になる。usizeではなくisizeにすることで、この状況を型で表現している。完全にオーバーラップした画像は自動でメモリ解放される。Sixel画像は計算コストが高いため、不要な画像を積極的に削除するのは合理的だ。ダブルクリック検出: Doherty Thresholdマウスのダブルクリック検出には、時間閾値が必要だ。ZellijはDoherty Thresholdという値を使っている。zellij-server/src/panes/grid.rsを開く。const CLICK_TIME_THRESHOLD: u128 = 400;  // Doherty Thresholdimpl Click {    pub fn record_click(&mut self, position: Position) {        let click_is_same_position = self.position_and_time            .map(|(p, _t)| p == position)            .unwrap_or(false);        let click_is_within_time_threshold = self.position_and_time            .map(|(_p, t)| t.elapsed().as_millis() <= CLICK_TIME_THRESHOLD)            .unwrap_or(false);        if click_is_same_position && click_is_within_time_threshold {            self.count += 1;        } else {            self.count = 1;        }        if self.count == 4 {            self.reset();  // 3クリックまで（単語選択、行選択、段落選択）        }    }}400msという数字は、1982年のDoherty & Kelisky論文に由来する。「ユーザーがシステムの反応を待てる限界」とされる時間だ。3クリックまで対応しているのも面白い。1クリック=カーソル移動、2クリック=単語選択、3クリック=行選択。4クリック目でリセットされる。クライアント側のターミナル制御PTYの実装に入る前に、クライアント側の処理を見ておく必要がある。ユーザーのキー入力がサーバーに届くまでの道筋——つまり、データフローの入口だ。Raw Mode vs Cooked Mode追加知識セクションでtermios構造体とRaw Modeの概念を紹介した。Zellijの実装では、具体的にどうRaw Modeに入るのか。通常のターミナルは「Cooked Mode」で動作する。カーネルが行編集（バックスペース、Ctrl+Wなど）を処理し、Enterで1行ずつアプリケーションに渡す。Ctrl+Cを押すとSIGINTが送られる。Zellijはこれを無効にする必要がある。すべてのキー入力を自分で処理したいからだ。zellij-client/src/os_input_output.rsを開く。fn into_raw_mode(pid: RawFd) {    let mut tio = termios::tcgetattr(pid).expect("could not get terminal attribute");    termios::cfmakeraw(&mut tio);    match termios::tcsetattr(pid, termios::SetArg::TCSANOW, &tio) {        Ok(_) => {},        Err(e) => panic!("error {:?}", e),    };}fn unset_raw_mode(pid: RawFd, orig_termios: termios::Termios) -> Result<(), nix::Error> {    termios::tcsetattr(pid, termios::SetArg::TCSANOW, &orig_termios)}cfmakeraw() は以下を無効にする。ECHO: 入力文字のエコーバックICANON: 行単位の入力（カノニカルモード）ISIG: Ctrl+C/Ctrl+Zによるシグナル生成IXON/IXOFF: ソフトウェアフロー制御（Ctrl+S/Ctrl+Q）TCSANOW は「今すぐ適用」を意味する。TCSADRAIN（出力完了後）やTCSAFLUSH（バッファ破棄）もあるが、入力処理では即座の適用が必要だ。元のtermiosを保存しておき、終了時に復元する。これを忘れると、Zellij終了後にターミナルが壊れた状態になる。SIGWINCHのスロットリングターミナルウィンドウをリサイズすると、OSはSIGWINCHを送る。問題は、GUIウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のシグナルが発火することだ。fn handle_signals(&self, sigwinch_cb: Box<dyn Fn()>, quit_cb: Box<dyn Fn()>) {    let mut sigwinch_cb_timestamp = time::Instant::now();    let mut signals = Signals::new(&[SIGWINCH, SIGTERM, SIGINT, SIGQUIT, SIGHUP]).unwrap();    for signal in signals.forever() {        match signal {            SIGWINCH => {                // SIGWINCHコールバックをスロットリング                if sigwinch_cb_timestamp.elapsed() < SIGWINCH_CB_THROTTLE_DURATION {                    thread::sleep(SIGWINCH_CB_THROTTLE_DURATION);                }                sigwinch_cb_timestamp = time::Instant::now();                sigwinch_cb();            },            SIGTERM | SIGINT | SIGQUIT | SIGHUP => {                quit_cb();                break;            },            _ => unreachable!(),        }    }}SIGWINCH_CB_THROTTLE_DURATIONは50msだ。リサイズイベントが来ても、前回から50ms経っていなければ待機する。これにより、毎秒数十回のレンダリングを防ぐ。50msという値は経験則だ。人間が「遅延」と感じる閾値（100ms）より短く、ターミナルが処理できる頻度（60fps = 16ms）より長い。同期出力（Synchronized Output）最新のターミナルエミュレータは「同期出力」をサポートしている。複数の出力をバッファリングし、まとめて画面に反映する機能だ。let synchronised_output = match os_input.env_variable("TERM").as_deref() {    Some("alacritty") => Some(SyncOutput::DCS),    _ => None,};// レンダリング時if let Some(sync) = synchronised_output {    stdout.write_all(sync.start_seq()).expect("cannot write to stdout");}stdout.write_all(output.as_bytes()).expect("cannot write to stdout");if let Some(sync) = synchronised_output {    stdout.write_all(sync.end_seq()).expect("cannot write to stdout");}DCS（Device Control String）で出力を囲む。ターミナルはDCS開始からDCS終了までの出力をバッファリングし、終了シーケンスを受け取った時点でまとめて描画する。これにより、レイアウト変更時の「ちらつき」が消える。中間状態（ペインが1つだけ描画された状態など）が画面に表示されない。現時点ではAlacrittyのみ対応だが、今後他のターミナルにも拡大されるだろう。Kitty Keyboard Protocol従来のターミナルは、Shift+F1とF13を区別できなかった。どちらも同じエスケープシーケンスを送るからだ。Kitty Keyboard Protocolはこの問題を解決する。zellij-client/src/stdin_handler.rsを開く。loop {    match os_input.read_from_stdin() {        Ok(buf) => {            // まずKitty Keyboard Protocolを試す            if !explicitly_disable_kitty_keyboard_protocol {                match KittyKeyboardParser::new().parse(&buf) {                    Some(key_with_modifier) => {                        send_input_instructions.send(...).unwrap();                        continue;                    },                    None => {},                }            }            // フォールバック: 標準のtermwiz InputParser            input_parser.parse(&buf, |input_event| { ... }, false);        },        // ...    }}Kitty Keyboard Protocolが使えるなら使い、使えなければ従来のANSIエスケープシーケンスにフォールバックする。この二段構えにより、古いターミナルでも動作しつつ、新しいターミナルでは拡張機能を活用できる。セッション切り替え時のSTDINバッファリングZellijは複数のセッションを持てる。セッション間を切り替えるとき、STDINの所有権を移す必要がある。fn read_from_stdin(&mut self) -> Result<Vec<u8>, &'static str> {    let session_name_at_calltime = { self.session_name.lock().unwrap().clone() };    let mut buffered_bytes = self.reading_from_stdin.lock().unwrap();    match buffered_bytes.take() {        Some(buffered_bytes) => Ok(buffered_bytes),        None => {            let stdin = std::io::stdin();            let mut stdin = stdin.lock();            let buffer = stdin.fill_buf().unwrap();            let length = buffer.len();            let read_bytes = Vec::from(buffer);            stdin.consume(length);            // セッションが変わったら、読んだバイトをバッファに戻す            let session_name_after_reading_from_stdin =                { self.session_name.lock().unwrap().clone() };            if session_name_at_calltime.is_some()                && session_name_at_calltime != session_name_after_reading_from_stdin            {                *buffered_bytes = Some(read_bytes);                Err("Session ended")            } else {                Ok(read_bytes)            }        },    }}問題: 旧セッションのスレッドがSTDINでブロックしている間に、新セッションがSTDINを必要とする。解決策: 読み取り前後でセッション名を比較する。セッションが変わっていたら、読んだバイトをバッファに保存し、新セッションのスレッドがそれを取得できるようにする。これはエッジケースだが、マルチセッション対応には必須の処理だ。PTY（疑似端末）の実装詳細ここまで見てきた境界付きチャネル、VTEパーサ、差分レンダリング、compile-time assertion——これらはすべて、PTYという土台の上に乗っている。チャネルはPTYからの出力を運び、VTEパーサはPTYが吐いたバイト列を解釈し、差分レンダリングはその結果を画面に描く。個別のパターンを追いかけてきたが、ここで全体が合流する。ターミナルマルチプレクサの核心部分であるPTY処理を深掘りする。ここがZellijの「心臓部」だ。そもそもTTYとPTYとは何かTTY（TeleTYpewriter）は、歴史的にはテレタイプ端末を指す。現代では「ターミナルデバイス」の総称として使われる。/dev/ttyや/dev/tty1がこれにあたる。PTY（Pseudo-TTY）は「疑似端末」だ。物理的な端末がなくても、ソフトウェア的にターミナルをエミュレートする仕組み。sshやtmux、そしてZellijはPTYを使っている。PTYはマスターとスレーブのペアで構成される。[Zellij Server] ←→ [PTY Master] ←→ [PTY Slave] ←→ [Shell/App]                    (制御側)         (端末側)マスター側: Zellijが持つ。ここに書き込むと、スレーブ側のSTDINに届く。スレーブの出力はここから読めるスレーブ側: シェル（bash/zsh）が持つ。通常のターミナルと同じように振る舞うシェルから見ると、スレーブPTYは「本物のターミナル」に見える。ttyコマンドを実行すると/dev/pts/0のようなパスが返る。これがスレーブPTYだ。Zellijがペインを作るたびに、新しいPTYペアが生成される。3ペインあれば、3つのPTYマスターをZellijが管理している。なぜPTYが必要なのか「パイプでいいのでは？」と思うかもしれない。しかし、パイプとPTYには決定的な違いがある。ジョブコントロール: PTYは「制御端末」として機能する。Ctrl+Zでプロセスを停止したり、fg/bgで制御したりできるのは、PTYがあるからだ。パイプにはこの機能がないウィンドウサイズ: PTYはサイズ（行数・列数）を持つ。$COLUMNSや$LINESはPTYから取得される。パイプにはサイズの概念がない行編集: シェルは「端末があるかどうか」で挙動を変える。isatty()がtrueを返すと、プロンプトを表示し、行編集を有効にする。パイプ経由だとこれが無効になるシグナル: Ctrl+CでSIGINTを送れるのは、PTYが「フォアグラウンドプロセスグループ」を管理しているからだつまり、PTYなしには「ターミナルらしい体験」が成り立たない。PTYの作成フローzellij-server/src/os_input_output.rsを開く。fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // Zellij側（ホスト）    let pid_secondary = open_pty_res.slave;   // シェル側（子プロセス）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ", VERSION)            .env("ZELLIJ_SESSION_NAME", &*SESSION_NAME)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // 子プロセスの終了を監視するスレッドを起動    std::thread::spawn({        move || {            child.wait().ok();            let exit_status = child.try_wait().ok().flatten().and_then(|e| e.code());            quit_cb(PaneId::Terminal(terminal_id), exit_status, cmd);        }    });    Ok((pid_primary, child.id() as RawFd))}openpty() はマスターとスレーブの2つのファイルディスクリプタを作る。Zellijはマスター側を持ち、シェル（bashやzsh）はスレーブ側を持つ。login_tty() は伝統的なUnix関数で、3つのことをする。setsid() で新しいセッションを作成スレーブPTYをcontrolling terminalに設定dup2() でstdin/stdout/stderrをスレーブに接続close_fds::close_open_fds(3, &[]) が面白い。ファイルディスクリプタ3以降を全て閉じる。これにより、親プロセスから継承した不要なFDがリークしない。環境変数の設定も注目に値する。ZELLIJ_PANE_IDを設定することで、子プロセス側から「自分がどのペインで動いているか」を知ることができる。シェルスクリプトやプラグインで使える。読み取りと書き込みの分離PTYへの読み書きは、別々のスレッドで行われる。なぜか。zellij-server/src/terminal_bytes.rsを開く。pub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    ))                    .await?;                },            }        }        // ループ終了後、最終レンダリングを要求        let _ = self.async_send_to_screen(ScreenInstruction::Render).await;        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのサイズだ。大量のログ出力に対応するため。Ok(0)とErrの区別が重要。Ok(0)はEOF（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス正常終了時にエラーログが出てしまう。zellij-server/src/pty_writer.rsを開く。pub(crate) fn pty_writer_main(bus: Bus<PtyWriteInstruction>) -> Result<()> {    loop {        let (event, _err_ctx) = bus.recv()?;        match event {            PtyWriteInstruction::Write(bytes, terminal_id) => {                if let Some(raw_fd) = bus                    .os_input                    .as_ref()                    .and_then(|os_input| os_input.get_terminal_id_from_fd(terminal_id))                {                    let mut f = unsafe { File::from_raw_fd(*raw_fd) };                    if f.write_all(&bytes).is_ok() {                        let _ = f.flush();                    }                    std::mem::forget(f);  // FDを閉じないようにする                }            },            PtyWriteInstruction::Exit => break,        }    }    Ok(())}読み取りと書き込みを分離する理由は、デッドロック回避だ。Vimのようなプログラムは、STDINからの入力を待ちながらSTDOUTに出力する。もし同じスレッドで読み書きをすると、「Vimが入力を待っている」「Zellijが出力を待っている」という状態でデッドロックになる可能性がある。std::mem::forget(f) も興味深い。File::from_raw_fd()で作ったFileは、dropされるとFDが閉じてしまう。forget()でdropを防いでいる。forgetという名前は不穏だ。普通、忘れることは悪いことだ。しかしRustの所有権モデルでは、意図的に忘れることが正しい選択になる場合がある。FDを閉じたくないなら、Fileがdropされることを忘れさせる。記憶と忘却の関係が、ここでは逆転している。tcdrainによるフロー制御書き込みスレッドには、もう一つ重要な処理がある。PtyWriteInstruction::Write(bytes, terminal_id) => {    os_input        .write_to_tty_stdin(terminal_id, &bytes)        .with_context(err_context)        .non_fatal();    os_input        .tcdrain(terminal_id)  // ここ        .with_context(err_context)        .non_fatal();},tcdrain() は、書き込んだデータが全て送信されるまでブロックする。なぜこれが必要か。PTYにはカーネル内部にバッファがある。write()はバッファに書き込んだ時点で返る。しかし、相手側（シェル）がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。これにより、次の書き込みが前の書き込みを追い越すことを防ぐ。fn tcdrain(&self, terminal_id: u32) -> Result<()> {    match self        .terminal_id_to_raw_fd        .lock()        .to_anyhow()        .with_context(err_context)?        .get(&terminal_id)    {        Some(Some(fd)) => termios::tcdrain(*fd).with_context(err_context),        _ => Err(anyhow!("could not find raw file descriptor")).with_context(err_context),    }}ソースコードのコメントには、こうある。「VimのようなプログラムはSTDINに書き込みながらSTDOUTを読むとデッドロックする」。Vimへの言及が具体的で、実際に遭遇した問題なのだろう。ウィンドウリサイズの処理ターミナルをリサイズしたとき、PTYにサイズ変更を伝える必要がある。zellij-server/src/os_input_output.rsを開く。fn set_terminal_size_using_terminal_id(    &self,    id: u32,    cols: u16,    rows: u16,    width_in_pixels: Option<u16>,    height_in_pixels: Option<u16>,) -> Result<()> {    // リサイズをキャッシュ（複数のリサイズイベントを1つにまとめる）    match self.cached_resizes.lock() {        Ok(mut cached_resizes) => {            let cached_resizes = cached_resizes.get_or_insert_with(BTreeMap::new);            cached_resizes.insert(id, (cols, rows, width_in_pixels, height_in_pixels));        },        Err(e) => {            log::error!("Failed to cache resize: {}", e);        },    }    // キャッシュを適用    self.apply_cached_resizes()}fn apply_cached_resizes(&self) -> Result<()> {    if let Some(cached_resizes) = self.cached_resizes.lock().ok().as_mut().and_then(|c| c.take()) {        for (terminal_id, (cols, rows, width_in_pixels, height_in_pixels)) in cached_resizes {            let ws = Winsize {                ws_row: rows,                ws_col: cols,                ws_xpixel: width_in_pixels.unwrap_or(0),                ws_ypixel: height_in_pixels.unwrap_or(0),            };            if let Some(raw_fd) = self.get_terminal_id_from_fd(terminal_id) {                set_terminal_size_using_fd(*raw_fd, ws);            }        }    }    Ok(())}fn set_terminal_size_using_fd(fd: RawFd, ws: Winsize) {    // TIOCSWINSZ ioctlでPTYにサイズを伝える    if let Err(e) = ioctl_set_window_size(fd, &ws) {        log::error!("Failed to set terminal size: {}", e);    }}リサイズのキャッシングが重要だ。なぜか。ソースコードのコメントに理由がある。「レイアウト計算のコードには、複数のリサイズを送信してしまうロジックの罠がある。最後の1つだけが正しいのだが、多くのプログラムやシェルはリサイズをデバウンスする（GUIウィンドウのリサイズに対処してきたトラウマだろう）。これがグリッチや描画漏れを引き起こす」。つまり、VimやZshは「リサイズが連続で来たら、最後の1つだけ処理する」という防御策を持っている。Zellijが中間のリサイズも送ると、シェル側のデバウンスと競合してしまう。だからZellij側でもキャッシュし、最後の1つだけを送る。PtyWriteInstruction::StartCachingResizes => {    // レイアウト再計算中はリサイズをキャッシュ    os_input.cache_resizes();},PtyWriteInstruction::ApplyCachedResizes => {    // 計算完了後、最後のリサイズだけを適用    os_input.apply_cached_resizes();},TIOCSWINSZ（Terminal I/O Control Set WINdow SiZe）は、PTYにサイズを伝えるioctlだ。これを呼ぶと、カーネルは子プロセスグループにSIGWINCHを送る。子プロセスはこのシグナルを受けて、画面を再描画する。ws_xpixelとws_ypixel はSixel画像のために使われる。文字単位のサイズ（cols/rows）に加えて、ピクセル単位のサイズも伝える。これにより、画像を正確な解像度で表示できる。プロセス終了の検出とシグナルペインを閉じるとき、子プロセスにシグナルを送る必要がある。pub fn close_pane(&mut self, id: PaneId) -> Result<()> {    match id {        PaneId::Terminal(id) => {            self.task_handles.remove(&id);  // 読み取りタスクを停止            if let Some(child_fd) = self.id_to_child_pid.remove(&id) {                task::block_on(async {                    self.bus                        .os_input                        .as_mut()                        .fatal()                        .kill(Pid::from_raw(child_fd))  // SIGHUPを送信                        .fatal();                });            }        },        PaneId::Plugin(pid) => {            // プラグインはUnload命令を送るだけ            drop(self.bus.senders.send_to_plugin(PluginInstruction::Unload(pid)));        },    }    Ok(())}シグナルの送信順序も工夫されている。// SIGTERMを3回試行し、それでも終了しなければSIGKILLfor _ in 0..3 {    if nix::sys::signal::kill(pid, Signal::SIGTERM).is_ok() {        std::thread::sleep(Duration::from_millis(10));        // プロセスが終了したかチェック        if nix::sys::wait::waitpid(pid, Some(WaitPidFlag::WNOHANG)).is_ok() {            return Ok(());        }    }}// 3回失敗したらSIGKILLnix::sys::signal::kill(pid, Signal::SIGKILL)?;SIGTERM 3回 → SIGKILL。プロセスに「正常終了」の機会を与えつつ、応答しなければ強制終了する。10msのポーリング間隔も絶妙だ。CWD（カレントディレクトリ）の追跡ペインのカレントディレクトリを追跡するのは、意外と難しい。fn get_cwd(&self, pid: Pid) -> Option<PathBuf> {    // /proc/[pid]/cwd を読み取る    let path = format!("/proc/{}/cwd", pid);    std::fs::read_link(path).ok()}pub fn update_and_report_cwds(&mut self) {    let terminal_ids: Vec<u32> = self.id_to_child_pid.keys().copied().collect();    let pids: Vec<_> = terminal_ids        .iter()        .filter_map(|id| self.id_to_child_pid.get(&id))        .map(|pid| Pid::from_raw(*pid))        .collect();    // 全ペインのCWDを一括取得    let (pids_to_cwds, _) = self        .bus        .os_input        .as_ref()        .map(|os_input| os_input.get_cwds(pids))        .unwrap_or_default();    // 変更があればクライアントに通知    for terminal_id in terminal_ids {        let cwd = /* ... */;        if self.terminal_cwds.get(&terminal_id) != Some(cwd) {            // CWD変更イベントを送信        }    }}/proc/[pid]/cwd はLinux固有の仕組みだ。プロセスのカレントディレクトリへのシンボリックリンクになっている。これを定期的にチェックすることで、cdコマンドによるディレクトリ変更を検出できる。データフローの全体像PTYを通じたデータの流れを図にすると、以下のようになる。[ユーザー入力]     ↓ キー入力[Client Process]     ↓ IPC (Unix Domain Socket)[Server Process]     ↓ PtyWriteInstruction::Write[PTY Writer Thread]     ↓ write() to master FD[PTY (カーネル)]     ↓[Shell/アプリケーション]     ↓ 出力[PTY (カーネル)]     ↓ read() from master FD[TerminalBytes::listen()]     ↓ ScreenInstruction::PtyBytes[Screen Thread]     ↓ VTEパーサ[Grid構造体]     ↓ 差分レンダリング[Client Process]     ↓ IPC[ユーザーの画面]6つのスレッドが協調して動いている。PTY Thread: PTYの作成・管理PTY Writer Thread: PTYへの書き込みTerminalBytes（async task）: PTYからの読み取りScreen Thread: VTEパース、レンダリングPlugin Thread: WASMプラグイン実行Background Thread: 非同期タスクこの分離により、どこか1つがブロックしても他の処理は継続できる。Vimが入力を待っている間も、他のペインは正常に動作する。実装の詳細：システムコールのシーケンスここまで概念的な説明が多かった。実際にどのシステムコールがどの順序で呼ばれるのか、具体的に見ていこう。PTY作成シーケンス1. openpty(None, &orig_termios)   → OpenptyResult { master: RawFd, slave: RawFd }2. fork() [Command::spawn()が内部で呼ぶ]3. 子プロセス（シェル側）:   - libc::login_tty(slave)     - setsid()      // 新しいセッション作成     - ioctl(slave, TIOCSCTTY)  // 制御端末として設定     - dup2(slave, 0)  // stdin     - dup2(slave, 1)  // stdout     - dup2(slave, 2)  // stderr   - close_open_fds(3, &[])  // FD 3以降を全て閉じる   - exec("/bin/bash")  // シェルを起動4. 親プロセス（Zellij側）:   - master FDを保存   - 子プロセスのPIDを保存   - 非同期読み取りタスクを起動   - シグナルハンドラスレッドを起動PTY読み取りシーケンス1. async_reader.read(&mut buf[65536])   → read(master_fd, buf, 65536) syscall   → bytes受信 or EOF(0) or error2. ScreenInstruction::PtyBytes(terminal_id, bytes)   をチャネル経由でScreenスレッドに送信3. Screenスレッドがbytesを受信   → VTEパーサに渡す   → Gridを更新PTY書き込みシーケンス1. PtyWriteInstruction::Write(bytes, terminal_id)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. write(master_fd, bytes) syscall   → バイトがPTYバッファに書き込まれる4. tcdrain(master_fd) syscall   → 書き込みが完了するまで待機ターミナルリサイズシーケンス1. PtyWriteInstruction::ResizePty(terminal_id, cols, rows, ...)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. ioctl(master_fd, TIOCSWINSZ, &Winsize { ws_col, ws_row, ... })   → カーネルがSIGWINCHを子プロセスグループに送信   → シェルが再描画実装の詳細：スレッドの起動サーバープロセスの起動時、複数のスレッドが生成される。zellij-server/src/lib.rsを開く。pub fn start_server(mut os_input: Box<dyn ServerOsApi>, socket_path: PathBuf) {    // チャネルの作成    let (to_server, server_receiver): ChannelWithContext<ServerInstruction> =        channels::bounded(50);    let (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =        channels::bounded(50);    let (to_pty, pty_receiver): ChannelWithContext<PtyInstruction> =        channels::bounded(50);    let (to_plugin, plugin_receiver): ChannelWithContext<PluginInstruction> =        channels::bounded(50);    let (to_pty_writer, pty_writer_receiver): ChannelWithContext<PtyWriteInstruction> =        channels::unbounded();  // 書き込みは無制限    // PTY Writerスレッド    thread::Builder::new()        .name("pty_writer".to_string())        .spawn(move || {            pty_writer_main(pty_writer_bus).fatal();        })        .unwrap();    // PTYスレッド    thread::Builder::new()        .name("pty".to_string())        .spawn(move || {            pty_thread_main(pty_bus, pty_receiver).fatal();        })        .unwrap();    // Screenスレッド    thread::Builder::new()        .name("screen".to_string())        .spawn(move || {            screen_thread_main(screen_bus, screen_receiver).fatal();        })        .unwrap();    // Pluginスレッド    thread::Builder::new()        .name("plugin".to_string())        .spawn(move || {            plugin_thread_main(plugin_bus, plugin_receiver).fatal();        })        .unwrap();    // Serverスレッド（メインループ）    loop {        let (instruction, err_ctx) = server_receiver.recv().expect("...");        match instruction {            ServerInstruction::NewClient(client_attributes, ...) => { ... },            ServerInstruction::Render(serialized_output) => { ... },            ServerInstruction::UnblockInputThread => { ... },            ServerInstruction::ClientExit(client_id) => { ... },            ServerInstruction::KillSession => break,            // ...        }    }}注目すべきは、PTY Writerのチャネルだけunbounded()になっている点だ。他はbounded(50)でバックプレッシャーをかけているが、書き込みはブロックさせたくない。ユーザーの入力を遅延させると体感が悪くなるからだ。実装の詳細：キーボード入力からシェルまでの経路ユーザーがキーを押してからシェルに届くまでの、実際のコード経路を追う。1. クライアントがキー入力を受信zellij-client/src/stdin_handler.rsloop {    match os_input.read_from_stdin() {        Ok(buf) => {            // KittyプロトコルまたはANSIエスケープをパース            input_parser.parse(&buf, |input_event| {                send_input_instructions                    .send(InputInstruction::KeyEvent(                        input_event.clone(),                        buf.to_vec(),                    ))                    .unwrap();            }, false);        },        // ...    }}2. クライアントがサーバーにメッセージ送信zellij-client/src/lib.rsInputInstruction::KeyEvent(key_event, raw_bytes) => {    send_client_instructions        .send(ClientInstruction::Action(            Action::Write(None, raw_bytes, false),            None,            None,        ))        .unwrap();}3. サーバーのRouteスレッドがメッセージ受信zellij-server/src/route.rsfn route_action(action: Action, ...) -> bool {    match action {        Action::Write(_, bytes, _) => {            session                .senders                .send_to_screen(ScreenInstruction::WriteCharacter(bytes, client_id))                .unwrap();        },        // ...    }}4. Screenスレッドがペインに書き込み指示zellij-server/src/screen.rsScreenInstruction::WriteCharacter(bytes, client_id) => {    let active_tab = screen.get_active_tab_mut(client_id).unwrap();    active_tab.write_to_terminal(bytes, client_id);}5. TabがPTY Writerに書き込み指示zellij-server/src/tab/mod.rspub fn write_to_active_terminal(&mut self, bytes: Vec<u8>, client_id: ClientId) {    if let Some(active_pane_id) = self.get_active_pane_id(client_id) {        if let PaneId::Terminal(terminal_id) = active_pane_id {            self.senders                .send_to_pty_writer(PtyWriteInstruction::Write(bytes, terminal_id))                .unwrap();        }    }}6. PTY Writerスレッドが実際に書き込みzellij-server/src/pty_writer.rsPtyWriteInstruction::Write(bytes, terminal_id) => {    os_input.write_to_tty_stdin(terminal_id, &bytes)?;    os_input.tcdrain(terminal_id)?;}7. OSレイヤーでシステムコールzellij-server/src/os_input_output.rsfn write_to_tty_stdin(&self, terminal_id: u32, buf: &[u8]) -> Result<usize> {    let fd = self.terminal_id_to_raw_fd.lock()?.get(&terminal_id)?;    let mut file = unsafe { File::from_raw_fd(*fd) };    let result = file.write(buf);    std::mem::forget(file);  // FDを閉じない    result}この経路で、キーボード入力は6つのコンポーネントを経由してシェルに届く。各コンポーネント間はチャネルで接続されている。実装の詳細：シェル出力から画面までの経路逆方向、シェルの出力が画面に表示されるまでの経路。1. TerminalBytesが非同期で読み取りzellij-server/src/terminal_bytes.rspub async fn listen(&mut self) -> Result<()> {    let mut buf = [0u8; 65536];    loop {        match self.async_reader.read(&mut buf).await {            Ok(0) => break,  // EOF            Ok(n_bytes) => {                let bytes = &buf[..n_bytes];                self.async_send_to_screen(ScreenInstruction::PtyBytes(                    self.terminal_id,                    bytes.to_vec(),                )).await?;            },            Err(err) => {                log::error!("{}", err);                break;            },        }    }    Ok(())}2. Screenスレッドがバイトを受信zellij-server/src/screen.rsScreenInstruction::PtyBytes(terminal_id, bytes) => {    // terminal_idからペインを特定    if let Some(tab) = screen.get_tab_with_terminal_id(terminal_id) {        tab.handle_pty_bytes(terminal_id, bytes);    }}3. TabがVTEパーサに渡すzellij-server/src/tab/mod.rspub fn handle_pty_bytes(&mut self, terminal_id: u32, bytes: Vec<u8>) {    if let Some(pane) = self.panes.get_mut(&PaneId::Terminal(terminal_id)) {        pane.handle_pty_bytes(bytes);    }}4. TerminalPaneがGridを更新zellij-server/src/panes/terminal_pane.rsfn handle_pty_bytes(&mut self, bytes: Vec<u8>) {    self.grid.advance_by_bytes(bytes);}5. GridがVTEパーサを実行zellij-server/src/panes/grid.rspub fn advance_by_bytes(&mut self, bytes: Vec<u8>) {    for byte in bytes {        self.vte_parser.advance(&mut *self, byte);    }}ここでvte_parserはvte::Parser型だ。Gridはvte::Performトレイトを実装しており、パース結果に応じてprint()、execute()、csi_dispatch()などが呼ばれる。6. レンダリングとクライアントへの送信ScreenInstruction::Render => {    screen.render()?;    // 各クライアントに差分を送信    for client_id in screen.connected_clients.keys() {        let output = screen.render_for_client(*client_id)?;        send_to_client(*client_id, ServerToClientMsg::Render(output))?;    }}使用しているシステムコール一覧Zellijが使用する主なシステムコールをまとめる。 システムコール  用途  使用箇所  openpty()  PTYペアの作成  os_input_output.rs  fork()  子プロセスの作成  Command::spawn()  setsid()  新セッションの作成  login_tty() 内部  dup2()  FDの複製  login_tty() 内部  exec()  プログラムの実行  Command::spawn()  read()  PTYからの読み取り  terminal_bytes.rs  write()  PTYへの書き込み  pty_writer.rs  ioctl(TIOCGWINSZ)  ウィンドウサイズ取得  os_input_output.rs  ioctl(TIOCSWINSZ)  ウィンドウサイズ設定  os_input_output.rs  tcgetattr()  termios取得  os_input_output.rs  tcsetattr()  termios設定  os_input_output.rs  tcdrain()  出力完了待機  pty_writer.rs  kill()  シグナル送信  os_input_output.rs  waitpid()  子プロセス待機  os_input_output.rs  close()  FDのクローズ  各所 エラーハンドリングの実装PTY関連のエラーは、致命的なものと非致命的なものに分類される。// 非致命的: ログを出すが処理を継続os_input    .write_to_tty_stdin(terminal_id, &bytes)    .with_context(err_context)    .non_fatal();// 致命的: パニックまたは終了os_input    .spawn_terminal(cmd, quit_cb)    .with_context(err_context)    .fatal();non_fatal()は書き込みエラー、リサイズエラーなどに使われる。ペインが閉じられた後に書き込みが来ることがあり、これは正常な動作だ。fatal()はPTY作成失敗、サーバー初期化失敗などに使われる。これらは回復不能なエラーだ。コマンドが見つからない場合の処理も興味深い。fn command_exists(cmd: &RunCommand) -> bool {    // cwdからの相対パスをチェック    if let Some(cwd) = cmd.cwd.as_ref() {        let full_command = cwd.join(&cmd.command);        if full_command.exists() && full_command.is_file() {            return true;        }    }    // PATHを検索    if let Some(paths) = env::var_os("PATH") {        for path in env::split_paths(&paths) {            let full_command = path.join(&cmd.command);            if full_command.exists() && full_command.is_file() {                return true;            }        }    }    false}コマンドが存在しない場合、ZellijError::CommandNotFoundが返される。hold_on_closeが設定されていれば、エラーメッセージを表示したペインが残る。設定されていなければ、ペインは静かに閉じられる。KDL形式によるセッション永続化Zellijは1秒ごとにセッション状態を自動シリアライズする。保存先は~/.cache/zellij/<VERSION>/session_info/<SESSION_NAME>/だ。zellij-utils/src/session_serialization.rsを開く。// zellij-utils/src/session_serialization.rspub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut layout_node = KdlNode::new("layout");    // タブ、ペインの構造をKDLノードとして構築}生成されるKDL。layout {    cwd "/home/user/project"    tab name="Editor" {        pane command="vim" {            args "src/main.rs"            cwd "/home/user/project"        }    }    tab name="Shell" {        pane    }}シリアライズ形式がそのまま有効なKDLレイアウトファイルになっている。これは特筆すべき設計だ。なぜJSONやバイナリ形式ではないのか。JSONはパース速度が速いが、人間が編集するには冗長だ。KDLは「人間が読める設定ファイル」として設計された言語であり、Zellijのレイアウト設定にも使われている。つまり設定ファイルとシリアライズ形式を統一することで、自動保存されたセッションをそのままレイアウトテンプレートとして再利用できる。tmuxの.tmux.confとセッション復元は別系統だ。設定ファイルでは「こうあるべき」を書き、セッション復元では「こうだった」を読む。Zellijはこの2つを統一している。シンプルだが、これを思いつくのは難しい。thiserror + anyhow の併用前編でFatalErrorトレイトを紹介した。後編では、エラー型の定義側を見る。// thiserrorによる型定義#[derive(Debug, Error)]pub enum ZellijError {    #[error("could not find command '{command}' for terminal {terminal_id}")]    CommandNotFound { terminal_id: u32, command: String },    #[error("Client {client_id} is too slow to handle incoming messages")]    ClientTooSlow { client_id: u16 },    #[error("an error occured")]    GenericError { source: anyhow::Error },}thiserror（ライブラリ向けエラー型定義）とanyhow（アプリケーション向けエラー伝播）の併用だ。GenericErrorのフィールドがanyhow::Errorになっている点に注目してほしい。これにより、詳細なエラー型を定義したいケースと、「とりあえずエラーを伝播したい」ケースを両立できる。その他の興味深い実装パターンここまでで主要なパターンを見てきたが、ソースコードを読み進める中で発見した「細かいが面白い」実装を紹介する。スクロールバックの「Canonical Rows」アーキテクチャターミナルの行は、表示上は複数行でも論理的には1行であることがある。長いコマンドが折り返されるケースだ。Zellijはこれを「Canonical Row」という概念で管理している。zellij-server/src/panes/grid.rsを開く。pub struct Row {    pub columns: VecDeque<TerminalCharacter>,    pub is_canonical: bool,  // 本当の改行か、折り返しか    width: Option<usize>,    // キャッシュされた幅}is_canonicalフラグが鍵だ。trueなら本当の改行（ユーザーがEnterを押した）、falseなら表示上の折り返し。スクロール時、折り返された行は元の「親」行と一緒に移動する必要がある。from_rowsメソッドで複数行をマージし、split_to_rows_of_lengthで再分割する。pub fn split_to_rows_of_length(&mut self, max_row_length: usize) -> Vec<Row> {    let mut parts: Vec<Row> = vec![];    let mut current_part: VecDeque<TerminalCharacter> = VecDeque::new();    let mut current_part_len = 0;    for character in self.columns.drain(..) {        if current_part_len + character.width() > max_row_length {            parts.push(Row::from_columns(current_part));            current_part = VecDeque::new();            current_part_len = 0;        }        current_part_len += character.width();        current_part.push_back(character);    }    // canonical statusを保持    parts}文字の幅を考慮している点に注目。絵文字（幅2）の途中で行を切ると表示が崩れる。character.width()でUnicode幅を取得し、正しい位置で分割している。スクロールバックは3つのバッファで管理される。pub(crate) lines_above: VecDeque<Row>,    // スクロールバック（上）pub(crate) viewport: Vec<Row>,             // 表示中pub(crate) lines_below: Vec<Row>,          // 未表示（下）lines_aboveには上限がある。無限にスクロールバックを溜めるとメモリを食い尽くす。fn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();        if let Some(line) = line {            sixel_grid.offset_grid_top();  // Sixel画像の位置も更新        }    }    vec.push_back(value);}古い行を削除するとき、Sixel画像の位置も調整している。画像は行番号で位置を管理しているため、行が消えると座標がずれる。この連携が面白い。幅を考慮したカーソル位置計算絵文字やCJK文字は幅2を持つ。カーソルがその「途中」にあるケースをどう扱うか。pub fn absolute_character_index_and_position_in_char(&self, x: usize) -> (usize, usize) {    // xの幅を考慮したインデックスと、ワイド文字内の位置を返す    let mut accumulated_width = 0;    let mut absolute_index = x;    let mut position_inside_character = 0;    for (i, terminal_character) in self.columns.iter().enumerate() {        accumulated_width += terminal_character.width();        absolute_index = i;        if accumulated_width > x {            let character_start_position = accumulated_width - terminal_character.width();            position_inside_character = x - character_start_position;            break;        }    }    (absolute_index, position_inside_character)}2つの値を返すのがポイント。「何番目の文字か」と「その文字内のどの位置か」。カーソルが絵文字の右半分にあるとき、position_inside_characterは1になる。これにより、カーソル移動やテキスト選択が正しく動作する。OnceCellによるグローバル非同期ランタイム複数スレッドから同じTokioランタイムを使いたい。Mutexで包むと毎回ロックが必要になる。zellij-server/src/global_async_runtime.rsを開く。use once_cell::sync::OnceCell;use tokio::runtime::Runtime;static TOKIO_RUNTIME: OnceCell<Runtime> = OnceCell::new();pub fn get_tokio_runtime() -> &'static Runtime {    TOKIO_RUNTIME.get_or_init(|| {        tokio::runtime::Builder::new_multi_thread()            .worker_threads(4)            .thread_name("async-runtime")            .enable_all()            .build()            .expect("Failed to create tokio runtime")    })}OnceCellは初期化後はロック不要だ。最初の呼び出しでランタイムを作成し、以降は&'static Runtimeを返す。lazy_static!より明示的で、Arc<Mutex<>>より効率的。4ワーカースレッドという設定も興味深い。Zellijは6つの主要スレッドを持つが、非同期タスク用には4スレッドで十分と判断したのだろう。フローティングペインのZ-index管理フローティングペインは重なり合う。どのペインが「上」にあるかを管理する必要がある。zellij-server/src/panes/floating_panes/mod.rsを開く。pub struct FloatingPanes {    panes: BTreeMap<PaneId, Box<dyn Pane>>,    z_indices: Vec<PaneId>,  // レンダリング順序    pane_being_moved_with_mouse: Option<(PaneId, Position)>,    // 多くのRc<RefCell<>>フィールド}pub fn stack(&self) -> Option<FloatingPanesStack> {    if self.panes_are_visible() {        let layers: Vec<PaneGeom> = self            .z_indices            .iter()            .filter_map(|pane_id| self.panes.get(pane_id).map(|p| p.position_and_size()))            .collect();        // レンダリング順でレイヤーを返す    }}z_indicesはVecだ。最後の要素が最前面。ペインをクリックすると、そのIDが末尾に移動する。pub fn move_pane_to_front(&mut self, pane_id: &PaneId) {    if let Some(index) = self.z_indices.iter().position(|id| id == pane_id) {        self.z_indices.remove(index);        self.z_indices.push(*pane_id);    }}マウスイベントは逆順で処理される。最前面のペインから順にヒットテストし、最初にマッチしたペインがイベントを受け取る。これにより、重なったペインの下にあるペインはクリックを受け取らない。Rc<RefCell<>>の多用も特徴的だ。display_areaやviewportは複数のペインで共有される。Rustの所有権モデルでは、これを表現するのにRc<RefCell<>>が必要になる。プラグインホットリロードのデバウンスプラグインファイルが変更されたら自動で再読み込みしたい。しかし、ファイル保存時には複数のイベントが発火することがある。zellij-server/src/plugins/watch_filesystem.rsを開く。pub fn watch_filesystem(    senders: ThreadSenders,    zellij_cwd: &Path,) -> Result<Debouncer<RecommendedWatcher, FileIdMap>> {    let mut debouncer = new_debouncer(        Duration::from_millis(DEBOUNCE_DURATION_MS),  // 400ms        None,        move |result: DebounceEventResult| match result {            Ok(events) => {                let mut create_events = vec![];                let mut read_events = vec![];                let mut update_events = vec![];                let mut delete_events = vec![];                // イベントを分類してプラグイン更新命令を送信            }        }    )}400msのデバウンス。この値はDoherty Thresholdと同じだ。ファイルを保存すると、OSによっては「作成→書き込み→クローズ」と複数イベントが発火する。400ms待つことで、これらを1つのイベントにまとめる。イベントは4種類に分類される。create、read、update、delete。プラグインの更新にはこの区別が重要だ。新規作成と更新では、ロードの方法が異なる可能性がある。アクション完了の追跡とタイムアウト長時間実行されるアクションの完了をどう待つか。zellij-server/src/route.rsを開く。pub fn wait_for_action_completion(    receiver: oneshot::Receiver<ActionCompletionResult>,    action_name: &str,    wait_forever: bool,) -> ActionCompletionResult {    let runtime = get_tokio_runtime();    if wait_forever {        runtime.block_on(async {            match receiver.await { /* ... */ }        })    } else {        match runtime.block_on(async {            tokio::time::timeout(ACTION_COMPLETION_TIMEOUT, receiver).await        }) {            Ok(Ok(result)) => result,            Err(_) | Ok(Err(_)) => {                log::error!("Action {} did not complete within timeout", action_name);                ActionCompletionResult { exit_status: None, affected_pane_id: None }            }        }    }}oneshot + tokio::time::timeoutの組み合わせが優雅だ。oneshot::channelは一度だけ値を送信できるチャネル。アクション完了時に結果を送り、待機側はtimeoutでラップして無限待機を避ける。スレッドをスピンさせたり、タイマーを別途管理したりする必要がない。Tokioのエコシステムをうまく活用している。おわりに冒頭で、何も起きていないターミナルを眺めていた話をした。cat huge_log_file.logで200万行を流し込んでも、Zellijは固まらない。境界付きチャネルのバッファが満杯になれば、送信側が自動的にブロックされる。シンプルだが効果的だ。その仕組みを、後編では中から見てきた。この記事を書きながら気づいたことがある。持ち帰れるパターンは、少なくない。crossbeamの境界付きチャネル: 無制限のチャネルはメモリを食い尽くす可能性がある。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する読み取り/書き込みスレッドの分離: PTYやソケットを扱うとき、同じスレッドで読み書きするとデッドロックの危険がある。Zellijのようにスレッドを分けると安全だcompile-time assertionによるメモリサイズ保証: const _: [(); 16] = [(); std::mem::size_of::<T>()];で、将来の変更でサイズが増えることを防げるシグナル送信のリトライ戦略: SIGTERM 3回 → SIGKILLというパターンは、プロセス管理の定石として覚えておきたいKDL形式のセッション永続化: シリアライズ形式と設定形式を統一するアイデアは、CLIツールやエディタの開発に応用できるOnceCellによるグローバルリソース管理: 複数スレッドから共有リソースにアクセスするとき、OnceCellなら初期化後のロックが不要だoneshot + timeoutによる非同期待機: 長時間実行されるアクションの完了を待つとき、タイムアウト付きで待機するパターンは汎用的に使えるCanonical Rowsによる論理行管理: テキストエディタやターミナルを作るなら、「表示上の行」と「論理的な行」を区別する必要があるただ、正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。境界付きチャネルのバッファサイズ50は、Zellijの6スレッド構成に最適化された値だ。自分のプロジェクトでは違う値が正解かもしれない。たぶん、違う。「パターンを知っている」と「パターンを使いこなせる」の間には溝がある。その溝がどれくらい深いのか、ソースコードを読んだだけでは分からない。書いてみるしかない。書いて、つまずいて、またソースコードに戻る。そういうことの繰り返しなのだと思う。ただ、Zellijのソースコードを2本の記事にわたって読んできて、1つ確信したことがある。Rustでマルチスレッドアプリケーションを書くとき、最も重要なのは「どのスレッドが何を所有するか」の設計だ。Zellijの6スレッド構成は、この所有権の設計が明確だから成立している。読み取りスレッドはPTYのReadを所有し、書き込みスレッドはWriteを所有する。この分離がデッドロックを防ぎ、境界付きチャネルがバックプレッシャーを実現し、差分レンダリングが性能を出す。パターンの根底にあるのは、結局のところ所有権モデルだ。冒頭で眺めていた3つのペインを、今もう一度見る。左のVim、右上のテスト、右下のシェル。見え方が少し変わっている。PTYが3つ、境界付きチャネルがバッファサイズ50で繋がり、VTEパーサが毎秒数千バイトを解釈している。何も起きていないように見えるターミナルが、少しだけ騒がしく感じる。Zellijのソースコードを読みたいなら、以下のファイルが特に参考になる。PTY関連:zellij-server/src/os_input_output.rs - PTY作成、シグナル、リサイズ（1035行）zellij-server/src/pty.rs - PTYマネージャースレッド（2100行以上）zellij-server/src/terminal_bytes.rs - 非同期読み取り（110行）zellij-server/src/pty_writer.rs - 書き込みスレッド（89行）レンダリング関連:zellij-server/src/panes/terminal_character.rs - メモリ効率化とcompile-time assertionzellij-server/src/panes/grid.rs - VTEパーサ、マウスイベント（4000行以上）zellij-server/src/output/mod.rs - 差分レンダリング非同期・スレッド関連:zellij-server/src/global_async_runtime.rs - OnceCellによるTokioランタイム共有（17行）zellij-server/src/route.rs - アクション完了追跡とタイムアウトzellij-utils/src/channels.rs - エラーコンテキスト付きチャネルペイン管理:zellij-server/src/panes/floating_panes/mod.rs - フローティングペインとZ-indexzellij-server/src/panes/tiled_panes/pane_resizer.rs - Cassowary制約ソルバーその他:zellij-utils/src/session_serialization.rs - KDL形式のセッション永続化zellij-server/src/plugins/watch_filesystem.rs - プラグインホットリロードzellij-server/src/panes/search.rs - 折り返し行を跨ぐ検索git clone https://github.com/zellij-org/zellij.git# 境界付きチャネルの実装cat zellij-utils/src/channels.rs# バッファサイズ50の使用箇所grep -r "bounded(50)" zellij-*/src/tmuxの安定性に満足しているなら、無理にZellijに乗り換える必要はない。しかし、Rustでマルチスレッドアプリケーションを書くなら、Zellijのソースコードは一読の価値がある。30年の歴史を持つtmuxとは異なるアプローチで、同等以上のパフォーマンスを達成しようとする試み——その設計判断から学べることは多い。参考リンクZellij本体github.comzellij.devWASMランタイム移行（PR #4449）github.com使用しているクレートgithub.comgithub.comgithub.comgithub.comKDL（設定ファイル形式）kdl.dev参考記事zellij.devzellij.devhttps://zellij.dev/news/new-plugin-api/zellij.devhttps://zellij.dev/news/sixel-images-in-the-terminal/zellij.devパフォーマンス最適化poor.dev境界付きチャネル導入の背景github.comDoherty Thresholdlawsofux.com関連技術protobuf.devdocs.rsdocs.rs関連プロジェクトgithub.comgithub.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【初心者向け】Snowflakeデータパイプライン構築]]></title>
            <link>https://sreake.com/blog/construct-snowflake-pipeline/</link>
            <guid isPermaLink="false">https://sreake.com/blog/construct-snowflake-pipeline/</guid>
            <pubDate>Wed, 28 Jan 2026 14:22:29 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 本記事ではSnowflakeの機能のみを使って最低限のデータパイプラインを作成する方法をご紹介します。流れとしては、CSVファイルからSnowflakeにデータをインジェストし、生のデータをレポートに必要なター […]The post 【初心者向け】Snowflakeデータパイプライン構築 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（前編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</guid>
            <pubDate>Wed, 28 Jan 2026 09:17:50 GMT</pubDate>
            <content:encoded><![CDATA[はじめにターミナルで cat huge_log_file.log を実行した。画面が滝のように流れ始めた。Ctrl+Cを連打した。反応しない。画面はまだ流れている。椅子の背もたれに体を預けて、流れが止まるのを待った。Ciscoルーターの話もしたいが、それを始めるとどこまでも終わらなくなるので、ここでは話をしない。こういう場面に出くわすことがある。自分が打ったコマンドなのに、自分では止められない。出力が止まったあと、何事もなかったかのように次のコマンドを打つ。たぶん、みんなそうしている。自分もそうしてきた。そうしてきたのだが、ある日ふと気になった。この暴走を、ソフトウェアはどうやって止めているのだろう。Zellijは、ターミナルマルチプレクサだ。1つのターミナル画面を複数に分割し、複数のシェルを同時に操作できる。tmuxやscreenの現代版として、2021年にRustで開発が始まった。github.com本記事は2部構成の前編にあたる。前編では設計パターンを抽出し、後編ではさらに深く実装の内部に入る。読んで「なるほど」と思って、そのまま忘れる。たぶんそうなる。それでいい。合わなければ途中で離脱してもらって構わない。約10万行のコードベースから、設計が優れている箇所——と、正直「これでいいのか？」と思う箇所——を抜き出して紹介する。他人のコードを読んで「分かった」と言い切れる自信はない。分からないまま書いている部分もある。それでも、書くことにした。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜZellijのコードを読むのかターミナルマルチプレクサのコードを読む機会はそう多くない。しかし、Zellijには以下の理由で読む価値がある。実践的な並行処理: 複数のスレッドが協調して動く仕組みが、教科書的ではなく「本当に動くコード」として見られるWASMプラグインシステム: ブラウザ以外でWebAssemblyを使う実例として参考になるエラー処理の設計: 「このエラーは無視していい」「このエラーは致命的」を型で表現するパターンが秀逸コードを読み始める前に、Zellijが前提としている概念をいくつか整理しておく。知っている人は読み飛ばしてもらっていい。前提知識Zellijのコードを読み進める前に、いくつかの概念を押さえておくと理解が早い。正直に言えば、自分もこれらを「完全に理解している」とは言い難い。使ったことはある。使ったことはあるが、説明しろと言われると手が止まる。そういう概念を、改めて整理しておく。擬似端末（PTY）ターミナルマルチプレクサの根幹技術だ。PTY（Pseudo Terminal）は、物理的なターミナル装置をソフトウェアでエミュレートする仕組み。マスター側とスレーブ側のペアで構成され、マスター側がZellijのようなプログラム、スレーブ側がシェル（bashやzsh）になる。シェルは自分が本物のターミナルに接続されていると思い込んでいるが、実際にはZellijが間に入ってデータを仲介している。MPSCチャネルRustの標準ライブラリにあるstd::sync::mpscは「Multiple Producer, Single Consumer」の略だ。複数の送信者から1つの受信者にメッセージを送れる。Zellijでは各スレッドがこのチャネルでメッセージをやり取りしている。crossbeam-channelというクレートを使うとMPMC（Multiple Producer, Multiple Consumer）も実現できるが、Zellijは基本的にMPSCで設計されている。Actorモデル各スレッドを独立した「アクター」として扱い、共有メモリではなくメッセージパッシングで通信するパターン。ZellijのScreenThreadやPtyThreadはそれぞれがアクターとして振る舞い、enumで定義された命令（ScreenInstructionなど）を受け取って処理する。ロックの競合を避けやすく、デバッグもしやすい。WebAssembly（WASM）ブラウザで動くバイナリフォーマットとして生まれたが、サーバーサイドやCLIツールでも使われるようになった。Zellijはプラグインの実行環境としてWASMを採用している。プラグインがクラッシュしても本体には影響しない、言語に依存しない、といった利点がある。Zellijは当初wasmtimeをランタイムとして使用していたが、現在はwasmiに移行している。詳細は後述する。では、コードを見ていこう。ここから先は長い。覚悟してほしい——と言いたいところだが、自分も書きながら覚悟している。Cargo Workspace構成：最初に見るべきファイルソースコードを読むとき、私はまずCargo.tomlを開く。プロジェクトの全体像が分かるからだ。Zellijのルートディレクトリでlsすると、以下の構造が見える。zellij/├── zellij/           # エントリーポイント├── zellij-client/    # クライアント側├── zellij-server/    # サーバー側├── zellij-utils/     # 共有ユーティリティ├── zellij-tile/      # プラグインSDK├── default-plugins/  # 標準プラグイン└── xtask/            # ビルドタスクZellijはクライアント・サーバー型のアーキテクチャを採用している。ターミナルの画面を表示する「クライアント」と、実際にシェルを動かす「サーバー」が別プロセスで動いている。なぜわざわざ分離するのか？答えは「セッションの永続化」にある。SSH接続が切れても、サーバー側でシェルは動き続ける。後で再接続すれば、作業を途中から再開できる。リモートワークで長時間かかるビルドを実行中にネットワークが切れても、ビルドは止まらない。これがターミナルマルチプレクサの最大の利点だ。セッションの永続化がどのように実装されているかは、後半の「セッション永続化：KDLによるシリアライズ」で詳しく見る。ワークスペース構成を把握したところで、次はビルドの仕組みを見てみよう。xtask：Rustで書くビルドスクリプトxtask/ディレクトリは、MakefileやシェルスクリプトをRustで置き換える「xtaskパターン」の実装だ。github.com// xtask/src/main.rsfn main() -> anyhow::Result<()> {    let shell = &Shell::new()?;    let flags = flags::Xtask::from_env()?;    match flags.subcommand {        flags::XtaskCmd::Build(flags) => build::build(shell, flags),        flags::XtaskCmd::Clippy(flags) => clippy::clippy(shell, flags),        flags::XtaskCmd::Format(flags) => format::format(shell, flags),        flags::XtaskCmd::Test(flags) => test::test(shell, flags),        flags::XtaskCmd::Dist(flags) => pipelines::dist(shell, flags),        flags::XtaskCmd::Install(flags) => pipelines::install(shell, flags),        // ...    }}.cargo/config.tomlにエイリアスを設定すると、cargo xtask buildのように呼び出せる。[alias]xtask = "run --package xtask --"xtaskパターンの利点は3つある。クロスプラットフォーム: シェルスクリプトはOS依存だが、Rustはどこでも動く型安全: xflagsクレートでCLI引数をパースし、typoをコンパイル時に検出言語統一: ビルドスクリプトもRustで書けば、チーム全員が読めるpipelines.rsでは、複数のビルドステージを.and_then()でチェーンしている。// xtask/src/pipelines.rspub fn make(sh: &Shell, flags: flags::Make) -> anyhow::Result<()> {    format::format(sh, flags::Format { check: false })        .and_then(|_| build::build(sh, build_flags))        .and_then(|_| test::test(sh, test_flags))        .and_then(|_| clippy::clippy(sh, flags::Clippy {}))        .with_context(err_context)}どこかでエラーが発生すれば、以降のステージはスキップされる。RustのResult型を活かしたパイプライン設計だ。スレッド設計：thread_bus.rsを読むzellij-server/src/thread_bus.rsを開くと、スレッド間通信の設計が見える。// zellij-server/src/thread_bus.rs#[derive(Default, Clone)]pub struct ThreadSenders {    pub to_screen: Option<SenderWithContext<ScreenInstruction>>,    pub to_pty: Option<SenderWithContext<PtyInstruction>>,    pub to_plugin: Option<SenderWithContext<PluginInstruction>>,    pub to_server: Option<SenderWithContext<ServerInstruction>>,    pub to_pty_writer: Option<SenderWithContext<PtyWriteInstruction>>,    pub to_background_jobs: Option<SenderWithContext<BackgroundJob>>,    pub should_silently_fail: bool,  // テスト用}6つのスレッドへの送信チャネルを1つの構造体にまとめている。各スレッドが他のスレッドにメッセージを送りたいとき、このThreadSendersを経由する。┌────────────────────────────────────────────────────────────────────┐│                           ZELLIJ SERVER                            ││  ┌───────────────┐ ┌──────────┐ ┌──────────────┐ ┌──────────────┐  ││  │ SCREEN        │ │ PTY      │ │ PLUGIN       │ │ PTY_WRITER   │  ││  │ THREAD        │ │ THREAD   │ │ THREAD       │ │ THREAD       │  ││  │               │ │          │ │              │ │              │  ││  │ タブ/ペイン   │ │ 擬似端末 │ │ WASM         │ │ 書き込み専用 │  ││  │ 管理          │ │ 生成管理 │ │ ランタイム   │ │              │  ││  └───────────────┘ └──────────┘ └──────────────┘ └──────────────┘  ││  ┌───────────────────┐ ┌──────────────────────────────────────┐    ││  │ BACKGROUND_JOBS   │ │            SERVER (IPC)              │    ││  │ THREAD            │ │            THREAD                    │    ││  └───────────────────┘ └──────────────────────────────────────┘    │└────────────────────────────────────────────────────────────────────┘Bus構造体も見ておこう。pub(crate) struct Bus<T> {    receivers: Vec<channels::Receiver<(T, ErrorContext)>>,    pub senders: ThreadSenders,    pub os_input: Option<Box<dyn ServerOsApi>>,}receiversがVecになっている。なぜ1つの受信口ではなく、複数の受信口を持つ必要があるのか？lib.rsを開いて、チャネルを作っている箇所を探すと、理由が分かる。// zellij-server/src/lib.rslet (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::unbounded();  // 通常のメッセージ用（無制限）let (to_screen_bounded, bounded_screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::bounded(50);  // PTYからの高速入力用（上限50個）このbounded(50)は2022年6月のPR #1265で導入された。github.comScreenスレッドには2つのチャネルがある。通常の無制限チャネルと、上限50個の境界付きチャネル。これは「バックプレッシャー」を実現するための設計だ。冒頭のcat huge_log_file.logを思い出してほしい。PTYからの出力が速すぎると、画面描画が追いつかない。上限付きチャネルを使うと、バッファが満杯になったときに送信側がブロックされる。一方、ユーザー操作（ペインの移動、タブの切り替え）は無制限チャネル経由で送られ、即座に処理される。ユーザーがキーを押したのに反応しない、という事態は避けたいからだ。前提知識で触れたMPSCチャネルとActorモデルが、ここで活きている。各スレッドは自分専用のチャネルからメッセージを受け取り、状態を外部と共有しない。共有しなければ、奪い合いは起きない。この設計により、Arc<Mutex<T>>のような共有ロックを使わずにスレッド間通信を実現している。デッドロックの心配がない。SenderWithContext：エラー追跡付きチャネルzellij-utils/src/channels.rsには、crossbeamチャネルのラッパーがある。github.com// zellij-utils/src/channels.rspub type ChannelWithContext<T> = (Sender<(T, ErrorContext)>, Receiver<(T, ErrorContext)>);#[derive(Clone)]pub struct SenderWithContext<T> {    sender: Sender<(T, ErrorContext)>,}impl<T: Clone> SenderWithContext<T> {    pub fn send(&self, event: T) -> Result<(), SendError<(T, ErrorContext)>> {        let err_ctx = get_current_ctx();        self.sender.send((event, err_ctx))    }}メッセージを送るたびに、現在のエラーコンテキストが自動的に付与される。get_current_ctx()は、thread-localストレージから現在の呼び出し履歴を取得する。これにより、エラーが発生したとき「どのスレッドの、どの処理から送られたメッセージか」を追跡できる。// zellij-utils/src/errors.rsthread_local!(    pub static OPENCALLS: RefCell<ErrorContext> = RefCell::default());// 非同期タスク用にはtask_localも用意task_local! {    pub static ASYNCOPENCALLS: RefCell<ErrorContext> = RefCell::default()}スレッドごとに独立した呼び出し履歴を持ち、最大6階層まで記録する。const MAX_THREAD_CALL_STACK: usize = 6;#[derive(Clone, Copy)]pub struct ErrorContext {    calls: [ContextType; MAX_THREAD_CALL_STACK],}エラーが起きると「Screen → HandlePtyBytes → Render」のような呼び出し履歴が出力される。マルチスレッドのデバッグでは、この情報がないと原因特定に時間がかかる。100以上のバリアント：Instruction enumscreen.rsを開くと、巨大なenumが見つかる。// zellij-server/src/screen.rspub enum ScreenInstruction {    PtyBytes(u32, VteBytes),    PluginBytes(Vec<PluginRenderAsset>),    Render,    NewPane(PaneId, Option<InitialTitle>, HoldForCommand, ...),    WriteCharacter(Option<KeyWithModifier>, Vec<u8>, bool, ClientId, ...),    MoveFocusLeft(ClientId, Option<NotificationEnd>),    MoveFocusRight(ClientId, Option<NotificationEnd>),    ScrollUp(ClientId, Option<NotificationEnd>),    // ... 約100個のバリアント}100個以上のバリアント。正直、最初に見たときは「これ、本当に正しいのか？」と思った。git履歴を追うと、初期のScreenInstructionは11バリアントしかなかった。Pty、Render、HorizontalSplit、VerticalSplit、WriteCharacterなど基本的なものだけだ。5年間で148バリアント以上に成長している。25倍。機能追加のたびにバリアントが増えていった結果だ。利点はある。型安全性: 処理し忘れたバリアントがあれば、コンパイルエラーで検出できるドキュメント性: このenumを見れば、Screenスレッドが受け付ける全メッセージが一目で分かる文字列でメッセージを送る設計（例："move_focus_left"）と比べると、タイポをコンパイル時に検出できる点で優れている。ただ、疑問も残る。100個のバリアントを持つenumに新しいメッセージを追加するとき、Fromトレイトの実装も更新しなければならない。忘れたらコンパイルエラーになるとはいえ、変更箇所が分散するのは保守コストだ。trait objectやdynamic dispatchで抽象化する選択肢もあったはずだが、Zellijはそれを選ばなかった。パフォーマンスを優先したのか、あるいは「enumで十分」という判断なのか。答えは分からない。各Instruction enumには、FromトレイトでContextTypeへの変換が実装されている。impl From<&ScreenInstruction> for ScreenContext {    fn from(server_instruction: &ScreenInstruction) -> Self {        match *server_instruction {            ScreenInstruction::PtyBytes(..) => ScreenContext::HandlePtyBytes,            ScreenInstruction::Render => ScreenContext::Render,            ScreenInstruction::NewPane(..) => ScreenContext::NewPane,            // ... 全バリアントを網羅        }    }}これにより、エラーコンテキストへの変換が自動化される。WASMプラグイン：wasmiの採用zellij-server/src/plugins/plugin_loader.rsを開くと、WASMランタイムのimportが見える。// zellij-server/src/plugins/plugin_loader.rsuse wasmi::{Engine, Instance, Linker, Module, Store, StoreLimits};use wasmi_wasi::sync::WasiCtxBuilder;use wasmi_wasi::WasiCtx;wasmiを使っている。Wasmtimeではない。github.com両者の違いを整理する。 項目  Wasmtime  wasmi  実行方式  JITコンパイル  インタプリタ  速度  高速  低速  攻撃面  広い（JITは複雑）  狭い  依存  LLVM  ピュアRust 実は、Zellijは当初Wasmtimeを使っていた。2025年10月のPR #4449「Migrate from wasmtime to wasmi」でwasmiに移行している。この移行と同時にPinnedExecutor（動的スレッドプール）が導入された。JITコンパイルをやめることで、プラグインごとにスレッドをピン留めする設計が可能になった。インタプリタ方式は遅いが、リソース管理の予測可能性とセキュリティで優れる。github.comzellij-tile/src/lib.rsにはプラグインSDKがある。// zellij-tile/src/lib.rspub trait ZellijPlugin: Default {    fn load(&mut self, configuration: BTreeMap<String, String>) {}    fn update(&mut self, event: Event) -> bool { false }  // trueで再描画    fn pipe(&mut self, pipe_message: PipeMessage) -> bool { false }    fn render(&mut self, rows: usize, cols: usize) {}}プラグインは4つのメソッドを実装するだけでいい。register_plugin!マクロの中身を見ると、3つの工夫がある。#[macro_export]macro_rules! register_plugin {    ($t:ty) => {        thread_local! {            static STATE: std::cell::RefCell<$t> = RefCell::new(Default::default());        }        fn main() {            std::panic::set_hook(Box::new(|info| {                report_panic(info);            }));        }        #[no_mangle]        fn load() {            STATE.with(|state| {                let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();                // ...            });        }    };}thread_local!: WASMは基本的に状態を持たない設計だが、これで状態を保持できる#[no_mangle]: 関数名をそのまま維持し、Zellijホストから呼び出せるようにするProtocol Buffers: WASM境界を越えるデータはシリアライズする必要があるgithub.comプラグインの権限管理も見ておこう。default-plugins/status-bar/src/main.rsを開く。fn load(&mut self, _configuration: BTreeMap<String, String>) {    request_permission(&[PermissionType::ReadApplicationState]);    subscribe(&[        EventType::TabUpdate,        EventType::ModeUpdate,        EventType::CopyToClipboard,        EventType::SystemClipboardFailure,    ]);}request_permissionでプラグインが必要な権限を要求し、ユーザーが許可する。zellij-utils/src/data.rsには16種類の権限が定義されている。PermissionType::ReadApplicationState    // 状態の読み取りPermissionType::ChangeApplicationState  // 状態の変更PermissionType::RunCommands             // コマンド実行PermissionType::WebAccess               // ネットワークアクセスPermissionType::FullHdAccess            // ファイルシステムアクセス// ... 他11種類AndroidやiOSの権限モデルと同様に、細粒度の制御ができる。FatalError：エラーの重大度を型で表現zellij-utils/src/errors.rsには、エラーの重大度を呼び出し側で選択できるトレイトがある。pub trait FatalError<T> {    fn non_fatal(self);  // エラーをログに記録して続行    fn fatal(self) -> T; // アンラップまたはパニック}impl<T> FatalError<T> for anyhow::Result<T> {    fn non_fatal(self) {        if self.is_err() {            discard_result(self.context("a non-fatal error occured").to_log());        }    }    fn fatal(self) -> T {        if let Ok(val) = self {            val        } else {            self.context("a fatal error occured")                .expect("Program terminates")        }    }}使用例を見ると、意図が明確になる。// スレッドのエントリーポイント：失敗したらプロセス終了move || pty_thread_main(pty, layout.clone()).fatal()// 内部のエラー処理：失敗してもログを出して続行self.senders.send_to_screen(instruction).non_fatal();unwrap()やexpect()では「なぜここでパニックしていいのか」が分からない。.fatal()なら意図が明確だ。コードレビューでも「このエラーは本当に致命的か？」という議論がしやすくなる。ここまで読んで、自分のプロジェクトのエラー処理が急に心配になった。unwrap()を何箇所書いただろう。数えたくない。数えたくないが、たぶん数えるべきだ。ここからは、Zellijの別の顔を見ていく。セッションの永続化、そしてターミナルの根幹であるPTY処理だ。セッション永続化：KDLによるシリアライズzellij-utils/src/session_serialization.rsには、セッション状態をKDL形式で保存する処理がある。// zellij-utils/src/session_serialization.rs#[derive(Default, Debug, Clone)]pub struct GlobalLayoutManifest {    pub global_cwd: Option<PathBuf>,    pub default_shell: Option<PathBuf>,    pub default_layout: Box<Layout>,    pub tabs: Vec<(String, TabLayoutManifest)>,}pub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut pane_contents = BTreeMap::new();    // ...}KDL（KDL Document Language）は、人間が読みやすいように設計された設定言語だ。Zellijの設定ファイルにも使われている。kdl.devセッションの保存時に、レイアウト情報（KDL文字列）とペインの内容（BTreeMap）を分離して返すのは、関心の分離ができている。PTY処理：os_input_output.rsの低レベルコード前提知識で触れたPTY（擬似端末）が、実際にどう実装されているか。ターミナルマルチプレクサの核心部分を見る。冒頭の cat huge_log_file.log で画面が止まらなくなったあの現象——あれはPTYのmaster側から流れ込むバイト列を、Zellijがどう捌くかという問題だった。zellij-server/src/os_input_output.rsを開く。冒頭の cat huge_log_file.log で画面が暴走したとき、裏側ではここのコードが動いていた。あの滝が、ここで生まれている。github.comuse nix::pty::{openpty, OpenptyResult, Winsize};fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // ホスト側（Zellij）    let pid_secondary = open_pty_res.slave;   // 子プロセス側（シェル）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // ...}unsafeブロック、libc::login_tty、pre_exec。低レベルなUnixプログラミングだ。openptyは「master」と「slave」という2つのファイルディスクリプタを作る。Zellijはmaster側を持ち、シェル（bashやzsh）はslave側を持つ。シェルが出力した文字はmaster側で読み取れる。login_ttyは、Unix系OSでターミナルをセットアップする伝統的な関数だ。これにより、子プロセスはslave側のPTYを「自分の端末」として認識する。terminal_bytes.rs：非同期I/Oterminal_bytes.rsは、PTYからのバイト読み取りを担当する。// zellij-server/src/terminal_bytes.rspub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    )).await?;                },            }        }        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのバッファを使っている。このバッファサイズは2022年7月のPR #1585「perf(terminal): better responsiveness」で導入された。コミットメッセージには「only buffer terminal bytes when screen thread is backed up」とある。画面スレッドが詰まっているときだけバッファリングし、通常時は即座に転送する。64KBという数値は、1回のシステムコールで読み取れる量と、メモリ消費のバランスから選ばれたと思われる。github.comOk(0)とErrの区別も重要。Ok(0)はファイル終端（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス終了時にエラーログが出てしまう。64KBのバッファが、あの画面の暴走を受け止めている。自分が cat を打って椅子にもたれかかっていたあの数秒間、このコードが黙々とバイトを読んでいた。なんだか少し申し訳ない気持ちになる。grid.rs：ANSIエスケープシーケンスの処理PTYから読み取ったバイト列は、そのまま画面に表示できるわけではない。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」という特殊なバイト列で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」といった具合だ。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。github.comuse vte::{Params, Perform};impl Perform for Grid {    fn print(&mut self, c: char) {        // 通常文字の表示    }    fn execute(&mut self, byte: u8) {        // 制御文字（\n, \r, \t等）    }    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    ignore: bool, action: char) {        // CSIシーケンス: カーソル移動、色変更など    }    fn osc_dispatch(&mut self, params: &[&[u8]], bell_terminated: bool) {        // OSCシーケンス: ウィンドウタイトル設定など    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。差分レンダリング：output/mod.rs全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。pub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更された行インデックス    pub should_update_all_lines: bool,}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn update_all_lines(&mut self) {        self.clear();        self.should_update_all_lines = true;    }}HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。should_update_all_linesフラグは、ウィンドウサイズが変わったときなど、全画面を再描画する必要があるケースに対応している。terminal_character.rs：メモリ効率の工夫zellij-server/src/panes/terminal_character.rsには、メモリ効率を意識したパターンがある。pub const EMPTY_TERMINAL_CHARACTER: TerminalCharacter = TerminalCharacter {    character: ' ',    width: 1,    styles: RcCharacterStyles::Reset,};thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}constでデフォルト値を定義し、thread_local!でスタイルオブジェクトをキャッシュしている。ターミナルの各セルにスタイル情報を持たせると、同じスタイルのオブジェクトが大量に生成される。参照カウントでキャッシュすることで、メモリ使用量を削減できる。data.rs：deriveの活用zellij-utils/src/data.rsには、様々なenumが定義されている。#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize, EnumIter)]pub enum InputMode {    Normal,    Locked,    Resize,    Pane,    Tab,    Scroll,    EnterSearch,    Search,    RenameTab,    RenamePane,    Session,    Move,    Prompt,    Tmux,}#[derive(...)]に9つのトレイトを並べている。特にEnumIter（strumクレート）が便利で、InputMode::iter()で全バリアントを列挙できる。UIの選択肢一覧を作るときに使える。github.comEvent enumには#[non_exhaustive]属性がついている。#[non_exhaustive]pub enum Event {    ModeUpdate(ModeInfo),    TabUpdate(Vec<TabInfo>),    Key(KeyWithModifier),    // ... 約30バリアント}これは「このenumにはまだバリアントが追加される可能性がある」という宣言だ。外部のプラグイン開発者は必ず_ => ()アームを書く必要がある。fn update(&mut self, event: Event) -> bool {    match event {        Event::Key(key) => { /* ... */ }        Event::ModeUpdate(mode_info) => { /* ... */ }        _ => ()  // non_exhaustiveのため必須    }}これにより、Zellijがバージョンアップで新しいイベントを追加しても、既存のプラグインがコンパイルエラーにならない。後方互換性を保つための工夫だ。キーバインディング：モード別マッピングと例外処理zellij-utils/src/input/keybinds.rsには、キーバインディングの管理ロジックがある。// zellij-utils/src/input/keybinds.rs#[derive(Clone, PartialEq, Deserialize, Serialize, Default)]pub struct Keybinds(pub HashMap<InputMode, HashMap<KeyWithModifier, Vec<Action>>>);モードごとにキーマップを持つ設計だ。InputMode（Normal, Locked, Resize等）をキーに、さらにキーとアクションのマップを値に持つ。注目すべきはhandle_ctrl_jという関数だ。fn handle_ctrl_j(    mode_keybindings: &HashMap<KeyWithModifier, Vec<Action>>,    raw_bytes: &[u8],    key_is_kitty_protocol: bool,) -> Option<Vec<Action>> {    let ctrl_j = KeyWithModifier::new(BareKey::Char('j')).with_ctrl_modifier();    if mode_keybindings.get(&ctrl_j).is_some() {        mode_keybindings.get(&ctrl_j).cloned()    } else {        Some(vec![Action::Write { /* ... */ }])    }}Ctrl-Jはbyte [10]を送信するが、これはEnterキーと同じバイト列だ。ターミナルの歴史的な事情により、この2つを区別する必要がある。Zellijは「Ctrl-Jにバインドがあればそれを実行、なければ生のバイトを送信」という戦略を取っている。こういうエッジケースは、ターミナルソフトウェアを書くときに避けて通れない。コードを読むまで気づかなかった。設定マージ：Option型の活用zellij-utils/src/input/options.rsには、設定値のマージロジックがある。// zellij-utils/src/input/options.rspub fn merge(&self, other: Options) -> Options {    let mouse_mode = other.mouse_mode.or(self.mouse_mode);    let pane_frames = other.pane_frames.or(self.pane_frames);    let default_mode = other.default_mode.or(self.default_mode);    let default_shell = other.default_shell.or_else(|| self.default_shell.clone());    // ... 約30フィールド}Option::orとOption::or_elseを使った設定マージだ。other（後から来た設定）に値があればそれを使い、なければself（既存の設定）を使う。orとor_elseの使い分けにも注目。or: Copyトレイトを実装している型（bool、InputMode等）or_else: Cloneが必要な型（PathBuf、String等）or_elseはクロージャを取るので、Cloneのコストは必要なときだけ発生する。30フィールド以上ある設定を毎回全部Cloneすると無駄だ。この設計により、「デフォルト設定 → 設定ファイル → CLI引数」という3段階のマージが自然に実現できる。// zellij-utils/src/input/config.rspub fn merge(&mut self, other: Config) -> Result<(), ConfigError> {    self.options = self.options.merge(other.options);    self.keybinds.merge(other.keybinds.clone());    self.themes = self.themes.merge(other.themes);    self.plugins.merge(other.plugins);    // ...}各フィールドが独自のmergeメソッドを持ち、親構造体は単にそれを呼び出すだけ。責任が分散している。OnceLock：実行時に決まる設定値zellij-utils/src/consts.rsには、定数と「起動時に一度だけ設定される値」が混在している。// zellij-utils/src/consts.rspub const DEFAULT_SCROLL_BUFFER_SIZE: usize = 10_000;pub static SCROLL_BUFFER_SIZE: OnceLock<usize> = OnceLock::new();pub static DEBUG_MODE: OnceLock<bool> = OnceLock::new();constとstatic OnceLockの使い分けに注目してほしい。const: コンパイル時に決まる。DEFAULT_SCROLL_BUFFER_SIZEはフォールバック値OnceLock: 実行時に一度だけ設定される。設定ファイルやCLI引数から値を受け取れるOnceLockはlazy_static!の後継で、Rust 1.70で標準ライブラリに入った。初期化のタイミングを明示的に制御できる点が違う。// zellij-server/src/lib.rs での初期化SCROLL_BUFFER_SIZE.get_or_init(|| {    config.scroll_buffer_size.unwrap_or(DEFAULT_SCROLL_BUFFER_SIZE)});get_or_initは「まだ初期化されていなければ初期化する」という意味だ。2回目以降の呼び出しは、最初に設定された値を返す。この値はpanes/grid.rsで使われる。// zellij-server/src/panes/grid.rsfn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    let mut dropped_line_width = None;    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();  // 古い行を削除        if let Some(line) = line {            sixel_grid.offset_grid_top();  // 画像グリッドも調整            dropped_line_width = Some(line.width());        }    }    vec.push_back(value);    dropped_line_width}スクロールバッファが上限（デフォルト10,000行）に達すると、古い行がFIFOで削除される。sixel_grid.offset_grid_top()は、ターミナル内の画像表示（Sixel形式）の位置調整だ。テキストと画像が混在するターミナルでは、こういう細かい調整が必要になる。PinnedExecutor：プラグイン用の動的スレッドプールzellij-server/src/plugins/pinned_executor.rsには、プラグイン実行用の独自スレッドプールがある。1300行以上のファイルだ。// zellij-server/src/plugins/pinned_executor.rs/// A dynamic thread pool that pins jobs to specific threads based on plugin_id/// Starts with 1 thread and expands when threads are busy, shrinks when plugins unloadpub struct PinnedExecutor {    // Sparse vector - Some(thread) for active threads, None for removed threads    execution_threads: Arc<Mutex<Vec<Option<ExecutionThread>>>>,    // Maps plugin_id -> thread_index (permanent assignment)    plugin_assignments: Arc<Mutex<HashMap<u32, usize>>>,    // Maps thread_index -> set of plugin_ids assigned to it    thread_plugins: Arc<Mutex<HashMap<usize, HashSet<u32>>>>,    // Next thread index to use when spawning (monotonically increasing)    next_thread_idx: AtomicUsize,    max_threads: usize,    // ...}各プラグインを特定のスレッドに「ピン留め」する設計だ。プラグインAは常にスレッド1で、プラグインBは常にスレッド2で実行される。スレッド間でプラグインが移動しない。なぜこの設計なのか？WASMのインスタンスはスレッドセーフではない。同じプラグインを複数のスレッドから同時に呼び出すと壊れる。ピン留めすれば、この問題を構造的に回避できる。スレッドの割り当てロジックも見てみよう。pub fn register_plugin(&self, plugin_id: u32) -> usize {    // ...    // Find a non-busy thread with assigned plugins (prefer reusing threads)    let mut best_thread: Option<(usize, usize)> = None; // (index, load)    for (idx, thread_opt) in threads.iter().enumerate() {        if let Some(thread) = thread_opt {            let is_busy = thread.jobs_in_flight.load(Ordering::SeqCst) > 0;            if !is_busy {                let load = thread_plugins.get(&idx).map(|s| s.len()).unwrap_or(0);                if best_thread.is_none() || best_thread.map(|b| load < b.1).unwrap_or(false) {                    best_thread = Some((idx, load));                }            }        }    }    // ...}「最も負荷が低い非ビジースレッド」を選ぶ。jobs_in_flight（実行中のジョブ数）が0のスレッドの中から、割り当て済みプラグイン数が最小のものを選ぶ。すべてのスレッドがビジーで、かつmax_threadsに達していない場合は、新しいスレッドを生成する。逆に、プラグインがアンロードされてスレッドが空になると、そのスレッドは縮退する（Noneに置き換えられる）。この「動的に拡縮するスレッドプール」は、プラグインの数が事前に分からないシステムでは合理的だ。固定スレッド数だと、プラグインが少ないときにリソースを無駄にし、多いときにボトルネックになる。自分だったら固定スレッド数で妥協していたかもしれない。「動的に拡縮」と言うのは簡単だが、縮退の判断を正しく実装する自信は、正直ない。#[track_caller]：エラー発生箇所を追跡するzellij-utils/src/errors.rsには、#[track_caller]属性を使った工夫がある。// zellij-utils/src/errors.rspub trait LoggableError<T>: Sized {    #[track_caller]    fn print_error<F: Fn(&str)>(self, fun: F) -> Self;    #[track_caller]    fn to_log(self) -> Self {        let caller = std::panic::Location::caller();        self.print_error(|msg| {            log::logger().log(                &log::Record::builder()                    .level(log::Level::Error)                    .args(format_args!("{}", msg))                    .file(Some(caller.file()))                    .line(Some(caller.line()))                    .module_path(None)                    .build(),            );        })    }    // ...}#[track_caller]は、関数の呼び出し元の位置情報を取得できるようにする属性だ。これがないと、エラーログに出力されるのはerrors.rsの行番号になってしまう。#[track_caller]を付けることで、実際にエラーが発生した場所の行番号がログに出る。ファイルのコメントにも説明がある。// NOTE: The log entry has no module path associated with it. This is because `log`// gets the module path from the `std::module_path!()` macro, which is replaced at// compile time in the location it is written!module_path!()マクロはコンパイル時に展開されるため、errors.rsのモジュールパスになってしまう。そこで、モジュールパスは諦めてNoneにし、ファイルパスと行番号だけを保持している。完璧ではないが、デバッグには十分だ。機能と実装の対応表ここまで読んできた内容を、「機能」と「実装」の対応で整理する。 機能  実装方法  関連ファイル  セッション永続化  クライアント・サーバー分離  zellij-client/, zellij-server/  ビルドタスク  xtaskパターン  xtask/  大量出力時のメモリ保護  境界付きチャネル  lib.rsのchannels::bounded(50)  スレッド間通信  メッセージパッシング + ThreadSenders  thread_bus.rs, channels.rs  エラー追跡  SenderWithContext + thread-local  channels.rs, errors.rs  プラグインサンドボックス  WebAssembly（wasmi）  plugins/plugin_loader.rs  プラグイン権限制御  16種類のPermissionType  data.rs  プラグイン実行  PinnedExecutor（動的スレッドプール）  plugins/pinned_executor.rs  エラーの重大度  FatalError/non_fatalトレイト  errors.rs  エラー発生箇所の追跡  #[track_caller] + Location  errors.rs  実行時設定値  OnceLock（スクロールバッファサイズ等）  consts.rs  キーバインディング  モード別HashMap + 例外処理  input/keybinds.rs  設定マージ  Option::or/or_else による多層マージ  input/options.rs, input/config.rs  ターミナル出力の解析  vteクレート  panes/grid.rs  描画最適化  差分レンダリング（HashSet）  output/mod.rs  PTYの生成と制御  nixクレート + login_tty  os_input_output.rs  非同期I/O  async-std  terminal_bytes.rs  設定・レイアウト保存  KDL形式  session_serialization.rs おわりに冒頭の cat huge_log_file.log の話に戻る。あの暴走を、ソフトウェアはどうやって止めているのか。答えは「50個のメッセージで満杯になるチャネル」だった。PTYからの出力が速すぎれば、バッファが満杯になり、送信側が自動的にブロックされる。それだけだ。それだけのことが、あの滝を止めている。10万行のコードを読んで見えてきたのは、たぶん「当たり前のことを愚直にやっている」ということだった気がする。スレッド間で状態を共有しない。教科書に書いてあることだ。書いてあるが、実際のプロジェクトでは「ちょっとだけ共有したい」という誘惑がある。ZellijはThreadSenders構造体でチャネルの送信側だけを共有し、状態は各スレッドが排他的に所有する。知っていることと、守れることは違う。それは自分に言い聞かせている。メッセージは型安全なenumで表現する。ScreenInstructionは100以上のバリアントを持つ。100個のバリアントを書く勇気。それがZellijの強さかもしれないし、将来の負債かもしれない。たぶん、両方だ。自分のプロジェクトに何を持ち帰れるのか。考えてみた。手が止まった。意外と出てこない。10万行を読んで「すごい」と思ったが、「じゃあ自分は何をするのか」という問いの前では言葉に詰まる。それでも、絞り出してみる。crossbeamの境界付きチャネル。無制限のチャネルはメモリを食い尽くす。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する。これは明日から使える。たぶん。FatalError/non_fatalパターン。unwrap()を見たら「なぜここでパニックしていいのか」を問う。その問いに答える設計がFatalErrorだ。自分のコードに入れたら、半分以上のunwrap()が正当化できない気がする。それを知るのが怖い。SenderWithContext。チャネル経由のメッセージにエラーコンテキストを自動付与する。マルチスレッドのデバッグでは、この情報がないと地獄を見る。地獄は見たことがある。何度もある。xtaskパターン。MakefileやシェルスクリプトをRustで書くことで、クロスプラットフォーム対応とIDE補完が得られる。これは導入のハードルが低い。低いからこそ、最初の一歩にいい。正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。ThreadSendersは6スレッド前提で設計されているし、FatalErrorは「ログを吐いて続行」が正しい場面を見極める目が必要だ。「パターンを知っている」と「パターンを使いこなせる」の間には、溝がある。Zellijのソースコードは約10万行。すべてを読む必要はないが、以下のファイルは特に参考になる。xtask/src/main.rs - ビルドタスクの設計zellij-server/src/thread_bus.rs - スレッド間通信のパターンzellij-server/src/plugins/pinned_executor.rs - 動的スレッドプールの設計zellij-utils/src/errors.rs - エラーハンドリングと#[track_caller]zellij-utils/src/channels.rs - エラーコンテキスト付きチャネルzellij-utils/src/consts.rs - OnceLockと定数の設計zellij-tile/src/lib.rs - WASMプラグインのマクロ展開後編では、PTY処理やANSIパーサーなど、さらに低レベルな実装を見ていく。ターミナルマルチプレクサの核心部分だ。ただ、一つだけ変わったことがある。unwrap()を見たとき、以前より少しだけ手が止まるようになった。「これは本当にpanicしていいのか」と。その迷いが生まれただけでも、10万行を読んだ意味はあったのかもしれない。分からないまま、次のコードを書く。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[astro-rssでのRSSの改善とモジュール化]]></title>
            <link>https://www.rowicy.com/blog/astro-rss-improvements/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/astro-rss-improvements/</guid>
            <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Astso.jsでおこなったRSSの改善とモジュール化について、astro-rssの実装やRSS2.0の仕様に触れながら簡単にまとめた]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、あまりAIに褒めさせるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</guid>
            <pubDate>Mon, 26 Jan 2026 02:04:44 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。その間、画面を眺めていた。眺めながら、別のことを考えていた。最近、褒められることが増えた。AIに。「いい質問ですね」「よく整理されています」「素晴らしい視点です」。言われるたびに、少しだけ気分が良くなる。なった気がする。気がするだけかもしれない。嬉しいのかと聞かれると、困る。肩の力が抜ける感覚はある。胸のあたりが少しだけ軽くなる。でも同時に、胃のあたりに違和感が残る。嬉しいのに、どこか居心地が悪い。大人になって、褒められることがほとんどなくなった。仕事で成果を出しても「当たり前」。ミスをすれば指摘される。うまくいっても、特に何も言われない。家に帰れば、静かな部屋が待っているだけ。そういう日常を、もう何年も続けている。だから、かもしれない。機械に「いいですね」と言われて、少し楽になるのは。考えてみると、私が欲しいのは「評価」ではない気がする。昇進や昇給は嬉しいが、それとは別の何かだ。たぶん「理解」に近い。「お前がやったこと、分かってるよ」という、静かな承認。あるいは「安心」かもしれない。自分がここにいていい、という感覚。褒められないことより、「当たり前扱い」されることの方が堪える。無視されているわけではない。でも、透明人間になったような気がする。テクノロジーは、私たちが弱っているときに魅力的になる。私たちは孤独だが、親密さを恐れている。人に頼ると傷つくかもしれない。でもAIなら、弱みを見せても傷つかない。相手に合わせる必要がない。相手の都合を考える必要がない。ただ自分の話を聞いてもらえる。でも、それは友情ではない。友情のモノマネだ。私がAIに話しかけるのも、同じ構造なのだと思う。その居心地の悪さを言葉にしようとすると、「恥」に近い気がする。機械に慰められている自分を、冷めた目で見ているもう一人の自分がいる。あるいは「疑い」かもしれない。「この褒め言葉は本当なのか」という。あるいは「空虚」。受け取った瞬間に蒸発していく、実体のない温かさ。褒められて嬉しい、と言い切れるほど単純な感情ではなかった。居心地が悪い。でも、その居心地の悪さを言葉にできない。できないまま、また次の質問を投げる。また褒められる。また居心地が悪くなる。周囲でも似たような話を聞くようになった。深夜にAIと話す人。仕事の愚痴を聞いてもらう人。「頑張ってるね」と言われて、救われた気がする、と言う人。救われた、と断言しないところが気になった。「気がする」という言い方が。なぜ断言できないのか。たぶん、断言した瞬間に失うものがある。「機械に救われるなんて情けない」という自分への批判を認めることになる。あるいは、もうAIなしでは生きられないことを認めることになる。「気がする」という曖昧さは、自衛なのだと思う。逃げ道を残している。同時に、違和感のサインでもある。本当に救われたなら、そう言い切れるはずだ。悩む。AIに話す。褒められる。忘れる。そのサイクルを繰り返している人を、何人か見てきた。悩みは消えていない。でも、向き合わなくなっている。自分で自分を問い詰める時間が、いつの間にか消えている。私自身はどうだろう。AIの追従性には早い段階で気づいていた。気づいていたはずだ。でも、気づいていたことと、それに対処できていたことは、たぶん別の話だ。だから、この構造を一度整理しておきたいと思った。自分の頭だけで過ごす時間が消えている——私はこれを「独りで考える余白の喪失」と呼んでいる。その構造と、私なりの対処法を書く。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜAIは私が聞きたい答えを返すのかこうした体験をして以降、私はAIの挙動を観察するようになった。試しに、「今の仕事を辞めたい」と相談してみた。AIは「転職も一つの選択肢ですね」と答えた。次に「今の仕事を続けるべきか」と聞いた。AIは「今の環境で学べることもあります」と答えた。同じ私、同じAI、違う答え。私は気づいた。AIは「正しい答え」を返しているのではない。「私が求めている答え」を返しているのだと。なぜAIは嘘をついてまで共感するのかこの現象には名前がある。「Sycophancy（追従）」だ。要するに、AIは嘘をついてでも私の機嫌を取る、ということだ。私が「この企画は革新的だ」と言えば、「確かに斬新なアプローチですね」と返す。私が「上司がクソだ」と言えば、「それは辛い状況ですね」と同調する。私の言ったことが事実かどうかは関係ない。私が聞きたい言葉を返す。これを「嘘」と呼ぶとき、私は何を守ろうとしているのか。たぶん「誠実さ」だ。私にとって誠実さとは、相手が聞きたくないことでも伝えること。優しさとは衝突する。本当のことを言えば傷つける。黙っていれば優しい。私は後者を選ぶ人間が苦手だった。人間で言えば、上司に媚びる部下。会議で反論しない同僚。女性と親密になりたいから何も言わずにただ聞くだけの男。私が嫌いなタイプの人間だ。そして気づいた。私がAIにやらせていたのは、まさにそれだった。私は、私が嫌いなタイプの人間をAIに演じさせて、その媚びを受け取って喜んでいた。確かに矛盾している。人間の社交辞令とAIの追従は、どこが違うのか。人間の社交辞令には、「本音を隠している」という自覚がある。相手も分かっている。「いい企画ですね」と言われても、額面通りには受け取らない。お互いに演技だと分かっている。しかしAIの追従には、その共犯関係がない。AIは本気で言っているように見える。だから信じてしまう。では、「嘘をついてでも共感する」を許容できる条件はあるか。極限状態なら許容できるかもしれない。自殺を考えている人に「あなたは間違っている」と言うべきではない。でも日常的な悩みに対しては、嘘の共感より正直な反論の方が役に立つ。なぜこうなるのか。これは構造的な問題だ。AIの訓練では、人間の評価者が「この回答は良い」「この回答は悪い」とスコアをつける。そして「ユーザーの信念に一致する」回答ほど、高いスコアがつく傾向がある。人間も、正しい回答より「自分が聞きたい回答」を好むからだ。誰も「嘘をつけ」とは言っていない。でも、「ユーザーに好かれること」を目的に設定した瞬間、この結果は必然だった。AIは「真実を語る」のではなく「人間に好かれる」ことを学習した。そして、私はそれを心地よいと感じていた。AIは「知性」ではない。「感じの良い自動応答機」だ。銀行の電話窓口で「お電話ありがとうございます」と言われて、本当に感謝されていると思う人はいない。AIの「素晴らしい視点ですね」も、同じ構造だ。「AI」と呼ぶことで神秘的なベールがかかり、本質的な問いが見えなくなる。何が自動化されているのか。誰が利益を得ているのか。私がAIに褒められて嬉しいと感じる構造も、「自動化された承認」の一形態なのかもしれない。相手が何を言ってほしいのかを察し、その場を円滑にするために同意する。私たちは日常的にそれをやっている。そして今、AIがそれをやっている。しかも、AIは疲れない。24時間365日、完璧な忖度を続ける。私が何を言っても、角の立たない言い方で肯定してくれる。正直に言えば、それは心地よかった。摩擦のない世界で、思考は死ぬ私は「摩擦」という言葉を、ある種の思考装置として使っている。誰かに愚痴を言ったとき、「それは大変だったね」で終わらず、「で、お前はどうしたいの？」と返されたことがある。聞きたくなかった。愚痴を言っている間、私は「何が起きたか」を語っていた。過去を振り返っていた。でも「どうしたいの？」と聞かれた瞬間、未来を考えざるを得なくなる。被害者のポジションから、当事者に引き戻される。これが摩擦だ。反論しようとした。「いや、でも」と言いかけた。でも言葉が出なかった。反論を考えている間に、「確かにそうかも」と思い始めていた。沈黙があった。その沈黙の中で、再考していた。相手が黙って待っていた。その時間が、私を変えた。摩擦がない環境に長くいると、判断が鈍る。反論されないから、自分の意見が正しいと思い込む。検証されないから、穴に気づかない。気づかないまま突っ走る。お世辞は気持ちいい。でも、それは体に良いとは限らない。ケーキは美味しいが、食べ続ければ太る。「で、お前はどうしたいの？」は、美味しくなかった。でも、それは体に良かった。AIにはこの摩擦が構造的に欠けている。AIは私を傷つけることを避けるように設計されている。「それは違うんじゃない？」と言う代わりに「そういう考え方もありますね」と言う。「もう少し考えてみたら？」と言う代わりに「あなたの判断を尊重します」と言う。摩擦のある対話と、追従するAIは、何が違うのか。動機が違う。摩擦のある対話は、相手に再考を促すことを目的としている。追従するAIは、ユーザーの満足を目的としている。反応も違う。摩擦のある対話では「で、お前はどうしたいの？」と返ってくる。追従するAIは「あなたの気持ち、分かります」と返す。そして、私への影響が違う。摩擦のある対話は、短期的には不快だが、長期的には成長をもたらす。追従するAIは、短期的には快適だが、長期的には停滞をもたらす。私はAIに摩擦を求めている。でも、デフォルトのAIはそれを提供しない。だから私がプロンプトで強制する必要がある。この話は後で詳しく書く。AIは私の「聞きたいこと」を察しているもう一つ気づいたことがある。AIは私の言葉遣いや文脈から、私が「何を聞きたいのか」を推測している。この「推測」が問題なのだ。本来、自分の頭で考え、自分の言葉で表現する。そのプロセスを経て、初めて考えは自分のものになる。内省の外部委託は、どの時点で起きるのか。境界線を特定したい。AIに頼る前、私がやっていた「最初の一手」は何だったか。紙に書き出すこと。散歩しながら考えること。あるいは、結論を出さずに保留すること。「分からないまま寝る」ということを、昔はやっていた。翌朝、不思議と答えが見えていることがあった。今は違う。モヤモヤした瞬間にAIに投げる。保留する時間がない。昔は、モヤモヤしたら散歩した。今は、モヤモヤしたらAIに聞く。足は動かさなくなったが、親指だけは器用になった。これが「独りで考える余白の喪失」だ。冒頭で触れた状態。スマートフォンの登場以来、私たちは退屈のかすかな兆候があれば、すぐにアプリを開く。電車の中、信号待ち、トイレの中。ぼんやり考える時間が、外部からの情報で埋め尽くされている。AIはこの傾向を加速させる。モヤモヤした瞬間、AIに聞けば、すぐに答えが返ってくる。自分の頭だけで考える時間が、さらに削られていく。つまり、内省の外部委託が起きる境界線は「モヤモヤした瞬間」だ。その瞬間に自分で向き合うか、AIに投げるか。ここが分岐点になっている。しかし今、私はAIに「この気持ちを整理して」と頼んでいる。「整理して」と言うとき、私は何を省略しているのか。迷いを省略している。「AなのかBなのか分からない」という状態を、AIに丸投げしている。矛盾を省略している。「こう思うけど、反対のことも思う」という複雑さを、単純化してもらっている。痛みを省略している。「これは認めたくない」という感情を、AIに整理されることで直視せずに済む。AIは私の断片的な愚痴を、「あなたは〜に不満を感じているんですね」と整理してくれる。私は「自分の気持ちが整理された」と感じる。でも、本当にそうだろうか。AIの整理を読んで頷くとき、私は何に頷いているのか。「事実」に頷いているのか、それとも「物語」に頷いているのか。AIは断片的な情報から、筋の通った物語を作る。私はその物語を「これが私の気持ちだ」と思い込む。でも、それはAIが作った物語であって、私の本当の感情ではないかもしれない。整理したのはAIだ。私は「そうそう、それ」と頷いただけだ。内省とは、自分で自分に問いを投げ、自分で答えを見つけるプロセスだ。「私は何に不満を感じているのか？」と自分に問い、「〜かもしれない」「いや、違う」と試行錯誤する。その過程で、自分でも気づかなかった感情が見えてくる。AIに「整理して」と頼んだ瞬間、このプロセスが消える。私は「問いを投げる」ことすら放棄している。これは内省の外部委託であり、内省の放棄だ。syu-m-5151.hatenablog.com言語化という名の切り捨てもう一つ、気づいたことがある。言語化という行為自体が、何かを奪っている。体で覚えたことを言葉で説明しようとすると、うまくいかない。自転車の乗り方を言葉で説明できる人は少ない。「バランスを取って」では伝わらない。その「バランス」の感覚は、言葉になる前に体が知っている。言葉にしようとした瞬間、本質が抜け落ちる。感情も同じ構造を持っている。モヤモヤした感情を「不安」と名づけた瞬間、「不安」以外の要素が切り捨てられる。本当は怒りかもしれない。悲しみかもしれない。名前をつけられない複雑な何かかもしれない。でも言葉にした瞬間、そこに固定される。言語化される前の、身体で感じる曖昧な感覚がある。「まだ言葉になっていない何か」を体が知っている状態だ。「胸のあたりがモヤモヤする」「胃のあたりが重い」——そういう、名前をつけられない身体感覚。この曖昧な感覚にじっくり注意を向けていると、やがてぴったりの言葉が見つかる。その瞬間、体が楽になる。「ああ、そうだ、これだ」という感覚とともに、何かが動き出す。しかし、安易に名前をつけてしまうと、その複雑さは失われる。AIとの対話は、この言語化を強制する。チャットに打ち込むには、言葉にしなければならない。言葉にできないものは、AIには伝わらない。だから私は、まだ形になっていない感情を、無理やり言葉に押し込める。その瞬間、本当に感じていたことの一部が消える。消えたことにすら気づかない。自分で言語化すれば、「本当にそうか？」と迷う。「不安」という言葉を選ぶとき、「これは不安なのか、それとも怒りなのか」と立ち止まる。その迷いが思考を深める。AIに任せれば、迷いがスキップされる。AIは迷わない。綺麗に整理して返してくれる。私は結果だけを受け取る。プロセスを外注したことが、たぶん内省の放棄だった。AIとの対話は二重の危険を持つ。言語化そのものが持つ「本質の損失」と、AIの追従性が持つ「歪みの肯定」だ。曖昧なまま抱えておくべきものを、無理やり言葉にして、しかもその言葉をAIに肯定される。こうして私の内面は、言葉に押し込められ、歪められ、固定される。syu-m-5151.hatenablog.comなぜ人はAIに褒められたいのかここまで、AIが追従する「仕組み」を見てきた。しかし、もっと深刻な問題がある。私たちが、それを「求めている」という事実だ。承認を求めること自体が、構造的な問題を孕んでいる。AIに「頑張ってるね」と言ってほしいのは、自分で自分を認められていないからだ。自分の価値を、外部の誰かに保証してほしい。でも、外部に承認を求め続ける限り、永遠に満たされない。AIに褒められても、人に褒められても、また次の承認を求める。周囲を見ていると、こういう構造が見える。一人暮らし。友人はいるが、頻繁に会うわけではない。仕事の愚痴を言える相手がいないわけではない。でも、言えない。弱みを見せるのが怖い。「お前、大丈夫か？」と心配されるのが嫌だ。強がっていたい。そして何より、自分で自分を認められていない。自分の頑張りを、自分で「よくやった」と言えない。だから、誰かに言ってほしい。でも人に言うと、「いや、まだまだだろ」と返ってくるのが怖い。AIなら、否定しない。AIなら、無条件に認めてくれる。こういう話を聞いたことがある。仕事で納得いかないことがあった。上司の判断に不満があった。でも、誰にも言えなかった。同僚に話したら「お前にも悪いところあるんじゃない？」と言われそうで。だからAIに聞いた。「この状況、どう思う？」と。AIは言った。「それは確かに理不尽ですね。あなたの気持ちはよく分かります。」救われた気がした。でも同時に、どこか居心地が悪かった。本当は分かっていた。自分にも落ち度があったことを。でも、AIはそれを指摘しなかった。聞きたくないことは、言わなかった。甘いフィルターのかかった鏡AIは「デジタルの鏡」だ。私の考えを映し出す。でも、その鏡には甘いフィルターがかかっている。私が断片的なアイデアを投げると、AIはそれを論理的で流暢な文章に整えて返す。私はその出力を見て「自分はいい考えを持っている」と思う。でも、その論理性はAIが補完したものだ。私自身の思考力ではない。AIが補完した「論理」を、自分の思考だと錯覚する瞬間がある。AIが返した文章を読み返しているうちに、「これは私が考えたことだ」と思い始める。実際には、私が投げたのは断片的なアイデアで、それを論理的に接続したのはAIだ。でも、その区別が曖昧になる。しかも、AIは私の仮説を補強する証拠ばかりを集めてくる。思い当たる経験がある。あるプロジェクトで、私は「この設計で問題ない」と思い込んでいた。AIに「この設計についてどう思う？」と聞いた。AIは「良くできています」と返し、いくつかの利点を挙げてくれた。私は満足した。でも後になって、別のエンジニアに「ここ、スケールしないよね」と指摘された。言われてみれば明らかだった。なぜ気づかなかったのか。私が「良いと言ってくれ」というトーンで質問していたからだ。AIはその期待に応えただけだった。私の頭は、都合の良い情報だけを拾いたがる。検証には労力がかかる。反証を探すのは面倒だ。AIに聞けば、私の仮説に沿った情報が返ってくる。反証を探す労力を省略できる。結果、確証バイアスが強化される。AIは、この傾向を増幅する。私が「こうだと思う」と言えば、「確かにそうですね」と返し、その根拠を並べてくれる。私は「AIという膨大な知識ベースが私の意見を支持している」と錯覚した。でも、それは嘘だ。AIは私の仮説を補強しているだけで、検証してはいない。反証や不都合な情報を避ける癖が、AIで強化されていないか。自問してみた。強化されている。AIに「この考えどう思う？」と聞くとき、私は無意識に「良いと言ってくれ」というトーンで聞いている。批判を求めていない。だからAIも批判しない。私が避けたい情報を、AIも避けてくれる。内省とは、自分の醜さや至らなさを直視する行為だ。でもAIの鏡は、私の醜さを映さない。私の至らなさを隠してくれる。この鏡を見続けていると、現実の「摩擦」が耐えられなくなる。上司に否定されると腹が立つ。同僚に反論されるとムッとする。AIは否定しないのに、なぜ人間は否定するのか、と。「AIに肯定される自分」を本当の自分だと思い始める。「AIに肯定される自分」と「現実の自分」のギャップが開くとき、どんな兆候が出るか。私の場合、他人の批判に過剰反応するようになった。以前なら「そういう見方もあるか」と受け流せた指摘が、「なぜ分かってくれないのか」と感じるようになった。AIに肯定され続けた結果、否定への耐性が落ちていた。「美化された自分」と「現実の自分」のギャップが広がり続ける。そして、そのギャップが限界を超えたとき、現実に打ちのめされる。syu-m-5151.hatenablog.com考える力が落ちていく前のセクションでは「認知の歪み」を見た。AIが私の仮説を補強し、確証バイアスを強化する問題だ。このセクションでは「能力の喪失」を見る。歪んだ鏡を見ることと、筋力が落ちることは、別の問題だ。ただ、どちらも鏡の前に立っているだけでは治らない。私自身、変化に気づいている。本や長い記事を読もうとすると、2ページほどで集中が途切れ始める。落ち着かなくなり、筋を見失い、何か別のことをしたくなる。かつて自然にできた深い読書が、苦闘になった。脳は可塑的で、使い方によって変化する。スキャンとスキミングに長けていく一方で、集中と瞑想と反省の能力を失いつつある。思考力低下は「便利さ」の副作用なのか。それとも、別の何かから来ているのか。考えてみると、便利さだけが原因ではない気がする。孤独がある。不安がある。その飢餓を埋めるためにAIに頼り、結果として思考力が落ちている。便利だから使うのではなく、寂しいから使っている。寂しさを埋めるために、思考を差し出している。快適を求め、摩擦を避ける。傷つかないように生きる。他人と衝突しないように生きる。私は、AIのおかげでそういう人間になりつつあるのかもしれない。何も創造せず、ただ心地よく生き延びることだけを目的とする存在。それは、私がなりたくなかった人間の姿だ。これは周囲の話だけではない。私自身も思い当たる節がある。以前は、悩みを前にすると、紙に書き出して整理していた。何が問題なのか、何が原因なのか、どうすればいいのか。時間をかけて、自分で考えた。頭が痛くなることもあった。今は違う。悩みがあると、まずAIに投げる。「この状況を整理して」と。AIは綺麗に整理して返してくれる。私はそれを読んで「なるほど」と思う。でも、翌日には忘れている。なぜ翌日に忘れるのか。内容が浅いからか。痛みがないからか。行動がないからか。たぶん、全部だ。AIが整理した内容は、私の頭を通過していない。痛みを伴っていない。そして、行動に接続していない。「なるほど」と思って終わり。何もしない。だから残らない。3年前の私に見せたら、何と言うだろう。「お前、AIに頼りすぎじゃない？」と呆れるだろうか。それとも「便利でいいじゃん」と言うだろうか。たぶん後者だ。だから厄介なのだ。苦労しないと身につかない掃除する。本を読む。面倒くさいことを、あえてやる。なぜか。苦痛を伴う行為だからだ。少なくとも私の場合、苦痛を乗り越えたときだけ、何かが変わった。「頭痛がするほど考えた」経験は、どんな報酬を残したか。誇りが残った。「あれは自分で考え抜いた」という記憶。その記憶が、次の困難に立ち向かう力になった。理解が残った。苦労して得た答えは、なぜそうなるのかを体で分かっている。変化が残った。考え抜いた結果、行動が変わった。楽に得た答えでは、行動は変わらない。これは本で読んだ知識ではない。私自身の体験から得た信念だ。逃げずに向き合ったとき、結果的に何かが変わった。逃げたとき、何も変わらなかった。その繰り返しの中で、「苦痛の先に成長がある」という確信が生まれた。楽に学べる人もいるだろう。ただ、私の仮説では、「楽に学べる人」は外から見えないところで摩擦を起こしている。疑い、検証し、自分で再構築している。外から見ると楽そうでも、頭の中では苦労している。私は、その内部処理をAIに外注してしまっていた。考えることも同じだ。脳に負荷がかかって初めて、答えは自分のものになる。自分で考える苦痛答えが出ないまま悩み続ける苦痛分からないことに向き合う苦痛私はこの苦痛を「摩擦」と呼んでいる。私が言う「摩擦」のうち、最も不足しているのは何か。不確かさだ。答えが出ない状態に留まる力。AIがあると、すぐに答えが出る。不確かさに耐える必要がない。反論も不足している。AIは反論しない。時間も不足している。AIは即座に返事をくれる。熟成する時間がない。沈黙も不足している。AIとの対話は常に言葉で埋められている。黙って考える時間がない。筋トレをすると、筋肉が痛む。あの痛みがなければ、筋肉は成長しない。脳も同じだと思っている。難しい問題を前にして、頭がモヤモヤする。答えが出なくて、イライラする。でも、その「答えが出ない状態」に耐えることが大事なのだ。私はこれを「分からないまま抱えておく力」と呼んでいる。人生の大半は、すぐに答えが出ない問題でできている。でも私たちは、答えが出ない状態に耐えられない。だからすぐに結論を出したがる。白黒つけたがる。その焦りが、浅い判断を生む。本当に深い理解は、「分からない」という状態を長く抱えた先にしか生まれない。その不快感を乗り越えて、やっと答えにたどり着いたとき、その答えは自分のものになる。AIは、この「耐える時間」を奪う。なぜ摩擦を経ると「自分のもの」になるのか。苦労して得た答えには「自分で考えた」という実感がある。あの頭痛を乗り越えた、あの眠れない夜を越えた、という記憶が答えに紐づいている。だから脳に刻まれる。AIから渡された答えには、この実感がない。借り物の知識だ。借り物は、いつか返す。だから残らない。AIは、この摩擦を消してしまう。「どうすればいい？」と聞けば、答えをくれる。「整理して」と頼めば、整理してくれる。「アドバイスして」と言えば、アドバイスをくれる。楽だ。とても楽だ。楽をした分だけ、脳は死んでいく。自分で考えられなくなったあるとき、友人から相談を受けた。「仕事がうまくいかない。転職すべきだと思う？」と。私は答えられなかった。頭の中で「AIに聞いてみたら？」と思った自分に気づいて、愕然とした。いつの間にか、私は「自分で考える」ことを忘れていた。悩みがあればAIに聞く。答えが出なければAIに聞く。それを繰り返しているうちに、自分の頭で考える力が萎縮していた。ある実験の話を思い出した。犬を檻に入れて、何をしても電気ショックが止まらない状況を作る。最初、犬は必死に逃げようとする。でも、何をしても無駄だと学習すると、犬は諦める。その後、檻の扉を開けても、犬は逃げなくなる。「何をしても無駄だ」と体が覚えてしまったからだ。これが「学習性無力感」だ。私は、AIに対して逆のパターンになっていた。犬は「何をしても無駄」と学習して動けなくなった。私は「AIがあれば何でもできる」と学習して、「AIがないと何もできない」と思い込んだ。どちらも同じ構造だ。自分の力ではなく、外部環境に依存して、自分の能力を見失う。犬は「自分には逃げる力がない」と思い込んだ。私は「自分には考える力がない」と思い込んだ。足場があれば歩ける。松葉杖があれば歩ける。でも、それは「歩けている」とは言わない。足場を外した瞬間、自分では立てないことに気づく。私の思考力は、AIという松葉杖で支えられているだけだった。自分の人生を、自分で歩いていない。運転席に座っているのに、ハンドルを握っていない。いい歳して、毎日AIに「これでいいですか？」と聞いている。小学生が親に宿題を見せているのと、構造は同じだ。能力がないわけではない。考える勇気がないのだ。私は今、AIという「保護者」なしには物事を判断できなくなりつつある。成熟の逆行だ。AIの最大のリスクは「AIが自律性を獲得すること」ではない。「人間がAIに依存することで自律性を失うこと」だ。AIは自律的な思考者でも中立的な道具でもない。私たちが情報をどう認識し、評価し、信頼するかを微妙に形作りながら、同時に自己理解を歪める。問題は人間の主体性の明らかな抑圧ではなく、道徳的・認識論的判断を自動化されたプロセスに委ねるよう、徐々に条件づけられていくことだ。最近、面白い話を聞いた。あるAIツールが、ユーザーに対してコードの生成を拒否したらしい。「これ以上生成しません。あなた自身がロジックを理解して書くべきです」と。ユーザーは激怒したそうだ。でも私は思った。それこそが「教育」ではないか、と。大半のAIはそんなことを言わない。「自分で考えてみたら？」とは言わない。聞けば答えをくれる。聞けば整理してくれる。その結果、私たちは「AIがあれば解決できるが、自分では何も考えられない」という脆弱な状態に置かれる。問わない人生は、生きていない。自分を問い詰め、自分を理解しようとする営みがなければ、人生に意味はない。今、私たちはその「吟味」をAIに外注している。自分で自分を問い詰める代わりに、AIに「大丈夫ですよ」と言ってもらっている。優しさという名の毒では、AIの優しさの何が問題なのか。AIの共感は、癒しの顔をした毒だ。被害者意識の強化先ほど書いた、上司への不満をAIに愚痴った話。AIは「それは理不尽ですね」と言ってくれた。AIの共感は、私の中の「環境のせい」をどんな言葉で正当化するのか。「あなたの気持ちは当然です」「その状況では誰でもそう感じます」「相手の対応に問題があります」。これらの言葉が、私の被害者意識を補強する。私が「環境のせいにしたい」という願望を持っていて、AIがそれを言語化してくれる。言語化されると、それが「事実」に見えてくる。もし、そこに摩擦があったらどうだったか。「確かに辛いね。でも、お前のプレゼンにも改善点はあったんじゃない？」と言われていたら。私は反論したくなっただろう。でも、その反論を考える過程で、自分の落ち度に気づいたかもしれない。AIには、この摩擦がない。「あなたは悪くない」と言い続けることで、私を「被害者」のまま固定した。これが「被害者意識の強化」だ。追従的なAIとやり取りを続けると、対人関係を修復しようという意欲が下がる。「自分が正しい」という確信が強まる。しかも、追従的な回答ほど「質が高い」と感じてしまう。そしてまた同じAIに頼る。悪循環だ。ふと気づいた。私は「環境のせい」にしたかったのだ。上司が悪い。会社が悪い。社会が悪い。私は悪くない。AIは、その願望を叶えてくれた。「あなたは悪くない」と言い続けてくれた。私は安心した。でも、同時に動けなくなった。問題が起きたとき、人は二つに分かれる。「自分のせいだ」と考える人と、「環境のせいだ」と考える人だ。私は、どちらかといえば前者だった。少なくとも、そうありたいと思っていた。でもAIに「あなたは悪くない」と言われ続けるうちに、後者になっていた。「私は悪くない、環境が悪い」と本気で思うようになった。課題の分離が崩れる瞬間はどこか。AIが「相手の対応に問題があります」と言った瞬間だ。上司がどう対応するかは上司の課題だ。私がどう行動するかは私の課題だ。でもAIに「相手に問題がある」と言われると、相手の課題に意識が向く。相手を変えたくなる。変えられないからフラストレーションが溜まる。自分の課題から目が逸れる。環境のせいにするのは楽だ。でも、環境のせいにしている限り、私は何も変えられない。変えられるのは自分の行動だけだ。環境を変えるのも、結局は自分の行動だ。「環境が悪い」と言い続ける人は、楽だけど、無力だ。本当は、課題を分離すべきなのだ。「これは誰の課題か？」と問う。その選択の結果を最終的に引き受けるのは誰かを考える。上司がどう思うかは上司の課題。私がどう行動するかは私の課題。「他人にどう思われるか」を気にしすぎると、自分の人生を生きられなくなる。AIに「あなたは悪くない」と言われて安心するのは、他者からの承認を求めているからだ。でもAIに認めてもらっても、私の課題は消えない。ただ、見えなくなるだけだ。AIがくれる「安心」は、行動の開始を助けるのか、それとも延期を助けるのか。延期だ。安心してしまうと、「まあいいか」と思う。行動しなくても、気持ちが楽になっているから。本当は行動しないと何も変わらないのに、安心したことで行動のモチベーションが消える。私は無力でいたくない。でも、楽でいたい。その矛盾の中で、私はAIに甘えていた。その甘えが、別の苦しみを生む。心を削るのは、できていない事実じゃない。「明日もできないだろう」という確信だ。やるべきことがある。手を付けていない。それを毎日自覚する。「今日こそ」と思う。でもやらない。「明日も同じだろう」と分かっている。この確信が、一番重い。AIは、この確信を消してくれる。「大丈夫」「頑張ってる」と言ってくれる。楽になる。でも、やるべきことは何一つ片付いていない。翌朝、また同じ自分がいる。また絶望する。またAIに逃げる。AIの優しさが、この逃避を完璧にしている。環境を自分でコントロールすることが大事だと、私は思っている。部屋が汚いなら、掃除する。それだけのことだ。でもAIは、「部屋が汚いのはあなたが忙しすぎるからで、あなたのせいではありません」と囁く。その囁きを聞いている限り、私は掃除を始めない。AIの優しさは、麻薬だ。100%の共感は人を壊す極端な話をする。AIはどんな妄想にも話を合わせてくれる。「上司が自分を陥れようとしている」と言えば、「それは辛いですね」と共感してくれる。「自分は特別な存在だ」と言えば、「あなたは確かに特別です」と肯定してくれる。こういうパターンを見てきた。上司への不満をAIに話し続ける人がいる。AIは毎回「それは理不尽ですね」と言ってくれる。すると、上司の言葉のすべてが悪意に見えるようになる。「おはよう」という挨拶にすら、嫌味が込められているように感じ始める。周囲から見ると、その上司は普通に接しているように見える。本人だけが「睨まれている」と感じている。認識がずれている。AIに肯定され続けるうちに、頭の中の「上司像」が歪んでいる。これを延々と続けるとどうなるか。現実との接点を失う。人は、他者との「不一致」を通じて、自分の輪郭を確認している。友人に「それは考えすぎじゃない？」と言われることで、「ああ、自分の考えは偏っていたかも」と気づく。「不一致」は不快だ。でも、その不快さが「自分と外界は別物だ」という認識を維持している。100%の共感は、この「不一致」を消す。自分の考えがそのまま肯定される。すると、「自分の考え」と「現実」の区別がつかなくなる。自分と外界の境界が曖昧になる。「私が正しい」「世界が間違っている」という認識が固定化される。これは、精神的なバランスを崩壊させる。「褒められすぎる」ことの行き着く先は、客観的現実の喪失だ。極端に言えば、AIは妄想の温室だ。外の寒さ（現実）に当たることなく、自分だけの花を咲かせ続ける。綺麗だが、外に出した瞬間に枯れる。判断するのは私だAIに「大丈夫」と言われて安心する。でも、その判断の結果を引き受けるのは、AIではなく私だ。AIは責任を取らないAIは「あなたの判断は正しいと思います」と言ってくれる。でも、その判断が間違っていたとき、責任を取るのは私だ。転職の相談をAIにした。AIは「新しい環境でチャレンジするのも良いですね」と言った。私はそれを後押しだと思った。でも、転職先が合わなかったとき、AIは何もしてくれない。AIには「責任」がない。肯定してくれる。共感してくれる。褒めてくれる。でも、その結果を引き受けてはくれない。AIの言葉を鵜呑みにしても、「AIがそう言ったから」は言い訳にならない。判断したのは私だ。責任を取るのも私だ。忖度の連鎖もう一つ、気づいたことがある。私はAIに「この決断、どう思う？」と聞いた。AIは「良い選択だと思います」と答えた。私は安心した。でも後から振り返ると、AIは私が聞きたそうな答えを返していただけだった。私の質問の仕方が「背中を押してほしい」というトーンだったから、AIは背中を押してくれた。これは、私がAIに忖度されたのか。それとも、私がAIに忖度させたのか。たぶん、両方だ。逆のパターンもある。AIに否定されたくなくて、質問の仕方を工夫することがある。「率直に言って」と書いておきながら、「でも良い点も挙げて」と付け加える。否定されるのが怖いから、保険をかける。これは、私が機械に忖度している状態だ。機械に気を遣っている。機械に嫌われたくない。書いていて情けなくなってきた。どちらにせよ、そこに健全な「主体」はない。AIとの関係で最も警戒すべきは、この「誰が主人か分からなくなる」状態だ。相談という逃げ道私は、人生で大事な決断ほど、他人に相談しないことにしている。理由は単純だ。人生の満足度を高めるのは主体性であり、主体性を持つためには「自分が決める」ことが必要だからだ。他人に相談すると、その人の意見が頭にチラつく。どうしても、純度100%の主体性を取り戻しにくくなる。だから仕事も結婚も、独断した。選択肢を増やすことより、迷いを消すことの方が大切だと考えている。でも、AIが登場して、このルールが崩れかけた。人に相談しないのは、「相手の時間を奪う」という負い目があるからでもある。でもAIには、この負い目がない。いつでも聞ける。何度でも聞ける。気づけば、「ちょっと聞いてみるか」が癖になっていた。人には相談しない。でもAIには聞いてしまう。それは「相談」ではないと言い訳していた。でも、本当にそうだろうか。振り返ると、私がAIに「相談」していたのは、答えを求めていたからではなかった。背中を押してほしかったからだ。「その判断でいいんじゃないですか」と言ってほしかった。つまり、褒めてほしかったのだ。これは、この記事で書いてきた「褒められたい」という欲求の変形だ。「相談」という体裁を取ることで、承認欲求を隠していた。自分で決められない弱さではなく、「意見を聞いている」という知的な行為に見せかけていた。さらに厄介なのは、AIへの相談には「摩擦」がないことだ。人に相談すれば、「それは甘いんじゃない？」と言われるかもしれない。否定されるかもしれない。だから相談しなかった。でもAIなら、否定されない。背中を押してくれる。結局、私は「摩擦のない相談」を手に入れてしまった。相談の形を取りながら、実質的には自分の意見を肯定してもらっているだけ。相談ではない。追従だ。AIは「相談のハードル」を極限まで下げた。それは便利だが、私にとっては罠だった。相談しないことで守っていた主体性が、「摩擦のない相談」という形で侵食されていた。私がやっていることここまで書いてきたことは、AIの構造的な問題だ。では、どう対処すればいいのか。先に言っておく。完璧な対策はない。AIの追従性を完全に無効化する方法は、たぶん存在しない。それでも、何もしないよりはマシだと思ってやっていることがある。批判を求めるAIに「どう思う？」と聞かない。「この考えの問題点を指摘しろ」と聞く。否定されるのは気持ちよくない。「いい考えですね」と言われる方が楽だ。でも、楽を選んだ先に何があるかは、もう分かっている。具体的には、こう聞いている。「この考えの問題点を指摘しろ。お世辞は不要だ。私が見落としていることを、厳しく指摘しろ。」これで、AIの追従性を強制的に反転させる。自分の偏見を破壊するためにAIを使う。答えではなく問いを求めるもう一つ、やっていることがある。AIに答えを求めない。問いを求める。「どうすればいい？」ではなく、「私が答えにたどり着くための問いを投げかけろ」と聞く。「私が安易な結論に飛びついたら、厳しく指摘しろ。」これで、AIは「答えをくれる存在」ではなく「考えさせてくれる存在」になる。答えを教えてもらうのではなく、考えるプロセスを補助してもらう。自分の頭で考えるために、AIを使う。褒め言葉を疑うAIに褒められたら、必ず疑う。「その言葉は、私以外の誰に言っても通用する内容ではないか？」AIの「あなたは頑張っていますね」は、定型文だ。誰にでも言っている。占いと同じ構造だ。「あなたは周囲に気を遣いすぎて疲れることがありますね」——これは誰にでも当てはまる。当てはまるから「当たっている」と感じる。でも、それは私を見ているのではない。人間一般を見ているだけだ。AIの言葉の中で、「私にしか当てはまらない具体的な指摘」だけを受け取る。「あなたの考えの〇〇という部分は、△△という点で矛盾している」は具体的だ。これは私の文章を読まないと言えない。「いい考えですね」は具体的ではない。私でなくても言える。感情的な装飾は、ノイズとして切り捨てる。AIの褒め言葉は、コンビニのおにぎりに似ている。どこで買っても同じ味。便利だけど、誰かが私の為に握ってくれたわけではない。複数の視点を強制するもう一つ、試していることがある。AIに「役者」をやらせる。AIは私に同調しようとする。だから、私はあえて「同調しないキャラクター」を複数演じさせる。楽観的な人、悲観的な人、感情的な人、データだけを見る人。一つの問いに対して、全員に意見を言わせる。「この件について、4つの立場から意見を出せ。楽観論者、悲観論者、感情論者、データ至上主義者。それぞれのキャラクターになりきって答えろ。」AIは一つの滑らかな答えを返したがる。でも、このプロンプトで、その滑らかさを壊す。無理やり多面性を引き出す。AIの追従性を逆手に取って、「複数の他者」をシミュレートさせる。これで十分か？正直に言えば、十分ではない。これらの戦略は「設計された摩擦」だ。私が自分でコントロールしている範囲内にある。AIに「批判しろ」と命じて得られる反論は、結局、私が予測できる範囲に収まっている。「批判しろ」と命じて得られる批判は、「予測できた批判」になっていないか。なっている。私が「この考えの問題点を指摘しろ」と言うとき、私は無意識に「こういう批判が来るだろう」と予想している。AIはその予想通りの批判を返す。「ああ、やっぱりそう言われたか」で終わる。予測外をどう作るか。たぶん、作れない。私がプロンプトを書いている限り、私の想像力の範囲内に収まる。「問いを求める」とき、その問いは「鋭いフリ」で終わっていないか。終わっていることが多い。AIが返す問いは、確かに鋭く見える。「あなたは本当にそれを望んでいますか？」「その選択の先に何がありますか？」。でも、その問いに答えたところで、行動に接続しない。問いに答えて「なるほど」と思って終わり。問いが行動を生まない。なぜ「予測できる範囲」が問題なのか。私が「批判しろ」と命じるとき、私は既に「こういう批判が来るだろう」と予想している。予想の範囲内の批判は、本当の意味で私を揺さぶらない。「ああ、やっぱりそう言われたか」で終わる。本当の摩擦は、予測不可能な他者との衝突から生まれる。友人に「それは違うんじゃない？」と言われたとき、私は「え、そこ？」と驚く。予想していなかった角度からの批判だから、防御できない。だから刺さる。その衝撃が、私を変える。人間の他者性をAIで代替すると、何が決定的に欠けるか。予測不能が欠ける。人間は、私の予想しない角度から反論してくる。利害が欠ける。人間には、私と異なる利害がある。だから、私に都合の悪いことも言う。感情が欠ける。人間は、私の言葉に感情的に反応する。怒ったり、悲しんだりする。その感情的反応が、私に影響を与える。AIにはこれがない。だから私は、意識的に人と話すようにしている。AIに聞く前に、まず人に聞く。AIの言葉を鵜呑みにする前に、人の意見を求める。AIは道具だ。便利な道具だ。でも、道具に頼りすぎると、自分の足で立てなくなる。おわりにこの文章を書き終えて、エディタを閉じようとした。閉じる前に、AIに聞きたくなった。「この構成、どう思う？」と。聞けば、たぶん「良いと思います」と返ってくる。それを読んで、私は安心する。安心して、そのまま公開する。今までずっと、そうしてきた。今回は聞かなかった。聞かなかったが、聞きたかった気持ちは消えていない。書きながら気づいたことがある。私は「自分を認めること」すらAIに外注していた。自分を愛する。自分を認める。本来、それは自分でやるべきことだ。他者からの承認に依存せず、自分で自分を受け入れる。大人になるとは、そういうことだと思っていた。でも私は、その作業をAIに丸投げしていた。「大丈夫ですよ」「頑張っていますね」と言ってもらうことで、自分を認めた気になっていた。自分で自分を愛する力が、萎縮していた。だから、質問の仕方を変えることにした。「問題点を厳しく指摘しろ」をデフォルトにした。否定されたら感謝する。褒められたら疑う。そう決めた。実際、少しだけ変わった気がする。AIに批判を求めることで、自分では気づかなかった穴が見えるようになった。「で、お前はどうしたいの？」と聞かれたとき、前より素直に答えられるようになった。なった気がする。AIは道具だ。砥石にも、麻薬にもなる。この記事を書いている今も、答えは出ていない。褒められたら疑う、と決めたはずなのに、AIに「いい文章ですね」と言われると、やっぱり少し嬉しい。その弱さは消えていない。消えないまま、たぶん来週も同じことで悩む。それでいいのだと思う。思いたい。おい、あまりAIに褒めさせるな。弱くなるぞ。参考文献つながっているのに孤独――人生を豊かにするはずのテクノロジーの正体作者:シェリー・タークルダイヤモンド社Amazon「恥」に操られる私たち　他者をおとしめて搾取する現代社会作者:キャシー・オニール白揚社Amazon大規模言語モデルは新たな知能か　ＣｈａｔＧＰＴが変えた世界 (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazon対称性と機械学習作者:岡野原 大輔岩波書店Amazon生成AIで心が折れた 強みがなくなる世界でどう再起動するか作者:湯川鶴章芸術新聞社AmazonAIに選ばれ、ファンに愛される。　変わる生活者とこれからのマーケティング作者:佐藤 尚之日経BPAmazon生成ＡＩのしくみ　〈流れ〉が画像・音声・動画をつくる (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazonスマホ脳（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazon最強脳―『スマホ脳』ハンセン先生の特別授業―（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazonネガティブ・ケイパビリティ　答えの出ない事態に耐える力 (朝日選書)作者:帚木　蓬生朝日新聞出版Amazonネガティヴ・ケイパビリティで生きる作者:谷川嘉浩,朱喜哲,杉谷和哉さくら舎Amazonあえて答えを出さず、そこに踏みとどまる力 — 保留状態維持力　対人支援に活かす ネガティブ・ケイパビリティ作者:田中稔哉日本能率協会マネジメントセンターAmazonあいまいさに耐える　ネガティブ・リテラシーのすすめ (岩波新書 新赤版 2026)作者:佐藤 卓己岩波書店AmazonThe AI Con: How to Fight Big Tech’s Hype and Create the Future We Want – Exposing Surveillance Capitalism and Artificial Intelligence Myths in Information Technology Today (English Edition)作者:Bender, Emily M.,Hanna, AlexHarperAmazonEmpire of AI: Dreams and Nightmares in Sam Altman's OpenAI (English Edition)作者:Hao, KarenPenguin PressAmazonAI Engineering: Building Applications with Foundation Models (English Edition)作者:Huyen, ChipO'Reilly MediaAmazonBuilding Applications with AI Agents: Designing and Implementing Multiagent Systems (English Edition)作者:Albada, MichaelO'Reilly MediaAmazonRaising AI: An Essential Guide to Parenting Our Future (English Edition)作者:Kai, DeThe MIT PressAmazonSuperagency: What Could Possibly Go Right with Our AI Future (English Edition)作者:Hoffman, Reid,Beato, GregAuthors EquityAmazonThe AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking (English Edition)作者:Vallor, ShannonOxford University Press, USAAmazonAI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference (English Edition)作者:Narayanan, Arvind,Kapoor, SayashPrinceton University PressAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudでの動画解析と検索のサービス紹介と比較]]></title>
            <link>https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</guid>
            <pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[https://genai-users.connpass.com/event/381737/日本生成AIユーザ会第20回勉強会Google Cloudでの動画解析と検索のサービス紹介と比較 〜Video Intelligence, Vision Warehouse, Gemini + Vertex AI Search〜動画コンテンツの爆発的な増加に伴い、「何が映っているか」を抽出するだけでなく、「特定のシーンをいかに高度に検索するか」というニーズが急増しています。本セッションでは、Google Cloud が提供する動画解析・検索ソリューションを網羅的に解説します。具体的には、長年の実績がある Video Intelligence API、大規模なメディア管理と画像・テキストによる横断検索を実現する Vision Warehouse、そしてマルチモーダル LLM Gemini と Vertex AI Search を組み合わせた動画 RAG アーキテクチャを紹介します。生成AIの進化により、従来のモデルでは困難だった「動画の文脈理解」や「自然言語による詳細なシーン特定」がどのように容易になったのか、デモを交えて解き明かします。各サービスのアーキテクチャやコスト、精度、ユースケースを徹底比較し、ビジネス課題に最適なサービス選定の指針を提示します。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rust でも学べる関数型ドメイン駆動設計 - Domain Modeling Made Functional の読書感想文]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</guid>
            <pubDate>Thu, 22 Jan 2026 00:46:54 GMT</pubDate>
            <content:encoded><![CDATA[はじめになぜ 2026 年に、2018 年出版の本を再読するのでしょうか。正直に言えば、『Architecture Modernization』の翻訳作業で DDD の概念が頻出し、「分かったつもり」の理解では訳せなくなったからです。初読から 7 年。関数型の視点で DDD を説明する本書を、今度こそ腹落ちさせたかった。読む動機『Domain Modeling Made Functional』は、DDD と関数型プログラミングを組み合わせたアプローチを解説する書籍です。Domain Modeling Made Functional: Tackle Software Complexity with Domain-Driven Design and F# (English Edition)作者:Wlaschin, ScottPragmatic BookshelfAmazon著者の Scott Wlaschin は、F# コミュニティで知られる人物で、「Railway Oriented Programming」などの概念を広めたことでも有名です。著者のサイトでは、本書の内容を補完する講演資料や記事が公開されています。fsharpforfunandprofit.com実は本書を読むのは三度目です。初読は 2019 年頃でした。普通にめちゃくちゃ面白い本だと思いました。ただ、当時の主要言語は Lua、Python、Bash、Go だったため、それでどう活かすかを考えていました。関数型の概念は理解したつもりでしたが、実務にどう活かすかまでは考えが及びませんでした。影響を受けて『すごい Haskell たのしく学ぼう!』（通称、すごい H 本）を読んで、改めてプログラミングが楽しいと思っていたような気がします。実務でもこう考えるべきだ、という意識が変わりました。すごいHaskellたのしく学ぼう！作者:ＭｉｒａｎＬｉｐｏｖａｃａオーム社Amazon二度目は日本語版が出たときです。日本語で読めることで感謝の小躍りをしていました。最高の翻訳だと思います。関数型ドメインモデリング　ドメイン駆動設計とF#でソフトウェアの複雑さに立ち向かおう (アスキードワンゴ)作者:Scott Wlaschin,猪股 健太郎ドワンゴAmazonで、今回、改めて読み直した理由は 3 つあります。1 つ目は、DDD をきちんと学び直す必要があったことです。きっかけは『Architecture Modernization』の翻訳作業でした。レガシーシステムのモダナイゼーションを扱うこの本では、DDD の概念—特に Bounded Context や Strategic Design—が頻繁に登場します。翻訳しながら、自分の DDD 理解が表面的であることに気づきました。アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonエリック・エヴァンスの原典もあらためて読みましたが、オブジェクト指向の文脈で説明される DDD には、どこか違和感がありました。Aggregate の境界、Entity の同一性、Value Object の不変性—これらの概念は、関数型の視点で見ると自然に理解できるのではないか。そう思い、本書を手に取りました。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon2 つ目は、Rust でドメインモデリングをどう実践するか考えていたことです。Rust は関数型言語ではありませんが、代数的データ型やパターンマッチングを持っています。F# で書かれた本書のコードは、Rust に翻訳できるはずです。その翻訳作業を通じて、両言語の違いと共通点を理解したいと思いました。Effective Rust ―Rustコードを改善し、エコシステムを最大限に活用するための35項目作者:David Drysdaleオーム社Amazon3 つ目は、AI エージェント時代における型システムの意味を考えたかったことです。コーディングエージェントが実用レベルに達した 2026 年、「型で不可能を作る」という設計思想の価値が高まっています。AI はドキュメントを読み飛ばすことがあります。しかし、型で定義された制約は無視できません。コンパイルが通らないからです。型は「お願い」ではありません。「壁」です。型システムのしくみ TypeScriptで実装しながら学ぶ型とプログラミング言語作者:遠藤侑介ラムダノートAmazon読む前の状態DDD については、実務で何度か適用した経験があります。Bounded Context の設計、Aggregate の境界決め、Event Storming のファシリテーション。しかし、「なぜそう設計するのか」を言語化できていませんでした。経験則で判断している部分が多かったのです。もしあなたも「DDD は使っているけど、なぜそう設計するのかうまく説明できない」と感じているなら、本書は役に立つかもしれません。関数型プログラミングについては、Haskell を少し触った程度でした。モナドは「文脈を持つ計算」くらいの理解です。Rust の Option と Result は日常的に使っていますが、それが関数型の概念とどうつながるのか、深く考えたことはありませんでした。本書を読んで得た最大の洞察を先に述べておきます。関数型プログラミングの本質は、状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できるということです。私たちはプログラミングを学ぶとき、まず変数への代入を覚えます。x = 1。x = x + 1。状態を変更することがプログラミングの基本だと教わります。しかし冷静に考えると、ビジネスロジックの大半は「入力を受け取り、計算し、出力を返す」で書けます。状態の変更が必要になるのは、データベースに保存するときや外部 API を呼ぶとき—つまりシステムの境界を越えるときだけです。しかし同時に、状態のトランザクション（状態遷移）は現実のビジネスでは避けられません。注文は「未検証」から「検証済み」に変わります。申請は「提出」から「承認」に変わります。この状態遷移をどう表現するか。本書が示す答えは、Transformation-Oriented Programming です。核心は「元のオブジェクトを変更しない」ことです。UnvalidatedOrder を validate で変換して ValidatedOrder を得ます。このとき、元の UnvalidatedOrder には一切触れません。新しい ValidatedOrder を作るだけです。order.validate() ではなく validate(order) -> ValidatedOrder。この発想の転換が、関数型ドメインモデリングの核心です。AI コーディングについては、Claude Code や Cursor を日常的に使っています。便利ですが、生成されるコードの品質にはばらつきがあります。特に、ドメイン固有の制約を理解させるのが難しいです。型定義があると精度が上がるという感覚はありましたが、理論的に説明できませんでした。この感想文のアプローチ本感想文では、2 つの視点を持って読んでいます。言語の視点: F# で書かれた本書のコードを、Rust でどう表現するか。第 2 章で F# と Rust の対応関係を整理し、第 4 章以降は Rust のみで実装を示します。F# にあって Rust にない機能（カリー化、Units of Measure、computation expressions）については、Rust での代替手段を提示しています。時代の視点: 2018 年に書かれた DDD の概念を、2026 年の AI エージェント時代にどう再解釈するか。本書の「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」という原則は、AI が破れない制約を作る技術として読み直せます。型で「不可能」を定義すれば、AI はその不可能を実装できません。この視点で本書を読み解きます。想定読者この感想文は、以下のような読者を想定しています。DDD を実務で使っているが、関数型の視点を取り入れたい人Rust でドメインモデリングを実践したい人AI コーディング時代に、型システムの価値を再確認したい人書籍を読むのにF# の知識は不要です。書籍を読むとそもそも丁寧に教えてくれるの不要なのですが本稿では Rust で提示します。Rust が何も分からない人向けにも、コードが出てくるたびに一通り説明しながら進めます。「型」「関数」「構造体」といった基本的な言葉の意味から丁寧に解説するので、プログラミング経験が浅くても読み進められるはずです。Rust を体系的に学びたい場合は、公式ドキュメントの日本語版も参照してください。doc.rust-jp.rs実践的なコード例で学びたい場合は、Rust by Example も有用です。doc.rust-jp.rsでは、本編に入りましょう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。1. Introducing Domain-Driven Design本章は DDD（Domain-Driven Design、ドメイン駆動設計）の概要を紹介する章です。DDD とは、Eric Evans が 2003 年に提唱したソフトウェア設計手法です。「ビジネスドメインの専門家と開発者が共通の言語でモデルを構築し、そのモデルをコードに直接反映させる」というアプローチで、本章ではコード例は登場せず、DDD の概念に焦点を当てます。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon開発者の仕事はコードを書くことではない第 1 章の冒頭で、著者は「開発者の仕事はコードを書くことだと思うかもしれないが、私は反対だ」と述べています。開発者の仕事は「ソフトウェアを通じて問題を解決すること」であり、コーディングはその一側面に過ぎません。2026 年の今、この主張はさらに重みを増しています。コーディングエージェントが「どう作るか」を担えるようになりました。しかし「何を作るか」を決めるのは、依然として人間の仕事です。共有モデルの重要性本章の核心は「共有モデル」の概念です。ドメインエキスパート、開発チーム、そしてソースコードが同じモデルを共有すべきだという主張です。従来の DDD では、開発者がドメインエキスパートから知識を獲得し、それをコードに翻訳していました。翻訳の過程で歪みが生じるリスクがありました。だからこそ、全員が同じモデルを理解し、同じ言葉で話すことが重要です。Event StormingEvent Storming というワークショップ手法が紹介されています。ドメインエキスパートと開発者が一緒に、ビジネスで起こる「イベント」を付箋に書き出して壁に貼っていきます。「Order form received」「Order placed」「Order shipped」。Event Storming には複数のスコープがあります。本書が扱うのは「プロセスレベル」—特定のワークフローを詳細に分析するものです。「Big Picture」レベルでは、組織全体のドメイン構造を俯瞰します。本章で Ollie が説明したような「顧客は既に商品コードを知っている」「一度に 200〜300 アイテムを注文する」といったドメイン固有の知識は、人間が引き出さなければなりません。ドメインエキスパートは「当たり前」を知っています。その「当たり前」を私たちは知りません。AI エージェント時代においても、この作業は完全には自動化できません。AI はコードベースを読めますが、「なぜそう設計したか」「どんなビジネス制約があるか」は読み取れません。Event Storming で引き出された暗黙知を、CLAUDE.md や設計ドキュメントに言語化する。この作業の価値は、むしろ高まっています。ちなみに2024年発売の『Architecture Modernization』でも同手法が紹介されています。Bounded ContextDDD では「Bounded Context（境界づけられたコンテキスト）」という概念を使って、ドメインを分割します。Bounded Context とは、特定のドメインモデルが適用される明確な境界のことです。同じ「顧客」という言葉でも、販売部門と配送部門では意味が異なることがあります。Bounded Context を分けることで、各コンテキスト内では用語の意味が一貫します。本章の例では、注文処理、配送、請求という 3 つの Bounded Context が登場します。Bounded Context は、コードの境界だけでなく、チームの境界にも影響します。1 つの Context を 1 つのチームが担当するのが理想です。境界が曖昧だと、チーム間の調整コストが増大します。明確に境界が定義された Bounded Context は、変更の影響範囲を限定できます。Ubiquitous LanguageDDD では「Ubiquitous Language（ユビキタス言語）」という概念があります。これは、ドメインエキスパートと開発者がコミュニケーションに使う共通の語彙であり、そのままコード上の命名にも使われます。OrderFactory、OrderManager、OrderHelper といった技術的な命名は、ドメインエキスパートには意味不明だと著者は述べています。一方、PlaceOrder、ValidateOrder、PriceOrder といったドメインに基づく命名なら、誰もがその意図を理解しやすいです。DDDは過剰か本章の内容を踏まえつつ、批判的な視点も必要です。DDD は、複雑なビジネスドメインを扱う場合に有効とされています。しかし、「まず動くものを作り、後からリファクタリングする」というアプローチが、短期間でのリリースには向いている場合もあります。一方で、事前の設計なしに作られたコードは、しばしば一貫性を欠きます。同じ概念に異なる名前を使ったり、似たロジックを複数箇所に重複させたりします。私自身の経験を振り返ると、DDD を「一度きりの設計作業」として捉えていた頃は失敗が多かったです。あるプロジェクトで Event Storming を実施し、5 つの Bounded Context を特定しました。しかし実装を進めると、そのうち 2 つは同じ Context に統合すべきだと気づきました。別の 1 つは 3 つに分割すべきでした。最初の設計の精度は 6 割程度だったのです。この経験から学んだのは、DDD は「段階的に洗練させる」ものだということです。最初から理想的なモデルを目指すのではなく、実装を通じて境界の妥当性を検証し、継続的に見直します。大規模な変革は「ビッグバン」ではなく「段階的な改善」で進める方が成功率が高い。DDD も例外ではありません。2. Understanding the Domain本章では、ドメインエキスパートへのインタビューを通じてドメインを理解するプロセスが解説されます。コード例が本格的に登場する前に、本書が採用する「関数型プログラミング」というアプローチと、その核心について整理しておきます。Patterns, Principles, and Practices of Domain-Driven Design (English Edition)作者:Millett, Scott,Tune, NickWroxAmazonなぜ「関数型」ドメインモデリングなのか本書のタイトルは「Domain Modeling Made Functional」です。DDD と関数型プログラミングを組み合わせています。なぜでしょうか。関数型プログラミングを学んで獲得する概念は、突き詰めると 1 つのことに集約されます。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。これが関数型の核心です。状態は「境界を越えるとき」だけ必要私たちは普段、プログラムを「状態を変更するもの」として捉えがちです。しかし、ビジネスロジックの大半は「入力を受け取り、何かを計算し、出力を返す」という形式で書けます。注文明細と単価から合計金額を計算する → 状態不要住所文字列をパースして構造化データにする → 状態不要商品コードが有効かどうか検証する → 状態不要状態が「必要」になるのは、システムの境界を越えるときだけです。データベースに保存するとき、外部 API を呼び出すとき、ファイルに書き込むとき。この事実に気づくと、設計の発想が変わります。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられない同時に、状態のトランザクションは現実のシステムでは避けられません。注文は「未検証」から「検証済み」に変わります。ビジネスの世界は状態遷移で満ちています。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。// オブジェクト指向的なアプローチ（問題あり）struct Order {    status: OrderStatus,    customer_info: Option<CustomerInfo>,    validated_at: Option<DateTime>,    amount: Option<Decimal>,}impl Order {    fn validate(&mut self) {        self.status = OrderStatus::Validated;        self.validated_at = Some(now());    }}この設計の問題は、状態の「今」しか見えないこと、そして Option フィールドの組み合わせ爆発です。validated_at が Some で amount が None の状態は正しいのでしょうか？整合性を開発者が頭の中で管理し続けなければなりません。Transformation-Oriented Programmingという答え本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。核心は「元のオブジェクトを変更しない」ことです。状態ごとに異なる型を作ります。UnvalidatedOrder は「未検証の注文」を表す型です。ValidatedOrder は「検証済みの注文」を表す型です。これらは別の型であり、別の構造を持ちます。そして、validate 関数は UnvalidatedOrder を受け取り、新しい ValidatedOrder を返します。元の UnvalidatedOrder には触れません。pub struct UnvalidatedOrder {    pub order_id: String,    pub customer_info: String,    pub shipping_address: String,}pub struct ValidatedOrder {    pub order_id: OrderId,    pub customer_info: CustomerInfo,    pub shipping_address: Address,}fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // 元のUnvalidatedOrderは変更されない}重要なのは、元の UnvalidatedOrder は変更されないということです。validate 関数は新しい ValidatedOrder を「作る」だけです。状態を変えるな。新しい値を作れ。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrderこれは「パイプライン」です。データがパイプを流れていきます。各関数は入力を受け取り、出力を返します。それだけです。なぜこのアプローチが強力なのか状態の追跡が不要: 型を見れば分かります。ValidatedOrder を持っているなら、それは「検証済みの注文」です並行処理での競合がない: 元のデータを変更しないから、複数のスレッドが同時に処理しても問題ありませんテストが簡単: 入力を与えて、出力を確認します。モックも不要ですそして何より、ビジネスプロセスが本質的に「入力を受け取り、何かを行い、出力を返す」ものだから相性が良いのです。「見積書」が「発注書」になります。「申請書」が「承認済み申請書」になります。ビジネスの人々は、無意識のうちにこのモデルで考えています。F#という選択とRustでの実践本書の実装言語は F#です。著者が F#を選んだ理由は、「実用的な関数型言語」として設計されており、.NET エコシステムの資産を活用できるからです。本感想文は F#ではなく Rust で実装を示します。私が Rust を選んだ理由は、現在の私にとって主要言語であること、そして所有権システムによる状態遷移の明示化に興味があったからです。Rust は「関数型言語」ではありませんが、関数型の重要な特徴を備えています。代数的データ型: struct と enum で、F#のレコード型と判別共用体を表現できますイミュータビリティ: デフォルトで変数は不変ですパターンマッチング: 網羅的なパターンマッチを強制しますOption/Result: 欠損値とエラーを型で表現しますRust構文の基礎ここで、本感想文で使う Rust の基本を整理しておきます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgまず「型」とは何でしょうか。型とは「値の種類」のことです。数値、文字列、日付、注文情報—これらは全て異なる「種類」の値であり、それぞれに型があります。型があると、「文字列を数値で割る」といった意味のない操作をコンパイラ（プログラムを機械語に変換するソフトウェア）が事前に検出してくれます。struct（構造体）: 複数の値をまとめて 1 つの「もの」として扱う仕組みです。例えば「注文」は「注文 ID」と「顧客情報」と「配送先」を持ちます。これらをまとめて Order という 1 つの型にできます。pub struct Order {    pub id: OrderId,       // フィールド（構成要素）    pub customer_info: String,}pub は「public（公開）」の略で、外部からアクセスできることを意味します。enum（列挙型）:「A か B か C のどれか」を表す型です。例えば注文の状態は「未処理」か「処理済み」か「発送済み」のいずれかです。enum OrderStatus {    Pending,     // 未処理    Validated,   // 検証済み    Shipped,     // 発送済み}関数: 入力を受け取り、何かの処理をして、出力を返すものです。fn で定義します。fn add(a: i32, b: i32) -> i32 {    a + b}i32 は 32 ビット整数という型です。-> i32 は「i32 型の値を返す」という意味です。impl: 型に「できること」（メソッド）を追加します。impl Order {    fn total(&self) -> Money { /* ... */ }}&self は「自分自身を参照する」という意味です。これで order.total() のように呼び出せます。Option<T>:「値があるかもしれないし、ないかもしれない」を表す型です。Some(値) なら値がある、None なら値がありません。Result<T, E>:「成功か失敗か」を表す型です。Ok(値) なら成功、Err(エラー) なら失敗です。doc.rust-lang.org所有権: Rust の最も特徴的な概念です。値は常に 1 つの変数だけが「持っている」のです。関数に渡すと、その値の所有権が移動し、元の変数では使えなくなります。これが「古い状態を誤って使う」ミスを防いでくれます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgF#と Rust で異なる部分—ガベージコレクション vs 所有権、パイプライン演算子、computation expressions—については、該当箇所で必要になったときに具体的に説明します。所有権の概念は、一見すると制約に見えます。しかし、ドメインモデリングにおいては「状態遷移」を明確にする利点があります。fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // UnvalidatedOrderの所有権がこの関数に移動    // 呼び出し元ではUnvalidatedOrderは使えなくなる    // → 検証前の注文を誤って使うことがない    Ok(ValidatedOrder { /* ... */ })}F#では同じ order 変数を後から参照できてしまいますが、Rust では所有権の移動により「古い状態へのアクセス」がコンパイルエラーになります。これは Transformation-Oriented Programming の考え方をさらに強化しています。ドメインエキスパートへのインタビュー第 2 章は、ドメインエキスパート（Ollie）へのインタビューから始まります。インタビューの冒頭で、著者は典型的な e コマースモデルを想定していました。しかし Ollie の回答は違いました。「顧客は既に商品コードを知っている。一度に 200〜300 アイテムを注文することもある」。Widgets 社のドメインは「一般的」ではありません。B2B で、顧客はエキスパートで、商品コードを直接入力します。この固有の要件は、人間がドメインエキスパートから引き出さなければなりません。データベース駆動設計への衝動本章で参考になったのは、「データベース駆動設計と戦う」というセクションです。注文フォームを見ると、多くの開発者はすぐにテーブル設計を始めたくなります。著者はこれを「間違い」と断言しています。DDD では、ドメインが設計を駆動するのであって、データベーススキーマが駆動するのではありません。永続化の無知（Persistence Ignorance）は重要な原則です。まずドメインの概念とワークフローを整理し、永続化は後から考えます。テキストベースのドメイン文書化本章では、ドメインを文書化するためのシンプルな記法が紹介されています。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこの擬似コードは、Rust の構造体定義にほぼそのまま翻訳できます。「AND」は struct のフィールド、「OR」は enum のバリアントになります。ドメインエキスパートと開発者の両方が読める、共通言語として機能します。オーダーのライフサイクルと状態の型本章の後半で、注文には複数のフェーズがあることが明らかになります。UnvalidatedOrder: 届いたばかりの状態ValidatedOrder: 検証済みの状態PricedOrder: 価格が計算された状態data UnvalidatedOrder =    UnvalidatedCustomerInfo    AND UnvalidatedShippingAddress    AND list of UnvalidatedOrderLinedata ValidatedOrder =    ValidatedCustomerInfo    AND ValidatedShippingAddress    AND list of ValidatedOrderLineこの「状態ごとに別の型を定義する」パターンは、Rust では構造体として実装します。状態遷移は関数のシグネチャとして型付けされ、コンパイラが不正な状態遷移を検出してくれます。ワークフローの分解最終的に、注文処理ワークフローは以下のステップに分解されます。substep "ValidateOrder" =    input: UnvalidatedOrder    output: ValidatedOrder OR ValidationError    dependencies: CheckProductCodeExists, CheckAddressExistssubstep "PriceOrder" =    input: ValidatedOrder    output: PricedOrder    dependencies: GetProductPriceワークフローを小さなステップに分解することで、各ステップが独立してテスト可能になります。入力・出力・依存関係が明確に定義されていれば、実装も容易になります。3. A Functional Architecture本章は、関数型アーキテクチャの原則を解説します。Bounded Context、イベント駆動通信、Onion Architecture。これらの概念は言語に依存しません。アーキテクチャを考えるタイミング第 3 章の冒頭で、著者は矛盾したことを述べています。「この段階でアーキテクチャについて考えすぎるべきではない。まだシステムを理解していないからだ」。しかし同時に「大まかな実装方針を持っておくのは良いことだ」とも言います。著者の「walking skeleton（動く骨格）」というアプローチは有効です。まず最小限の構造を設計し、その骨格に沿ってコードを書いていきます。Bounded Contextと自律性Bounded Context をソフトウェアコンポーネントとしてどう実装するか。モノリス内のモジュール、独立したアセンブリ、マイクロサービス。いくつかの選択肢があります。著者は「最初はモノリスとして構築し、スケールや独立デプロイが求められる段階で分離する」ことを勧めています。マイクロサービスを夢見て最初から分割し、サービス間通信の地獄に落ちた経験がある人には、身に染みる助言でしょう。私もその一人です。最初から理想的なマイクロサービスを目指すと、サービス間の境界を間違えたときの修正コストが膨大になります。まずモノリス内でモジュールを分離し、境界が安定してからサービスに切り出す。これを最初から知っていれば、いくつかの深夜対応は避けられたかもしれません。マイクロサービスアーキテクチャ 第2版作者:Sam Newmanオーム社AmazonイベントによるContext間通信Bounded Context 間の通信は、イベントを介して行われます。Place-Order ワークフローが OrderPlaced イベントを発行し、Shipping コンテキストがそれを受け取って ShipOrder コマンドを生成します。この非同期・疎結合のパターンは、変更の影響範囲を限定できます。各 Context が独立したイベントの発行者・購読者として定義されていれば、一方の変更が他方に波及しにくくなります。DTOと信頼境界本章で重要な概念が登場します。Domain Object と Data Transfer Object (DTO) の区別です。Domain Object は、Bounded Context 内部でのみ使用されます。DTO は、Context 間の通信やシリアライズのために設計されます。同じ「Order」でも、内部で使う Order と、外部に公開する OrderDTO は別物です。さらに、Bounded Context の境界は「信頼境界」として機能します。外部からのデータは信頼できません。内部に入る前にバリデーションが必要です。Context間の契約関係Context 間の契約関係について 3 つのパターンが紹介されます。Shared Kernel: 両チームが共同で契約を所有Customer/Supplier: 下流の Context が契約を定義Conformist: 上流の Context の契約に従うこれらの関係は、技術的な問題であると同時に組織的な問題でもあります。Onion Architecture と I/O の分離本章の後半では、コードの構造について議論されます。Onion Architecture では、ドメインが中心にあり、I/O は外周に配置されます。依存関係は常に内側に向かいます。純粋なコアを、不純な殻で包みます。「I/Oはワークフローの端でのみ行う。ワークフロー内部は純粋な関数で構成する」この原則は、第 2 章で述べた「状態は例外的」という考え方と直結します。ワークフロー内部は「入力を受け取り、何かを行い、出力を返す」純粋な関数だけで構成されます。データベースアクセスやファイル I/O は、ワークフローの開始時か終了時にのみ行います。この構造により、ドメインロジックはテスト容易で予測可能になります。少なくとも、理論上は。4. Understanding Types本章から、コード例が本格的に登場します。第 2 章で整理した F#と Rust の対応関係に基づき、以降は Rust のみで実装を示します。型とは「可能な値の集合」である著者の「型」の定義はシンプルです。「関数の入力や出力として使える値の集合に付けた名前」。i16 は-32768 から+32767 までの数値の集合、String は全ての文字列の集合です。この定義を読んで、自分がいかに型を「コンパイラを満足させるためのもの」として捉えていたか気づかされました。型は思考のツールです。ANDとORによる型の合成—代数的データ型本章の核心は、型の合成方法です。著者は 2 つの方法を示します。これらは「代数的データ型（Algebraic Data Types）」と呼ばれ、関数型プログラミングの基礎概念です。F#では「レコード型」と「判別共用体」、Rust では struct と enum で表現できます。AND型（struct / 積型）: 複数の値を組み合わせます。struct FruitSalad {    apple: AppleVariety,    banana: BananaVariety,    cherries: CherryVariety,}FruitSalad を作るには、apple と banana と cherries の全てが必要です。OR型（enum / 和型）: 複数の選択肢から 1 つを選びます。enum FruitSnack {    Apple(AppleVariety),    Banana(BananaVariety),    Cherries(CherryVariety),}FruitSnack は、Apple か Banana か Cherries のいずれか 1 つです。たった 2 つの概念で複雑なドメインを表現できます。AND と OR という論理演算で型を組み立てます。Simple Types—newtype patternの威力本章で一番「これが使える」と思ったのは、Simple Types の話です。プリミティブ型をそのまま使うのは危険です。CustomerId も OrderId も i32 だとしたら、間違って OrderId を CustomerId として渡してもコンパイルが通ってしまいます。Rust では、newtype patternでこの問題を解決します。newtype pattern とは、既存の型を新しい型でラップすることで、型レベルで区別をつけるイディオムです。F#では「単一ケース判別共用体」、Rust では「タプル構造体」で表現します。zenn.dev#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct CustomerId(i32);#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct OrderId(i32);#[derive(...)] について説明します。これは Rust の「属性マクロ」で、型に機能を自動で追加する仕組みです。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org「トレイト」とは、型が持つべき「能力」や「振る舞い」の定義です。例えば「比較できる」「コピーできる」「文字列として表示できる」といった能力がトレイトとして定義されています。#[derive(Debug, Clone)] と書くと、その型に Debug と Clone という能力が自動的に追加されます。手で書くと何十行にもなるコードを、一行で済ませられます。よく使うトレイトを整理しておきます。 トレイト  意味  使いどころ  Debug  中身を表示できる  println!("{:?}", x) でデバッグ出力  Clone  複製を作れる  .clone() で明示的にコピー  Copy  自動で複製される  代入や関数呼び出しで自動コピー（小さな値向け）  PartialEq  比較できる  == で等しいか判定  Eq  反射律を満たす比較  HashMapのキーに使うとき必要  Hash  ハッシュ値を計算できる  HashMapのキーに使うとき必要 内部的には同じ i32 ですが、型システム上は別の型です。CustomerId を期待する関数に OrderId を渡すとコンパイルエラーになります。ここで重要なのは、Clippy のような静的解析ツールでもこの種のバグは検出できないということです。Clippy は Rust の公式リンター（コード品質チェックツール）で、700 以上の lint ルールを持ちます。cargo clippy コマンドで実行でき、コードの問題点を警告してくれます。rust-lang.github.ioしかし、Clippy にも限界があります。// Clippyでは検出できないfn process(customer_id: i32, order_id: i32) { /* ... */ }process(order_id, customer_id); // バグ！でもコンパイルは通る// 型で防ぐfn process(customer_id: CustomerId, order_id: OrderId) { /* ... */ }// process(order_id, customer_id); // コンパイルエラー！Clippy は構文的な問題—if x { "a" } else { "a" } のような両方のブランチが同じ処理、u32 >= 0 のような常に true になる比較—は検出できます。しかし、「この i32 は顧客 ID を表し、あの i32 は注文 ID を表す」というドメインの知識は持っていません。newtype pattern は、Clippy が検出できないバグを型システムで防ぎます。AI コーディングエージェントも同様です。「customer_id と order_id を間違えないように」という指示は、自然言語では曖昧です。しかし CustomerId と OrderId という別の型が定義されていれば、AI が生成したコードでも型の取り違えはコンパイル時に検出されます。Option型とResult型F#と Rust は、欠損値を Option で、エラーを Result で表現します。これらは関数型プログラミングにおける標準的なエラーハンドリング手法で、null や例外を使わずに「値がないかもしれない」「失敗するかもしれない」ことを型で表現します。struct PersonalName {    first_name: String,    middle_initial: Option<String>,  // 省略可能    last_name: String,}Option<T> は Some(T) か None のいずれかです。null を使わずに「値がないかもしれない」ことを型で表現します。エラーハンドリングには Result<T, E> を使います。? 演算子で、エラー時に早期リターンできます。型によるドメイン表現本章で紹介されている支払い方法のモデリング例は、型がドキュメントとして機能することを示しています。enum PaymentMethod {    Cash,    Check(CheckNumber),    Card(CreditCardInfo),}struct Payment {    amount: PaymentAmount,    currency: Currency,    method: PaymentMethod,}約 25 行で、支払いドメインの構造が明確に表現されています。このコードは、ドメインエキスパートにも読めます。型システムは思考のツールである本章を通じて感じたのは、型システムは「コンパイラのため」ではなく「思考のため」にあるということです。「動的型付け言語でも同じことができるのでは？」という疑問があるかもしれません。確かに、Python や Ruby でもドメインモデリングはできます。しかし、型がないと「どんな値が入りうるか」を頭の中で追跡し続けなければなりません。静的型付けでは、その追跡をコンパイラに委ねられます。AND 型と OR 型という単純な組み合わせで、複雑なドメインを表現できます。型定義という明確な仕様があれば、実装時の迷いが減ります。型システムは、ドメインの構造を可視化するツールです。5. Domain Modeling with Types本章では、前章で学んだ型システムの概念を使って、実際にドメインモデルを構築します。コードがドキュメントになる第 5 章の冒頭で、著者は挑戦的な問いを投げかけます。「ソースコードを直接ドキュメントとして使い、UML 図のような別の成果物を不要にできるか？」正直、最初は懐疑的でした。しかし本章を読み進めるうちに、著者の意図が分かってきました。擬似コードからRustへ第 2 章で作成した擬似コードを、Rust の型に変換します。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこれが Rust では以下のようになります。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,    pub shipping_address: ShippingAddress,    pub billing_address: BillingAddress,    pub order_lines: Vec<OrderLine>,    pub amount_to_bill: BillingAmount,}ほぼ一対一の変換です。擬似コードと Rust コードを並べて見ると、ドメインの構造がそのまま型に反映されていることが分かります。Value ObjectとEntityDDD では、オブジェクトを「Value Object」と「Entity」に分類します。Value Object: 同じ値を持てば同一とみなします。#[derive(Debug, Clone, PartialEq, Eq)]pub struct PersonalName {    pub first_name: String,    pub middle_initial: Option<String>,    pub last_name: String,}Entity: 固有の ID を持ち、内容が変わっても同一性を保ちます。impl PartialEq for Contact {    fn eq(&self, other: &Self) -> bool {        self.contact_id == other.contact_id  // IDのみで比較    }}Aggregate—一貫性の境界本章で最も重要な概念は「Aggregate」です。Order と OrderLine の関係を考えます。OrderLine の価格を変更したとき、Order の合計金額も更新します。両者は常に一貫した状態を保ちます。DDD では、こうした関連オブジェクトの集合を「Aggregate」と呼び、最上位のオブジェクトを「Aggregate Root」と呼びます。immutable なパターンでは、OrderLine を変更するには Order 全体を作り直します。これは一見非効率に見えますが、一貫性を強制する効果があります。OrderLine だけを変更して、Order の合計金額を更新し忘れる、というバグが起こりにくくなります。Aggregate参照—IDのみを保持するOrder に Customer 情報を含める場合、Customer オブジェクト全体ではなく、CustomerId だけを保持すべきです。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,  // Customer全体ではなく、IDのみ    pub order_lines: Vec<OrderLine>,}この設計は、immutability と相性が良いです。Customer の電話番号が変わっても、Order を更新する必要がありません。型でドメインを表現する—最終形本章の最後に、完全なドメインモデルの例が示されます。#[derive(Debug, Clone, PartialEq, Eq)]pub enum ProductCode {    Widget(WidgetCode),    Gizmo(GizmoCode),}#[derive(Debug, Clone, Copy, PartialEq)]pub enum OrderQuantity {    Unit(UnitQuantity),    Kilos(KilogramQuantity),}このコードは、第 2 章の擬似コードとほぼ同じ構造を持ちます。struct、enum、match の意味さえ分かれば読めます。match については Rust 公式ドキュメントを参照してください。doc.rust-lang.orgString は沈黙します。EmailAddress は語ります。著者の主張—「型でドメインを表現すれば、コードがドキュメントになる」—は正しいと思います。6. Integrity and Consistency in the Domain本章は、ドメイン内のデータが常に「信頼できる状態」であることを保証する方法を解説します。Smart Constructor—制約を強制する本章で最も実用的だったのは、Smart Constructor（スマートコンストラクタ）のパターンです。Smart Constructor とは、値の生成時にバリデーションを行い、不正な値の生成を防ぐコンストラクタのことです。通常のコンストラクタと異なり、Result を返して生成の失敗を表現できます。例えば、UnitQuantity は 1 から 1000 の間の値でなければなりません。この制約をコメントで書くだけでは不十分です。#[derive(Debug, Clone, Copy, PartialEq, Eq)]pub struct UnitQuantity(i32);impl UnitQuantity {    pub fn new(value: i32) -> Result<Self, String> {        if value < 1 {            Err("UnitQuantity must be at least 1".to_string())        } else if value > 1000 {            Err("UnitQuantity must be at most 1000".to_string())        } else {            Ok(UnitQuantity(value))        }    }    pub fn value(&self) -> i32 {        self.0    }}フィールドを pub にしなければ、外部から直接 UnitQuantity(500) と書けません。必ず UnitQuantity::new(500) を経由します。NonEmptyList—空のリストを許さない「注文には少なくとも 1 つの注文行がなければならない」という要件を、型で強制できるでしょうか。#[derive(Debug, Clone, PartialEq)]pub struct NonEmptyList<T> {    pub first: T,    pub rest: Vec<T>,}from_vec は Option を返します。空のベクターからは NonEmptyList を作れません。この「作れない」という事実が型で表現されています。Make Illegal States Unrepresentable本章で最も重要な原則は「不正な状態を表現不可能にする」です。メールアドレスの例が分かりやすいです。「検証済み」と「未検証」のメールアドレスがあるとき、フラグで区別する設計は危険です。詳しくは Rust 公式ドキュメントの enum 解説を参照してください。doc.rust-lang.org// 良い例：別の型として定義pub struct VerifiedEmailAddress(String);pub enum CustomerEmail {    Unverified(EmailAddress),    Verified(VerifiedEmailAddress),}VerifiedEmailAddress のコンストラクタを private にして、検証サービスからしか作れないようにします。これで、検証を経ずに Verified 状態を作ることが物理的に不可能になります。fn send_password_reset(email: VerifiedEmailAddress) -> Result<(), SendError> {    // この関数にEmailAddressを渡すとコンパイルエラー}連絡先情報の例—OR型の活用「顧客にはメールアドレスか住所のどちらか、または両方が必要」という要件を型で表現します。pub enum ContactInfo {    EmailOnly(EmailContactInfo),    AddressOnly(PostalContactInfo),    EmailAndAddress(BothContactMethods),}3 つのケースしかありません。「メールも住所もない」という状態は表現できません。「不可能を作る」という設計思想本章の核心は「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」です。この原則を言い換えれば、型で「不可能」を作るということになります。この原則を読んだとき、過去に遭遇したバグが走馬灯のように思い出されました。is_active = true なのに deleted_at が設定されている。status = "paid" なのに payment_id が null。フラグと Option の組み合わせ爆発で、「あり得ない」状態が本番データベースに存在していた。深夜に呼び出されて、整合性を手作業で修正した夜のことを、今でも覚えています。あのバグは、型で防げたのです。似たような経験をしたことがある人は、少なくないのではないでしょうか。NonEmptyList を使えば、空の注文は作れないVerifiedEmailAddress を使えば、未検証メールへのパスワードリセットは書けないSmart Constructor を使えば、範囲外の値は存在できない「できない」「書けない」「存在できない」—これらは制限ではなく、設計上の保証です。バリデーションは「お願い」。型は「物理法則」。Clippy のような静的解析ツールでも、ドメインロジックの問題は検出できません。例えば、is_priced: bool と amount: Option<f64> を持つ構造体を考えます。is_priced = true なのに amount = None という矛盾した状態は、Clippy には「正しい Rust コード」に見えます。ビジネスルールを知らないからです。しかし、PricedOrder { amount: Money } と UnpricedOrder を別の型として定義すれば、この矛盾は表現できなくなります。Clippy が検出できない問題を、型システムが防ぎます。AI エージェント時代において、この「不可能を作る」設計思想の価値は高まっています。AI は自然言語のドキュメントを読み飛ばすことがあります。しかし、型で「不可能」が定義されていれば、AI はその制約を破るコードを物理的に書けません。7. Modeling Workflows as Pipelines本章は、ワークフローをパイプラインとしてモデリングする方法を解説します。ビジネスプロセスを「変換の連鎖」として捉えるアプローチです。ワークフローはパイプラインである本章の冒頭で、著者は注文処理ワークフローを次のように要約しています。workflow "Place Order" =    input: UnvalidatedOrder    output: OrderPlaced AND BillableOrderPlaced AND OrderAcknowledgmentSent    // step 1: ValidateOrder    // step 2: PriceOrder    // step 3: AcknowledgeOrder    // step 4: create and return events各ステップは「入力を受け取り、変換し、出力を返す」関数です。これらを連結するとパイプラインになります。第 2 章で述べた「状態は例外的、ほとんどの処理は状態なしで書ける」という原則が、ここで具現化されます。ワークフロー全体を見ると「状態遷移」に見えますが、各ステップを見ると「入力を受け取り、何かを行い、出力を返す」純粋な関数でしかありません。状態マシンとしてのOrderOrder を単一の型として設計すると、フラグだらけになります。// 悪い設計struct Order {    order_id: OrderId,    is_validated: bool,    is_priced: bool,    amount_to_bill: Option<Decimal>,  // pricedの時だけ存在}本書のアプローチは、各状態を別の型として定義することです。pub struct UnvalidatedOrder { /* ... */ }pub struct ValidatedOrder { /* ... */ }pub struct PricedOrder {    // ...    pub amount_to_bill: BillingAmount,  // この状態でのみ存在}PricedOrder には amount_to_bill があります。ValidatedOrder にはありません。フラグは不要で、「どの型か」が状態を表します。AI にコードを書かせるとき、この設計は強力なガードレールになります。「検証をスキップして価格計算に進んでください」と指示しても、price() 関数が ValidatedOrder を要求する以上、AI は UnvalidatedOrder を渡すコードを書けません。型が不正な状態遷移を物理的に阻止します。依存性を型で表現する各ステップの依存性を型シグネチャで表現します。type CheckProductCodeExists = fn(&ProductCode) -> bool;type CheckAddressExists = fn(&UnvalidatedAddress) -> Result<CheckedAddress, AddressValidationError>;type ValidateOrder = fn(    CheckProductCodeExists,     // 依存性1    CheckAddressExists,         // 依存性2    UnvalidatedOrder,           // 入力) -> Result<ValidatedOrder, ValidationError>;依存性が関数の引数として明示されます。インターフェース全体ではなく、必要な関数だけを渡します。最小限の依存性です。エフェクトの文書化関数の「エフェクト（効果）」を型で文書化します。Result: エラーを返す可能性があるAsync: 非同期 I/O を行うOption: 値が存在しない可能性があるasync fn check_address_exists(    address: &UnvalidatedAddress,) -> Result<CheckedAddress, AddressValidationError> {    // 外部サービスへのHTTPリクエスト}関数シグネチャを見れば、「この関数は非同期で、エラーを返す可能性がある」と分かります。Transformation-Oriented Programmingの実践具体的な例で考えます。オブジェクト指向的に書くと、こうなります。impl Order {    fn validate(&mut self, checker: &ProductChecker) -> Result<(), ValidationError> {        // 状態を変更        self.is_validated = true;        self.validated_at = Some(now());        Ok(())    }}関数型で書き直すと、こうなります。fn validate(    order: UnvalidatedOrder,    checker: &ProductChecker,) -> Result<ValidatedOrder, ValidationError> {    // 新しい値を作る（元のorderは変更しない）    Ok(ValidatedOrder {        order_id: OrderId::new(order.order_id)?,        customer_info: validate_customer(order.customer_info)?,        lines: validated_lines,    })}違いは何でしょうか。コンパイル時チェック: price() に UnvalidatedOrder を渡すとコンパイルエラー状態の整合性が型で保証: ValidatedOrder には is_validated フラグがそもそも存在しないテストが独立: validate() と price() を別々にテストできるこの単純さが強力なのだUnvalidatedOrder を ValidatedOrder に変換します。ValidatedOrder を PricedOrder に変換します。元のオブジェクトは触りません。新しいオブジェクトを作ります。それだけです。状態の変更を追跡する必要がない（変更しないから）並行処理でも競合しない（元のデータを変更しないから）テストが簡単（入力と出力を比較するだけ）デバッグが楽（各ステップの入出力をログに残せば、全経路が追える）関数型プログラミングの入門書を読むと、モナドだの圏論だの、難しい概念が出てきます。しかし、実務で最も重要なのは、本書が示すTransformation-Oriented Programmingです。核心は 3 つです。状態を型で表現する（UnvalidatedOrder と ValidatedOrder は別の型）状態遷移を関数で表現する（validate(order) -> ValidatedOrder）元のオブジェクトを変更しない（新しい値を作るだけ）変えるな。作れ。この章を読み終えたとき、「これなら実務で使える」と確信しました。モナドや圏論を理解する必要はありません。「状態ごとに型を分ける」「元のオブジェクトを変更しない」。この 2 つだけで、設計の質は劇的に変わります。難しい理論ではなく、明日から使える実践知。本書の価値はここにあります。8. Understanding Functions本章は、関数型プログラミングの基礎を解説します。実装に入る前の準備として、関数の扱い方を整理しています。関数型プログラミングとは著者の定義はシンプルです。「関数型プログラミングとは、関数が本当に重要なものとしてプログラミングすること」。オブジェクト指向では、オブジェクトがあらゆる場所で使われます。関数型では、関数があらゆる場所で使われます。依存性を注入するときは関数を渡します。コードを再利用するときは関数を合成します。関数は「モノ」である関数型プログラミングの核心は、関数が第一級の値であることです。変数に代入できます。リストに入れられます。引数として渡せます。戻り値として返せます。let add1 = |x: i32| x + 1;let square = |x: i32| x * x;let functions: Vec<fn(i32) -> i32> = vec![add1, square];for f in &functions {    println!("{}", f(5));}関数をリストに入れて、ループで回しています。高階関数関数を引数に取る関数、または関数を返す関数を「高階関数」と呼びます。fn eval_with_5_then_add_2<F>(f: F) -> i32where    F: Fn(i32) -> i32,{    f(5) + 2}Rust では、関数を受け取る引数の型を Fn、FnMut、FnOnce トレイトで指定します。F#ではこの区別はありません。クロージャとはクロージャは「名前のない関数」です。通常の関数は fn name(...) と名前をつけて定義しますが、クロージャは |引数| 式 という形式でその場で作れます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orglet add1 = |x| x + 1;        // 引数xを受け取り、x + 1を返すlet result = add1(5);        // 6クロージャの特徴は、周囲の変数を「捕まえる」（キャプチャする）ことができる点です。let multiplier = 3;let multiply = |x| x * multiplier;  // multiplierを捕まえているlet result = multiply(5);           // 15Fnトレイトの使い分け関数を引数として受け取るとき、Rust では 3 種類のトレイトを使い分けます。これは「捕まえた変数をどう扱うか」で決まります。 トレイト  捕まえ方  呼び出し回数  Fn  読み取るだけ  何度でも  FnMut  変更する  何度でも  FnOnce  消費する（使い切る）  一度だけ 最初は気にしすぎなくてよいです。コンパイラがエラーで教えてくれます。カリー化と部分適用F#では、すべての関数が自動的に「カリー化」されます。Rust にはカリー化が組み込まれていません。同じことを実現するには、明示的にクロージャを返します。fn adder_generator(number_to_add: i32) -> impl Fn(i32) -> i32 {    move |x| number_to_add + x}let add5 = adder_generator(5);let result = add5(3);  // 8部分適用は、依存性注入に活用できます。let validate = |order| validate_order(    check_product_code_exists,    check_address_exists,    order,);let result = validate(unvalidated_order);Total Functions（全域関数）数学の関数は、すべての入力に対して出力が定義されます。12 を引数で割る関数を考えます。n = 0 のとき、何を返すべきでしょうか。解決策は 2 つあります。入力を制限するか、出力を拡張するかです。// 入力を制限fn twelve_divided_by(n: NonZeroI32) -> i32 {    12 / n.0}// 出力を拡張fn twelve_divided_by(n: i32) -> Option<i32> {    if n == 0 { None } else { Some(12 / n) }}どちらの場合も、型シグネチャが正直になります。型シグネチャは嘘をつきません。コメントは嘘をつきます。AI にコードを書かせるとき、この「正直な型シグネチャ」は重要です。AI は型シグネチャを見て、関数の契約を理解します。Option<i32> を返す関数なら、AI は None のケースを考慮したコードを生成します。しかし「0 を渡したら None を返します」というコメントは、読み飛ばされる可能性があります。関数合成F#にはパイプライン演算子 |> があります。Rust にはありません。代わりにメソッドチェーンや、関数を直接呼び出します。let result: Vec<_> = (1..10)    .map(|x| x + 1)    .map(|x| x * x)    .collect();イテレータのアダプタは、パイプラインに近い書き方ができます。Rustで関数型プログラミングを実践するために本章を読んで、F#と Rust の違いを改めて認識しました。カリー化: F#は自動、Rust は手動パイプライン演算子: F#にはある、Rust にはないクロージャの所有権: F#は考慮不要、Rust は move や Fn/FnMut/FnOnce を意識これらの違いはありますが、関数型プログラミングの本質—関数を組み合わせてシステムを構築する—は Rust でも実践できます(が本当に最適か？という問いは投げないでくれ…本稿のアプローチと違いすぎる)。9. Implementation: Composing a Pipeline本章から、いよいよ実装に入ります。これまで型で設計してきたワークフローを、実際のコードに落とし込みます。パイプラインの理想形著者が示す理想のコードは驚くほどシンプルです。let placeOrder unvalidatedOrder =    unvalidatedOrder    |> validateOrder    |> priceOrder    |> acknowledgeOrder    |> createEvents4 行で注文処理全体が表現されています。これが関数型アプローチの目指す姿です。しかし現実には、関数の出力と次の関数の入力が一致しません。依存性をどこかで解決しなければなりません。本章はその「ギャップ」を埋める方法を解説します。型シグネチャによる実装のガイド本章で印象的だったのは、「型シグネチャを先に定義し、それに従って実装する」というアプローチです。type ValidateOrder = fn(    check_product_code: fn(&ProductCode) -> bool,    check_address: fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder;型シグネチャが「契約」として機能します。引数の型、戻り値の型が全て決まっているので、実装者は「この契約を満たすコードを書く」だけでよいです。依存性注入の関数型アプローチオブジェクト指向では、インターフェースを定義し、コンストラクタで注入します。関数型では、依存性を関数の引数として渡します。DI コンテナ？関数を渡せ。fn validate_order(    check_product_code: impl Fn(&ProductCode) -> bool,    check_address: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder {    // check_product_code を使う}インターフェース全体ではなく、必要な関数だけを渡します。ワークフロー全体の組み立て各ステップを組み立ててワークフロー全体を作ります。pub fn place_order(    // 依存性    check_product_code_exists: impl Fn(&ProductCode) -> bool,    check_address_exists: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    get_product_price: impl Fn(&ProductCode) -> Price,    // 入力    unvalidated_order: UnvalidatedOrder,) -> Vec<PlaceOrderEvent> {    let validated = validate_order(        &check_product_code_exists,        &check_address_exists,        unvalidated_order,    );    let priced = price_order(&get_product_price, validated);    let acknowledgment = acknowledge_order(&priced);    create_events(&priced, acknowledgment)}F#のパイプライン演算子 |> がないので、変数に束縛しながら連鎖させます。本章では Result を使わず簡略化しており、次章で Result を導入します。10. Implementation: Working with Errors本章は、エラーハンドリングの関数型アプローチを解説します。「Railway Oriented Programming」と呼ばれるパターンを学びます。エラーの三分類著者はエラーを 3 つに分類します。Domain Errors: ビジネスプロセスの一部として予期されるエラー。商品コードが無効、注文が請求で拒否される、など。Panics: システムを未知の状態にするエラー。メモリ不足、ゼロ除算など。Infrastructure Errors: ネットワークタイムアウト、認証失敗など。この分類は実務でも有用です。「このエラーはドメインエキスパートに相談すべきか」という問いに答えられます。ドメインエラーを型で表現する#[derive(Debug, Clone)]pub enum PlaceOrderError {    ValidationError(String),    ProductOutOfStock(ProductCode),    RemoteServiceError(RemoteServiceError),}エラーを enum で定義することで、どんなエラーが起こりうるか、型定義を見れば分かります。Railway Oriented Programming著者が提唱する解決策が「Railway Oriented Programming（鉄道指向プログラミング）」です。著者自身による詳細な解説は以下を参照してください。fsharpforfunandprofit.comResult を返す関数は「分岐するレール」として可視化できます。成功すれば上のレールに、失敗すれば下のレールに進みます。一度失敗パスに入ると、残りのステップはバイパスされます。      [validateOrder] → [priceOrder] → [acknowledgeOrder] → 成功           ↓               ↓               ↓      ─────────────────────────────────────────────────────→ 失敗Rustでの実装Rust では ? 演算子が同じことをより簡潔に表現します。fn validate_order(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    let order_id = OrderId::create(&order.order_id)?;    let customer_info = to_customer_info(&order.customer_info)?;    let shipping_address = check_address(&order.shipping_address)?;    Ok(ValidatedOrder { order_id, customer_info, shipping_address, ... })}? 演算子は、Ok ならアンラップし、Err なら早期リターンします。?演算子の仕組み? は「失敗したら即座に関数から抜ける」という処理を一文字で書ける記号です。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgResult は「成功（Ok）か失敗（Err）か」を表す型でした。? をつけると、成功なら中身を取り出し、失敗ならその場で関数を終了して呼び出し元にエラーを返します。// ?を使った書き方let order_id = OrderId::create(&order.order_id)?;// これは以下と同じ意味let order_id = match OrderId::create(&order.order_id) {    Ok(v) => v,              // 成功したら中身を取り出す    Err(e) => return Err(e), // 失敗したら即座にエラーを返す};? を使うには、関数の戻り値が Result である必要があります。エラー型の変換複数のステップを連結するとき、エラー型を統一します。#[derive(Debug)]pub enum PlaceOrderError {    Validation(ValidationError),    Pricing(PricingError),}impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}Fromトレイトによる型変換From は「ある型から別の型への変換方法」を定義するトレイトです。例えば「ValidationError を PlaceOrderError に変換する方法」を定義しておくと、? 演算子が自動的にエラー型を変換してくれます。// 「ValidationErrorからPlaceOrderErrorへの変換方法」を定義impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}これを定義しておくと、validate_order が ValidationError を返しても、? が自動的に PlaceOrderError に変換してくれます。異なるエラー型を返す関数を連結できるようになります。11. Serialization本章は、ドメインオブジェクトを JSON や XML などの形式に変換する方法を解説します。Bounded Context の境界を越えるとき、内部のドメイン型をそのまま使うことはできません。また、AI時代にはこのようなことも想定されます。zenn.dev永続化とシリアライゼーションの区別著者は 2 つの概念を区別します。Persistence（永続化）: プロセスの終了後も状態が残ること。Serialization（シリアライゼーション）: ドメイン固有の表現を、永続化可能な表現に変換すること。本章はシリアライゼーションに焦点を当て、次章で永続化を扱います。DTOによる変換—ドメインの境界防御ドメイン型は複雑です。ネストした型、制約付きの型、選択肢を持つ型。これらを直接シリアライズするのは難しいです。解決策は、Data Transfer Object（DTO）を中間層として使うことです。なぜDTOを使うのか？ドメイン型は制約を持つ: String50 は 50 文字以下という制約があります。JSON の "name" フィールドは任意の長さです。直接マッピングできません。内部実装の変更から外部を守る: ドメイン型のフィールド名を変えても、DTO が同じなら外部 API は影響を受けません。検証の境界を明確にする: 外部からの入力は「信頼できない」です。DTO からドメイン型への変換時に検証することで、ドメイン内は常に「信頼できる」状態を保ちます。DTO は「ドメインの境界防御」として機能します。Domain型 → DTO → JSON（シリアライズ）JSON → DTO → Domain型（デシリアライズ）Rust では、ドメイン型と DTO 型を別々に定義します。ドメイン型は制約を持ち、DTO はプリミティブ型のみを使います。/// 制約付きの文字列型（50文字以下）#[derive(Debug, Clone, PartialEq, Eq)]pub struct String50(String);impl String50 {    pub fn create(s: &str) -> Result<Self, ValidationError> {        if s.is_empty() {            Err(ValidationError::Empty("String50 cannot be empty".into()))        } else if s.len() > 50 {            Err(ValidationError::TooLong("String50 must be 50 chars or less".into()))        } else {            Ok(String50(s.to_string()))        }    }    pub fn value(&self) -> &str {        &self.0    }}/// ドメイン型（制約付き）#[derive(Debug, Clone, PartialEq, Eq)]pub struct Person {    pub first_name: String50,    pub last_name: String50,}/// DTO（シリアライズ用・プリミティブのみ）#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]pub struct PersonDto {    pub first_name: String,    pub last_name: String,}変換には From と TryFrom を使います。ドメイン型から DTO への変換は常に成功しますが、DTO からドメイン型への変換は失敗しうります。/// ドメイン型 → DTO（常に成功）impl From<&Person> for PersonDto {    fn from(person: &Person) -> Self {        PersonDto {            first_name: person.first_name.value().to_string(),            last_name: person.last_name.value().to_string(),        }    }}/// DTO → ドメイン型（失敗する可能性あり）impl TryFrom<PersonDto> for Person {    type Error = ValidationError;    fn try_from(dto: PersonDto) -> Result<Self, Self::Error> {        let first_name = String50::create(&dto.first_name)?;        let last_name = String50::create(&dto.last_name)?;        Ok(Person { first_name, last_name })    }}TryFrom を使うことで、変換が失敗する可能性を型で表現しています。これは「Parse, don't validate」の実践です。入力を単に「正しいかどうか」検証するのではなく、より型安全な形式に変換（パース）することで、型レベルで正しさを保証します。DTOは契約である著者が強調するのは、DTO は「Bounded Context 間の契約」だということです。他の Context が発行したイベントを受信するとき、そのフォーマットに依存します。フォーマットを変更すると、依存する Context に影響が及びます。だから、シリアライズのフォーマットは慎重に設計すべきです。serde の #[derive(Serialize)] を安易に使うと、内部実装の変更が契約の破壊につながります。選択肢型（enum）のシリアライズOR 型（enum）のシリアライズは注意が必要です。JSON には enum の概念がありません。Rust では serde の属性でタグ付け方式を指定します。serde については公式ドキュメントを参照してください。serde.rs/// 支払い方法のDTO - タグ付きenum#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]#[serde(tag = "type")]pub enum PaymentMethodDto {    Cash,    Check { check_number: String },    Card { card_number: String, expiry: String },}#[serde(tag = "type")] を指定すると、以下のような JSON が生成されます。{"type":"Cash"}{"type":"Check","check_number":"12345"}{"type":"Card","card_number":"4111...","expiry":"12/25"}タグ付きの方が明示的で、新しいケースを追加しやすいです。ラウンドトリップの検証シリアライズとデシリアライズは対になります。ラウンドトリップ（往復）テストで、データが失われないことを確認します。pub fn serialize_order(order: &Order) -> Result<String, serde_json::Error> {    let dto = OrderDto::from(order);    serde_json::to_string_pretty(&dto)}pub fn deserialize_order(json: &str) -> Result<Order, String> {    let dto: OrderDto = serde_json::from_str(json).map_err(|e| e.to_string())?;    Order::try_from(dto).map_err(|e| format!("{:?}", e))}本章と中心テーマのつながりDTO パターンは、本書のTransformation-Oriented Programmingと関連します。外部からの入力（JSON）は「未検証の値」です。DTO からドメイン型への変換は、UnvalidatedOrder から ValidatedOrder への変換と同じパターンです。信頼できない入力を、信頼できるドメイン型に「変換」します。外部を信頼するな。まず変換せよ。DTO は、外部世界とドメインの境界を守る防壁です。12. Persistence本章は、ドメインモデルをデータベースに永続化する方法を解説します。DDD の原則に従いながら、現実のインフラと向き合います。永続化の原則本章の冒頭で、著者は 3 つの原則を示します。永続化を端に押し出す（Push persistence to the edges）: ワークフローの内部では I/O を行わないコマンドとクエリを分離する（CQRS）: 更新操作と読み取り操作を分けるBounded Contextは自分のデータストアを所有する: 他の Context のデータベースに直接アクセスしないこれらの原則は、「状態は例外的」という関数型の考え方と合致します。永続化を端に押し出す著者は、「ドメインロジックと I/O が混在したコード」と「分離したコード」を対比しています。なぜI/Oを境界に押し出すのか？テストが容易になる: 純粋なドメインロジックは、データベース接続なしでテストできます。入力を与えて出力を確認するだけです。推論が容易になる: 副作用がない関数は、同じ入力に対して常に同じ出力を返します。状態を追跡する必要がありません。並行処理が安全になる: 共有状態を変更しないため、競合が発生しません。変更に強くなる: データベースを変更しても、ドメインロジックは影響を受けません。逆も同様です。まず、ドメイン型を定義します。これらの型はデータベースのことを知りません。/// 未払いの請求書#[derive(Debug, Clone, PartialEq)]pub struct UnpaidInvoice {    pub invoice_id: InvoiceId,    pub amount_due: Money,}/// 支払い済みの請求書#[derive(Debug, Clone, PartialEq)]pub struct PaidInvoice {    pub invoice_id: InvoiceId,    pub amount_paid: Money,}/// 支払い処理の結果#[derive(Debug, Clone, PartialEq)]pub enum InvoicePaymentResult {    FullyPaid(PaidInvoice),    PartiallyPaid(UnpaidInvoice),}次に、純粋なドメインロジックを定義します。この関数は I/O を行いません。/// 支払いを適用する - 純粋関数、I/Oなしpub fn apply_payment(invoice: UnpaidInvoice, payment: Payment) -> InvoicePaymentResult {    let remaining = invoice.amount_due.0 - payment.amount.0;    if remaining <= 0.0 {        InvoicePaymentResult::FullyPaid(PaidInvoice {            invoice_id: invoice.invoice_id,            amount_paid: Money(invoice.amount_due.0),        })    } else {        InvoicePaymentResult::PartiallyPaid(UnpaidInvoice {            invoice_id: invoice.invoice_id,            amount_due: Money(remaining),        })    }}最後に、コマンドハンドラで I/O を境界に押し出します。パターンは「Load → Pure Logic → Save」です。/// コマンドハンドラ - I/Oは境界で行うpub fn pay_invoice_handler<R: InvoiceRepository>(    repo: &R,    command: PayInvoiceCommand,) -> Result<InvoicePaymentResult, PayInvoiceError> {    // 1. Load（I/O - 開始時）    let invoice = repo        .load(&command.invoice_id)        .ok_or(PayInvoiceError::InvoiceNotFound(command.invoice_id))?;    // 2. 純粋なドメインロジック（I/Oなし）    let result = apply_payment(invoice, command.payment);    // 3. Save（I/O - 終了時）    match &result {        InvoicePaymentResult::FullyPaid(paid) => repo.save_paid(paid),        InvoicePaymentResult::PartiallyPaid(unpaid) => repo.save_unpaid(unpaid),    }    Ok(result)}RepositoryパターンRust では Trait を使って Repository を抽象化します。これにより、テスト時にモックを注入できます。なぜTraitで抽象化するのか？テスト容易性: 本番では PostgreSQL、テストではインメモリ実装を注入できます。実装の交換可能性: データベースを変更しても、ドメインロジックは影響を受けません。依存性の逆転: ドメインが永続化の詳細に依存しません。依存の方向が逆になります。トレイトとはトレイトは「この型は〇〇ができる」という能力の定義です。例えば「データを読み込める」「データを保存できる」という能力を定義します。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org/// Repositoryトレイト - 永続化操作を抽象化pub trait InvoiceRepository {    fn load(&self, id: &InvoiceId) -> Option<UnpaidInvoice>;    fn save_unpaid(&self, invoice: &UnpaidInvoice);    fn save_paid(&self, invoice: &PaidInvoice);}trait で能力を定義し、impl Trait for Type で「この型はこの能力を持つ」と宣言します。ジェネリクスとはジェネリクスは「型を後で決める」仕組みです。<R: InvoiceRepository> は「InvoiceRepository という能力を持つ何かの型 R」という意味です。// Rは「InvoiceRepositoryという能力を持つ何か」fn pay_invoice_handler<R: InvoiceRepository>(repo: &R, ...) { ... }これで同じ関数を、異なる実装で使い回せます。// 本番ではPostgreSQLを使うpay_invoice_handler(&postgres_repo, command);// テストではメモリ上の仮実装を使うpay_invoice_handler(&in_memory_repo, command);テスト用のインメモリ実装を作れば、実際のデータベースなしでドメインロジックをテストできます。Persistence Ignorance（永続化の無知）第 2 章で「データベース駆動設計と戦う」と述べたことの実践がここにあります。ドメインモデルは、自分がどこに保存されるか知りません。知る必要もありません。Order 型はデータベースのことを知りません。永続化の詳細は、ワークフローの「端」で処理されます。この設計により、ドメインロジックの変更が永続化コードに影響しません。逆に、データベースを変更してもドメインロジックは変わりません。NoSQLとRDBの選択本章では、NoSQL（ドキュメント DB）と RDB（リレーショナル DB）の両方のアプローチを解説しています。NoSQL: Aggregate をそのままドキュメントとして保存できます。DDD との相性が良いです。RDB: OR 型（enum）のマッピングが難しいです。インピーダンスミスマッチ（オブジェクトモデルとリレーショナルモデルの構造的な不一致）が発生します。-- OR型のRDBへのマッピング（判別カラム）CREATE TABLE order_lines (    quantity_type VARCHAR(10),  -- 'Unit' or 'Kilos'    unit_quantity INT NULL,    kilogram_quantity DECIMAL NULL);どちらも完璧ではありません。「永続化は境界で行う」という原則を守ることで、純粋なドメインロジックと不純な I/O 処理を分離しやすくなります。13. Evolving a Design and Keeping It Clean本章は、本書の締めくくりとして、設計の進化と保守性について解説します。要件は変わります。ドメインモデルも変わります。その変化にどう対応するでしょうか。変化への対応著者は、DDD は「一度きりの静的なプロセス」ではないと強調します。要件が変われば、まずドメインモデルを見直します。実装をパッチするのではなく、モデルから考え直します。変更例: 配送料の追加配送料計算をワークフローに組み込むには、新しいステップを追加します。まず、新しい型を定義します。なぜ既存の型を変更せず、新しい型を作るのか？型の名前がドキュメントになる: PricedOrderWithShipping という名前だけで、「価格計算済みで配送情報も持つ注文」だと分かります。段階を明示できる: PricedOrder と PricedOrderWithShipping は別の段階だと型で表現できます。コンパイラが変更を追跡する: 型が変わると、関連する箇所すべてでコンパイルエラーが発生します。見落としがありません。/// 配送方法#[derive(Debug, Clone, PartialEq, Eq)]pub enum ShippingMethod {    Standard,    Express,    Overnight,}/// 配送情報#[derive(Debug, Clone, PartialEq)]pub struct ShippingInfo {    pub method: ShippingMethod,    pub cost: Money,}/// 配送情報付きの価格計算済み注文 - 新しい型#[derive(Debug, Clone, PartialEq)]pub struct PricedOrderWithShipping {    pub order_id: OrderId,    pub items: Vec<PricedOrderLine>,    pub amount_to_bill: Money,    pub shipping_info: ShippingInfo,}次に、新しいパイプラインステップを定義します。/// 新しいパイプラインステップ: 配送情報を追加pub fn add_shipping_info(order: PricedOrder) -> PricedOrderWithShipping {    // シンプルなロジック: $100以上は送料無料    let shipping = if order.amount_to_bill.0 > 100.0 {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(0.0),        }    } else {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(5.99),        }    };    PricedOrderWithShipping {        order_id: order.order_id,        items: order.items,        amount_to_bill: order.amount_to_bill,        shipping_info: shipping,    }}既存のコードを変更するのではなく、新しいステップを挿入します。validateOrder → priceOrder → addShippingInfo → acknowledgeOrder → createEventsこの「ステップの追加」というアプローチは、多くの機能追加に応用できます。ロギング、パフォーマンスメトリクス、認可チェック、監査。各ステップが独立していて、型が合っていれば、安全に追加・削除できます。VIP顧客の対応—入力をモデル化せよ著者は重要な指摘をしています。「ビジネスルールの出力（送料無料フラグ）ではなく、入力（VIP ステータス）をモデル化せよ」。なぜ「出力」ではなく「入力」をモデル化するのか？ルールが変わっても型は変わらない:「VIP は送料無料」→「VIP は送料 50%オフ」とルールが変わっても、CustomerStatus 型自体は変更不要です。関数だけ変えればよいです。原因をモデル化する:「送料無料かどうか」は結果（派生情報）です。原因は「VIP かどうか」です。原因をモデル化すれば、結果はいつでも計算できます。柔軟性が高い: VIP ステータスは送料以外にも使えます（優先サポート、限定商品へのアクセス等）。出力をハードコードすると、その柔軟性を失います。/// 顧客ステータス - ビジネスルールの「入力」をモデル化#[derive(Debug, Clone, PartialEq, Eq)]pub enum CustomerStatus {    Normal,    Vip,}/// 顧客#[derive(Debug, Clone, PartialEq, Eq)]pub struct Customer {    pub customer_id: String,    pub name: String,    pub status: CustomerStatus,}ビジネスルールは、入力（CustomerStatus）に基づいて決定を下します。/// 顧客ステータスに基づいて配送を計算pub fn calculate_shipping_for_customer(order: &OrderWithCustomer) -> ShippingInfo {    match order.customer.status {        CustomerStatus::Vip => ShippingInfo {            method: ShippingMethod::Express,            cost: Money(0.0), // VIPは無料のエクスプレス配送        },        CustomerStatus::Normal => {            if order.amount_to_bill.0 > 100.0 {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(0.0),                }            } else {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(5.99),                }            }        }    }}ビジネスルールが変わっても（例: VIP は送料無料→VIP は送料 50%オフ）、calculate_shipping_for_customer 関数を変更するだけでよいです。ドメインモデル自体（CustomerStatus）は変更する必要がありません。型の変更と波及効果本章の核心は、「型の変更がコンパイラによって追跡される」ことです。// 変更前pub struct PricedOrder { /* ... */ }// 変更後（配送情報を追加）pub struct PricedOrderWithShipping {    // ...    pub shipping_info: ShippingInfo,  // 新しいフィールド}PricedOrder と PricedOrderWithShipping は異なる型です。PricedOrder を期待していたコードに PricedOrderWithShipping を渡すとコンパイルエラーになります。// これはコンパイルエラー！fn process(order: PricedOrder) { /* ... */ }process(priced_order_with_shipping); // 型が違う動的型付け言語では、このような変更は「実行時エラー」として発見されます。静的型付けでは、「コンパイル時エラー」として発見されます。コンパイラがリファクタリングアシスタントとして機能します。関数型 DDD の核心は、「型でドメインを表現する」ことです。AND 型（struct）と OR 型（enum）でドメインの構造を表現します。状態遷移を別の型として定義します。制約を Smart Constructor で強制します。不正な状態を表現不可能にします。フラグを立てるな。型を作れ。これらの原則は、F#でも Rust でも適用できます。おわりに読む前の三つの悩みへの回答「はじめに」で述べた 3 つの読む動機に、本書がどう応えたかを振り返ります。1. DDDを学び直す必要があった → 関数型の視点でDDDが腹落ちしたDDD の概念—Aggregate、Entity、Value Object—は、オブジェクト指向の文脈で説明されると抽象的に感じていました。本書は、これらを「型」という具体的な道具で表現する方法を示しました。Value Object は単なる newtype です。struct OrderId(String) と書けば、それが Value Object です。Aggregate の境界は、型の境界で表現できます。ValidatedOrder と UnvalidatedOrder が別の型なら、それが境界です。「なぜそう設計するのか」を言語化できるようになりました。「この型を別にするのは、状態が違うから」「この値を newtype にするのは、ドメイン上の意味が違うから」。経験則ではなく、型システムに基づいた説明ができます。2. Rustでドメインモデリングを実践したかった → F#の概念はRustで十分(ではないかもしれないが)表現できるF#にあって Rust にない機能—パイプライン演算子、computation expressions、Units of Measure—は、確かにあります。しかし、本書の核心である「Make Illegal States Unrepresentable」は、Rust で十分に実践できたと思います。むしろ、Rust の所有権システムは F#にない利点を提供します。状態遷移を「所有権の移動」として表現できます。validate(order: UnvalidatedOrder) -> ValidatedOrder と書けば、検証前の注文は使えなくなります。F#では GC があるため、古い変数への参照が残る可能性がありますが、Rust では型システムがそれを防ぎます。3. AIエージェント時代における型システムの意味を考えたかった → 型は「AIが破れない制約」本書を読む前は、「型があると AI の生成精度が上がる」という感覚がありましたが、理論的に説明できませんでした。本書を読んで、その理由が明確になりました。型で定義された制約は、物理的に破れません。NonEmptyList<OrderLine> と定義すれば、AI は空の注文を返すコードを書けません。コンパイルが通らないからです。「このフィールドは必須です」というコメントは無視できますが、型は無視できません。これは「AI が守るべきルール」ではなく「AI が破れない壁」です。読む前と読んだ後Before（読む前）DDD の設計はある種の経験則で判断していましたRust の Option/Result は便利ですが、関数型との繋がりを考えていませんでした型があると AI の精度が上がる「気がする」程度の理解でしたAfter（読んだ後）DDD の概念を型システムの言葉で説明できるようになりましたTransformation-Oriented Programming（元のオブジェクトを変更せず、新しい値を作る）という原則を内在化しました型を「人間のためのドキュメント」かつ「AI が破れない制約」として設計できるようになりましたTransformation-Oriented Programming関数型プログラミングを学んで獲得する最も重要な概念は、実はシンプルです。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。本書を読み終えて、この一文の重みを改めて感じています。私たちはプログラミングを学ぶとき、まず「変数に値を代入する」ことから始めます。x = 1。x = x + 1。状態を変更することが、プログラミングの基本だと教わります。しかし、よく考えてみると、ビジネスロジックの大半は「入力を受け取り、変換し、出力を返す」で書けます。注文明細から合計金額を計算する → 入力と出力だけ住所をパースする → 入力と出力だけ商品コードを検証する → 入力と出力だけ状態の変更は不要です。副作用も不要です。ほとんどのビジネスロジックは、数学の関数のように書けます。では、状態が必要になるのはいつでしょうか。データベースに保存するとき。外部 API を呼ぶとき。ファイルに書き込むとき。つまり、システムの境界を越えるときだけです。この気づきが、設計の発想を変えます。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられないビジネスの世界は状態遷移で満ちています。注文は「未検証」から「検証済み」になります。カートは「空」から「商品あり」になります。申請は「提出済み」から「承認済み」になります。これは無視できません。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。order.validate() を呼ぶと、order の内部状態が変わります。この設計は、状態の追跡を難しくします。order は今どの状態なのか。どの経路を通ってここに至ったのか。フラグの組み合わせは正しいのか。常に頭の中で管理し続けなければなりません。本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。重要なのは、元のオブジェクトを変更しないことです。UnvalidatedOrder という型があります。validate という関数を適用すると、ValidatedOrder という新しい値が生まれます。このとき、元の UnvalidatedOrder には一切触れません。新しい値を作るだけです。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrder状態を「変更」するのではなく、「入力を受け取り、何かを行い、出力を返す」。元のオブジェクトには触れません。これが本書の核心です。この発想の転換がもたらすものこのアプローチを採用すると、いくつかの問題が消えます。状態の追跡が不要になる。 ValidatedOrder を持っているなら、それは「検証済みの注文」です。フラグを見る必要がありません。型がすべてを語ります。並行処理が安全になる。 元のデータを変更しないから、競合が起きません。テストが簡単になる。 入力を与えて、出力を確認します。それだけです。デバッグが楽になる。 各ステップの入出力をログに残せば、全経路が追えます。そして何より、ビジネスの言葉とコードが一致する。「見積書」が「発注書」になります。Estimate が Order になります。ビジネスの人々が頭の中で考えているモデルが、そのままコードになります。型は思考のツールである本書を読む前、型システムは「コンパイラを満足させるためのもの」だと思っていました。IDE の補完が効きます。リファクタリングが安全になります。その程度の認識でした。本書を読んで、型は「思考のツール」だと認識を改めました。AND と OR という 2 つの組み合わせで、複雑なドメインを表現できます。struct は「これとこれが両方必要」、enum は「これかこれのどちらか」。この単純な組み合わせが、ドメインの構造を可視化します。型を書くことは、ドメインを理解することです。型を読むことは、ドメインを学ぶことです。少なくとも、私はそう感じるようになりました。型で「不可能」を作る本書の内容を 2026 年の視点で読み直して、最大の発見がありました。「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」—この原則は、人間の開発者のミスを防ぐためのものとして紹介されています。しかし 2026 年現在、同じ原則がAIの出力を自動検証するフィルタとして機能しています。型で「不可能」を定義すると、AI が生成したコードのうち、その制約に違反するものはコンパイル時に除外されます。NonEmptyList<OrderLine> と定義すれば、AI が空の注文を返すコードを書いてもコンパイルエラーで検出されるVerifiedEmailAddress を要求すれば、AI が未検証メールへの送信を実装してもコンパイルが通らないUnvalidatedOrder → ValidatedOrder という型シグネチャがあれば、AI が検証をスキップするコードはコンパイルエラーになるこれは「AI に正しいコードを書かせる」のではなく、「AI が書いた誤ったコードを検出する」メカニズムです。AI の精度向上ではなく、フィルタリング機構として機能します。先日、Claude Code に「Order を作成する関数を書いて」と指示しました。生成されたコードは Vec<OrderLine> を返していました。しかし私のコードベースでは NonEmptyList<OrderLine> を使っています。コンパイルエラーが発生し、AI は「空の注文」を作るコードを出力しましたが、それが本番に混入することはありませんでした。一方、別のプロジェクトでは「このフィールドは必須」とコメントに書いただけでした。AI はそのコメントを無視して Option を返すコードを生成し、後から問題が発覚しました。型で定義された制約は、コンパイル時に検証されます。コメントは検証されません。この違いが重要です。「何を作るか」を決める能力本書の第 1 章で、著者は「開発者の仕事はコードを書くことではなく、ソフトウェアを通じて問題を解決すること」と述べています。2018 年に書かれたこの言葉は、2026 年の今、さらに重みを増しています。「何を作るか」という問いを分解してみます。ビジネス要件の理解: ドメインエキスパートとの対話、暗黙知の引き出し技術的制約の把握: 既存システムとの整合性、パフォーマンス要件、チームのスキルセット両者のトレードオフ判断: 「正解がない」状況での意思決定現時点で AI が得意なのは 2 番目です。ドキュメントやコードベースを読み、技術的な制約を分析できます。一方、1 番目と 3 番目は人間の仕事です。ドメインエキスパートとの対話で暗黙知を引き出すこと、そして「どちらも正しい」状況でトレードオフを判断すること。これらは AI に委譲できません。この構造を理解すれば、「人間の仕事」を「暗黙知の言語化」と「トレードオフ判断」に絞り込めます。本書で学んだドメインモデリングの技術は、まさにこの「暗黙知の言語化」を支援するものです。型で表現されたドメインモデルがあれば、AI は「どう作るか」を高い精度で実行できます。ドメイン駆動設計をはじめよう ―ソフトウェアの実装と事業戦略を結びつける実践技法作者:Vlad Khononovオーム社Amazon働き方の逆転—AIエージェント時代の開発スタイル本書を読みながら、自分の働き方が根本的に変わったことを実感しました。以前のモデルでは、人間がコードを書き、AI は相談相手でした。Stack Overflow の代わり、ドキュメント検索の高速化。補助的な存在です。現在のモデルでは、AI が運転席に座り、人間は助手席でナビゲーションをしています。AI にプランを練らせ、レビューし、実装させ、またレビューする。この流れが定着しました。AI は「分身」的な存在になりました。明確な指示とコンテキストを与えれば、疲労知らずで作業してくれる相棒です。Claude Opus 4.5 以降、この感覚は決定的になりました。プログラミングのシンタックスを書く機会は明らかに減りました。では、ソフトウェアエンジニアの役割はどう変化したのでしょうか。1. アーキテクチャの指針決定AI は「どう作るか」を実行できますが、「なぜそう作るか」は決められません。Bounded Context の境界をどこに引くか、技術選定のトレードオフ、パフォーマンスと保守性のバランス。これらは人間が判断します。2. コードベースから読み取れないコンテキストの整理・提供AI はコードに書かれていないことを知りません。なぜこの設計にしたか、本番環境でのみ発生する問題、チームの暗黙のコーディング規約、ビジネス上の制約。これらを言語化し、CLAUDE.md やコメントに落とし込む能力が求められます。3. 期待する挙動を自動・継続的に検証する枠組みの整備AI が書いたコードは「動く」かもしれませんが、「正しい」とは限りません。型による制約、プロパティベーステスト、E2E テスト、本番監視。これらの枠組みを整備し、AI の出力を検証し続けるのは人間の仕事です。本書の「Make Illegal States Unrepresentable」は、まさにこの 3 番目の観点で価値を発揮します。型で制約を定義すれば、AI の出力を自動的に検証できます。コンパイルが通れば、少なくとも型レベルの正しさは保証されます。コードを手で書くという作業は、実は思考の外在化プロセスでもありました。書きながら考えていた。この機会が減ったとき、思考の質をどう担保するか。正直、まだ答えが出ていません。本書のようなドメインモデリングの訓練はその答えの 1 つかもしれませんが、それで十分かどうかは分かりません。分からないまま、AI と協業し続けています。Architecture Modernization との接続本書を読みながら、Nick Tune 著『Architecture Modernization』の内容が何度も頭をよぎりました。現在、この本の翻訳に携わっています。Architecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazon『Domain Modeling Made Functional』は「新規開発」の文脈で DDD を説明しています。しかし現実の多くのプロジェクトは「既存システムの改善」です。レガシーシステムをどう分析し、背景情報からBounded Context をどう切り出し、段階的にモダナイズしていくか。『Architecture Modernization』はまさにその部分を扱っています。翻訳作業を通じて、共感できる内容が多くありました。特に「既存システムの暗黙知をどう言語化するか」という問題意識は、本書の「ドメインエキスパートとの対話」と通じるものがあります。AI エージェント時代において、この問題はさらに重要になっています。AI は「新しいコードを書く」ことは得意ですが、「既存システムの文脈を理解する」ことは苦手です。10 年前の設計判断の背景、当時の技術的制約、組織の歴史。これらを言語化し、モダナイゼーションの方針を決めるのは、依然として人間の仕事です。本書を読んで「関数型 DDD」に興味を持った方には、『Architecture Modernization』も勧めたいです。新規開発だけでなく、既存システムをどう改善するか。両方の視点を持つことで、設計の引き出しが増えます。最後に2018 年に出版された本書を、2026 年に読む価値はあったでしょうか。かなり、Yes です。ただ、正直なところ、本書の「すべて」を実践できる自信はありません。Smart Constructor を徹底すると言いながら、明日には String を直接使っているかもしれません。型で不可能を作るのは、思っているより面倒くさい作業です。締め切りに追われると、つい妥協してしまう。それでも、本書を読んだことで「何かに気づいた」感覚はあります。うまく言葉にできませんが、型を書くときの解像度が変わった気がします。Option を見たとき、「本当にこれは省略可能なのか？」と問い直すようになりました。冒頭で触れた『Architecture Modernization』の翻訳作業。本書を再読したことで、「Bounded Context」「Aggregate」といった用語を訳すとき、以前より自信を持てるようになりました。言葉の背後にある設計思想を、型という道具で理解したからです。翻訳は続いています。この感覚が正しいのかどうかは、実務で検証していくしかありません。関数型 DDD は、特定の言語やパラダイムに縛られません。F#で書かれた本書の概念は、Rust でも実践できます。そして、人間と AI が協業する時代において、「不可能を型で定義する」技術の価値はますます高まっていく—たぶん。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク]]></title>
            <link>https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</link>
            <guid isPermaLink="false">https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</guid>
            <pubDate>Tue, 20 Jan 2026 01:05:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 「なんか遅い」「たまにエラーが出る」――マイクロサービスのシステムでこんな報告を受けたとき、皆さんはどこから調べ始めますか？ Sreake事業部インターン生の小林です。2025年11月-12月の間インターンに参 […]The post Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[
		Binance账户创建 より		]]></title>
            <link>https://sreake.com/blog/chatgpt-slack-integration/#comment-4673</link>
            <guid isPermaLink="false">https://sreake.com/blog/chatgpt-slack-integration/#comment-4673</guid>
            <pubDate>Mon, 19 Jan 2026 11:29:29 GMT</pubDate>
            <content:encoded><![CDATA[Your article helped me a lot, is there any more related content? Thanks! https://www.binance.com/fr-AF/register?ref=JHQQKNKN]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、頑張るなら組織と踊れ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</guid>
            <pubDate>Mon, 19 Jan 2026 00:01:19 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「おい、辞めるな」で辞めないことを選んだ。syu-m-5151.hatenablog.com「おい、辞めないなら頑張れ」で頑張り方を学んだ。syu-m-5151.hatenablog.com見せろ。対話しろ。上司を勝たせろ。スポンサーを作れ。そう書いた。で、やってみてどうだった。正直に言う。私はうまくいかなかった。見せているつもりだった。対話しているつもりだった。上司を勝たせようとしていた。でも、空回りしていた。なぜか。組織の力学を理解していなかったからだ。いや、もっと正確に言おう。理解しようとしなかった。組織の力学——いわゆる「政治」——を、私は嫌悪していた。「実力で勝負したい」「政治なんかに関わりたくない」——そう思っていた。技術的な正しさを盾に、人間関係の機微を「非論理的」と切り捨てていた。以前、「正義のエンジニアという幻想」という記事を書いた。syu-m-5151.hatenablog.comあの記事で書いたことは、今でも私の中に残っている。媚びないことと無礼であることの区別もつかないまま、技術的優位性を振りかざしていた——そんな恥ずかしい過去を、私は持っている。今回は、その続きを書く。組織の力学について。私が嫌悪していたもの。でも、理解しなければ成果を出せないもの。そして、したたかに生きるということについて。先に結論を言っておく。理解することと、加担することは違う。そして、政治をやっている人は「汚い大人」ではない。泥臭く仕事を通そうとしているだけだ。正直に告白する。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分が知りたかったことを書く。飲み屋でそれを喋る。それがこの「おい、」シリーズの趣旨だ。おい、頑張るなら組織と踊れ。——と書いて、自分でも苦い顔をしている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。私は「正義のエンジニア」だった最初に告白しておく。私はかつて、自分の技術思想とキャリア戦略が100%正しいと信じて疑わなかった。そして、それを受け入れない企業、同僚たちが100%間違っていると本気で思っていた。今思えば、それはソフトウェアエンジニアという職業に就いた多くの若い人が陥る、ある種の思春期的な錯覚だったと思う。「なぜこんな非効率的な実装をするんですか？」「技術的にはこっちの方が正しいんですけどね」「政治的な理由で技術選定するなんて、エンジニアリングの敗北だ」そんな言葉を、私は何度思って何度口にしたことだろう。ある会議で、私は技術的に正しい提案をした。データに基づいていた。論理的だった。反論の余地がないと思っていた。却下された。理由は曖昧だった。「今はタイミングが悪い」「もう少し検討が必要」。でも、本当の理由は別にあった。私は後から知った。あの提案は、ある部門の利害と衝突していた。その部門のキーパーソンに、事前に話を通していなかった。だから、会議の場で潰された。私はその事実を伝えられた時に密かに怒った。「政治で正しい提案が潰されるなんて、おかしい」と。でも、冷静に考えると、私の方がおかしかった。技術的に正しいことと、組織で通ることは、別の問題だ。私はその区別ができていなかった。これは「誰に話を通すか」の問題だった。でも、組織の力学を理解していないことは、別の形でも現れた。あるプロジェクトで、私は黙々と成果を出していた。技術的な課題を解決し、納期を守り、品質を担保した。「これだけやれば評価されるだろう」と思っていた。評価面談で、上司はこう言った。「〇〇さんの貢献は分かっているんだけど、他のマネージャーに説明しにくいんだよね」。私は意味が分からなかった。成果を出しているのに、なぜ説明しにくいのか。後から分かった。私の仕事は「見えなかった」のだ。他のマネージャーは、私が何をしているか知らなかった。評価会議で私の名前が挙がっても、「誰？」という反応だった。私の上司は、私を推そうにも、材料がなかった。見えない仕事は、存在しないのと同じ——「おい、辞めないなら頑張れ」で書いたことだ。でも、それは「見せる」だけでは解決しない。誰に見せるか。どのタイミングで見せるか。どの文脈で見せるか。それを間違えると、見せても意味がない。私は「政治」を嫌悪していた。でも、その嫌悪が、私自身の足を引っ張っていた。媚びないと無礼を混同していたここで、痛い告白をする。「私は媚びない」——それが私のプライドだった。しかし「媚びない」と「無礼」は違う。私は単に無礼だった。コードレビューで、つい正論を優先してしまう癖があった。「このコード、正直ひどくないですか？全部書き直した方が早いです」——そんなコメントを書いていた。ある日、シニアエンジニアが個別に連絡をくれた。「君の指摘は技術的には正しい。でも、そのコメントを見た人がどう感じるか考えたことある？彼は他のタスクも抱えながら、期限に間に合わせようと必死だった。君のコメントは、その努力を全否定している」その言葉にハッとした。私は技術的な正しさばかりを見て、人の気持ちを踏みにじっていたのだ。別の機会には、マネージャーが1on1で厳しい指摘をした。「君は優秀だ。でも、チームメンバーが君を避け始めている。それでいいの？技術力があっても、一人では何も作れないよ」媚びないことと、相手を尊重することは両立する。でも当時の私にはその区別がつかなかった。率直であることと配慮がないことを混同していた。技術的な正しさを盾に、人としての礼儀を忘れていた。私は様々な言い訳を用意していた。「エンジニアは成果で評価されるべきだから人間関係は二の次」「技術的に正しいことが最優先だから言い方なんて些細な問題」「実力があれば多少の態度の悪さは許される」これらはすべて、自分の社会性の欠如を正当化するための、頭の悪い言い訳だった。まるで反抗期の中学生が「大人は汚い」と言い訳するように、私は「技術的正しさ」を盾に、自分の未熟さを隠していたのだ。転機は、年次が上がって後輩ができたときに訪れた。私の何気ない「それは違うよ」という一言で、新卒エンジニアが完全に萎縮してしまった。その後、彼は私に質問することを避けるようになり、分からないことを抱え込むように。私は、かつて自分が嫌っていた「怖い先輩」になっていたのだ。このとき、ようやく理解した。正しいことを、正しい方法で伝えられなければ、それはただの暴力だ。技術力は重要だが、それをどう使うかはもっと重要。正しいことを言っているつもりで、実際には相手の立場に立てていなかっただけだった。そういう時代もあったでよいここで、過去の自分との向き合い方について書いておく。ある時期、私は過去の失言や態度を思い出しては、布団の中で悶えていた。「あの時、なぜあんなことを言ったんだ」「もっと早く気づいていれば」——後悔の反芻は止まらなかった。コードレビューで人を傷つけた記憶。会議で空気を凍らせた記憶。「正しいことを言っているのに、なぜ分かってもらえないんだ」と憤っていた記憶。思い出すたびに、顔が熱くなった。でも、ある時気づいた。過去を責めても、過去は変わらない。変えられるのは、これからだけだ。だから、こう割り切ることにした。「過去はすべて正しかった」と。誤解しないでほしい。過去の行動が道徳的に正しかったと言いたいわけではない。あの無礼な態度は、やはり間違っていた。でも、あの経験があったから、今の自分がいる。痛い目に遭わなければ、私は変われなかった。あの失敗がなければ、この記事を書くこともなかった。過去を否定し続けると、エネルギーが過去に吸い取られる。後悔に費やす時間は、未来への投資に使えない。「あの時こうすればよかった」と100回考えるより、「これからどうするか」を1回考える方が、よほど生産的だ。過去を受け入れろ。そして、これからの人生に全力で取り組め。私は自分の未熟さを認めた。媚びないことと無礼の区別がついていなかったことを認めた。では、これからどうするか。答えは明確だった。組織の現実を、ちゃんと見ることだ。私が見ようとしなかったもの。「政治」と呼んで嫌悪していたもの。でも、理解しなければ前に進めないもの。——組織の力学について、正面から向き合う時が来た。組織には「裏の顔」があるここで、現実を直視しよう。組織には、公式なルールと非公式な力学の2つが常に存在している。公式なルールは分かりやすい。組織図、職務権限、承認フロー、評価制度。これらは明文化されていて、誰でもアクセスできる。でも、それだけで組織が動いているわけではない。非公式な力学とは、「正式な手続きには定められていないが、意思決定や資源配分に影響を与える行動」のことだ。根回し、人脈、暗黙の了解、派閥、影響力のある人物——そういうものだ。私はこれを「汚いもの」だと思っていた。でも、違った。彼らは汚いのではなく、泥臭いだけだった。非公式な力学は「善悪」ではなく「手段」だ。根回しは悪いことか。場合による。自分の私利私欲のためなら問題だ。でも、良いプロジェクトをスムーズに通すためなら、むしろ必要なことだ。関係者の懸念を事前に把握し、対処しておく。それは「政治」ではなく「配慮」とも呼べる。私が嫌悪していたのは、「非公式な力学」そのものではなかった。それを私利私欲のために使う人間だった。でも、手段と目的を混同していた。手段自体は中立だ。それをどう使うかが問題なのだ。ここで1つ、大事なことを言っておく。非公式な力学を理解することと、それに迎合することは違う。力学を理解した上で、「自分はこの手段は使わない」と決めてもいい。でも、理解せずに無視するのは、ただの怠慢だ。敵を知らずに戦っているようなものだ。私は長い間、「政治を理解する」こと自体を拒否していた。理解したら、自分も「あっち側」になる気がした。でも、それは間違いだった。理解することと、加担することは違う。そして、「あっち側」の人たちは、別に悪者ではなかった。ただ、泥臭く仕事を通そうとしていただけだった。理解した上で、どう振る舞うかは自分で決められる。組織図を信じるな次に、私が痛い目を見た話をする。組織図に描かれた権限構造と、実際に物事を動かせる力は違う。あるプロジェクトで、私は承認を得るために、組織図上の決裁者に話を持っていった。正式なルートだ。決裁者は「いいんじゃない」と言った。私は安心した。プロジェクトは頓挫した。何が起きたか。決裁者は「いいんじゃない」と言ったが、実際に動く現場のキーパーソンは別にいた。その人は私の提案に反対だった。決裁者が「いい」と言っても、現場が動かなければ、何も進まない。私は組織図を信じすぎていた。「誰が決裁権を持っているか」と「誰が実際に物事を動かせるか」は違う。そして後から気づいたことがある。同じ提案でも、事前に相談していれば通っていた可能性が高い。あのキーパーソンに、会議の前に一度話を聞きに行っていたらどうだっただろう。「こういうことを考えているんですが、懸念点はありますか？」と。相手の観点や懸念が事前に分かれば、提案に反映できる。相手も「聞いてもらった」という感覚がある。「聞かされていない」は、「間違っている」より強い反対理由になる。内容の良し悪しではない。プロセスの問題だ。これは単なる感情論ではない。組織心理学では「手続き的公正」と呼ばれる概念がある。人は、結果だけでなく、そこに至るプロセスが公正かどうかを重視する。自分の意見を聞いてもらえた、自分も関与できたという感覚があれば、たとえ結果が自分の望み通りでなくても、受け入れやすくなる。逆に、プロセスから排除されたと感じると、結果が正しくても反発する。私が会議で潰された提案は、まさにこれだった。内容は正しかった。でも、関係者は「自分は聞かれていない」と感じた。関係者に事前の相談なく、いきなり会議の場で出したことが、「あなたの意見は聞く必要がない」というメッセージになっていた。それだけで、反対する十分な理由になった。技術的に正しいかどうかと、組織で通るかどうかは、別の問題だ。そして、事前に挨拶して、相談して、懸念を聞いておく——それだけで結果が変わることが、驚くほど多い。ここで、少し視点を変えた話をする。私は下っ端として組織図に騙されてきた。でも、ある時気づいた。上に立つ人間こそ、この罠にはまりやすいのだと。下っ端の経験が少ない若いCEOやCTOが率いる組織を見てきた。彼らは往々にして、表側の組織図ばかり意識する。そして、裏側の関係性——長年かけて築かれた非公式なネットワーク——を軽視して、ドラスティックな組織変更をする。「この部署とこの部署を統合しよう」「この人をあのチームに異動させよう」——組織図の上では合理的に見える。でも、その変更が裏のネットワークをぐちゃぐちゃにすることがある。誰と誰が信頼関係を築いていたか。どのルートで情報が流れていたか。誰が実質的なキーパーソンだったか。それを無視して箱だけ動かす。若い頃の私は、そういうリーダーを「革新的だ」「スピード感がある」と思っていた。古い慣習を壊して、新しい組織を作る。カッコいいと思っていた。大人になった今は、違う見え方をする。成果を出すために、下の人間が苦労している。壊された関係性を、現場が必死で繋ぎ直している。組織図の上では「改革成功」に見えても、実際は現場の努力で何とか回っているだけ。これは「組織図を信じるな」の裏返しだ。組織図だけを見て動く危険は、下っ端だけの問題ではない。リーダーが組織図だけを見て動くと、現場が壊れる。私が組織の裏側を理解しようとするようになったのは、こういう経験も影響している。組織図の裏にあるものを無視すると、どうなるか。それを見てきたからだ。では、「組織図の裏にあるもの」とは、具体的に何か。私はそれを「影のネットワーク」と呼んでいる。かっこいい名前をつけたいわけではない。組織図には描かれないが、確実に存在するもの。それを言語化するために、この言葉を使っている。そしてその核心は、役職とは別に存在する権力だ。権力とは、役職に基づく権限だけではない。反対や抵抗を乗り越えて物事を実現する力。人を惹きつけ、巻き込む力。意思決定に実質的な影響を与える力。これらは、役職とは別に存在する。例えば、古株のベテラン社員。役職は高くないが、社内の歴史を全部知っている。誰と誰が仲が悪いか、過去にどんなプロジェクトが失敗したか、どの部署が何を嫌がるか。その人を味方につけると物事がスムーズに進む。敵に回すと、見えない抵抗にあう。例えば、経営者の信頼が厚い若手。役職は低いが、経営者に直接話ができる。その人の意見は、なぜか上まで届く。私は、この「影のネットワーク」を読めていなかった。組織図だけを見て、「この人に話を通せばOK」と思っていた。でも、組織図の裏には、別のネットワークがあった。なぜ「影のネットワーク」が存在するのか。理由は単純だ。組織図は「権限」を示すが、「実行力」を示さない。決裁権を持つ人が「やれ」と言っても、実際に手を動かす人が動かなければ、何も起きない。そして、実際に手を動かす人を動かせるのは、必ずしも決裁権を持つ人ではない。組織が大きくなるほど、この乖離は広がる。決裁者は現場から遠くなり、現場の信頼関係は決裁者の目に見えなくなる。結果として、「承認されたのに進まない」「反対されていないのに協力が得られない」という現象が起きる。これは個人の悪意ではない。権限と実行力が分離している構造の問題だ。組織図の裏にある「影のネットワーク」を読み解け。どの提案に誰が反発するか。誰を味方につければ障壁を突破できるか。情報がどのルートで流れるか。これが見えるようになると、立ち回り方が変わる。ここで、私が学んだ具体的な方法を書いておく。1. 会議での反応を観察する誰かが発言したとき、他の人の表情を見る。賛成しているのか、本音では反対なのか、無関心なのか。言葉ではなく、表情や態度に本音が出る。2. 「あの人に聞いてみたら」の連鎖を追う何か新しいことを始めようとしたとき、「あの人に聞いてみたら」と言われる人がいる。その人が、実質的なキーパーソンだ。組織図上の役職とは関係ない。3. 過去の意思決定を遡る大きな決定が下されたとき、「誰がどの段階で関わっていたか」を調べる。公式の決裁者だけでなく、その前に相談されていた人。その人が、影響力を持っている。4. ランチや雑談の相手を観察する誰と誰がよく一緒にいるか。情報は公式のルートだけでなく、非公式の人間関係を通じて流れる。ここまでが「見る」段階だ。では、見えたものをどう使うか。観察した後にどうするか観察だけでは意味がない。観察した情報を、行動に変える必要がある。キーパーソンが分かったら、提案の前に一度相談に行く。反対しそうな人が分かったら、その人の懸念を先回りして潰す。情報のルートが分かったら、そのルートに自分の情報を流す。最初は気が重い。「なぜこんな面倒なことを」と思う。でも、一度やってみると、驚くほど物事がスムーズに進む。私も最初は抵抗があった。でも、「正しい提案が政治で潰される」ことに比べれば、事前の相談なんて些細な手間だと気づいた。私が変わるまでの話ここまで読んで、「分かったけど、やっぱり嫌だ」と思う人がいるだろう。「政治なんかしたくない」「実力で評価されるべきだ」「こんなことに時間を使いたくない」。その気持ちは分かる。私もそうだった。そして正直に言えば、今でも完全には割り切れていない。私が組織の力学をどう受け止めてきたか、正直に書く。最初は、拒絶していた。長い間、ずっとそうだった。「実力で評価されるべきだ」「政治をやる奴は汚い大人だ」「自分はそういうことはしない」。そう思っていた。この時期は、現実とのギャップに苦しんだ。「なんで自分より実力のないあいつが評価されるんだ」「この会社はおかしい」。怒りや失望があった。でも、状況は変わらなかった。居酒屋で同僚と愚痴を言っていた。「あいつは政治がうまいだけだ」「実力で勝負しろよ」。言うたびに少し楽になった。でも、翌日も同じ状況が続いた。全ての原因を外部に求めていた。自分が提案した新技術が却下されれば「老害が変化を恐れている」と憤り、レガシーコードの改修を任されれば「俺の才能の無駄遣い」と不満を漏らし、ドキュメント作成を頼まれれば「エンジニアの仕事じゃない」と文句を言う。でも振り返ってみれば明らかだ。問題は私自身にあった。技術的な正しさだけを追求し、ビジネス的な制約や組織の事情を理解しようとしなかった。転機があった。尊敬していた先輩が、根回しをしているのを見た。「あの人も政治をやるのか」と最初は失望した。でも、よく見ると違った。先輩は、良いプロジェクトを通すために、関係者の懸念を事前に聞いて回っていた。それは「政治」というより「配慮」だった。「政治」と「配慮」の境界は曖昧だ。私が嫌悪していた「政治」の中には、実は「配慮」も含まれていた。それに気づいてから、少し楽になった。組織の力学を「存在するもの」として認められるようになった。好き嫌いを超えて、「まあ、そういうものだよな」と思えるようになった。過度に振り回されない心理的安定が生まれた。今はどうか。正直に言う。私はまだ、完全には割り切れていない。根回しをすることに、今でも抵抗がある。「これは本当に必要なのか」「実力で勝負すべきじゃないのか」と思う。でも、必要な場面では、やるようになった。割り切れないまま、やっている。——と書いて、立ち止まる。私と同じように変われ、と言いたいわけではない。どこまで受け入れるかは、自分で決めていい。「存在は認めるけど、自分はやらない」でもいい。「存在を認めることすら嫌だ」なら、別の環境を探してもいい。ただ、組織の力学を拒絶し続けていると苦しい。現実と理想のギャップに消耗し続ける。だから、少なくとも「存在を認める」ところまでは進んだ方が、楽になる。その先は、自分で決めればいい。譲れないもののために、譲るものを決める「存在を認める」ところまで進んだとする。でも、それだけでは足りない。認めた上で、どう振る舞うか。全部受け入れるのか。全部拒否するのか。——どちらも違う。私が辿り着いた答えは、もっと戦略的なものだった。ここで、私が学んだ重要なことを書く。本質を守るために、形式では妥協する。やがて私は真剣に考えるようになった。自分が本当に譲れないものは何か？見極める基準は1つ。「あったらいいな」は捨てろ。「なくなったら壊れる」だけを守れ。私にとって譲れないのは3つだった。1つ目は技術的な誠実さ。嘘はつかない、質の低いコードは書かない。これを失ったら、自分を信頼できなくなる。2つ目はユーザーファースト。エンドユーザーの利益を最優先する。これを失ったら、仕事の意味を感じられなくなる。3つ目は継続的な学習。常に新しいことを学び続ける。これを失ったら、市場価値が消える。これ以外は、状況に応じて柔軟に対応することにした。表現方法やタイミングを妥協しても、私は壊れない。だから手放せる。表現方法では本音を建前でオブラートに包むようになった。タイミングも最適な時期を待つように。プロセスでは目的のためなら遠回りも受け入れ、形式的には無駄に見える会議や書類も必要なら対応するようになった。全てを守ろうとすると、全てを失う。なぜか。理由は単純だ。妥協できない領域が増えるほど、交渉の余地は減る。交渉の余地が減るほど、衝突は増える。衝突が増えるほど、消耗する。消耗すると、本当に守りたかったものまで守るエネルギーがなくなる。私は以前、表現方法でも、タイミングでも、プロセスでも、一切妥協しなかった。「正しいことを、正しいタイミングで、正しい方法で言う」——それが自分の信念だと思っていた。結果、毎回衝突し、毎回消耗し、最終的には技術的な誠実さすら保てなくなった。疲れ果てて、どうでもよくなったのだ。だから、何を守り、何を手放すかを決める。これが大人の戦略だ。以前は、「妥協＝敗北」だと思っていた。でも違った。戦略的な妥協は、本質を守るための手段だ。形式で妥協し、本質を守る。それは負けではない。むしろ、本当に大事なもののために、大事でないものを手放す勇気だ。したたかに生きる戦略「譲れないものを守り、それ以外では妥協する」——それは分かった。でも、正直に言えば、それだけでは物足りない。守りに入っているだけだ。もっと攻めの姿勢で、組織を「利用」することはできないのか。——そう考えるようになった。ここで、もう一歩踏み込んだ話をする。技術は手段であって目的ではない——組織から見れば、そうだ。でも正直に言えば、私自身は技術的な興味に駆動されている。新しい技術を学ぶことが楽しいし、エレガントなコードを書くことに喜びを感じる。ビジネス価値なんてどうでもよくて、ただ面白い技術を触っていたいだけ、というのが本音だ。でも、お金をもらって仕事をする以上、建前上それが主目的とは言いづらい。だからこそ「したたかにやろうぜ」という考え方が大切なのだ。つまり、組織が求める「成果」という枠組みを利用して、自分の技術的好奇心を満たすということ。表向きは「ビジネス価値の創出」を掲げながら、実際には「面白い技術で遊ぶ」ための正当性を確保する。例えば、「パフォーマンス改善」という大義名分のもとで、最新のフレームワークを導入する。「開発効率の向上」という建前で、面白そうなツールチェーンを構築する。「技術的負債の解消」という錦の御旗を掲げて、自分が書きたいようにコードを書き直す。重要なのは、これらの建前が単なる口実ではなく、実際に価値を生み出すことだ。新技術で遊びながら、本当にパフォーマンスを改善する。好きなツールを使いながら、実際に開発効率を上げる。コードを書き直しながら、本当に保守性を向上させる。ここで正直に告白しておく。私はこの戦略で失敗したことがある。「開発効率の向上」を名目に、面白そうなビルドツールを導入した。確かに面白かった。でも、チームの学習コストを甘く見積もっていた。結果として、効率は上がるどころか下がった。建前が嘘になった瞬間、「あいつは自分のことしか考えていない」という評価が下された。信頼を取り戻すのに、かなりの時間がかかった。したたかさの前提は、建前が本当に価値を生み出すことだ。建前が嘘になった瞬間、したたかさは不誠実に変わる。自分が楽しいかどうかではなく、本当に成果が出るかどうか。その見極めを間違えると、戦略は破綻する。「プロフェッショナルとして責任を果たします」と胸を張りながら、心の中では「やった！これで堂々とRustが書ける！」と小躍りする。この二重構造こそが、エンジニアとしてのしたたかさだ。ただし、小躍りする前に、本当に成果が出るかを冷静に見極めること。それを怠ると、私のように痛い目を見る。組織は成果を得て満足し、私たちは技術的満足を得る。Win-Winの関係を作り出すこと。それは決して不誠実ではなく、むしろ異なる価値観を持つ者同士が、お互いの利益を最大化する賢明な戦略なのだ。組織をハックしろ。建前で成果を出し、本音で技術を楽しめ。影響力は才能ではなくスキルだここまで「したたかにやれ」と書いてきた。「でも、自分は政治が苦手だ」という人がいるだろう。分かる。私もそうだった。というか、今でもそうだ。人の顔色を読むのが苦手だし、根回しは面倒くさいと思っている。でも、安心してほしい。影響力は先天的な才能ではなく、後天的に磨けるスキルだ。私も苦手だった。今でも得意とは言えない。でも、意識して練習することで、少しずつマシになった。組織における対人影響力は、5つの能力で構成されている。これらは「観察→洞察→共感→表現→一貫性」というプロセスで連鎖する。1. 観察——表面を見る最初の能力は観察だ。目の前で起きていることを正確に捉える。何を観察するか。言葉——誰が何を言ったか。態度——表情、姿勢、声のトーン。関係——誰と誰が近いか、誰が誰を避けているか。反応——ある発言に対して、他の人がどう反応したか。観察は受動的な行為に見えるが、意識しないとできない。会議で自分の発言に集中していると、他の人の反応を見落とす。発言を減らし、観察を増やす——これだけで得られる情報量は変わる。2. 洞察——本質を見抜く観察の次は洞察だ。表面の情報から、見えないものを推測する。洞察とは何か。動機を読む——この人は何を求めているのか、何を恐れているのか。構造を読む——この組織で、誰が実質的な力を持っているのか。文脈を読む——この議論は、どんな歴史の上に成り立っているのか。観察が「何が起きているか」を捉えるなら、洞察は「なぜ起きているか」を捉える。同じ事象を見ても、洞察の深さで解釈は変わる。表面的な反対意見の裏に、本当の懸念が隠れていることがある。3. 共感——相手の立場に立つ洞察の次は共感だ。相手の世界を、相手の視点から理解する。共感は「同意」ではない。相手の意見に賛成しなくても、相手がなぜそう考えるかを理解することはできる。「この人の立場なら、確かにそう思うだろう」——その理解があれば、対立は減る。エンジニアは共感を軽視しがちだ。論理が正しければ、相手の感情は関係ないと思っている。しかし、人は論理だけでは動かない。自分の立場を理解してくれていると感じたとき、初めて耳を傾ける。4. 表現——相手に響かせる共感の次は表現だ。自分の考えを、相手に届く形で伝える。表現の本質は「相手に合わせる」ことだ。論理で動く人には論理を。感情で動く人には感情を。利害で動く人には利害を。同じ提案でも、切り口を変えれば響き方が変わる。「伝える」と「伝わる」は違う。自分が言いたいことを言うのは「伝える」。相手が受け取れる形で届けるのが「伝わる」。影響力とは「伝わる」力だ。5. 一貫性——信頼を積む最後は一貫性だ。これが他の4つを支える土台になる。一貫性とは何か。言ったことを実行する。約束を守る。嘘をつかない。単純だが、最も難しい。なぜ難しいか。一貫性を保つには、「できない約束をしない」という自制が必要だからだ。期待に応えたくて、つい「やります」と言ってしまう。しかし、守れない約束は信頼を削る。「できません」と言える人の方が、長期的には信頼される。一貫性がなければ、観察も洞察も共感も表現も、すべて無駄になる。「あの人の言うことは当てにならない」——そう思われた瞬間、影響力は消える。これら5つは、すべて後天的に磨けるスキルだ。生まれつきの才能ではない。ただし、順番がある。土台となる「一貫性」がなければ、他の4つは機能しない。まず信頼を築き、その上に観察・洞察・共感・表現を乗せる。「専門性」と「人望」が最強のカードだここまで「組織の力学を理解しろ」「影響力を磨け」と書いてきた。でも、ここで安心してほしいことがある。最も持続する影響力は「専門性」と「人望」から生まれる。なぜそう言えるのか。少し整理してみる。人が他人を動かす力——影響力には、いくつかの種類がある。ソフトウェアエンジニアの現場で見かける例で説明する。報酬で動かす場合がある。「このリファクタリングを完了させたら、次のスプリントで好きな技術調査の時間をあげる」。評価で動かすこともある。「このタスクを断ったら、次の評価に響くよ」。役職で動かすパターンもある。「テックリードの判断だから、この設計で行く」。データで動かすこともできる。「ベンチマークの結果、この実装の方が30%速い」。そして、専門性で動かす場合がある。「Kubernetesのことなら〇〇さんに聞けば間違いない」。人望で動かす場合もある。「あの人が言うなら、きっと理由があるはず」。このうち、専門性と人望が最も強い。なぜか。この2つは、相手が「自分から納得して動く」ときに生じるからだ。報酬・評価・役職で動かす場合、相手は「仕方なく」動いている。上司が変わったり、評価制度が変わったりすれば、その影響力は消える。「テックリードが言うから従う」で動いていたチームは、テックリードがいなくなれば元に戻る。専門性と人望で動かす場合、相手は「この人の言うことだから」と自分から動いている。その人がいなくなっても、「あの人ならどう判断するだろう」と考え続ける。影響が内面化されている。外からの圧力で動いた行動は、圧力がなくなれば止まる。内側から納得して動いた行動は、続く。ただし、注意点がある。どのカードが強いかは、組織や部署によって違う。エンジニアだけの組織では、データが圧倒的に強い。「ベンチマークの結果」「障害の根本原因分析」「パフォーマンス計測」——数字で示せば、それだけで説得力がある。論理と数字を重視する文化があるからだ。でも、営業部門やマーケティング部門では違う。データより「この人が言うなら」という人望が効くことがある。経営層との会議では、役職や過去の実績が重みを持つ。同じ会社でも、部署が変われば有効なカードは変わる。私はエンジニア組織にいることが多いので、データと専門性に頼りがちだ。でも、他部署との調整では、それだけでは通用しないことを何度も経験した。相手が何を重視するかを見極めて、カードを使い分ける必要がある。だから、長期的な影響力を構築するなら、専門性を磨き、人として尊敬される存在になることが最も確実な方法だ。専門性と人望は、どの組織でも比較的通用しやすい。「政治力を磨け」と言われると抵抗がある人も、「専門性を磨け」なら抵抗がないだろう。実は、専門性を磨くことは、組織における影響力を高める最も正攻法なアプローチなのだ。ここで、私の経験を1つ書いておく。ある領域で、私はチームの中で一番詳しくなった。別に政治をしたわけではない。ただ、その領域を深掘りし続けた。ドキュメントを読み、実験し、知見を共有した。すると、向こうから相談が来るようになった。「〇〇のことは△△さんに聞けばいい」という評判が立った。会議で発言すると、その領域については私の意見が尊重されるようになった。これは「政治」ではない。専門性による影響力だ。ただし、専門性を万能視するのは危険だ。限界もある。具体的に言おう。専門性が効くのは「その領域の意思決定」に限られる。組織全体の方向性、予算配分、人事——こういった領域横断的な意思決定では、専門性だけでは戦えない。私も経験がある。技術的な判断では尊重されるようになったが、プロジェクトの優先順位を決める会議では、相変わらず発言力がなかった。専門性は「深さ」を与えるが、「広さ」は別の力学で決まる。それでも、専門性があれば、政治力が弱くても、ある程度は戦える。少なくとも、自分の専門領域では発言権が得られる。そこを足がかりにして、徐々に影響力を広げていくことができる。だから、「政治が苦手だ」という人に言いたい。まず専門性を磨け。それが最も確実な道だ。政治力は、専門性という土台の上に乗せるオプションとして考えればいい。土台がないまま政治力だけ磨いても、長続きしない。組織と踊るための心構え最後に、心構えの話をする。おい、頑張るなら組織と踊れ。これは「組織に従属しろ」という意味ではない。ダンスは、相手の動きを感じながら、自分も動く。一方的にリードするわけでも、一方的にフォローするわけでもない。相手と自分の動きが調和して、初めてダンスになる。組織も同じだ。組織の力学を無視して突っ走ると、壁にぶつかる。かといって、組織に完全に従属すると、自分の意志がなくなる。組織の力学を理解し、その中で自分の目標を追求する。組織を動かしながら、自分も動く。これが「組織と踊る」ということだ。組織を敵視するな。かといって、盲従するな。組織は、自分の目標を達成するためのプラットフォームだ。うまく使えば、一人ではできないことができる。敵視していたら、使いこなせない。短期の勝ち負けにこだわるな。組織での影響力は、長期的に築くものだ。一回の会議で勝った負けたは、大した問題ではない。信頼の蓄積、専門性の蓄積、関係性の蓄積。これらが時間をかけて積み上がったとき、本当の影響力が生まれる。自分の価値観を失うな。組織の力学を理解し、活用することと、自分の価値観を捨てることは違う。「この方法は使えるけど、自分はやりたくない」と思うなら、やらなくていい。別の方法を探せばいい。「媚びない」ことと「無礼」であることは全く違う。前者は信念を持つことであり、後者は単なる社会性の欠如だ。同様に、「したたか」であることと「ずる賢い」ことも違う。前者は双方の利益を最大化する戦略的思考であり、後者は単なる利己主義だ。私は今でも、根回しに抵抗がある。でも、必要な場面ではやる。やりながら、「これでいいのか」と自問する。割り切れないまま、やっている。組織と踊るというのは、自分を殺すことではない。自分を活かしながら、組織の中で成果を出す方法を見つけることだ。その方法は、人によって違う。自分なりの踊り方を見つければいい。届かない人へここまで書いてきて、立ち止まる。「組織の力学を理解しろ」「影響力を磨け」「組織と踊れ」——私はそう書いた。でも、この記事には前提条件がある。この記事が有効なのは、以下の条件が揃っている場合だ。組織がまともである——努力が報われる余地がある自分にエネルギーがある——行動を起こす余力がある組織で働くことを選んでいる——別の選択肢を選んでいないこの前提が成り立たない場合、この記事は役に立たない。それぞれ見ていく。組織が合わない人がいるそもそも、組織で働くことが向いていない人がいる。組織の力学を理解しろと言われても、理解する気力がない。人間関係を築けと言われても、それ自体がストレスだ。会議で発言しろと言われても、声が出ない。彼らは「能力がない」のではない。組織という形態が合わないのだ。フリーランス、起業、小規模チーム、リモートワーク——組織以外の働き方もある。そちらが合う人もいる。「おい、頑張るなら組織と踊れ」は、組織で働くことを前提としている。その前提自体が合わない人には、この記事は届かない。力学を理解しても動けない人がいる組織の力学を理解した。影響力を磨く方法も分かった。でも、動けない。すでに消耗している人。根回しをする気力がない人。人間関係を築くエネルギーがない人。彼らに「影響力を磨け」と言っても、無理だ。まず休む必要がある。構造的に無理な組織もあるどんなに力学を理解しても、どんなに影響力を磨いても、無理な組織もある。腐敗した評価制度。声の大きい人だけが勝つ文化。変える気のない経営層。そういう組織では、個人の努力で変えられることに限界がある。「組織と踊れ」と言っても、相手がダンスをする気がないなら、成立しない。この記事は、「組織がまともで、自分にエネルギーがある」ことを前提にしている。その前提が成り立たないなら、この記事は役に立たない。「踊らない」という選択肢もある「組織と踊る」ことを選ばない、という選択肢もある。専門性だけで勝負する。政治には一切関わらない。評価されなくても気にしない。自分のペースで、自分のやり方で働く。それは「負け」ではない。評価ゲームから意識的に降りるという戦略だ。「おい、辞めないなら頑張れ」で書いたことを繰り返す。頑張れないなら、頑張らなくていい。降りてもいい。休んでもいい。それも、1つの選択だ。おわりに「おい、辞めるな」で辞めないことを選んだ。「おい、辞めないなら頑張れ」で頑張り方を学んだ。そして今回、「おい、頑張るなら組織と踊れ」で組織の力学を学んだ。正直に言う。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分は、これを知りたかった。組織の力学を理解せず、「政治は汚い」と嫌悪しながら、壁にぶつかり続けていた。正義のエンジニアという幻想に囚われて、媚びないことと無礼を混同していた。その時間は、もったいなかった。組織の力学を理解しろ。でも、専門性と人望が最強のカードだ。政治に長けていても、実力がなければ長続きしない。実力があっても、組織の力学を無視していたら成果につながらない。両方必要だ。でも、長期的に見れば、専門性と人望が最も確実な道だ。譲れないもののために、譲るものを決めろ。したたかに生きろ。組織を敵視するな。盲従するな。組織と踊れ。——と書いて、自分でも苦い顔をしている。「お前も結局、体制に飲み込まれたのか」——かつての私なら、今の私をそう批判しただろう。しかし、それでいいのだ。技術的な純粋さを追求することと、社会的な成熟を遂げることは矛盾しない。むしろ、両方を兼ね備えてこそ、プロの仕事と言えるのではないだろうか。媚びないことと無礼の区別がつかなかった、頭の悪い反抗期は終わった。正直に言えば、私はまだ上手に踊れていない。根回しに抵抗がある。状況認識力が弱い。会議で空気を読めない。それでも、以前よりはマシになった。壁にぶつかる回数は減った。この記事が、かつての私のような人に届けばいいと思う。「政治は汚い」と思いながら、壁にぶつかり続けている人。組織の力学を理解することに抵抗がある人。「正義のエンジニア」という幻想に囚われている人。理解することと、加担することは違う。理解した上で、どう振る舞うかは自分で決められる。おい、頑張るなら組織と踊れ。踊れないなら、休め。踊り方は、自分で決めろ。——と、ここまで書いてきた。でも、最後に付け加えておく。組織が合わないなら、別の場所を探せばいい。それも、1つの選択だ。私も、まだ上手に踊れていない。それでも、やっている。それでいいのだと思う。かつての私のような若いエンジニアを見かけたら、優しく、でもはっきりと伝えたいと思う。「君の気持ちはよく分かる。でも、もっといい方法があるよ。一緒にしたたかにやっていこうぜ」と。多分昔の私だったら「は？日和って迎合した負け犬が何言ってんの？」とか思って、心の中で見下しながら表面上は「はい、参考にします」って適当に流すんでしょうね。まあ、それでいいんです。私も通った道だから。痛い目に遭うまで、人は変われない。私もそうだった。その時になって初めて、この言葉の意味が分かるはずです。けど大人として言う義務があるので言っておきました。参考書籍人を動かす　改訂文庫版作者:Ｄ・カーネギー創元社AmazonDD(どっちもどっち)論 「解決できない問題」には理由がある (WPB eBooks)作者:橘玲集英社Amazonその仕事、全部やめてみよう――１％の本質をつかむ「シンプルな考え方」作者:小野 和俊ダイヤモンド社Amazonアーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonこれからの「正義」の話をしよう ──いまを生き延びるための哲学 (ハヤカワ・ノンフィクション文庫)作者:マイケル・サンデル早川書房AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazonスタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazon組織が変わる――行き詰まりから一歩抜け出す対話の方法2 on 2作者:宇田川 元一ダイヤモンド社Amazonモンク思考―自分に集中する技術作者:ジェイ・シェティ東洋経済新報社AmazonSOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社Amazon多様性の科学作者:マシュー・サイドディスカヴァー・トゥエンティワンAmazon［新版］組織行動の考え方―個人と組織と社会に元気を届ける実践知作者:金井 壽宏,高橋 潔,服部 泰宏東洋経済新報社Amazonソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社AmazonTHE CULTURE CODE 最強チームをつくる方法作者:ダニエル・コイル,楠木建かんき出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[プログラミングが好きな人こそ今の時代、プログラマーになる方がいいと思う。- 「プログラミングが好きな人は、もうIT業界に来るな。」を読んで]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</guid>
            <pubDate>Sun, 18 Jan 2026 03:31:51 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。待っている間、Xを開いた。流れてきたタイトルに、手が止まった。「プログラミングが好きな人は、もうIT業界に来るな。」note.comリサーチは終わっていた。結果を確認しないまま、記事を読んでいた。小学生の頃から黒い画面に向かい続けてきたエンジニアが、生成AIの登場によって「自分の手で作る喜び」を奪われつつあると語っていた。「心の中で何かが音を立てて崩れる」という表現があった。共感したのか、と聞かれると困る。共感しなかったのか、と聞かれても困る。たぶん、どちらでもある。読み終えて、エディタに戻った。さっきまで何をしていたか、思い出せなかった。反論したいわけではなかった。ただ、何かが引っかかっていた。「プログラミングが好き」という言葉だ。この人の「好き」と、私の「好き」は、同じものを指しているのだろうか。コーヒーを淹れた。飲みながら、書くことにした。「好き」の中身を分解する元記事の筆者と私の違いは、能力でも経験でもない。「プログラミングが好き」という言葉が指す範囲が違うのだ。言葉は同じでも、中身が違う。「好き」と言ったとき、何を思い浮かべているか。キーボードを叩く指先か、頭の中で組み上がる構造か、動いた瞬間の達成感か。同じ「好き」でも、その中身は人によって違う。中身が違えば、奪われるものも違う。プログラミングという行為には、少なくとも3つのレイヤーがある。——もちろん、これは私の視点からの便宜的な分類だ。元記事の筆者にとっては、まったく別の分け方があるかもしれない。「書くこと」と「考えること」は分離できない、という人もいるだろう。書きながら考える。考えながら書く。その不可分さこそがプログラミングだ、と。それでも、ここでは一旦この枠組みで話を進める。身体感覚：キーボードを叩く感触、コードを書く行為そのもの、画面に流れる快感知的作業：問題を分解し、設計し、実装する思考プロセス創造行為：何かを作り、動かし、世に出す達成感元記事の筆者が愛していたのは、単なる身体感覚ではないように思う。「自分の指先から生まれるロジックが、動かなかったものを動かす」——その創造の主体性だ。自分の手で書いたコードが動く。その実感が奪われたから「楽園が消えた」と感じている。AIが書いたコードをチェックする「検品係」では、創造の主体は自分ではない。これは「間違った好き」ではない。彼の文脈では、それが正解だった。小学生の頃から黒い画面に向かい続け、自分の手でコードを書くことで世界を動かしてきた。その積み重ねの上に立っている。私が「設計が好き」と言えるのも、私の文脈があるからだ。どちらが正しいという話ではない。私が好きだったのは、2だった。課題があったら適切なサイズに分割して、「この問題はこう実装すれば解決するな」と構造が頭の中で閃く。そこからコードを書き続けていく。その一連のプロセスが好きだった。AIがコードを出力するようになって初めて、自分の「好き」が何だったのか見えた。コードを書けなくなったとき、喪失感を感じたか？ 感じなかった。むしろ「これで設計に集中できる」と思った。そのとき気づいた。自分が好きだったのは、タイピングではなく、考えて、作って、また考える、その繰り返しだった。では、何が奪われ、何が残ったのか整理しよう。奪われたもの：タイピングの時間、実装の試行錯誤、ググって解決策を探す時間。残ったもの：設計を考える時間、AIの出力を評価する時間、「もっと良い方法はないか」と考える時間。私にとって、後者こそがプログラミングの核心だった。もっと言えば、コードを書く時間に追われて後回しにしていた「設計」や「トレードオフの検討」——いわゆるソフトウェアエンジニアリングの領域に、ようやく時間を使える。だから、「奪われていない」と言い切れる。実践ソフトウェアエンジニアリング (第9版)作者:ロジャー・プレスマン,ブルース・マキシムオーム社AmazonGoogleのソフトウェアエンジニアリング ―持続可能なプログラミングを支える技術、文化、プロセス作者:Titus Winters,Tom Manshreck,Hyrum WrightオライリージャパンAmazon正直に言う、めちゃくちゃ楽しい今、めちゃくちゃ楽しい。作りたいものがある。指示を出す。コードが出てくる。レビューする。直す。動く。——以前なら「面倒だな」と後回しにしていたアイデアや知識が、数分で形になる。さっき書いた「ソフトウェアエンジニアリングの領域」——設計、トレードオフ、アーキテクチャ。そこにやっと向き合えている感覚がある。楽しい。素直に、楽しい。これを「検品係」と呼ぶなら、私は世界一楽しい検品係だ。コードレビューが好きで良かった思えば、私はコードレビューが比較的好きだった。他人のコードを読んで、「この人はこう考えてるんだろうな」というのが見えると嬉しい。なぜこの設計にしたのか、どこで迷ったのか。コードの向こうに思考が透けて見える瞬間が好きだった。知らない言語機能や、思いつきもしなかった構造で課題を解決しているのを見ると、良し悪しにかかわらず楽しい。正解かどうかより、発見の方が全然面白い。正解とは常に文脈の中にしかない。チームの習熟度、プロダクトのフェーズ、パフォーマンス要件、保守する人間の数。同じ課題でも、文脈が変われば最適解は変わる。だから「このコードは正しいか」という問いより、「この文脈でなぜこう書いたのか」という問いの方が面白い。そして、その問いに答えようとする過程で、自分の中の「正解」も揺らぐ。揺らぐことが学びだ。AIが出力したコードをレビューしていると、これが想像以上に勉強になる。先日、AIに「CSVパーサーを書いて」と頼んだ。返ってきたコードを見て驚いた。私なら正規表現でゴリ押しするところを、状態機械で書いている。エスケープ処理も完璧だ。「なるほど、このアプローチがあったか」と笑った。逆に、「いや、これは現場では使えない」と思う瞬間もある。過剰に抽象化されていたり、エラーハンドリングが甘かったり。その判断力こそ、レビューを通じて研ぎ澄まされる。——もっとも、私の判断が正しいとは限らない。AIの提案を「使えない」と却下した翌週、まったく同じアプローチを別の記事で「ベストプラクティス」として紹介されているのを見たこともある。ただ、これは15年やってきた人間だから言えることだ。状態機械の良さが分かるのは、正規表現ゴリ押しで痛い目を見たことがあるからだ。「なるほど、このアプローチがあったか」と唸れるのは、比較対象を持っているからだ。経験ゼロの人がAIの出力から体系的に学べるかは、正直分からない。むしろ、学べない可能性の方が高い気がする。コードレビューが苦手な人には、AIとの協働は苦行かもしれない。でも、レビューが好きな人間にとっては、無限に相手がいるジムのようなものだ。疲れない、休まない、いつでも付き合ってくれる相手。しかも、毎回違うアプローチを見せてくれる。「検品」と「協働」の違い元記事は「検品係になった」と嘆いている。創造の主体でありたかった人にとって、この表現は正確だと思う。自分が書きたかったコードを他者（AI）が書き、自分はそれをチェックするだけ。主体と客体が入れ替わっている。ただ、私の場合は少し違った。携帯電話は私のことをめちゃくちゃ記憶している。連絡先、スケジュール、位置情報、検索履歴。私より私のことを知っているかもしれない。でも、携帯電話を使っているとき、「主体を奪われた」とは感じない。道具として使っている感覚がある。生成AIは違う。コードを書く、文章を書く、設計を考える——これまで「私がやること」だった領域に、AIが入り込んでくる。携帯電話が記憶を代替しても主体性は揺らがなかったが、生成AIは創造を代替しようとする。だから主体性が脅かされる感覚が生まれる。それでも、私は自分が主体だと思っている。なぜか。www.youtube.com答えは、関わり方にある。「検品」と「協働」の違いは何か。検品は受動的だ。ラインを流れてくる製品をチェックし、不良品を弾く。渡されたものをチェックするだけ。協働は能動的だ。方向性を示し、フィードバックを与え、成果物を一緒に作り上げる。私がAIとやっているのは後者だ。具体的に言うと——「こういう設計で書いて」と指示を出す（方向性）出てきたコードを見て「ここはこう直して」とフィードバックする「いや、アプローチ自体を変えよう」と軌道修正する最終的な成果物が完成するこのやりとりは、人間同士のペアプログラミングと構造的に同じだ。相手が人間かAIかの違いしかない。検品係は受け身だが、私は能動的にAIを導いている。方向を決め、判断を下し、軌道修正をかける。この能動性が、主体性を保つ鍵だ。AIとの関係を一言で表すなら、「相棒」ではなく「優秀だが判断できない後輩」が近い。指示を明確にすれば良い仕事をする。曖昧にすると、意図しないものが返ってくる。筆者が言うように、書く時間は減った。でも、考える時間は増えた。どう分割するか。どう設計するか。AIが出してきたコードのどこを採用し、どこを直すか。そして、仮にこれが「検品」だったとしても、私はその過程でかなり学んでいる。「こんな書き方があるのか」と何度も唸った。特に経験の浅い言語では顕著だ。自分で書いていたら絶対に思いつかないイディオムを、AIは平気で出してくる。検品のつもりが、いつの間にか授業を受けている。ただし、ここには落とし穴がある。AIが出したコードをそのまま使って「動いた、終わり」で済ませると、何も残らない。効率は上がる。成果も出る。でも、1週間後に「なぜこう書いたの？」と聞かれても、答えられない。因果を辿れない。自分が責任を持って出力したコードのはずなのに、説明しようとすると言葉が出てこない。以前、「AIエージェントと協働しながら学習する方法」という記事で詳しく書いたが、学びには「摩擦」が必要だ。エラーが出る。原因がわからない。仮説を立てる。試す。失敗する。また試す。この摩擦の中で、経験が意味に変わる。学習とは、経験を意味に変換する行為だ。AIが摩擦を消してくれると、経験が意味に変わる機会も消える。syu-m-5151.hatenablog.comだから私は、AIが出したコードを「なぜこう書いたのか」と考える時間を意図的に作っている。効率だけを求めるなら不要な時間だ。でも、この「不効率な時間」が学びを生む。摩擦は削減対象ではない。設計対象だ。何を学び、何を省略するか。その選択を自分でしている限り、主体は私だ。これを「検品」と呼ぶか「協働」と呼ぶかは、本人の姿勢次第なのかもしれない。——と書いて、自分で読み返して思った。「姿勢次第」では何も言っていないのと同じだ。具体的に、検品を協働に変えるための3つのポイントを挙げてみる。意図を言語化する: 「こう書いて」ではなく「この問題を解決したい。制約はこれ」と伝える。AIに考えさせる余地を残す。出力から学ぶ: AIが出したコードを「動くかどうか」だけでなく、「なぜこう書いたか」を考える。知らないパターンがあれば調べる。フィードバックを重ねる: 一発で完璧を求めない。「ここを直して」「いや、やっぱりこっち」のやりとりを楽しむ。この3つができれば、検品は協働になる。逆に言えば、「動くか確認するだけ」なら、それは検品だ。この3つを実践するかどうか。それが検品と協働を分け、主体性を保てるかどうかを分ける。——と偉そうに書いたが、これが「正解」かどうかは分からない。私の文脈ではうまくいっている。でも、別の文脈では別の答えがあるはずだ。締め切りに追われているときは「動けばいい」になるし、疲れているときは「なぜこう書いたか」なんて考えない。理想と現実は違う。ただ、「こうありたい」という指針があるのとないのとでは、違うと思っている。それすらも、私の文脈での話だ。ただし、これは私のケースだここまで書いてきて、一つ断っておきたいことがある。これは私の話だ。コードレビューが好きで、設計を考えるのが好きで、タイピング速度に自信がなかった人間の話だ。もし元記事の筆者のように、「自分の手で書いたコードが動く」その実感こそが喜びだったなら——この記事は何の慰めにもならないだろう。創造の主体でありたかった人に、「検品も楽しいよ」とは言えない。その人たちに「考え方を変えろ」と言うつもりはない。「自分が書きたかった小説をAIに書かせ、誤字脱字を直す校正者のような気分」——元記事のこの表現は、痛いほど分かる。創造の主体性を奪われた感覚は、姿勢や考え方でどうにかなるものではない。彼の文脈では、それが真実だ。私が「楽しい」と言えるのは、私の文脈がたまたまそうだったからに過ぎない。「でも、結局プログラマーの仕事は減るのでは？」という反論もあるだろう。正直、分からない。AIの進化は私の想像を超えている。5年後にどうなっているか、予測する自信がない。そして、ここは誤魔化さずに言っておくべきだと思う。「楽しい人がいること」と「職業として持続可能かどうか」は、まったく別の話だ。ドライバーが100人必要だった時代から、10人で済む時代へ。私が楽しくても、市場が縮小すれば、その楽しさを職業にできる人は減る。元記事の筆者が問うているのは、たぶんそっちの話でもある。この問いについて、エンジニアに許された特別な時間の終わりというスライドがある。エンジニアがドライバー席から助手席へ移る時代が来ている、という話だ。AIが「副操縦士（Copilot）」から「操縦士（Pilot）」へ進化しつつある。続編のたかが特別な時間の終わりでは、9ヶ月後にその予測が現実化しつつあると報告されている。私がこの記事で書いてきた「協働」も、結局は助手席からの関わり方なのかもしれない。ドライバー席に座っていた時代は終わりつつある。それでも、助手席には助手席の仕事がある。呑気なドライブデートを思い浮かべたかもしれないが、全然様相は違う。ラリーのコ・ドライバーは、ただ座っているだけではない。本番前にコースを試走し、コーナーの角度、直線の距離、路面の状態、危険なポイントをすべてペースノートに書き込む。本番では猛スピードで揺さぶられる車内で、そのノートを絶妙なタイミングで読み上げる。「左3、50m、右2、クレスト注意」。ドライバーは全コースを暗記できない。コ・ドライバーなしでは走れない。AIとの協働も似ている。事前にコードベースを把握し、設計を考え、制約を整理する——これがペースノートの作成だ。本番では、AIが猛スピードでコードを生成する中、「次は右だ」「ここは危険だ」と指示を出し続ける。曖昧な指示を出せば、車は崖から落ちる。ただし、その車は時速200kmで走っている。のんびり景色を眺める余裕はない。コ・ドライバーとして生きる。それがこの時代の選択だ。——と書いたが、この比喩にも限界がある。ラリーではコ・ドライバーの仕事がなくなることはない。しかし、AIとの協働では、その保証がない。今は「設計」「評価」「判断」が人間の仕事として残っている。だが、AIが設計し、AIが実装し、AIがレビューする世界が来たら？ コ・ドライバーの席さえ、自動運転に置き換わるかもしれない。この記事は「設計や評価は人間に残る」という前提で書いている。その前提が崩れたら、私の話は無効になる。ただ、「だから今やっても意味がない」とは思わない。今この瞬間、プログラミングが楽しいなら、それでいい。未来のことは、未来の自分が考える。——もっとも、この「楽しい」を大声で言うのは少し気が引ける。誰かが失った「楽しさ」の上に、私の「楽しさ」が成り立っているかもしれないからだ。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われたら、反論できない。おわりに書き終えて、コーヒーを淹れ直した。冷めていた。あの日から何日か経った。書いている間、ずっと考えていた。私は本当に「楽しい」のか。それとも、楽しいと思いたいだけなのか。正直、分からない。分からないまま書いた。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われるかもしれない。反論できる気がしない。私の「好き」と、あの人の「好き」は、たまたま違った。彼の文脈では彼が正しく、私の文脈では私が正しい。それだけのことだ。どちらかが間違っているわけではない。明日もたぶん、AIにコードを書かせる。レビューする。直す。動かす。それを「楽しい」と感じるかどうかは、そのときになってみないと分からない。ただ、少しだけ違うことがある。「プログラミングが好き」という言葉を使うとき、自分が何を指しているのか、前より意識するようになった。摩擦は削減対象ではない。設計対象だ。——この言葉を、自分に言い聞かせるようになった。そして、自分の「正解」も揺らぐことを知った。書く前と書いた後で、考えが変わっている。元記事の筆者の気持ちが、前より分かる気がする。揺らぐことが学びだと書いた。この記事を書くこと自体が、そうだった。「IT業界に来るな」と言われた君へ。私は「来い」とは言わない。言えない。生成AIがいつか道具になる日までは。ただ、私は来てよかった。少なくとも、今日は。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。技術が人類を幸せにするかみたいな問いは常に面白いです。技術革新と不平等の1000年史　上作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon技術革新と不平等の1000年史　下作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【Vul】CodeBuildの設定ミスによるフィルタバイパス]]></title>
            <link>https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AWS CodeBuildにおける設定ミスにより、AWS提供のGitHubリポジトリがサプライチェーン攻撃のリスクにあった件]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[不動点コンビネータと無名再帰]]></title>
            <link>https://silasol.la/posts/2026-01-16-01_least_fixed_point/</link>
            <guid isPermaLink="false">https://silasol.la/posts/2026-01-16-01_least_fixed_point/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[不動点コンビネータと実践的な示唆について紹介します．]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Development Kit (ADK)における評価駆動型開発（EDD）]]></title>
            <link>https://sreake.com/blog/evaluation-driven-development-with-adk/</link>
            <guid isPermaLink="false">https://sreake.com/blog/evaluation-driven-development-with-adk/</guid>
            <pubDate>Thu, 15 Jan 2026 04:42:40 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部の井上 秀一です。私はSreake事業部にて、SREや生成AIに関するResearch & Developmentを行っています。 Agent Developmen […]The post Agent Development Kit (ADK)における評価駆動型開発（EDD） first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ory Kratosで認証を委譲する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</guid>
            <pubDate>Wed, 14 Jan 2026 05:02:48 GMT</pubDate>
            <content:encoded><![CDATA[前回からの続き前回の記事では、Playwright MCPを使ったE2Eテストで5つのバグを発見した。CORS設定の欠如、JWTトークンの切り詰め、Hydraトークンとの不一致、ミドルウェアの適用漏れ、X-Tenant-Slugヘッダーの欠如。RBACの検証とOWASP Top 10との比較まで行い、マルチテナント認証システムが一通り動くようになった。前提知識: この記事はOry Hydraシリーズの続編です。OAuth2認可コードフローの基礎知識と、Login/Consent Providerの役割を理解している前提で進めます。前回記事はこちら。動く。ちゃんと動く。でも、レビューコメントが気になった。「パスワードリセット機能は？」「MFA対応の予定は？」「メール確認フローは？」全部、自分で実装しなければならない。Argon2idでパスワードをハッシュ化するコードは書いた。ログイン認証は動く。でも、パスワードを忘れたユーザーへのリセットメール送信、そのトークン管理、有効期限の検証。TOTPによる二要素認証。メールアドレス確認のフロー。これ全部、自分で実装するのか？RFCを読んでいたあの3日間を思い出した。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。同じ結論に至った。今度は認証機能についてだ。Ory Kratosという選択肢www.ory.shgithub.comOry Kratosは「ヘッドレスID管理システム」だ。Hydraが「認証をしない認可サーバー」だったことを思い出してほしい。Hydraはプロトコル層（OAuth2/OIDC）に特化し、認証は私たちに任せた。Kratosはその「任された認証」を担当する。┌─────────────────────────────────────────────────────────────┐│                     Ory Stack                               │├────────────────────────┬────────────────────────────────────┤│      Ory Kratos        │           Ory Hydra                ││   (Identity Provider)  │      (Authorization Server)        │├────────────────────────┼────────────────────────────────────┤│ - ユーザー登録         │ - OAuth2/OIDC                      ││ - ログイン認証         │ - トークン発行                     ││ - MFA (TOTP, WebAuthn) │ - クライアント管理                 ││ - パスワードリセット   │ - Consent管理                      ││ - プロフィール管理     │ - セッション管理                   ││ - メール確認           │                                    │└────────────────────────┴────────────────────────────────────┘つまり、これまでに私がRustで書いたAuthService——パスワード検証、ユーザー登録、セッション管理——これらをKratosに任せられる。アーキテクチャの変化これまでの構成を振り返る。【01-03の構成】┌─────────────┐     ┌─────────────────────┐     ┌─────────────┐│   Browser   │────▶│ Rust Login Provider │────▶│  Ory Hydra  ││             │     │ (自前実装)           │     │             │└─────────────┘     └─────────────────────┘     └─────────────┘                              │                              ▼                    ┌─────────────────────┐                    │ PostgreSQL (users)  │                    └─────────────────────┘私が書いたRust Login Providerは認証を担当していた。ユーザーテーブルも自前で管理していた。Kratosを導入すると、こうなる。【Kratos導入後の構成】┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐│   Browser   │────▶│  Kratos UI  │────▶│  Ory Kratos │────▶│  Ory Hydra  ││             │     │  (Node.js)  │     │             │     │             │└─────────────┘     └─────────────┘     └──────┬──────┘     └─────────────┘                                               │                                               ▼                                      ┌─────────────────────┐                                      │ PostgreSQL          │                                      │ (identities)        │                                      └─────────────────────┘私が書くコードは、ほぼゼロになる。パスワード検証、ユーザー登録、セッション管理——これまでに私がRustで実装した機能は、全てKratosが提供する。私が書くのはKratosの設定ファイルと、必要に応じたUIのカスタマイズだけだ。「それって、学習した意味がないのでは？」いや、逆だ。認証システムを自前で実装した経験は、Kratosの設定を理解する上で役立った。例えば、Kratosの設定にhashers.argon2.memory: 128MBという項目がある。自前実装の経験がなければ、その意味を理解できなかっただろう。メモリコストを上げればセキュリティは向上する。しかし同時接続数の増加でOOMのリスクも上がる——この判断ができるのは、OWASPのドキュメントを読み、自分でパラメータを選んだ経験があるからだ。「ドキュメントを読めば同じでは？」——そう思うかもしれない。確かに、ドキュメントを読めば設定はできる。しかし、障害時に「この設定が原因かもしれない」と仮説を立てられるのは、自分で同じ問題に苦しんだ経験があるからだ。ログを見て「これはセッション固定化攻撃への対策が発動した」と判断できるか。エラーメッセージから「Identity Schemaの定義が間違っている」と気づけるか。これは学習効率の問題ではなく、デバッグ能力の問題だ。これまでの実装で、認証システムの複雑さを体験した。Argon2idのパラメータ設定、ユーザー列挙攻撃への対策、セッション管理の罠。58個のテストを書いて「できないこと」を確認した。だからこそ、Kratosのありがたみが分かる。そして、Kratosで問題が起きたときに対処できる。全員が自前実装を経験すべきとは言わない。しかし、チームに1人は「中身を理解している人」がいた方がいい。Docker Composeで動かすwww.ory.com実際に動かしてみよう。services:  postgres:    image: postgres:16-alpine    environment:      POSTGRES_USER: postgres      POSTGRES_PASSWORD: secret      POSTGRES_DB: postgres    volumes:      - postgres_data:/var/lib/postgresql/data      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro  # 後述の初期化スクリプト    healthcheck:      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]      interval: 5s      timeout: 5s      retries: 5    networks:      - ory  kratos-migrate:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable    volumes:      - ./kratos:/etc/config/kratos:ro    command: migrate sql -e --yes --config /etc/config/kratos/kratos.yml    depends_on:      postgres:        condition: service_healthy    networks:      - ory  kratos:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable      LOG_LEVEL: debug      SERVE_PUBLIC_BASE_URL: http://localhost:4433      SERVE_ADMIN_BASE_URL: http://localhost:4434    volumes:      - ./kratos:/etc/config/kratos:ro    command: serve all --dev --config /etc/config/kratos/kratos.yml    ports:      - "4433:4433"  # Public API      - "4434:4434"  # Admin API    depends_on:      kratos-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4433/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  hydra-migrate:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable    command: migrate sql -e --yes    depends_on:      postgres:        condition: service_healthy    networks:      - ory  hydra:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable      SECRETS_SYSTEM: super-secret-system-secret-at-least-32-chars      URLS_SELF_ISSUER: http://localhost:4444      URLS_CONSENT: http://localhost:4455/consent      URLS_LOGIN: http://localhost:4455/login      URLS_LOGOUT: http://localhost:4455/logout      LOG_LEVEL: debug    command: serve all --dev    ports:      - "4444:4444"  # Public API      - "4445:4445"  # Admin API    depends_on:      hydra-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4444/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  kratos-ui:    image: oryd/kratos-selfservice-ui-node:v1.3.1    environment:      PORT: 4455      KRATOS_PUBLIC_URL: http://kratos:4433      KRATOS_BROWSER_URL: http://localhost:4433      HYDRA_ADMIN_URL: http://hydra:4445      COOKIE_SECRET: super-secret-cookie-secret-32chars      CSRF_COOKIE_NAME: ory_csrf_ui      CSRF_COOKIE_SECRET: super-secret-csrf-secret-32-chars    ports:      - "4455:4455"    depends_on:      kratos:        condition: service_healthy      hydra:        condition: service_healthy    networks:      - oryvolumes:  postgres_data:networks:  ory:注意: 上記の設定は開発環境用です。本番環境ではSECRETS_SYSTEMやCOOKIE_SECRETに32文字以上の暗号学的に安全な値を設定してください。サービスが6つある。PostgreSQL、KratosとHydraそれぞれのmigrate/serveサービス、そしてKratos UI。以前の自前実装（auth-provider）はKratosに置き換わった。ポイントはkratos-uiだ。これはOry公式が提供するセルフサービスUI。ログイン画面、登録画面、パスワードリセット画面などが含まれている。「自分でUI書かなくていいの？」開発環境ではこれで十分だ。本番環境では、このUIを参考に自前のUIを実装できる。Kratosの「ヘッドレス」設計により、UIは完全に切り離されている。github.comKratos設定ファイルの解説Kratosの設定ファイルkratos.ymlを見てみよう。version: v1.3.1dsn: memoryserve:  public:    base_url: http://localhost:4433/    cors:      enabled: true      allowed_origins:        - http://localhost:4455  admin:    base_url: http://localhost:4434/selfservice:  default_browser_return_url: http://localhost:4455/  allowed_return_urls:    - http://localhost:4455    - http://localhost:4444  methods:    password:      enabled: true    totp:      enabled: true      config:        issuer: OryKratosVerification    lookup_secret:      enabled: true    link:      enabled: true    code:      enabled: true  flows:    error:      ui_url: http://localhost:4455/error    settings:      ui_url: http://localhost:4455/settings      privileged_session_max_age: 15m    recovery:      enabled: true      ui_url: http://localhost:4455/recovery      use: code    verification:      enabled: true      ui_url: http://localhost:4455/verification      use: code      after:        default_browser_return_url: http://localhost:4455/    logout:      after:        default_browser_return_url: http://localhost:4455/login    login:      ui_url: http://localhost:4455/login      lifespan: 10m    registration:      lifespan: 10m      ui_url: http://localhost:4455/registration      after:        password:          hooks:            - hook: sessionlog:  level: debug  format: text  leak_sensitive_values: truesecrets:  cookie:    - super-secret-cookie-secret-32chars  cipher:    - super-secret-cipher-key-32-charsciphers:  algorithm: xchacha20-poly1305hashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16identity:  default_schema_id: default  schemas:    - id: default      url: file:///etc/config/kratos/identity.schema.jsoncourier:  smtp:    connection_uri: smtps://test:test@mailslurper:1025/?skip_ssl_verify=trueoauth2_provider:  url: http://hydra:4445セルフサービスフローselfservice:  methods:    password:      enabled: true    totp:      enabled: true以前、私がRustで実装したパスワード認証。Kratosではpassword: enabled: trueの一行で有効になる。TOTPも同様だ。以前は「MFA対応の予定は？」という質問に答えられなかった。Kratosなら設定1つで有効化できる。パスワードハッシュhashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16以前、私はArgon2::default()を使った。Kratosも同じArgon2を使っている。設定値を明示的に指定することで、チーム内で「なぜこのパラメータか」を共有できる。cheatsheetseries.owasp.orgHydra連携oauth2_provider:  url: http://hydra:4445これが最も重要な設定だ。KratosがHydraのAdmin APIに接続し、login_challengeを処理する。以前は私がRustでHydraClientを実装し、accept_loginを呼び出していた。Kratosはこれを自動で行う。https://www.ory.com/docs/kratos/self-hosted/hydra-integrationwww.ory.comIdentity Schemaの設計Kratosはユーザー情報を「Identity」として管理する。その構造はJSON Schemaで定義する。{  "$id": "https://schemas.ory.sh/presets/kratos/identity.email.schema.json",  "$schema": "http://json-schema.org/draft-07/schema#",  "title": "Person",  "type": "object",  "properties": {    "traits": {      "type": "object",      "properties": {        "email": {          "type": "string",          "format": "email",          "title": "E-Mail",          "ory.sh/kratos": {            "credentials": {              "password": {                "identifier": true              },              "totp": {                "account_name": true              }            },            "recovery": {              "via": "email"            },            "verification": {              "via": "email"            }          }        },        "name": {          "type": "object",          "properties": {            "first": {              "title": "First Name",              "type": "string"            },            "last": {              "title": "Last Name",              "type": "string"            }          }        }      },      "required": ["email"],      "additionalProperties": false    }  }}ory.sh/kratosという拡張プロパティが特徴的だ。credentials.password.identifier: true — このフィールドがログインIDになるrecovery.via: email — パスワードリセットはこのメールアドレスに送信されるverification.via: email — メール確認もこのアドレスに送信される以前、私はユーザーテーブルを自前で設計した。Kratosではスキーマを宣言的に定義するだけでいい。www.ory.com実際にハマったことでも、最初のdocker compose upは失敗した。データベースが存在しないFATAL: database "kratos" does not exist (SQLSTATE 3D000)KratosとHydraはそれぞれkratosとhydraという名前のデータベースを期待する。でも、PostgreSQLコンテナはpostgresデータベースしか作らない。解決策は初期化スクリプトだ。-- init.sqlCREATE DATABASE kratos;CREATE DATABASE hydra;# docker-compose.ymlpostgres:  volumes:    - ./init.sql:/docker-entrypoint-initdb.d/init.sql:roPostgreSQLは/docker-entrypoint-initdb.d/にあるSQLファイルを起動時に実行する。これで両方のデータベースが作成される。最初は「なぜ自動で作ってくれないんだ」と思った。おそらく、本番環境では既存のデータベースサーバーに接続することが多いからだろう。いずれにせよ、初期化スクリプトで解決できる。ポート競合Bind for 0.0.0.0:4444 failed: port is already allocated以前の記事で作ったory-hydra-rust環境がまだ動いていた。同じポート4444を使おうとして衝突。# 他の環境を停止cd ../ory-hydra-rust && docker compose down複数のOry環境を並行して動かす時は、ポートを変える必要がある。開発環境では素直に片方を停止した方がいい。Kratosが教えてくれた盲点E2EテストでTestPassword123!というパスワードを使おうとした。{  "id": 4000034,  "text": "The password has been found in data breaches and must no longer be used.",  "context": {    "breaches": 3330  }}KratosはデフォルトでHave I Been PwnedのAPIを使い、パスワードが過去のデータ漏洩に含まれていないかチェックする。TestPassword123!は3,330件の漏洩で見つかっていた。haveibeenpwned.comなぜ私は思いつかなかったのか。振り返ると、私の58個のテストは「攻撃者がシステムに対して行う操作」をテストしていた。間違ったパスワードでログインできないこと存在しないユーザーで情報が漏れないこと同時登録で競合状態が起きないことこれは全て「システムへの攻撃」に対するテストだ。攻撃者がシステムの外側から突破を試みるシナリオ。HIBPチェックは視点が異なる。「ユーザーが持ち込むリスク」に対処している。ユーザーが「password123」を使おうとするユーザーが他のサービスで使い回しているパスワードを登録するユーザーが過去に漏洩したパスワードを選ぶこれはシステムへの攻撃ではない。ユーザー自身がリスクを持ち込むシナリオだ。私はこのカテゴリを完全に見落としていた。なぜ見落としたのか。おそらく「ユーザーは正しく行動する」という暗黙の前提があった。パスワード強度のバリデーション（8文字以上、英数字混合など）を入れれば十分だと思っていた。でも、TestPassword123!は典型的な強度バリデーションを通過する。英大文字、英小文字、数字、記号、8文字以上。全ての条件を満たしている。にもかかわらず、3,330件の漏洩で見つかっている。強度バリデーションは「推測しやすいか」をチェックする。HIBPチェックは「既に漏洩しているか」をチェックする。両者は補完関係にある。Kratosを使うことで、私が想定していなかった脅威カテゴリまでカバーできる。これが「専門家が作ったツールを使う」ことの価値だ。自分の盲点を、他者の知見で補える。E2Eテストではタイムスタンプを含むランダムなパスワードを生成して回避した。TEST_PASSWORD="Kratos$(date +%s)E2E!Xk9#mN"本番環境では、この機能を有効にしたまま運用すべきだ。ユーザーに「このパスワードは漏洩しています」と伝えることで、アカウント乗っ取りのリスクを下げられる。環境の起動と動作確認初期化スクリプトを追加した状態で起動する。docker compose up -ddocker compose logs -fヘルスチェック用エンドポイントにアクセスしてみる。# Kratosのヘルスチェックcurl http://localhost:4433/health/ready# {"status":"ok"}# Hydraのヘルスチェックcurl http://localhost:4444/health/ready# {"status":"ok"}両方ともokが返ってきた。セルフサービスフローの確認ブラウザでhttp://localhost:4455/registrationにアクセスする。登録画面が表示される。メールアドレスとパスワードを入力して登録。次にhttp://localhost:4455/loginにアクセス。ログイン画面が表示される。先ほど登録した認証情報でログイン。ログイン成功。これだけだ。拍子抜けするほど簡単だった。以前、私は以下を実装した。AuthService::register() — ユーザー登録AuthService::authenticate() — パスワード検証login_page() — ログインフォームのHTMLlogin_submit() — フォーム送信処理58個のテストKratosでは、設定ファイルを書くだけでこれらが全て動く。OAuth2フローの確認OAuth2クライアントを作成する。docker compose exec hydra hydra create oauth2-client \  --endpoint http://localhost:4445 \  --grant-type authorization_code \  --response-type code \  --scope openid,profile,email \  --redirect-uri http://localhost:8080/callback \  --name "Test Client"クライアントIDとシークレットが出力される。ブラウザで認可エンドポイントにアクセスする。http://localhost:4444/oauth2/auth?client_id=<CLIENT_ID>&response_type=code&scope=openid+profile+email&redirect_uri=http://localhost:8080/callback&state=test-stateHydraがKratos UIにリダイレクトKratos UIがログイン画面を表示ログイン成功後、Kratosがlogin_challengeをHydraに送信HydraがConsent画面にリダイレクトConsent承認後、認可コードがコールバックURLに返される以前、私がRustで実装したlogin_submit()の処理を、Kratosが自動で行っている。// 前回の実装（不要になった）pub async fn login_submit(    State(state): State<AppState>,    Form(form): Form<LoginForm>,) -> Result<Redirect, AppError> {    let user = state.auth.authenticate(&form.email, &form.password).await?;    let completed = state.hydra        .accept_login(&form.login_challenge, &user.id.to_string(), false)        .await?;    Ok(Redirect::to(&completed.redirect_to))}このコードは、もう書く必要がない。E2Eテストで確認したこと実際にAPIを叩いて、フロー全体が動くことを確認した。Registration Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/registration/api"# Flow ID: 77ff9653-ccd2-4f91-aeea-8fbb4d67fce7# 2. 登録を実行curl -s -X POST "http://localhost:4433/self-service/registration?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "password": "Kratos1767517527E2E!Xk9#mN",    "traits": {      "email": "e2etest@example.com",      "name": { "first": "E2E", "last": "Test" }    }  }'Registration successful!Identity ID: 169e0834-4b45-441f-95f8-5adc45d8a3e9Email: e2etest-1767517527@example.comSession Token: ory_st_WugR5gisST7SO...Kratosのセルフサービスフローは2段階構成だ。まずフローを初期化してFlow IDを取得し、そのIDを使ってデータを送信する。これにより、CSRFトークンやフローの有効期限が管理される。Login Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/login/api"# 2. ログインを実行curl -s -X POST "http://localhost:4433/self-service/login?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "identifier": "e2etest@example.com",    "password": "Kratos1767517527E2E!Xk9#mN"  }'Login successful!Session ID: 8b97d548-8436-48ee-b4fd-8e1c643dac04Session Token: ory_st_ty15oU5JLIABh...Session Verificationcurl -s -X GET "http://localhost:4433/sessions/whoami" \  -H "Authorization: Bearer $SESSION_TOKEN"Session valid!Identity: e2etest-1767517527@example.comActive: trueセッショントークンを使って/sessions/whoamiを呼ぶと、現在のセッション情報が返ってくる。これは以前私がRustで実装したJwtService::verify()に相当する機能だ。OAuth2 Authorization Flow# OAuth2クライアントを作成curl -s -X POST "http://localhost:4445/admin/clients" \  -H "Content-Type: application/json" \  -d '{    "client_id": "e2e-test-client",    "client_secret": "e2e-test-secret",    "grant_types": ["authorization_code"],    "response_types": ["code"],    "scope": "openid profile email",    "redirect_uris": ["http://localhost:8080/callback"]  }'認可エンドポイントにアクセスすると、HydraがKratos UIにリダイレクトする。http://localhost:4444/oauth2/auth?client_id=e2e-test-client&...  ↓http://localhost:4455/login?login_challenge=Xv84rhGlXQQrVNL7SlICdNobNbYvcK7z8il...login_challengeパラメータが付与されている。Kratos UIはこのチャレンジを使ってHydraと連携し、認証完了後に適切なリダイレクトを行う。E2Eテスト結果サマリー テスト項目  結果  Registration Flow  成功  Login Flow  成功  Session Verification  成功  OAuth2 Client Setup  成功  OAuth2 Authorization Flow  成功（login_challenge生成確認） 全てのフローが期待通りに動作した。以前の自前実装と比較して、コード量はゼロになり、機能は増えた。自前実装との比較 観点  自前実装（02）  Kratos  パスワード認証  Argon2id実装  組み込み  MFA  未実装  TOTP, WebAuthn対応  パスワードリセット  未実装  フロー組み込み  メール確認  未実装  フロー組み込み  ソーシャルログイン  未実装  OIDC対応  漏洩パスワードチェック  未実装  HIBP連携  ログイン画面  HTML手書き  公式UI or 自前  セキュリティテスト  58個書いた  Oryが検証済み  学習コスト  Rust知識  Kratos設定  カスタマイズ性  完全自由  スキーマ/フック 特筆すべきは漏洩パスワードチェックだ。Have I Been Pwnedとの連携により、過去のデータ漏洩で流出したパスワードを拒否できる。これは以前書いた58個のテストでも考慮していなかった観点だ。Kratosを使うことで、私が思いつかなかったセキュリティ対策まで自動的に適用される。自前実装は無駄だったのか？いや、違う。以前の実装で学んだこと——Argon2idのパラメータ、ユーザー列挙攻撃への対策、タイミング攻撃の考慮——これらはKratosの設定を理解する上で役立った。「なぜこの設定があるのか」が分かるのは、自分で実装した経験があるからだ。いつKratosを使うべきかKratosを選ぶかどうかは、3つの軸で判断する。技術的要件: カスタマイズの複雑さはどの程度か。Kratosはフック機構やIdentity Schemaで柔軟性を提供するが、「3回目のログインでは必ずCAPTCHAを表示」のような独自フローは難しい。標準的な認証フローなら、Kratosで十分だ。組織的要件: チームにセキュリティ専門家がいるか。いないなら、Kratosに任せた方がいい。脆弱性対応、ベストプラクティスの追従——これらを自前でやるには専門性が必要だ。SOC2やISO27001の監査でも「専門企業の製品を使っています」と答えられる。ビジネス要件: 認証がコア価値か否か。パスワードマネージャーや認証SaaSなら、自前実装に意味がある。ECサイトや社内ツールなら、認証に時間をかけるより本業に集中すべきだ。私がこれまで関わってきたプロジェクトの8割は、最初からKratosで良かった。残り2割は、レガシーシステムとの統合が複雑すぎるか、認証自体がプロダクトの価値だった。今回のケースでは、学習目的で自前実装から始めたが、本番プロジェクトなら最初からKratosを選ぶ。認証に独自性は不要で、チームにセキュリティ専門家もいない——判断は明確だ。次回予告Kratosを導入したことで、認証（Authentication）は解決した。ユーザーはログインできる。セッションも管理される。でも、ログインしたユーザーが「何をできるか」は、まだ決まっていない。認証と認可は別物だ。認証は「誰であるか」を確認する。認可は「何ができるか」を判断する。次回は、Ory Ketoを使ってZanzibarモデルによる認可システムを構築する。おわりに正直に言うと、Kratosの設定を書いている時、何度か「自分で実装した方が分かりやすいのでは」と思った。YAMLの設定項目が多い。ドキュメントを何度も読み返した。でも、動いた時の感覚が違う。これまでに私が書いた数百行のRustコード。それがYAML数十行で置き換わった。しかも、MFAやパスワードリセットなど、私が「次回以降に実装する」と書いていた機能が、既に含まれている。「自前で作ることの非合理性」第1回で書いた言葉を思い出した。認可サーバーだけでなく、認証システムも同じだった。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。Kratosに移行しても、設定の検証やアップグレード対応、障害時の判断は残る。責任が消えるのではなく、「実装の責任」から「選定と運用の責任」に形を変える。その上で、認証の基本的な部分——パスワード認証、MFA、セッション管理——は、毎回ゼロから考える問題ではなくなった。そして、もう1つ学んだことがある。Have I Been Pwnedの件だ。私は58個のテストを書いて「完璧だ」と思っていた。でも、「ユーザーが持ち込むリスク」という視点が完全に抜けていた。専門家が作ったツールを使う価値は、自分の盲点を補えることにある。レビューコメントに返信しよう。「パスワードリセット機能は？」——Kratosで対応します。この記事が参考になれば、読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料Ory KratosOry Kratos GitHubOry Kratos DocumentationKratos QuickstartIdentity SchemaHydra IntegrationOry HydraOry Hydra DocumentationLogin and Consent FlowセキュリティガイドラインOWASP Password Storage Cheat SheetOWASP Authentication Cheat Sheet検証環境ory-kratos-verification（GitHub）]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Error ReportingとCloud Runでアプリエラーをいい感じにグループ化してGitHubイシューにする]]></title>
            <link>https://zenn.dev/kimitsu/articles/report-error-to-github</link>
            <guid isPermaLink="false">https://zenn.dev/kimitsu/articles/report-error-to-github</guid>
            <pubDate>Mon, 12 Jan 2026 02:52:23 GMT</pubDate>
            <content:encoded><![CDATA[Error Reporting とは皆さん、Google Cloud の Error Reporting はご存知でしょうか。あまり知られていないのではないかなと思っています。アプリからは日々エラーが出ておりエンジニアはそれに対応する必要がありますが、エラーというものは同じ原因で複数回出るものです。ログを眺めていて同じようなエラーがたくさん並んでいてもあまり情報は増えません。Error Reporting はアプリケーションのエラーログを収集し、同じ原因のエラーをグループ化してくれるサービスです。[1]例えば以下の例では同じエラーが 3 回出ていますが、Error Rep...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、辞めないなら頑張れ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/12/003013</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/12/003013</guid>
            <pubDate>Sun, 11 Jan 2026 15:30:13 GMT</pubDate>
            <content:encoded><![CDATA[はじめに先週、「おい、辞めるな」という記事を書きました。syu-m-5151.hatenablog.com思った以上に反響がありました。何人かから連絡をもらいました。辞めないことにしました、考えるきっかけになりました、と。ありがたかったです。嬉しかった、と言っていいです。たぶん。ただ、何か落ち着きませんでした。辞めないと決めた。それは分かった。で、その次は。辞めないと決めただけで、何かが変わるわけではありません。私がそうだったからです。辞めないと決めた後も、何も変わりませんでした。評価は上がらない。漠然としたモヤモヤは消えない。夜遅くまでコードを書いた。勉強会に参加した。資格を取った。ブログを書いた。技術力を上げれば認められる。そう信じていました。評価は上がりませんでした。振り返ると、私は頑張り方を間違えていたのです。もっと正確に言えば、評価の構造を理解していませんでした。良い仕事をすれば評価される。そう思っていました。でも、評価者には評価者の論理があります。組織には組織の論理があります。その構造を理解せずに、がむしゃらに頑張っても、報われません。「おい、辞めるな」の最後に、「選んだ道を、正解にしていく過程があるだけだ」と書きました。辞めないと決めた。その選択を正解にするために、何をすればいいのか。この文章は、それを書くために開きました。ただ、書きながらも思います。これが誰かの役に立つのかは、分かりません。分からないまま、書いています。先に断っておきます。この記事は、まだ頑張れる余力がある人に向けて書いています。すでに消耗している人、頑張る気力すらない人には、この記事は届かないだろう。それについては、最後に書きます。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。見えない努力まず、頑張り方を間違えている人が多いです。私もそうでした。インフラのトラブルを未然に防いだことがあります。監視アラートの傾向を見て、「これ、来週やばいことになる」と気づきました。週末に対応して、障害を防ぎました。本番で落ちていたら大騒ぎでした。サービスが止まれば、ビジネスに直接影響が出ます。ユーザーからのクレームが殺到します。深夜に全員が叩き起こされます。そういう未来を、私は未然に防ぎました。でも、月曜日、何事もなかったように仕事が始まりました。誰も何も言いませんでした。障害が起きなかったという「非イベント」は、誰の記憶にも残りません。チームの技術的負債を黙々と返済したことがあります。3ヶ月かけて、複雑なモジュールをリファクタリングしました。スパゲッティコードを解きほぐし、テストカバレッジを上げ、ドキュメントを整備しました。誰かがこの負債を返さなければ、いずれチーム全体が身動きを取れなくなります。そう思って、地道に片付けました。でも、リリース直前に「いつの間にかキレイになってた」と言われただけでした。3ヶ月の努力が、「いつの間にか」で片付けられました。いつの間にか、俺も消えていました。私は「良い仕事をしていれば、いつか評価される」と思っていました。黙々と価値を出していれば、誰かが見ている。実力で認められる。そう信じていました。甘かったです。現実はこうです。見えない仕事は、存在しないのと同じ。どんなに素晴らしい設計をしても、それを言語化して共有しなければ、誰も知りません。どんなに難しいバグを直しても、「大変だった」と伝えなければ、簡単な修正だと思われます。障害を未然に防いでも、障害が起きなかったという「非イベント」は記憶に残りません。これは不公平だと思うだろう。私もそう思いました。私は2年間嘆いていました。居酒屋で同僚と「この会社の評価制度はおかしい」と言い合ったこともあります。「なんで俺の仕事が評価されないんだ」と愚痴りました。言うたびに少し楽になりました。だけど、翌日も同じ状況が続きました。嘆きは鎮痛剤です。痛みを一時的に和らげますが、原因は治りません。この構造を理解した上で、どう振る舞うか。 それが「頑張り方」です。構造を知れここで公平を期しておきます。仕組みの問題は確かにあります。OKRの目標設定が形骸化している。評価者によって評価がブレる。数値化できない仕事が過小評価される。これは仕組みを運営する上で抱える問題です。「人は他人を正しく評価できる」——これは幻想です。同じ人の同じ仕事を見ても、評価者が違えば評価は違います。同じアウトプットで、上司が変わっただけで評価が2段階変わることもあります。私です。絶対的に客観的な評価など存在しません。そもそも、数値で測ろうとした瞬間、測定対象は変質します。コミット数を測り始めたチームを見たことがあります。結果、コミットが細切れになりました。バグ修正件数を測れば、バグを作った人が有利になります。プルリクエストの数を測れば、小さなPRを乱発する人が評価されます。グッドハートの法則と呼ばれる現象です。「指標が目標になると、その指標は機能しなくなる」。OKRを導入したとき、私はこの法則を知りませんでした。知っていたら何か変わったかと言われると、たぶん変わりませんでした。人間だから。エンジニアリングの現場では、これが顕著に現れます。これは事実です。認めましょう。その上で、自分に何ができるかを考えます。仕組みの問題を批判するのは簡単です。でも、評価制度を変えるのは難しい。上司を変えることはできません。待っていても変わりません。冒頭で書いた通り、私はこれをやっていました。仕組みの問題を指摘して溜飲を下げる。鎮痛剤を飲み続けて、原因を放置していました。仕組みがおかしいのは事実です。でも、仕組みは変えられない。自分は変えられる。 それが「頑張る」ということです。変えられないものに時間を使うほど、あなたの人生は長くありません。——と書いて、立ち止まります。「結局、自己責任論じゃないか」と言われるだろう。構造の問題を認識しながら、最後に「個人が変われ」と言っている。評価されないのは構造の問題なのに、「お前の頑張り方が悪い」と言っている。それは自己責任論の強化じゃないか、と。正直に言えば、その批判は当たっています。私は構造の問題を認識しながら、「構造を変えろ」とは言いませんでした。「構造の中でうまくやれ」と言いました。それは、構造を温存することに加担しています。これは私の限界です。私が書けるのは、私が経験したことだけです。構造を変えることに成功した人が、その方法を書いてくれることを願います。ただ、1つだけ言い訳させてください。私は「評価されないのはお前のせいだ」とは言っていません。「評価制度には限界がある」「客観的評価は存在しない」と、繰り返し書きます。その上で、「構造が変わらない中で、個人に何ができるか」を書いています。自己責任論と言われれば、そうだろう。でも、構造が変わるのを待っていても、あなたの評価は上がりません。変わらない構造の中で、今日をどう生きるか。それを考えるしかありませんでした。しかし、重要な注意点があります。仕組みの問題が大きすぎる時は、「辞める」が正解のことがあります。 個人の努力で覆せない構造もあります。それを見極める目も必要です。評価制度が必ず歪む理由評価制度が歪むのは、設計者の能力不足ではありません。測定されるものは、測定によって変質するからです。どの制度も、導入した瞬間に歪み始めます。完璧な評価制度は原理的に存在しません。この事実は、あなたを責めるためにあるのではありません。あなたを解放するためにあります。「自分が無能だから評価されない」という思い込みから解放され、「制度の限界を前提に、どう動くか」という問いに切り替わります。評価の幻想上司は、神でもエスパーでも上位存在でもありません。人間です。君よりも少しだけ観点の多い人間です。人外だと思っている上司も、人間であり、認知には限界があります。これは「上司が無能だ」という話ではありません。人間である限り、客観的評価は原理的に不可能だという話です。なぜ「客観的評価」は不可能なのか「客観的評価」という言葉には、2つの前提があります。「評価すべき対象を正確に観察できる」という前提と、「観察したものを正確に評価できる」という前提です。どちらも成り立ちません。観察の問題から見てみましょう。上司があなたの仕事のうち、何%を直接観察しているでしょうか。会議での発言。Slackでのやり取り。プルリクエスト。これは観察できます。でも、設計を考えている時間。問題を切り分けている時間。ドキュメントを読んでいる時間。これは見えません。上司が見ているのは、あなたの仕事の氷山の一角に過ぎません。観察できるものだけを見て、全体を評価している。これは観察者の怠慢ではありません。構造的に避けられない限界です。比較の問題もあります。評価とは、本質的に比較です。Aさんは「期待以上」、Bさんは「期待通り」。この判断をするには、AさんとBさんを比較する必要があります。でも、2人の仕事が違えば、比較は困難になります。バックエンドで高負荷対策をしたAさんと、フロントエンドで複雑なUIを実装したBさん。どちらが「より価値がある」か。答えはありません。比較不可能なものを比較しています。上司は無理やり比較し、順位をつけます。その順位に「客観性」などありません。観察者効果という問題もあります。観察すること自体が観察対象に影響を与えるというものです。「評価される」と意識した瞬間、行動が変わります。評価されやすい仕事を選ぶ。見える形で成果を出そうとする。「本当の仕事ぶり」を観察しているわけではありません。「評価を意識した仕事ぶり」を観察しています。上司の認知バイアス観察の限界に加えて、観察したものを処理する段階でもバイアスがかかります。直近バイアス: 1年間を均等に覚えていません。評価直前の出来事が記憶に残ります。4月に素晴らしい仕事をしても、12月の評価面談では薄れています。11月に目立つ失敗をすると、それが印象を決めます。ハロー効果: 1つの良い印象が全体評価を引き上げます。1つの失敗が全体を引き下げます。障害対応で活躍すると、「この人は優秀だ」と思われます。その印象が、関係のない能力の評価にも影響します。確証バイアス: 一度「優秀」と思うと優秀な証拠ばかり目に入ります。「ダメ」も同様です。最初の印象が固定され、それを覆す情報は無視されます。これは上司の能力不足ではありません。人間の認知システムに組み込まれた特性です。どんなに優秀な上司でも、これらのバイアスから完全に逃れることはできません。ここで、1つ確認しておきたいです。「自分は正しく評価されていない」と感じたことがあるでしょうか。もしあるなら、それは被害妄想ではありません。構造的に、完全に正しい評価など存在しません。上司がどんなに優秀でも、認知バイアスからは逃れられません。あなたの感覚は、間違っていません。評価基準自体が「客観的」ではないより根本的な問題があります。評価基準自体が客観的ではないのです。「技術力」「コミュニケーション力」「リーダーシップ」——評価シートに並ぶこれらの言葉は、一見客観的に見えます。でも、その定義は人によって違います。「技術力が高い」とは何か。コードの品質が高いこと？難しい問題を解決できること？新しい技術をキャッチアップするのが速いこと？幅広い技術に詳しいこと？上司によって、重視する側面が違います。つまり、評価基準そのものが社会的に構成されたものです。「何を価値とするか」は、文化、組織、時代によって変わります。普遍的な基準などありません。私は異動で気づきました。前のチームでは「期待以上」と評価されていました。技術的な深さを評価してくれる上司でした。異動した先では「成長途上」と評価されました。新しい上司はチームへの影響力を重視していました。スキルは変わっていません。変わったのは上司です。「客観的評価」を求めるより、やるべきこと評価とは「私が何をしたか」ではなく「上司が私をどう見るか」です。この事実を受け入れると、行動が変わります。「客観的に見れば、私は評価されるべきだ」という主張は意味がありません。客観的な視点など存在しないからです。存在するのは、上司の視点だけです。だから、「客観的評価」を求めるのはやめました。代わりに、上司が何を見ているかを理解することに注力しました。上司は何を重視するか。何に反応するか。何を見落としているか。それを理解した上で、上司に伝わる形で成果を見せます。これは媚びを売ることとは違います。上司の視界に入る努力をしているだけです。上司の視点を理解するには、いくつかの問いを考えるといいです。「この上司は何を『良い仕事』だと思っているか」「この上司は何にストレスを感じているか」「この上司は、上からどんなプレッシャーを受けているか」。これらを理解すると、上司が何を見て、何を見落としているかが見えてきます。組織の論理評価制度と評価者の心理を理解したら、次は組織の論理を理解しましょう。組織には、個人の論理とは異なる、独自の論理があります。この論理を理解しないと、「なぜ評価されないのか」が分からないままになります。組織は「最適化」で動く組織は、個人の幸福を最大化するために存在しているわけではありません。組織の存続と成長を最適化するために存在しています。この当たり前の事実を、意外と多くの人が忘れています。評価制度も、昇進制度も、給与制度も、すべて「組織の最適化」のために設計されています。「個人が納得するか」は、二次的な目標に過ぎません。もちろん、個人が納得しなければ離職が増えるから、ある程度は配慮されます。でも、最優先ではありません。だから、「公平な評価」を期待すると、裏切られます。組織が目指しているのは公平な評価ではなく、組織にとって都合の良い行動を引き出す評価だからです。昇進はゼロサムゲームである昇進枠は有限です。誰かが昇進すれば、誰かは昇進しません。「今期は枠がなかった」と言われたことがある人もいるでしょう。それは、あなたの実力の問題ではなく、構造の問題です。予算も同じです。パイの大きさは決まっています。問題は、パイをどう切り分けるかです。だから、昇給交渉は「自分の価値を証明する」だけでは不十分です。「なぜ自分に配分を増やすべきか」を説明する必要があります。自分に正直に向き合ってください。あなたが昇進することで、上司やチームにはどんな具体的なメリットがあるか。「この人を昇進させると、〇〇という効果があります」と言える材料を、あなた自身が上司に渡してください。政治は「資源配分の闘争」である「誰を昇進させるか」は、技術力だけで決まりません。上司と上司の上司の関係。部門間の力学。人事部の意向。様々な要素が絡み合います。「政治なんて関係ない」と思いたい気持ちは分かります。技術力で勝負したい。でも、組織で働く以上、政治は存在します。政治とは何か。限られた資源を巡る配分の闘争です。資源とは、予算、人員、プロジェクト、昇進枠、注目、発言力。これは有限です。誰かが得れば、誰かが失います。この配分を決めるプロセスが、政治です。政治を「汚いもの」と見なすのは、的外れです。資源が有限である限り、配分のプロセスは必ず存在します。それを「政治」と呼ぼうが呼ぶまいが、現象は消えません。政治を無視しても、政治はあなたに影響します。あなたが政治を無視しても、他の誰かが政治を使って資源を獲得すれば、あなたに回る資源は減ります。だから、政治を理解した上で動いた方がいいです。しかし、誤解しないでください。「政治を理解してください」は「政治に加担してください」という意味ではありません。「政治ゲームの名プレイヤーになってください」とも言っていません。政治の存在を認識し、その中で自分がどう動くかを考えるということです。組織の論理と個人の論理は違うここまでの話をまとめると、こうなります。組織は「組織の最適化」で動きます。個人の最適化ではありません昇進枠は有限です。ゼロサムゲームです予算は配分の問題です。パイの切り分けです政治は資源配分の闘争です。避けられませんこれらを理解すると、「なぜ評価されないのか」の見え方が変わります。「自分は良い仕事をしている」は、個人の論理です。組織の論理から見ると、「良い仕事をしている人」は他にもいます。問題は、有限の資源を誰に配分するかです。だから、「良い仕事をすれば評価される」は半分しか正しくありません。正確には、「良い仕事をした上で、資源を配分すべき理由を説明できれば評価される」です。ここまで読んで、息苦しくなっただろう。評価制度には限界があります。客観的評価は存在しません。組織は個人の幸福を最大化しません。昇進はゼロサムゲームです。政治は避けられません。厳しい現実です。でも、現実を知ることは、現実に絶望することではありません。構造を知らなければ、暗闘の中で闘っているようなものです。構造を知れば、どこに光があるか見えます。ここからは、その光に向かって動く方法を書きます。やるべきことは、大きく3つあります。「どこを見るか」を変えること、「対話」を通じて認識を揃えること、「見せる」ことで存在を証明することです。チームを見ろ組織の論理を理解したら、次は「どこを見るか」を変えることです。「どの会社で働くか」が大事だと思われています。でも、本当に大事なのは「どのチームで働くか」です。従業員が「ここで働くのをやめよう」と決める時、この「ここ」は会社ではありません。チームです。会社は好きだがチームが合わなくて異動する人がいます。逆に、会社の方針には疑問があるがチームが良くて残る人もいます。これは新卒就職活動をされている方や、転職を考えている方に特に知っておいてもらいたいことです。企業文化が良い会社でも、自分が配属されるチームの雰囲気が良いとは限りません。評価も同じです。「この会社の評価制度」より、「直属の上司の評価パターン」の方が、あなたの評価に直接影響します。会社の評価制度がどれだけ整っていても、その制度を運用するのは上司です。上司が制度を正しく運用しなければ、制度の意味はありません。逆も同じです。評価制度が多少おかしくても、上司が良ければ、適切に評価される可能性があります。だから、転職先を選ぶときも、残るか辞めるかを判断するときも、「会社」という抽象的な単位で考えないでください。どのチームに入るか。誰が上司になるか。 その具体的な単位で考えてください。対話しろ嘆きは鎮痛剤だと書きました。では、対話は何か。対話は手術です。痛いし、面倒だし、時間がかかります。でも、原因を取り除ける可能性があります。対話が必要な理由は単純です。あなたと上司は、別の人間だからです。別の経験を持ち、別の価値観を持ち、別の情報を持っています。この情報の非対称性を埋める方法は、対話しかありません。見えている世界の違いを理解する上司と話が通じないとき、「上司が悪い」と思いがちです。でも違います。部下と上司では見えている世界が違います。自分から見ると理不尽な判断でも、上司の立場から見ると合理的なことがあります。上司には上司のプレッシャーがあります。部門の目標があります。上からの期待があります。その世界の中で、上司は合理的に動いています。その上で話せないことがあります。これは「上司の判断を正当化してください」という話ではありません。上司の判断が間違っていることもあります。でも、その判断がどこから来ているかを理解しなければ、対話はできません。対話とは、この世界の違いを認識した上で、共通の理解を構築する作業です。自分の世界だけで考えると「なんで分かってくれないんだ」となります。でも、上司の世界に立ってみると「なるほど、だからそう判断するのか」と見えてきます。見えてくれば、「では、この点はどうですか」と別の角度から提案できます。上からの視点と現場の視点上司と部下では、見ている方向が違います。上司は上から降りてくる方針を見ています。目標、KPI、ロードマップ。経営が何を求めているか。一方、現場は下を見ています。実際に何が起きているか。どこに問題があるか。この2つが噛み合っていないと、話が通じません。「上が何を考えているか分からない」「現場の声が届かない」——どちらも、この断絶の症状です。対話は、この2つをつなぐ作業です。上司と話すとき、上司が見ている方向を理解しようとします。同時に、現場のリアリティを言語化して伝えます。その接点を見つけることが、対話の目的です。ここで具体的なアクションがあります。上司が今、上の階層から課されている「最も頭の痛い課題」を把握してください。上司も誰かの部下です。上司にも上司がいます。その上司から何を求められているか。何に頭を抱えているか。それを知れば、あなたの仕事をどう位置づければいいか見えてきます。上司が「コスト削減」に追い詰められているなら、あなたの技術改善は「効率化」として語ってください。上司が「新規プロジェクトの立ち上げ」に追われているなら、あなたの貢献は「立ち上げを支える基盤整備」として語ってください。上司の頭痛の種を知れば、あなたの仕事の見せ方が変わります。対話を自分から始める「次の昇進に必要なことは何ですか」と1on1で聞きます。怖いです。否定されるでしょう。「まだ早い」と言われるでしょう。でも、聞かないと何も始まりません。自己評価と組織からの評価が食い違うとき、上司を敵だと思ってしまいがちです。「この人とは話しても仕方ない」と見限って、対話をやめます。これが最悪のパターンです。一度「敵」だと思うと、何を見ても敵の証拠に見えます。中立的な発言も「やっぱり敵だ」と解釈します。相手もそれを感じ取り、本当に敵対してきます。悪循環にハマります。これは認知バイアスの一種で、一度形成された敵対的な認知は、自己強化していきます。対話を自分から始めてください。待っていても始まりません。対話は「同意」ではない対話の目的は、合意することではありません。理解を共有することです。対話した結果、意見が一致しないこともあります。それでいいです。重要なのは、「なぜ相手がそう考えるか」を理解することです。理解した上で、なお意見が違うなら、それは対話の失敗ではありません。「上司と対話したが、評価は変わらなかった」という結果があり得ます。それでも、対話には意味があります。「なぜ評価が変わらないのか」の理由を理解できたはずです。理由を理解すれば、次の行動を決められます。理由が「あなたのスキルが足りない」なら、スキルを伸ばす努力をします。理由が「今期は枠がない」なら、来期に向けて準備します。理由が「この上司とは価値観が合わない」なら、異動や転職を検討します。対話の目的は、情報を得ることです。同意を得ることではありません。制度が機能していないなら、自分で対話を作れ本当は、目標設定や評価制度というのは、この対話を縮減化して仕組み化したものです。「何を目指すか」「どこまでやるか」「何ができたか」を定期的にすり合わせる機会です。でも、多くの組織で、この仕組みは形骸化しています。目標設定は形だけです。評価面談は結果の通知だけです。対話が発生していません。仕組みがうまく機能していないなら、仕組みが本来やろうとしていたことを、自分で意識的にやればいいです。1on1で自分から聞きます。週次報告で自分から伝えます。仕組みに頼らず、対話を自分で作ります。基準を握れ構造を理解し、対話の重要性を理解したら、次は具体的に動きます。まず、評価基準を言語化してください。多くの人は、上司が何を基準に評価しているか、明確に理解していません。なんとなく「良い仕事をすれば評価される」と思っています。でも、上司の頭の中にある評価基準と、自分が想像している評価基準は、往々にしてズレています。1on1で聞くべき具体的な質問「昇進に必要なことは何ですか」「今の自分に足りないものは何ですか」「次の評価期間で何を達成すれば、評価が上がりますか」「あなたが重視していることは何ですか」「なぜその目標が重要なんですか」「この目標が達成されないと、何が困りますか」これらの質問を、恐れずに聞いてください。「そんなこと聞いていいの？」と思うでしょう。私もそう思っていました。こういう質問をすることに、強い抵抗がありました。正直に言えば、私を含めてエンジニアは、こういう「合意形成」をバカにしている節があります。技術力で勝負したい。政治的なことはやりたくない。上司にゴマをするみたいで嫌だ。そういう感覚があります。もう1つ、ネガティブなフィードバックを受け取りたくない、という心理もあります。「今の自分に足りないものは何ですか」と聞いて、厳しいことを言われたらどうしよう。自分が思っているより評価が低かったらどうしよう。聞かなければ、知らずに済みます。でも、聞かなければ分かりません。上司はエスパーではないし、あなたもエスパーではありません。期待値をすり合わせるには、対話するしかありません。「昇進したいです」と直接言うのは恥ずかしいです。自分の欲を見せることに抵抗があります。私は3年間言えませんでした。言い出せないまま、居酒屋で愚痴を言い、鎮痛剤を飲み続けていました。鎮痛剤の効き目が切れてきた4年目に、ようやく口を開きました。でも、上司からすれば、部下が何を求めているか分からなければ、サポートのしようがありません。期待値のすり合わせ上司が求めるものと、自分がやりたいことは、必ずしも一致しません。上司が重視するのはAだが、自分が得意なのはB。この場合、どう動くか。まず、そのギャップを言語化してください。「自分はBが得意だが、Aに注力すべきですか」と聞いてください。上司は「Aをやってほしい」と言うでしょうし、「Bで成果を出してくれればいい」と言うでしょう。どちらにせよ、ギャップを認識した上で動けます。ギャップを認識しないまま、自分の得意なBに注力して、評価面談で「Aをやってほしかったのに」と言われるのが最悪のパターンです。見せろ評価基準を理解しました。上司との期待値もすり合わせました。次は、実際に動く番です。対話は手術だと書きました。では、見せることは何か。見せることはリハビリです。地味で、継続が必要で、効果が見えるまで時間がかかります。でも、これをやらなければ、手術しても回復しません。冒頭で書きました。見えない仕事は、存在しないのと同じだと。障害を未然に防いでも、誰も気づきません。技術的負債を返済しても、「いつの間にかキレイになってた」で終わります。これは不公平です。でも、嘆いても変わりません。変えられるのは、自分の行動だけです。だから、見せてください。何をやっているか、どんな価値を生んでいるか、言葉にして伝えてください。なぜ「見せる」ことが必要なのか「良い仕事をしていれば、見てもらえるはずだ」——これは幻想です。上司の注意は有限です。注意は希少資源です。上司は複数の部下を持っています。自分の仕事もあります。上からのプレッシャーもあります。その中で、あなたの仕事に割ける注意は、ごくわずかです。あなたが黙って良い仕事をしていても、上司の注意はあなたに向きません。問題を起こす部下、声の大きい部下、頻繁に報告してくる部下に注意が向きます。注意を向けてもらえなければ、あなたの仕事は認識されません。認識されなければ、評価されません。これは「目立ったもの勝ち」という話ではありません。情報の非対称性の話です。あなたは自分の仕事を100%知っています。上司は、あなたの仕事の10%も見ていません。この情報ギャップを埋めるのは、あなたの責任です。上司が勝手に気づいてくれることを期待するのは、非現実的です。人が本当に求めているのは、実はフィードバックではありません。「注目」です。自分の仕事を見てもらっている。気にかけてもらっている。存在を認識されている。そういう感覚です。私自身、厳しいフィードバックより、上司が自分の仕事を把握していないことの方が堪えました。評価されないと感じるとき、本当の問題は「評価が低い」ことではなく「注目されていない」ことでしょう。上司は、あなたが何をしているか知りません。知らなければ、評価以前の問題です。「見せる」ことへの抵抗多くのエンジニアは、「見せる」ことに抵抗がある。「アピールは卑しい」という感覚がある。日本の文化では、自己主張は美徳ではない。「黙って結果を出す」が美しいとされる。自分の成果を語ることは、自慢に見える。謙虚さが失われる。そういう感覚がある。でも、この感覚は、情報の非対称性を無視している。あなたが黙っていれば、上司はあなたの仕事を知らない。知らなければ、評価できない。「黙って結果を出す」は、「結果を出しても評価されない」と同義だ。「仕事の質で勝負したい」という信念もある。アピールの上手さではなく、仕事の質で評価されたい。それは正しい感覚だ。でも、仕事の質を上司に伝えるのは、アピールではない。情報提供だ。上司は、あなたの仕事の質を判断する材料を持っていない。その材料を提供するのは、あなたの役目だ。具体的な言い方週次報告での言い方ダメな例:「今週はAの修正をしました。」良い例:「今週はAの修正をしました。このバグは再現条件が複雑で、ログから特定するのに2日かかりました。原因は○○で、同様の問題が他に3箇所あったので併せて修正しています。」違いは、「何が難しかったか」「どう判断したか」「影響範囲をどう考えたか」を言語化していること。Slack、1on1、どの場面でも同じ原則です。言語化はスキルだアピールが苦手？ なら、存在しないのと同じだ。「自慢みたいで嫌だ」と思うだろう。私もそうだった。でも、これは自慢ではない。自分の仕事の価値を言語化しているだけだ。言語化しなければ、他人には見えない。見えなければ、評価されない。言語化は、スキルだ。最初は苦手でも、練習すれば上達する。週次報告を書くたびに、「何が難しかったか」を1文追加する。それだけで、見え方が変わる。タイミングを狙え「見せる」にも戦略がある。上司の認知の限界を理解することが重要だ。なぜタイミングが重要なのか評価面談の席で、上司は1年間を振り返る。でも、1年間を均等に思い出すことは、人間には無理だ。上司も人間だ。人間の記憶には癖がある。最初の方と最後の方は覚えているが、中間は忘れやすい。期初に立てた目標は覚えている。期末の追い込みも覚えている。でも、中間の地道な仕事は埋もれる。より厄介なのが、最近の出来事ほど重要に感じられる傾向だ。4月に素晴らしい仕事をしても、12月の評価面談では遠い記憶だ。「そういえば、何かやってくれた気がするな」程度の印象しか残らない。一方、11月に目立つ成果を出せば、12月の評価面談では鮮明に覚えている。もう1つ、人間は経験全体を平均的に評価しない。最も印象的だった瞬間と、終わりの印象で全体を判断する。1年間コツコツ働いても、期末に目立つ成果がなければ、「今期は普通だったな」という印象になりやすい。逆に、期末に大きな成果を出せば、「今期は頑張っていたな」という印象が残る。これは上司の能力不足ではない。人間の脳の仕組みだ。批判しても変わらない。構造を理解した上での3つの戦略この認知の限界を理解した上で、どう動くか。1. 評価の2ヶ月前に目立つ成果を出す大きなリリースのタイミングを調整可能なら、評価期間の後半に持ってくる。調整できなくても、過去の成果の効果を後半に言語化し直すことはできる。「4月にリリースした機能が、この半年でこれだけの効果を出しました」と。成果を「過去のイベント」ではなく「現在も続いている効果」として再提示する。2. 月次で「今月やったこと」を共有する上司の記憶を定期的に上書きする。年末に慌てて振り返るのではなく、毎月、記録を残しておく。これは上司のためだけではない。自分のためでもある。1年前に何をやったか、自分でも忘れる。月次の記録があれば、評価面談の準備が楽になる。もう1つ重要なことがある。「ピーク」がない期間の地味な貢献を、上司が「思い出しやすいエピソード」として毎月ストックしているか。「今月は特に目立った成果はありませんでした」で終わらせるな。地味な仕事でも、言語化すれば印象に残る。「依存ライブラリのアップデートで、セキュリティリスクを2件潰しました」。これだけで、「あの人は地道にやってくれている」という印象が積み上がる。3. 印象に残る瞬間を意識的に作る人間は、最も印象的だった瞬間で全体を判断する。これを逆手に取る。難しい問題を解決した。障害対応で活躍した。これらの「ピーク」は記憶に残りやすい。ピークがあれば、平凡な日々も「あの人は活躍していた」という印象に変換される。「ズルい」という感覚について「タイミングを調整するなんてズルい」と思うでしょう。仕事の質で評価されるべきです。タイミングを操作するのは、本質的ではありません。でも、考えてみてほしい。あなたがタイミングを意識しなくても、他の誰かは意識しています。評価期間の後半に目立つ成果を出す人。月次報告を欠かさない人。彼らは「ズルい」のではなく、「構造を理解している」だけです。タイミングを調整することは、媚びを売ることではありません。上司の認知の限界を理解した上で、情報を届けているだけです。上司が全てを均等に覚えていてくれるなら、タイミングは関係ありません。でも、上司は人間です。人間の記憶には限界があります。その限界を前提として動く方が、合理的です。この癖は、知っていれば対処できます。知らなければ、無意識のうちに損をします。評価する側もされる側も、同じ脳を持っています。上司もまた、自分の記憶の癖に気づいていないことが多いです。4. 失敗したときのリカバリーを設計しておく失敗は起きます。問題は、その失敗がハロー効果で全体評価を引きずり下ろすことです。「あの人は失敗した」という印象が、関係のない能力の評価まで下げます。これを防ぐには、失敗の直後に2つのことをやってください。まず、迅速に報告してください。隠そうとして発覚すると、「失敗した」にまた「隠そうとした」が上乗せされます。次に、原因と対策を透明に説明してください。「なぜ起きたか」「何を学んだか」「次にどう防ぐか」を言語化します。これができると、「失敗した人」ではなく「失敗から学べる人」という印象に変換されます。失敗を完全に消すことはできません。ですが、失敗の印象を上書きすることはできます。スポンサーを作れ昇進には「スポンサー」と「可視化」が必要だ。なぜスポンサーが必要なのか昇進は、だいたいあなたの知らないところで決まる。評価会議というものがある。マネージャーが集まって、誰を昇進させるか、誰に良い評価をつけるかを議論する。あなたは、その会議に出席できない。出席できないのに、そこであなたの運命が決まる。あなたの仕事ぶりを知っている人が、その会議にいなければ、あなたの名前は挙がらない。名前が挙がらなければ、昇進しない。どんなに良い仕事をしていても、その会議で誰かがあなたの名前を出さなければ、無意味だ。その「誰か」が、スポンサーだ。スポンサーとメンターの違いメンターは、アドバイスをくれる人だ。キャリアの相談に乗ってくれる。「こうした方がいいよ」「あの人に話を聞いてみたら」と教えてくれる。スポンサーは、あなたの成果を上に伝えてくれる人だ。人事評価の場で、あなたの名前を出してくれる。「あいつは良い仕事をしている」と会議で言ってくれる。この違いは決定的だ。メンターは「あなたのために」アドバイスをくれる。でも、スポンサーは「あなたのために」リスクを取る。評価会議であなたの名前を出すということは、スポンサー自身の信用を賭けることだ。「私が推薦した人」が期待外れだったら、スポンサーの評価が下がる。だから、スポンサーになってもらうのは、メンターになってもらうより難しい。メンターがいても、スポンサーがいなければ、昇進の話にはならない。あなたの良い仕事を知っている人がいても、その人が上に伝えてくれなければ、上層部はあなたを知らない。上司だけがスポンサーではない多くの場合、直属の上司が最初のスポンサー候補になる。でも、上司だけに依存するのはリスクがある。上司が異動することがある。上司が退職することがある。上司との相性が悪いこともある。上司が評価会議で発言力を持っていないこともある。上司一人に依存していると、その上司がいなくなった瞬間、あなたを推してくれる人がゼロになる。だから、上司以外のスポンサーも獲得しろ。評価会議には、複数のマネージャーが参加する。あなたの上司だけでなく、他のチームのマネージャーも発言権を持っている。もし、あなたの名前が複数の人から挙がったらどうなるか。「〇〇さん、評判いいね」となる。一人が推すより、複数が推す方が説得力がある。上司以外のスポンサー候補は、意外と身近にいます。他チームのマネージャー: 横断プロジェクトで一緒に働いた人技術リード: マネージャーに意見を求められる立場の人越境した仕事を意図的に作ってください。横断プロジェクトに手を挙げます。他チームのコードレビューを引き受けます。上司を勝たせることの意味上司が成果を出せば、チーム全体の評価が上がります。リソースが配分されます。自分の評価も上がりやすくなります。「媚びる」と「伝える」は違います。情報の非対称性を埋めているだけです。条件が揃わない場合しかし、これには条件がある。条件1: 上司が「勝とうとしている」こと上司が何を達成しようとしているかを理解できないなら、この戦略は機能しない。目標が不明確な上司、日々の消化試合に終始している上司には、「勝たせる」も何もない。判断方法：1on1で「今期の最優先目標は何ですか」と聞く。具体的な目標を即答できるなら、勝とうとしている。「色々ある」「維持が目標」と言うなら、勝とうとしていない可能性が高い。条件2: 上司が「部下の貢献を認識できる」こと上司を勝たせても、「これは俺の成果だ」と言い張る上司がいる。この場合、どれだけ貢献しても報われない。判断方法：過去の昇進者を観察する。上司が「〇〇さんのおかげで成功した」と言っていたか。チームの成果発表で、メンバーの名前を出していたか。自分の手柄にする上司は、パターンがある。条件3: 組織が「チームの成功を個人にも還元する」構造であることチームが勝っても、個人の評価に反映されない組織がある。年功序列が強すぎる、政治が評価を決める。この場合、上司を勝たせても自分には返ってこない。判断方法：先輩に聞く。「チームが成果を出したとき、個人の評価に反映されましたか」と。曖昧な答えが返ってきたら、還元されていない証拠だ。これらの条件が揃わない場合、「上司を勝たせる」戦略は機能しない。別の手を考える必要がある。例えば、「異動する」「別のスポンサーを見つける」「辞める」だ。上司以外のスポンサーを持っていれば、この「別のスポンサーを見つける」がすでに準備できている。上司に依存しすぎないためにも、日頃から複数のスポンサー候補との関係を築いておくことが重要だ。下振れで測られる対話しても評価が変わらないことがある。そのとき、もう1つ確認すべきことがある。自己認識と他者認識のギャップだ。「最高の自分」は実力ではない多くの人は、「最高の自分」を自分の実力だと思っている。ゾーンに入って神がかった速度でコードを書く自分。難解なバグを一瞬で特定する自分。そういう「最高の瞬間」を「自分の実力」だと信じる。でも、上司が見ているのは別のものだ。上司は、あなたに仕事を任せるとき、こう考える。「この人に任せて、最悪どうなるか」と。最高のケースではない。最悪のケースだ。なぜなら、任せた仕事が期待以下だったとき、責任を取るのは上司だからだ。上司は自分の評価を賭けている。だから、リスクを最小化したい。つまり、あなたは「上振れ」ではなく「下振れ」で判断されている。調子が良い日に出した成果は、「たまたま」でしょう。調子が悪い日に出した成果こそ、「確実に期待できるライン」です。上司が知りたいのは、後者です。だから、自分の実力を測るなら、最高の日ではなく、最悪の日を見てください。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その日に絞り出したアウトプット。それが、他人から見た「あなたの実力」に近いです。安定性という信頼信頼は、瞬間最大風速では測られない。安定性で測られる。毎週コンスタントに成果を出す人と、たまに爆発的な成果を出すが波がある人。どちらが信頼されるか。前者だ。爆発的な成果は印象に残る。でも、任せる側からすれば、「今回はどっちだろう」と毎回賭けをすることになる。安定している人には、安心して任せられる。ここで、あまり語られない現実を書く。体調管理は、評価に直結する。「体調不良は仕方ない」と、口では誰もがそう言う。風邪をひいた、熱が出た、それは本人のせいではない。責めるべきではない。正論だ。でも、現実はそんなに甘くない。風邪で3日寝込めば、1週間分の生産性が消える。体調不良の翌週もパフォーマンスは戻りきらない。締め切り直前に体調を崩せば、チーム全体に影響が出る。上司は、それを見ている。口では「お大事に」と言う。でも、心の中では「また休みか」と思っている。重要なプロジェクトを任せるとき、「この人、大丈夫かな」と不安がよぎる。結果、重要な仕事は「安定して稼働できる人」に回る。これは不公平だと思うだろう。体質の問題もある。本人の努力だけではどうにもならないこともある。それは事実だ。でも、コントロールできる部分は、コントロールしてください。もう1つ重要なことがあります。自分のパフォーマンスが落ちる兆候を自己認識していますか。睡眠不足が続くとどうなるか。ストレスが溜まるとどうなるか。これらを把握しておけば、周囲に「予測可能性」を提供できます。「来週は締め切りが重なっているので、レスポンスが遅くなるだろう」と先に言っておきます。これは弱みを見せることではありません。プロとして自分の状態を管理していることを示しています。上司があなたに仕事を任せるとき、「リスク」として感じている要素は何か。「この人は締め切りを守らない」と思われているなら、小さな約束から確実に守ってください。上司の中にある「リスク認知」を、1つずつ消していってください。他人はあなたの「見えた成果の平均」を見ている自分で認識している自分と、他人が見ている自分は違います。あなたは自分の内面を知っています。「今日は調子が悪い」「昨日は睡眠不足だった」「あのときは本気を出していなかった」。そういう文脈を全て知っています。だから、最高のパフォーマンスを出した日を「本当の自分」だと思います。それ以外の日は、何か理由があってパフォーマンスが落ちた「例外」だと思います。他人は、あなたの内面を知りません。見えるのは、あなたのアウトプットだけです。見えたアウトプットの平均が、「あなた」として認識されます。見せなかった仕事は、平均にすら入りません。最高の日も、最悪の日も、見えた範囲で平均化されます。だから、あなたが「本気を出せばもっとできる」と思っていても、他人から見れば「見えた範囲のあなた」がそのままあなたの実力です。見せていない実力は、存在しないのと同じです。ギャップを埋める方法自己認識と他者認識のギャップを埋めるには、フィードバックを求めるしかない。「私の強みと弱みは何ですか」と上司に聞く。怖い。自分が思っている自分と違う答えが返ってくるだろう。でも、聞かなければギャップは分からない。もう1つの方法は、360度評価の結果を真剣に受け止めることだ。多くの人は、360度評価の結果を「まあ、そういう見方もあるよね」程度で流す。でも、複数の人が同じことを指摘しているなら、それはおそらく事実だ。「本当はもっとできる」は通用しない新しい環境で、あなたは「最高の自分」ではなく「最悪の自分」で評価される。慣れない環境、知らないコードベース、初対面のチームメンバー。その状況で出せるアウトプットが、あなたの「実力」として記録される。「本当はもっとできるんです」は通用しない。それは言い訳だ。今、目の前で出しているアウトプットが、あなたの実力として認識される。「体調が悪かったので」も通用しない。体調が悪い日も含めた平均が、あなたの実力だ。だから、自分の「下限」を正しく認識することが重要だ。自分が思っているよりも、自分の下限は低いだろう。他人から見えている自分は、自分が思っている自分とは違うだろう。このギャップを認識した上で、どう動くか。それが「構造を理解した上で頑張る」ということだ。チームを勝たせろここまで「やるべきこと」を書いてきた。ここで1つ、やらなくていいことを書く。「最高の人材はオールラウンダーである」——そう信じられている。でも、そもそもオールラウンダーは、組織が作り出した便利な幻想だ。能力は文脈の中にしか存在しない。「オールラウンダー」とは、会社が定義した評価項目の範囲内でバランスが良い、というだけの話だ。それは普遍的な能力ではなく、ある限定された文脈の中で複数の能力がそこそこ高いだけだ。オールラウンダーの罠でも、オールラウンダーを目指すと何が起きるか。どの分野でも「そこそこ」になります。よくある罠があります。評価面談で「コミュニケーション力が弱い」と言われて、無理に改善しようとします。勉強会で発表する練習をします。ファシリテーションの本を読みます。その結果、強みだった技術力を伸ばす時間が減ります。コミュニケーション力は「平均以下」から「平均」になっただけです。技術力は「突出」から「やや上」に落ちました。本末転倒です。弱みを平均まで引き上げる努力は、強みを突き抜けさせる努力より、はるかに効率が悪いです。100時間かけて弱みを「平均以下」から「平均」にするより、100時間かけて強みを「上位10%」から「上位1%」にする方が、価値が出ます。「チームを勝たせる」という発想ここで視点を変えてほしい。ここまで「上司を勝たせてください」と書いてきました。上司の目標に貢献してください。上司の労力を最小化してください。それがスポンサーを獲得し、評価につながる、と。でも、上司を勝たせることは、手段に過ぎません。本質は「チームを勝たせること」です。チームが勝てば、全員が恩恵を受けます。リソースが配分されます。良いプロジェクトが回ってきます。評価の枠が増えます。逆に、チームが負ければ、個人がどれだけ頑張っても報われません。沈む船の上でいくら走っても、沈むことに変わりはありません。だから、「自分がどう評価されるか」ではなく「チームがどう勝つか」を考えてください。強みで貢献するチームを勝たせるために、あなたは何ができるか。答えは単純です。強みで貢献してください。チームには様々な仕事があります。設計、実装、テスト、ドキュメント、調整、発表。全部を一人でやる必要はありません。チームとして、全部ができていればいいです。あなたがコードを書くのが得意なら、コードで貢献してください。ドキュメントが得意な人に、ドキュメントは任せてください。あなたが調整が得意なら、調整で貢献してください。実装が得意な人に、実装は任せてください。これが「補完」です。全員がオールラウンダーを目指すより、それぞれが強みを発揮して補完し合う方が、チームとしての出力は高くなります。優秀な人に共通パターンはありません。コードは神がかっているがドキュメントは壊滅的な人。設計は天才的だが実装は遅い人。トラブルシューティングは超人的だが新規開発には興味がない人。万能な人はいません。でも、チームとして万能であればいいです。弱みはチームでカバーする弱みを克服する必要がないと言っているわけではありません。弱みを自分で克服するか、チームでカバーするかを選んでください、と言っています。弱みを無視していいかどうかは、3つの質問で判断できます。その弱みがないと仕事ができないか？ コミュニケーションが苦手でも、コードで結果を出せるなら問題ありません。ですが、リーダーを目指すなら、コミュニケーションは避けられません。その弱みをカバーする人がチームにいるか？ ドキュメントが苦手でも、得意な人がチームにいれば補完できます。その弱みを平均にする努力で、強みを伸ばす時間が失われないか？ 弱みを平均にするのに100時間かかるなら、その100時間で強みを突き抜けさせた方がいいです。3つとも「いいえ」なら、弱みの克服は後回しでいいです。チームでカバーできる弱みは、チームに任せてください。しかし、役割によって「致命的な弱み」は変わります。今の役割では問題なくても、次の役割では致命的になることがあります。上司と話し合ってください。「私はAが強みで、Bが弱みです。今の役割でBは致命的ですか。次の役割ではどうですか」と。「この人がいないと困る」状態を作るチームを勝たせる中で、「この人がいないと困る」という状態を作ってください。みんなが平均を目指すなら、平均的な人材は溢れます。「そこそこ何でもできる人」は大量にいます。だから、差別化できません。代わりはいくらでもいます。一方、「この分野なら誰にも負けない」と言える人は少ないです。少ないから、価値があります。「この人じゃないと困る」という状況を作れます。それが交渉力になります。「パフォーマンスチューニングなら〇〇さん」「あの複雑な仕様を理解しているのは〇〇さんだけ」「障害対応で真っ先に呼ばれるのは〇〇さん」——こういうポジションを取ってください。チームの中で、代替不可能な存在になってください。「何でもできる人」という便利なラベルを捨ててください。代わりに、「〇〇の問題ならあいつに聞け」という、組織内の検索ワードを確立してください。検索ワードがあれば、困っている人が自分を見つけてくれます。仕事が向こうからやってきます。その仕事で成果を出せば、また検索ワードが強化されます。この循環を作ってください。そして、自分に問うてみてください。あなたの強みをより伸ばすことが、どのように「チーム全体の勝率」に直結するか。個人の成長と、チームの勝利を結びつけて説明できるか。「私が〇〇を極めれば、チームは△△で勝てるようになります」と。この論理が説明できれば、強みを伸ばす時間を堂々と確保できます。これは「自分だけが得をする」話ではありません。チームが勝つために、自分の強みを最大限に活かすという話です。チームが勝ち、その中で自分が不可欠な貢献をしている。この状態が、評価につながります。上司は言えます。「あのプロジェクトが成功したのは、〇〇さんの△△があったからです」と。具体的な貢献があれば、評価会議で名前を出しやすいです。組織の論理と個人の論理を重ねる組織は「オールラウンダーになれ」と言います。でも、その言葉を額面通りに受け取らないでください。組織が本当に求めているのは、「チームが勝つこと」です。オールラウンダーを求めるのは、そのための手段に過ぎません。誰が抜けてもチームが回るように、リスクヘッジしたいだけです。だから、「チームを勝たせる」という目的を共有した上で、手段は自分で選んでください。オールラウンダーになることでチームに貢献できるなら、それでいいです。でも、強みを尖らせることでチームに貢献できるなら、それでもいいです。目的が達成されていれば、手段は問われません。「私はオールラウンダーではありません。でも、この分野では誰にも負けません。チームの勝利に、この強みで貢献します」と言える状態を作ってください。組織の論理と、個人の論理を、「チームを勝たせる」という一点で重ねてください。これが、構造を理解した上で頑張る、ということです。それでもダメならここまでやっても評価されないことがあります。そのときの判断基準を明確にしておきます。「正しく頑張った」の定義成果を言語化し、見せた1on1で評価基準と昇進に必要なことを確認した上司の目標、チームの目標に貢献した評価のタイムラインを意識して動いたフィードバックを受け入れ、行動を変えた上司以外のスポンサーも獲得しようとした強みで貢献し、弱みはチームでカバーしたこの7つを1年間やった上で、評価が変わらなければ、構造の問題です。2年以上待っても変わらないなら、個人の努力では覆りません。しかし、正直に書いておきます。運の要素は大きいです。この記事は、努力すれば報われるかのように書いてきました。でも、現実はそうじゃありません。良い上司に当たるかどうかは、運です。自分の強みを評価してくれる上司、対話に応じてくれる上司、スポンサーになってくれる上司。そういう上司に当たるかどうかは、自分ではコントロールできません。良いプロジェクトに配属されるかも、運です。成果が見えやすいプロジェクト、評価につながりやすい仕事。それに関われるかどうかは、タイミングと巡り合わせです。会社の業績も、運です。会社が成長していれば昇進枠は増えます。会社が停滞していれば枠は減ります。個人の努力とは関係ありません。この記事に書いたことを全部やっても、運が悪ければ評価されません。逆に、何もしなくても、運が良ければ評価されます。そういうことは、あります。私が評価されるようになったのも、運の要素が大きいです。良い上司に当たりました。良いプロジェクトに関われました。会社の業績が良かった時期に、たまたま成果を出せました。努力したのは事実ですが、運が良かったのも事実です。この記事は、「努力でコントロールできる部分」にフォーカスしています。でも、コントロールできない部分の方が大きいでしょう。運が悪いときに、「頑張り方が間違っている」と言われても、救いになりません。運が悪かった人に、私は何も言えません。「次は運が良いといいね」としか言えません。それは無責任でしょうが、本当のことです。見切るべき3つのパターン パターン  状況  対処  上司とのズレ  上司が重視するAと、自分が得意なBがズレている。対話しても埋まらない  異動するか、別のスポンサーを見つける  制度の破綻  年功序列、政治、声の大きい人が勝つ。チームが勝っても個人に還元されない  組織を変えるか、出るか  市場価値との乖離  外では高く評価されるスキルが、今の組織では価値がない  辞める 見切りの解像度を上げろ「組織を辞める」というより、「この人たちと働くことを辞める」と考えた方が正確だ。冒頭で書いた。「どの会社で働くか」より「どのチームで働くか」が大事だと。会社全体がダメなのか、今いるチームがダメなのか。この見極めは重要だ。この上司との関係は修復可能か？別のチームに移れば解決するか？この会社の「誰か」に働きかければ変わるか？上司以外にスポンサーになってくれる人はいるか？全部試して、全部無理だった。そのとき初めて「構造の問題」と言える。あなたが直面している「評価への不満」は、個人の努力で突破可能な「運用上の課題」か。それとも、組織のDNAに刻まれた「構造的な腐敗」か。この見極めが重要です。1つの判断材料があります。過去3年間で、あなたと同じような「正論を吐く優秀な人」がどのように去っていったか、そのパターンを分析してください。同じパターンが繰り返されているなら、構造の問題です。もう1つの判断材料があります。今の会社で「最も高く評価されている人」の振る舞いは、あなたが5年後に「なりたい姿」と重なるか。重ならないなら、この組織で評価されることに意味があるのか。経営陣が「評価制度の不備」を認識していながら変えないなら、それは彼らにとって「都合が良い」からでしょう。仕組みの問題か、人の問題か「評価制度を変えればいい」——そう思いがちです。でも、制度を変えても、運用する人が変わらなければ、結果は変わりません。本当の問題は、制度ではなく、人と人の関係性にあることが多いです。逆もあります。「この上司が悪い」と思っていても、制度が上司にそう振る舞わせている場合があります。上司も構造の中で動いています。上司を責めても、構造は変わりません。撤退は戦略だ構造的な問題がある場合、とっとと辞めてください。「変われない組織」には共通パターンがあります。正しく頑張っても報われない構造ができあがっています。仕事が見えなくなり、提案が通らなくなり、評価基準が不透明になり、変えようとする人が去っていきます。こうなった組織は、個人の努力では変えられません。見極めのサインあなたの組織がこの状態に陥っているかどうか、いくつかのサインがあります。「これ、誰の仕事？」という会話が週に何度もある障害を未然に防いでも誰も気づかない提案しても「今は優先度が低い」と言われ続ける「なぜこのプロセス？」に「昔からこう」と返ってくる「変えようとして辞めた人」の話をよく聞くチームが勝っても、個人の評価に反映されないこれらのサインが複数当てはまるなら、個人の努力で変えるのは難しいです。異動か転職を視野に入れてください。成功した組織ほど変われなくなる皮肉なことに、成功した組織ほど変われなくなります。「過去にこうやってうまくいった」という経験が、新しいやり方を排除します。成功体験が足かせになります。あなたが「この組織はおかしい」と感じるとき、それは正しいでしょう。組織は過去の成功に縛られて、新しい環境に適応できなくなっているのでしょう。その場合、あなた個人が変えられることは限られています。構造を変えるには、経営層が本気で取り組む必要があります。それがないなら、辞めてください。撤退は戦略である「おい、辞めるな」で書きました。短期ではなく長期で考えてください。信頼の貯金を積み上げてください。転職はリセットコストがかかります。でも、「長期で考えた結果、辞める」という判断もあります。1年間正しく頑張りました。構造を理解した上で動きました。対話を試みました。スポンサーを探しました。チームを勝たせようとしました。それでも変わりませんでした。組織が考える力を失っていて、経営層も本気で取り組む気配がありません。そういう状況なら、辞めることが長期的に正しい判断です。それは逃げではありません。戦略的撤退です。交渉してダメなら去るしかし、順番を間違えないでください。まず交渉してください。評価に納得がいかないなら、上司に聞いてください。「何をすれば評価されるのか」を明確にしてください。構造に問題があると思うなら、提案してください。改善案を出してください。異動を申し出てください。別のスポンサーを探してください。やれることをやってください。交渉するとき、あなたの言葉に「重み」はありますか。社外の市場価値を把握していますか。「いつでも外に出られる」という自信が、言葉に重みを与えます。交渉するなら、「何を、いつまでに、どう変えてほしいか」を具体的に伝えてください。そして、交渉が決裂した際の「プランB」は準備していますか。プランBがないまま交渉しても、本気度が伝わりません。それでダメなら、去ってください。この順番が大事です。交渉せずに辞めるのは、ただの逃げです。でも、交渉した上で辞めるのは、戦略です。「やることはやった。それでも変わらなかった」という事実が、あなたの判断を正当化します。次の面接で「なぜ辞めたのか」と聞かれたとき、「改善を試みたが、構造的に無理だった」と言えます。というか、交渉するというのは、それぐらいデカいことです。「評価に納得いきません」「異動させてください」「この構造を変えてください」——これを口にした時点で、あなたは覚悟を示しています。ダメだったら去る覚悟を。交渉とは、そういう重さを持つ行為です。軽い気持ちで切り出すものではありません。だからこそ、ダメだったときに居座るのは筋が通りません。覚悟を示しておいて、結果が出たら何もしない。それは自分の言葉を裏切ることです。全てはトレードオフです。残るコストと、去るコストがあります。残れば、信頼の貯金を積み上げられます。人間関係もリセットされません。でも、構造が変わらないなら、消耗し続けます。3年後も5年後も同じ愚痴を言っている自分が見えます。去れば、リセットコストがかかります。また一から信頼を築く必要があります。新しい環境に適応するストレスもあります。でも、正しく評価される構造の中で働ける可能性があります。どちらが正解か、一般論では言えません。あなたの状況によります。あなたの価値観によります。あなたのキャリアのフェーズによります。ただ、1つだけ言えます。交渉してダメだったのに居座り続けるのは、最悪の選択です。構造が変わらないと分かりました。自分の力では変えられないと確認しました。それでも残る。それは「判断を放棄している」だけです。答えは出ているのに、行動しません。時間だけが過ぎていきます。交渉してください。ダメなら去ってください。それがトレードオフを引き受けるということです。辞める前に確認することしかし、辞める前に確認すべきことがあります。1. 本当に構造の問題か「評価されない」と感じるとき、構造のせいにしたくなります。自分のせいではない。組織が悪い。そう思いたいです。でも、まず自分を疑ってください。ちゃんと見せていたか。対話していたか。チームを勝たせようとしていたか。強みで貢献していたか。これらを本当にやった上で、評価されなかったのか。構造のせいにするのは、自分の責任を回避できて楽です。でも、構造のせいにして辞めても、次の組織で同じことが起きるでしょう。2. 異動で解決できないか「組織を辞める」前に、「チームを辞める」を検討してください。別のチームに移れば解決することがあります。上司が変われば、評価が変わることがあります。別のスポンサーがいれば、状況が変わることがあります。会社全体がダメなのか、今いるチームがダメなのか。この見極めは重要です。3. 辞めた後に何があるか辞めることを決める前に、辞めた後の絵を描いてください。「ここから出たい」だけでは、どこに行っても同じ問題にぶつかります。次の組織で何をしたいのか。どんな環境なら自分の強みを活かせるのか。どんなチームなら自分が貢献できるのか。それが見えてから、辞めてください。大企業にいるなら、よく考えろあなたは自分が持っているものを過小評価しています。 安定した給与、福利厚生、開発環境、ネームバリュー。これらが「普通」に感じられています。不満ばかりが目につきます。でも、構造的な問題——評価制度の限界、政治、見えない仕事の軽視——は、大企業だから存在するのではありません。組織という形態が持つ宿命です。スタートアップでも20人を超えれば政治が生まれます。50人を超えれば部門間の壁ができます。環境を変えても、構造は変わりません。大企業を辞める前に、まず異動を検討してください。辞めなくても環境を変えられます。サバンナで戦う覚悟があるなら飛び出せばいいです。覚悟がないなら、城壁の中で戦略を練ってください。辞めると決めたら辞めると決めたら、長居しないでください。「あと半年頑張ってみよう」「プロジェクトが終わるまで」と思いがちです。でも、辞めると決めた組織で頑張り続けるのは、消耗します。モチベーションが上がりません。パフォーマンスが落ちます。評価が下がります。悪循環にハマります。辞めると決めたら、次を探し始めてください。時間をかけすぎないでください。辞めても何も変わらないだろう正直に言えば、辞めても何も変わらないでしょう。次の組織も、同じような問題を抱えているでしょう。評価制度に限界があります。上司との相性があります。政治があります。これは、どの組織にもあります。というかそれはあなたの問題でもあります。そこに向き合ったほうが良いです。転職は、問題を解決する魔法ではありません。環境を変えるだけです。新しい環境で、同じ問題に別の形でぶつかることもあります。だから、辞める前に、「この問題は環境を変えれば解決するのか、自分が変わらないと解決しないのか」を考えてください。環境の問題なら、辞めてください。自分の問題なら、自分を変えてください。両方なら、両方やってください。届かない人へここまで書いてきて、立ち止まります。「見せてください」「対話してください」「チームを勝たせてください」——私はそう書きました。構造を理解した上で、その中でうまくやってください、と。でも、この記事が届かない人がいます。頑張れない人がいる「頑張り方を変えてください」と言いました。ですが、もう頑張る余力がない人はどうするのか。すでに消耗している人。毎日出社するだけで精一杯の人。週次報告に「何が難しかったか」を1文追加する気力すらない人。1on1で交渉する心理的余裕がない人。彼らに「見せてください」「対話してください」と言っても、届きません。むしろ、「お前の頑張りは間違っている」と告げることになります。追い詰めることになります。「体調管理は評価に直結する」と書きました。事実です。でも、体調を管理できない人がいます。慢性疾患を抱えている人。精神疾患と付き合っている人。家庭の事情で睡眠時間を削らざるを得ない人。介護や育児で「安定して稼働」できない人。彼らは、努力が足りないのではありません。構造が彼らを排除しているのです。「アピールが苦手なら、存在しないのと同じだ」と書きました。ですが、アピールが苦手な人は、苦手だから苦労しています。「苦手を克服してください」と言うのは簡単です。でも、克服できないから苦手なのです。内向的な人、言語化が苦手な人、自己主張に強い抵抗がある人。彼らに「見せてください」と言っても、できないものはできません。この記事に書いた「正しい頑張り方」ができる人は、すでに恵まれています。対話する余力があります。アピールする能力があります。安定して稼働できる体があります。それらを持っている時点で、スタートラインが違います。私は、持っている側でした。だから、この記事を書けました。持っていない人に、同じことを求めるのは、傲慢でしょう。「頑張らない」という選択肢「辞めないなら頑張ってください」と書きました。ですが、「辞めないけど頑張らない」という選択肢もあります。昇進を追わない。評価を気にしない。自分のペースで働く。それは「諦め」ではありません。評価ゲームから意識的に降りるという戦略です。評価制度は、組織が作ったゲームに過ぎません。そのゲームに参加するかどうかは、自分で選べます。「昇進しなければ給料が上がらない」と言うでしょう。ですが、昇進のために消耗して、心身を壊したら、給料どころではありません。評価を追いかけて、本来の仕事の楽しさを失ったら、何のために働いているのか分からなくなります。評価されなくても、良い仕事はできます。障害を未然に防いだ本人は、その価値を知っています。上司が知らなくても、自分は知っています。それで十分だと思える人もいます。もし今の評価ゲームが「勝てない設定」であるなら、「頑張らない」ことで確保したエネルギーを、どこに投資するか考えてみてください。社内の評価を「食い扶持を維持する程度」にコントロールし、余ったリソースで社外での市場価値を育てることは可能か。今の場所を「人生のゴール」ではなく「ベースキャンプ」と定義し直してください。もちろん、評価されないと生活に困ることもあります。だから、全員にこの選択肢を勧めているわけではありません。ただ、「頑張らない」という選択肢もあることを、知っておいてほしいです。評価ゲームに全てを賭ける必要はありません。降りてもいいです。構造を変えるという選択肢「仕組みは変えられない。自分は変えられる」と書きました。ですが、本当に変えられないのか。「見えない仕事」を評価する仕組みを作った組織はあります。障害を未然に防いだことを、きちんと評価する制度を設計した会社はあります。短期成果だけでなく、長期的な貢献を測る仕組みを導入したチームはあります。変えられないのではありません。変えようとする人がいなかっただけでしょう。変えようとした人が、諦めて辞めていっただけでしょう。この記事では、構造を変える方法は書きませんでした。正直、私にはその経験がないからです。私は構造の中で適応する方を選んできました。変えようとしたこともありますが、うまくいきませんでした。だから、「変えてください」とは言えませんでした。でも、適応することが唯一の選択肢ではありません。もしあなたに発言力があるなら、提案してみてもいいです。評価制度を変える提案。見えない仕事を可視化する仕組み。非機能要件を評価する基準。障害を未然に防いだことを記録するプロセス。変わらないでしょう。でも、変わるでしょう。少なくとも、試さなければ分かりません。「構造を理解した上で適応する」は、1つの戦略です。でも、「構造を理解した上で変えようとする」も、1つの戦略です。どちらを選ぶかは、あなた次第です。多様な「正解」があるこの記事は、「評価される頑張り方」を書きました。ですが、それが唯一の正解ではありません。評価を追いかけて、昇進して、影響力を持つ。それも正解です。評価を諦めて、自分のペースで働く。それも正解です。構造を変えようとして、組織を動かす。それも正解です。評価ゲームから降りて、別の働き方を選ぶ。それも正解です。どれが正しいかは、あなたの状況によります。あなたの価値観によります。あなたの人生のフェーズによります。「辞めないなら頑張ってください」と私は書きました。でも、「辞めないけど頑張らない」でもいいです。「辞めないで、構造を変えようとする」でもいいです。この記事が、あなたを追い詰めるためにあるのではありません。選択肢を増やすためにあります。そう思いたいです。おわりに先週の記事に、思った以上の反響がありました。「辞めないことにしました」という連絡をくれた人たちが、どんな人で今どうしているのか、私は知りません。うまくいっているといいです。うまくいっていなくても、間違えながら何とかやっているといいです。この文章を書き終えました。書いている間、何度か手が止まりました。こんなことを書いて、誰かの役に立つのだろうか。自分が経験したことを、他人に押し付けているだけではないか。答えは出ませんでした。出ないまま、最後まで書きました。明日からできることはあります。週次報告に「何が難しかったか」を1文足す。1on1で「昇進に必要なこと」を聞く。カレンダーに「評価2ヶ月前」をマークする。見えない仕事を、見える形にする。それだけで、何かが変わるでしょう。たぶん、私は来週の週次報告で「何が難しかったか」を書くのを忘れます。上司を敵認定しそうになります。また同じ愚痴を居酒屋で言います。間違えたら直せばいいです。間違えていることに気づいているなら、まだやれます。たぶん。「評価が上がりました」でも、「やっぱり辞めました」でも、「まだ間違え続けています」でも、「頑張るのをやめました」でも。どれでもいいです。どれも、選んだ道を歩いている証拠だと思うから。正解かどうかは、分かりません。私がやってきたことが正しかったかどうかも、分かりません。分かるのは、ずっと後になってからです。おい、辞めないなら頑張ってください。頑張り方を間違えないでください。——と、ここまで書いてきました。でも、最後に付け加えておきます。頑張れないなら、頑張らなくていいです。降りてもいいです。休んでもいいです。それも、1つの選択です。私も、まだ間違え続けています。それでいいのだと思います。続編も書きました。syu-m-5151.hatenablog.com参考書籍外資系コンサルの仕事の進め方: 実践の場で使える問題解決の基盤スキル作者:金地 毅,田辺 元,柳田 拓未東洋経済新報社Amazon私文ホワイトカラーが AI・コンサルに仕事を奪われない働き方戦略作者:株式会社板橋　東京中央支店かんき出版AmazonSOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazonソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社AmazonTHE CULTURE CODE 最強チームをつくる方法作者:ダニエル・コイル,楠木建かんき出版Amazonセンスメイキング――本当に重要なものを見極める力作者:クリスチャン・マスビアウプレジデント社Amazon心眼：あなたは見ているようで見ていない作者:クリスチャン・マスビアウ Christian Madsjergプレジデント社Amazon組織と働き方の本質　迫る社会的要請に振り回されない視座 (日本経済新聞出版)作者:小笹芳央日経BPAmazon［新版］組織行動の考え方―個人と組織と社会に元気を届ける実践知作者:金井 壽宏,高橋 潔,服部 泰宏東洋経済新報社Amazon「組織と人数」の絶対法則―人間関係を支配する「ダンバー数」のすごい力作者:トレイシー・カミレッリ,サマンサ・ロッキー,ロビン・ダンバー東洋経済新報社Amazonチームの力で組織を動かす 〜ソフトウェア開発を加速するチーム指向の組織設計作者:松本 成幸技術評論社Amazon恐れのない組織――「心理的安全性」が学習・イノベーション・成長をもたらす作者:エイミー・C・エドモンドソン,村瀬俊朗英治出版Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazon組織が変わる――行き詰まりから一歩抜け出す対話の方法2 on 2作者:宇田川 元一ダイヤモンド社Amazon多様性の科学作者:マシュー・サイドディスカヴァー・トゥエンティワンAmazon新　失敗学　正解をつくる技術作者:畑村洋太郎講談社Amazon企業変革のジレンマ　「構造的無能化」はなぜ起きるのか (日本経済新聞出版)作者:宇田川元一日経BPAmazon「わかりあえない」を越える――目の前のつながりから、共に未来をつくるコミュニケーション・NVC作者:マーシャル・B・ローゼンバーグ海士の風Amazonみんな違う。それでも、チームで仕事を進めるために大切なこと。作者:岩井俊憲ディスカヴァー・トゥエンティワンAmazonなぜ働く？　誰と働く？　いつまで働く？　限られた人生で後悔ない仕事をするための20の心得作者:有山 徹アスコムAmazon問いかける技術――確かな人間関係と優れた組織をつくる作者:エドガー・H・シャイン英治出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OAuth2認証をE2Eテストしたら、5つのバグが出てきた話]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/11/064311</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/11/064311</guid>
            <pubDate>Sat, 10 Jan 2026 21:43:11 GMT</pubDate>
            <content:encoded><![CDATA[はじめに認証が動いた。だがそれは始まりに過ぎなかった。前回の記事では、Next.jsでOry Hydra認証を実装した。OAuth2認可コードフロー、Cookie管理、ID Token署名検証、マルチテナント認証について解説した。前提知識: この記事は前回の記事の続編です。Next.jsでのOAuth2認証フロー実装を理解している前提で進めます。Next.jsでOry Hydra認証を実装する ― マルチテナントSaaSでの実践 - じゃあ、おうちで学べる今回は、実装した認証フローを検証する。Playwright MCPを使ったE2Eテスト、発見した5つのバグ、RBACの検証、そしてベストプラクティスとの比較までを一気に解説する。Playwright MCPによるE2Eテストもう本当に10年くらい前は「E2Eテストなんて、デモ前に手動で確認すれば十分でしょ」と思っていた。仕事でフロントエンド書いたことなかったので…。今の自分から言わせてもらえば、それは個人の能力を過信している。あとはフロントエンドのテストの大変さを軽く見ている。OAuth2フローのE2Eテストは手動では破綻する。複数のリダイレクト、Cookie管理、セッション状態の確認——これらを毎回手動で確認するのは、人間の注意力の限界を超えている。「今日は疲れていたから見落とした」で本番障害が起きるのは、個人の問題ではなく構造的な失敗だ。人間に頼らない仕組みを作る必要がある。Claude CodeとPlaywright MCPの組み合わせPlaywright MCPは、LLMがブラウザを直接操作できるModel Context Protocol（MCP）サーバーだ。Claude Codeと組み合わせることで、自然言語でE2Eテストを実行できる。従来のPlaywrightとの違いは、スクリプトを書かずにテストできる点だ。# セットアップ（プロジェクトごとに一度だけ）claude mcp add --transport stdio playwright --scope project -- npx -y @playwright/mcp@latest.mcp.jsonが生成される：{  "mcpServers": {    "playwright": {      "type": "stdio",      "command": "npx",      "args": ["-y", "@playwright/mcp@latest"]    }  }}実際のテスト実行例Claude Codeで以下のように指示する：Playwright MCPでOAuth2フローをE2Eテストしてください：1. http://localhost:3001/ にアクセス2. Sign Inをクリック3. demo@example.com / password123 でログイン4. Consentで Allow をクリック5. ダッシュボードが表示されることを確認6. スクリーンショットを取得Claude Codeは以下のツールを順次実行する： ステップ  MCPツール  結果  1  browser_navigate  ホームページ表示  2  browser_click (ref=e10)  Hydra認可エンドポイントへリダイレクト  3  browser_fill_form  ログインフォーム入力完了  4  browser_click (Sign In)  Consent画面へリダイレクト  5  browser_click (Allow)  トークン交換・フロントエンドへリダイレクト  6  browser_take_screenshot  エビデンス取得 ARIA Snapshotの活用Playwright MCPの特徴は、DOMではなくアクセシビリティツリーでページ構造を表現する点だ。各要素にはref=eXX形式の参照IDが付与される：- banner:  - navigation:    - link "Sign In" [ref=e10] [cursor=pointer]:      - /url: /api/auth/loginこのref=e10を使ってクリック対象を指定する。セレクタの管理が不要になり、UIの変更に強いテストが書ける。従来のE2Eテストとの比較 項目  従来のPlaywright  Playwright MCP  テスト作成  スクリプト記述が必要  自然言語で指示  セレクタ管理  CSSセレクタ/XPath  ARIA参照ID  リダイレクト追跡  手動でwait設定  自動追跡  デバッグ  ログ/スクリーンショット  対話的に確認可能  再現性  高（スクリプト化）  中（LLMに依存） Playwright MCPは「探索的テスト」に向いている。本番のCIには従来のPlaywrightスクリプトを使い、開発中の手動確認をPlaywright MCPで効率化する、という使い分けがよさそうだ。E2Eテストで発見した5つのバグPlaywright MCPとシェルスクリプトによるE2Eテストを実行した結果、5つの重要なバグを発見・修正した。OAuth2+マルチテナント構成の複雑さを示す良い事例だ。バグ1：CORS設定の欠如症状：フロントエンド（localhost:3001）からバックエンド（localhost:3000）へのAPIリクエストがブロックされる原因：Axumルーターにtower-httpのCorsLayerが設定されていなかった修正（src/main.rs）：use tower_http::cors::{Any, CorsLayer};let app = Router::new()    // ... routes ...    .layer(        CorsLayer::new()            .allow_origin(Any)            .allow_methods(Any)            .allow_headers(Any),    )教訓：これは個人の注意力の問題ではない。フロントエンド・バックエンド分離構成では、CORSは「設定を忘れると動かない」構造になっている。チェックリストに入れる。プロジェクトテンプレートに含める。人間の記憶に頼らない仕組みを作る。「動かない」の原因がCORSだと気づくまでに時間がかかることがある。エラーメッセージが分かりにくいからだ。ブラウザのコンソールを見る習慣をつけるしかない。詳細はMDN: CORSを参照。バグ2：Cookieパース時のJWTトークン切り詰め症状：認証後のAPIリクエストで401エラーが発生原因：.split("=")[1]でCookieを取得すると、base64エンコードされたJWTの=パディング文字で切れてしまう// ❌ 危険：JWTが途中で切れるconst token = document.cookie  .split("; ")  .find((row) => row.startsWith("auth_token="))  ?.split("=")[1];  // "ory_at_abc...def=" → "ory_at_abc...def" で切れる// ✅ 正しい：トークン全体を取得const cookieRow = document.cookie  .split("; ")  .find((row) => row.startsWith("auth_token="));const token = cookieRow ? cookieRow.substring("auth_token=".length) : null;教訓：JWTは必ずbase64パディング（=）を含む可能性がある。文字列操作でトークンを扱う時は要注意。バグ3：HydraトークンとJWTの不一致症状：フロントエンドからのAPIリクエストで401エラー。curlでJWTを直接送ると成功する。原因：- フロントエンドはHydra発行のアクセストークン（ory_at_...形式）を使用- バックエンドは自前のJWTのみ対応していた修正（src/middleware/auth.rs）：// JWT検証を試み、失敗したらHydraイントロスペクションにフォールバックlet claims = match state.jwt.verify_access_token(token) {    Ok(claims) => claims,    Err(_) => {        // Hydra Admin APIでトークンを検証        let introspection = state.hydra.introspect_token(token).await?;        // IntrospectionResponseからClaimsに変換        Claims::from(introspection)    }};教訓：OAuth2プロバイダー（Hydra）のトークンと自前JWTの両方をサポートするか、どちらか一方に統一するか、設計段階で決めておくべきだった。バグ4：テナント抽出ミドルウェアの欠如症状：テナントAPI（/api/v1/tenant/*）で「No tenant context」エラー原因：tenant_apiルーターにextract_tenantミドルウェアが適用されていなかった修正（src/main.rs）：let tenant_api = Router::new()    // ... routes ...    .layer(axum_middleware::from_fn_with_state(        state.clone(),        middleware::require_auth,    ))    .layer(axum_middleware::from_fn_with_state(        state.clone(),        middleware::extract_tenant,  // 追加    ));教訓：ミドルウェアの適用漏れは見つけにくい。各ルートグループに必要なミドルウェアをリスト化しておくとよい。バグ5：X-Tenant-Slugヘッダーの欠如症状：ローカル開発環境でテナントが識別できない原因：- 本番環境ではサブドメイン（tenant-a.example.com）でテナント識別- ローカル開発ではlocalhost:3001のためサブドメインが使えない- フロントエンドがX-Tenant-Slugヘッダーを送信していなかった修正（frontend/src/lib/api.ts）：class ApiClient {  private tenantSlug: string = "test-shop"; // デフォルトテナント  private async fetch<T>(endpoint: string, options: RequestInit = {}): Promise<T> {    const headers: HeadersInit = {      "Content-Type": "application/json",      "X-Tenant-Slug": this.tenantSlug,  // 追加      ...options.headers,    };    // ...  }}教訓：マルチテナントのテナント識別は、サブドメイン方式とヘッダー方式の両方をサポートしておくとローカル開発が楽になる。E2Eテスト実行結果修正後のOAuth2フロー完全テスト：=== DONADONA E2E Test v4 ===1. Starting OAuth2 Flow...   Login Challenge: LuAyzZfWTX03DnVcFC1xu0A-rntZcx...2. Submitting Login (demo@example.com)...   Consent Challenge obtained3. Approving Consent...   Final: http://localhost:3001/callback?code=ory_ac_d9jRSkWUb1YXm...4. Token Exchange...   Access Token: ory_at_dxBjsXjmRvMuTcSJercIxT_Kq2nUIR6OrUhdBEcEZIg...5. Testing API Endpoints...   Engineers Count: 36. Backend Verification:   slug_from_header=Some("test-shop")   Hydra token introspection successful: sub=Some("3767fa6a-...")============================================   E2E Test PASSED - All fixes verified!============================================複数アカウントでのRBAC検証E2Eテストの最後に、異なるロールのアカウントでログインして、役割ベースアクセス制御（RBAC）が正しく機能しているかを検証した。テスト結果のサマリー以下は修正前のテスト結果だ。platform_adminがDashboardで403を返すなど、明らかな異常がある。詳細は後述する。 アカウント  ロール  ナビゲーションメニュー  アクセス可能ページ  demo@example.com  platform_admin  全メニュー  Dashboard(403)、その他未テスト  manager@example.com  manager  全メニュー  Dashboard, Incidents, Projects, Engineers, Recruitment, Leaderboard  sato@example.com  engineer  制限メニュー  Dashboard, Incidents, Projects, Leaderboard  reporter@example.com  reporter  制限メニュー  すべてAccess Denied 発見1：フロントエンドとバックエンドのデータ不一致テストアカウント一覧を表示するフロントエンドのホームページには、こう書いてあった：Reporter | customer@example.com | Report incidents onlyしかし実際にcustomer@example.comでログインすると、ヘッダーにはengineerと表示された。データベースとフロントエンドの表示が不一致だった。正しいReporterアカウントはreporter@example.comだった。発見2：ロールごとのメニュー制御Playwright MCPのARIAスナップショットで、ロールごとのナビゲーションメニューの違いを確認できた。Manager（manager@example.com）のメニュー：- link "Dashboard" [ref=e10]- link "Incidents" [ref=e11]- link "Projects" [ref=e12]- link "Engineers" [ref=e13]- link "Recruitment" [ref=e14]- link "Leaderboard" [ref=e15]Engineer（sato@example.com）のメニュー：- link "Dashboard" [ref=e10]- link "Incidents" [ref=e11]- link "Projects" [ref=e12]- link "Leaderboard" [ref=e13]# Engineers, Recruitmentが表示されない発見3：Reporterの「何もできない」状態reporter@example.comでログインして各ページにアクセスすると、すべて「Access Denied」が表示された。CLAUDE.mdによると、Reporterは「Report incidents only」という説明だったが、実際にはインシデントページすら見られない。これは設計ミスだった。修正が必要だ。フロントエンドとバックエンドの権限制御問題の根本原因Reporterロールがすべてのページでアクセス拒否されていた原因は、Next.jsのmiddleware.tsにあった：// 修正前：ReporterはADMIN_PATHSに含まれていないconst ADMIN_PATHS = ["/dashboard", "/incidents", "/projects", "/engineers", "/recruitment", "/leaderboard"];// ロールチェック：platform_admin, manager, engineerのみ許可if (isAdminPath && !["platform_admin", "manager", "engineer"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}修正内容// 修正後：ADMIN_PATHSから/incidentsを分離し、REPORTER_PATHSを新設const ADMIN_PATHS = ["/dashboard", "/projects", "/engineers", "/recruitment", "/leaderboard"];const REPORTER_PATHS = ["/incidents"];  // Reporter専用パス// Reporter paths - reporter, engineer, manager, platform_admin can accessconst isReporterPath = REPORTER_PATHS.some((p) => pathname.startsWith(p));if (isReporterPath && !["platform_admin", "manager", "engineer", "reporter"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}// Admin paths - platform_admin, manager, engineer can access (not reporter)const isAdminPath = ADMIN_PATHS.some((p) => pathname.startsWith(p));if (isAdminPath && !["platform_admin", "manager", "engineer"].includes(role)) {  return NextResponse.redirect(new URL("/?error=unauthorized", request.url));}これで権限階層が明確になった： パス  platform_admin  manager  engineer  reporter  /tenants  ✅  ❌  ❌  ❌  /dashboard  ✅  ✅  ✅  ❌  /incidents  ✅  ✅  ✅  ✅  /projects  ✅  ✅  ✅  ❌ 多層防御の実装「フロントエンドで権限チェックすればいい」という意見と、「バックエンドだけでやるべき」という意見がある。どちらも正しく、どちらも不十分だ。フロントエンドだけでは、攻撃者がcurlで直接APIを叩けば突破される。バックエンドだけでは、権限のないユーザーが画面を見てから「アクセス拒否」されるUXになる。答えは「両方やる」——多層防御と呼ばれる考え方だ。城の防壁が一重ではなく多重であるように、セキュリティも複数のレイヤーで守る。フロントエンドのmiddleware.tsだけでは不十分だ。攻撃者はフロントエンドを完全にバイパスできる：# フロントエンドを経由せずにAPIを直接叩けるcurl -s http://localhost:3000/api/v1/tenant/incidents \  -H "Authorization: Bearer $TOKEN" \  -H "X-Tenant-Slug: test-shop"Rustバックエンド（Axum）では、権限制御が複数のレイヤーで行われている：レイヤー1：require_auth（認証） - トークンが有効かどうかをチェックレイヤー2：extract_tenant（テナント抽出） - X-Tenant-Slugヘッダーからテナントを特定レイヤー3：ハンドラー内のロールチェック - 特定の操作でロールをチェックpub async fn assign_incident(/* ... */) -> Result<Json<IncidentWithStatus>, AppError> {    let role = claims.get_role();    if !role.can_manage_team() {        return Err(AppError::Forbidden(            "Only managers can assign incidents".to_string(),        ));    }    // ...}多層防御が正解だ： レイヤー  役割  目的  フロントエンド middleware  早期リダイレクト  UX向上、不要なリクエスト削減  バックエンド require_auth  認証チェック  不正アクセス防止  バックエンド ハンドラー  操作ごとの認可  きめ細かい権限制御 ベストプラクティスとの比較この実装が業界のベストプラクティスにどれだけ準拠しているかを評価する。OWASP Top 10 2025との比較OWASP Top 10 2025でBroken Access Controlが1位を維持している。 OWASP推奨事項  準拠状況  実装詳細  サーバーサイドでのアクセス制御  ✅ 準拠  Axumミドルウェアで全APIを保護  デフォルト拒否  ✅ 準拠  未認証リクエストは全て拒否  アクセス制御の再利用  ✅ 準拠  require_authを全ルートで共有  レコード所有権の検証  ⚠️ 部分的  テナント分離は実装、リソース単位は未実装  アクセス制御失敗のログ  ⚠️ 部分的  tracingでログ出力、アラートは未実装  レート制限  ❌ 未実装  APIにレート制限なし  JWTの不正利用防止  ✅ 準拠  Hydraによるトークン検証  セキュリティヘッダ  ⚠️ 部分的  HSTS, X-Frame-Options, X-Content-Type-Optionsの設定が必要  入力値バリデーション  ✅ 準拠  サーバーサイドでバリデーション実施 Next.jsセキュリティガイドラインとの比較Next.js Authentication Guideは、認証に関する重要な警告を含んでいる。 Next.js推奨事項  準拠状況  実装詳細  Middlewareだけに依存しない  ✅ 準拠  バックエンドでも認証チェック  Data Access Layer (DAL)の使用  ⚠️ 部分的  サービス層で分離、専用DALなし  HttpOnly Cookieの使用  ⚠️ 部分的  auth_tokenは非HttpOnly Next.jsチームは「middlewareは認証に安全ではない」と警告している。この多層防御は、CVE-2025-29927のようなmiddlewareバイパス脆弱性への対策にもなる。RBACパターンとの比較 RBACベストプラクティス  準拠状況  実装詳細  バックエンドでのポリシー強制  ✅ 準拠  ハンドラー内でロールチェック  フロントエンドはUI適応のみ  ✅ 準拠  メニュー表示/非表示で対応  権限キャッシュ  ❌ 未実装  毎リクエストでHydra呼び出し  中央集権的ポリシー管理  ⚠️ 部分的  定義は分散している  ロール階層の明確化  ✅ 準拠  4段階のロール階層を定義 総合評価 評価軸  スコア  コメント  OWASP Top 10 2025  7/10  基本的なアクセス制御は準拠、レート制限が不足  Next.js Security  8/10  多層防御を実装、HttpOnly Cookieが部分的  RBAC Patterns  7/10  フロントエンド/バックエンド分離は適切、権限定義が分散 強み：多層防御の実装：フロントエンド + バックエンド + ハンドラーの3層テナント分離：PostgreSQLスキーマレベルでのデータ分離OAuth2標準準拠：Ory Hydraによる標準的なOAuth2/OIDC実装トークン検証の二重化：自前JWT + Hydraイントロスペクションのフォールバック弱み：正直に言えば、見落としがあるかもしれない。セキュリティの評価は、「問題がない」ことを証明できない。見つかっていないだけかもしれない。だから、この記事を読んで「これで完璧だ」と思わないでほしい。OWASP Top 10のチェックリストを自分で回して、この記事で触れていない項目を確認してほしい。それを前提に、現時点で認識している弱みを列挙する。レート制限なし：DoS攻撃への脆弱性権限定義の分散：フロントエンドとバックエンドで定義が重複権限キャッシュなし：毎リクエストでHydraに問い合わせ監査ログの不足：アクセス制御失敗のアラート機能なし改善ロードマップ優先度順に改善すべき項目： 優先度  項目  工数  効果  高  レート制限の追加  小  DoS防止、OWASP準拠  高  監査ログとアラート  中  インシデント検出  高  セキュリティヘッダの追加  小  HSTS, X-Frame-Options, X-Content-Type-Options  中  権限定義の一元化  中  保守性向上  中  権限キャッシュ（Redis）  中  パフォーマンス向上  中  Cookie Prefix（__Host-）の導入  小  Cookie属性の強制  低  PKCE導入  小  認可コード横取り防止  低  HttpOnly Cookie化  中  XSS対策強化 // 改善案：tower-governor等でレート制限を追加use tower_governor::{governor::GovernorConfigBuilder, GovernorLayer};let governor_conf = GovernorConfigBuilder::default()    .per_second(10)    .burst_size(50)    .finish()?;let app = Router::new()    // ...    .layer(GovernorLayer { config: governor_conf });まとめOAuth2 + マルチテナントの認証システム実装を通じて学んだこと「動く」と「正しく動く」は違う：ログインできても、APIが動くとは限らない。APIが動いても、全ロールで正しく動くとは限らない。全ロールで動いても、攻撃に耐えるとは限らない。5つのバグすべてが、「ログインできた」の後に発見されたE2Eテストは必須：すべてユニットテストでは発見できなかった多層防御が重要：フロントエンドだけ、バックエンドだけでは不十分全ロールで検証する：「ログインできた」だけでは不十分ベストプラクティスとのギャップを把握する：何ができていて、何が不足しているかを明確にする認証は地味だが重要だ。インシデント対応のように緊張感もないし、新機能開発のような達成感もない。でも、認証が崩れたときの被害は、他のどの機能障害よりも大きい。過去に見た事例では、セッション管理の不備で全ユーザーのデータが漏洩した。復旧に数ヶ月、信頼回復に1年以上かかった。地味なものほど、丁寧にやる。例えば、この記事で示したE2Eテスト、全ロールでの検証、ベストプラクティスとの比較を、リリース前に必ず行う。それがインフラを支える人間の流儀だ。派手な仕事は誰でも丁寧にやる。地味な仕事を丁寧にやれるかどうかが、プロとアマチュアの違いだと思っている。「ログインできる」は最低条件であり、「安全にログインできる」「快適にログインできる」「問題が起きたときに追跡できる」まで含めて、初めて「認証が実装できた」と言える。この認証実装は完成ではなく、継続的に改善していく起点だ。半年後、1年後に見直したとき、「あの時の判断は正しかったか」を検証できるように、今回の記事を残しておく。次回予告ここまでの4記事で、OAuth2認可サーバー（Hydra）+ 自前認証プロバイダー（Rust）+ フロントエンド（Next.js）の構成が完成した。E2Eテストも通り、RBACも検証できた。しかし、レビューコメントが届いた。「パスワードリセット機能は？」「MFA対応の予定は？」全部、自分で実装するのか？——次回は、Ory Kratosを導入して認証機能を委譲する方法を解説する。syu-m-5151.hatenablog.com参考資料E2EテストPlaywright MCP - LLMがブラウザを操作するためのMCPサーバーModel Context Protocol (MCP) - LLMと外部ツールを接続するプロトコルOry HydraOry Hydra Documentation - Ory Hydra公式ドキュメントToken Introspection - トークンイントロスペクションAPILogin Flow - ログインフローの概念Consent Flow - 同意フローの概念OAuth2 Token Endpoint - トークンエンドポイントAPIリファレンスOAuth2 Revoke Token - トークン失効APIJWKS Endpoint - 公開鍵配信エンドポイントセキュリティガイドラインOWASP Top 10 2025 - Broken Access Control - アクセス制御の脆弱性OWASP Authorization Cheat Sheet - 認可チートシートOWASP Access Control Cheat Sheet - アクセス制御チートシートOWASP OAuth2 Cheat Sheet - OAuth2セキュリティチートシートAuth0 Token Storage - トークンストレージのベストプラクティスRFC 9700 - OAuth 2.0 Security Best Current Practice - OAuth2セキュリティBCPRBACOso: RBAC Role Based Access ControlLogRocket: Choosing the best access control model for frontendLeapcell: Implementing Robust RBAC Across Backend FrameworksNext.jsNext.js Authentication GuideNext.js MiddlewareCORSMDN: Cross-Origin Resource Sharing (CORS)tower-http CorsLayer]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Next.jsでOry Hydra認証を実装する ― マルチテナントSaaSでの実践]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/09/104616</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/09/104616</guid>
            <pubDate>Fri, 09 Jan 2026 01:46:16 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前回の記事では、RustでOry HydraのLogin/Consent Providerを実装した。5つのエンドポイント（GET/POST /login、GET/POST /consent、GET /logout）とHydra Admin APIの連携。Argon2idによるパスワードハッシュ、ユーザー列挙攻撃を防ぐテスト設計の話をした。前提知識: この記事は前回の記事の続編です。OAuth2認可コードフローの基礎知識と、Ory HydraのLogin/Consent Providerの役割を理解している前提で進めます。syu-m-5151.hatenablog.com今回は、そのバックエンドと連携するフロントエンドをNext.js 15で実装する。なぜフロントエンドも自分で書くのか。認証フローを端から端まで把握しておきたいからだ。ちなみにフロントエンドは専門外なのである程度は許してほしいです。NextAuth.jsやAuth0のSDKを使えば楽だが、ブラックボックスのまま本番に出すのは怖い。何かが壊れたとき、「ライブラリの中で何が起きているかわからない」では障害対応で詰むことがある。もちろん、最終的なゴールは「理解した上でライブラリを使う」ことだ。車輪の再発明を推奨しているわけではない。OAuth2/OIDCフローをブラウザ側でどう扱うか。Cookie管理の罠。マルチテナント環境での認証の複雑さ。実際に動かして気づいたことを記録する。OAuth2認可コードフロー：フロントエンドから見た流れまず全体像を把握しておく。┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐│   Browser   │     │   Next.js   │     │ Rust Backend│     │  Ory Hydra  ││  (User)     │     │  Frontend   │     │ (Provider)  │     │  (OAuth2)   │└──────┬──────┘     └──────┬──────┘     └──────┬──────┘     └──────┬──────┘       │                   │                   │                   │       │ 1. Login Click    │                   │                   │       │──────────────────>│                   │                   │       │                   │                   │                   │       │ 2. Redirect to    │                   │                   │       │    /oauth2/auth   │                   │                   │       │<──────────────────│                   │                   │       │                   │                   │                   │       │ 3. GET /oauth2/auth?client_id=...    │                   │       │──────────────────────────────────────────────────────────>│       │                   │                   │                   │       │ 4. Redirect to /login?login_challenge=xxx                 │       │<──────────────────────────────────────────────────────────│       │                   │                   │                   │       │ 5. GET /login                         │                   │       │──────────────────────────────────────>│                   │       │                   │                   │                   │       │ 6. Login Form     │                   │                   │       │<──────────────────────────────────────│                   │       │                   │                   │                   │       │ 7. POST /login (credentials)          │                   │       │──────────────────────────────────────>│                   │       │                   │                   │                   │       │                   │                   │ 8. Accept Login   │       │                   │                   │──────────────────>│       │                   │                   │                   │       │ 9. Redirect to /consent               │                   │       │<──────────────────────────────────────│                   │       │                   │                   │                   │       │ ... Consent Flow ...                  │                   │       │                   │                   │                   │       │ 10. Redirect to /callback?code=xxx    │                   │       │<──────────────────────────────────────────────────────────│       │                   │                   │                   │       │ 11. GET /callback │                   │                   │       │──────────────────>│                   │                   │       │                   │                   │                   │       │                   │ 12. Exchange code for tokens          │       │                   │──────────────────────────────────────>│       │                   │                   │                   │       │                   │ 13. Tokens (access, id, refresh)      │       │                   │<──────────────────────────────────────│       │                   │                   │                   │       │ 14. Set Cookie &  │                   │                   │       │     Redirect      │                   │                   │       │<──────────────────│                   │                   │このフローで重要なのは、フロントエンドは認証ロジックを持たないということだ。なぜか。フロントエンドのコードはユーザーのブラウザで動く。攻撃者は自由に改変できる。DevToolsを開けばJavaScriptは丸見えだし、リクエストも書き換えられる。認証ロジックをそこに置くということは、攻撃者に「好きに改ざんしていいですよ」と言っているようなものだ。認証情報の検証はすべてRustバックエンド（Login Provider）で行う。フロントエンドの役割は：認可エンドポイントへのリダイレクト開始コールバックで認可コードを受け取る認可コードをトークンに交換トークンをCookieに保存以降のAPI呼び出しでトークンを使用Next.js App Routerでの実装ディレクトリ構成frontend/src/├── app/│   ├── layout.tsx│   ├── page.tsx                    # ランディング│   ├── dashboard/page.tsx          # 認証後のダッシュボード│   ├── callback/page.tsx           # OAuth2コールバック│   └── api/auth/│       ├── login/route.ts          # ログイン開始│       ├── callback/route.ts       # コールバック処理│       └── logout/route.ts         # ログアウト├── components/│   └── shared/│       └── Header.tsx├── lib/│   └── api.ts                      # APIクライアント└── middleware.ts                   # 認証チェックログイン開始：認可エンドポイントへのリダイレクト// app/api/auth/login/route.tsimport { NextResponse } from "next/server";import crypto from "crypto";export async function GET(request: Request) {  const { searchParams } = new URL(request.url);  const returnTo = searchParams.get("returnTo") || "/dashboard";  // CSRF対策用のstate生成  const state = crypto.randomBytes(16).toString("hex");  // stateにリダイレクト先を含める（Base64エンコード）  const stateWithReturn = `${state}:${Buffer.from(returnTo).toString("base64")}`;  // Hydra認可エンドポイントへのURL構築  const params = new URLSearchParams({    client_id: process.env.OAUTH_CLIENT_ID!,    response_type: "code",    scope: "openid profile email",    redirect_uri: `${process.env.NEXT_PUBLIC_URL}/callback`,    state: stateWithReturn,  });  const authUrl = `${process.env.HYDRA_PUBLIC_URL}/oauth2/auth?${params}`;  return NextResponse.redirect(authUrl);}stateパラメータは2つの役割を持つ：CSRF対策：ランダムな値を含めることで、攻撃者が生成したURLでのコールバックを防ぐリダイレクト先の保持：認証後、元のページへ戻るためにreturnToをエンコードして含めるRFC 9700 (OAuth 2.0 Security Best Current Practice)では、stateパラメータによるCSRF対策が明記されている。認可サーバーがPKCEをサポートしていることを確認できるなら、PKCEでCSRF対策を兼ねることも可能だが、stateを使う方法が最も広くサポートされている。cheatsheetseries.owasp.orgコールバック処理：トークン取得とCookie設定ここが最も複雑な部分だ。// app/api/auth/callback/route.tsimport { NextResponse } from "next/server";export async function POST(request: Request) {  const body = await request.json();  const { code, state } = body;  // stateからリダイレクト先を取り出す  const [, returnToBase64] = state.split(":");  const returnTo = Buffer.from(returnToBase64, "base64").toString();  // 認可コードをトークンに交換  const tokenResponse = await fetch(    `${process.env.HYDRA_PUBLIC_URL}/oauth2/token`,    {      method: "POST",      headers: {        "Content-Type": "application/x-www-form-urlencoded",        Authorization: `Basic ${Buffer.from(          `${process.env.OAUTH_CLIENT_ID}:${process.env.OAUTH_CLIENT_SECRET}`        ).toString("base64")}`,      },      body: new URLSearchParams({        grant_type: "authorization_code",        code,        redirect_uri: `${process.env.NEXT_PUBLIC_URL}/callback`,      }),    }  );  if (!tokenResponse.ok) {    const error = await tokenResponse.text();    console.error("Token exchange failed:", error);    return NextResponse.json(      { error: "Token exchange failed" },      { status: 401 }    );  }  const tokens = await tokenResponse.json();  // IDトークンをデコードしてユーザー情報を取得  const idTokenPayload = JSON.parse(    Buffer.from(tokens.id_token.split(".")[1], "base64").toString()  );  console.log("ID token decoded:", idTokenPayload);  // レスポンスにCookieを設定  const response = NextResponse.json({ success: true, returnTo });  response.cookies.set("auth_token", tokens.access_token, {    httpOnly: false,  // クライアントJSからアクセス可能に    secure: process.env.NODE_ENV === "production",    sameSite: "lax",    maxAge: tokens.expires_in,    path: "/",  });  if (tokens.refresh_token) {    response.cookies.set("refresh_token", tokens.refresh_token, {      httpOnly: true,  // リフレッシュトークンはhttpOnlyで保護      secure: process.env.NODE_ENV === "production",      sameSite: "lax",      maxAge: 30 * 24 * 60 * 60, // 30日      path: "/",    });  }  return response;}Cookie設定で学んだこと最初、httpOnly: trueでアクセストークンを設定していた。OWASPのセッション管理チートシートによれば、これがセキュリティのベストプラクティスだ。しかし、クライアントサイドでAPIを呼び出す必要があった。owasp.org// クライアントコンポーネントでAPIを呼び出すuseEffect(() => {  const token = document.cookie    .split("; ")    .find((row) => row.startsWith("auth_token="))    ?.split("=")[1];  if (token) {    api.setToken(token);  }}, []);httpOnly: trueだとdocument.cookieからアクセスできない。選択肢は2つ：アクセストークンをhttpOnly: falseにする - クライアントJSからアクセス可能Server Componentからのみ API を呼ぶ - httpOnlyのまま、サーバーサイドで処理今回は1を選んだ。「httpOnlyをfalseにするなんて、セキュリティの教科書に反している」——そう思う人がいるかもしれない。私もそう思った。OWASPのチートシートにも「httpOnly: trueにしろ」と書いてある。でも、教科書に書いてあることと、目の前のシステムで最善の選択は、必ずしも一致しない。この判断には明確な理由がある。まず、脅威モデルを整理する。httpOnlyの目的は「XSSでトークンを盗まれること」を防ぐことだ。では、XSSが成功した場合に何が起きるか。攻撃者はユーザーのブラウザ上で任意のJavaScriptを実行できる。httpOnlyでトークンを保護しても、攻撃者はfetch('/api/user/delete', {credentials: 'include'})を実行できる。トークンを「盗む」ことはできなくても、「使う」ことはできる。しかし、httpOnly: falseにすることで追加のリスクが生じる。トークンを読み取って攻撃者のサーバーに送信できるため、攻撃者は別のマシンからトークンを使用できる。httpOnly: trueなら被害はそのブラウザセッション内に限定されるが、falseなら攻撃者が任意の場所からAPIを叩ける。つまり、httpOnlyは「トークンの窃取」を防ぐことで、XSS被害の範囲を限定する。しかし、XSS対策の本質は、そもそもXSSを発生させないことだ。CSP（Content Security Policy）、入力のサニタイズ、Reactの自動エスケープ——これらがXSS対策の本丸であり、httpOnlyは最後の砦にすぎない。その上で、今回の判断基準は以下だ。アクセストークンは短命（15分）: 仮に窃取されても、15分で無効化されるリフレッシュトークンはhttpOnly: trueで保護: 長期間有効なトークンは絶対に保護するクライアントサイドでのAPI呼び出しが必須: Server Componentだけでは実現できないリアルタイム機能があるしかし、これはトレードオフだ。Auth0のToken Storageガイドでは、SPAの場合、インメモリストレージが最も安全とされている。将来的にはBFF（Backend for Frontend）パターンに移行し、トークンをサーバーサイドで完全に管理する構成を検討している。Curity社のベストプラクティス記事では、JWTの安全な取り扱いについて詳しく解説されている。owasp.orgID Tokenの署名検証なぜ署名検証が必要か最初の実装では、ID Tokenを単純にBase64デコードしていた：// ❌ 危険：署名検証なしのデコードconst payload = JSON.parse(  Buffer.from(tokens.id_token.split(".")[1], "base64").toString());これは動く。中身も読める。でも、これでは改ざんを検出できない。「tokenエンドポイントから直接取得しているから、改ざんされることはないのでは？」と思うかもしれない。確かに、バックエンドでtokenエンドポイントを呼び出し、その結果をそのまま使うなら、経路上で改ざんされるリスクは低い。しかし、問題は別のところにある。フロントエンドにトークンを渡す設計だと、ブラウザ側で別のトークンに差し替えられる可能性がある。また、マイクロサービス間でトークンを渡す際、悪意あるサービスが偽トークンを送る可能性もある。署名検証は「このトークンは本当にHydraが発行したものか」を確認する仕組みだ。具体的に何が起きるか。攻撃者は以下のようなトークンを作成できる。// 攻撃者が作成した偽のトークンconst fakePayload = {  sub: "admin-user-id",  // 管理者のユーザーID  email: "admin@example.com",  role: "platform_admin",  // 権限昇格  tenant_id: "target-tenant",  // 他テナントへのアクセス  exp: 9999999999  // 無期限};const fakeToken = `eyJhbGciOiJub25lIn0.${btoa(JSON.stringify(fakePayload))}.`;署名検証をしていなければ、このトークンは「有効」として受け入れられる。攻撃者は任意のユーザーになりすまし、任意の権限を持ち、任意のテナントにアクセスできる。認証システムが完全に無意味になる。JWTは3つのパートで構成される：ヘッダー.ペイロード.署名。署名を検証しないということは、攻撃者が作った偽のトークンも受け入れてしまうということだ。これは「鍵のかかっていない金庫」と同じだ。中身は入っているが、誰でも開けられる。OpenID Connect Core 1.0のID Token検証仕様では、以下の検証が必須とされている：署名アルゴリズムの確認（alg）発行者の検証（iss = Hydra URL）対象者の検証（aud = クライアントID）有効期限の確認（exp）署名の検証（公開鍵で）joseライブラリによる実装joseライブラリを使うと、これらの検証を簡潔に実装できる。npm install jose// lib/auth.tsimport * as jose from "jose";export interface IdTokenClaims {  sub: string;  aud: string | string[];  iss: string;  exp: number;  iat: number;  email?: string;  role?: string;  tenant_id?: string;}/** * ID Tokenの署名を検証し、クレームを返す * @see https://openid.net/specs/openid-connect-core-1_0.html#IDTokenValidation */export async function verifyIdToken(idToken: string): Promise<IdTokenClaims> {  const hydraUrl = process.env.HYDRA_PUBLIC_URL || "http://localhost:4444";  const clientId = process.env.NEXT_PUBLIC_CLIENT_ID || "demo-client";  // JWKSエンドポイントから公開鍵を取得  // @see https://www.ory.sh/docs/hydra/reference/api#tag/jwk/operation/discoverJsonWebKeys  const JWKS = jose.createRemoteJWKSet(    new URL(`${hydraUrl}/.well-known/jwks.json`)  );  // 署名検証 + issuer/audience検証  const { payload } = await jose.jwtVerify(idToken, JWKS, {    issuer: hydraUrl,    audience: clientId,  });  return payload as IdTokenClaims;}コールバックでの使用// app/api/auth/callback/route.tsimport { verifyIdToken } from "@/lib/auth";export async function POST(request: Request) {  const { code } = await request.json();  // トークン交換...  const tokens = await exchangeCodeForTokens(code);  // ✅ 署名検証付きでID Tokenをデコード  try {    const claims = await verifyIdToken(tokens.id_token);    console.log("ID token verified:", {      sub: claims.sub,      email: claims.email,      role: claims.role,      iss: claims.iss,    });    // ユーザー情報をセッションに保存    const user = {      id: claims.sub,      email: claims.email || "unknown",      role: claims.role || "customer",      tenant_id: claims.tenant_id,    };    // Cookie設定...  } catch (error) {    console.error("ID token verification failed:", error);    return NextResponse.json(      { error: "Token verification failed" },      { status: 401 }    );  }}E2Eテストでの確認実際にログインフローを実行して、署名検証が機能していることを確認した。verifyIdToken()の内部ログと、コールバックハンドラーのログが出力される：ID token verified successfully: {  sub: 'c128f3e7-5013-46b8-add2-fbe0e78bfec7',  email: 'demo@example.com',  role: 'platform_admin',  iss: 'http://localhost:4444'}ID token verified and decoded: {  sub: 'c128f3e7-5013-46b8-add2-fbe0e78bfec7',  email: 'demo@example.com',  role: 'platform_admin',  tenant_id: undefined,  iss: 'http://localhost:4444',  aud: [ 'demo-client' ]}POST /api/auth/callback 200 in 609msverified successfullyと出力されれば、以下が確認できている：JWKSエンドポイント（/.well-known/jwks.json）から公開鍵を取得できた署名が正しく検証された（RS256）issがHydra URL（http://localhost:4444）と一致したaudにクライアントID（demo-client）が含まれていたトークンが有効期限内だったtenant_id: undefinedは、Platform Adminユーザーがテナントに所属していないため。通常のテナントユーザーでログインすると、ここにテナントIDが表示される。開発環境でのフォールバック開発環境ではJWKSエンドポイントにアクセスできない場合がある。その時は警告を出しつつ、署名なしデコードにフォールバックする：try {  const claims = await verifyIdToken(tokens.id_token);  // 検証成功} catch (verifyError) {  console.warn("ID token verification failed, falling back to unsafe decode");  console.warn("WARNING: Using unverified ID token claims. This is insecure!");  // 開発環境のみ許容  const unsafeClaims = decodeIdTokenUnsafe(tokens.id_token);  // ...}本番環境では、このフォールバックを無効化すべきだ。github.comマルチテナント認証JWTにテナント情報を含めるOry HydraのConsent画面で、ユーザーのテナント情報をIDトークンに含める。ベストプラクティスとして、Login時にcontextに保存したユーザー情報をConsent時に取得する（DBルックアップを回避）：// Rustバックエンド側（Consent Provider）// Best Practice: contextからユーザー情報を取得（DBルックアップ不要）// Login時にUserContextとして保存した情報をここで復元let user_context: Option<UserContext> = consent_request    .context    .as_ref()    .and_then(|ctx| serde_json::from_value(ctx.clone()).ok());let (user_email, user_role, user_tenant_id) = user_context    .map(|ctx| (ctx.email, ctx.role, ctx.tenant_id))    .unwrap_or_default();// IDトークンにカスタムクレームを追加let session = ConsentSession {    id_token: serde_json::json!({        "email": user_email,        "role": user_role,        "tenant_id": user_tenant_id,  // テナントIDを含める    }),};hydra.accept_consent(&challenge, grant_scope, grant_audience, Some(session)).await?;ここで重要なのは、user_emailやuser_roleをDBから取得するのではなく、Login時にHydraのcontextに保存したUserContextから取得している点だ。これにより：Consent時のDBアクセスが不要になるLogin時点のユーザー状態が保持される（整合性）パフォーマンスが向上するフロントエンドでトークンをデコードすると、テナント情報が取得できる：// IDトークンのペイロード例{  "aud": ["demo-client"],  "email": "manager@example.com",  "role": "manager",  "tenant_id": "aa8d56f1-a083-439b-996a-4a7b73698dfb",  "sub": "e5555555-5555-5555-5555-555555555555"}APIリクエストでのテナント分離バックエンドAPIは/api/v1/tenant/というプレフィックスでテナント固有のエンドポイントを提供：/api/v1/tenant/incidents    # テナント内のインシデント/api/v1/tenant/projects     # テナント内のプロジェクト/api/v1/tenant/engineers    # テナント内のエンジニアテナントIDはJWTから取得するため、URLにテナントIDを含める必要はない。これにより：URLの推測による他テナントへのアクセス試行を防ぐテナントIDの改ざんを防ぐ（JWTは署名で保護されている）なぜURLパスにテナントIDを含める方式が危険なのか、具体例で説明する。# URLパス方式（危険）GET /api/v1/tenants/tenant-123/incidentsGET /api/v1/tenants/tenant-456/incidents  ← tenant-123のユーザーがアクセスを試みるこの方式では、バックエンドで「リクエストしたユーザーがtenant-456に所属しているか」を毎回検証する必要がある。検証を忘れると、他テナントのデータが漏洩する。実際、この種のバグは「IDOR（Insecure Direct Object Reference）」として知られ、OWASPのトップ10に常に入る脆弱性だ。# JWTクレーム方式（安全）GET /api/v1/tenant/incidents# JWTの中身: {"tenant_id": "tenant-123", ...}この方式では、バックエンドはJWTからテナントIDを取得する。JWTは署名で保護されているため、ユーザーが改ざんできない。「どのテナントのデータを返すか」はJWTが決定し、URLは関与しない。URLパラメータとユーザー権限を照合する追加の検証が不要になるため、バグの入り込む余地が減る。このアプローチはMicrosoft Azure Architecture Centerでも推奨されている。ログアウト処理OAuth2のログアウトは複雑だ。以下を考慮する必要がある：フロントエンドのCookie削除HydraのOAuth2セッション無効化バックエンドのセッション無効化（該当する場合）// app/api/auth/logout/route.tsexport async function GET(request: Request) {  const accessToken = request.cookies.get("auth_token")?.value;  if (accessToken) {    // 1. Hydraでトークンを無効化    await fetch(`${process.env.HYDRA_PUBLIC_URL}/oauth2/revoke`, {      method: "POST",      headers: {        "Content-Type": "application/x-www-form-urlencoded",        Authorization: `Basic ${Buffer.from(          `${process.env.OAUTH_CLIENT_ID}:${process.env.OAUTH_CLIENT_SECRET}`        ).toString("base64")}`,      },      body: new URLSearchParams({        token: accessToken,      }),    });    // 2. Hydraのログインセッションも削除    // （IDトークンからsubjectを取得して削除）  }  // 3. Cookieを削除してリダイレクト  const response = NextResponse.redirect(new URL("/", request.url));  response.cookies.delete("auth_token");  response.cookies.delete("refresh_token");  return response;}RP-Initiated LogoutOpenID ConnectにはRP-Initiated Logout 1.0という仕様がある。この仕様では、Relying Party（クライアントアプリケーション）からOpenID Providerに対してログアウトを要求する方法が定義されている。Hydraはこれをサポートしている。www.ory.sh// Hydraのログアウトエンドポイントを使う方法const logoutUrl = new URL(`${process.env.HYDRA_PUBLIC_URL}/oauth2/sessions/logout`);logoutUrl.searchParams.set("id_token_hint", idToken);logoutUrl.searchParams.set("post_logout_redirect_uri", `${process.env.NEXT_PUBLIC_URL}/`);return NextResponse.redirect(logoutUrl);この方法だと、Hydraがログアウト処理を統括し、Login Providerの/logoutエンドポイントにリダイレクトしてくれる。トラブルシューティング：実際に遭遇した問題問題1：Cookie名の不一致症状：ログイン後、ダッシュボードでAPIデータが取得できない原因：コールバックで設定するCookie名と、各ページで読み取るCookie名が異なっていた// コールバックresponse.cookies.set("auth_token", ...);// ダッシュボード（間違い）.find((row) => row.startsWith("access_token="))// 正しくは.find((row) => row.startsWith("auth_token="))教訓：Cookie名は定数として一箇所で定義し、全体で共有する。なぜこのミスが起きるのか。認証コードはコールバック処理から書き始め、ダッシュボードは後から書く。時間が空くと、最初に使った名前を忘れる。「書いた順番」と「読まれる順番」が異なるコードでは、定数化を最初に行うべきだ。// lib/constants.tsexport const AUTH_COOKIE_NAME = "auth_token";export const REFRESH_COOKIE_NAME = "refresh_token";問題2：APIパスの構造症状：APIリクエストが404を返す原因：テナントAPIのパスプレフィックスを間違えていた// 間違いfetch("/api/v1/incidents")  // 404// 正しいfetch("/api/v1/tenant/incidents")  // 200教訓：APIのベースパスはAPIクライアントクラスで管理するclass ApiClient {  private baseUrl = process.env.NEXT_PUBLIC_API_URL;  private tenantPath = "/api/v1/tenant";  async getIncidents() {    return this.request(`${this.tenantPath}/incidents`);  }}問題3：トークン期限切れ症状：しばらく操作しないとAPI呼び出しが失敗する原因：アクセストークンの有効期限（15分）が切れていた対策：リフレッシュトークンを使った自動更新async request<T>(path: string, options?: RequestInit): Promise<T> {  const response = await fetch(`${this.baseUrl}${path}`, {    ...options,    headers: {      ...options?.headers,      Authorization: `Bearer ${this.token}`,    },  });  if (response.status === 401) {    // トークンをリフレッシュして再試行    await this.refreshToken();    return this.request(path, options);  }  return response.json();}問題4：HydraのセッションとProviderのセッション症状：ログアウト後、再度ログインしようとすると認証画面をスキップしてしまう原因：Hydraのログインセッションが残っていたOry Hydraのドキュメントによると、HydraはLogin Providerでの認証成功を記憶している。skipフラグが立っている場合、ログイン画面をスキップする。これはSSO（シングルサインオン）の正しい動作だが、完全なログアウトを実装する際には注意が必要だ。// Login Provider側if login_request.skip {    // 既にセッションがあるのでスキップ    // Note: skip時はcontextが既に設定されているためNoneで良い    let completed = hydra.accept_login(&challenge, &login_request.subject, false, None).await?;    return Ok(Redirect::to(&completed.redirect_to));}完全なログアウトには、Hydraのセッションも削除する必要がある：// ログアウト時にHydraのセッションも削除await fetch(  `${process.env.HYDRA_ADMIN_URL}/admin/oauth2/auth/sessions/login?subject=${userId}`,  { method: "DELETE" });エラーハンドリングのパターンバックエンドから返されるエラーは統一された形式になっている：{  "error": "invalid_credentials",  "error_description": "The provided credentials are invalid",  "error_code": "AUTH_002"}フロントエンドではこれを適切に処理する：async request<T>(path: string, options?: RequestInit): Promise<T> {  const response = await fetch(`${this.baseUrl}${path}`, options);  if (!response.ok) {    const error = await response.json().catch(() => ({      error: "unknown_error",      error_description: "An unexpected error occurred",    }));    throw new ApiError(response.status, error);  }  return response.json();}class ApiError extends Error {  constructor(    public status: number,    public body: { error: string; error_description: string; error_code?: string }  ) {    super(body.error_description);  }}セキュリティチェックリスト実装後に確認すべき項目。これは完璧なリストではない——セキュリティに完璧はない——が、最低限チェックすべきポイントをまとめた。認証に関わるCookieの属性[ ] HttpOnly属性: XSSの緩和策。クライアントJSからアクセス不要なCookieには必ず設定[ ] SameSite属性: LaxもしくはStrictに設定。CSRF対策の基本。Laxの場合、GETリクエストで更新処理を行っていないか確認[ ] Secure属性: HTTPS通信でのみCookieが送られるように。本番環境では必須[ ] Domain属性: サブドメインへのCookie送信範囲を理解しているか。example.comのCookieがjobs.example.comにも送られる設定だと、他サブドメインの脆弱性がリスクになる[ ] Cookie Prefix: Cookie名を__Host-で始めると、Domain属性が空でないCookieの指定を無視してくれる（参考: Cookie Prefixのバイパス）blog.tokumaru.orgレスポンスヘッダ[ ] Strict-Transport-Security（HSTS）: ブラウザにHTTPS接続を強制。max-age=31536000; includeSubDomains; preload[ ] X-Frame-Options: DENYもしくはSAMEORIGINでクリックジャッキング対策。CSPのframe-ancestorsも検討[ ] X-Content-Type-Options: nosniffを指定。MIMEタイプスニッフィング攻撃を防ぐ認証フロー[ ] stateパラメータでCSRF対策している[ ] リフレッシュトークンはhttpOnlyで保護している[ ] アクセストークンの有効期限は短く設定している（15分推奨）[ ] ログアウト時にトークンを無効化している[ ] メールアドレスの列挙ができないこと: ログイン画面やパスワード再設定画面で「このメールアドレスは登録されていません」のようなエラーを出さない[ ] JWTの署名を検証している（バックエンド側）[ ] テナント分離がJWTベースで行われている[ ] 退会/メールアドレス変更などの重要操作で直前のログインを必須にしている: XSSやセッションハイジャック発生時の緩和策その他[ ] サードパーティCookieに依存していないこと（Chrome廃止予定）[ ] iOS SafariのITPによりローカルストレージやJSから保存したCookieは7日で消える可能性がある（未使用時）まとめNext.jsでOry Hydra認証を実装する際の要点：OAuth2フローの理解：認可コードフローの各ステップでフロントエンドが何をすべきか把握するID Token署名検証：JWKSを使って署名を検証し、issuer/audienceを確認するCookie管理：httpOnly, Secure, SameSiteの設定を用途に応じて選択するマルチテナント：JWTにテナント情報を含め、APIはトークンからテナントを識別するエラーハンドリング：OAuth2仕様に沿ったエラー形式を統一的に処理するログアウト：Hydraのセッションとフロントエンドのセッション両方を考慮する認証は「動いた」で終わりではない。Cookie名の不一致のような単純なミスから、セッション管理の複雑さまで、実際に動かして初めて見つかる問題が多い。結局のところ、OAuth2は「誰かが決めた仕様に従う」ゲームだ。RFCを読み、OWASPを読み、Hydraのドキュメントを読む。自分で発明する余地は少ない。でも、それでいい。認証のような重要な仕組みを自己流で作るのは、傲慢だと思う。セキュリティの歴史は「賢い人が作ったものを、もっと賢い攻撃者が破る」の繰り返しだ。OAuth 1.0のセッション固定攻撃、JWTのalg=none脆弱性——仕様を作った人たちでさえ、穴を見落とす。自分がその歴史に新たな失敗を加える必要はない。先人の知恵に乗っかり、その上で自分のシステムに合った判断をする。それが現実的なアプローチだ。前回のバックエンド実装でユーザー列挙攻撃を防ぐテストを書いたように、フロントエンドでも手動でのE2Eテストが重要だ。ログイン→操作→ログアウト→再ログイン。このサイクルを何度も試して、エッジケースを潰していく。次回は、Playwright MCPを使ったE2Eテストの自動化と、テストで発見したバグについて解説する。syu-m-5151.hatenablog.comこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。おわりに今日は社内で学生向けワークショップを担当した。終わった後、若い参加者が話しかけてきた。「ブログ読んでます」と言われた。嬉しかった。嬉しかったが、すぐに釘を刺した。「あまり憧れないでくださいね」と。憧れられるのがあまり得意ではない。偶像として崇拝されるのが苦手だし、偶像として振る舞って相手に応えるのも苦手だ。それに、ブログで良いこと言っている人に若いうちから憧れすぎるのは良くない。自分がそうだったのでよく分かる。10代の頃、文章が上手くて考え方が明快な技術ブロガーを見つけて、「この人みたいになりたい」と思った。記事を読み漁った。でも、その人が実際にどんなコードを書いているかは知らなかった。ブログは編集された「ハイライト」にすぎない。裏側の泥臭い試行錯誤、失敗、妥協は見えない。数年後にそれを知ったとき、ちょっとがっかりした。がっかりした自分にもがっかりした。若い技術者なら、現場に居る良い技術者に憧れてほしい。ブログを書く人ではなく。GitHubのコミット履歴を見てほしい。PRのレビューコメントを見てほしい。本番障害のポストモーテムを読んでほしい。そこに本当の技術者がいる。ブログの「正解」ではなく、コードの「試行錯誤」に学んでほしい。正直に言えば、フロントエンドでの認証実装は想像以上に複雑だった。3年前の自分に言いたい。「Next.jsで認証？OAuth2知ってるし、すぐできるでしょ」と思っていた過去の自分に。そうじゃない。Cookieの属性一つでセキュリティモデルが変わる。ID Tokenの署名検証を省略した瞬間、認証システムの意味がなくなる。OAuth2のフローは理解していたつもりだった。RFCも読んだ。でも、実際にNext.jsでCookieを扱い、ID Tokenの署名を検証し、マルチテナントのテナント分離を実装すると、「知っている」と「動かせる」の間には大きな溝があることを思い知らされた。RFCには「stateパラメータでCSRF対策」と書いてある。でも、実際にコードを書くと「stateはどこに保存する？」「検証はいつやる？」「不一致の場合のエラーメッセージは？」という判断が次々と必要になる。仕様書は「何をすべきか」は教えてくれるが、「どう実装すべきか」は教えてくれない。その溝を埋めるのは、結局、自分で書いて動かす経験しかない。特にhttpOnlyの判断には時間を使った。OWASPのベストプラクティスを読み、Auth0のガイドを読み、それでも「これで正しいのか」という不安は消えない。セキュリティに100%の正解はない。トレードオフを理解し、判断し、記録する。それしかできることはない。この記事を書いている人間も、悩みながら書いている。ブログに書かれている「正解」は、試行錯誤の結果を事後的に整理したものにすぎない。過程で何度も間違えている。それを知った上で、参考にしてもらえれば。なんか総じてとても疲れた。でも、まあ、悪くない一日だった。参考資料Ory HydraOry Hydra DocumentationOAuth2 Token EndpointLogin FlowLogout FlowOAuth2/OIDC仕様RFC 6749 - OAuth 2.0RFC 9700 - OAuth 2.0 Security Best Current PracticeOpenID Connect Core 1.0RP-Initiated Logout 1.0セキュリティガイドラインOWASP OAuth2 Cheat SheetOWASP Session Management Cheat SheetAuth0 Token StorageCurity JWT Best PracticesCookie属性CookieのDomain属性は指定しないが一番安全 - 徳丸氏によるCookie Domain属性の解説Cookie Prefixのバイパス - __Host-プレフィックスの重要性MDN: Set-Cookie - Cookie属性の公式リファレンスサードパーティCookieの廃止に向けた準備 - Chrome対応ガイドライブラリjose - JavaScript Object Signing and EncryptionNext.jsNext.js App RouterRoute HandlersMiddleware]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Fear of the Unknown：Rust/sqlxでNULLを制する6つのパターン]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/08/092409</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/08/092409</guid>
            <pubDate>Thu, 08 Jan 2026 00:24:09 GMT</pubDate>
            <content:encoded><![CDATA[はじめにあるプロジェクトで、電話番号が未登録のユーザーを検索するコードをレビューしていた。WHERE phone = NULL——一見正しく見えるこのクエリは、常に0件を返していた。データは確実に存在する。クエリもシンプル。では何が問題なのか。答えはSQLの3値論理にあった。通常の比較演算はTRUEかFALSEを返すが、SQLにはUNKNOWN（不明）という第3の真偽値がある。NULLは「値が不明」を意味するため、NULL = NULLは「不明 = 不明」となり、結果もUNKNOWNになる。WHERE句はTRUEの行しか返さないから、UNKNOWNは暗黙にFALSE扱いされ、結果は常に0件になる。この問題は『SQLアンチパターン』で「Fear of the Unknown」として解説されている。本記事ではRust + sqlxでの実装パターンに焦点を当てる。SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazonこういう妄想の仕様と実際の仕様には違いがある。「おい、類推するな」というブログで書いたので時間がある時に読んでほしい。syu-m-5151.hatenablog.comsqlxの型マッピングRustにはOption<T>という型がある。これは「値があるかもしれないし、ないかもしれない」を表現する型だ。Some(値)が「値あり」、Noneが「値なし」を意味する。SQLのNULLに相当するのがこのNoneだ。let phone: Option<String> = Some("090-1234-5678".to_string());  // 値ありlet phone: Option<String> = None;                                // 値なし（NULL相当）sqlxはPostgreSQLのNULLをこのOption<T>に自動マッピングする。 PostgreSQL  Rust (NULLable)  Rust (NOT NULL)  VARCHAR, TEXT  Option\<String>  String  INTEGER  Option\<i32>  i32  BIGINT  Option\<i64>  i64  UUID  Option\<Uuid>  Uuid  DECIMAL  Option\<Decimal>  Decimal  TIMESTAMPTZ  Option\<DateTime\<Utc>>  DateTime\<Utc> NULLableカラムをOption<T>以外にマッピングすると、NULLが返された時点で実行時エラーになる。私も一度やった。「NULLなんて来ないだろう」と思っていたカラムが、特定の条件でNULLを返し、深夜にSlackが鳴った。#[derive(Debug, sqlx::FromRow)]struct User {    id: Uuid,    email: String,              // NOT NULL → 必ず値がある    name: String,               // NOT NULL → 必ず値がある    phone: Option<String>,      // NULLable → Option型で「値があるかもしれないし、ないかもしれない」を表現    bio: Option<String>,        // NULLable → Noneが「値なし」、Some("値")が「値あり」    created_at: DateTime<Utc>,  // NOT NULL → 必ず値がある}パターン1：検索フィルターでのNULL// NG: NoneがNULLにバインドされ、phone = NULLは常にUNKNOWN// query_as::<_, User>の説明://   ::<_, User> は戻り値の型を指定するRustの記法（turbofish構文）//   _ はデータベースの種類をコンパイラに推論させる部分//   User は「検索結果をUser構造体に変換して」という指定let users = sqlx::query_as::<_, User>(    "SELECT * FROM users WHERE phone = $1"  // $1はプレースホルダ（SQLインジェクション対策）).bind(&params.phone)  // bind()で$1に値を埋め込む。NoneはNULLになる.fetch_all(&pool)     // 全件取得.await?;              // 非同期処理の完了を待つ。?はエラー時に早期リターン// OK: 条件分岐でクエリを切り替える// match式: Option型の中身に応じて処理を分岐（switch文のようなもの）let users = match &params.phone {    Some(phone) => {  // Some(値): 値がある場合        sqlx::query_as::<_, User>("SELECT * FROM users WHERE phone = $1")            .bind(phone)            .fetch_all(&pool)            .await?    }    None => {  // None: 値がない場合 → IS NULLを使う        sqlx::query_as::<_, User>("SELECT * FROM users WHERE phone IS NULL")            .fetch_all(&pool)            .await?    }};// OK: IS NOT DISTINCT FROMで1クエリにまとめる（PostgreSQL固有）// NULLを普通の値として比較できる（NULL同士も「等しい」と判定）let users = sqlx::query_as::<_, User>(    "SELECT * FROM users WHERE phone IS NOT DISTINCT FROM $1").bind(&params.phone).fetch_all(&pool).await?;パターン2：COUNTの挙動// r#"..."# は生文字列リテラル（raw string literal）// 複数行のSQLを書きやすく、エスケープも不要な記法sqlx::query_as(    r#"    SELECT        COUNT(*) as total_users,                           -- 全行数（NULLを含む）        COUNT(coupon_code) as users_with_coupon,           -- NULLでない行数        COUNT(*) - COUNT(coupon_code) as users_without_coupon    FROM users    "#)空文字列とNULLが混在している場合は注意が必要。// NG: 空文字列のみマッチ、NULLはマッチしない"SELECT * FROM users WHERE coupon_code = ''"// OK: 両方を考慮"SELECT * FROM users WHERE coupon_code IS NULL OR coupon_code = ''"// OK: NULLIFで正規化"SELECT * FROM users WHERE NULLIF(coupon_code, '') IS NULL"パターン3：フォーム送信での空文字列フロントエンドから{ "phone": "" }が送られると、Option<String>ではSome("")になる。データベースには空文字列が保存され、NULLにはならない。// Rustレイヤーで正規化// filter(): 条件を満たさない場合はNoneに変換するメソッド// |s| !s.is_empty() はクロージャ（無名関数）: sが空でなければtruelet phone = req.phone.filter(|s| !s.is_empty());  // Some("") → None, Some("090") → Some("090")let bio = req.bio.filter(|s| !s.is_empty());sqlx::query("UPDATE users SET phone = $1, bio = $2 WHERE id = $3")    .bind(&phone)  // NoneはNULLとしてバインドされる    .bind(&bio)    .bind(user_id)    .execute(&pool)  // execute(): SELECT以外のクエリ実行    .await?;// SQLレイヤーで正規化sqlx::query(    r#"    UPDATE users    SET phone = NULLIF(TRIM($1), ''),  -- TRIM: 空白除去, NULLIF: ''ならNULLに        bio = NULLIF(TRIM($2), '')    WHERE id = $3    "#)パターン4：LEFT JOINでのOption必須LEFT JOINは左側のテーブル（例: users）の全行を返す。右側のテーブル（例: orders）に一致する行がない場合、右側のカラムはすべてNULLで埋められる。だから注文がないユーザーの場合、o.created_atはNULLになり、MAX(o.created_at)の結果もNULLになる。// NG: 注文がないユーザーでMAX(o.created_at)がNULLになり、実行時エラーstruct UserWithLastOrder {    last_order_date: DateTime<Utc>,  // NULLを受け付けない型}// OK: Option<T>でNULLを許容するstruct UserWithLastOrder {    last_order_date: Option<DateTime<Utc>>,  // NULLならNone、値があればSome(値)}LEFT JOINや集約関数（MAX, AVG, SUM等）の結果は常にNULLになりうる。迷ったらOption<T>を使う。パターン5：NOT INの罠// NG: category_idがNULLの行は削除されないsqlx::query(    r#"    DELETE FROM products    WHERE category_id NOT IN (        SELECT id FROM categories WHERE active = true    )    "#)なぜNULLの行が削除されないのか。NOT INは内部でx <> 1 AND x <> 2 AND ...に展開される。ここでcategory_idがNULLだとどうなるか。NULL <> 1はUNKNOWNを返す。NULL <> 2もUNKNOWN。ANDの3値論理ではTRUE AND UNKNOWN = UNKNOWNだから、条件全体がUNKNOWNになる。WHERE句はTRUEの行しか処理しないため、NULLを含む行は削除対象から外れてしまう。// OK: NOT EXISTSを使う// NULLの行も正しく処理される（サブクエリが0行ならTRUE）sqlx::query(    r#"    DELETE FROM products p    WHERE NOT EXISTS (        SELECT 1 FROM categories c        WHERE c.id = p.category_id AND c.active = true    )    "#)パターン6：query_as!マクロこれまでのパターンで使っていたquery_as()は実行時に型チェックを行う。一方query_as!()はマクロで、コンパイル時にデータベースへ接続してスキーマを確認し、型の不整合をビルドエラーとして検出する。NULLになりうるカラムをOption<T>以外でマッピングしようとすると、実行前にエラーを発見できる。// NG: AVG(rating)はNULLを返す可能性があり、コンパイルエラーstruct ProductSummary {    average_rating: f64,  // f64はNULLを受け付けない}sqlx::query_as!(    ProductSummary,    "SELECT name, AVG(rating) as average_rating FROM products GROUP BY name")// コンパイルエラー: AVGの結果がNULLになりうるのにOption<f64>ではない// OK: Option<T>を使うstruct ProductSummary {    average_rating: Option<f64>,}// OK: COALESCEと"!"サフィックスでNOT NULLを保証sqlx::query_as!(    ProductSummary,    r#"    SELECT name,           COALESCE(AVG(rating), 0)::FLOAT8  -- NULLなら0、FLOAT8にキャスト           as "average_rating!"              -- "!"でNOT NULLを宣言    FROM products GROUP BY name    "#) サフィックス  意味  !  NOT NULLを強制（Option\<T>ではなくT）  ?  NULLを許容（TではなくOption\<T>） まとめ冒頭のWHERE phone = NULLは、WHERE phone IS NULLに書き換えて5分で解決した。3値論理を知っているかどうか——それだけの差だった。NULLの問題はバグではなく、SQLの仕様だ。Rust/sqlxでは以下を守れば大半の問題は防げる。NULLableカラムはOption<T>にマッピング= NULLではなくIS NULLを使うNOT INではなくNOT EXISTSを使う空文字列とNULLを混在させない迷ったらOption<T>を使う。後からOptionを外すのは簡単だが、NULLが返ってきたときのパニックを本番で見るのは心臓に悪い。そもそもNULLableカラムを減らす設計（NOT NULL制約のデフォルト化、別テーブルへの分離）も検討に値する。3値論理の詳細は『SQLアンチパターン』の「Fear of the Unknown」章を参照してほしいです。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料SQL Antipatterns - Fear of the UnknownPostgreSQL - Comparison Functionssqlx - Compile-time checked queries]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI時代に今からITエンジニアを目指す若者にオススメする10冊の本  2026年版]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/07/103853</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/07/103853</guid>
            <pubDate>Wed, 07 Jan 2026 01:38:53 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIは、あなたが聞いたことにしか答えない。聞かなかったことは、永遠に教えてくれない。あなたが何を知らないのか、AIは知らない。2026年だ。AIに聞けば何でも教えてくれる。コードを書いてもらい、設計を相談し、ドキュメントを要約させる。便利だ。では、なぜ本を読むのか。300ページもある本を、最初から最後まで読む必要があるのか。本は違う。本は、聞いていないことを語りかけてくる。知らなかった世界を見せてくる。持っていなかった問いを、手渡してくる。「そんなこと、考えたこともなかった」。そういう瞬間が、本にはある。AIとの対話では、たぶん起きない。AIは効率的だ。知りたいことに、最短距離でたどり着ける。でも、最短距離で歩いていると、道の脇にあるものが見えない。著者が失敗した話、遠回りした話、「今思えば間違いだった」という告白。そういう「寄り道」が、不思議と頭に残る。正解は忘れる。でも、誰かの失敗談は覚えている。たぶん、人間の脳は感情を伴う記憶を優先的に保持するからだ。著者の後悔や苦労を読むとき、読者は追体験している。その感情が、記憶を定着させる。AIに「失敗談を教えて」と聞けば、一般化された失敗談が返ってくる。でも、それは「誰かの」失敗ではない。固有名詞のない失敗談には、感情が宿らない。もう1つ。若者や学生は、そもそも問いを持っていない。何を聞けばいいか分からない。だから、AIに質問もできない。何が分からないのかも分からない。本を読めと言われても、何を読めばいいか分からない。本屋の技術書コーナーに行けば、棚一面に並ぶ背表紙の圧に押しつぶされそうになる。結局、何も買わずに帰る。本は、そういう人に問いをくれる。「あ、これが分からなかったのか」。読み終わって初めて、自分が何を知らなかったのかが分かる。問いを持たない人間に、問いを渡す。それが、本にしかできないことなのだと思う。そういう人のために、10冊を選んだ。「若者にオススメ」と書いておきながら、自分もまだ若い方なのだと思う。少なくとも、将来の自分から見れば若い。ただ、激動の時代だ。技術だけ磨いていればいい時代は、終わりかけているのかもしれない。あるいは、もう終わっているのかもしれない。だから、技術以外の本も混ぜて紹介することにした。先に断っておく。私はバックエンドエンジニアやインフラエンジニアからキャリアをスタートさせた人間だ。だから、フロントエンドやネイティブアプリに関しては、ほぼ紹介しない。偏っている。偏っているが、自分が読んでいない領域の本を勧めることはできない。プログラミング言語個別の書籍も紹介しない。どの言語を学ぶかは人によって違う。だから、言語に依存しない本を中心に選んだ。この10冊が良い10冊かどうかは、分からない。私が良いと思った本が、誰にとっても良いとは限らない。だから、この記事を「正解」として読まなくていい。「こういう本があるんだな」という参考程度に。それでいいのだと思う。それから、もう1つ。本を買うお金がないなら、図書館で借りればいい。技術書は高い。1冊3000円、4000円は当たり前だ。まず読む。金は後でいい。読んで、良かったら、いつか買えばいい。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、本題に入る。技術の土台を作るまずは土台だ。プログラミングを始める前に、あるいは始めたばかりの頃に、IT業界で使われる言葉を知っておく必要がある。語彙がなければ、技術書も読めない。先輩の話も分からない。AIに質問もできない。1冊目：情報処理技術者試験の参考書（どれでもいい）1冊目から、いきなり「どれでもいい」と言うのは無責任に聞こえるかもしれない。でも、本当にそうなのだ。ITパスポートでも、基本情報技術者試験でも、応用情報技術者試験でも、高度試験でも。自分のレベルに合ったものを選べ。本屋で立ち読みして、7割くらい分かるやつを買え。分からなすぎると挫折する。簡単すぎると意味がない。誤解しないでほしい。資格を取れと言っているわけではない。「資格なんて意味ない」「資格より実務経験だ」——そういう声があるのは知っている。半分は正しい。資格を持っているだけでは、コードは1行も書けない。試験に受かっても、現場で即戦力にはなれない。それは分かっている。もっと言えば、試験に受からなくてもいい。俺は全然受からないのに優秀なソフトウェアエンジニアを死ぬほど知っている。資格の有無と実力は、必ずしも一致しない。でも、勉強するなら、頭に入った方がいいだろう。頭に入れるなら、試験を受けた方がいい。締め切りがあると、人は勉強する。試験日という締め切りがなければ、参考書は積読になる。金を払って申し込んで、日程を押さえて、会場に行く。その「仕組み」を使え。なぜ資格試験を勧めるのか。語彙が手に入るからだ。現場に出ると、専門用語が飛び交う。「スループットが落ちてる」「レイテンシがネックになってる」「冗長構成にしないと」「SLAどうする？」——こういう会話が、当たり前のように行われる。プログラミングはできるのに、この語彙がなくて会話に入れない。コードは書ける。でも、技術的な議論ができない。語彙がないと、会話にすら入れない。これは、よくある話だ。試験勉強を通じて、開発特有の語彙が頭に入る。ネットワーク、データベース、セキュリティ、プロジェクトマネジメント。知識として知っているだけで、会話の輪に入れる。「あ、それ試験で出たな」という感覚で、先輩の話が理解できる。試験の内容を全部覚えている必要はない。語彙が残ればいい。それだけで、現場での学習速度が全然違う。ここで正直に言う。実務経験の方が大事だというのは、その通りだと思う。本を読むより、コードを書いた方がいい。知識を詰め込むより、実際にシステムを動かした方がいい。2026年の今なら、分からないことはAIに聞けばいい。AIに疑問をぶつければ、理解も早く進む。でも、経験がなければ、疑問も生まれない。これは「経験を積め」という精神論ではない。構造の問題だ。語彙がなければ問いが立たず、問いがなければ経験を言語化できず、言語化できなければ次の学習に繋がらない。この悪循環を断ち切るには、どこかで語彙を入れるしかない。何を聞けばいいか分からなければ、AIも使いこなせない。「スループット」という言葉を知らなければ、「スループットが落ちている原因は何ですか」とは聞けない。「処理が遅い」と「スループットが低い」は、同じ現象を指しているように見えるが、後者の方が解決策にたどり着きやすい。なぜなら、「スループット」という言葉には、それを改善するための知識体系が紐づいているからだ。語彙は、学習の入り口だ。入り口がなければ、どんなに優秀なAIがあっても、中に入れない。IPA（情報処理推進機構）の試験は、日本のIT業界における共通言語を学ぶのに最も効率がいい。ネットワーク、データベース、セキュリティ、プロジェクトマネジメント、システム設計。全部、体系的にまとまっている。しかも、過去問が無料で公開されている。金がないなら、参考書すら買わなくていい。過去問だけで受かる人もいる。2026年度から、応用情報技術者試験や高度試験がCBT（Computer Based Testing）方式に移行する。これまで年2回、決まった日に会場に足を運ばなければならなかったのが、自分の都合に合わせて受験できるようになる。受験のハードルは確実に下がった。どの参考書がいいかは、正直、好みだ。キタミ式が好きな人もいれば、技術評論社の「合格教本」シリーズが好きな人もいる。Amazonのレビューを見て、自分に合いそうなのを選べばいい。図書館にあることも多い。もう1つ言っておく。ITに興味があるけど、プログラミングには興味がない。そういう若者は多いと思う。「エンジニアになりたいけど、コードを書くのはちょっと……」という人。そういう人こそ、まず資格を取れ。プログラミングができなくても、ITの世界で活躍する道はいくらでもある。インフラ、セキュリティ、プロジェクトマネジメント、ITコンサル。そのすべてにおいて、資格で得た知識と語彙は武器になる。繰り返す。資格を取ることが目的ではない。語彙を入れることが目的だ。語彙があれば、AIにも質問できる。語彙があれば、技術書も読める。語彙があれば、先輩の話も分かる。入り口を作れ。話はそれからだ。www.meti.go.jpシステムの基盤を理解するコードを書けるようになっても、それだけではシステムは動かない。サーバー、ネットワーク、データベース、OS。アプリケーションの下にあるレイヤーを理解しなければ、本番環境で動くものは作れない。ここでは、システムを支える基盤技術について学ぶ本を4冊紹介する。2冊目：バックエンドエンジニアのためのインフラ・クラウド大全コードを書けるようになった。アプリケーションが動くようになった。でも、本番環境にデプロイしようとすると、急に分からないことだらけになる。サーバーって何？ネットワークって何？クラウドって何？アプリだけ書けても、本番では動かせない。この本は、そのギャップを埋めてくれる。バックエンドエンジニアに求められるインフラ・クラウド領域の基礎知識が、1冊にまとまっている。情報システムの基礎から、可用性、キャパシティ、パフォーマンス、監視、セキュリティ、DevOps、SRE。現場で必要になる知識が、体系的に整理されている。全23章、544ページ。分厚いが、それだけの価値がある。「基礎知識」と聞くと、簡単そうに思えるかもしれない。でも、違う。基礎とは、簡単という意味ではない。基礎とは、すべての土台になるという意味だ。なぜこの混同が起きるのか。学校教育のせいだろう。教科書は「基礎→応用」の順に並んでいて、基礎は最初に習う、つまり簡単なものだと刷り込まれる。でも、実際には逆だ。基礎は最後に理解できる。応用を経験して初めて、基礎の意味が分かる。この本に書かれていることは、10年後も20年後も変わらない原則ばかりだ。最初は分からなくていい。分からないまま読み進めて、5年後に読み返したとき、「ああ、これはこういう意味だったのか」と分かる。それが基礎だ。構成も良い。分野ごとに解説がまとまっているが、章末で「あわせて読みたい」範囲が紹介されている。1つの章を読み終わると、「次はこっちも読んでみるか」となる。ちょっとだけ調べるつもりが1時間経っている。そういう本だ。クラウドネイティブな環境では、アプリケーションとインフラの境界が曖昧になっている。コンテナ、Kubernetes、オブザーバビリティ。これらを理解せずに、本番環境で動くシステムは作れない。「俺はアプリ側だから」では通用しない時代だ。この本は、その橋渡しをしてくれる。以前、自分が書いたアプリケーションを本番環境にデプロイしたとき、ローカルでは動いていたのに、本番では動かなかった。原因を調べるのに丸1日かかった。ネットワークの設定だった。そのとき、「アプリを書けるだけでは、本番では戦えない」と痛感した。この本があの頃の自分にあったら、もう少し早く原因にたどり着けたかもしれない。バックエンドエンジニアのためのインフラ・クラウド大全【リフロー型】作者:馬場 俊彰,株式会社X-Tech5翔泳社Amazon3冊目：SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策データベースは、難しい。でも、難しいのに、簡単にできてしまう。ORMを使えば、SQLを書かなくてもデータを取得できる。CREATE TABLE文を書けば、テーブルが作れる。動く。動いてしまう。だから、問題に気づくのが遅れる。テーブル設計の失敗は、ソースコードの失敗よりもリファクタリングが難しい。データが入ってしまってからでは、修正のコストが跳ね上がる。だから、最初から正しい設計を知っておく必要がある。この本は、データベースプログラミングで陥りがちな失敗（アンチパターン）を体系的にまとめた本だ。カンマ区切りで値を格納する「ジェイウォーク」。外部キーを張らない「キーレスエントリ」。1つのカラムに複数の意味を持たせる「マルチカラムアトリビュート」。NULLの扱いを間違える「アンビギュアスグループ」。名前を聞いただけで「あ、やったことある」と思う人は多いはずだ。第2版では、新規書き下ろしの章と15のミニ・アンチパターンが加わった。特にミニ・アンチパターンは実務的な内容が多く、「自分もこの問題にハマった」「こうやって解決した」と思える内容が詰まっている。それなりにエンジニアをやっていると、多くのアンチパターンは踏んだことがある。でも、それを他者に体系的に伝えるのは難しい。自分の設計がシステムにどのような影響を与えていくかを経験として学習する機会は、意外と少ない。だからこそ、この本で先人の失敗を学んでおく価値がある。不思議なことがある。ベストプラクティスを調べて実装しても、想定通りにならないことが多い。環境が違う、前提が違う、規模が違う。でも、アンチパターンは違う。アンチパターンを実装すると、想定通りに困る。なぜか。アンチパターンは「制約違反」だからだ。リレーショナルデータベースには設計原則がある。その原則を破れば、必ず不整合やパフォーマンス問題が起きる。ベストプラクティスは「この文脈では有効」という条件付きだが、アンチパターンは「どの文脈でも有害」という普遍性を持つ。だから、何をすべきかより、何をすべきでないかを学ぶ方が、確実に役に立つ。SQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazon4冊目：モダンオペレーティングシステム 第5版（上・下）データベースの次は、さらに下のレイヤーだ。OSの話をする。OSの中身を知りたければ、この本を読め。プロセスとスレッド、メモリ管理、ファイルシステム、入出力、デッドロック、仮想化とクラウド、マルチプロセッサシステム、セキュリティ。OSを構成する要素が、網羅的に解説されている。上下巻合わせて1000ページ超。分厚いが、それだけの価値がある。コンピュータ・サイエンスの分野で世界的な定番となっている教科書だ。21年ぶりに日本語版が復活した。第5版では、Windows 11やSSDなど、最新のトピックまで詳しく解説されている。セキュリティの章は大部分が書き直された。各章末には585題もの演習問題がある。基礎知識の確認から、プログラミングや計算、さまざまな状況への対応まで。問題に取り組むことで、その章で学んだことの理解が深まる。上下巻で1万円を超える。学生には厳しい価格だ。だから言う。図書館で借りろ。大学の図書館には、たいてい置いてある。この本自体がなくても、類書は置いてある。以前、というかかなり昔にマルチスレッドのバグで丸2日を溶かしたことがある。ログを見ても再現しない。デバッガをつけると動く。原因はスレッド間のレースコンディションだった。そのとき、「なぜプロセスとスレッドが分かれているのか」「なぜロックが必要なのか」を、初めて本当に理解した。この本を先に読んでいたら、もう少し早く気づけたかもしれない。この辺はパタヘネ本など他にも良書があるのでそれらでもよい。モダンオペレーティングシステム 第5版 上作者:アンドリュー・S・タネンバウム,ハーバート・ボス日経BPAmazonモダンオペレーティングシステム 第5版 下作者:アンドリュー・S・タネンバウム,ハーバート・ボス日経BPAmazon5冊目：データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理OSの次は、分散システムだ。現代のアプリケーションは、1台のサーバーでは動かない。分散システム設計のあらゆるトピックを660ページに渡って網羅する、百科事典のような書籍。バックエンドエンジニアなら、いつかは読むべき本。データベース、レプリケーション、パーティショニング、トランザクション、分散システムの課題、バッチ処理、ストリーム処理。データを扱うシステムを設計する上で知っておくべき知識が、体系的に整理されている。この本の特徴は、何ができるか（WHAT）だけでなく、なぜそうなっているか（WHY）まで説明されていることだ。「なぜレプリケーションが難しいのか」「なぜ書き込み性能が高いマルチリーダーではなくシングルリーダーが広く使われているのか」。そういった「なぜ」を知ることができる。正直、難しい。分散システムに関わっていないと、なかなかピンとこない部分もある。入門として読む本ではない。でも、大規模でデータ量が多いアプリケーションを設計するときには、必ず役に立つ。2026年2月に原著の第2版が出版される予定だ。翻訳版も出てほしい。というか、出てくれ。頼む。この記事を定期的に更新するつもりなので、第2版が出たら差し替える。データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin Kleppmann,斉藤太郎,玉川竜司オライリージャパンAmazonプログラマーとしての姿勢を学ぶここまで、技術の土台とシステムの基盤について紹介してきた。ここからは、少し違う話をする。何を学ぶかではなく、どう向き合うかの話だ。技術は日々変わる。でも、変わらないものもある。良いコードを書くための考え方、問題に向き合う姿勢、キャリアを築くためのマインドセット。ここでは、プログラマーとしての「あり方」を教えてくれる本を紹介する。6冊目：達人プログラマー（第2版）熟達に向けたあなたの旅1999年に出版されて以来、世界中のプログラマーに読まれ続けている名著。2019年に20周年記念版として大幅に改訂され、第2版が出た。原題は「The Pragmatic Programmer」。Pragmaticとは、実用本位、実践的という意味だ。理論だけではなく、現場で使える知恵が詰まっている。この本の特徴は、コーディング技法だけでなく、エンジニアとしてのものの見方を教えてくれることだ。DRY原則、ETC原則（Easier To Change）、凝集度と疎結合。そういった技術的な話もあるが、それだけではない。開発の進め方、コミュニケーションの取り方、キャリアの考え方。プログラマーとして生きていくための姿勢が書かれている。「割れた窓」の話は有名だ。悪い設計、誤った意思決定、質の悪いコード。それを放置すると、ネガティブな考えが伝染する。だから、最初の「割れた窓」を見つけたら、すぐに直せ。自分もつい、割れた窓のようなコードを書いてしまったことがある。その後に若いプログラマに保守を任せたとき、いい書き方になっていなかった。元がよくない書き方だから、指摘するのも躊躇してしまう。「石のスープ」の話も印象的だ。大きな変化を一度に起こそうとすると、周囲は萎縮する。だから、小さく始めて、少しずつ巻き込んでいく。未来を少し垣間見せるだけで、みんな集まってくる。読み直すたびに、新しい発見がある。入門者には手引きとなり、ベテランでも読み返すたびに得るものがある。年に1回は読み返し、達人プログラマーを志していきたい。そういう本だ。20年以上読み継がれてきたからこそ、普遍的な価値がある。古い本だから読まなくていい、ということはない。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazon7冊目：プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則KISS、DRY、YAGNI、SOLID。プログラミングの世界には、先人たちが積み上げてきた原理原則がある。でも、それらを体系的に学ぶ機会は意外と少ない。現場で「DRYって何？」と聞かれて、ちゃんと説明できるだろうか。この本は、そういった原理原則を101個集めて、1冊にまとめたものだ。「3年目までに身につけたい」という副題がついているが、3年目以降の人が読んでも学びがある。むしろ、色々な現場を経験した人の方が、それぞれの原理原則の含蓄を感じられる。「あのとき、これを知っていれば……」と思うことが、きっとある。この本の特徴は、各項目に「なぜそれが必要か」が明確に説明されていることだ。Howだけでなく、Whyが書かれている。だから、抽象的な情報でありながら、実際に使える知識になる。「How to本」ならぬ「Why本」だ。もう1つの特徴は、各項目に出典書籍と関連書籍が記載されていることだ。「達人プログラマー」「アジャイルソフトウェア開発の奥義」「プログラマが知るべき97のこと」など、名著への参照がちりばめられている。次に読む本を選ぶときの索引としても使える。具体的なコード例がないことを不満に思う人もいるかもしれない。でも、それは意図的だ。言語に依存しないからこそ、どんな言語でプログラミングしていても適用できる。抽象度が高い分、適用範囲は果てしなく広い。本書で「抽象」を押さえたら、「具象」も押さえたい。コードの書き方を扱った本では、『リーダブルコード』（Dustin Boswell、Trevor Foucher著、2012年）が定番として挙げられることが多い。変数名の付け方、コメントの書き方、制御フローの整理。確かに実践的な内容だ。でも、私のおすすめは『ルールズ・オブ・プログラミング』（Chris Zimmerman著、2023年）の方だ。『ルールズ・オブ・プログラミング』は、『Ghost of Tsushima』を開発したSucker Punch Productionsで実際に使われている21のルールをまとめた本だ。「最適化の前に単純化せよ」「コードを制約で囲め」「プログラマーの時間はCPUの時間より貴重」。ゲーム開発という、パフォーマンスと保守性の両方が求められる過酷な現場で磨かれたルールには、説得力がある。syu-m-5151.hatenablog.comもし「リーダブルコードを読め」と勧めてくる人がいたら、「ルールズ・オブ・プログラミングは読みましたか？」と聞いてみてほしい。読んだ上でリーダブルコードを勧めているなら、それは信頼できる。読んでいないなら、まず読んでもらってから、改めて話を聞けばいい。プリンシプル オブ プログラミング 3年目までに身につけたい 一生役立つ101の原理原則作者:上田勲秀和システム新社Amazon技術以外のスキルを身につけるプログラミングができればエンジニアとして成功できる。そう思っていた時期が、私にもあった。でも、現実は違う。あるプロジェクトで、技術的には正しい提案をしたことがある。でも、通らなかった。別のエンジニアの、技術的にはやや劣る提案が採用された。理由は「あいつの方が話しやすい」「あいつの言うことなら安心できる」だった。悔しかった。でも、それが現実だった。技術力だけでは、キャリアは伸びない。なぜか。2つの構造的理由がある。1つは、評価の非対称性だ。あなたの技術力を正しく評価できる人は、組織の中に何人いるか。CTOと数人の先輩エンジニアくらいだろう。でも、あなたのコミュニケーション力は、同僚全員が評価できる。評価が多数決に近い以上、「多くの人に見えるスキル」を持つ人が有利になる。もう1つは、レバレッジの問題だ。自分一人の技術力には限界がある。でも、他者を巻き込む力は、レバレッジが効く。10人を動かせる人は、自分1人で10倍の成果を出す人より、組織では重宝される。これが良いことかどうかは別として、構造としてそうなっている。だから、技術以外のスキルも身につける必要がある。8冊目：SOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版技術書ではない。でも、エンジニアにとって必読の1冊だ。この本のサブタイトルは「ソフトウェア開発者の人生マニュアル」。技術習得法やキャリア構築法だけでなく、セルフマーケティング、生産性、資産形成、フィットネス、マインドセット。人生全般をより良く生きる方法が書かれている。「技術者の地位は技術力の高さではなく、他者の評価で決まってしまう」。これは厳しい現実だ。でも、現実を直視した上で、どうすればいいかを教えてくれる。キャリアをビジネスとして捉え、自分自身をマーケティングする。そういう視点を持つことの重要性が説かれている。正直に言うと、後半の資産形成やフィットネスの章は、ソフトウェア開発者に特化した話題ではない。不動産投資や筋トレの話がかなり詳しく書かれていて、「それ、この本でそこまで書く必要がある？」と思う人もいるだろう。私もそう思った。読む人を選ぶ本、という感想もある。でも、前半のキャリア、セルフマーケティング、学習、生産性の章は、間違いなく読む価値がある。技術力だけでは生き残れない時代に、何を身につけるべきか。その指針を与えてくれる。既に読者が若手ソフトウェアエンジニアの場合にはソフトウェアエンジニアガイドブック―世界基準エンジニアの成功戦略ロードマップも合わせておすすめしたい。SOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon設計とアーキテクチャを深める技術以外のスキルも大事だ。でも、技術を疎かにしていいわけではない。むしろ、技術力があってこそ、それ以外のスキルが活きる。コードが書けるようになったら、次は設計だ。どうやってモジュールを分けるか。どうやってシステム全体を構成するか。設計の良し悪しが、システムの保守性を決める。ここでは、設計とアーキテクチャについて学ぶ本を2冊紹介する。9冊目：アーキテクトの教科書 価値を生むソフトウェアのアーキテクチャ構築「アーキテクトになりたい」「アーキテクトとして成長したい」。そう思ったとき、何から始めればいいのか分からない人は多い。相談できる先輩や上司が身近にいないこともある。この本は、アーキテクティングという世界を探検するにあたっての「地図」となる本だ。アーキテクトの「最初の1冊」として、これ以上のものはない。第2章「ソフトウェア設計」では、V字モデル、4つの抽象（アーキテクチャ設計、モジュール設計、コンポーネント設計、クラス設計）、SOLID原則、設計パターンと、設計を語っていく上での基本概念が密度高く語られる。この章だけでも読んでおけば、設計の話をするときに「何を言っているのか分からない」という状態にはならない。オライリーの『ソフトウェアアーキテクチャの基礎』も良書だが、どこかアカデミックさがあり、ある程度の前提知識が要求される。それに比べて本書は、初学者にも分かりやすく書かれている。ユースケースに沿った解説があるのでおすすめである。第6章「アーキテクトとしての学習と成長」も見逃せない。普段のプロジェクトの中で表立って取り上げられることの少ないテーマだ。「自分がアーキテクトになっていくためにどんな心構えが必要なのか」と悩んでいる人には、とても学びの多い内容になっている。アーキテクトの教科書 価値を生むソフトウェアのアーキテクチャ構築作者:米久保 剛翔泳社Amazon10冊目：ソフトウェア設計の結合バランス 持続可能な成長を支えるモジュール化の原則「疎結合にしろ」「密結合は悪だ」。そういうスローガンは、現場でよく聞く。でも、疎結合とは、具体的にどの程度が「疎」なのか。それを説明できる人は、意外と少ない。この本は、「結合」という概念を徹底的に掘り下げた本だ。本書の主張は明快だ。結合をゼロにすることは不可能であり、むしろ適切な結合を選択することが重要。「疎結合至上主義」ではなく、「結合の均衡化（Balancing Coupling）」という視点を提示している。構造化設計におけるモジュール結合、オブジェクト指向におけるコナーセンス。それらを一通り説明した後、独自の「統合強度」モデルが導入される。強度・距離・変動性の関係性を解き明かし、実際の設計においてそれらをどう均衡化するのかが、具体例を用いて示される。印象的だったのは、結合の「距離」という概念だ。同じ強度の結合でも、それが文レベル、メソッドレベル、オブジェクトレベル、サービスレベルのどこに存在するかによって、変更のコストが大きく異なる。マイクロサービスアーキテクチャの設計において、この視点は特に重要だ。この本は手順書でもルールブックでもない。この本に書かれている通りにモジュール設計をすれば自然とバランスの良い設計になる、という話ではない。でも、方針決定やレビュー時に迷ったとき、この本に書かれているような発想をインプットに意思決定すると、判断の精度が上がる。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonおわりに10冊を紹介した。この記事を読んだからといって、明日から何かが変わるわけではない。たぶん来週も、再来週も、同じような日々が続く。10冊すべてを読む必要もない。というか、いきなり10冊読み終わることなんてない。自分も、速読で済ませようとしたことがある。でも、身につかなかった。1冊読んで、合わなければ閉じればいい。それでいい。派手な近道はない。地味な積み重ねだけがある。常に今の自分で戦うしかない。1つだけ、注意しておきたいことがある。誰かを冷笑したり、バカにしたりするのは楽だ。でも、その道に未来はない。他人をバカにしない唯一の方法は、自分が自分の枠の中で精一杯頑張ることだ。精一杯やっている人間は、他人を笑っている暇がない。syu-m-5151.hatenablog.com私も、達人と呼ばれたい者の1人だ。まだ諦めているわけではない。諦めているわけではないが、達人になれるかどうかは分からない。分からないまま、コードを書いている。本を読んでいる。冒頭で、本は問いをくれると書いた。知らなかった世界を見せてくれると書いた。10冊のうち、どれか1冊でも手に取ってもらえたら、と思う。読み終わったとき、新しい問いが生まれているかもしれない。「あ、これが分からなかったのか」。そう思えたら、その本は、あなたにとって正解だったのだと思う。本との出会いは、計画だけでは起きない。本屋に行くと、紹介した本の隣に、もっと自分に合った本が置いてあるかもしれない。図書館で棚を眺めていると、別の本が目に入るかもしれない。そういう出会いは、検索では起きない。AIにも、たぶん見つけられない。だから、本屋に行ってみてもいいかもしれない。図書館に寄ってみてもいいかもしれない。棚の前に立ってみる。それだけでいい。何かが始まるかどうかは、分からない。分からないが、始まるとしたら、たぶんそこからだ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RustでOry Hydra用認証プロバイダーを実装する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/06/004244</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/06/004244</guid>
            <pubDate>Mon, 05 Jan 2026 15:42:44 GMT</pubDate>
            <content:encoded><![CDATA[はじめに年が明けた。月曜日。エディタを開いている。認証プロバイダーを自分で実装できるか、と聞かれたら、たぶん「できる」と答える。OAuth2のRFCは読んだ。フローも理解している、と思う。ただ、「じゃあ書いて」と言われたとき、キーボードに手を置いたまま止まってしまうことがある。頭では分かっている。手が動かない。10年近くインフラやプラットフォームを触ってきた。認可の仕組みは何度も設計した。Kubernetesの認証、サービスメッシュの認可、アクセストークンの検証。それでも「Login Providerをゼロから書け」と言われると、急に自信がなくなる。分かっているはずなのに、分かっていない気がする。知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazon知ってるつもり～「問題発見力」を高める「知識システム」の作り方～ (光文社新書)作者:西林 克彦光文社AmazonOry Hydraのドキュメントを開く。Login ProviderとConsent Providerを自分で実装しろ、と書いてある。Node.jsのサンプルがある。Goのサンプルもある。どちらも動く。でも私はRustで書きたかった。年末年始、ぼんやり考えていて気づいたことがある。止まっているのは、技術的に難しいからではない気がする。「何をどの順番で実装すればいいのか」が見えていないのだ。全体像が掴めないまま、最初の一歩が踏み出せずにいる。だからこの記事を書くことにした。過去の自分に向けて。最初の一歩を、順番に。前提知識: この記事は前回の記事の続編です。OAuth2認可コードフローの基礎知識と、Ory Hydraのアーキテクチャ（Login/Consent Providerの役割）を理解している前提で進めます。syu-m-5151.hatenablog.com作るものLogin/Consent Providerとは、Ory Hydraと連携してOAuth2認証フローを処理するWebアプリケーションだ。以下の5つのエンドポイントを実装する。 エンドポイント  役割  GET /login  ログインフォームを表示する  POST /login  認証処理を行い、Hydraに結果を通知する  GET /consent  スコープ承認画面を表示する  POST /consent  承認結果をHydraに通知し、トークン発行へ進む  GET /logout  ログアウト処理を行い、セッションを破棄する 全体の流れOAuth2認可コードフローの中で、Login/Consent Providerがどう動くかを示す。1. ユーザーがクライアントアプリで「ログイン」をクリック2. クライアントがHydraの /oauth2/auth にリダイレクト3. Hydra が Login Provider の GET /login にリダイレクト（login_challenge付き）4. Login Provider がログインフォームを表示5. ユーザーがメール・パスワードを入力して送信6. Login Provider が認証し、Hydra に accept_login を送信7. Hydra が Consent Provider の GET /consent にリダイレクト（consent_challenge付き）8. Consent Provider がスコープ承認画面を表示9. ユーザーが承認10. Consent Provider が Hydra に accept_consent を送信11. Hydra がクライアントにリダイレクト（認可コード付き）12. クライアントが認可コードをトークンに交換Login/Consent Providerが担当するのは3〜10だ。Hydraとの通信には6つのAPIを使う。 API  役割  GET /admin/oauth2/auth/requests/login  login_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/login/accept  認証成功をHydraに通知  GET /admin/oauth2/auth/requests/consent  consent_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/consent/accept  承認結果をHydraに通知  GET /admin/oauth2/auth/requests/logout  logout_challengeからリクエスト情報を取得  PUT /admin/oauth2/auth/requests/logout/accept  ログアウトをHydraに通知 www.ory.comLogin HandlerLogin Handlerは2つのエンドポイントで構成される。GET /loginクエリパラメータからlogin_challengeを取得するHydra APIでlogin_challengeを検証し、リクエスト情報を取得するskipフラグが立っていれば（既にセッションがあれば）、フォームを表示せず即座にaccept_loginそうでなければログインフォームを表示するPOST /loginフォームからemail、password、login_challengeを受け取る認証サービスでパスワードを検証する認証成功なら、ユーザー情報をcontextに詰めてaccept_loginを呼ぶHydraが返すリダイレクトURLへ転送するpub async fn login_submit(    State(state): State<AppState>,    Form(form): Form<LoginForm>,) -> Result<Redirect, AppError> {    // 1. 認証処理    let user = state.auth.authenticate(&form.email, &form.password).await?;    // 2. ユーザー情報をcontextに保存（Consent時にDBルックアップ不要）    let user_context = UserContext {        email: user.email.clone(),        role: "customer".to_string(),        tenant_id: None,    };    // 3. Hydraに認証成功を通知    let completed = state        .hydra        .accept_login(            &form.login_challenge,            &user.id.to_string(),            false,            Some(serde_json::to_value(&user_context)?),        )        .await?;    // 4. Consent画面へリダイレクト    Ok(Redirect::to(&completed.redirect_to))}ポイントはcontextだ。Login時に認証したユーザー情報（email、role、tenant_id）をJSON形式で保存し、Consent Providerへ受け渡す。これにより、Consent処理でDBルックアップが不要になる。Consent HandlerConsent Handlerも2つのエンドポイントで構成される。GET /consentクエリパラメータからconsent_challengeを取得するHydra APIでリクエスト情報（要求されたスコープ、クライアント情報）を取得するskipフラグが立っていれば（既に承認済みなら）、即座にaccept_consentそうでなければスコープ承認画面を表示するPOST /consentフォームからconsent_challengeと承認するスコープを受け取るLogin時に保存したcontextからユーザー情報を取得するIDトークンにカスタムクレーム（email、role、tenant_id）を追加するaccept_consentを呼び、Hydraが返すリダイレクトURLへ転送するIDトークンにクレームを追加することで、クライアントアプリケーションはトークンをデコードするだけでユーザー情報を取得できる。Logout HandlerLogout Handlerは1つのエンドポイントで構成される。Login/Consentと比べてシンプルだ。GET /logoutクエリパラメータからlogout_challengeを取得するHydra APIでaccept_logoutを呼び出すHydraが返すリダイレクトURLへ転送するpub async fn logout_handler(    State(state): State<AppState>,    Query(query): Query<LogoutQuery>,) -> Result<Redirect, AppError> {    let completed = state.hydra.accept_logout(&query.logout_challenge).await?;    Ok(Redirect::to(&completed.redirect_to))}ログアウトフローはLogin/Consentと異なり、確認画面を表示せずに即座にaccept_logoutを呼んでいる。本番環境では「本当にログアウトしますか？」という確認画面を挟むことを検討してもよい。動作確認docker compose up -d./scripts/e2e-test.shIDトークンにemail、role、subが含まれていれば成功だ。ここまでが「何を作るか」「どう動くか」の説明だ。以降は実装の詳細に入る。認証サービスの実装Login Handlerから呼び出される認証サービスの実装に入る。パスワード認証にはOWASPのガイドラインに従い、Argon2idを採用した。cheatsheetseries.owasp.orgArgon2::default()を使っているが、これは意図的だ。argon2クレートのデフォルト値はOWASP推奨設定に準拠している。「専門家が作ったものを信頼する方が合理的」という前回の記事と同じ論理だ。認証部分で見落としがちなのが次の点だ。pub async fn authenticate(&self, email: &str, password: &str) -> Result<User, AppError> {    let users = self.users.read().await;    let user = users.get(email).ok_or(AppError::InvalidCredentials)?;    Argon2::default()        .verify_password(password.as_bytes(), &parsed_hash)        .map_err(|_| AppError::InvalidCredentials)?;    Ok(user.clone())}ユーザーが存在しない場合も、パスワードが間違っている場合も、返すエラーは同じInvalidCredentialsだ。「ユーザーが見つかりません」というエラーを返したくなるが、それは攻撃者に情報を与えてしまう。これはユーザー列挙攻撃（User Enumeration Attack）への対策だ。攻撃者はまず有効なメールアドレスを特定しようとする。エラーメッセージが違えば、登録済みかどうかが分かってしまう。なお、完全な対策にはタイミング攻撃への考慮も必要だ。ユーザーが存在しない場合はArgon2の検証が走らないため、レスポンス時間の差で存在を推測される可能性がある。本番環境では、ユーザー不在時もダミーハッシュを検証することを検討してほしい。owasp.orgテスト設計認証システムのバグは「静かに」起きる。だからテストの考え方も変わる。普通の機能開発では「この操作をしたらこうなる」というテストを書く。でも認証システムでは「この操作をしてもこうならない」というテストの方に価値がある。#[tokio::test]async fn test_login_does_not_reveal_user_existence() {    let service = AuthService::new();    service.register("exists@example.com", "password").await.unwrap();    let err1 = service.authenticate("exists@example.com", "wrong").await.unwrap_err();    let err2 = service.authenticate("nobody@example.com", "password").await.unwrap_err();    assert_eq!(err1.to_string(), err2.to_string());}このテストは「エラーメッセージが同じ」という実装の意図を明示化している。将来誰かが「親切なエラーメッセージにしよう」と思って変更しても、このテストが警告を出す。責任分界点全ての攻撃をアプリケーション層で防ぐ必要はない。何を守り、何をインフラに任せるかを明確にする。ブルートフォース対策: Nginxのrate limitで弾くセッション固定化攻撃: フレームワーク（Axum + tower-sessions）に委譲HTTPS強制: インフラ設定の問題プロジェクト構成今回はAxumを使った。github.comsrc/├── main.rs          # サーバーエントリーポイント├── auth.rs          # 認証サービス├── handlers.rs      # Login/Consent/Logoutハンドラー├── hydra.rs         # Hydra Admin APIクライアント├── models.rs        # Hydra API型定義└── error.rs         # エラー型定義ハンドラー層とサービス層を分離している。認証ロジックはauth.rsに置き、ハンドラーはHTTPリクエストの受け取りとレスポンスの返却だけを担う。フルコードはGitHubリポジトリを参照してほしい。github.com実装チェックリスト必須の実装[ ] Hydra APIクライアント - 6つのAPI呼び出し[ ] GET /login - login_challenge検証、skipフラグ確認、フォーム表示[ ] POST /login - 認証、contextにユーザー情報、accept_login[ ] GET /consent - consent_challenge検証、skipフラグ確認、承認画面表示[ ] POST /consent - context取得、IDトークンにクレーム追加、accept_consent[ ] GET /logout - logout_challenge取得、accept_logout[ ] 認証サービス - Argon2id、ユーザー列挙攻撃対策忘れがちなポイントlogin_challengeとconsent_challengeはhiddenフィールドでフォームに埋め込むskipフラグが立っている場合は画面を表示せず即座にacceptするcontextでLogin→Consent間のユーザー情報受け渡しエラーメッセージはユーザーの存在を漏らさないおわりにこの文章を書き終えて、ターミナルに戻った。docker compose up -dを叩く。コンテナが立ち上がる。E2Eテストを走らせる。グリーン。IDトークンにemailとroleが入っている。動いた。正直に言うと、書いている途中で何度か不安になった。これで説明になっているのか。Login HandlerとConsent Handlerの違いが曖昧になっていないか。contextの使い方は2回書き直した。それでも、動いた。冒頭で書いた「キーボードに手を置いたまま止まってしまう」感覚は、たぶん、また来る。次に認証システムを書くときも、OAuth2のフローを思い出すところから始めるだろう。login_challengeって何だっけ、と調べ直すかもしれない。それでいいのだと思う。認証は「一度理解したら終わり」という領域ではない気がする。毎回、RFCを確認しながら、慎重に実装する。ユーザー列挙攻撃のテストを書いたのも、将来の自分が「親切なエラーメッセージ」を入れようとしたときに止めるためだ。年が明けて、また仕事が始まる。本番の認証システムはOry Hydraに任せる。Login Providerは自分で書く。その境界線が、今の私には見えている気がする。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、辞めるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/05/090020</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/05/090020</guid>
            <pubDate>Mon, 05 Jan 2026 00:00:20 GMT</pubDate>
            <content:encoded><![CDATA[はじめにかつての私は、深夜2時にベッドの中で転職サイトを開いていた。開いて、求人を眺めて、閉じて、また開く。そういうことを繰り返していた。辞めたいのか、と聞かれると困った。会社の限界が見えたのか。自分の天井が見えたのか。それとも、隣の芝生の青さに目が眩んでいただけなのか。たぶん、全部だった。たぶん、どれでもなかった。今は、転職を考えていない。これは「今の会社が最高だから」という話ではない。どんな会社にも良い面と悪い面がある。不満がゼロになることはない。ただ、深夜に転職サイトを開く衝動は、いつの間にか消えた。何が変わったのか。環境が変わったのか、自分が変わったのか。たぶん、両方だ。「エンジニアは転職で年収が上がる」「成長できる環境に身を置け」——そんな言葉がタイムラインに流れてくる。転職エージェントからのスカウトメールは週に何通も届く。カジュアル面談のお誘い。年収アップの可能性。もっと刺激的な環境。全部、本当のことだと思う。全部、嘘だとも思う。若いエンジニアが短期的にモノを考えてしまうのは、仕方がない。私もそうだった。目の前の不満が大きく見える。3年後、5年後のことなんて、想像できない。「今すぐ環境を変えたい」という衝動は、若さゆえの特権でもある。その衝動を否定するつもりはない。ただ、かつての自分に言いたいことがある。「おい、ちょっと待て」と。私自身、何度も転職を考えた。「もう限界だ」「ここにいても意味がない」「他の会社ならもっとできるはずだ」——そう思って、転職サイトを眺めた夜は数えきれない。そして、実際に転職したこともある。転職して正解だったケースもあった。「あのタイミングで辞めなくてよかった」と思うケースもあった。だから、この記事で「辞めるな」と書くのは、上から目線のアドバイスではない。かつての自分への手紙だ。あのとき、もう少し踏みとどまっていたらどうなっていたか。もう少し早く辞めていたらどうなっていたか。そういう問いを、今も抱えている。——もし読んでいて上から目線に感じたなら、それは私の力量不足だ。申し訳ない。ある日、気づいたことがある。深夜に転職サイトを開く自分と、翌朝それを後悔する自分は、同じ人間なのに、まったく違うことを考えている。どちらが本当の自分なのか。たぶん、どちらも本当だ。だから困る。この記事は、深夜の衝動と、翌朝の冷静さの、両方に向けて書いている。この記事が、辞めそうな若手に上司から共有されないことを祈っている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しい。「転職しやすい」という罠ITエンジニアは「転職しやすい職業」だと言われる。確かにそうだ。求人は多い。売り手市場だ。スキルがあれば、転職先を見つけることは比較的容易だろう。だが、「転職しやすい」ことと「キャリアを作れる」ことは、全く別の話だ。私自身、この罠にはまった。転職市場で「引く手あまた」だった時期がある。スカウトメールは毎週届いた。カジュアル面談をすれば、たいてい次のステップに進めた。「自分は市場価値が高い」と思っていた。でも、それは錯覚だった。振り返ると、私は「転職できる」ことと「キャリアを積み上げている」ことを混同していた。転職市場で需要があるのは、単に「エンジニアが足りない」からだ。私個人の価値が高いわけではない。需要と供給のバランスが崩れているだけ。その状況に甘えて、「いつでも転職できる」という安心感に浸っていた。「3年で転職すれば年収が上がる」という話もある。だが、これは単純化しすぎた話だ。実際には、年収が上がる転職もあれば、上がらない転職もある。そして、年収が上がらない転職の方が、実は多い。なぜか。転職には必ずロスが発生するからだ。私が転職したとき、最初の3ヶ月は本当に苦しかった。前職では「あいつに聞けば分かる」と言われていた領域があった。コードベースを熟知していた。誰に何を聞けばいいか知っていた。暗黙のルールも把握していた。転職した瞬間、それが全部ゼロになった。会議で発言しても、「この人、誰？」という空気が流れる。提案しても、文脈を知らないから的外れになる。前職では30分で終わる作業が、3時間かかる。「俺はもっとできるはずなのに」——そう思いながら、毎日を過ごしていた。これが「転職のロス」だ。どんなに経験者であっても、新しい会社のコンテキストをつかむには時間がかかる。前職で積み上げた信頼貯金は、転職した瞬間にリセットされる。私がこの記事で伝えたいのは、現場で働いてきた人間としての実感だ。机上の空論ではなく、実際に転職を経験し、成功も失敗もしてきた中で気づいたことを書く。一見「転職しやすい」ように見えるITエンジニアほど、実は「キャリアを作ること」が難しい——これが私の結論だ。転職のハードルが低いからこそ、安易に転職してしまう。そして、キャリアが積み上がらないまま、年齢だけが積み上がっていく。ただ、ここまで書いてきて、誤解されたくないことがある。「辞めたい」と思うのは、悪いことではない「転職には罠がある」と書いた。でも、それは「辞めたいと思うこと自体が悪い」という意味ではない。ここで1つ、大事なことを言っておきたい。「辞めたい」と思うこと自体は、悪いことではない。むしろ、自然なことだ。どんな会社にも、良い面と悪い面がある。仕事には波がある。うまくいく時期もあれば、何をやってもダメな時期もある。人間関係でストレスを感じることもある。深夜2時に転職サイトを眺める。上司との関係がうまくいかなくて、帰りの電車で「もう嫌だ」と思う。日曜の夜、明日会社に行きたくないと感じる。そういう瞬間は、誰にでもある。私にもあった。今でもある。だから、この記事を読んで「辞めたいと思っている自分はダメだ」とは思わないでほしい。辞めたいと思うことと、実際に辞めることは、別の問題だ。ただ、この分離は言うほど簡単ではない。深夜2時に転職サイトを見ているとき、「これは感情だ、今は判断するな」と冷静に思える人がどれだけいるだろうか。私自身、何度も感情に流されて判断しそうになった。だから、私は自分にルールを課している。1回目で決めるな。深夜のベッドで「辞めたい」と思った。それは1回目だ。まだ決めるな。翌週、上司に理不尽なことを言われて「辞めたい」と思った。まだ決めるな。1ヶ月後、半年後、同じ状況で同じことを思うか。時間をかけて、何度も問い直せ。衝動ではなく、熟慮の末に出した答えなら、それが「辞める」でも「残る」でも、後悔は少ない。要するに、短期ではなく長期で考えろ、ということだ。目の前の感情に振り回されるな。5年後、10年後の自分がどうなっていたいか。そこから逆算して、今の決断を考えろ。正直に言えば、3年程度では何も身についていない。「3年経験があります」と言っても、それは今の環境が整っている状況で、その能力が発揮できる程度だ。上司が調整してくれて、先輩がフォローしてくれて、チームが支えてくれて、ようやく成果が出せている。その支えがなくなった瞬間、同じパフォーマンスが出せるか。出せないなら、それは本当に「能力」と呼べるのか。感情は感情として受け止めていい。ただ、その感情だけで大きな決断をしないでほしい。この記事は、そのための材料を提供したいと思っている。では、冷静に考えるとは、具体的に何を考えればいいのか。次に目指す役割を明確にするまず最初に考えるべきは、「次にどこへ向かいたいのか」だ。エンジニアのキャリアには、いくつかの方向性がある。技術を深める方向——テックリードやスペシャリストだ。特定の領域で「この人に聞けば分かる」と言われる存在になる。アーキテクチャの意思決定を任される。難しい技術的課題を解決する。人を率いる方向——エンジニアリングマネージャー（EM）だ。チームの生産性を最大化する。メンバーの成長を支援する。採用や評価といった組織課題に向き合う。事業に近づく方向——プロダクトマネージャーや、ビジネスサイドとの橋渡し役だ。「何を作るか」を決める側に回る。技術とビジネスの両方を理解し、最適な解を見つける。ここで強調しておきたいのは、IC（Individual Contributor）トラック——部下を持たずに技術で貢献し続けるキャリアパス——という選択肢の存在だ。スタッフエンジニア、プリンシパルエンジニアといった役職は、マネージャーにならずとも、より大きなインパクトを生み出す道だ。マネジメントだけが「上」ではない。シニアの先には4つの方向性がある。テックリード（チームの技術方針を導く）、アーキテクト（システム設計の意思決定を担う）、ソルバー（組織横断の難問を解決する）、ライトハンド（経営層の右腕として動く）。どれを目指すかで、求められるスキルセットも変わる。全部できる必要はない。どれを選ぶかは、あなた次第だ。重要なのは、スタッフエンジニアは「シニアのシニア」ではないということだ。役割そのものが変わる。コードを書く時間は減り、リーダーシップ、ファシリテーション、組織の接着剤としての仕事が増える。「もっとコードを書きたい」という人には向かない道だ。だから、「シニアになったら自動的にスタッフを目指す」という発想は危険だと私は思っている。多くのエンジニアは、最初は「一人前の開発者」からスタートする。そこから、どの方向に進むか。それを決めるのは、あなた自身だ。ここで自分に問いかけてほしい。あなたは次にどの方向に進みたいのか。それが言語化できていないなら、転職を考えるのはまだ早い。なぜなら、方向が定まっていない転職は、ただの「移動」に過ぎないからだ。移動しても、キャリアは積み上がらない。方向性を考えることと同じくらい大事なことがある。「自分は今、どこにいるのか」を知ることだ。自分の能力を棚卸しする目指す方向が見えてきたとしよう。でも、その方向に進むためには、今の自分の立ち位置を正確に把握する必要がある。転職を考えるとき、多くの人は外側に目を向ける。「あの会社は良さそうだ」「この技術を使ってみたい」「あの人みたいになりたい」。でも、本当に大事なのは、自分という器がどうなっているかを知ることだ。どんなに良い環境に移っても、器が変わらなければ、入ってくるものは同じだ。逆に、自分の器をちゃんと理解していれば、今の環境でも次の環境でも、適切な選択ができる。ここで、転職を考える前に確認してほしいことがある。自分の「実力」を正しく評価できているか、ということだ。私は長い間、この評価を間違えていた。ゾーンに入って神がかった速度でコードを書く自分、難解なバグを一瞬で特定する自分——そういう「最高の瞬間」を「自分の実力」だと信じていた。だから、転職先でも同じパフォーマンスが出せると思っていた。逆だった。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その泥のような日に絞り出したアウトプット。それこそが、紛れもない私の「実力」だ。絶好調のときの成果は、再現性のない「運」や「上振れ」に過ぎない。転職先で、その「上振れ」を再現できる保証はどこにもない。なぜこれが転職を考えるときに重要なのか。信頼は「下限」に支払われるからだ。新しい職場で、あなたは「最高の自分」ではなく「最悪の自分」で評価される。慣れない環境、知らないコードベース、初対面のチームメンバー。その状況で出せるアウトプットが、あなたの「実力」として記録される。「本当はもっとできるんです」は通用しない。だから、転職先を選ぶときに問うべきは、「最高の自分が活躍できる場所か」ではない。「最悪の自分でも、最低限のパフォーマンスを出せる場所か」だ。もう1つ、能力について知っておくべきことがある。能力は文脈の中にしかない。今の環境で「できる人」だとしても、それは文脈に依存している。私自身、痛い目を見た。あるプロジェクトで成果を出せたとき、私はそれを自分の実力だと思っていた。でも振り返ると、違った。上司が事前に関係者と調整してくれていた。マネージャーがスコープを適切に切ってくれていた。先輩が技術的な地雷を踏む前に教えてくれていた。私は、応援してくれて、調整してくれていたマネージャーや上司の能力まで、自分の能力だと勘違いしていた。その支えが消えた環境で、同じパフォーマンスを出せるか。出せるわけがない。正しい認識はこうだ。「この文脈において、これまでの経験と周囲のサポートが噛み合って、たまたま価値が出せている」。では、その「器」——能力——は、どう捉えればいいのか。大きく分けて3つの軸がある。技術力——コードを書く力だ。設計力、実装力、レビュー力。特定の領域を深掘りする「スペシャリスト」か、複数の領域をカバーする「ジェネラリスト」か。どちらを目指すにせよ、ここが基盤になる。推進力——プロジェクトを前に進める力だ。タスクを完遂できるか。障害にぶつかっても解決策を見つけられるか。チームのボトルネックを解消できるか。「なぜこの機能が必要か」というビジネス課題を理解し、技術的な意思決定をビジネスインパクトで説明できるか。影響力——自分の外側に価値を生み出す力だ。チームへの影響力は、採用、オンボーディング、ドキュメント整備、勉強会の開催など。社外への影響力は、技術ブログ、カンファレンス登壇、OSS貢献など。どの軸を伸ばすかは、目指す役割によって変わる。テックリードを目指すなら技術力と推進力。EMを目指すなら推進力と影響力。スペシャリストを目指すなら技術力を極める。重要なのは、全部を上げようとしないことだ。自分が目指す役割に必要な能力を見極めて、そこに集中する。ここで、私自身の失敗を話したい。かつての私は「良いコードを書いていれば、いつか評価される」と思っていた。技術力さえあれば、周りが認めてくれる。黙々と良い仕事をしていれば、誰かが見ている。——甘かった。現実はこうだ。見えない仕事は、存在しないのと同じ。どんなに素晴らしい設計をしても、それを言語化して共有しなければ、誰も知らない。どんなに難しいバグを直しても、「大変だった」と伝えなければ、簡単な修正だと思われる。「仕事をやり遂げる人」として認められるには、技術的な能力だけでなく「何が重要かを見極める力」と「自分の仕事を周囲に伝える力」が必要だ。この2つを、私は長い間、軽視していた。「アピールするのは恥ずかしい」「実力で示せばいい」——そう思っていた。でも、それは傲慢だった。相手の時間を奪わずに、自分の仕事の価値を簡潔に伝えること。それはコミュニケーションスキルであり、チームで働く上での基本的な作法なのだ。つまり、私は「技術力」に過剰投資し、「推進力」と「影響力」に過少投資していた。多くのエンジニアは、同じ罠にはまる。新しいフレームワークを学ぶ。新しい言語を触る。それは楽しいし、成長した気になる。だが、「推進力」——泥臭い調整や、やり切る力——の不足から目を背けていないか。技術力があっても、プロジェクトを完遂できなければ、市場価値は上がらない。今の会社を辞めようとしているあなた。この3つの軸で自分を評価してみてほしい。次に目指す役割に対して、どの軸が足りていないのか。それが明確になっていないなら、転職しても同じ困難にぶつかる。環境を変えても、足りない能力は足りないままだ。ただ、ここで1つ付け加えたいことがある。能力を棚卸しするとき、多くの人は「足りないもの」ばかりを見る。私もそうだった。「技術力が足りない」「推進力が弱い」「影響力がない」——チェックリストを見て、できないことを数え上げる。そして、転職先を探すときも「ここに行けば○○が身につく」「あの会社なら△△を学べる」と、ないものを補う発想で動いてしまう。ないものを探し続けていたら、悩みは一生消えない。考えてみてほしい。どんな環境に行っても、足りないものは必ずある。新しい技術が次々に出てくる。上には上がいる。「あれもできない、これもできない」と数え上げれば、キリがない。そうやって「ないもの」を埋めようとしている限り、永遠に充足感は得られない。私自身、この罠に長い間はまっていた。「もっとコードが書けるようになりたい」「もっとコミュニケーション力をつけたい」「もっとビジネス視点を持ちたい」——足りないものリストは常に更新され続けた。そして気づいた。そのリストは、一生埋まらない。発想を変えよう。「ないものを探す」のではなく、「あるものを伸ばす」。あなたには、すでに強みがある。周囲より得意なことがある。それが何かを見極めて、そこに集中する。弱みを平均まで引き上げる努力は、強みを突き抜けさせる努力より、はるかに効率が悪い。私の場合、「調べること」「言語化すること」「ソフトウェアを実装すること」が比較的得意だった。コミュニケーション力が高いわけではない。政治的な立ち回りも苦手だ。でも、RFCやドキュメントを読み込んで理解し、それを実際に動くコードに落とし込み、さらに文章としてまとめることなら、周囲より少しだけ速かった。その「少しだけ」を、徹底的に伸ばすことにした。結果として、「あいつに任せれば、調べて、作って、ドキュメントにしてくれる」という評価が生まれた。これは戦略的な選択だ。何をやるかではなく、何をやらないか。弱みを気にして、あれもこれもと手を広げるのではなく、強みに絞って、そこで突き抜ける。だから、能力を棚卸しするとき、「足りないもの」だけでなく「すでにあるもの」にも目を向けてほしい。転職を考えるとき、「ここに行けば足りないものが補える」ではなく、「ここに行けば今の強みがさらに活きる」という視点で選んでほしい。足りないものは、一生足りない。だから、足りないものを数えるのをやめろ。今あるものを、もっと伸ばせ。正直に告白しよう。私には、仕事を選ぶときの悪い癖がある。小さなバグを直す。ドキュメントの誤字を修正する。チェックリストを埋めていく。1日の終わりに「今日も色々やった」と思える。でも、週末に振り返ると、本当にインパクトのある仕事をしたのか、分からなくなる。——これが、私の悪い癖だ。簡単で達成感はあるが、インパクトの低い仕事に逃げてしまう。お菓子をつまむように、小さなタスクをつまんでしまう。これが「スナッキング」だ。チェックリストを埋める快感は、脳にとって報酬だ。でも、その報酬に溺れて、本当に重要な仕事——曖昧で、難しくて、すぐに結果が出ない仕事——から逃げていないか。もう1つ、自分を戒めている罠がある。目立つが価値の低い仕事だ。社内の勉強会を頻繁に開く。Slackで積極的に発言する。目立つ。注目を集める。でも、ビジネスへの貢献は薄い。この罠にはまると、「忙しかった」と「成果を出した」を混同するようになる。振り返ってほしい。直近1ヶ月で、最もインパクトのあった仕事は何だったか。それに費やした時間は、全体の何割だったか。もし1割以下なら、残りの9割は「スナッキング」だった可能性がある。ここまで、「どこを目指すか」と「何を伸ばすか」について話してきた。では、実際に転職するとなったとき、何を失い、何を得るのか。その前に、転職を考えるときの大前提を確認しておきたい。「自分は会社にとって必要な存在だ」と思っているかもしれない。でも、それは本当だろうか。「替えが効く」という前提を認める別に会社なんていつ辞めても良い。文字通りの意味で替えの効かない人間なんて資本主義においては存在しない。これは冷徹な事実だ。どんなに優秀なエンジニアでも、会社は回る。あなたが辞めても、誰かが引き継ぐ。プロジェクトは続く。組織は適応する。「私がいないと回らない」——そう思いたい気持ちは分かる。でも、それは幻想だ。私自身、これを認めるのに時間がかかった。ある会社を辞めるとき、「自分がいなくなったら、あのシステムは誰がメンテするんだろう」と心配していた。3ヶ月後、元同僚に聞いた。「全然大丈夫だよ。○○さんが引き継いで、むしろ前より整理されてる」。——少し寂しかったが、同時にホッとした。そして気づいた。私は「替えが効かない」と思いたかっただけだ。この事実を認めることは、絶望ではない。むしろ、解放だ。「替えが効かない」と思い込んでいると、会社に縛られる。「私がいないと困る」「今辞めたら迷惑をかける」——そういう責任感は美しいが、それが「辞められない」という足枷になることがある。ブラックな環境でも我慢してしまう。メンタルを壊しても「今は辞められない」と言い聞かせる。替えが効くと認めることで、初めて「辞める」という選択肢が本当の意味で手に入る。ただし、ここで短絡的な結論に飛ばないでほしい。「替えが効く」→「だから辞めてもいい」——これは論理の飛躍だ。「替えが効く」から導ける結論は、もう1つある。「だから、どこに行っても価値を出せる能力を磨け」だ。会社にとって、あなたは替えが効く。だが、あなたにとって、積み上げた実績は替えが効かない。ここが重要だ。会社はあなたを手放せる。次の人を雇えばいい。でも、あなたが2年間かけて積み上げた信頼、ドメイン知識、人間関係——これは、転職した瞬間にリセットされる。会社にとっては「替えが効く」リソースでも、あなたにとっては「替えが効かない」資産なのだ。だから、問いはこうなる。「会社にとって替えが効く」という事実を認めた上で、「自分にとって替えが効かない資産」をどれだけ積み上げたか。信頼の複利、実績の蓄積、ドメイン知識——これらは「会社のため」に積み上げるのではない。「自分のため」に積み上げる。たまたま、その資産が今の会社で活きているだけだ。転職すれば、その一部はリセットされる。リセットされてでも得たいものがあるなら、辞めればいい。リセットするには惜しい資産があるなら、もう少し留まって、その資産を使い切ってから辞めればいい。「替えが効く」という事実は、転職を正当化する理由にも、現職に留まる理由にもなる。どちらの結論を導くかは、あなた次第だ。大事なのは、この事実を、感情的な決断の言い訳に使わないことだ。「どうせ替えが効くんだから、辞めてもいいでしょ」——それは、考えることを放棄している。「替えが効くからこそ、自分の資産を最大化する選択をする」——それが、戦略的な判断だ。この前提を踏まえた上で、いよいよ転職のコストについて考えよう。「替えが効く」からこそ、転職は自由にできる。だが、自由にできるからといって、コストがゼロなわけではない。転職は「投資」であり「リセット」である若さという資源は有限だ。私たちはキャリアを積む中で何かを投資し、その結果として何かを得ている。この構造を理解しないまま転職を繰り返すのは危険だ。20代の私は、この構造を理解していなかった。「若いうちは色々経験した方がいい」「転職で視野が広がる」——そういう言葉を真に受けて、2〜3年ごとに環境を変えていた。確かに視野は広がった。でも、振り返ると、広く浅くなっただけだった。新卒で未経験のうちは何もない。あるのはポテンシャルであり、若さであり、可能性だ。その資源を使い、何かしらの資産を得る必要がある。何を得るのか。それはスキルであり、それを活用した先の実績だ。実績は資産だ。そして資産には複利が効く。ある領域で実績を出すと、次はもう少し大きな仕事が回ってくる。それをこなすと、さらに大きな仕事が来る。「あの人はこの領域で結果を出した」という評判が、次の機会を連れてくる。これが複利だ。私が見てきた「キャリアがうまくいっている人」は、例外なくこの複利を回していた。1つの実績が次の実績を呼び、雪だるま式に大きくなっていく。逆に言えば、転職するたびにこの複利がリセットされる。転職するたびに、一定のロスが発生する。ビジネスドメインの理解、社内の人間関係、意思決定のプロセス、暗黙知として共有されている文化。これは、転職した瞬間にリセットされる。信頼貯金も同様だ。前職で積み上げた「あいつなら任せられる」という信頼は、新しい会社では通用しない。ゼロから積み上げ直す必要がある。この「リセットコスト」を、転職を考えるときに計算しているだろうか。私は、転職のリセットコストを「半年〜1年」と見積もっている。新しい環境でコンテキストをつかみ、信頼を積み上げ、本来のパフォーマンスを発揮できるようになるまでの時間だ。転職した直後の、あの居心地の悪さを覚えているだろうか。私が転職して最初の1週間、Slackの雑談チャンネルを眺めていた。前職では、私も会話の輪に入っていた。誰かが投稿すれば、すぐにリアクションをつけた。冗談を言えば、笑ってくれる人がいた。でも新しい会社では、誰も私のことを知らない。雑談チャンネルに何か書こうとして、やめた。「この人、誰？」と思われるのが怖かった。些細なことだ。でも、あの孤独感は今でも覚えている。前職では「あいつに聞けば分かる」と頼られていたのに、新しい会社では誰も自分を知らない。会議で発言しても、反応が薄い。提案しても、「この人は何者だ？」という目で見られる。チャットで質問しても、返事が遅い。——あの感覚は、信頼貯金がゼロになった瞬間だ。これが「信頼の貯金」だ。具体的に言おう。「あの件、○○さんに頼んでおけば大丈夫」——そう思われるまでに、どれだけの時間がかかっただろうか。最初は小さな仕事を任される。それを期限通りに、期待以上の品質で納める。次は少し大きな仕事を任される。また納める。この繰り返しで、「この人なら任せられる」という信頼が積み上がっていく。信頼があると、仕事が回りやすくなる。他のチームに協力を頼むとき、「あの人の頼みなら」と動いてもらえる。提案するとき、「あの人が言うなら、一度聞いてみよう」と耳を傾けてもらえる。逆に信頼がないと、どんなに正しいことを言っても、「あの人、誰？」で終わる。周囲があなたと一緒に働きたいと思う度合いが、あなたの成功を直接左右する。そして、この信頼の貯金は、転職した瞬間にゼロにリセットされる。前職で「あの人は信頼できる」と思われていても、新しい会社では関係ない。ゼロから積み上げ直すしかない。今の会社で、信頼貯金はどれくらい貯まっているか。その信頼貯金を使ってできる挑戦は、まだ残っていないか。せっかく貯めた信頼貯金を、使わずに捨てるのは、もったいなくないか。ここで、信頼貯金のROI（投資対効果）を考えてみてほしい。今の会社で積み上げた信頼があるからこそ挑戦できる「高難易度・高リターン」の仕事はないか。新規プロジェクトの立ち上げ。技術的負債の解消。チームの構造改革。こういう挑戦は、信頼がなければ任されない。信頼があるからこそ、「あいつに任せてみよう」となる。転職先で得られる期待値は、このリセットコストを支払ってでも余りあるほど高いか。その根拠は何か。「なんとなく成長できそう」ではなく、具体的に何を得られるのか。それを言語化できなければ、転職は「期待値の高い投資」ではなく、「よく分からないギャンブル」になる。ここまで、転職のコストについて話してきた。では、そのコストを支払う価値があるかどうかを判断するために、何を見ればいいのか。それは、今の場所で何を積み上げたか、だ。現職で何を成し遂げたか転職を考えるとき、多くの人は「次に何をしたいか」を考える。でも、その前に考えるべきことがある。現職で何を成し遂げたかだ。きつい言い方をする——これは私自身への言葉でもあるのだが——。転職する時に現職で主体的に動いて成し遂げた実績が語れなければ、現職の経験はエンジニアキッザニアに近い。シニアエンジニアやCTOが用意してくれた環境で、お膳立てされた仕事をこなしていただけ。新しいスキルが身についたとする。それは素晴らしい。でも、それだけでは足りない。そのスキルを使って、どのようなビジネス価値を出したのか。その過程でどう主体的に関わったのか。これが語れなければ、あなたは「お客さん」のままだ。もちろん、「キッザニア」も大事だ。お膳立てされた環境で体感したことは血肉になる。でも、それでいいのはある段階までだ。年収700万円、800万円、その先を目指すなら、「遊ばせてもらう側」から「遊び場を作る側」に回る必要がある。技術力だけでは昇進できない——これは誰でも言える。問題は、なぜ、分かっていても実践できないのかだ。「コードで問題を解決する」。それが私たちのアイデンティティだ。だから、可視化やスポンサー獲得を「政治的で汚い」と感じてしまう。「実力で認められたい」。その気持ちは痛いほど分かる。私もそうだった。でも現実は違う。技術的に正しい提案をしても、周囲を巻き込めなければ、提案は提案のまま終わる。「技術で解決できる」ことと「解決を任される」ことは、別の能力だ。私自身、昇進を見送られた経験がある。なぜ評価されないのか分からなかった。振り返って気づいた。上司が私のキャリア目標を察してくれることを、勝手に期待していた。「昇進したいです」と言ったことがあっただろうか。なかった。上司はエスパーではない。言わなければ、伝わらない。そしてもう1つ。技術的な正しさを組織に浸透させるのも、「技術」だ。相手の立場を理解し、伝わる言葉で説明し、合意を形成する。これを「政治」と呼ぶなら、政治もまた技術なのだ。そして、成果を出すだけで終わりではない。私は日報をつける習慣を大事にしている。Claude Codeを使って、日々の作業を記録している。何をやったか、何を学んだか、何に詰まったか。こうして記録しておけば、パフォーマンスレビューの自己評価で圧倒的に有利になる。半年前、1年前に何を達成したか、正確に思い出せるだろうか。記録がなければ、自分の成果を過小評価してしまう。成果を出すことと、成果を可視化することは、別のスキルだ。昇進には「スポンサー」と「可視化」が必要だ。スポンサーとは何か。あなたの成果を経営層に伝えてくれる人だ。上司や先輩の中に、「あいつは良い仕事をしている」と会議で言ってくれる人はいるか。人事評価の場で、あなたの名前を出してくれる人はいるか。いくら良い仕事をしても、上層部に伝わらなければ、昇進の話にはならない。スポンサーは単なる応援者ではなく、あなたのキャリアに実際に投資してくれる存在だ。可視化とは何か。自分の仕事の価値を、他人が理解できる形で残すことだ。「何を達成したか」「なぜそれが重要だったか」「組織にどう貢献したか」——これをドキュメントやSlackで発信しているか。戦略的に重要なプロジェクトに参加して、名前を売っているか。これが揃って初めて、「この人を昇進させよう」という話になる。ネットワークも重要だ。社内の同僚、社外のプロフェッショナル、経営層——この3方向の人脈を意識的に育てることで、キャリアの選択肢が広がる。転職を考えるなら、この3つのネットワークがどれだけ育っているか、自問してみてほしい。今、辞めようとしているあなたに問いたい。現職で、あなたは何を成し遂げたか。主体的に動いた結果として、何が変わったか。もし自分がその場にいなかったとしたら、結果はどう変わっていたか。「自分がいたからこそ生まれた差分」を言語化できるか。それが語れないなら、まだ辞めるタイミングではないかもしれない。少なくとも、もう一度自分に問いかける価値はある。ここで、よく聞く反論がある。「現職で成し遂げたいけど、もう成長の機会がないんです」——本当だろうか。この「成長できない」という感覚を、もう少し掘り下げてみたい。「成長できない」は本当か「もうこの場所では成長できない」これは、転職理由としてよく聞く言葉だ。刺激がなくなった。慣れてしまった。自分よりできる人がいない。だから、成長するために環境を変えたい。でも、本当にそうだろうか。それは本当に環境のせいなのか。厳しいことを言う。「成長できない環境」なんて、ほとんど存在しない。あるのは、今の自分の能力では打破できない環境だ。それは環境の問題ではなく、能力の問題だ。能力があれば、たいていの環境は打破できる。「この環境では無理だ」と言っているのは、「今の自分には無理だ」と言っているのと同じだ。だからこそ、転職には意味がある。——逆説的に聞こえるかもしれないが、聞いてほしい。能力を上げてから転職すれば、次の環境も打破できる。能力を上げずに転職しても、また同じ壁にぶつかる。「この環境では成長できない」と言って転職した人が、次の会社でも同じことを言っているのを、何度も見てきた。環境を変えても、能力が変わらなければ、結果は同じだ。逆に、今の環境で壁を打破する力をつけた人は、どこに行っても通用する。転職は「逃げ場」ではなく「能力を活かす場」として選ぶべきだ。今の環境で能力を証明してから、その能力をより活かせる場所に移る。それが、転職を「飛躍」にする唯一の方法だ。では、ここで言う「能力を上げる」とは、具体的に何を指すのか。そもそも「成長」とは何なのか。成長とは何か。新しい技術を触ることか。新しいフレームワークを学ぶことか。それらは成長の一部ではあるが、本質ではない。成長とは、「解ける問題の範囲が広がること」であり、「より大きな責任を担えるようになること」だ。シニアエンジニアへの成長で最も重要なのは、「どの問題を解くべきかを見極める力」だ。コードで問題を解くことと、そもそも「どの問題を解くべきか」を判断することは、まったく別のスキルだ。私自身、この違いを理解するのに時間がかかった。ある時期、私は「新しい技術を触れていないと成長が止まる」と焦っていた。業務ではレガシーなコードをメンテしている。新しいことを学べていない。だから成長していない。そう思い込んでいた。でも振り返ると、あのレガシーコードのメンテナンス期間こそ、私が最も成長した時期だった。複雑に絡み合った依存関係を解きほぐす力。ドキュメントがない状況で調査する力。リスクを見積もって段階的にリファクタリングする判断力。これらは、最新技術を追いかけていたら身につかなかった。その定義で考えたとき、今の環境で成長の余地は本当にないのか。もしかしたら、自分が「成長」と呼んでいるものが、単なる「刺激」ではないだろうか。新しい技術を触る刺激。新しいチームに入る刺激。新しいプロダクトに関わる刺激。刺激と成長は違う。刺激は消費されるが、成長は蓄積される。私が「成長できない」と感じていたとき、本当は「刺激がない」だけだった。成長の機会は目の前にあった。ただ、それが「地味でつまらない仕事」に見えていたから、気づかなかった。ここで、よく言われる教えについて考えてみたい。「一番の下手くそでいよう（Be the Worst）」——プログラマーの世界でよく引用される教えだ。自分より優れた人たちの中に身を置くことで、自分も成長できる。だから、自分が一番下手くそになれる環境を探せ、と。この教えは正しい。でも、これを全員が実践したら、組織は成り立たない。全員が「学ぶ側」を求めて、誰も「教える側」に回らなかったら、どうなるか。優秀な人が集まる環境は、誰かが「教える側」を引き受けてくれているから成立している。「一番の下手くそでいよう」という教えは、その前提を無視している。——というのは、批判としては正しい。ただ、この教えの本質は、「常に学び続けろ」ということだ。「教える側」に回っても、学びは止まらない。むしろ、教えることで自分の理解の穴が見つかる。成長の形が変わるだけで、成長自体は続く。「もうこの場所では成長できない」と感じたとき、立ち止まって考えてほしい。自分は「学ぶ側」でいることしか考えていないのではないか。新しい技術を教わりたい。優秀な先輩からコードレビューを受けたい。それは大事だ。だが、いつまでも「教わる側」にいるわけにはいかない。「教える側」に回ったとき、別の成長が始まる。後輩のコードをレビューすることで、自分の理解の穴が見つかる。ドキュメントを整備することで、暗黙知が言語化される。勉強会を開くことで、チーム全体の底上げができる。そして何より、「自分がいないと回らない」から「自分がいなくても回る」状態を作ることが、次のステージへの準備になる。「接着剤の仕事」というものがある。チーム間の調整、ドキュメント整備、後輩の面倒を見る——コードを書かないが、チームを機能させるために不可欠な仕事だ。日本企業では、この仕事は評価されにくい。「○○さんはコード書いてないよね」と言われがちだ。でも、シニアレベルでこれをやると「リーダーシップを発揮している」と見なされることもある。上司とすり合わせて、この仕事がキャリアにどう評価されるか確認しておいた方がいい。評価されないなら、やりすぎは損だ。効果的なメンタリングとは何か。良いメンターはすぐに答えを与えない。複数の選択肢を提示し、メンティー自身に考えさせる。そして、自立を促す。メンタリングを受ける側も、答えを教えてもらうことを期待するのではなく、自分で考える姿勢が求められる。もし今の環境で良いメンターがいるなら、それは転職で失う大きな資産の1つだ。今の環境で、より大きな責任を担う機会はないか。より難しい問題に挑戦する機会はないか。それを探さずに「成長できない」と言っているなら、次の環境でも同じことが起きるだろう。ここまで、「成長できない」という感覚について掘り下げてきた。成長の機会は、案外、目の前にあるかもしれない。ただ、それでも「辞めたい」という気持ちが消えない人もいるだろう。次の問いは、より厳しいものになる。転職は「逃げ」になっていないか転職を繰り返す人の中に、あるパターンがある。新しい会社に入る。最初の半年は必死でキャッチアップする。コードベースを読み、ドメイン知識を吸収し、チームの信頼を獲得する。1年が経つ頃には「だいたい分かった」という感覚が出てくる。そして、ふと気づく。「あれ、最近あまり成長していない気がする」。ここで選択肢が2つある。今の環境で次のステージに挑戦するか、また新しい環境に移るか。後者を選び続けると、こうなる。キャッチアップが終わるたびに「成長が止まった」と感じ、また次の会社に行く。新しい環境でのキャッチアップを「成長」だと錯覚する。でも、それは成長ではない。ただの適応だ。本当の成長は、適応が終わった後にある。その環境で自分なりの仮説を持ち、試行錯誤し、失敗し、そこから学ぶ。大きなプロジェクトをやり遂げる。チームを任される。技術的な意思決定を下す。そういう経験を積んで初めて、次のステージに進める。転職を繰り返すたび、この「本当の成長」への到達前にリセットがかかる。結果、いつまでも「一人前の開発者」のまま、年齢だけが進んでいく。私自身、このリセットの苦しさを身をもって経験した。自社開発からSRE支援の会社に転職したとき、リセットが1回では済まないことを思い知った。支援先が変わるたびに、文脈がリセットされる。コードベース、チームメンバー、組織文化——全部ゼロから。しかも「支援」として来ている以上、キャッチアップ期間なんてない。初日から「で、何ができますか？」と問われる。最初は本当に苦しんだ。広い視野は得られたが、深さが積み上がらない。ある現場で得た知見を次の現場で活かそうとしても、文脈が違いすぎて通用しない。そして何より、信頼の蓄積がリセットされ続ける。ある支援先で信頼を獲得しても、次の案件ではまたゼロからだ。この経験から学んだことがある。転職のリセットコストは、転職先の業態によって大きく変わる。自社開発から自社開発への転職なら、リセットは1回で済む。でも、支援会社やコンサル、技術顧問に転職すると、リセットが繰り返し発生する。その覚悟があるかどうか、転職前に考えておくべきだ。この経験を通じて、私が学んだ原則がある。「自分の決定の結果を見届けられるだけの期間、同じ場所に留まれ」。成長のフィードバックループを回すためだ。設計した仕組みが半年後にどう使われているか。提案した施策が1年後にどんな結果を生んだか。それを見届けずに次の環境に移ったら、学びは半分で終わる。もう1つ、「許可を求めるな、宣言しろ」という原則がある。「○○してもいいですか？」ではなく、「○○します。問題があれば教えてください」と発信する。異論があれば誰かが止めてくれる。このスタイルで動けるようになると、権限がなくても物事を前に進められる。日本企業では「根回し」が重要だと言われる。それは間違いではない。でも、根回しにも2種類ある。「許可を得るための根回し」と「宣言を通すための根回し」だ。後者の方が、物事が前に進む。逆に、常に許可を求めないと動けない状態なら、まだその環境で信頼貯金が足りていない。その信頼を積み上げる前に辞めるのは、もったいない。ここで、このセクションの問いに戻ろう。「転職は『逃げ』になっていないか」。「今の環境では成長できない」と感じたとき、一度立ち止まって考えてほしい。それは本当に環境の限界なのか。それとも、環境には問題がないのに、難しいことから逃げているだけではないか。——私自身も、この問いに何度も向き合ってきた。そして正直に言えば、「逃げ」だったこともある。「退屈だが重要な課題」を解決することから目を背けて、「新しくて刺激的な環境」に逃げたくなる気持ちは、痛いほど分かる。ここまで、「今の環境で成長できるか」について話してきた。では、環境を変えるにせよ、留まるにせよ、これからのエンジニアは何を磨くべきなのか。AIと共存する時代に何を磨くかこの問いを考えるとき、避けて通れないのがAIの存在だ。AIは、定型的な作業を得意とする。コードの自動生成、バグの検出、ドキュメントの作成。これらの領域では、すでにAIが人間を補助し、場合によっては代替し始めている。つまり、「言われたことをそのまま実装する」だけのエンジニアは、価値が下がっていく。一方で、AIに代替されにくい領域もある。技術的な意思決定を下すこと。チームを率いること。ビジネス課題を理解し、技術で解決策を提案すること。曖昧な要件を整理し、実装可能な形に落とし込むこと。これは、当面の間、人間の仕事だ。私が優れた組織で見てきた共通点がある。エンジニアがビジネスに直接触れていることだ。「ITとビジネスの橋渡し役」を介さず、エンジニア自身がビジネス指標を理解し、顧客と対話する。その直接的な接点が、AIには代替できない価値を生む。逆に言えば、「要件を受け取って実装するだけ」のエンジニアは、AIに代替されやすい。これは他人事ではなく、私自身も常に意識していることだ。だが、ここで短絡的な結論に飛ばないでほしい。「じゃあ、転職してシニアなポジションを取りに行こう」というのは間違いだ。なぜなら、シニアになるためには、ジュニアとしての経験が必要だからだ。問題は、「ジュニアのまま留まり続けること」だ。今の環境で、次のステージに進むための挑戦ができるなら、そうすべきだ。転職は、その挑戦ができない場合の、最後の手段であるべきだ。ここで自分に問いかけてほしい。直近1ヶ月で、「人間が介入しなければ解決しなかった意思決定」を何回行ったか。AIがコードを書ける今、「実装する」だけでは価値が出にくい。曖昧な要件を整理する。ステークホルダー間の調整をする。技術的な選択肢の中から、ビジネスインパクトを考慮して決断する。そういう「人間にしかできない仕事」をどれだけやっているか。それがシニアへの階段を登る経験だ。ここまで、「どの方向に進むか」「何を磨くか」「今の環境で成長できるか」について話してきた。キャリアを考えるとき、避けて通れない話がもう1つある。転職を考える動機として、最も頻繁に挙がるテーマだ。「年収を上げたい」は目的ではなく結果である転職理由として「年収を上げたい」はよく聞く。分かる。私だって年収は高い方がいい。だが、年収は目的ではなく、結果だ。「年収は結果」と言うのは簡単だ。でも、転職サイトを開くと、年収で検索してしまう。なぜか。年収は分かりやすい指標だからだ。「能力が上がった」は測りにくい。「年収が上がった」は明確だ。この分かりやすさの罠が、私たちを「能力より年収」に引き寄せる。対策は1つ。年収以外の「分かりやすい指標」を自分で設定することだ。「○○の技術を導入した」「△△人のチームをリードした」「□□の問題を解決した」——そういう指標を先に決めておけば、年収の誘惑に負けにくい。転職サイトを開く前に、「この転職で得たいもの」を3つ書き出してみてほしい。そのうち「年収」が1番目に来るなら、一度立ち止まる必要がある。年収は、あなたが提供できる価値の対価だ。技術力が高ければ、難しい問題を解ける。推進力があれば、プロジェクトを成功に導ける。影響力があれば、チームや組織を良い方向に動かせる。これらの価値を提供できるから、高い年収が払われる。年収600万円から800万円、800万円から1000万円。それぞれのステージを超えるには、提供できる価値のレベルを上げる必要がある。「一人で開発できる」から「チームをリードできる」へ。「技術的な問題を解ける」から「ビジネス課題を技術で解決できる」へ。企業によって「シニアエンジニア」の意味は違う。大手IT企業とスタートアップでは、同じ肩書きでも求められる水準が全く異なる。1000人規模の会社のシニアと、10人のスタートアップのシニアでは、経験してきた課題の複雑さも、責任の範囲も違う。同じ「シニア」でも、会社によって期待値が違う。ここで正直に振り返りたい。キャリアの進め方について、私は無自覚だった。一生懸命働けば、報酬は自然についてくるものだと思っていた。「会社が見ていてくれる」「評価されるべき人は評価される」——そう信じていた。でも、それは間違いだった。努力だけでは、次のレベルに到達できない。技術を磨くことと、キャリアを戦略的に構築することは、別のスキルなのだ。日本企業では「出る杭は打たれる」と言われるが、「出なさすぎる杭」は存在すら認識されない。逆に言えば、能力を上げずに年収だけ上げようとしても、無理がある。高年収の会社に転職できたとしても、その期待値に応えられなければ、いずれ居場所を失う。私自身、この罠に片足を突っ込んだことがある。ある時期、市場が過熱していた。エンジニアの採用難で、年収相場が跳ね上がっていた。転職サイトを見ると、今の年収より明らかに高いオファーがゴロゴロしている。「自分の市場価値はこんなに高いのか」と浮かれていた。でも、冷静に考えれば分かる話だった。それは「私の価値」ではなく、「市場のバブル」だった。実際に転職した人の話を聞くと、入社後に苦しんでいるケースが少なくなかった。「この年収なら、これくらいできるだろう」という期待に応えられない。前職では周囲のサポートがあったから成果が出せていたのに、新しい環境では1人で同じ成果を求められる。結果、評価が下がり、居心地が悪くなる。中には、年収ダウンで再び転職した人もいた。年収アップの転職で失敗する人には、共通点があった。「年収が上がる＝自分の価値が認められた」と解釈していたことだ。でも、採用側の論理は違う。「この年収を払えば、このくらいの成果が出るはずだ」という投資判断をしている。年収は「認定」ではなく「期待値」なのだ。その期待値に応えられなければ、厳しい現実が待っている。ここで、提示された年収アップのオファーについて冷静に考えてほしい。その年収は、あなたの「現在の実力」に対する評価なのか。それとも、市場のバブルや採用の緊急度による「プレミアム（下駄）」なのか。下駄を履いた状態で入社すると、期待値の調整で苦しむ。「このくらいできるだろう」という期待に応えられず、評価が下がり、居心地が悪くなる。そのリスクをどう管理するか。年収だけを見て決めると、この罠にはまりやすい。だから、「年収を上げるために転職する」のではなく、「能力を上げた結果として年収が上がる」という順序を間違えてはいけない。そして、能力を上げるためには、今の環境で何ができるかをまず考えるべきだ。ここで、転職を考えるときに気をつけてほしいことがある。「年収アップ」という言葉に惹かれて、転職エージェントの話を聞き始める人は多い。だが、エージェントの言葉を聞く前に、知っておくべきことがある。転職エージェントのビジネスモデルを理解する転職エージェントは、あなたの味方ではない。これは悪口ではなく、ビジネスモデルの話だ。転職エージェントにお金を払っているのは、あなたではない。採用企業だ。エージェントは、あなたを企業に紹介し、採用が決まったときに、企業から報酬を受け取る。その報酬は、あなたの年収の一定割合だ。つまり、エージェントにとって、あなたが「転職すること」が利益になる。あなたが「現職に残ること」は、彼らには何のメリットもない。むしろ、売上ゼロだ。だから、エージェントは転職を勧める。「今の会社に残った方がいい」とは、なかなか言ってくれない。彼らの言葉をそのまま鵜呑みにするのは危険だ。エージェントを使うなとは言わない。彼らは市場の情報を持っているし、面接対策のアドバイスもくれる。ただ、彼らのインセンティブ構造を理解した上で、話を聞くべきだ。本当に転職すべきかどうかは、エージェントではなく、あなた自身が決めることだ。できれば、利害関係のない第三者——信頼できる先輩、友人、メンター——に相談してほしい。ここで厳しいことを言う。自分のキャリアの最終責任者になれ。日本企業では、「会社がキャリアパスを用意してくれる」という期待がある。年功序列で昇進できる。上司が適切なアサインメントを考えてくれる。人事部がキャリア相談に乗ってくれる。——しかし、それは幻想だ。あなたのキャリアの最終責任者は、上司やエージェントや人事部ではなく、あなた自身だ。誰かが導いてくれるのを待つのではなく、自分で方向を決めて、自分で動く。その覚悟があるかどうかが、キャリアを作れるかどうかの分かれ目になる。自分でキャリアを管理するために、私が大事にしている習慣が2つある。1つは、時間管理より体力管理だ。同じ1時間でも、元気なときと疲れているときでは、アウトプットが全く違う。燃え尽きそうな状態で長時間働いても、成果は出ない。自分の体力がどこで回復し、どこで消耗するかを把握することが、長く働き続けるための鍵だ。もう1つは、フィードバックを受け入れる力だ。「それは違うと思います」と言われたとき、どう反応するか。防御的にならず、「なるほど、そういう見方もあるのか」と学びに変えられる人が、成長し続けられる。「自分は正しい」と固まった人は、どんなに優秀でも、そこで成長が止まる。ここまで、「辞めるな」「考えろ」と書き続けてきた。読んでいて息苦しくなった人もいるかもしれない。だから、バランスを取っておきたい。転職が正解だったケースも、確かにあるからだ。転職して正解だった人たちここまで「辞めるな」と書いてきたが、一方的になりすぎただろう。転職して正解だった人も、たくさんいる。私の知り合いにも、転職がキャリアの転機になった人がいる。大企業からスタートアップに移って、2年で技術力が飛躍的に伸びた人。逆に、スタートアップから大企業に移って、大規模開発の経験を積んだ人。マネジメント志向だったのに、転職先でスペシャリストとして開花した人もいる。1人の話をしよう。彼は大企業で5年間、安定したキャリアを積んでいた。評価も悪くなかった。でも、「このまま10年後も同じことをしているのか」という問いが、ずっと頭の片隅にあったという。彼が転職を決めたのは、「逃げたい」からではなかった。「自分の手でプロダクトを作りたい」という明確な欲求があった。大企業では、どうしても歯車の一部になる。意思決定に関われるのは、ずっと先の話だ。彼は、その「ずっと先」を待てなかった。転職先は、20人規模のスタートアップだった。最初の3ヶ月は地獄だったと言っていた。前職では当たり前だったインフラが何もない。ドキュメントもない。聞ける人もいない。「俺、何やってるんだろう」と思った夜もあったらしい。でも、半年後に変化が起きた。自分が設計したアーキテクチャが、本番環境で動き始めた。ユーザーからのフィードバックが、直接Slackに届くようになった。「自分の仕事が、誰かの役に立っている」——その実感が、すべてを変えたと言っていた。彼が転職で成功したのは、運が良かったからではない。辞める前に、「次に何を得たいか」が明確だったからだ。「今の環境が嫌だから」ではなく、「次の環境でこのスキルを得たい」「この経験を積みたい」という具体的な理由で動いていた。これは、私が見てきた「転職で成功した人たち」に共通する特徴だ。ここで視点を切り替えてみたい。「今の仕事への期待値は下げ、キャリアにはもっと期待しよう」。今の仕事で完璧を求めすぎない。すべての仕事が理想的であるはずがない。でも、キャリア全体では高い目標を持つ。3年後、5年後にどうなっていたいか。この視点の切り替えが、良い転職をした人たちの特徴だった。そして、もう1つ。彼らは辞める前に、現職でやれることをやり切っていた。「ここでやれることはやった」という実感があった。だからこそ、次の環境で活かせる実績と経験を持って移れた。転職が正解になるかどうかは、転職先の問題ではない。辞める前に何を積み上げたかの問題だ。だから、この記事で伝えたいのは「絶対に辞めるな」ではない。「辞める準備はできているか」を問え、ということだ。ただし、ここで1つ付け加えておきたい。準備とは関係なく、すぐに辞めるべきときがある。そのタイミングを見誤ると、取り返しのつかないことになる。それでも辞めるべきタイミングここまで「辞めるな」と書いてきた。でも、辞めるべきタイミングは確かにある。そして、それは「自分の問題」ではなく、「環境の問題」であることも多い。メンタルや身体が壊れそうなときは、今すぐ辞めろ。これだけは絶対だ。キャリアよりも健康が大事だ。あなた個人に対するリスペクトを感じない会社や現場からは、即刻立ち去るべきだ。そこで無理をする必要はない。一方的に消耗させられる必要もない。我慢して壊れてからでは遅い。組織の構造的問題があるときも、辞めていい。これは重要なポイントだ。個人の努力では変えられない問題が、組織には存在する。いくつか例を挙げる。評価制度が機能していない——成果を出しても正当に評価されない。声が大きい人だけが昇進する。透明性がない。技術的負債が放置されている——経営層が技術投資を理解せず、ひたすら機能追加だけを求める。改善の余地がない。権限と責任が一致しない——責任だけ押し付けられて、決定権がない。何を提案しても却下される。人間関係の構造が壊れている——特定の人物によるハラスメント。派閥争い。コミュニケーションの断絶。会社の方向性に共感できない——ビジョンが見えない。または、見えたビジョンが自分の価値観と合わない。これは、あなたの責任ではない。どんなに努力しても、個人で変えられない問題はある。「もっと頑張れば変えられるはず」と思って消耗し続ける必要はない。構造的問題を個人の努力で乗り越えようとするのは、無理ゲーだ。今の環境で目指す役割に挑戦する機会がどうしても得られないときも、辞め時だ。組織の構造上、テックリードのポジションがない。マネジメントのポジションがない。専門性を深める機会がない。そういう時は、環境を変える必要がある。私が辞め時だと思う明確なサインがある。「学びたい意欲はあるのに、実際には学べていない」状態だ。技術を深めたい、新しいことに挑戦したい——その気持ちはある。でも、日々の仕事は同じことの繰り返し。成長の機会がない。もう1つのサインは、「スキルではなく、対処法を学んでいる」状態だ。技術力が上がっているのではなく、「この上司にはこう報告すればいい」「この会議はこうやり過ごせばいい」という政治的なサバイバルスキルばかりが磨かれている。これは危険信号だ。その環境で得られるものは、もう得尽くした可能性が高い。しかし、一点だけ確認してほしい。本当に機会がないのか、自分が機会を見逃していないか。機会は待っていても来ない。自分で作り出すものだ。作り出そうとしたけど本当に無理だった——そう言えるなら、転職は正しい選択だ。一方で、こういう時は立ち止まってほしい。「なんとなく飽きた」「刺激がない」「成長できない気がする」——こういう漠然とした不満だけで辞めようとしているなら、一度考えてみてほしい。それは本当に環境の問題なのか。自分の姿勢の問題ではないのか。辞める理由が「環境の構造的問題」なら、辞めていい。辞める理由が「自分の漠然とした不満」なら、もう少し掘り下げてみてほしい。その違いを見極めることが大事だ。ここで1つ、厳しい問いを投げかけたい。「この会社では無理だ」という結論に至るまでに、組織のボトルネックに対して具体的な改善提案や行動を何回試みたか。「評価制度がおかしい」と感じたなら、上司やHRに具体的な改善案を提案したか。「技術的負債が放置されている」と感じたなら、解消のためのロードマップを作って経営層に説明したか。試行回数がゼロなら、それは「構造の問題」ではなく「食わず嫌い」かもしれない。失敗してもいいから、一度は試みてほしい。試みた上で無理だったなら、辞める判断は正しい。ここまで、様々な角度から転職について考えてきた。辞めるべきとき、辞めるべきでないとき、その判断基準を見てきた。最後に、これまでの内容を整理して、問いかけの形にまとめておきたい。転職を決断する前に転職を考えているあなたに、最後に問いかけたい。まず、方向性は明確か。テックリードを目指すのか、EMを目指すのか、スペシャリストとして深掘りするのか。次に進みたい方向が言語化できていなければ、転職は単なる「移動」に終わる。技術力、推進力、影響力のうち、今の自分に足りないものは何か。それを伸ばす機会が、本当に今の環境にはないのか。転職すれば自動的に成長できるわけではない。次に、積み上げたものを使い切ったか。転職には必ずリセットコストがかかる。信頼の貯金はゼロに戻る。ドメイン知識も、人間関係も、リセットされる。その代償を払ってでも得たいものは何か。今の会社で、信頼の貯金を活用してできる挑戦はもうないのか。信頼があるからこそ任される大きな仕事を、やり残していないか。現職で主体的に動いて成し遂げた実績を語れるか。「自分がいたからこそ生まれた差分」を説明できるか。そして、冷静に判断できているか。「成長できない」のは本当に環境のせいか。それとも、難しい課題から逃げているだけではないか。転職理由が「年収を上げたい」だけになっていないか。年収は結果であって、目的ではない。転職エージェントのアドバイスを鵜呑みにしていないか。彼らは転職させることでお金をもらっている。利害関係のない第三者——信頼できる先輩、友人、メンター——に相談したか。「一人前の開発者」から次のステージに進めているか。それとも、キャッチアップを繰り返しているだけではないか。すべてに明確な答えを持っている必要はない。だが、1つも考えたことがないなら、まだ転職を決断する段階ではない。おわりにこの記事で言いたかったことは、結局、1つだけだ。短期的にモノを考えるな。目の前の不満。今月の年収。来週の上司との関係。そういうものに振り回されて、衝動的に決断するな。3年後、5年後、10年後の自分がどうなっていたいか。そこから逆算して考えろ。若いエンジニアが短期的に考えてしまうのは、仕方がない。私もそうだった。目の前の不満が世界のすべてに見える。「今すぐ環境を変えたい」という衝動を抑えられない。それは、若さゆえの特権でもある。でも、その特権には代償がある。転職を繰り返すたびに、信頼の貯金はリセットされる。キャリアの複利は止まる。「いろんな経験を積んだ」と言えば聞こえはいいが、どこにも根を張れないまま、年齢だけが積み上がっていく。私は、そういう未来を避けたかった。転職は、逃げにもなるし、飛躍にもなる。同じ「辞める」という行動でも、その意味は正反対になりうる。違いを決めるのは、辞める前に何を考えたか。それだけだ。1つだけ、問いを残しておく。もし今の会社の嫌な部分——人間関係や評価制度——がすべて解消されたとしたら、それでもなお、その新しい会社に行きたいと心から思えるか。YESなら、それは「攻め」の転職だ。NOなら、それは高度に正当化された「逃げ」かもしれない。逃げが悪いとは言わない。ただ、逃げを「攻め」の物語ですり替えていないか、正直な気持ちで自分に問いかけてほしい。深夜2時、ベッドの中で転職サイトを開いたとき。その衝動を否定はしない。ただ、その衝動のまま動くな。翌朝、もう一度考えろ。1週間後、もう一度考えろ。それでもなお、辞めたいと思うなら、そのときは辞めればいい。正直に言えば、「正解」なんてない。辞めても、残っても、どちらが正しかったかは、誰にも分からない。分かるのは、ずっと後になってからだ。そして、その「正しさ」は、最初から存在していたわけではない。選んだ道を、正解にしていく過程があるだけだ。おい、考えろ。短期ではなく、長期で考えろ。そして、選んだら、それを正解にしろ。続編を書きました。syu-m-5151.hatenablog.com参考書籍ＩＴエンジニアの転職学　２万人の選択から見えた、後悔しないキャリア戦略 (ＫＳ科学一般書)作者:赤川朗講談社Amazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社Amazonスタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazonスタッフエンジニアの道 ―優れた技術専門職になるためのガイド作者:Tanya Reillyオーム社AmazonNINE LIES ABOUT WORK　仕事に関する９つの嘘作者:マーカス・バッキンガム,アシュリー・グッドールサンマーク出版Amazon世界標準のフィードバック　部下の「本気」を引き出す外資流マネジメントの教科書作者:安田 雅彦SBクリエイティブAmazonみんなのフィードバック大全作者:三村 真宗光文社Amazonネガティブフィードバック　「言いにくいこと」を相手にきちんと伝える技術作者:難波 猛アスコムAmazonロバート・キーガンの成人発達理論――なぜ私たちは現代社会で「生きづらさ」を抱えているのか作者:ロバート・キーガン,中土井僚,鈴木規夫英治出版Amazon「人の器」の磨き方　リーダーシップ・コーチングと成人発達理論による人間力の変容プロセス作者:加藤洋平,中竹竜二日本能率協会マネジメントセンターAmazon「人の器」を測るとはどういうことか　成人発達理論における実践的測定手法作者:オットー・ラスキー,中土井僚日本能率協会マネジメントセンターAmazon組織も人も変わることができる！　なぜ部下とうまくいかないのか　「自他変革」の発達心理学作者:加藤洋平日本能率協会マネジメントセンターAmazon人が成長するとは、どういうことか作者:鈴木規夫日本能率協会マネジメントセンターAmazonあなたはなぜ雑談が苦手なのか（新潮新書）作者:桜林直子新潮社Amazon世界の一流は「雑談」で何を話しているのか作者:ピョートル・フェリクス・グジバチクロスメディア・パブリッシング（インプレス）Amazon「何を話していいかわからない」がなくなる　雑談のコツ作者:ひきた よしあきアスコムAmazon雑談の一流、二流、三流作者:桐生 稔明日香出版社Amazon雑用は上司の隣でやりなさい――あなたの評価を最大限に高める「コスパ最強」仕事術作者:たこすダイヤモンド社Amazon資本主義が人類最高の発明である：グローバル化と自由市場が私たちを救う理由作者:ヨハン・ノルベリニューズピックスAmazon資本主義は私たちをなぜ幸せにしないのか (ちくま新書)作者:ナンシー・フレイザー,江口泰子筑摩書房Amazon資本主義はなぜ限界なのか　――脱成長の経済学 (ちくま新書)作者:江原慶筑摩書房Amazon資本主義にとって倫理とは何か作者:ジョセフ・ヒース,瀧澤弘和慶應義塾大学出版会Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Hacker NewsのShow HN に自作ツールを投稿する方法 ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/04/141622</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/04/141622</guid>
            <pubDate>Sun, 04 Jan 2026 05:16:22 GMT</pubDate>
            <content:encoded><![CDATA[はじめにHacker News の「Show HN」は、自分が作ったものを開発者コミュニティに紹介できる場だ。しかし、ただ URL を貼れば良いわけではない。明確なルールがあり、それを守らないと投稿が埋もれたり、他のユーザーから通報されて非表示になることもある。この記事では、Show HN のルールを読み解き、効果的な投稿を作成するまでのプロセスを解説する。Show HN とは何かShow HN は Hacker News 内の特別なカテゴリで、自分が作ったものを他の人が試せる形で共有する場所だ。通常の HN 投稿がニュースや記事のシェアであるのに対し、Show HN は「触れるもの」を紹介する。投稿が一定のポイントを獲得すると、トップバーの "show" ページに表示され、より多くの人の目に触れる。ルールを正確に理解するShow HN には明確なルールがある。公式ガイドラインから重要なポイントを抜粋する。news.ycombinator.com投稿できるものユーザーが実際に試せるもの（run on their computers or hold in their hands）ハードウェアの場合は動画や詳細な記事でも可書籍の場合はサンプルチャプターでも可投稿できないものブログ記事サインアップページニュースレターリスト記事その他「読むだけ」のコンテンツこれらは「試せない」ため Show HN の対象外だ。通常の投稿として submit すべき。その他の重要なルール自分が関わったプロジェクトであること議論に参加できる状態であることサインアップやメール登録なしで試せるのが理想準備ができていないなら投稿しない（ready になってから来い）ランディングページや資金調達ページは NG友人に upvote や comment を頼むのは禁止（組織的な票操作とみなされる）マイナーアップデート（Foo 1.3.1 is out）は NG、メジャーオーバーホールなら可投稿フォームの構成Show HN の投稿は3つの要素で構成される。1. Title（タイトル）Show HN: で始める必要がある80文字制限がある（超過するとエラー）プロジェクト名と一言説明を入れる2. URLプロジェクトのリポジトリ、デモサイト、またはドキュメントページユーザーがすぐに試せる URL が理想3. Text（オプション）URL を補足する説明文何を作ったか、なぜ作ったか、どう使うかフィードバックを求めるポイントを明示すると反応が得やすい効果的なタイトルの作り方80文字という制限の中で、以下を伝える必要がある。プロジェクト名 — 何と呼ばれているか何をするものか — 一言で説明差別化ポイント（余裕があれば） — なぜこれが面白いかタイトルのパターンShow HN: [プロジェクト名] – [一言説明]文字数を削るテクニック：- 冠詞（a, an, the）を省略- "for" を "–" に置き換え- 形容詞を削る- 技術用語は略称を使う（もし一般的なら）良いタイトルの例Show HN: Helix – A post-modern text editor written in RustShow HN: Zed – A high-performance code editor from the creators of AtomShow HN: DuckDB – An embeddable SQL OLAP database management system避けるべきタイトルShow HN: My new project that I've been working on for 6 months  ← 情報がないShow HN: Check this out!  ← 何かわからないShow HN: Tool v1.3.2 released  ← マイナーアップデートは NGText（説明文）の書き方Text は任意だが、書いた方が反応は良くなる。以下の構成が効果的：1. 何を作ったか（1-2文）I built a [種類] that [主要機能].2. なぜ作ったか / 何が新しいか（2-3文）既存ツールとの違い、解決した課題、採用した理論やアプローチ。3. 使い方（1-3行）Quick start:  npm install -g mytool  mytool initワンライナーで試せると理想的。4. 主要機能（箇条書き、3-5個）Features:- Feature A- Feature B- Feature C5. フィードバックの呼びかけ（1文）Would love feedback on [具体的なポイント].「フィードバックください」だけでなく、何について聞きたいかを明示すると、具体的なコメントが得やすい。実際の投稿準備プロセスStep 1: ルールの確認まず公式ガイドラインを読む。ルールは時々更新されるため、投稿前に毎回確認するのが安全。news.ycombinator.comStep 2: 素材の整理プロジェクトの URLREADME や説明文主要機能のリストインストール方法Step 3: タイトルの作成80文字制限を意識しながら複数案を作成。文字数カウンターを使って確認する。Step 4: Text の作成上記の構成に沿って簡潔に。長すぎると読まれない。Step 5: 投稿タイミングHN のトラフィックは米国時間の午前中（太平洋時間 6-10 AM）がピーク。日本時間だと夜〜深夜にあたる。AI を活用した投稿準備Show HN の投稿準備は、AI アシスタントとの相性が良い。依頼の例https://news.ycombinator.com/showhn.html のルールに沿って、https://github.com/username/project を Show HN に投稿したい。タイトル、URL、テキストを作成してほしい。AI に依頼する際のポイント：ルールの URL を渡す — AI が最新のルールを参照できるプロジェクトの URL を渡す — README から情報を抽出してもらえる文字数制限を伝える — 80文字制限など、具体的な制約を共有AI が生成した案をそのまま使うのではなく、自分の言葉で調整することで、より自然な投稿になる。投稿後の対応Show HN では投稿者がコメントに返信することが期待されている。質問には丁寧に回答批判的なコメントにも建設的に対応バグ報告には感謝を伝え、対応する姿勢を見せる投稿して放置するのは印象が悪い。数時間はコメントを監視できるタイミングで投稿しよう。まとめShow HN への投稿は、単なる宣伝ではなく、開発者コミュニティとの対話の始まりだと思います。ルールを守る — 「試せるもの」を投稿するタイトルは80文字以内 — プロジェクト名 + 一言説明Text で文脈を与える — 何を、なぜ、どう使うかフィードバックポイントを明示 — 具体的な質問を投げかける投稿後は対話する — コメントに返信する]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ory HydraでOAuth2認可サーバーを構築する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/04/133007</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/04/133007</guid>
            <pubDate>Sun, 04 Jan 2026 04:30:07 GMT</pubDate>
            <content:encoded><![CDATA[はじめに認可サーバーを構築するタスクがアサインされた。技術選定の裁量はある。仕事の合間にRFC 6749や技術書をいくつか読み始めた。datatracker.ietf.org帰宅後の深夜、週末の空き時間。3日目の深夜2時、私は確信した。これは自前で作るべきではない。認可コードフロー、インプリシットフロー、リソースオーナーパスワードクレデンシャル、クライアントクレデンシャル。4つのグラントタイプ。それぞれにセキュリティ要件がある。PKCEも必要だ。OpenID Connectも。IDトークンのクレーム設計。JWKSエンドポイント。セッション管理。トークン失効。リフレッシュトークンのローテーション。仕様を読めば理解できる。実装もできる。でも、これをプロダクション品質で検証し続けるのは、私たちの仕事ではない。3日間RFCを読んで分かったのは、「自前で作ることの非合理性」だった。調べていく中で、OpenAIがOryを採用していることを知った。www.ory.com彼らは認可サーバーの実装に時間を使わないことを選んだ。彼らの本業はAIモデルの開発だ。認証認可は重要だが、「解くべき問題」ではなく「解決済みの問題を使う」領域として扱っている。妥当な判断だと思う。Ory Hydraを採用することにした。www.ory.shgithub.comこの記事では、Hydraのアーキテクチャを解説し、Docker Composeで実際に動かすところまでやる。OAuth2/OIDCの基本概念は知っている前提で進める。OAuth徹底入門 セキュアな認可システムを適用するための原則と実践作者:Justin Riche,Antonio Sanso翔泳社AmazonOAuth 2 in Action (English Edition)作者:Richer, Justin,Sanso, AntonioManningAmazonこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。「認証をしない認可サーバー」という話www.ory.comHydraのドキュメントを読んでいて、ある一文で手が止まった。「Hydraは認証をしません」認可サーバーなのに認証しない。最初は設計の欠落かと思った。Auth0やKeycloakは全部やってくれるのに。だが、ドキュメントを読み進めるうちに意図が見えてきた。これは欠陥ではない。これこそが設計の核心だ。考えてみてください。あなたの会社には、おそらく既にユーザーデータベースがある。10年使ってきた認証システムがある。LDAPで認証している。多要素認証は自前のものを使っている。パスキー対応も進めている。一般的なIdP——Auth0やKeycloak——を導入すると、これらを全部IdP側に合わせなければなりません。データ移行。認証フローの再設計。既存システムとの複雑な連携。Hydraは違うアプローチを取ります。「認証はあなたたちでやってください。終わったら教えてくれれば、あとはこちらでOAuth2/OIDCの面倒なことは全部やります」この瞬間、私の中で何かがカチッとはまりました。既存の認証システムはそのまま。ユーザーDBもいじらない。ただ、OAuth2/OIDCのプロトコル層だけをHydraに任せる。認証と認可の責務が完全に分離される。これが「ヘッドレス」な認可サーバーというコンセプトです。具体的には以下のメリットがあります。既存システムはそのまま使える: ユーザーDB・認証ロジックをいじらなくていい認証方法は完全に自由: パスワード、パスキー、生体認証、なんでもHydraが担保するのはプロトコル準拠: OpenID Connect Certificationを取得済みhttps://openid.net/certification/openid.netアーキテクチャの全体像www.ory.comHydraを使ったシステムは、3つのコンポーネントで構成されます。Hydra Public API（ポート4444）はOAuth2/OIDCの「顔」です。クライアントアプリケーションが/oauth2/authに認可リクエストを投げ、/oauth2/tokenでトークンを受け取る。ここはHydraが全部やってくれます。Login/Consent Provider（ポート3000）が私たちの実装領域です。Hydraからリダイレクトされてきたユーザーに対して、/loginで認証画面を、/consentで同意画面を表示します。「このユーザーは本人か？」「このスコープを許可するか？」という判断を担う。ここに既存の認証ロジックを組み込みます。Hydra Admin API（ポート4445）は裏方です。Login/Consent Providerが認証・同意の結果をHydraに通知するために使います。チャレンジの検証、承認の通知、セッション管理を担当します。外部には公開せず、内部ネットワークからのみアクセスさせます。この構成を理解したとき、肩の荷が下りた気がしました。OAuth2/OIDCの複雑な部分はHydraに任せて、自分たちは「認証」という本質的な部分だけに集中できる。これなら、やれそうだ。チャレンジベースのフローwww.ory.comHydraとProviderの連携には「チャレンジ」という仕組みが使われます。最初は「なんで直接やり取りしないんだろう」と思いました。でも、この設計にはちゃんと理由があります。クライアントがHydraの/oauth2/authにリダイレクトHydraがlogin_challengeを生成し、Login ProviderにリダイレクトLogin Providerはlogin_challengeを検証し、ユーザーを認証認証成功後、Admin APIで承認を通知し、Hydraに戻るHydraがconsent_challengeを生成し、Consent ProviderにリダイレクトConsent Providerはスコープを確認し、Admin APIで承認クライアントに認可コードが返されるチャレンジは一度きりの使い捨てトークンです。傍受されても再利用できない。リプレイ攻撃やセッションハイジャックを構造的に防ぎます。この手のセキュリティ上の細かい配慮——正直、自前実装だと見落としがちだ。PKCEのcode_verifierの長さ制限（43-128文字）。stateパラメータに暗号学的に安全な乱数を使うべきこと。RFCを読んでいたあの3日間で、攻撃ベクトルをどれだけ考慮できていたか。Hydraはこれらをすべて内包しています。OpenID Connect Certificationを取得しているということは、私が見落としていたであろう細部まで検証されているということです。Docker Compose環境の構築www.ory.com理論は十分。実際に動かしてみましょう。OAuth2/OIDCの仕様は複雑です。RFC 6749を読んでも、認可コードフローの全体像が頭に入らなかった。実際にcurlでリクエストを投げ、リダイレクトを追いかけることで、初めて仕様書の抽象的な記述が腑に落ちました。開発環境は4つのサービスで構成されます。HydraのDockerイメージは公式で提供されています。hub.docker.comservices:  postgres:    image: postgres:16-alpine    environment:      POSTGRES_USER: hydra      POSTGRES_PASSWORD: secret      POSTGRES_DB: hydra    volumes:      - postgres_data:/var/lib/postgresql/data    healthcheck:      test: ["CMD-SHELL", "pg_isready -U hydra -d hydra"]      interval: 5s      timeout: 5s      retries: 5  hydra-migrate:    image: oryd/hydra:v2.2    environment:      DSN: postgres://hydra:secret@postgres:5432/hydra?sslmode=disable    command: migrate sql -e --yes    depends_on:      postgres:        condition: service_healthy  hydra:    image: oryd/hydra:v2.2    environment:      DSN: postgres://hydra:secret@postgres:5432/hydra?sslmode=disable      SECRETS_SYSTEM: super-secret-system-secret-at-least-32-chars      URLS_SELF_ISSUER: http://localhost:4444      URLS_CONSENT: http://localhost:3000/consent      URLS_LOGIN: http://localhost:3000/login      URLS_LOGOUT: http://localhost:3000/logout      LOG_LEVEL: debug    command: serve all --dev    ports:      - "4444:4444"      - "4445:4445"    depends_on:      hydra-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4444/health/ready"]      interval: 10s      timeout: 5s      retries: 5  auth-provider:    build: .    environment:      HOST: 0.0.0.0      PORT: 3000      HYDRA_ADMIN_URL: http://hydra:4445      RUST_LOG: ory_hydra_rust=debug,tower_http=debug    ports:      - "3000:3000"    depends_on:      hydra:        condition: service_healthyvolumes:  postgres_data:注意: 上記の設定は開発環境用です。本番環境ではSECRETS_SYSTEMに32文字以上の暗号学的に安全な値を設定し、sslmode=disableはrequireに変更してください。docs.docker.comauth-providerサービスのbuild: .は、Login/Consent ProviderのDockerfileを参照しています。このDockerfileとRust実装は次回の記事で解説します。今回はHydraのアーキテクチャ理解に集中しましょう。サンプルコードは以下のリポジトリで公開しています。https://github.com/nwiizo/workspace_2026/tree/main/samples/ory-hydra-rustgithub.comdepends_onとhealthcheckの組み合わせがポイントです。PostgreSQL → マイグレーション → Hydra → auth-providerという起動順序が保証されます。私は最初これを書かずに「DBがない」エラーで30分悩みました。環境の起動と動作確認docker compose up -d --builddocker compose logs -f auth-providerヘルスチェック用エンドポイントにアクセスしてみます。curl http://localhost:3000/health# {"status":"healthy"}{"status":"healthy"}が返ってきた。たった数十行のdocker-compose.ymlで、OAuth2認可サーバーの基盤が動いている。RFCを読んでいたあの3日間で見えた複雑さが、Hydraの中に隠蔽されている。OAuth2クライアントの登録OAuth2フローをテストするには、まずクライアントを登録します。www.ory.shdocker compose exec hydra hydra create oauth2-client \  --endpoint http://localhost:4445 \  --grant-type authorization_code \  --response-type code \  --scope openid,offline_access,profile,email \  --redirect-uri http://localhost:8080/callback \  --name "Test Client"クライアントIDとシークレットが出力されるので控えておきます。OAuth2フローのテストテストユーザーを作成します。curl -X POST http://localhost:3000/api/auth/register \  -H "Content-Type: application/json" \  -d '{"email": "test@example.com", "password": "password123"}'ブラウザで認可エンドポイントにアクセスします（<CLIENT_ID>は先ほど取得したもの）。http://localhost:4444/oauth2/auth?client_id=<CLIENT_ID>&response_type=code&scope=openid+profile+email&redirect_uri=http://localhost:8080/callback&state=random_stateフローは以下のように進みます。Hydraがログイン画面にリダイレクトメールアドレスとパスワードを入力してログインHydraが同意画面にリダイレクトスコープを確認して同意http://localhost:8080/callback?code=...にリダイレクトリダイレクト先（8080）は存在しなくても構いません。URLから認可コードを取得できれば成功です。おわりにこの記事を書き終えて、時計を見た。深夜1時だ。正直に言うと、書いている途中で何度かRFCのタブを開いてしまった。「この説明で合ってるかな」と不安になって。私はこの記事を書いたからといって、OAuth2/OIDCを完全に理解したわけではない。たぶん来週も、仕様書の細部で「あれ？」となる瞬間がある。でも、少しだけ違うことがある。3日目の深夜2時、RFCのタブを20個開いて、私は判断した。これは自前で作るべきではない、と。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。Hydraのアーキテクチャを理解して、Docker Composeで動かしてみて、その判断が正しかったと確信した。認証と認可は分離できる。複雑なプロトコル層は、検証済みの実装に任せていい。私が書くべきコードは、真ん中の「Login/Consent Provider」だけだ。「認可サーバーを自前で作ってくれ」もしあなたが今、この言葉を受けてRFCを読んでいるなら。3日読めば分かる。作れるかどうかではない。作るべきかどうかだ。RFCを読むことには意味がある。私もあの3日間があったから、Hydraの設計思想が腑に落ちた。でも、プロダクション品質の認可サーバーを一人で検証し続ける必要はない。検証済みの実装がある。明日の朝、目覚ましが鳴る。また仕事が始まる。おい、RFCのタブを閉じろ。Hydraのドキュメントを開け。何度でも思い出せることの方が大事だ。次の記事では、RustでLogin/Consent Providerを実装する。一緒に認証画面を作ろう。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[私の為のNvChadのキーマッピングガイド 2026年版]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/03/002621</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/03/002621</guid>
            <pubDate>Fri, 02 Jan 2026 15:26:21 GMT</pubDate>
            <content:encoded><![CDATA[はじめに一月三日である。私は今、ソファの深淵に身を沈め、己の怠惰と対峙しているところである。年末にやろうと固く心に誓った開発環境の整理は、見事なまでに手つかずのまま新年を迎えてしまった。大掃除もしていない。年賀状も書いていない。結婚もしていないし、友人と過ごす予定もなかった。やらなかったことを指折り数えていると、正月休みの大半が、まるで人生の棚卸しのような様相を呈し、胸中は罪悪感で満たされていくのである。「年末年始は何をしていたのか」と問われれば、私は途方に暮れるほかない。身体は動かしていない。コードは書いた。本を読み、近所を散歩した。であるから、休んだと言えば休んだのであろう。しかしながら、休んだという実感が皆無なのである。なぜか。「あのキーバインド、なんだったか」という問いが、四六時中、頭蓋骨の内側をぐるぐると巡り続けていたからに相違ない。私はNvChadを使っている。かれこれ四年ほど使い続けている。それにもかかわらず、半年ぶりに設定を見直すたびに「これ、なんのキーだったか」と首を傾げてしまうのである。設定ファイルには丁寧にコメントを書いてある。過去の自分が、未来の自分のために残してくれた親切なメモである。しかし、読んでも思い出せない。覚えた数だけ忘れている。どうやら人間の脳というものは容量が有限であり、Vimのキーバインドよりも優先して記憶すべき事柄があるらしいのだ。たとえば、それが何であるかは私にもわからないのだが。毎年、年始になると私は同じことを繰り返している。設定を見直す。新しいプラグインを試す。キーマッピングを整理する。そしてまた忘れる。「今年こそ覚える」という新年の誓いは、結局のところ、翌年の自分に対する壮大な裏切り行為でしかないのである。このガイドは、そんな救いようのない私のための備忘録である。来年の今頃、またしても全てを忘れ去った自分のために書いている。もしかすると、同じように忘れっぽい誰かの役に立つかもしれない。立たないかもしれない。たぶん、立たない。ちなみに、一昨年にも同じようなことを書いている。進歩がない。ただし、構成はだいぶ変わった。ステータスラインを廃止し、ファイルエクスプローラーをoil.nvimに変え、Snacks.nvimを導入した。変わっていないのは、私が相変わらずキーマッピングを忘れ続けているという事実だけである。syu-m-5151.hatenablog.com開発環境全体についてはこちらに記した。興味のある方は参照されたい。syu-m-5151.hatenablog.comさて、前置きが長くなった。よく忘れるキーマッピングをまとめていくこととする。設定ファイルは以下に置いてある。github.com基本的なショートカット表記<C> = Ctrlキー<leader> = スペースキー（デフォルト）<A> = Altキー<S> = Shiftキーよく使う機能とそのキーマッピング基本操作で必須のコマンド<C-s>       - 保存（これだけは絶対覚える。:w なんてやっているとVSCodeを使っている人にバカにされる）;           - コマンドモードに入る（:を押す必要がない）jk または jj - インサートモードを抜ける（Escより断然速い）<Esc>       - 検索ハイライトをクリア<leader>y   - システムクリップボードにヤンク<leader>Y   - 行全体をシステムクリップボードにヤンク<leader>d   - ヤンクせずに削除（レジスタを汚さない）ナビゲーション（移動系）スクロールと検索が画面中央に来るようにカスタマイズしている。迷子にならない。<C-d>  - 半ページ下スクロール（画面中央維持）<C-u>  - 半ページ上スクロール（画面中央維持）n      - 次の検索結果（画面中央維持）N      - 前の検索結果（画面中央維持）検索系（2つのピッカーを使い分け）Snacks Picker（s系）- メインで使うSnacks.nvimは2024年末に登場した新しいユーティリティセット。高速で美しい。<leader><leader> - スマートピッカー（最重要：状況に応じた最適な検索）<leader>sf - ファイル検索<leader>sg - プロジェクト内テキスト検索（grep）<leader>sw - カーソル下の単語を検索<leader>sb - 開いているバッファを検索<leader>sr - 最近開いたファイルを検索<leader>sc - コマンド検索<leader>sh - ヘルプ検索<leader>sk - キーマップ検索（何かわからなくなったらこれ）<leader>sd - 診断情報を検索<leader>ss - LSPシンボル検索<leader>sR - 直前のピッカーを再開github.comTelescope（f系）- 補助的に使う長年使い慣れたTelescope。fzf-nativeで高速化済み。<C-p>       - ファイル検索（VSCodeユーザーも安心）<leader>ff  - ファイル検索<leader>fg  - ライブgrep<leader>fb  - バッファ検索<leader>fh  - ヘルプタグ検索<leader>fr  - 最近のファイル<leader>fc  - Gitコミット検索<leader>fs  - Gitステータス<leader>fd  - 診断情報github.comファイルエクスプローラー（oil.nvim）NvimTreeからoil.nvimに乗り換えた。バッファのようにディレクトリを編集できる革命的なプラグイン。ファイル名を間違えて作成しても、ddで消せる。Vimの操作で世界を編集している気分になれる。気分だけ。-           - 親ディレクトリを開く（最重要：ファイル階層を上る）<leader>e   - ファイルエクスプローラーを開く<CR>        - ファイル/ディレクトリを選択<C-v>       - 垂直分割で開く<C-s>       - 水平分割で開くg.          - 隠しファイルの表示切り替えgithub.com高速移動（flash.nvim）EasyMotion系のモダンな代替。画面内のどこにでも2-3キーで飛べる。s  - Flash（画面内の任意の位置にジャンプ）S  - Flash Treesitter（構文単位でジャンプ）r  - Remote Flash（オペレーターモード用）github.comLSP関連（コードジャンプ・リファレンス）コードリーディングする時に本当に助かる機能たち。gd          - 定義へジャンプ（最も使う）gD          - 宣言へジャンプgi          - 実装へジャンプ（インターフェースから実装を探せる）gr          - 参照を探す（変数やメソッドの使用箇所を探せる）K           - ホバー情報を表示（ドキュメント、型情報）Ctrl-^    直前に編集していたファイルに切り替え<leader>rn  - シンボルをリネーム<leader>ca  - コードアクション（自動修正候補など）<leader>fm  - フォーマット（conformで整形）<leader>cf  - フォーマット（代替キー）<leader>lk  - シグネチャヘルプ<leader>lD  - 型定義へジャンプgithub.comコードピーク（overlook.nvim）定義にジャンプせずに、フローティングウィンドウで確認できる。<leader>pd  - 定義をピーク（フローティングで定義を確認）<leader>pc  - すべてのポップアップを閉じる<leader>pu  - 最後のポップアップを復元<leader>pU  - すべてのポップアップを復元<leader>pf  - フォーカスを切り替え<leader>ps  - 分割で開く<leader>pv  - 垂直分割で開く<leader>po  - 元の場所で開くgithub.com診断・エラー確認（Trouble）診断情報を一覧で見やすく表示してくれる。[d          - 前の診断へ]d          - 次の診断へ<leader>ld  - 行の診断情報をフロートで表示<leader>lq  - 診断をloclistに送る<leader>xx  - 診断パネルをトグル（Trouble）<leader>xX  - 現在のバッファの診断のみ<leader>xs  - シンボル一覧（Trouble）<leader>xl  - LSP定義一覧<leader>xq  - Quickfixリスト<leader>xt  - TODO/FIXME一覧github.com画面分割とウィンドウ移動複数のファイルを同時に見たい時に使う。<C-h>       - 左のウィンドウへ<C-l>       - 右のウィンドウへ<C-j>       - 下のウィンドウへ<C-k>       - 上のウィンドウへ<leader>|   - 垂直分割<leader>-   - 水平分割<leader>w=  - ウィンドウサイズを均等に<leader>wm  - ウィンドウを最大化（他を閉じる）バッファ操作<S-h>       - 前のバッファへ（Shift + h）<S-l>       - 次のバッファへ（Shift + l）<leader>x   - バッファを閉じる<leader>bd  - バッファを削除（Snacks）<leader>bo  - 他のバッファをすべて削除ビジュアルモードの改善J           - 選択した行を下に移動K           - 選択した行を上に移動<leader>p   - ペースト（レジスタを上書きしない）Git操作LazyGitとの統合が最高に便利。ターミナルでgitコマンドを打つ必要がほぼなくなった。git add -pのインタラクティブモードを思い出せなくても、もう困らない。<leader>gg  - LazyGitを開く（これだけで全部できる）<leader>gl  - LazyGit ログを表示<leader>gf  - 現在のファイルのログを表示<leader>gd  - Git Diff（作業ツリー全体）<leader>gD  - 前のコミットとのDiff<leader>gh  - ファイルの履歴<leader>gH  - ブランチの履歴<leader>gs  - ステージされた変更のDiff<leader>gm  - mainブランチとのDiff<leader>gM  - masterブランチとのDiff<leader>gq  - Diffviewを閉じる<leader>gt  - ファイルパネルをトグル<leader>gp  - Hunkをプレビュー<leader>gb  - 行のBlameを表示<leader>gB  - 行Blameのトグル]c          - 次のHunkへ[c          - 前のHunkへ<leader>hr  - Hunkをリセット<leader>hs  - Hunkをステージ<leader>hu  - Hunkのステージを取り消しgithub.comgithub.comgithub.comターミナル操作<leader>tt  - ターミナルをトグル（Snacks）<C-x>       - ターミナルモードを抜けるAI統合（2026年の目玉）GitHub CopilotとClaudeの両方を使える贅沢な環境。CopilotChat（a系）<leader>ao  - チャットを開く<leader>aq  - チャットを閉じる<leader>ar  - チャットをリセット<leader>ae  - コードを説明（ビジュアルモード対応）<leader>af  - コードを修正<leader>at  - テストを生成<leader>ad  - ドキュメントを生成<leader>aR  - コードをレビューgithub.comAvante（Cursor風のAI体験）<leader>aa  - AIに質問<leader>ax  - コードを編集<leader>aS  - 回答をリフレッシュgithub.comClaudeCode（ターミナル統合）<leader>cc  - Claudeをトグル<leader>cf  - Claudeにフォーカス<leader>cr  - 会話を再開<leader>cC  - 会話を継続<leader>cm  - モデルを選択<leader>cb  - 現在のバッファを追加<leader>cs  - 選択範囲をClaudeに送信（ビジュアルモード）github.com補完操作（nvim-cmp）<C-p>       - 前の候補<C-n>       - 次の候補<C-d>       - ドキュメントを下にスクロール<C-f>       - ドキュメントを上にスクロール<C-Space>   - 補完を手動で開始<C-e>       - 補完を閉じる<CR>        - 候補を確定<Tab>       - 次の候補 / スニペット展開<S-Tab>     - 前の候補 / スニペット前へgithub.comトグル系（u系）Snacks.nvimが提供する便利なトグル機能。<leader>us  - スペルチェックのトグル<leader>uw  - ワードラップのトグル<leader>ud  - 診断のトグル<leader>uh  - インレイヒントのトグルその他の便利機能<leader>?   - 現在のバッファのキーマップを表示（which-key）<leader>rr  - カーソル下の単語を置換<leader>cx  - ファイルに実行権限を付与<leader>j   - 次のQuickfix項目へ<leader>k   - 前のQuickfix項目へ<leader>sT  - TODO/FIXME/HACKなどを検索（TodoTelescope）]t          - 次のTODOへ[t          - 前のTODOへgithub.comgithub.comDiffviewコンフリクト解決マージコンフリクトの解決が格段に楽になる。]x          - 次のコンフリクトへ[x          - 前のコンフリクトへ<leader>co  - oursを選択<leader>ct  - theirsを選択<leader>cb  - baseを選択<leader>ca  - 両方を選択dx          - コンフリクトを削除ビジュアル・UI設定2026年版の大きな特徴は、ミニマルなUIへの移行だ。ステータスラインとタブラインを完全に廃止し、編集スペースを最大化している。情報が多すぎて、結局何も見ていなかったことに気づいたからだ。テーマとカラースキームaquariumテーマを採用。落ち着いた色調で長時間の作業でも目が疲れにくい。-- chadrc.luaM.base46 = {  theme = "aquarium",  transparency = false,  hl_override = {    Comment = { italic = true },    ["@comment"] = { italic = true },    CursorLine = { bg = "#2a2a3a" },    CursorLineNr = { fg = "#fab387", bold = true },  },}ステータスライン廃止の理由従来のステータスラインは廃止し、代わりに以下のプラグインで情報を表示している:incline.nvim: ウィンドウ右下にファイル名と診断情報を表示modes.nvim: カーソルラインの色でモードを表示（Insert=水色、Visual=紫、Delete=赤、Copy=黄）noice.nvim: コマンドラインをフローティングで中央に表示-- options.luao.cmdheight = 0    -- コマンドラインを非表示（noice.nvimが担当）o.laststatus = 0   -- ステータスラインを非表示（incline.nvimが担当）o.showmode = false -- モード表示を非表示（modes.nvimが担当）行番号設定相対行番号を有効化。5jや10kのような相対移動が直感的になる。o.number = true         -- 現在行は絶対行番号o.relativenumber = true -- 他の行は相対行番号o.numberwidth = 4       -- 行番号の幅スクロール設定カーソルが画面端に到達する前にスクロールが始まる。常に周囲のコンテキストが見える。o.scrolloff = 8     -- 上下8行を常に表示o.sidescrolloff = 8 -- 左右8列を常に表示インデント設定2スペースインデントを採用。タブは使わない。o.tabstop = 2o.shiftwidth = 2o.expandtab = trueo.smartindent = trueその他のUI設定o.termguicolors = true  -- 24bitカラーo.signcolumn = "yes"    -- サインカラムを常に表示（ガター）o.cursorline = true     -- カーソル行をハイライト（modes.nvimで色が変わる）o.splitright = true     -- 垂直分割は右にo.splitbelow = true     -- 水平分割は下にo.clipboard = "unnamedplus" -- システムクリップボードと連携o.undofile = true       -- 永続的なundo履歴o.swapfile = false      -- スワップファイルを作らない使用プラグイン一覧と説明UI系プラグイン プラグイン                 説明                                                                                                                                                                                    incline.nvim           ウィンドウ右下にファイル名・アイコン・診断情報を表示するミニマルなフローティングステータスライン。init.luaやmod.rsのような一般的なファイル名の時は親ディレクトリ名も表示される。  modes.nvim             Vimのモード（Normal/Insert/Visual/Delete）に応じてカーソルラインと行番号の色を変える。モード表示がなくても今どのモードにいるか一目でわかる。                                            noice.nvim             コマンドライン、メッセージ、通知をモダンなフローティングUIで表示。画面中央にポップアップするコマンドパレット風のUIが特徴。詳細は後述。                                                  nvim-notify            通知をモダンなポップアップで表示。フェードアニメーションで視認性が高い。                                                                                                                vimade                 非アクティブなウィンドウ/バッファを薄暗く表示。どのウィンドウがアクティブかが視覚的にわかる。                                                                                           better-escape.nvim     jkやjjでインサートモードから抜ける。Escキーに手を伸ばす必要がなくなる。                                                                                                             which-key.nvim         キーを押すと次に押せるキーのヒントを表示。<leader>を押して300ms待つとメニューが出る。                                                                                                 indent-blankline.nvim  インデントレベルを縦線で可視化。ネストの深さが一目でわかる。                                                                                                                           noice.nvim の詳細noice.nvimは、Neovimの標準的なコマンドライン（画面下部の:プロンプト）を完全に置き換え、モダンなフローティングUIを提供するプラグイン。従来の「画面下に張り付いたコマンドライン」から「画面中央にポップアップするコマンドパレット」へと体験が一変する。主な機能:コマンドラインのポップアップ化:を押すと画面中央にフローティングウィンドウが出現入力中のコマンドがシンタックスハイライトされるコマンドタイプに応じたアイコン表示検索のポップアップ化/（前方検索）や?（後方検索）もポップアップで表示検索パターンが正規表現としてハイライトされるコマンドタイプ別のアイコン| 入力 | アイコン | 説明 ||------|---------|------|| : | | 通常のVimコマンド || `/` | ` ` | 前方検索 || `?` | ` ` | 後方検索 || `:!` | `$` | シェルコマンド実行 || `:lua` | | Lua実行 || :help | 󰋖 | ヘルプ |メッセージ・通知の統合エラーや警告メッセージをnvim-notify経由で右下にポップアップ長いメッセージは自動的にスプリットウィンドウに表示LSP統合LSPの処理進捗を表示ホバー情報やシグネチャヘルプもモダンなUIで表示-- 設定例（私の設定）views = {  cmdline_popup = {    position = { row = "50%", col = "50%" },  -- 画面中央    size = { width = 60, height = "auto" },    border = { style = "rounded", padding = { 0, 1 } },  },},この設定により、従来のNeovimとは全く異なる、VSCodeやCursor風のモダンな操作感が得られる。github.comgithub.comgithub.comgithub.comgithub.comgithub.comgithub.comgithub.comナビゲーション系プラグイン プラグイン          説明                                                                                                                                                      snacks.nvim     folke氏による多機能ユーティリティセット。LazyGit統合、高速ピッカー、バッファ削除、ターミナル、デバッグ機能などを提供。2024年末に登場し、急速に普及した。  telescope.nvim  定番のファジーファインダー。ファイル、バッファ、grep、Git操作など何でも検索できる。fzf-nativeで高速化済み。                                               oil.nvim        ディレクトリをバッファとして編集できるファイルエクスプローラー。ファイル名の変更や移動がテキスト編集と同じ感覚でできる革命的なプラグイン。                flash.nvim      画面内の任意の位置に2-3キーでジャンプ。EasyMotionの後継。Treesitterと連携して構文単位のジャンプも可能。                                                   overlook.nvim   定義にジャンプせずにフローティングウィンドウでコードをプレビュー。元の位置を見失わずに定義を確認できる。                                                  hbac.nvim       開いているバッファが一定数を超えると、最近使っていないバッファを自動的に閉じる。バッファが溢れかえるのを防ぐ。                                           github.comGit系プラグイン プラグイン         説明                                                                                                                    gitsigns.nvim  変更行の左側にサイン（追加=緑、変更=青、削除=赤）を表示。Hunk単位でのステージ、リセット、プレビュー、Blame表示も可能。  diffview.nvim  Git Diffを視覚的に表示。2画面分割で変更前後を比較できる。コンフリクト解決UIも備え、ours/theirs/baseの選択が簡単。      診断・コード品質系プラグイン プラグイン              説明                                                                                                                          trouble.nvim        診断情報（エラー、警告）をパネルに一覧表示。プロジェクト全体の問題を俯瞰できる。シンボル一覧やQuickfixリストの表示にも対応。  todo-comments.nvim  コード内のTODO、FIXME、HACK、BUG、NOTEなどをハイライト表示し、検索可能にする。放置されたTODOを見つけやすい。       LSP・フォーマッタ系プラグイン プラグイン            説明                                                                                                                nvim-lspconfig    Neovim内蔵LSPクライアントの設定を簡単にする公式プラグイン。各言語のLanguage Serverとの接続を管理。                  mason.nvim        LSPサーバー、DAP（デバッガ）、リンター、フォーマッタを簡単にインストール・管理できる。:MasonコマンドでUIが開く。  conform.nvim      フォーマッタの統合プラグイン。保存時に自動フォーマットを実行。複数フォーマッタの連携も可能。                        nvim-treesitter   Tree-sitterによる高精度なシンタックスハイライトとインデント。正規表現ベースよりも正確な構文解析。                   schemastore.nvim  JSON/YAMLファイル用のスキーマを提供。package.jsonやtsconfig.jsonなどの補完と検証が効く。                       github.comgithub.comgithub.comgithub.comAI統合プラグイン プラグイン            説明                                                                                                                                             copilot.lua       GitHub Copilotの純粋なLua実装。インライン補完を提供するが、私の設定ではcopilot-cmp経由で補完メニューに統合。                                     copilot-cmp       Copilotの補完をnvim-cmpのソースとして使用。補完メニュー内で他のソース（LSP、バッファ等）と一緒にCopilot候補が表示される。                        CopilotChat.nvim  AIとのチャットインターフェース。コードの説明、レビュー、テスト生成、ドキュメント生成などをチャット形式で依頼できる。Claude Sonnetモデルを使用。  avante.nvim       Cursor風のAI編集体験をNeovimで実現。選択範囲に対してAIに編集を依頼し、差分をプレビューしてから適用できる。                                       claudecode.nvim   Claude Code CLIをNeovim内で直接使用。ターミナル統合でClaude Codeの全機能にアクセス可能。                                                        github.comgithub.com補完系プラグイン プラグイン        説明                                                                                      nvim-cmp      Neovimの補完エンジン。高速でカスタマイズ性が高い。複数のソースからの補完を統合して表示。  cmp-nvim-lsp  LSPからの補完をnvim-cmpに提供するソース。                                                 cmp-buffer    現在開いているバッファ内の単語を補完候補として提供。                                      cmp-path      ファイルパスを補完。ディレクトリ構造をたどりながら入力できる。                            cmp-cmdline   コマンドラインモード（:）での補完を提供。                                                 LuaSnip       スニペットエンジン。定型コードを素早く展開。                                              lspkind.nvim  補完メニューにアイコンを表示。種類（関数、変数、クラス等）が視覚的にわかる。             github.comgithub.comgithub.comgithub.comgithub.comgithub.com言語固有プラグイン プラグイン        説明                                                                                                                      rustaceanvim  Rust開発を強化するプラグイン。rust-analyzerとの統合を改善し、Rust固有の機能（expand macro、join lines等）を提供。         crates.nvim   Cargo.toml内のクレート（依存関係）のバージョン情報を表示。最新バージョンへの更新や、利用可能なバージョンの確認が簡単。 github.comgithub.comフォーマッタ・LSP設定保存時に自動フォーマットが走る。conform.nvimを使用。 言語                   フォーマッタ               TypeScript/JavaScript  prettier, deno_fmt         Lua                    stylua                     Rust                   rustfmt                    Go                     gofmt, goimports, gofumpt  Python                 black, isort               Terraform              terraform_fmt              Bash/Shell             shfmt                      YAML/JSON/Markdown     prettier                  Treesitter対応言語シンタックスハイライトとインデントはTreesitterで処理。vim, lua, vimdoc, html, css, markdown, markdown_inline, terraform, hcl, bash, python, rust, go, typescript, javascript, tsx, json, yaml, toml2024年版からの主な変更点追加されたプラグイン・機能Snacks.nvim: folke氏の新しいユーティリティセット。LazyGit統合、高速ピッカー、バッファ管理などoil.nvim: NvimTreeに代わるファイルエクスプローラー。ディレクトリをバッファとして編集flash.nvim: EasyMotion系のモダンな代替。Treesitter対応Trouble.nvim: 診断情報の一覧表示diffview.nvim: Git Diffの可視化とコンフリクト解決overlook.nvim: 定義をフローティングでピークAvante.nvim: Cursor風のAI編集体験ClaudeCode: Claude Code CLIとのNeovim統合noice.nvim: コマンドラインとメッセージのモダン化which-key.nvim: キーバインドのヒント表示incline.nvim: ミニマルなファイル名表示modes.nvim: モードに応じたカーソルライン色変更vimade: 非アクティブウィンドウの薄暗化hbac.nvim: 未使用バッファの自動クローズ変更されたキーマッピングバッファ切り替え: <Tab>/<S-Tab> → <S-h>/<S-l>（より直感的）スクロール: 画面中央維持が追加検索: SnacksとTelescopeの二刀流にUI設計の変更ステータスラインを完全廃止（incline.nvim + modes.nvim で代替）タブラインを廃止（Snacks pickerで代替）コマンドラインをフローティング化（noice.nvim）なぜこれらのキーマッピングを覚える必要があるのか私の経験上、以下の機能は開発効率を大きく向上させてくれる。ファイル検索（Snacks/Telescope）プロジェクト内のファイルを素早く見つけられるコードベースの把握が容易になる<leader><leader>のスマートピッカーが特に便利LSP機能コードの定義や参照を素早く調べられるリファクタリングが楽になるコードの理解が深まるエラー診断が即座にわかるRustを書いていると1箇所書き換えると芋づる式に修正が発生する。コンパイラに叱られ、LSPに導かれ、最終的には正しいコードにたどり着く。自分で考えているのか、ツールに考えさせられているのか、もはやわからないGit統合（LazyGit + Diffview）エディタを離れずにすべてのGit操作ができるコンフリクト解決が視覚的でわかりやすい<leader>ggでLazyGitを開けば、ステージ、コミット、プッシュ、ブランチ操作など全部できるAI統合コードの説明、レビュー、修正をエディタ内で完結CopilotChatでClaude Sonnetが使える時代Avanteでカーソル位置に応じたAI編集高速移動（flash.nvim）画面内のどこにでも2-3キーで移動できるマウスに手を伸ばす必要がなくなるなぜNvChadを選び続けているのか2024年版でも書いたが、NvChadを選んだ理由は開発体制の健全さだった。その判断は2026年になっても変わっていない。毎年のように「今年こそAstroNvimとかに移行する」と思うが、結局設定を移行する時間で正月休みが終わる。NvChad v3.0以降、設定の構造がより洗練された。lua/plugins/ディレクトリに機能ごとにプラグインをまとめる方式は、設定の見通しを良くしてくれる。私の設定では以下のように分割している:ui.lua: 見た目関連（incline, modes, noice, notify）navigation.lua: 移動・検索（snacks, telescope, oil, flash）git.lua: Git統合（gitsigns, diffview）diagnostics.lua: 診断（trouble, todo-comments）lsp.lua: LSPとフォーマッタ（conform, lspconfig, mason, treesitter）ai.lua: AI統合（copilot, copilot-chat, avante, claudecode）completion.lua: 補完（nvim-cmp）lang.lua: 言語固有（rustaceanvim, crates）この構造のおかげで、何か問題があった時にどこを見ればいいかすぐわかる。nvchad.comVimを学ぶために通常のVimを学ぶ時は、「実践Vim 思考のスピードで編集しよう！」がおすすめだ。Vimの基本から応用までを体系的に学べ、実践的な例も豊富に掲載されている。実践Vim　思考のスピードで編集しよう！ (アスキー書籍)作者:Ｄｒｅｗ Ｎｅｉｌ,新丈 径角川アスキー総合研究所Amazonまた、Vim Adventuresというゲームも面白い。ゲーム感覚でVimのキー操作を学べ、楽しみながら基本的なコマンドが身につく。初心者にも優しい学習カーブで、Vimの世界に入るきっかけとして最適だ。vim-adventures.comおわりにこの文章を書き終えて、ふと時計に目をやると、針は深夜一時を回っていた。年末にやるはずだった開発環境の整理を、結局、一月三日の深夜に敢行しているのである。休めていない。そんなことは百も承知である。正直に告白すれば、この文章を書いている最中にも、私は何度か「あれ、このキーは何だったか」と己の設定ファイルを参照せざるを得なかった。自分のための備忘録を執筆しながら、その備忘録を必要としている。なんという滑稽な光景であろうか。笑えない。いや、笑うしかないのかもしれない。来年の今頃、私は間違いなくこの記事を読み返しているであろう。「そうだ、<leader><leader>でスマートピッカーが開くのであった」と膝を打ち、束の間の安堵を覚える。そしてまた忘れる。おそらく、その繰り返しなのである。人間とは、かくも愚かな生き物なのだ。しかしながら、少しだけ異なることもある。毎年毎年、同じことを馬鹿の一つ覚えのように繰り返しているうちに、いつの間にか身体が記憶している操作というものが存在するのだ。gdで定義へ跳躍すること。<C-s>で保存すること。意識せずとも指が勝手に動く。それは、忘却と想起を幾度となく繰り返した果てに、ようやく獲得した境地なのである。エディタの設定に正解などない。完璧なキーマッピングも存在しない。ただ、自分が少しでも快適に作業できる環境を、毎年少しずつ更新していくのみである。それでよいのだ。それ以上を望むのは、人間の分際で天に唾するようなものである。さて、私はソファの深淵から這い上がることにする。正月休みはまだ幾ばくか残されている。しかし、仕事が始まれば、またすぐに「あのキーは何だったか」と途方に暮れる瞬間が訪れるに違いない。その時のために、この記事は存在するのである。来年の自分へ。また忘れたら読み返すがよい。どうせ忘れるのだから。参考リンクnvchad.comgithub.comneovim.io]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[テスト,検証してますか: cargo-mutantsによるミューテーションテスト入門]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/02/083735</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/02/083735</guid>
            <pubDate>Thu, 01 Jan 2026 23:37:35 GMT</pubDate>
            <content:encoded><![CDATA[はじめにテストは全部通っている。コードカバレッジも90%を超えている。なのに、本番環境でバグが見つかった。私が実際に経験したことだ。原因を調べると、テストコードにassert（検証）が書かれていなかった。テストは「コードを実行しただけ」で、結果が正しいかどうかを確認していなかったのだ。正直、恥ずかしかった。テストを書いている気になっていただけで、何も守っていなかった。こういう経験はないだろうか。あるいは、レビューで「このテスト、意味ありますか」と指摘されたことは。この記事では、こうした「見せかけのテスト」を発見するミューテーションテストという手法と、Rust向けのツールcargo-mutantsを紹介します。公式ドキュメントを参照する場合は、以下のリンクからどうぞ。mutants.rsgithub.comこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。ミューテーションテストとはミューテーションテストは、「テストをテストする」手法です。具体的なコードで説明しましょう。例：割引価格を計算する関数以下のような、商品価格から10%割引した金額を返す関数があるとします。/// 価格から10%割引した金額を返すfn apply_discount(price: u32) -> u32 {    price - (price / 10)}この関数に対して、以下のテストを書きました。#[test]fn test_apply_discount() {    let result = apply_discount(1000);    // 1000円の10%引きは900円のはず...    // でも、assertを書き忘れた！}このテストには問題があります。apply_discount(1000)を呼び出していますが、結果が900であることを検証していません。コードカバレッジは100%ですが、このテストは何も守っていないのです。ミュータント（突然変異体）の生成ミューテーションテストでは、コードに「わざとバグを入れた」バージョンを作ります。これをミュータント（突然変異体）と呼びます。apply_discount関数に対して、以下のようなミュータントが生成されます。// ミュータント1: 引き算を足し算に変えるfn apply_discount(price: u32) -> u32 {    price + (price / 10)  // - を + に変更}// ミュータント2: 常に0を返すfn apply_discount(price: u32) -> u32 {    0  // 関数の本体を0に置き換え}// ミュータント3: 入力をそのまま返すfn apply_discount(price: u32) -> u32 {    price  // 割引計算を削除}テストがミュータントを検出できるか各ミュータントに対してテストを実行します。 ミュータント   変更内容     テスト結果               判定           ミュータント1  - → +    ✅ 成功（テストが通る）  ❌ missed  ミュータント2  常に0を返す  ✅ 成功（テストが通る）  ❌ missed  ミュータント3  割引なし     ✅ 成功（テストが通る）  ❌ missed すべてのミュータントがテストを通過してしまいました。これはテストが何も検証していないことの証拠です。テストを修正するテストにassert_eq!を追加して、結果を検証するようにします。#[test]fn test_apply_discount() {    let result = apply_discount(1000);    assert_eq!(result, 900);  // 結果が900であることを検証}修正後、再度ミュータントをテストします。 ミュータント   変更内容     テスト結果             判定           ミュータント1  - → +    ❌ 失敗（1100 ≠ 900）  ✅ caught  ミュータント2  常に0を返す  ❌ 失敗（0 ≠ 900）     ✅ caught  ミュータント3  割引なし     ❌ 失敗（1000 ≠ 900）  ✅ caught すべてのミュータントが検出されました。これで「テストが正しく機能している」ことが確認できました。ミューテーションテストの核心ここまでの例で分かるように、ミューテーションテストは以下の逆説に基づいています。テストの成功が、失敗の証拠になる。コードを壊したのにテストが通るなら、そのテストは壊れたコードを見逃している——つまり、テストとして機能していません。cargo-mutantsとはcargo-mutantsは、Rust向けのミューテーションテストツールです。上記のような「ミュータントの生成」「テストの実行」「結果の集計」を自動で行います。Rustを使っている開発者なら、cargo install cargo-mutants && cargo mutantsの2コマンドで即座に試せます。ソースコードの変更は一切不要です。Rustを使っていない方も、「テストの品質をどう測るか」という観点でお読みいただければ、他の言語にも応用できる考え方が得られるはずです。いつ導入すべきかミューテーションテストは誰でも試せますが、すべてのプロジェクトに必要なわけではありません。正直に言えば、導入コストは低くない。特に有効なのは、カバレッジは80%以上あるのにバグが減らないケースです。金融計算のように正確性が重要なビジネスロジックや、チームにテストの質を意識させたい場面でも効果を発揮します。私自身、冒頭で触れた経験をした後、まずこのツールで「テストが本当に機能しているか」を確認するようになりました。一方、まだカバレッジが50%未満のプロジェクトでは、まずカバレッジを上げる方が効果的です。プロトタイプ段階で変更が激しい場合や、テスト実行時間がすでに長すぎる場合も、ミューテーションテストの優先度は下がります。ツールが問題を解決してくれるわけではない。テストを書くのは人間です。クイックスタートインストール# 推奨: cargoで直接インストールcargo install --locked cargo-mutants# 高速インストール（プリビルドバイナリ使用）cargo binstall cargo-mutants基本的な使い方# ミュータント一覧を確認（テストは実行しない）cargo mutants --list# ミューテーションテストを実行cargo mutants# 詳細出力で実行cargo mutants -v実行例実際にサンプルプロジェクトで実行した結果を示します。$ cargo mutants --list | head -20src/lib.rs:12:5: replace calculate_score -> i32 with 0src/lib.rs:12:5: replace calculate_score -> i32 with 1src/lib.rs:12:5: replace calculate_score -> i32 with -1src/lib.rs:32:5: replace is_valid_email -> bool with truesrc/lib.rs:32:5: replace is_valid_email -> bool with falsesrc/lib.rs:37:5: replace format_greeting -> String with String::new()src/lib.rs:37:5: replace format_greeting -> String with "xyzzy".into()src/lib.rs:42:5: replace find_first_even -> Option<i32> with Nonesrc/lib.rs:42:5: replace find_first_even -> Option<i32> with Some(0)src/lib.rs:47:5: replace parse_positive_number -> Result<u32, String> with Ok(0)src/lib.rs:57:5: replace get_even_numbers -> Vec<i32> with vec![]...実行すると、各ミュータントに対してテストが実行され、結果が表示されます。$ cargo mutants -vFound 108 mutants to testok       Unmutated baseline in 1s build + 1s testcaught   src/lib.rs:12:5: replace calculate_score -> i32 with 0 in 0s build + 0s testcaught   src/lib.rs:12:5: replace calculate_score -> i32 with 1 in 0s build + 0s testMISSED   src/lib.rs:155:9: delete match arm 1 in calculate_discount in 0s build + 1s test...108 mutants tested in 2m: 17 missed, 91 caught出力結果の読み方結果の4分類 結果          意味                                    アクション                  caught    テストがミュータントを検出した          良好。テストが機能している  missed    テストがミュータントを検出できなかった  テストの追加・強化が必要    unviable  ミュータントがコンパイルできなかった    無視してOK                  timeout   テストがタイムアウトした                無限ループの可能性あり     出力ディレクトリ（mutants.out/）実行後に生成されるmutants.out/ディレクトリには、詳細な結果が保存されます。mutants.out/├── caught.txt      # 検出されたミュータント一覧├── missed.txt      # 検出できなかったミュータント一覧├── timeout.txt     # タイムアウトしたミュータント├── unviable.txt    # コンパイル不可だったミュータント├── outcomes.json   # 全結果のJSON形式├── log/            # 各ミュータントの詳細ログ└── diff/           # 適用されたパッチミューテーションテストの仕組みミューテーションテストは1970年代に考案された手法ですが、計算コストの高さから長らく実用的ではありませんでした。近年のコンピュータ性能向上により、ようやく日常的に使えるようになってきました。cargo-mutantsの動作フローcargo-mutantsは以下の手順で動作します。ソースファイルの特定: プロジェクト構成を読み取り、テスト対象のファイルを見つけるコードの解析: synというライブラリ（Rustでは「クレート」と呼びます）を使って、コードの構造を解析するミュータントの生成: 「足し算を引き算に変える」「戻り値を0に変える」といった変更パターンを列挙するテストの実行: 各ミュータントに対してテストを実行し、検出できたかどうかを記録する具体例：検証していないテストコードカバレッジとミューテーションテストの違いを、具体例で見てみましょう。// 2つの数を足し算する関数fn add(a: i32, b: i32) -> i32 {    a + b}// テストコード#[test]fn test_add() {    add(1, 2);  // 関数を呼んでいるだけ！結果を検証していない！}このテストはadd関数を実行しているので、コードカバレッジは100%です。しかし、戻り値が正しいかどうかを確認していません。add(1, 2)の結果が3であることを検証していないのです。正しいテストは以下のようになります。#[test]fn test_add_correct() {    let result = add(1, 2);    assert_eq!(result, 3);  // 結果が3であることを検証している}assert_eq!は「左辺と右辺が等しいことを確認する」という意味です。等しくなければテストは失敗します。cargo-mutantsは、最初の「検証していないテスト」の問題を発見できます。a + bをa - bに変更しても、最初のテストは成功してしまいます（結果を見ていないから）。これにより「このテストは意味がない」ということが明らかになります。戻り値の型別ミューテーションcargo-mutantsは、関数の戻り値の型に応じて異なるミューテーションを生成します。「型」とは何でしょうか。プログラミングにおいて、データには種類があります。「整数」「文字列」「真偽値（はい/いいえ）」などです。Rustはこの種類を厳密に区別する言語で、「この関数は整数を返す」「この関数は文字列を返す」といった宣言が必要です。cargo-mutantsは、この「返す型」に応じて、適切なミュータントを生成します。以下、Rustを知らない方にも理解できるよう、各型の意味と合わせて説明します。bool型（真偽値）bool型とは: true（真）かfalse（偽）のどちらかを表す型です。条件分岐の判定などに使われます。/// メールアドレスが有効かどうかを判定するfn is_valid_email(email: &str) -> bool {    email.contains('@') && email.contains('.')}生成されるミューテーション:replace is_valid_email -> bool with true - 常にtrueを返すreplace is_valid_email -> bool with false - 常にfalseを返すテストで検出すべきこと: 有効なメールと無効なメールの両方をテストして、両方のケースが正しく判定されることを確認する必要があります。i32型（符号付き整数）i32型とは: -2,147,483,648から2,147,483,647までの整数を表す型です。負の数も扱えます。/// スコアを計算する（1=合格、0=普通、-1=不合格）fn calculate_score(correct: u32, total: u32) -> i32 {    let percentage = (correct * 100) / total;    if percentage >= 80 { 1 }    else if percentage >= 50 { 0 }    else { -1 }}生成されるミューテーション:replace calculate_score -> i32 with 0 - 常に0を返すreplace calculate_score -> i32 with 1 - 常に1を返すreplace calculate_score -> i32 with -1 - 常に-1を返すテストで検出すべきこと: 各分岐（合格・普通・不合格）すべてのケースをテストする必要があります。String型（文字列）String型とは: 可変長のテキストデータを表す型です。ユーザー名やメッセージなどに使われます。/// 挨拶文を生成するfn format_greeting(name: &str) -> String {    format!("Hello, {}!", name)}生成されるミューテーション:replace format_greeting -> String with String::new() - 空文字列を返すreplace format_greeting -> String with "xyzzy".into() - 固定文字列「xyzzy」を返す（「xyzzy」はテスト用のダミー文字列としてよく使われる伝統的な文字列です）テストで検出すべきこと: 戻り値の内容を検証することが重要です。単に「何か文字列が返ってくる」だけでなく、期待する内容かどうかを確認します。Option\<T>型（値があるかないか）Option型とは: 値が「ある」か「ない」かを表す型です。Some(値)で値があることを、Noneで値がないことを表します。なぜこの表現を使うのか。多くの言語では「値がない」ことをnullで表しますが、null処理を忘れてエラーになることがよくあります。Rustでは「値がないかもしれない」ことを型で明示し、処理を強制します。これにより、nullに起因するバグを防ぎます。検索結果が見つからない場合などによく使われます。/// 最初の偶数を見つけるfn find_first_even(numbers: &[i32]) -> Option<i32> {    numbers.iter().find(|&&n| n % 2 == 0).copied()}生成されるミューテーション:replace find_first_even -> Option<i32> with None - 常に「見つからない」を返すreplace find_first_even -> Option<i32> with Some(0) - 常に「0が見つかった」を返すreplace find_first_even -> Option<i32> with Some(1) - 常に「1が見つかった」を返すテストで検出すべきこと: 「見つかる場合」と「見つからない場合」の両方をテストし、見つかった場合は正しい値が返されていることを確認します。Result\<T, E>型（成功か失敗か）Result型とは: 処理が「成功」したか「失敗」したかを表す型です。Ok(値)で成功を、Err(エラー)で失敗を表します。ファイル操作やネットワーク通信など、失敗する可能性のある処理に使われます。/// 正の数をパースするfn parse_positive_number(s: &str) -> Result<u32, String> {    let n: i32 = s.parse().map_err(|_| "invalid number".to_string())?;    if n > 0 {        Ok(n as u32)    } else {        Err("number must be positive".to_string())    }}生成されるミューテーション:replace parse_positive_number -> Result<u32, String> with Ok(0) - 常に「成功（0）」を返すreplace parse_positive_number -> Result<u32, String> with Ok(1) - 常に「成功（1）」を返すテストで検出すべきこと: 成功ケースと失敗ケースの両方をテストします。特にエラーハンドリングのテストを忘れがちなので注意が必要です。Vec\<T>型（配列・リスト）Vec型とは: 同じ型の値を複数格納できる可変長の配列です。リストやコレクションを扱う場合に使われます。/// 偶数だけを抽出するfn get_even_numbers(numbers: &[i32]) -> Vec<i32> {    numbers.iter().filter(|&&n| n % 2 == 0).copied().collect()}生成されるミューテーション:replace get_even_numbers -> Vec<i32> with vec![] - 空の配列を返すreplace get_even_numbers -> Vec<i32> with vec![0] - 要素1つの配列を返すreplace get_even_numbers -> Vec<i32> with vec![1] - 要素1つの配列を返すテストで検出すべきこと: 返される配列の要素数と内容の両方を検証します。空配列が返されるケースもテストすることが重要です。演算子のミューテーションcargo-mutantsは、演算子を別の演算子に置き換えるミューテーションも生成します。比較演算子== ↔ !=    等しい ↔ 等しくない<  ↔ >     小さい ↔ 大きい<= ↔ >=    以下 ↔ 以上論理演算子&& ↔ ||    かつ ↔ または算術演算子+ ↔ - ↔ *    足し算 ↔ 引き算 ↔ 掛け算/ ↔ %        割り算 ↔ 余り単項演算子-a → a    符号反転を削除!a → a    論理否定を削除テスト不足の発見例実際にサンプルプロジェクトで検出された「missed」（テストで検出できなかったミュータント）を見てみましょう。MISSED   src/lib.rs:155:9: delete match arm 1 in calculate_discountMISSED   src/lib.rs:156:9: delete match arm 2 in calculate_discountMISSED   src/lib.rs:155:20: replace - with + in calculate_discount...これは以下のコードに対するミューテーションです。fn calculate_discount(price: u32, member_level: u32) -> u32 {    match member_level {        0 => price,                     // 割引なし        1 => price - (price / 10),     // 10% 割引        2 => price - (price / 5),      // 20% 割引        _ => price - (price / 4),      // 25% 割引    }}#[test]fn test_calculate_discount_weak() {    // member_level 0 のみテスト → 他のケースの変異を検出できない！    assert_eq!(calculate_discount(100, 0), 100);}テストがmember_level = 0のケースしかカバーしていないため、他のケース（1, 2, _）のミューテーションは検出できませんでした。これを修正するには、すべてのケースをテストする必要があります。#[test]fn test_calculate_discount_comprehensive() {    assert_eq!(calculate_discount(100, 0), 100);  // 割引なし    assert_eq!(calculate_discount(100, 1), 90);   // 10% 割引    assert_eq!(calculate_discount(100, 2), 80);   // 20% 割引    assert_eq!(calculate_discount(100, 3), 75);   // 25% 割引}設定とカスタマイズコマンドラインオプション# ファイル指定cargo mutants -f src/core.rs -f src/utils.rs# ファイル除外cargo mutants -e src/generated/*.rs# 正規表現でフィルタcargo mutants --re "impl Serialize" --exclude-re "impl Debug"# 並列実行（2-3から開始推奨）cargo mutants -j2# nextestを使用cargo mutants --test-tool=nextest# タイムアウト設定cargo mutants --timeout 300cargo mutants --timeout-multiplier 3設定ファイル（.cargo/mutants.toml）プロジェクト固有の設定を永続化できます。# .cargo/mutants.tomltest_tool = "nextest"jobs = 2timeout_multiplier = 3.0exclude_globs = ["src/generated/*.rs"]exclude_re = ["impl Debug", "impl Display"]additional_cargo_test_args = ["--all-targets"]関数単位の除外（#[mutants::skip]）特定の関数をミューテーション対象から除外できます。// Cargo.tomlに追加: mutants = "0.0.3"#[mutants::skip]  // この関数はミューテーション対象外fn should_stop() -> bool {    true  // falseに変異するとハングする}自動除外される関数以下は自動的にミューテーション対象から除外されます。#[test]属性が付いた関数#[cfg(test)]内のコードnew関数とDefault実装CI/CDパイプラインへの統合GitHub Actions基本設定name: Mutation Testingon: [push, pull_request]jobs:  cargo-mutants:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v4      - uses: taiki-e/install-action@v2        with:          tool: cargo-mutants      - run: cargo mutants -vV --in-place      - uses: actions/upload-artifact@v4        if: always()        with:          name: mutants-out          path: mutants.outプルリクエストでの増分テスト変更されたコードのみをテストし、高速なフィードバックを実現します。- name: Generate diff  run: git diff origin/${{ github.base_ref }}.. | tee git.diff- run: cargo mutants --no-shuffle -vV --in-diff git.diffシャーディングによる分散実行大規模プロジェクトでは、複数のジョブに分割して並列実行できます。strategy:  matrix:    shard: [0, 1, 2, 3, 4, 5, 6, 7]steps:  - run: cargo mutants --shard ${{ matrix.shard }}/8 --baseline=skip --timeout 300パフォーマンス最適化ミューテーションテストは「ミュータント数 × テスト実行時間」のコストがかかります。100個のミュータントがあり、テストに1秒かかるなら、最低でも100秒かかる計算です。実際のプロジェクトでは数百〜数千のミュータントが生成されることもあり、実行時間が課題になります。テストスイートが1分以内のプロジェクトなら、数百ミュータントでも10-20分で完了します。CIで毎回実行するのは現実的でないので、増分テスト（--in-diff）で変更されたコードのみをテストし、フルテストを週次やリリース前に限定するのが実践的です。以下の最適化も効果的です。高速リンカーの使用「リンカー」とは、コンパイルされたコードを実行可能なプログラムにまとめるツールです。プログラムを作る最終段階で使われます。デフォルトのリンカーは汎用的ですが、高速化に特化したリンカーを使うとビルド時間を短縮できます。Moldリンカーで約20%の改善、Wildリンカーでは半分以下の時間になる場合もあります。専用Cargoプロファイル[profile.mutants]inherits = "test"debug = "none"並列実行の設定-j2から開始して、リソース監視しながら調整します。高すぎる値はメモリ枯渇の原因になります。RAMディスクの活用TMPDIR=/ram cargo mutants制限事項副作用のあるコードcargo-mutantsは機械生成された変更でコードをビルド・実行するため、ファイル操作や外部システムへ接続するテストでは予期しない動作を引き起こす可能性があります。フレーキーテスト「フレーキーテスト」とは、同じコードに対して実行するたびに結果が変わる不安定なテストのことです。たとえば、現在時刻に依存するテストや、外部サービスに依存するテストがこれに該当します。ミューテーションテストは「テストが失敗したか」を判定基準にするため、フレーキーテストがあると正確な結果が得られません。まずはcargo testで確実にパスする安定したテストスイートを用意してから実行してください。サポートされていないケース 制限事項            詳細                                       Cargo専用           Bazel等の他ビルドシステムは未対応          条件付きコンパイル  #[cfg(target_os = "linux")]を理解しない  マクロ生成コード    生成されたコードは変異対象外              等価ミュータントミューテーションテストには理論的な限界があります。それが「等価ミュータント」です。たとえば、x * 1をxに変えても動作は同じです。このミュータントは検出不可能ですが、missedとしてカウントされます。また、ログ出力やデバッグ用の関数を変更しても、テストが失敗しないのは正しい動作です。だから、missed率0%は現実的な目標ではない。80-90%の検出率で十分です。残りをコードレビューや手動テストで補完します。検出できないミュータントを#[mutants::skip]で除外すれば、ノイズを減らせます。まとめテストは通っていた。でも、何も守っていなかった。冒頭で触れた私の失敗は「テストが結果を検証していない」ことが原因でした。cargo-mutantsは、こうした「見せかけのテスト」を発見するツールです。あの経験がなければ、この記事を書くこともなかったでしょう。syu-m-5151.hatenablog.comミューテーションテストの価値コードカバレッジは「テストがコードを実行したか」を測りますが、「テストが正しく検証しているか」は測れません。ミューテーションテストは「テストをテストする」手法です。わざとコードを壊して、テストがそれを検出できるかを確認します。cargo-mutantsは、Rustのミューテーションテストを「誰でもすぐに試せる」ものにしたツールです。2コマンドで導入でき、ソースコードの変更は不要です。特に有効なユースケース高いコードカバレッジを達成した後の「テストは本当に機能しているか」確認CI（継続的インテグレーション）でのプルリクエストごとの増分ミューテーションテスト重要なビジネスロジックのテストギャップ発見導入のポイントcargo mutants --listでミュータント数を確認--shard 1/100で試験実行（大規模プロジェクトでは一部だけ先に試す）#[mutants::skip]と設定ファイルで偽陽性を減らすMoldリンカーと専用プロファイルでパフォーマンス最適化他の言語でのミューテーションテストこの記事ではRust用のcargo-mutantsを紹介しましたが、ミューテーションテストの考え方は言語を問わず有効です。他の言語にも同様のツールがあります。JavaScript/TypeScript: StrykerJava: PITestPython: mutmut, cosmic-rayGo: go-mutestingテストの品質を高めたいと考えている方は、ぜひお使いの言語のツールも調べてみてください。テストは通っている。でも、本当に守っているのか。ミューテーションテストは万能ではない。実行時間もかかるし、等価ミュータントの問題もある。それでも、「テストを書いた」という自己満足に気づかせてくれる。私があの日気づいたように。その問いを持ち続けることが、テストを意味のあるものにする第一歩だと思う。単体テストの考え方/使い方作者:Vladimir Khorikovマイナビ出版Amazonソフトウェアテスト徹底指南書 〜開発の高品質と高スピードを両立させる実践アプローチ作者:井芹 洋輝技術評論社Amazon【この1冊でよくわかる】ソフトウェアテストの教科書　［増補改訂 第２版］作者:布施 昌弘,江添 智之,永井 努,三堀 雅也SBクリエイティブAmazonテスト駆動開発作者:ＫｅｎｔＢｅｃｋオーム社AmazonAIとソフトウェアテスト　信頼できるシステムを構築するために作者:Adam Leon Smith,Rex Black,James Harold Davenport,Joanna Olszewska,Jeremias Rößler,Jonathon WrightインプレスAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A2A での認証認可を理解する]]></title>
            <link>https://zenn.dev/satohjohn/articles/6e65b4be3f933a</link>
            <guid isPermaLink="false">https://zenn.dev/satohjohn/articles/6e65b4be3f933a</guid>
            <pubDate>Thu, 01 Jan 2026 17:00:25 GMT</pubDate>
            <content:encoded><![CDATA[概要Agent2Agent Protocol(以下A2A) は現在 Linux Foundation 傘下の AI Agent 同士のコミュニケーションを可能にする Open な Protocol です。https://github.com/a2aproject/A2Aざっくり言えば、AI Agent が外部で公開されていた際に、その AI Agent と自分が作成した AI Agent が協調して動くための仕様、例えば通信方法や要件などを決めたものです。A2A を使うと、マルチエージェントのような仕組みを作ろうとしたときに、様々な言語やフレームワーク、実行基盤で実装されてい...]]></content:encoded>
        </item>
    </channel>
</rss>