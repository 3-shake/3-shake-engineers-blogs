<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sat, 10 Jun 2023 18:30:23 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[Kubernetes 1.27 以降のバッチ処理の改善]]></title>
            <link>https://zenn.dev/toversus/articles/d6065bea460871</link>
            <guid>https://zenn.dev/toversus/articles/d6065bea460871</guid>
            <pubDate>Thu, 08 Jun 2023 03:46:32 GMT</pubDate>
            <content:encoded><![CDATA[Kubernetes 1.27 以降で実装済みまたは予定されているバッチ処理の改善に繋がる KEP や Kubernetes のサブプロジェクトの現状を見ていきます。 KEP-3673: Kubelet limit of Parallel Image Pulls!Kubernetes 1.27 時点でアルファ機能です。1.28 でベータを目指していますが、設定はデフォルトで無効化されています。Pod の起動にノードのスケールアウトが必要な場合に、Pod の起動時間の短縮が期待できます。バッチ処理の Pod が一斉に起動するケースで恩恵を受けられそうです。Kubelet は...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[お前のパケットはもう死んでいる。TCPに死亡フラグを実装してみた]]></title>
            <link>https://zenn.dev/satoken/articles/golang-rfc9401</link>
            <guid>https://zenn.dev/satoken/articles/golang-rfc9401</guid>
            <pubDate>Wed, 07 Jun 2023 00:32:17 GMT</pubDate>
            <content:encoded><![CDATA[はじめにプロトコルの仕様などIETFが発行しているRFCにはジョークRFCというものが存在しています。伝書鳩でIP通信するとか、コーヒーポットを制御するなどが有名です。鳥類キャリアによるIPHyper Text Coffee Pot Control Protocol (HTCPCP/1.0) 日本語訳今年そんなジョークRFCに、TCPに死亡フラグを実装するというRFC9401が追加されました。The Addition of the Death (DTH) Flag to TCP 日本語訳この記事ではこのTCPに死亡フラグを実装するというRFC9401を真面目に実装してみ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenAI API を利用して Terraform から構成図っぽい Mermaid を出力してくれるコマンドを作った話]]></title>
            <link>https://sreake.com/blog/mermaid-with-openai-api/</link>
            <guid>https://sreake.com/blog/mermaid-with-openai-api/</guid>
            <pubDate>Tue, 06 Jun 2023 02:44:12 GMT</pubDate>
            <content:encoded><![CDATA[前段 Sreake事業部の橋本です。 ChatGPTで話題のOpenAIのモデルは、現在画像の取り扱いはまだ発展途上です。文章から画像を作るAPIや画像入力が検討されていますが、システム運用にクリティカルに使えそうになる […]The post OpenAI API を利用して Terraform から構成図っぽい Mermaid を出力してくれるコマンドを作った話 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Redis公式のGoクライアントライブラリrueidisを試してみた]]></title>
            <link>https://qiita.com/bayobayo0324/items/8ac3e27eef360a316ad2</link>
            <guid>https://qiita.com/bayobayo0324/items/8ac3e27eef360a316ad2</guid>
            <pubDate>Wed, 31 May 2023 12:02:25 GMT</pubDate>
            <content:encoded><![CDATA[This 記事 is 何？Twitterぼんやり見てたらRedis公式のGo用クライアントライブラリが出てたとかで、自身のプロジェクトにどの程度簡単に入れられるのかなーと思い試してみました。公式…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform 1.5 で既存リソースからの HCL 生成ができるようになるので試してみる]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/tf-generate-config</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/tf-generate-config</guid>
            <pubDate>Mon, 29 May 2023 09:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Terraform 1.5 のベータ版がリリースされています。https://github.com/hashicorp/terraform/releases/tag/v1.5.0-beta1https://github.com/hashicorp/terraform/releases/tag/v1.5.0-beta2Terraform 1.5 で追加される機能の中には以下のようなものが含まれています。import ブロックterraform plan の -generate-config-out オプションTerraform では手作業などで作成済みの既存リソースも ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OLAPデータベースを支える技術]]></title>
            <link>https://zenn.dev/nnaka2992/articles/technics_behind_analytical_database</link>
            <guid>https://zenn.dev/nnaka2992/articles/technics_behind_analytical_database</guid>
            <pubDate>Thu, 25 May 2023 00:02:49 GMT</pubDate>
            <content:encoded><![CDATA[今年に入ってからCarnegie Mellon UniversityのAdvanced Database SystemsでReading Assignmentとして出ている論文リストで必須とされているものや講義資料を読みました。https://nnaka2992.hatenablog.com/archive/category/論文この記事では紹介されていた論文やAdvanced Database Systemsの講義資料・動画を振り替えることで、BigQueryやRedShift、Snowflakeといった最新の分析用データベースがどのように優れたパフォーマンスを実現しているかを考え...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubernetes の運用効率化を ChatGPT-4 で実現する 障害対応編]]></title>
            <link>https://sreake.com/blog/kubernetes-operation-with-chatgpt4/</link>
            <guid>https://sreake.com/blog/kubernetes-operation-with-chatgpt4/</guid>
            <pubDate>Mon, 22 May 2023 23:46:38 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部インターン生の井上秀一です。 Sreake事業部はSRE関連技術に強みを持つエンジニアによるコンサルテーションサービスを提供する事業部であり、私たちもSRE技術の調査と研究 […]The post Kubernetes の運用効率化を ChatGPT-4 で実現する 障害対応編 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform 1.5 で追加される import ブロックの使い方]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/tf-import-block</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/tf-import-block</guid>
            <pubDate>Mon, 22 May 2023 09:00:00 GMT</pubDate>
            <content:encoded><![CDATA[先日 Terraform v1.5.0-beta1 がリリースされました。https://github.com/hashicorp/terraform/releases/tag/v1.5.0-beta1NEW FEATURES を眺めてみると、どうやら import ブロックというものが追加されているみたいです。今までは既存のリソースを Terraform の管理下に追加するためには terraform import コマンドを使用して 1 つ 1 つ import する必要がありました。import ブロックを使用することでリソースの import を宣言的に実行することができ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform Modules で再利用できるので最高ではないでしょうか？]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/05/19/154346</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/05/19/154346</guid>
            <pubDate>Fri, 19 May 2023 06:43:46 GMT</pubDate>
            <content:encoded><![CDATA[概要ModuleはTerraformの複数のリソースをまとめて再利用可能な単位として扱うことができます。Moduleを使うことで複雑なリソース構成を抽象化し、システムの構造の把握やリソース構成の再利用が可能になり、読みやすさや可読性が向上し、修正箇所が単一になるなどのメリットがあります。ただし、理解には初期コストが必要です。Moduleの設計では、1つの機能を持つように小さくシンプルに保つことが重要で、それが難しい場合は大抵複雑と言えます。また、公式のModuleを利用することで、自身で定義やドキュメントの整備、メンテナンスの手間を省きつつ、プロジェクトを超えて共通認識として扱えるため、Module理解のコストが減ります。しかし、どのタイミングでModuleに組み込むかの正解は、個々のプロジェクトの特性や開発チームの状況により大いに変わるでしょう。絶えず試行錯誤を繰り返しながら個々のプロジェクトごとに最適な解を見つけることが求められます。このブログではそれらの話の前にTerraform Modulesについて利用方法をまとめてみました。概要Module を利用するmoduleの使い方moduleの入力ローカルをうまく利用するmoduleの出力module を使ったときの失敗についてバージョンファイルパスインラインブロック状態の差分は可能な限り小さくすべきアップグレードは自動されるべきいい感じのデフォルトの変数最後に参考Module を利用するシステムを構築するにあたって開発、検証、本番環境をそれぞれ用意することが多いですが、Terraformを環境ごと（例：開発環境、ステージング環境、本番環境）にシンプルなWebサーバーの構成を例にしてModuleを使わないときと使ったときの構成を比較してみましょう。Terraform Configuration|--- Development Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)|--- Staging Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)|--- Production Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)この構成では、 環境毎にVM Instances 、Firewall Rules 、 Load Balancer 、Storage Bucket などのリソースが定義されていて環境間で異なるリソース設定を利用します。一方、moduleを使用した場合の構成は以下のようになります。Terraform Configuration|--- modules|   |--- user_service_cluster|       |--- main.tf|       |   |--- VM Instances (Web servers)|       |   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|       |   |--- Load Balancer (Balance traffic among VM instances)|       |   |--- Storage Bucket (Store static content)|       |--- variables.tf|       |--- output.tf|--- Development Environment|   |--- User-Service-Cluster  Module (source: ../modules/user_service_cluster)|--- Staging Environment|   |--- User-Service-Cluster Module (source: ../modules/user_service_cluster)|--- Production Environment|   |--- User-Service-Cluster  Module (source: ../modules/user_service_cluster)この構成では、 user_service_cluster moduleのmain.tfファイル内にVM Instances 、Firewall Rules 、 Load Balancer 、Storage Bucket などのリソースが定義されています。各環境はこのuser_service_clustermoduleを参照しており、環境間で共通のリソース設定を再利用します。これによって再利用性、可読性が上がり維持管理性を高める事ができると思います。moduleの使い方Terraformの moduleは、リソース設定の再利用可能な部品で、コードの抽象化と組織化をサポートします。 moduleは一つ以上のリソースを定義し、それらをまとめて管理することができます。 moduleを使用するためには、 moduleブロックをmain.tf（またはその他の.tfファイル）に追加し、そこでmoduleのソースと任意の入力変数を指定します。以下に、user_service_cluster moduleを使用するための基本的なmodule ブロックの例を示します。module "user_service_cluster" {  source = "../modules/user_service_cluster"  instance_type  = "n1-standard-1"  instance_count = 3  firewall_rules = {    allow_http  = true    allow_https = true  }  load_balancer_config = {    protocol = "HTTP"    port     = 80  }  bucket_name = "dev-bucket"}source属性にmoduleのソースコードが存在するパスを指定しています。そして、user_service_cluster moduleが定義する各入力変数を設定しています。moduleは、そのソース内でvariableブロックを使用して入力変数を定義します。これらの入力変数は、moduleの使用者が値を提供することでmoduleの振る舞いをカスタマイズできます。また、moduleはoutputブロックを使用して出力値を定義します。出力値は、moduleの内部リソースの属性をmoduleの外部に公開するために使用されます。これにより、他のリソースやmoduleがmoduleから生成されるリソースを参照することが可能になります。module化はTerraformのコードベースを組織化し、再利用可能なコードを作成するための重要な手段です。これにより、一貫性が保たれ、メンテナンスが容易になり、エラーの可能性も低減します。moduleの入力Terraformのmoduleは再利用可能なコードブロックで、入力変数（input variables）を使用してカスタマイズできます。これらの入力変数は、moduleブロックで設定します。以下に、user_service_cluster moduleで使用する入力変数の例を示します。まず、module自体のvariables.tfファイルには以下のように入力変数を定義しますvariable "instance_type" {  description = "The type of instance to start"  type        = string  default     = "n1-standard-1"}variable "instance_count" {  description = "Number of instances to create"  type        = number  default     = 1}variable "firewall_rules" {  description = "Firewall rules for instances"  type        = map(any)  default     = {}}variable "load_balancer_config" {  description = "Configuration for load balancer"  type        = map(any)  default     = {}}variable "bucket_name" {  description = "Name of the storage bucket"  type        = string  default     = "default-bucket"}そして、このmodule を呼び出す際に、具体的な値を設定します：module "user_service_cluster" {  source = "../modules/user_service_cluster"  instance_type  = "n1-standard-1"  instance_count = 3  firewall_rules = {    allow_http  = true    allow_https = true  }  load_balancer_config = {    protocol = "HTTP"    port     = 80  }  bucket_name = "dev-bucket"}上記の例では、user_service_cluster moduleはsourceで指定されたソースからロードされ、instance_type、instance_count、firewall_rules、load_balancer_config、bucket_nameという入力変数を設定しています。module に入力変数を提供することで、module の動作をカスタマイズし、異なる環境や条件で再利用することが可能になります。ローカルをうまく利用するTerraformのlocalsブロックを使用すると、再利用可能な内部変数をmodule内で定義することができます。localsはmodule内で共有され、module外からは参照できません。以下に、user_service_cluster module のlocalsの例を示します。この例では、HTTPポート、任意のポート、任意のプロトコル、TCPプロトコル、そして全てのIPアドレスをローカル変数として定義しています。locals {  http_port    = 80  any_port     = 0  any_protocol = "-1"  tcp_protocol = "tcp"  all_ips      = ["0.0.0.0/0"]}ローカル変数はlocal.<NAME>の形式で参照します。以下のリソース定義では、ロードバランサーリスナーとセキュリティグループの設定にローカル変数を使用しています。resource "google_compute_instance" "http" {  name         = "web-instance"  machine_type = "n1-standard-1"  network_interface {    network = "default"    access_config {      // Assign an ephemeral IP to the instance    }  }    // Other configuration...}resource "google_compute_firewall" "default" {  name    = "default-firewall"  network = "default"  allow {    protocol = local.tcp_protocol    ports    = [local.http_port]  }  source_ranges = local.all_ips}上記の例では、ロードバランサーリスナーとセキュリティグループでlocalsブロックに定義したローカル変数を参照しています。local.http_port、local.tcp_protocol、local.all_ipsを各リソースブロックで参照することで、コードがDRYに保たれ、より読みやすく、メンテナンスがしやすくなります。localsブロックを使用することで、コードの冗長性を減らし、module全体の一貫性を保つことができます。また、ローカル変数を使用することで、moduleの一部で使用する変数をmodule全体で共有することが可能になります。moduleの出力Terraformのmoduleは、出力変数（outputs）を提供できます。出力変数はmoduleの値を外部に公開するための手段で、moduleを使用しているコードからアクセスできます。また、Terraformがapplyコマンドを実行した後にこれらの値を表示することもできます。以下に、user_service_cluster moduleの出力変数の例を示します。この例では、output.tf にクラスタのURLとインスタンスのIDを出力しています。output "cluster_url" {  description = "The URL of the load balancer for the cluster"  value       = "http://${google_compute_global_address.default.address}"}output "instance_ids" {  description = "The IDs of the instances in the cluster"  value       = google_compute_instance.default.*.id}これらの出力をmodule の使用側でアクセスするためには、moduleの名前と出力の名前を組み合わせて参照します。output "user_service_cluster_url" {  description = "The URL of the load balancer for the user service cluster"  value       = module.user_service_cluster.cluster_url}output "user_service_cluster_instance_ids" {  description = "The IDs of the instances in the user service cluster"  value       = module.user_service_cluster.instance_ids}このようにして、moduleの出力変数を使用することで、moduleの内部データをmodule外部に公開し、他のTerraformコードがそのデータを参照できるようにします。出力変数はmodule間の情報共有を可能にし、moduleの再利用性を向上させます。Terraformはファイル名に特別な意味を持たせません。すなわち、variables.tfやoutputs.tfという名前は慣習にすぎないので、入力変数と出力変数を1つのファイルにまとめることも技術的には可能です。module を使ったときの失敗についてmodule を作る時に注意する点について実際にハマったことをベースに3つ紹介します。バージョンModuleのバージョンが異なると意図しない挙動やエラーが引き起こされる可能性があるので、バージョンを固定し実行環境を統一しましょう。Providerやパッケージにしても同じでバージョンを指定して再利用性を高めろ！！！ファイルパスTerraformのtemplatefile関数を使用する際、ファイルパスは絶対パスではなく相対パスを使用する必要があります。しかし、これはどのパスに対して相対的なのでしょうか？デフォルトでは、Terraformはパスを現在の作業ディレクトリに対して相対的に解釈します。そのため、terraform applyを実行しているディレクトリと同じディレクトリにTerraform設定ファイルがある場合、これはうまく動作します。しかし、別のフォルダに定義されたmodule内でtemplatefileを使用する場合、これは問題となります。この問題を解決するためには、path.moduleなどのパス参照を使用します。これを使用すると、module自体に対する相対パスが得られます。インラインブロックTerraformリソースの一部の設定は、インラインブロックか別のリソースとして定義することができます。インラインブロックとは、リソース内で設定する引数のことで、次の形式を持っています。resource "xxx" "yyy" {  <NAME> {    [CONFIG...]  }}ここでNAMEはインラインブロックの名前（例えば、ingress）、CONFIGはそのインラインブロックに特有の一つ以上の引数（例えば、from_portやto_port）です。しかし、インラインブロックと別のリソースを混在して使用すると、Terraformの設計上、設定が衝突し互いに上書きされてエラーが発生します。したがって、どちらか一方を使用する必要があります。moduleを作成する際には、別のリソースを使用することを常に推奨します。これらの注意点を理解しておくことで、Terraformのmoduleをより効果的に利用することができます。状態の差分は可能な限り小さくすべきいつでもアップグレードを状態差分なしで行うことはできません。依存するリソースの変更やセキュリティ問題ができるだけ早くパッチを適用する必要があるなど、破壊的な変更を導入する必要がある場合があります。その場合、コストをどのように減らすかについて考える必要があります。状態の差分が少なければ、アップグレードのコストは少なくなります。破壊的な変更を導入するときは、それを文書化できるCHANGELOGやユーザーガイドを通じてユーザーに伝える必要がありますアップグレードは自動されるべきアップグレードは長期的に開発されるソフトウェアの最も重要なタスクの一つです。ただし、一般的に使用され、広く使用されているTerraform Moduleの場合、これは大きな問題でもあります。また、Moduleを頻繁に更新する場合、自動アップデートの機能を準備する必要があります。ユーザーにアップグレードを依頼しても、通常、彼らはより重要なタスクを行うためにそれを行うことはありません。そのため、代わりに、彼らのためにPRを作成します。PRがTerraformの差分がない場合に自動的にマージされるメカニズムを持っています。これと後方互換性の維持の組み合わせにより、最新バージョンのModuleを使用するユーザーの率を増やすことができますいい感じのデフォルトの変数完全にカスタマイズできるModuleには魅力がないです。Moduleの変数には、80％のユーザーをカバーするスマートデフォルト値を持つべきです。ただし、同時に、通常のユーザーとは異なる方法でModuleを使用するパワーユーザーのための設定も用意するべきです。変数を変更したときに何が起こるかは、ユーザーにとって明白で予測可能でなければなりません。この設定は適切に設計され、安易に浅いインターフェースを持つべきではありません最後にmoduleを活用することで、インフラストラクチャの再利用性と効率性が大幅に向上します。開発者は証明済み、テスト済み、文書化済みのインフラストラクチャの一部を再利用できるようになるため、迅速かつ確実にシステムを構築できます。例えば、マイクロサービスのデプロイメントを定義するmoduleを作成し、各チームが数行のコードで自身のマイクロサービスを管理できるようにすることが可能です。しかし、このようなmoduleを複数のチームで活用するためには、module内のTerraformコードは柔軟性と設定可能性が必要です。異なるチームや状況に応じて、ロードバランサーなしの単一インスタンスやロードバランサー付きの複数インスタンスといった、さまざまなデプロイメント要件を満たすことができます。Terraformの柔軟な構文を活用することで、より多機能なmoduleを設計し、インフラストラクチャの構築を一層楽しく効果的に行うことができます。また、どれぐらいの規模からmodule化するのかなど迷う場面が多いと思いますがこの辺は経験としか言えずにみんな雰囲気でやっているなぁって思いました。このブログが伸びたらもっと実装に基づいた話をしていこうと思います。ちなみにベストプラクティスなんかは俺にはわからない。自分を信じても…信頼に足る仲間を信じても…誰にもわからない…今の構成が一番変更しやすくて誇れるものならそれが正解なんだとおもう。実践Terraform　AWSにおけるシステム設計とベストプラクティス (技術の泉シリーズ（NextPublishing）)作者:野村 友規インプレスR&DAmazon参考Terraform: Up & Running; Writing Infrastructure As CodeDeveloper/Terraform/Configuration Language/Modulesterraform-module/terraform-module-blueprinthttps://registry.terraform.io/namespaces/terraform-aws-moduleshttps://registry.terraform.io/namespaces/terraform-google-modulesHashiCorp LearnModule Creation - Recommended PatternAWSとTerraformで学ぶプロダクションレディなKubernetes 技術の泉シリーズ (技術の泉シリーズ（NextPublishing）)]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GitLab トラブル事例紹介]]></title>
            <link>https://sreake.com/blog/gitlab-trouble-case-study/</link>
            <guid>https://sreake.com/blog/gitlab-trouble-case-study/</guid>
            <pubDate>Tue, 16 May 2023 02:23:30 GMT</pubDate>
            <content:encoded><![CDATA[本ドキュメントでは、トラブルシューティングの事例を取り上げ、それぞれのトラブルの原因調査の流れと判明した原因、解決方法について記載します。 また、トラブルシューティングを円滑に進められるように心がけていることをご紹介しま […]The post GitLab トラブル事例紹介 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Go で GitHub CLI 拡張機能を作る]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/gh-cli-extension-in-go</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/gh-cli-extension-in-go</guid>
            <pubDate>Mon, 15 May 2023 09:00:00 GMT</pubDate>
            <content:encoded><![CDATA[先日 gh-grass という Go 製の GitHub CLI 拡張機能を開発してみたのですが、意外と簡単にできたので手順のメモです。gh-grass については次の記事をご参照ください。https://zenn.dev/kou_pg_0131/articles/gh-grass-introduction 拡張機能を作成する!GitHub CLI がインストールされている必要があります。GitHub CLI のインストール方法については GitHub CLI 公式リポジトリの README をご参照ください。まずは gh extension create を次のように...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[現在のDremelの実装を解説した論文を読みました ]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/05/15/111420</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/05/15/111420</guid>
            <pubDate>Mon, 15 May 2023 02:14:20 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2020年に発表されたBigQueryの元となったGoogle内で利用されている分析向けデータベースであるDremelの実装を解説した論文を読みました。Dremel: A Decade of Interactive SQL Analysis at Web Scale著者についてSergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton,Theo Vassilakisら2010年のDremel発表論文の著者らと、Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, Jeff ShuteらGoogleで分析ワークロードと分散処理に関わる著者らによる論文。概要BigQueryの元となったGoogleのDremelの10年間を振り替えってアーキテクチャについて説明した論文。Dremelは現代のクラウドネイティブ分析ツールで一般的になっている、計算リソースとストレージの分解、カラムナストレージ、in situデータ分析などを統合した最初のツールである。手法SQLの採用Googleでは殆どのデータはBigTableなどNoSQLデータベースで管理されていたため、SQLを用いないデータアクセスが主流であった。しかしトランザクション型ビッグデータシステムにおける、SQLの採用に共ないDremelでもSQLを採用した。ストレージの分離メモリの分離MapReduceのシャッフルのボトルネックを回避するためにDisaggregated Memory Shuffle Systemを採用した。In situデータ分析への対応DBMSへのデータロードを必要としないデータ分析のことで、DremelではGFSに移行するときにGoogle内で共有のストレージフォーマットを使用することでGoogle内のデータに対応した。加えてGoogle Cloud StorageやGoogle Drive、MySQL、BigTableなどからのデータ取得もフェデレーションとして対応した。サーバレスアーキテクチャフォールトトレラントリスタート、仮想スケジューリングユニットによりマルチテナントかつオンデマンドなリソースを提供可能とし、低価格な利用を可能とした。現在ではサーバレスアーキテクチャを進化させ、集中型スケジューリングやShuffle Persistent Layer、柔軟なDAG実行、動的クエリ実行などを実装することでより優れたサーバレスアーキテクチャを実現した。ネストデータにおけるカラムナストレージ[[32])]Figure 5Figure 6Figure 7クエリレイテンシの最小化インタラクティブな実行のレイテンシは大きくなる。それを解決するためにDremelではスタンバイサーバプール、マルチレベル実行ツリー、列指向スキーマ表現、CPUとIO負荷のバランス調整、ファイルオペレーションの再利用、保証されたキャパシティ、適合的なクエリスケーリングにより実現している。作業時間read27:5027:50author32:024:12summary68:5026:48]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Connection draining for Service type LoadBalancer]]></title>
            <link>https://zenn.dev/toversus/articles/1682d275ef1bb7</link>
            <guid>https://zenn.dev/toversus/articles/1682d275ef1bb7</guid>
            <pubDate>Thu, 11 May 2023 09:43:47 GMT</pubDate>
            <content:encoded><![CDATA[はじめにService リソースは Kubernetes のサービス検出を支えるコアリソースです。Service のデータプレーンとして kube-proxy を使用している場合は、各ノード上の iptables や ipvs を設定することで L4 負荷分散を実現しています。Kubernetes は、結果整合性 (Eventual Consistency) の上に成り立つ分散システムです。Kubernetes のコントロールプレーンが Pod を削除する時に、全てのノード上のルーティングルールを更新してから Pod を削除したりはしません。削除中の Pod にもトラフィックが流...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2022 年の Accelerate State of DevOps Report の内容をまとめてみた]]></title>
            <link>https://sreake.com/blog/accelerate-state-of-devops-report-2022/</link>
            <guid>https://sreake.com/blog/accelerate-state-of-devops-report-2022/</guid>
            <pubDate>Thu, 11 May 2023 05:07:56 GMT</pubDate>
            <content:encoded><![CDATA[１．はじめに Sreake 事業部でインターンしている村山です。インターンをするにあたり、DevOps の理解と最近の動向を掴むことが必要であると感じ、Accelerate State of DevOps Report  […]The post 2022 年の Accelerate State of DevOps Report の内容をまとめてみた first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[TiDBで学ぶNewSQLのアーキテクチャ for Beginners]]></title>
            <link>https://zenn.dev/nnaka2992/articles/learning_tidb_internal_for_beginner</link>
            <guid>https://zenn.dev/nnaka2992/articles/learning_tidb_internal_for_beginner</guid>
            <pubDate>Thu, 11 May 2023 01:18:19 GMT</pubDate>
            <content:encoded><![CDATA[はじめにこの記事ではNewSQLの特徴であるノード間の分散とトランザクションや分断耐性などがTiDBではどのような技術によって実現されているかを説明することを目的としています。Spannerの論文が2012年に発表されてから10年以上の年月が流れ、優れた論文や実装ドキュメント、個人による解説ブログなど技術的詳細について述べた資料は多くあります。加えてこの記事を入門的なものと位置づけているため各コンポーネントを網羅的に解説するというよりは、キーコンセプトをどのように実装しているのかを実験を混じえながら動作の実現方法の解説を中心に扱います。また今回はTiDBをベースに説明し...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[GitHub の Secret scanning’s push protection を試してみる]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/gh-secret-scannings-push-protection</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/gh-secret-scannings-push-protection</guid>
            <pubDate>Wed, 10 May 2023 10:00:00 GMT</pubDate>
            <content:encoded><![CDATA[GitHub の Secret scanning's push protection がパブリックリポジトリで無料で使えるようになりました 🎉🎉🎉https://github.blog/changelog/2023-05-09-secret-scannings-push-protection-is-available-on-public-repositories-for-free/Secret scanning's push protection を使うと、例えば AWS のアクセスキーなどといったシークレットをリポジトリへ push するのをブロックすることができます。シークレッ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[はてなブログのコードブロックを”クリップボードにコピーする方法”について]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/05/09/181943</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/05/09/181943</guid>
            <pubDate>Tue, 09 May 2023 09:19:43 GMT</pubDate>
            <content:encoded><![CDATA[はてなブログの設定から、Markdown記法で書いた記事にコードブロックのコピーボタンを自動的に追加することができます。また、こちらのブログは完全に非公式ですし自分のブログ以外では試してません。🦾 デザインの設定まず、はてなブログの管理画面にログインし、デザイン設定を開きます。🦾 CSS の設定を行うデザイン設定で、「カスタマイズ」タブをクリックし、「デザインCSS」を開きます。ここで、先ほど紹介したコピーボタンのスタイルを追加します。pre.code {  position: relative;}.copy-button {  position: absolute;  top: 4px;  right: 4px;  display: inline-block;  padding: 8px 16px;  border: none;  border-radius: 4px;  background-color: #1e90ff;  color: #ffffff;  cursor: pointer;  font-size: 14px;  font-weight: bold;  text-decoration: none;  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);  transition: background-color 0.3s, box-shadow 0.3s;  opacity: 0;  transition: opacity 0.3s;}pre.code:hover .copy-button {  opacity: 1;}.copy-button:hover {  background-color: #2980b9;  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.4);}.copy-button:focus {  outline: none;}.copy-button:active {  box-shadow: none;  transform: translateY(2px);}🦾 フッターHTMLの設定を行う 次に、「カスタマイズ」タブの「フッターHTML」に、以下のコードを追加します。<script>  document.addEventListener('DOMContentLoaded', function() {    // コードブロックを取得    var codeBlocks = document.querySelectorAll('pre.code');        // すべてのコードブロックにコピーボタンを追加    for (var i = 0; i < codeBlocks.length; i++) {      var copyButton = document.createElement('button');      copyButton.className = 'copy-button';      copyButton.textContent = 'Copy code';      copyButton.onclick = function() {        var codeElem = this.parentNode.querySelector('code') || this.parentNode;        var textArea = document.createElement('textarea');        textArea.value = codeElem.textContent.replace(/Copy code$/, ''); // "Copy code" テキストを削除        document.body.appendChild(textArea);        textArea.select();        document.execCommand('copy');        document.body.removeChild(textArea);      }      codeBlocks[i].appendChild(copyButton);    }  });</script>🔍 確認していくhttps://syu-m-5151.hatenablog.com/entry/2023/04/11/084428 にて確認]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[リッチなプログレスバー付きの sleep コマンド「slp」の紹介]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/slp-introduction</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/slp-introduction</guid>
            <pubDate>Mon, 08 May 2023 09:32:10 GMT</pubDate>
            <content:encoded><![CDATA[リッチなプログレスバー付きの sleep コマンドである slp を作りました。https://github.com/koki-develop/slpこういうのこの記事では slp のインストール方法 ~ 使い方についてまとめます。インストール使い方まとめ インストールHomebrew を使用している場合は brew install でインストールできます。$ brew install koki-develop/tap/slpもしくは、 slp は Go で作られているため go install でインストールすることもできます。$ go install g...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[クエリオプティマイザの精度を検証した論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/05/08/111343</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/05/08/111343</guid>
            <pubDate>Mon, 08 May 2023 02:13:43 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2015年に発表されたクエリオプティマイザにおけるカーディナリティ推定とコストモデル、列挙アルゴリズムの貢献度を評価した論文を読んでいきます。How Good Are Query Optimizers, Really?著者についてViktor Leis、Andrey Gubichev、Atanas Mirchev、Peter Boncz、Alfons Kemper、Thomas Neumannらのグループによる論文。ほとんどのメンバーはDBMSにおける最適化について研究しているが、Atanas Mirchevはより統計や探索といった最適化よりの研究をしている。問題意識良い結合順序を見つけることはクエリの性能に対して大きな影響を与えるため、熱心に研究されてきた。古典的なクエリ最適化のアプローチでは以下のステップで動的計画方に基づいた最適化を行なう。1. 有効な結合順序の列挙1. カーディナリティ推定値を入力としたコストモデルの選択理論的にはカーディナリティとコストモデルの推定値が正確であれば、最適なクエリプランを選択することができる。しかし現実にはカーディナリティ推定は一様性や独立性といった単純化された仮定に基づいており、しばしばそのような仮定は間違っているため悲惨な計画を作成する。手法この論文ではカーディナリティ推定器の評価と正確なコストモデルの重要性の評価、そして列挙された結合順序の空間がどの程度影響するのかを以下の方法で検証し、貢献を行なっている。1. IMDBデータを用いたJoin Order BenchmarkというJOINにフォーカスしたベンチマークによる評価を行なう1. 実世界のデータセットにおける現実的なクエリを用いたE2Eの検証を行なう。1. クエリ性能に対するカーディナリティ・コストモデル・列挙アルゴリズムの貢献度を定量化し、最適なクエリプラン生成のためのガイドラインを策定している。作業時間read29:3829:38author33:083:30summary48:4414:36感想時間が無くまとめ途中で切り上げてしまった。やらないよりマシではあるものの、ちゃんと纏めるときにくらべて理解度に影響が出そうなので時間に余裕を持っておきたい。内容自体はGW中にPostgreSQLの実装を読んでいたこともあり、わりと理解しやすかった。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[Kubernetes 1.27] Dynamic Resource Allocation のいま]]></title>
            <link>https://zenn.dev/toversus/articles/fe2aa06f133b49</link>
            <guid>https://zenn.dev/toversus/articles/fe2aa06f133b49</guid>
            <pubDate>Sat, 06 May 2023 02:11:55 GMT</pubDate>
            <content:encoded><![CDATA[!Kubernetes 1.27 時点でアルファ機能のため、実装が大きく変わる可能性があります。 はじめにKubeCon Europe 2023 で KEP-3063 Dynamic Resource Allocation (DRA) についての深い話と DRA Resource Driver の実装方法の話があったので、kubernetes-sigs/dra-example-driver をベースに触りながら検証してみました。toVersus/fake-dra-driver で公開しています。Device Plugins 2.0: How to Build a Drive...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【ArgoCD🐙】ArgoCDのアーキテクチャと自動デプロイの仕組み]]></title>
            <link>https://hiroki-hasegawa.hatenablog.jp/entry/2023/05/02/145115</link>
            <guid>https://hiroki-hasegawa.hatenablog.jp/entry/2023/05/02/145115</guid>
            <pubDate>Tue, 02 May 2023 05:42:57 GMT</pubDate>
            <content:encoded><![CDATA[01. はじめに02. 概要アーキテクチャ▼ レイヤー▼ コンポーネント仕組み【１】【２】【３】【４】【５】【６】【７】【８】【９】【１０】03. repo-serverrepo-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】04. application-controller、redis-serverapplication-controllerとはredis-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】05. dex-serverdex-serverとは仕組み【１】【２】【３】【４】【５】【６】06. argocd-server (argocd-apiserver)argocd-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】【８】【９】【１０】07. アーキテクチャのまとめ08. おわりに謝辞01. はじめにロケットに乗るArgoくんのツラが腹立つわー。さて最近の業務で、全プロダクトが共通基盤として使用するArgoCDとAWS EKS Clusterのアーキテクチャをリプレイスしています。採用した設計プラクティスの紹介も兼ねて、ArgoCDのアーキテクチャと自動デプロイの仕組みを記事で解説しました🚀ArgoCDは、kubectlコマンドによるマニフェストのデプロイを自動化するツールです。現在に至るまでArgoCDのアーキテクチャには変遷があり、今回紹介するのは執筆時点 (2023/05/02) 時点で最新の 2.6 系のアーキテクチャです。アーキテクチャや仕組みはもちろん、個々のマニフェストの実装にもちょっとだけ言及します。それでは、もりもり布教していきます😗02. 概要アーキテクチャ▼ レイヤーまずは、ArgoCDのアーキテクチャのレイヤーがどのようになっているかを見ていきましょう。ArgoCD公式から、コンポーネント図が公開されています。図から、次のようなことがわかります👇下位レイヤー向きにしか依存方向がなく、例えばコアドメインとインフラのレイヤー間で依存性は逆転させていない。レイヤーの種類 (UI、アプリケーション、コアドメイン、インフラ) とそれらの依存方向から、レイヤードアーキテクチャのような構成になっている。特にコアドメインレイヤーが独立したコンポーネントに分割されており、マイクロサービスアーキテクチャを採用している。↪️：argo-cd/components.md at master · argoproj/argo-cd · GitHubArgoCDのマイクロサービスアーキテクチャは、機能単位の分割方法を採用していると推測しています。本記事では詳しく言及しませんが、マイクロサービスアーキテクチャの分割方法には大小いくつかの種類があり、境界付けられたコンテキストで分割することがベタープラクティスと言われています😎(境界付けられたコンテキストについても、ちゃんと記事を投稿したい...)機能単位による分割は、境界付けられたコンテキストのそれよりも粒度が小さくなります。↪️：https://www.amazon.co.jp/dp/4873119316/ArgoCDでは、マイクロサービスアーキテクチャの設計図にコンポーネント図を使用しています。コンポーネント図では、依存方向 (そのコンポーネントがいずれのコンポーネントを使用するのか) に着目できます。そのため、これはマイクロサービス間の依存方向を視覚化するために有効なUML図です🙆🏻‍↪️：Component Diagrams - Code With Engineering Playbook▼ コンポーネント次に、コンポーネントの種類を紹介します。ArgoCDの各コンポーネントが組み合わさり、マニフェストの自動的なデプロイを実現します。ArgoCD (2.6系) のコンポーネントはいくつかあり、主要なコンポーネントの種類とレイヤーは以下の通りです👇 コンポーネント                    レイヤー               機能                                                                                                                                                                                                             argocd-server (argocd-apiserver)  UI / アプリケーション  みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能します。現在、複数のレイヤーの責務を持っており、将来的にUIとアプリケーションは異なるコンポーネントに分割されるかもしれません。  application-controller            コアドメイン           Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。                                                                                            repo-server                       コアドメイン           マニフェスト/チャートリポジトリからクローンを取得します。また、クローンからマニフェストを作成します。                                                                                                        redis-server                      インフラ               application-controllerの処理結果のキャッシュを保管します。                                                                                                                                                       dex-server                        インフラ               SSOを採用する場合に、argocd-serverの代わりに認可リクエストを作成し、IDプロバイダーにこれを送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。                             ↪️：https://www.amazon.co.jp/dp/1617297275仕組みそれでは、ArgoCDは、どのようにコンポーネントを組み合わせて、マニフェストをデプロイするのでしょうか？ここではデプロイ先Cluster管理者 (デプロイ先Clusterを管理するエンジニア) は、ArgoCDのダッシュボードを介してマニフェストをデプロイするとしましょう。まずは、概要を説明していきます。【１】ArgoCDのCluster上で、repo-serverがマニフェスト/チャートリポジトリのクローンを取得します。【２】application-controllerは、repo-serverからマニフェストを取得します。【３】application-controllerは、デプロイ先Clusterの現状を確認します。【４】application-controllerは、処理結果をredis-serverに保管します。【５】argocd-serverは、redis-serverからキャッシュを取得します。【６】デプロイ先Cluster管理者は、argocd-serverにログインしようとします。【７】argocd-serverは、ログイン時にIDプロバイダーに認可フェーズを委譲するために、dex-serverをコールします。【８】dex-serverは、IDプロバイダーに認可リクエストを作成し、これをIDプロバイダーに送信します。【９】argocd-serverで認可フェーズを実施します。ログインが完了し、デプロイ先Cluster管理者は認可スコープに応じてダッシュボードを操作できます。【１０】application-controllerは、Clusterにマニフェストをデプロイします。マニフェストのデプロイの仕組みをざっくり紹介しました。ただこれだと全く面白くないので、各コンポーネントの具体的な処理と、各々がどのように通信しているのかを説明します✌️03. repo-serverrepo-serverとはまずは、コアドメインレイヤーにあるrepo-serverです。マニフェスト/チャートリポジトリ (例：GiHub、GitHub Pages、Artifact Hub、AWS ECR、Artifact Registry、など) からクローンを取得します。repo-serverを持つPodには、他に軽量コンテナイメージからなるInitContainerとサイドカー (cmp-server) がおり、それぞれ機能が切り分けられています👍仕組み【１】repo-serverの起動時に、InitContainerでお好きなマニフェスト管理ツール (Helm、Kustomize、など) やプラグイン (helm-secrets、KSOPS、SOPS、argocd-vault-plugin、など) をインストールします。また、サイドカーのcmp-serverでは起動時に/var/run/argocd/argocd-cmp-serverコマンドを実行する必要があり、InitContainer (ここではcopyutilコンテナ) を使用して、ArgoCDのコンテナイメージからargocd-cliのバイナリファイルをコピーします。repo-serverのざっくりした実装例は以下の通りです👇ここでは、ArgoCDで使いたいツール (Helm、SOPS、helm-secrets) をInitContainerでインストールしています。apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: argocdspec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  initContainers:    # HelmをインストールするInitContainer    - name: helm-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # SOPSをインストールするInitContainer    - name: sops-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # helm-secretsをインストールするInitContainer    - name: helm-secrets-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /helm-working-dir/plugins          name: helm-working-dir    ...    # cmp-serverにargocd-cliのバイナリをコピーするInitContainer    - name: copyutil      image: quay.io/argoproj/argocd:latest      command:        - cp        - -n        - /usr/local/bin/argocd        - /var/run/argocd/argocd-cmp-server      volumeMounts:        - name: var-files          mountPath: /var/run/argocd  # Podの共有ボリューム  volumes:    - name: custom-tools      emptyDir: {}    - name: var-files      emptyDir: {}↪️：Custom Tooling - Argo CD - Declarative GitOps CD for KubernetesArgoCDのコンテナイメージ (quay.io/argoproj/argocd) には、いくつかのツール (例：Helm、Kustomize、Ks、Jsonnet、など) の推奨バージョンがあらかじめインストールされています。そのため、サイドカーのcmp-serverを採用しない場合は、これらのツールをインストールする必要はありません。反対に、cmp-serverを採用する場合はインストールが必要になります🙇🏻 公式が、推奨バージョンを含むサイドカー用イメージを公開してくれるのを待ちたいところなのですが、リポジトリからバージョンを取得することもできなくはないです🙆🏻‍# ArgoCDのコンテナイメージに内蔵されたHelmの推奨バージョンを取得する$ curl -s https://raw.githubusercontent.com/argoproj/argo-cd/<バージョンタグ>/hack/tool-versions.sh \    | grep helm3_version | sed -e 's/^[^=]*=//'↪️：argo-cd/tool-versions.sh at master · argoproj/argo-cd · GitHubKustomizeによるマニフェスト作成は、サイドカーではなくrepo-serverで実行した方が良いかもしれません (Helmはサイドカーで問題ないです)。執筆時点 (2023/05/02) では、ArgoCDとKustomizeが密に結合しています。例えば、ArgoCD上のKustomize系オプションはrepo-serverでマニフェストを作成することを想定して設計されています。無理やりサイドカーでKustomizeを実行しようとすると、ArgoCDの既存のオプションを無視した実装になってしまうため、Kustomizeだけはrepo-serverで実行することをお勧めします😢【２】repo-serverは、Secret (argocd-repo-creds) からリポジトリの認証情報を取得します。argocd-repo-credsではリポジトリの認証情報のテンプレートを管理しています。指定した文字列から始まる (最長一致) URLを持つリポジトリに接続する場合に、それらの接続で認証情報を一括して適用できます。argocd-repo-credsのざっくりした実装例は以下の通りです👇ここでは、リポジトリのSSH公開鍵認証を採用し、argocd-repo-credsに共通の秘密鍵を設定しています。apiVersion: v1kind: Secretmetadata:  name: argocd-repo-creds-github  namespace: argocd  labels:    argocd.argoproj.io/secret-type: repo-credstype: Opaquedata:  type: git  url: https://github.com/hiroki-hasegawa  # 秘密鍵  sshPrivateKey: |    MIIC2 ...あとは、各リポジトリのSecret (argocd-repo) にURLを設定しておきます。すると、先ほどのargocd-repo-credsのURLに最長一致するURLを持つSecretには、一括して秘密鍵が適用されます。# foo-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: argocd  name: foo-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: bar-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/bar-chart.git---# baz-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: foo  name: baz-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: baz-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/baz-chart.git↪️：Declarative Setup - Argo CD - Declarative GitOps CD for Kubernetes【３】repo-serverは、認証情報を使用して、リポジトリにgit cloneコマンドを実行します。取得したクローンを、/tmp/_argocd-repoディレクトリ配下にUUIDの名前で保管します。また、リポジトリの変更をポーリングし、変更を検知した場合はgit fetchコマンドを実行します。# クローンが保管されていることを確認できる$ kubectl -it exec argocd-repo-server \    -c repo-server \    -n foo \    -- bash -c "ls /tmp/_argocd-repo/<URLに基づくUUID>"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yaml↪️：custom repo-server - where is the local cache kept? · argoproj/argo-cd · Discussion #9889 · GitHub2.3以前では、repo-serverは/tmpディレクトリ配下にURLに基づく名前でクローンを保管します。$ kubectl -it exec argocd-repo-server \    -c repo-server \    -n foo \    -- bash -c "ls /tmp/https___github.com_hiroki-hasegawa_foo-repository"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yaml【４】repo-serverは、Volume上のUnixドメインソケットを介して、サイドカー (cmp-server) によるプラグインの実行をコールします。Unixドメインソケットのエンドポイントの実体は.sockファイルです。$ kubectl exec -it argocd-repo-server -c foo-plugin-cmp-server\    -- bash -c "ls /home/argocd/cmp-server/plugins/"foo-plugin.sockUnixソケットドメインは、同じOS上のファイルシステムを介して、データを直接的に送受信する仕組みです。Unixソケットドメインを使用すると、同じVolumeがマウントされたコンテナのプロセス間で、データを送受信できます👍↪️：ASCII.jp：Unixドメインソケット (1/2)【５】cmp-serverは、暗号化キー (例：AWS KMS、Google CKM、など) を使用してSecretストア (AWS SecretManager、Google SecretManager、SOPS、Vault、など) の暗号化変数を復号化します。クラウドプロバイダーがHTTPSプロトコルの使用を求める場合があります。cmp-serverに軽量なコンテナイメージを使用していると、/etc/sslディレクトリ (OSによる)　にSSL証明書が無く、cmp-serverがHTTPSプロトコルを使用できない可能性があります。その場合は、お好きな方法で証明書をインストールし、コンテナにマウントするようにしてください👍apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: foospec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  ...    # サイドカーのcmp-server    - name: helm-secrets-cmp-server      image: ubuntu:latest      ...      volumeMounts:        # サイドカーがAWS KMSを使用する時にHTTPSリクエストを送信する必要があるため、SSL証明書をマウントする        - name: certificate          mountPath: /etc/ssl  ...  initContainers:    - name: certificate-installer      image: ubuntu:latest      command:        - /bin/sh        - -c      args:        - |          apt-get update -y          # ルート証明書をインストールする          apt-get install -y ca-certificates          # 証明書を更新する          update-ca-certificates      volumeMounts:        - mountPath: /etc/ssl          name: certificate  volumes:    - name: certificate      emptyDir: {}【６】cmp-serverがマニフェスト管理ツール (例：Helm、Kustomize) でマニフェストを作成する時に、これのプラグイン (helm-secrets、argocd-vault-plugin、など) を使用したいようなユースケースがあるかと思います。マニフェストの作成時の追加処理として、ConfigMap配下のConfigManagementPluginでプラグインの処理を定義します。ざっくりした実装例は以下の通りです👇ここでは、プラグインとしてhelm-secretsを採用し、helm secrets templateコマンドの実行を定義します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmp-cm  namespace: foodata:  helm-secrets-plugin.yaml: |    apiVersion: argoproj.io/v1alpha1    kind: ConfigManagementPlugin    metadata:      namespace: foo      name: helm-secrets    spec:      generate:        command:          - /bin/bash          - -c        args:          - |            set -o pipefail            helm secrets template -f $ARGOCD_ENV_SECRETS -f $ARGOCD_ENV_VALUES -n $ARGOCD_APP_NAMESPACE $ARGOCD_APP_NAME .  foo-plugin.yaml: |    ...複数のConfigManagementPluginのマニフェストを定義できるように、各ConfigManagementPluginで異なるファイル名とし、ConfigMapで管理すると良いです👍【７】cmp-serverは、を実行し、Secretを含むマニフェストを作成します。ConfigMap配下のファイルをplugin.yamlの名前でサイドカーにマウントする必要があります。また、先ほどのUnixドメインソケットの.sockファイルや、 cmp-serverがプラグインを実行するための各種バイナリファイルもマウントが必要です。ざっくりした実装例は以下の通りです👇ここでは、helm-secretsプラグインを実行するサイドカー (helm-secrets-cmp-server) を作成します。apiVersion: v1kind: Podmetadata:  name: argocd-repo-serverspec:  containers:    # repo-server    - name: repo-server      image: quay.io/argoproj/argocd:latest    ...    # helm-secretsのcmp-server    - name: helm-secrets-cmp-server      # コンテナイメージは軽量にする      image: ubuntu:latest      command:        - /var/run/argocd/argocd-cmp-server      env:        # helmプラグインの場所を設定する        - name: HELM_PLUGINS          value: /helm-working-dir/plugins      securityContext:        runAsNonRoot: true        runAsUser: 999      volumeMounts:        # リポジトリのクローンをコンテナにマウントする        - name: tmp          mountPath: /tmp        # ConfigManagementPluginのマニフェスト (helm-secrets.yaml) を "plugin.yaml" の名前でコンテナにマウントする        - name: argocd-cmp-cm          mountPath: /home/argocd/cmp-server/config/plugin.yaml          subPath: helm-secrets.yaml        # コンテナ間で通信するためのUnixドメインソケットファイルをコンテナにマウントする        - name: plugins          mountPath: /home/argocd/cmp-server/plugins        # 任意のツールのバイナリファイルをコンテナにマウントする        - name: custom-tools          mountPath: /usr/local/bin        # helmプラグインのバイナリをコンテナにマウントする        - name: helm-working-dir          mountPath: /helm-working-dir/plugins      ...  # Podの共有ボリューム  volumes:    # リポジトリのクローンを含む    - name: tmp      emptyDir: {}    # Helmなどの任意のツールを含む    - name: custom-tools      emptyDir: {}    # helmプラグインを含む    - name: helm-working-dir      emptyDir: {}ArgoCDのv2.6では、ConfigManagementPluginのマニフェストを/home/argocd/cmp-server/configディレクトリに、plugin.yamlの名前でマウントしないといけません。これは、cmp-serverの起動コマンド (/var/run/argocd/argocd-cmp-server) がplugin.yamlの名前しか扱えないためです。今後のアップグレードで改善される可能性がありますが、v2.6では、ConfigManagementPluginの数だけcmp-serverが必要になってしまいます🙇🏻‍今回は詳しく言及しませんが、クラウドプロバイダーのSecretストア (例：AWS SecretManager、Google SecretManager、など) の変数を使用する場合は、Secretのデータ注入ツールのプラグイン (特にargocd-vault-plugin) は必須ではありません。この場合、代わりにSecretsストアCSIドライバーやExternalSecretsOperatorを使用できます。これらは、クラウドプロバイダーから変数を取得し、これをSecretにデータとして注入してくれます🙇🏻‍↪️：How to manage Kubernetes secrets with GitOps? | Akuity04. application-controller、redis-serverapplication-controllerとはコアドメインレイヤーにあるapplication-controllerです。Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。redis-serverとはインフラレイヤーにあるredis-serverです。application-controllerの処理結果のキャッシュを保管します。仕組み【１】ArgoCD用Clusterの管理者は、ClusterにArgoCD系のカスタムリソース (例：Application、AppProject、など)　をデプロイします。マニフェストを作成できます。️↪️：GitHub - argoproj/argo-helm: ArgoProj Helm ChartsただしHelmの重要な仕様として、チャートの更新時に使用するhelm upgradeコマンドは、CRDを作成できる一方でこれを変更できません。HelmでCRDを作成するとHelmの管理ラベルが挿入されてしまうため、作成の時点からCRDがHelmの管理外となるように、kubectlコマンドでCRDを作成した方がよいです👍$ kubectl diff -k "https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>"$ kubectl apply -k "https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>"ArgoCD上でHelmを使用してデプロイする場合はこの仕様を気にしなくて良いのかな、と思った方がいるかもしれないです。ですが本記事で解説した通り、ArgoCDはcmp-serverのhelm templateコマンド (この時、--include-crdsオプションが有効になっている) や、application-controllerのkubectl applyコマンドを組み合わせてマニフェストをデプロイしているため、CRDもちゃんと更新してくれます👍🏻️↪️：Helm | Custom Resource Definitions【２】kube-controller-managerは、application-controllerを操作し、Reconciliationを実施します。application-controllerは、Etcd上に永続化されたマニフェストと同じ状態のArgoCD系カスタムリソースを作成/変更します。How Operators work in Kubernetes | Red Hat Developer【３】application-controllerは、repo-serverからリポジトリのマニフェストを取得します。取得したマニフェストは、repo-serverのサイドカーであるcmp-serverが作成したものです。【４】application-controllerは、デプロイ先Clusterをヘルスチェックします。application-controllerには、gitops-engineパッケージが内蔵されており、これはヘルスチェックからデプロイまでの基本的な処理を実行します。ディレクトリからなります👇gitops-engine/├── pkg│   ├── cache│   ├── diff   # リポジトリとClusterの間のマニフェストの差分を検出する。ArgoCDのDiff機能に相当する。│   ├── engine # 他のパッケージを使い、GitOpsの一連の処理を実行する。│   ├── health # Clusterのステータスをチェックする。ArgoCDのヘルスチェック機能に相当する。│   ├── sync   # Clusterにマニフェストをデプロイする。ArgoCDのSync機能に相当する。│   └── utils  # 他のパッケージに汎用的な関数を提供する。│...↪️：gitops-engine/design-top-down.md at master · argoproj/gitops-engine · GitHub【５】application-controllerは、デプロイ先Clusterのマニフェストと、repo-serverから取得したマニフェストの差分を検出します。ここで、kubectl diffコマンドの実行が自動化されています。【６】application-controllerは、処理結果をredis-serverに保管します。redis-serverは、Applicationやリポジトリのコミットの単位で、application-controllerの処理結果を保管しています。$ kubectl exec -it argocd-redis-server \    -n foo \    -- sh -c "redis-cli --raw"127.0.0.1:6379> keys *...app|resources-tree|<Application名>|<キャッシュバージョン>cluster|info|<デプロイ先ClusterのURL>|<キャッシュバージョン>git-refs|<マニフェスト/チャートリポジトリのURL>|<キャッシュバージョン>mfst|app.kubernetes.io/instance|<Application名>|<最新のコミットハッシュ値>|<デプロイ先Namespace>|*****|<キャッシュバージョン>...【７】application-controllerは、Applicationの操作に応じて、Clusterにマニフェストをデプロイします。ここで、kubectl applyコマンドの実行が自動化されています。Kubernetesリソースのマニフェストには、metadata.managedFieldsキーがあり、何がそのマニフェストを作成/変更したのかを確認できます。実際にマニフェストを確認してみると、確かにapplication-controllerがマニフェストを作成/変更してくれたことを確認できます。apiVersion: apps/v1kind: Deploymentmetadata:  managedFields:    # ArgoCDのapplication-controllerによる管理    - manager: argocd-application-controller      apiVersion: apps/v1      # kube-apiserverに対するリクエスト内容      operation: Update      time: "2022-01-01T16:00:00.000Z"      # ArgoCDのapplication-controllerが管理するマニフェストのキー部分      fields: ...️↪️：Server-Side Apply | Kubernetes05. dex-serverdex-serverとはインフラレイヤーにあるdex-serverです。SSO (例：OAuth 2.0、SAML、OIDC) を採用する場合に、argocd-serverの代わりに認可リクエストを作成し、IDプロバイダー (例：GitHub、Keycloak、AWS Cognito、Google Auth、など) にこれを送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。↪️：GitHub - dexidp/dex: OpenID Connect (OIDC) identity and OAuth 2.0 provider with pluggable connectorsdex-serverを使わずに、argocd-serverからIDプロバイダーに認可リクエストを直接的に送信することもできます。執筆時点 (2023/05/02) で、argocd-serverは特にOIDCの認可リクエストを作成できるため、ログイン要件がOIDCの場合は、dex-serverを必ずしも採用してなくもよいです。言い換えれば、その他のSSO (例：OAuth 2.0、SAML) を使用する場合は、dex-serverを採用する必要があります👍️↪️：Overview - Argo CD - Declarative GitOps CD for Kubernetes仕組み【１】デプロイ先Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。【２】argocd-serverは、認証フェーズをIDプロバイダーに委譲するために、dex-serverをコールします。argo-cd/authz-authn.md at master · argoproj/argo-cd · GitHub【３】dex-serverは、認可リクエストを作成します。認可リクエストに必要な情報は、ConfigMap (argocd-cm) で設定しておく必要があります。argocd-cmのざっくりした実装例は以下の通りです👇ここでは、IDプロバイダーをGitHubとし、認可リクエストに必要なクライアントIDとクライアントシークレットを設定しています。apiVersion: v1kind: ConfigMapmetadata:  namespace: foo  name: argocd-cmdata:  dex.config: |    connectors:      - type: github        id: github        name: GitHub SSO        config:          clientID: *****          clientSecret: *****        # dex-serverが認可レスポンスを受信するURLを設定する        redirectURI: https://example.com/api/dex/callbackdex.configキー配下の設定方法に関しては、dexのドキュメントをみると良いです👍↪️：Dex【４】dex-serverは、前の手順で作成した認可リクエストをIDプロバイダーに送信します。【５】IDプロバイダー側でSSOの認証フェーズを実施します。IDプロバイダーは、コールバックURL (<ArgoCDのドメイン名>/api/dex/callback) を指定して、認可レスポンスを送信します。認可レスポンスは、argocd-serverを介して、dex-serverに届きます。GitHubをIDプロバイダーとする場合、 Developer settingsタブ でSSOを設定する必要があり、この時にAuthorization callback URLという設定箇所があるはずです👍🏻【６】argocd-serverは、AuthZで認可フェーズを実施します。ConfigMap (argocd-rbac-cm) を参照し、IDプロバイダーから取得したユーザーやグループに、ArgoCD系リソースに関する認可スコープを付与します。ざっくりした実装例は以下の通りです👇ここでは、developerロールにはdevというAppProjectに属するArgoCD系リソースにのみ、またmaintainerロールには全てのAppProjectの操作を許可しています。またこれらのロールを、IDプロバイダーで認証されたグループに紐づけています。特定のArgoCD系リソースのみへのアクセスを許可すれば、結果として特定のClusterへのデプロイのみを許可したことになります👍apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  # デフォルトのロール  policy.default: role:developer  policy.csv: |    # ロールとArgoCD系リソースの認可スコープを定義する。    p, role:developer, *, *, dev/*, allow    p, role:maintainer, *, *, dev/*, allow    p, role:maintainer, *, *, prd/*, allow    # IDプロバイダーで認証されたグループにロールを紐付ける。    g, developers, role:developer    g, maintainers, role:maintainer  scopes: "[groups]"ConfigMap (argocd-rbac-cm) の認可スコープの定義には、 Casbin の記法を使用します。今回の実装例で使用したpとgでは、以下を定義できます。    記号        説明        記法    p (パーミッション)         ロールとArgoCD系リソースの認可スコープを定義する。        p, <ロール名> <Kubernetesリソースの種類> <アクション名> <AppProject名>/<Kubernetesリソースの識別名>    g (グループ)         グループにロールを紐付ける。        g, <グループ名> <ロール名>    RBAC Configuration - Argo CD - Declarative GitOps CD for Kubernetes06. argocd-server (argocd-apiserver)argocd-serverとは最後に、インフラレイヤーにあるargocd-serverです。『argocd-apiserver』とも呼ばれます。みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能し、他のコンポーネントと通信します🦄仕組み【１】application-controllerは、デプロイ先Clusterをヘルスチェックします。【２】application-controllerは、デプロイ先Clusterのマニフェストと、ポーリング対象のリポジトリのマニフェストの差分を検出します。【３】application-controllerは、処理結果をredis-serverに保管します。【４】argocd-serverは、redis-serverから処理結果を取得します。【５】デプロイ先Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。【６】Ingressコントローラーは、Ingressのルーティングルールを参照し、argocd-serverにルーティングします。【７】argocd-serverは、ログイン時にIDプロバイダーに認可フェーズを委譲するために、dex-serverをコールします。【８】IDプロバイダー上で認証フェーズが完了します。argocd-serverは、ConfigMap (argocd-rbac-cm) を参照し、デプロイ先Cluster管理者に認可スコープを付与します。【９】argocd-serverは、認可スコープに応じて、デプロイ先Cluster管理者がApplicationを操作できるようにします。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:  # 設定してはダメ  # application.namespaces: "*" # 全てのNamespaceを許可する。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: dev-foo-project  namespace: foospec:  # 設定してはダメ  # sourceNamespaces:  #  - "foo"これらにより、fooのNamespaceに属するArgoCDは、他のNamespaceにはアクセスできなくなります👍↪️：Installation - Argo CD - Declarative GitOps CD for Kubernetes【１０】デプロイ先Cluster管理者は、ダッシュボード (argocd-server) を使用して、ClusterにマニフェストをSyncします。この時、Applicationを介してapplication-controllerを操作し、マニフェストをデプロイします。図では、App-Of-Appsパターンを採用したと仮定しています👨‍👩‍👧‍👦デザインパターンがあります。これは、Applicationを階層上に作成するものであり、最下層のApplication配下のマニフェストをより疎結合に管理できます✌️例えば以下の画像の通り、最上位のApplication配下に、チーム別の親Applicationを配置します (アプリチームの親Application、インフラチームのそれ) 。その後、両方のApplication配下にさらにチャート別に最下層の子Applicationを配置し、チャートのデプロイを管理します。アプリチーム最下層の子Applicationではアプリコンテナのチャート、インフラチームの子Applicationでは監視/ネットワーク/ハードウェアリソース管理系のチャートを管理します👍07. アーキテクチャのまとめ今までの全ての情報をざっくり整理して簡略化すると、ArgoCDは以下の仕組みでマニフェストをデプロイすることになります👇08. おわりにArgoCDによるデプロイの仕組みの仕組みをもりもり布教しました。ArgoCDは、UIが使いやすく、仕組みの詳細を知らずとも比較的簡単に運用できるため、ユーザーフレンドリーなツールだと思っています。もしArgoCDを使わずにマニフェストをデプロイしている方は、ArgoCDの採用をハイパー・ウルトラ・アルティメットおすすめします👍なお、登場した設計プラクティスのいくつかは、以下の書籍にも記載されていますので、ぜひご一読いただけると🙇🏻‍↪️：https://www.amazon.co.jp/dp/B0BQL1CBPXhttps://www.amazon.co.jp/dp/180323332X謝辞ArgoCDの設計にあたり、@yaml_villager さんに有益なプラクティスをご教授いただきました。この場で感謝申し上げます🙇🏻‍]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[現代のクエリオプティマイザの基礎となる技術をまとめた論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/05/02/105429</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/05/02/105429</guid>
            <pubDate>Tue, 02 May 2023 01:54:29 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨1998年に発表されたクエリオプティマイザの基礎としてとくに重要な手法をまとめた論文を読みました。An Overview of Query Optimization in Relational Systems著者についてSurajit Chaudhuriによる論文Microsoft所属の研究者でRDBMSの研究を行なっており、近年ではCloudにおけるDBMSの研究を行なっている。概要RDBMSが提案された1970年代からクエリ最適化は大規模で幅の広く研究が行なわれてきた。この論文では執筆当時(1998年)までの重要な研究の基礎を説明している。手法探索空間統計情報とコストの推定列挙アルゴリズムアルゴリズムについて説明している。論文内では拡張可能なオプティマイザとして、StarburstとVolcano/Cascadeの2種類のオプティマイザの詳細を論じている。最新(当時)の最適化リアライズドビューについて説明している。作業時間read31:4031:40author33:402:00summary52:5519:15感想ベクトル化やパラレルジョインで扱われていたVolcanoオプティマイザの端に触れることが出来ました。内容としては基礎的な内容が多いものの、知らない概念もいくつかあり引用している論文も読みたいです。クエリ最適化の基礎を学ぶのに非常にいい内容でした。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[コンソールに GitHub の草を生やす GitHub CLI 拡張機能「gh-grass」の紹介]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/gh-grass-introduction</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/gh-grass-introduction</guid>
            <pubDate>Mon, 01 May 2023 09:19:56 GMT</pubDate>
            <content:encoded><![CDATA[GitHub の草 ( Contribution Graph ) をどうしてもコンソールに生やしたくなることはありませんか？ありますよね？僕はありません。そんなときに便利な GitHub CLI 拡張機能「gh-grass」を作りました。https://github.com/koki-develop/gh-grassこういうことができます。他にもこんなことや、文字をカスタマイズこんなこともできます。アニメーションこの記事では gh-grass のインストール方法から基本的な使い方についてまとめます。 インストール!GitHub CLI がインストールされてい...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DBMSとクライアント間におけるデータ転送を最適化する論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/05/01/123418</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/05/01/123418</guid>
            <pubDate>Mon, 01 May 2023 03:34:18 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2017年に出版されたリモートDBMSとクライアント間の大量データ転送を最適化する手法を提案する論文を読みました。Don’t Hold My Data Hostage – A Case For Client Protocol Redesign著者についてMark Raasveldt、Hannes Muhleisenらのグループによる論文。いずれもCentrum Wiskunde & Informaticaの所属で、DuckDBのCxO。DBMSと分析システムにおけるパフォーマンス最適化を研究している。問題意識DBMSからクライアントプログラムに大量のデータを転送することは一般的なタスクである。例えばRやPythonなどを用いた分析システムはしばしばデータベース・インターフェースを利用してデータの取得を行なっている。一方でネットワーク越しにデータを転送することはレイテンシを増加させ、転送時間を長引かせる要因である。そのため分析用途で大量のデータ転送を避け、一部のデータをサンプルとして利用するに止まることが多い。このアプローチはパフォーマンスの低下を押さえられるものの、分析や機械学習の精度を下げることに繋がる。とくに既存のクライアントではネットワークによるレイテンシとスループットの制限に大きな影響を受けパフォーマンスを劣化させる。この問題はデータベースが別マシンやクラウドで動作するときにより大きな問題となる。手法本論文では既存のシリアライズ手法と圧縮手法によるパフォーマンスへの影響を計測し、新しいプロトコルとして以下の特性を持つ手法を提案している。1. チャンク毎のデータ転送と(デ)シリアライゼーション1. ヒューリスティックによる圧縮方法の決定1. text/binaryによるカスタムシリアライゼーションを使用する1. NULL終端によるテキストの取り扱い実験結果提案手法を実装したMonetDB(表内ではMonetDB++)とPostgreSQL(表内ではPostgreSQL++)を既存のDBMSやnetcatと比較することで評価を行なっている。TCP-Hのlineitem、American Community Survay、Airline On-Time Statisticsの3つのデータセットで評価を行なったところ、ローカル通信における非圧縮netcatを除き殆どのケースでMonetDB++系が最良のパフォーマンスを発揮し次点でPostgreSQL++系が優れた結果を残している。Table 10Table 11Table 12PostgreSQLに比べMonetDBが優れている理由はPostgreSQLの行指向データを列指向に変換するコストのためである。作業時間read31:2131:21author35:384:17summary70:1334:35感想論文出版時にはTPC/IPプロトコルが前提でQuic登場前のため、ネットワークプロトコル自体は考慮されていない。現在であればTPC/IPとQuicに適合した手法の比較が行なわれると思うので気になるところ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SQL ServerにおけるUDF最適化の論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/04/28/112905</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/04/28/112905</guid>
            <pubDate>Fri, 28 Apr 2023 02:29:05 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2017年に発表されたSQL ServerでUDFを最適化しているFroidという手法についての論文を読みました。Froid: Optimization of Imperative Programs in a Relational Database著者についてKarthik Ramachandra、Kwanghyun Park、K. Venkatesh Emani、Alan Halverson、Cesar Galindo-Legaria、Conor Cunninghamのグループによる論文。ほとんどの著者はMicrosoftに所属しており、いずれもトランザクショナルワークロードでのRDBMSの最適化や分析ワークロードにおけるRDBMS最適化の研究をしている。問題意識RDBMSではSQLによるデータ処理アプローチと、UDFやストアドプロシージャなどによる命令型のデータ処理アプローチを提供している。SQLによるデータアクセスは高度に最適化されてきた一方で、命令型のデータ処理は非効率なため性能を阻害し利用を禁止している組織すらある。UDFによるデータアクセスは非効率であるものの、SQLに比べ下記のような利点を提供するため幅広く利用されているのも事実である。1. SQL間でコードの再利用方法を提供する1. 複雑なビジネスロジックやMLアルゴリズムなどSQLでは難しい表現を可能にする1. 単純なSQLの組み合わせのため、ユーザーの意図が明確に表現できるこれらのメリットを享受するためにRDBMSにおける命令型データアクセス手法のパフォーマンスを向上しする必要があった。手法提案手法であるFroidはMicrosoft SQL Serverにおける命令型コードのパフォーマンス向上の手法として、UDFを複雑なサブクエリとしてみなすアプローチを取っている。UDFを構成する命令はDECLARE、SELECT、IF/ELSE、RETURN、他のUDF、リレーショナルオペレーションの6つに分ることができる。提案手法ではこれらの命令を一般的なT-SQLに置き換え、Apply演算により一つの関係式に結合する方法で実現している。Table 1命令が一般SQLに置き換えられることでUDFに対して、SQLに用いられていた高度な最適化を導入することが出来る。また提案手法ではい以下の理由から、SQLとして命令を置換するときにクエリ最適化時に行なうのではなくバインド時に置換をしている。1. 実際のワークロードでの実験ではほぼ全てのケースでバインド時のほうが性能がよかった1. クエリオプティマイザの変更が不要1. バインディング時に特定の最適化を行なえるとくにクエリオプティマイザの変更はSQL Serverが商用データベースなため重要であった。作業時間read28:5028:50author32:103:20summary57:0024:50]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Pull Requestで意識していること]]></title>
            <link>https://qiita.com/bayobayo0324/items/0d986370e0de95705b6f</link>
            <guid>https://qiita.com/bayobayo0324/items/0d986370e0de95705b6f</guid>
            <pubDate>Fri, 28 Apr 2023 02:19:23 GMT</pubDate>
            <content:encoded><![CDATA[どんな記事？Pull Request（以後PR）で自分がレビュイのときに意識していることをまとめてみました。PRだけじゃなくSlack等のテキストベースコミュニケーションでも同じようなことを意識…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[DBMSの歴史とNewSQL]]></title>
            <link>https://zenn.dev/nnaka2992/articles/history_of_db_and_newsql</link>
            <guid>https://zenn.dev/nnaka2992/articles/history_of_db_and_newsql</guid>
            <pubDate>Wed, 26 Apr 2023 14:28:19 GMT</pubDate>
            <content:encoded><![CDATA[この記事はDBMSの登場以前から現代のDBMSを取り巻く環境までを振り返ることで、なぜNewSQLが必要とされ登場したのかをまとめます。 おことわり筆者はあくまでDBMSユーザーであり、研究者ではないため内容は個人の見解です。また対象読者はある程度DBMSに関わりがあり、OLTPやOLAP、列指向や行指向といった基本的な単語を理解しているものとします。またNewSQLの技術的詳細はスコープ外とします。 DBMS以前データベースという言葉は1950年代に米軍が情報基地を集約したことに由来します。一方で学術的なデータベースの起源はW. C. McGeeが1959年に発表...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[[DBREブログ] Snowflake とは？]]></title>
            <link>https://sreake.com/blog/what-is-snowflake/</link>
            <guid>https://sreake.com/blog/what-is-snowflake/</guid>
            <pubDate>Wed, 26 Apr 2023 02:37:56 GMT</pubDate>
            <content:encoded><![CDATA[はじめに クラウドデータウェアハウスの Snowflake についての解説です。Snowflake のアーキテクチャや料金体系、特徴、セキュリティについて説明しています。 概要 プラットフォームの概要 Snowflake […]The post [DBREブログ] Snowflake とは？ first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[中間結果が莫大になるときの結合を最適化する最悪ケース最適化結合をRDBMSに適応する論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/04/26/110646</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/04/26/110646</guid>
            <pubDate>Wed, 26 Apr 2023 02:06:46 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2018年に発表された分析ワークロードなどで発生しがちな最終結果に比べ、非常に大きな中間結果を作成してしまうクエリを多方向結合で最適化する論文を読みました。Adopting Worst-Case Optimal Joins in Relational Database Systems著者についてMichael Freitag、Maximilian Bandle、Tobias Schmidt、Alfons Kemper、Thomas Neumannによるグループの論文いずれの著者もDBMSにおける最適化を中心に研究しており、それぞれ分析ワークロードにおける最適化や最新のハードウェアにおける最適化などを研究している。問題意識従来のRDBMSにおける結合処理のほとんどはバイナリ結合に依存して複数のリレーションにまたがるクエリを処理してきた。数十年に渡る研究によりバイナリ結合は幅広い柔軟性と優れた性能を発揮するようになった。その一方でバイナリ結合による実行計画は特定のワークロードでは最適ではないケースを示すことが知られている。主な原因として実際のクエリ結果に比べて非常に大きな中間結果を生成するためである。とくにPK以外のキーによる結合が多くなる分析ワークロードではそのような状態を避けることが難しく、またグラフ分析のようなクエリパターンでも多く見られる。近年の論理的な進歩により中間結果の列挙を避ける多方向結合のアルゴリズムが開発可能になった。この手法はバイナリ結合計画より優れた実行時間を保証できるため、RDBMSの堅牢性を大幅に向上させる可能性を持っている。しかし現状最悪ケース最適化結合アルゴリズムでは以下のような問題を抱えている。1. 膨大なストレージとメンテナンスを必要とする結合に参加出来るカラムを含むインデックスを必要とする。1. RDBMSは挿入と更新のサポートが必要なものの、既存のアルゴリズムは高価な事前計算を必要とする。そのため本論文は以下の制約を満たすアプローチを提案している1. 多方向結合が有益な場合のみ多方向結合を使用するオプティマイザを必要とする。1. 実行中に効率的に実行でき、ディスクのに永続化する必要のないパフォーマントインデックスを必要とする。手法提案手法では比較ベースではなくハッシュベースの結合のため、2の「実行中に効率的に実行でき、ディスクのに永続化する必要のないパフォーマントインデックスを必要とする。」という要素の考慮を除いている。またオプティマイザについては既存のコストベースのものを拡張し適応している。提案手法では潜在的に成長している結合のカスケードを最悪の場合の最適結合に置き換えることで、最適化されたバイナリ結合計画を洗練させるヒューリスティックなアプローチを提案している。通常の結合順序最適化で使用されるのと同じカーディナリティ推定値に基づいて、中間テーブルが膨大になる結合を特定する。作業時間read22:1322:13author25:483:35summary52:5826:50感想とても難しい内容に感じてしまい、殆ど頭を通りすぎてしまった気がする。今まで最適化は触れずに来たため、理解が浅い領域だった。よくよく考えるとDBMSの話しに最適化が登場するのはあたりまえなので、今後はその方面にも触れて行きたい。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[羊を眠らせる sleep コマンド「sheep」の紹介]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/sheep-introduction</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/sheep-introduction</guid>
            <pubDate>Tue, 25 Apr 2023 09:00:22 GMT</pubDate>
            <content:encoded><![CDATA[羊を眠らせる sleep コマンドである sheep を作りました。https://github.com/koki-develop/sheepこの記事では sheep のインストール方法 ~ 使い方についてまとめます。インストール使い方まとめ インストールHomebrew を使用している場合は brew install でインストールできます。$ brew install koki-develop/tap/sheepもしくは、 sheep は Go で作られているため go install でインストールすることもできます。$ go install githu...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[マルチコアメインメモリにおけるソートジョインとハッシュジョインのパフォーマンスを検証した論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/04/24/112354</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/04/24/112354</guid>
            <pubDate>Mon, 24 Apr 2023 02:23:54 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2013年に発表された"Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited"という論文を読みました。当時最新のアルゴリズムとハードウェアにおける、ソートとハッシュによる結合のパフォーマンスを比べた論文です。Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited著者についてCagri Balkesen、Gustavo Alonso、Jens Teubner、M. Tamer Ozsuらのグループによる論文いずれもDBMSにおけるクエリ最適化やビッグデータにおけるパフォーマンスを研究している。またGustavo Alonsoはハードウェアや分散システムもメインのフィールドとしている。問題意識DBMSにおいて常にソートマージとハッシュ結合の性能比較が行われており、最新の研究ではSIMDやNUMAへの適正に基づいてソートマージがより優れていると結論づけられていた。しかしこれらの分野は常に研究が重ねられ、過去の検証時には登場していなったハッシュ結合の最適化手法が生れた。この論文ではそれらを適用し再度ソートマージとハッシュ結合の性能比較を行なう。手法本論文では以下に分けて結合手法の評価を行なっている。1. ソートフェーズの評価SIMDソートアルゴリズムとC++のSTLソートアルゴリズムを比較している。マージフェーズの評価入力サイズの調整によるマージフェーズの最適化パーマンスを検証している。ソートマージジョインにおける影響要因の特定結果結合対象のデータサイズに拘わらずハッシュによる結合がソートベースの結合のパフォーマンスを上回っている。Figure 14ソートマージによる結合は入力サイズが著しく大きくなったときのみハッシュ結合のパフォーマンスに近づく。Figure 15ソートマージ、ハッシュ結合におけるデータの偏りはパフォーマンスに大きな影響を及ぼさなかった。Figure 16いずれのアルゴリズムも物理的なコア数では線形にスケールした。Figure 17作業時間read23:1123:11author27:093:58summary60:1232:57]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[RDBでの結合手法を比較した論文を読みました]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2023/04/23/231628</link>
            <guid>https://nnaka2992.hatenablog.com/entry/2023/04/23/231628</guid>
            <pubDate>Sun, 23 Apr 2023 14:16:28 GMT</pubDate>
            <content:encoded><![CDATA[この記事の趣旨2016年に発表された"An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory"という論文を読みました。様々な結合手法を包括的に比較した論文でどのような結合方法がどのような時に適しているかを示しています。An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory著者についてStefan Schuh、Xiao Chen、Jens Dittrichのグループによる論文。いずれもDBMSや分析システム、Hadoopなどにおける検索高速化・最適化の研究を行なっている。問題意識関係結合はほとんど全てのクエリプランにおいて中核をなす処理であり、定期的に研究・改良され再検討されてきた。新たな手法が提案され実験を行なわれるものの、それぞれ結果において比較を困難にする要素や幾らかの矛盾を孕んでいた。例えば同じハッシュベースの結合アルゴリズムの比較でも実装が異なったり、複数の論文でパフォーマンス比較で正反対の結果を示しているためである。そのため単純に論文執筆時点で最も高速な結合アルゴリズムを結論づけることが困難であった。手法本論文では結合方法を以下の3つに分類した1. パーティションベースハッシュジョインパーティションに分割し結合する手法。ハッシュテーブルの構築と結合されるデータの探索のキャッシュミスを最小にする事を目的としている。非パーティションベースハッシュジョインパーティションテーブルを構築しながら結合を行なう手法で、マルチスレッドと順番に依存しない実行によりキャッシュミスのパフォーマンス劣化を隠蔽している。ソートマージジョインSIMDによりベクトル化される。検証ではこれらの結合方法を以下の3つのテストで使用するために、全部で13のアルゴリズムを検証している。1. ブラックボックス比較ブラックボックス的に比較する。ホワイトボックス比較ブラックボックス比較で検証する結合方法に先行研究で示された最適化を施した上で比較を行なう。パラレルラディックスジョイン比較Table 2結果パーティション結合の一種であるリモート書込みを排除したCPR系アルゴリズムは小さな入力に対して有効ではないスケールの大きい結合ではとくに理由が無い場合、パーティションベースのジョインを利用する大きなサイズのページを利用するソフトウェアライトコンバインバッファ()を利用するパーティションジョインでは適切なパーティションビットを利用するできるかぎりシンプルなアルゴリズムを利用するNUMAを考慮したアルゴリズムを利用する実行時間とクエリ時間は同一ではない作業時間read31:3431:34author35:183:46summary77:5042:32]]></content:encoded>
        </item>
    </channel>
</rss>