<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sun, 15 Feb 2026 11:45:21 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[Claude CodeのPLAN MODEは使ったほうがいい]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/13/122228</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/13/122228</guid>
            <pubDate>Fri, 13 Feb 2026 03:22:28 GMT</pubDate>
            <content:encoded><![CDATA[Claude Codeを使い始めて、様々な発信をしてきました。今回は「PLAN MODE」と「パーミッションモード」について。Claude Codeには複数の動作モードがあり、これを使い分けられるかどうかで出力品質が劇的に変わります。Claude Code の settings.json は設定した方がいい - じゃあ、おうちで学べるClaude Code の CLAUDE.mdは設定した方がいい - じゃあ、おうちで学べるClaude Code の .claude/commands/**.md は設定した方がいい - じゃあ、おうちで学べるClaude CodeのHooksは設定したほうがいい - じゃあ、おうちで学べるClaude CodeのSubagentsは設定したほうがいい - じゃあ、おうちで学べるClaude Codeの Agent Skills は設定したほうがいい - じゃあ、おうちで学べるはじめに「この機能を実装して」。Claude Codeにそう言って、Enter を押す。Claude は即座にファイルを読み、コードを書き始める。5分後、20ファイルが変更されている。ビルドは通る。テストも通る。しかし見返してみると、設計意図と違う。やり直し。また5分。今度は別の方向にずれている。こういう経験、ありませんか。私自身、Rustプロジェクトの認証モジュールのリファクタリングをClaude Codeに任せたとき、まさにこの罠にはまりました。「OAuth2対応して」と言ったら、既存のセッション管理を全部書き換え始めた。後方互換性は？DBマイグレーションは？聞いてもいないのに勝手にやってくれる。やり直し。2回目も方向性がずれる。3回目でようやく「まず計画を立てよう」と思い至りました。この問題の根本にあるのは、「考える」と「実行する」を同時にやらせていることです。人間のエンジニアだって、設計書なしにいきなりコーディングすれば事故が起きます。「え、何作ってんの？」と聞かれた経験があるのは私だけではないはずです。Claude Codeも同じです。LLMは与えられたタスクを「前に進める」ことに最適化されています。立ち止まって全体を俯瞰することは、明示的に指示しない限りやりません。そして、もう一つの問題はパーミッションの確認地獄です。Normal Modeでは、mkdirひとつ、git statusひとつに対しても確認ダイアログが出ます。これはセキュリティ上は正しい。しかし、複雑なタスクで20回、30回と「Allow」を押し続けるのは、ワークフローの断絶そのものです。コーヒーを取りに行って戻ってきたら、ステップ2で止まっている。あるあるです。Claude Codeはこの2つの問題に対して、段階的なパーミッションモードを用意しています。PLAN MODEで「考える」と「実行する」を分離し、CLAUDE.mdやRulesで品質の枠組みを整え、最終的に--dangerously-skip-permissionsで自律実行させる。この段階的なアプローチが、私が最近たどり着いたワークフローです。ただし、ここで強調しておきたいことがあります。設定を1行書くたびに、Claudeの手綱が1段緩められる。 CLAUDE.mdとRulesが未整備なら、PLAN MODEで人間が計画をレビューすることが唯一の品質保証手段になる。逆に、CLAUDE.mdとRulesを丁寧に整備すればするほど、Claudeへの信頼度が上がり、最終的には--dangerously-skip-permissionsでの自律実行まで到達できる。ガードレールの精度が、委任できる範囲を決める。前回の記事で紹介したAgent Skillsが「専門知識の注入」だとすれば、PLAN MODEは「思考プロセスの制御」、Rulesは「行動の制約」、そしてパーミッション設定は「自律性のコントロール」です。この4つは独立して機能するものではなく、掛け算で効く。 1つが欠けると、他の3つの効果も減衰します。code.claude.comcode.claude.comこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。他の設定・機能との位置づけ今まで紹介してきた機能にPLAN MODE、Rules、パーミッションモードを加えて、改めて整理します。 機能                    役割                            例                                            CLAUDE.md           プロジェクトのグローバルな文脈  「うちはRustで、こういうアーキテクチャ」      Rules               関心事ごとのルール適用      セキュリティ、テスト、Git規約など         commands            手動で呼び出すショートカット    /test-and-commit で一連の作業を実行         Hooks               特定のイベントで自動実行        ファイル保存後に自動フォーマット              Subagents           専門家を自動で呼び出す          デバッグ時にdebugger subagentが起動           Agent Skills        専門知識をオンボーディング      PDF操作、独自ワークフロー                     PLAN MODE           思考と実行を分離する        設計レビュー → 承認 → 実装                パーミッション設定  自律性のレベルを制御        Normal → Auto-Accept → YOLO              この表を見ていて気づくのは、上6行と下2行の性質の違いです。CLAUDE.mdからSkillsまでは「何を伝えるか」——つまり知識の問題です。PLAN MODEとパーミッション設定は違う。「どう振る舞わせるか」の問題です。 入力を整えるのと、出力を制御するのとでは、アプローチがまったく異なります。前者をいくら積み上げても、後者の代わりにはならない。Rulesは、両者の境界に位置する存在です。「知識」として読み込まれながら、実質的には「行動制約」として機能する。CLAUDE.mdと同じ「高優先度」でロードされながら、関心事ごとにファイルを分離できる。Rulesの質がそのまま自律実行の安全性を決める。 正直に言えば、CLAUDE.mdの整備に時間をかけていた初期の私は、このRulesの重要性を完全に見落としていました。CLAUDE.mdのリファクタリングとRulesの整備PLAN MODEの話に入る前に、避けて通れない前提があります。CLAUDE.mdとRulesの整備です。ここが雑だと、どんなモードを使っても意味がない。道具の使い方以前に、道具を使う土台の問題です。モノリシックなCLAUDE.mdの問題正直に告白すると、私のCLAUDE.mdも一時期500行を超えていました。最初は「ビルドはcargo fmtから」くらいだったのが、使い込むうちにあらゆる規約を書き足していく。APIの設計指針、テストの規約、セキュリティ要件、デプロイ手順。気づけば、プロジェクトの全知識を一つのファイルに詰め込んでいた。問題の本質は「注意の希釈」です。 CLAUDE.mdはセッション開始時に全文がロードされます。バックエンドのAPIを修正しているのにフロントエンドのCSS規約も読み込まれる。前回の記事で説明したProgressive Disclosure（段階的開示）の真逆です。人間に例えるなら、料理をしようとしている人に家中のマニュアルを全部渡すようなもの。情報が多いほど、個々の指示の「重み」が希釈される。500行の中に埋もれた「unwrap()は禁止」の1行を、Claudeが確実に拾ってくれる保証はありません。CLAUDE.mdを50行以下にリファクタリングする私は、CLAUDE.mdをルーティングテーブルにまで圧縮しています。具体的には、以下のプロンプトをClaude Codeに投げます。Analyze CLAUDE.md and propose an extraction plan before making changes. Decompose into: .claude/rules/*.md for file-type or directory-scoped conventions (with paths: glob frontmatter), .claude/skills/<name>/SKILL.md for multi-step procedures and code generation patterns (with name:/description: frontmatter), .claude/commands/<name>.md for user-invoked reusable prompts, .claude/agents/<name>.md for delegatable specialist roles (with name:/description:/tools: frontmatter). After extraction, reduce CLAUDE.md to <50 lines containing only project overview, tech stack, universal commands, and @imports to extracted files. Report line count and estimated token count before and after.このプロンプト自体がPLAN MODEの実践例です。 「まず計画を見せて」で計画を先に提示させ、確認してから実行する。これが後述するワークフローの基本パターンです。プロンプトの中に抽出先ごとのfrontmatter形式まで指定しているのは、Claudeに「どういう形式で出力すべきか」を明示するためです。曖昧に任せると、ファイル構造がプロジェクトごとにバラバラになる。リファクタリング後のCLAUDE.mdはこうなります。# プロジェクト概要RustによるgRPC APIサーバー。sqlxでDB接続。# 優先順位正しさ > 保守性 > パフォーマンス > 簡潔さ# ビルド・テストcargo fmt --check && cargo clippy -- -D warnings && cargo test --workspace# 禁止事項- unwrap() / expect() は本番コードで使用禁止- unsafe ブロック追加禁止- 既存のpublic APIシグネチャ変更禁止# 参照See @README.md for architecture overview50行以下。プロジェクトの最も重要な制約だけが残ります。残りはすべて、適切な場所に分配されています。Rulesへの抽出：関心事で分離し、パスでスコープするCLAUDE.mdから抽出した情報のうち、特定の関心事に属するルールは.claude/rules/に配置します。セキュリティ、テスト、Git規約、パフォーマンス——それぞれ独立したファイルに分離する。さらに、各ルールファイルの先頭にpaths: frontmatterを書くことで、特定のパスのファイルを編集しているときだけルールがロードされるようにできます。.claude/rules/├── security.md          # セキュリティ規約（NEVER/MUST形式）├── coding-style.md      # コーディング規約（命名規則、構造体設計）├── testing.md           # テスト規約（命名規則、カバレッジ目標）├── git-workflow.md      # Git規約（Conventional Commits、ブランチ戦略）├── agents.md            # エージェント委譲ルール（いつ誰に任せるか）└── performance.md       # パフォーマンス規約（クローン回避、ビルド最適化）各ルールファイルは、関心事ごとに分離し、paths: frontmatterでスコープを切ります。 たとえばセキュリティルールは「NEVER（絶対禁止）」と「MUST（必須）」の2カテゴリで構成し、Rustのソースコードを触るときだけロードされるようにする。---paths:  - src/**/*.rs---# セキュリティルール## NEVER- unwrap()/expect() のプロダクション使用- 機密情報のハードコード- 未検証の unsafe ブロック## MUST- 入力バリデーション- 整数オーバーフロー対策（checked_* 使用）- SQLパラメータ化（文字列結合禁止）- 依存関係の定期監査テスト規約も同様に、テストファイルを触るときだけ読み込まれるようスコープを切ります。---paths:  - tests/**/*.rs  - src/**/tests.rs---# テスト規約- テスト関数名は `test_<対象>_<条件>_<期待結果>` 形式- カバレッジ目標: ビジネスロジック90%、APIハンドラ80%、ユーティリティ70%- ミューテーションテスト（cargo-mutants）で品質を検証- 非同期テストは #[tokio::test] を使用ポイントは二つあります。 一つはルールの粒度。「すべてを網羅する」のではなく、「Claudeが最も間違えやすいポイント」に絞る。私の経験では、セキュリティ（unwrap禁止、入力バリデーション）とテスト（命名規則、カバレッジ目標）の2つが、Rulesの効果が最も顕著に現れる領域です。もう一つはpaths: frontmatterによる条件付きロード。テストファイルを書いているときにセキュリティルールは読み込まれない。必要なルールだけが、必要なときに、高優先度で読み込まれる。前回の記事で説明したSkillsの段階的開示と同じ発想です。情報を「足す」のではなく、「絞る」ことで精度を上げる。 引き算の設計思想がここにもあります。なぜこの整備がPLAN MODEと--dangerously-skip-permissionsに直結するのかここが本記事の核心的な主張です。PLAN MODEとの関係: CLAUDE.mdが肥大化していると、PLAN MODEで立てた計画の品質も下がります。Claudeは500行のCLAUDE.mdを読んで「このプロジェクトの規約はこうだ」と理解しようとしますが、情報が多すぎると重要なルールを見落とす。結果、計画に「unwrap()を使う」ようなコードが含まれる。ルールが整備されていれば、PLAN MODEで立てる計画の精度が上がる。--dangerously-skip-permissionsとの関係: YOLO実行中はClaude が自律的にファイルを編集しコマンドを実行します。このとき、Claudeの行動を制約するのはCLAUDE.mdとRulesだけです。人間が逐一チェックしない以上、ルールの品質＝自律実行の安全性です。Rulesが整備されていなければ、YOLO実行は文字通りギャンブルになります。つまり、CLAUDE.mdのリファクタリングとRulesの整備は、単なる整理整頓ではなく、PLAN MODEの計画精度と--dangerously-skip-permissionsの安全性を決定する基盤なのです。Rulesでも防げないもの：Linterという最終防衛線ここで正直に言っておくべきことがあります。Rulesを整備しても、Claudeが100%ルールに従うとは限らない。 CLAUDE.mdに「unwrap()禁止」と書いてあっても、複雑なリファクタリングの最中にClaudeがunwrap()を使うことはある。Rulesはあくまで「指示」であって、「強制」ではないからです。では、どうするか。Linterを作るしかない。 エージェントは「鏡のないバスルーム」では身だしなみを整えられない。自分の出力を検証するフィードバックループがなければ、ミスに気づけないまま突き進む。テスト、Linter、型チェック——これらは「鏡」です。鏡を置いてやることで、人間の監視コストが劇的に下がる。cargo clippyのカスタムルール、ESLintのプロジェクト固有設定、Ruffのルールセット。RulesがClaudeへの「お願い」だとすれば、Linterは「物理的な壁」です。unwrap()が混入しても、cargo clippy -- -D warningsがCIで弾く。人間がレビューで指摘しなくても、機械が確実に止める。正直に告白すると、私の業務時間のかなりの部分はLinterの整備に充てられています。プロジェクト固有のLintルールを書き、CIに組み込み、Hooksで自動実行させる。地味な仕事です。しかし、この投資が「YOLO実行」の安全性を根本から支えている。 Rulesが「Claudeの行動指針」、Linterが「機械的な品質保証」、その二重構造があって初めて、自律実行は「ギャンブル」から「運用」になります。ここで一つ、原則を明確にしておきます。「二度目の失敗」は仕組みの欠陥です。 エージェントの同じミスに二度怒るのは、ガードレールのない道路で事故を運転者のせいにするのに似ている。Claudeがunwrap()を使ったら、一度目は指摘して直す。二度目があったなら、それはClaudeの問題ではなく、ルールの問題です。ミスを観測したら即座に構造で封じる——Rulesに追記するか、Linterのルールを追加するか、Hooksで自動検出させる。これはAI固有の話ではなく、あらゆる自動化の鉄則の再発見です。私がRulesファイルを毎週のように更新しているのは、この原則を愚直に回しているからにすぎません。後述するワークフローの「[テストコマンド]を継続的に実行して」というキーフレーズも、この思想の延長線上にあります。テストもLintも、Claudeの出力を機械的に検証する仕組みです。人間のレビューに依存しない品質保証の仕組みをどれだけ積み上げられるかが、自律実行の信頼性を決める。Skillsやcommands、agentsへの分配Rulesの他にも、CLAUDE.mdから抽出すべき情報はあります。 抽出先               対象                              例                                                  .claude/rules/     関心事ごとのルール                セキュリティ、テスト、Git、パフォーマンス            .claude/skills/    専門知識のリファレンス            パターン集、コーディング標準                         .claude/commands/  ユーザーが手動で呼ぶテンプレート  /review、/plan、/cargo-check                   .claude/agents/    専門的な独立タスク                コードレビュアー、プランナー、セキュリティレビュアー この分配を行った後の変更前/変更後を比較すると、その効果は数字で見えます。 指標                  変更前  変更後  CLAUDE.md行数         487行   42行    起動時ロードトークン  ~8,000  ~1,200  ルールファイル        0個     6個     Skill/Agent           0個     11個   起動時のトークン消費が85%削減されました。その分のコンテキストが、実際のタスクに使えるようになります。Claude Codeのパーミッションモード全体像では、パーミッションモードの全体像に入ります。これを理解することが、PLAN MODEと--dangerously-skip-permissionsを使いこなす前提です。4つのモード モード                          動作                                        切り替え                          Normal Mode（Ask）          すべての操作に承認が必要                    デフォルト                        Auto-Accept Mode            ファイル編集は自動承認、コマンド実行は確認  Shift+Tab × 1                     Plan Mode                   読み取り専用、変更操作は一切不可            Shift+Tab × 2                     Bypass Permissions（YOLO）  すべての操作を自動実行                      --dangerously-skip-permissions Shift+Tabを押すたびにモードが切り替わります。Normal → Auto-Accept → Plan → Normal → ...のサイクルです。Normal Mode：安全だが遅いデフォルトのモードです。すべての操作に承認が必要。Claudeがファイルを編集するたびに差分表示、コマンドを実行するたびに確認ダイアログ。安全だが、安全なだけだ。 安全であることは目的ではない。しかし、CLAUDE.mdもRulesも整備されていない初期段階では、このモード一択です。信頼は実績の上に築くものであって、最初から与えるものではない。 これはAIに限らず、人間同士の協働でも同じことです。Auto-Accept Mode：ファイル編集の信頼Shift+Tabを1回押すと入るモードです。ターミナル下部に ⏵⏵ accept edits on と表示されます。ファイル編集は自動承認、Bashコマンドは確認が必要。私はこのモードを「半信半疑モード」と呼んでいます。Claudeのコード変更は信頼するが、システムに影響を与える操作はまだ自分の目で確認したい。CLAUDE.mdを整備した段階で、自然とここに移行することが多いです。Plan Mode：読み取り専用の安全地帯Shift+Tabを2回押すと入るモードです。ターミナル下部に ⏸ plan mode on と表示されます。次のセクションで詳しく説明します。--dangerously-skip-permissions：完全自律実行いわゆる「YOLO Mode」です。すべてのパーミッションチェックをバイパスし、Claudeが中断なく最後までタスクを実行します。claude --dangerously-skip-permissions名前に「dangerously」が入っているのは伊達ではありません。しかし、正しく使えば最も生産性が高いモードでもあります。PLAN MODEで計画を立て、CLAUDE.mdとRulesで品質の枠組みを整えた上で使えば、「計画通りに最後まで自動実行」が実現します。安全に使うための前提条件:Gitで必ずチェックポイントを作る: 実行前にgit add -A && git commit -m "checkpoint"隔離環境で実行する: Docker コンテナやDevcontainerがベストCLAUDE.mdとRulesを整備済みであること: 前セクションで述べた基盤が必要計画が承認済みであること: PLAN MODEで立てた計画に沿って実行させるPLAN MODEとは何かPLAN MODEは、Claude Codeを読み取り専用の調査・計画フェーズに制限するモードです。ファイルを読み、コードベースを検索し、質問をし、計画を立てることができます。しかし、ファイルの編集、コマンドの実行、外部への変更は一切できません。Plan Mode instructs Claude to create a plan by analyzing the codebase with read-only operations, perfect for exploring codebases, planning complex changes, or reviewing code safely.使えるToolと使えないTool 使えるTool（読み取り専用）       使えないTool（変更操作）         Read - ファイル閲覧              Edit/MultiEdit - ファイル編集    LS - ディレクトリ一覧            Write - ファイル作成             Glob - ファイルパターン検索      Bash - コマンド実行              Grep - コンテンツ検索            NotebookEdit - ノートブック編集  Task - リサーチエージェント      状態を変更するMCPツール          TodoRead/TodoWrite - タスク管理                                   WebFetch/WebSearch - Web調査                                     この制約が「安全な探索」を可能にします。 通常モードだとClaudeが「ついでにここ直しておきますね」と勝手に変更を加えることがある。PLAN MODEならそれが物理的にできない。安心してコードベースの深層を探索させられます。なぜ「分離」が重要なのかAIコーディングの失敗の大半は、「考えながら手を動かす」ことに起因する。 これは私の実感です。ペアプログラミングにドライバーとナビゲーターの分離があるように、思考と実行は本来別のプロセスです。一人の人間が同時にやるとミスが増える。LLMはさらに顕著で、「これ直しましょう」と「全体の整合性は大丈夫か」を同時に処理させると、ほぼ確実にどちらかが疎かになる。私はこれを「フクロウを描け」問題と呼んでいます。ステップ1：丸を描く。ステップ2：フクロウの残りを描く。巨大で曖昧な指示は人間にとっても困難なのだから、AIにとってはなおさらです。冒頭のOAuth2の事故がまさにそうだった。「OAuth2対応して」はフクロウを描けと言っているのと同じです。構想と施工を一つの息でやらせない。設計図を引く工程と、釘を打つ工程は、別のセッションに分離する。PLAN MODEはこの分離を物理的に強制する仕組みです。——と書いて、立ち止まる。「まだ書かないで」と自然言語で指示すればいいのでは？ 実はそれも試しました。結果、Claudeは3回に1回くらい従わない。計画の途中で「ここは明らかなので修正しておきますね」と手を動かし始める。PLAN MODEの価値は、この「うっかり実装を始める」を物理的に不可能にする点にあります。Tool自体が無効化されている。プロンプトの曖昧さとは次元が違う確実性です。ガードレールが未整備な段階では、この分離がさらに重要になります。 CLAUDE.mdもRulesもない状態でClaudeに自由に実装させると、出力は制御不能になる。計画段階で人間がレビューし方向性を確認すること。それが、未整備な環境における唯一の品質保証手段です。基本的な使い方Shift+Tabで切り替えるNormal Mode → Auto-Accept Mode → Plan Mode → Normal Mode → ...Shift+Tabを1回押すと「Auto-Accept Mode」（⏵⏵ accept edits on）。もう1回押すとPLAN MODE（⏸ plan mode on）。さらにもう1回でNormal Modeに戻ります。セッション開始時からPLAN MODEで始めるclaude --permission-mode planヘッドレスモードでの利用claude --permission-mode plan -p "認証システムを分析して改善点を提案して"デフォルトをPLAN MODEに設定する// .claude/settings.json{  "permissions": {    "defaultMode": "plan"  }}Ctrl+Gで計画を編集するPLAN MODEの隠れた強力機能です。Claudeが計画を提示したあと、Ctrl+Gを押すとデフォルトのテキストエディタで計画ファイルが開きます。AIが作った計画を人間が手で修正できる。この双方向性がPLAN MODEの真価です。推奨ワークフロー：調査 → 計画 → 実装ここから紹介するワークフローは、Anthropicの公式ベストプラクティスをベースに、私自身がPLAN MODEを使い込む中で形になったものです。核心は単純です。コードを書く前に、必ず承認済みの計画書を用意する。 言ってしまえば当たり前のことですが、AIの実行速度が上がるほど、この「当たり前」を飛ばしたくなる。飛ばした結果が、冒頭のOAuth2の事故です。Phase 1: 調査（Research）PLAN MODEに入って、コードベースを徹底的に調査します。> src/authを深く読み込んで、セッション管理とログインの仕組みを徹底的に理解して。> 環境変数によるシークレット管理の方法も調べて。> 調査結果をresearch.mdに書き出して。ここでの言葉遣いが重要です。「深く」「徹底的に」「詳細に」。これはただの修飾語ではない。Claudeの調査深度を決定するパラメータです。 省略すると、Claudeは関数のシグネチャだけ見て「理解した」と判断する。人間のエンジニアでいえば、READMEだけ読んで「把握しました」と言うようなものです。表面的な理解は、無理解より危険です。 「分かったつもり」の状態で書かれたコードは、正しく見えるだけに発見が遅れる。調査結果をresearch.mdとして永続化させるのも、私が習慣にしていることです。チャットの中の口頭説明は流れていく。ファイルに書かせれば、Claudeの理解度を可視化できる。理解が間違っていれば、計画に入る前に軌道修正できます。AIコーディングで最もコストの高い失敗は、バグではありません。「既存システムを無視した実装」です。 既存のキャッシュレイヤーを無視した関数。ORMの規約を無視したマイグレーション。既にあるロジックの重複実装。冒頭で触れたOAuth2の事故も、根本はこれでした。既存のセッション管理を理解せずに上書きした。調査フェーズは、この種の構造的な失敗を防ぐための投資です。Phase 2: 計画（Plan）調査結果をもとに、詳細な実装計画を作成させます。> Google OAuthを追加したい。調査結果をもとに、> 実装方法を詳細にまとめたplan.mdを作成して。> コードスニペットとファイルパスを含めること。まだ実装しないで。「まだ実装しないで」は必須のガードレールです。 これがないと、Claudeは計画が「十分良い」と判断した瞬間にコードを書き始めます。Phase 2.5: アノテーションサイクル（計画の研磨）ここが本記事で最も伝えたい点です。 計画をClaudeに書かせ、それを自分のエディタで「赤入れ」する。このアノテーションサイクルが、私がClaude Codeの運用で最も効果を実感した手法です。一見すると二重作業に見えます。Claudeに計画を書かせて、自分でも同じ計画を読み込んで修正する。しかし、この「自分の仕事を写経させる」苦痛こそが、エージェントの能力境界を自分の身体感覚として刻み込むための最速のコストです。他人のブログを読んで「PLAN MODEが良いらしい」と知るのと、自分でアノテーションを3回繰り返して「ここはClaudeに任せられる、ここは任せられない」を体で覚えるのとでは、得られる知識の質がまったく違う。Claudeがplan.mdを書いたら、自分のエディタで開いて、インラインでメモを書き込みます。## 認証フロー- OAuth2のコールバックURLを設定 [注: ステージングと本番で別URLにすること]- セッショントークンを生成 [注: 既存のJWT生成ロジックを再利用。新しく作らない]- ユーザーテーブルにOAuthプロバイダーカラムを追加 [注: PATCHマイグレーションで。テーブル再作成はNG]メモの粒度は様々です。2語で済むこともあれば、パラグラフ丸ごと書くこともある。ドメイン固有の知識、設計判断の根拠、「これはやるな」という明示的な拒否。Claudeが知り得ない文脈を、計画書の正確な位置に注入する。 これはコードレビューのコメントに近い感覚です。ただし、レビュー対象がコードではなく計画書である、という点が決定的に違います。> plan.mdにメモを追加した。すべてのメモに対応してドキュメントを更新して。> まだ実装しないで。このサイクルを1〜6回繰り返します。 毎回「まだ実装しないで」を付けるのが鉄則です。人間の同僚に「計画だけ書いて。実装はまだ」を6回繰り返したら、さすがに関係が悪化します。しかしコーディングエージェントは疲れないし、不満も持たない。納得いくまで計画を練り直させられるのは、人間相手にはない非対称な利点です。遠慮なく使い倒していい。なぜこれが強力なのか。 plan.mdは自分とClaudeの間の「共有されたミュータブルステート」です。チャットで方針を伝えると、過去の発言は流れていく。しかし計画書なら全体を俯瞰でき、問題箇所をピンポイントで修正できる。私の経験では、3回のアノテーションサイクルで、Claudeが書いた汎用的な計画が「自分のシステムの文脈に完全にフィットする設計書」に変わります。ここでもRulesの整備が効いてきます。セキュリティルールが読み込まれていれば、Claudeは計画段階から入力バリデーションやパラメータ化クエリの使用を考慮に入れる。ルールが未整備だと、この種の観点を人間が毎回アノテーションで指摘するしかない。Rulesの整備度は、アノテーションサイクルの回数に反比例する。 設定への投資が、ここでも回収されます。Phase 2.75: Todoリスト化計画の内容が固まったら、次は「実装中に迷子にならない」ための準備です。> 計画に詳細なTodoリストを追加して。全フェーズと個別タスクを含めること。> まだ実装しないで。計画書とTodoリストは役割が違います。計画書は「何をどう作るか」の設計図。Todoリストは「今どこにいるか」の現在地表示です。Claudeはタスク完了時にチェックを入れるので、数時間のセッションでも進捗が可視化される。長いセッションで最も怖いのは、Claudeが途中で方向を見失うことです。 Todoリストはその防波堤になります。Phase 3: 実装（Implement）計画に納得したら、モードを切り替えて実装させます。ここで私が最近採用しているのが、--dangerously-skip-permissionsとの組み合わせです。claude --dangerously-skip-permissions> plan.mdの計画を実装して。完了したタスクは随時チェックを入れて。> すべてのタスクが完了するまで止まらないで。> cargo fmt && cargo clippy -- -D warnings && cargo testを継続的に実行して。なぜここで--dangerously-skip-permissionsを使うのか。 計画フェーズですべての創造的な判断は済んでいるからです。実装は退屈であるべきだ。 これは私が最近強く実感していることです。計画書に従って機械的にコードを書くフェーズで、mkdirの確認を一つ一つ承認するのは、集中力の浪費でしかない。もう一つ、発想の転換があります。「エージェントで時短する」——多くの人がこう考えます。私も最初はそう思っていた。しかし実際にYOLO実行を組み込んでみて気づいたのは、奪うべきは他人の時間ではなく「自分の不在時間」だということです。自分が寝ている間、コーヒーを飲んでいる間、散歩に出ている間——その「ゼロ生産性」の時間帯を正の値にする。これは「速くやる」よりも摩擦が少ない。YOLO実行が最も力を発揮するのは、人間がモニターの前に座っていない時間帯です。Gitチェックポイントを打ち、計画書を渡し、席を立つ。戻ってきたらdiffを確認する。——と書いて、少し躊躇する。実装中に「あれ、この型だとエラーパスが3つ増えるぞ」と気づくことがある。計画段階では見えなかった問題が、コードを書く手を通じて初めて表面化する。完全な退屈は達成できない。しかし、退屈に近づくほど、実装の品質は安定する。もし実装中に「創造的な判断」が頻発しているなら、それは計画が不十分だったということです。計画に戻るべきであって、実装を続けるべきではない。ただし、順番を間違えてはいけません。 計画を研磨し、CLAUDE.mdとRulesで品質の枠組みを整えた上で、初めてこのアプローチが成立する。計画なし・設定なしのYOLOモードは、自律実行ではなくただの放任です。私のワークフロー全体像：整備 → Plan → YOLOPhase 0: 環境整備（一度だけ、以後メンテナンス）  ├── CLAUDE.md を50行以下に圧縮  ├── .claude/rules/ に関心事ごとのルールを配置  ├── .claude/skills/ にワークフローSkillを配置  └── .claude/settings.json にdenyルールを設定Phase 1: 調査（PLAN MODE）  └── コードベースを深く調査 → research.mdPhase 2: 計画（PLAN MODE）  └── plan.md 作成 → アノテーションサイクル × 1-6回 → Todoリスト化Phase 3: 実装（--dangerously-skip-permissions）  └── git checkpoint → 計画に基づいて自律実行Phase 4: 検証  └── diff確認 → テスト → 必要なら修正このワークフローで見落とされがちなのは、本当のボトルネックが「エージェントの実行速度」ではないことです。「常にエージェントを回す」は手段であって目的ではない。ボトルネックは「委任可能な良質タスクの在庫管理」にあります。仕事を適切な粒度に分解し、優先順位をつけ、パイプラインとして流し続ける能力。Phase 1の調査とPhase 2の計画が重要なのは、まさにこの「タスクの仕込み」を行う工程だからです。考えてみれば、これはAI以前から優れたエンジニアの条件でした。チケットの切り方がうまい人は、昔からチームの生産性を何倍にもしていた。Phase 0の詳細：denyルールで致命的操作を防ぐ--dangerously-skip-permissionsを使う場合でも、.claude/settings.jsonで特定の操作を禁止できます。{  "permissions": {    "allow": [      "Read",      "Edit",      "Write",      "Grep",      "Glob",      "Bash(cargo *)",      "Bash(git add *)",      "Bash(git commit *)",      "Bash(mkdir *)"    ],    "deny": ["Bash(rm -rf *)", "Bash(git push *)", "Bash(curl *)"]  }}rm -rf、git push、curlを拒否。cargo系やGitの基本操作は許可。--dangerously-skip-permissionsの利便性を保ちつつ、致命的な操作を防ぐバランスです。Phase 3の実践：安全なYOLO実行1. Gitチェックポイントは絶対git add -A && git commit -m "checkpoint: before YOLO implementation"何が起きてもgit reset --hard HEADで戻せる状態にしておく。2. 実装指示は具体的にYOLO実行時に私が毎回含めるキーフレーズは5つです。「plan.mdの計画を実装して」: 計画書を実装の唯一の根拠にする。計画にないことはやらせない「すべて実装して」: 全タスクを実行させる。つまみ食いさせない「完了したタスクにチェックを入れて」: plan.mdを進捗トラッカーとして機能させる。どこまで終わったか可視化する「すべてのタスクが完了するまで止まらないで」: 途中で確認を求めさせない。最後まで走り切らせる「[テストコマンド]を継続的に実行して」: 壊れたら即座に気づかせる。テストが落ちたまま次に進ませない3. 隔離環境の推奨docker run -it --rm -v $(pwd):/workspace -w /workspace \  --network none \  claude-code:latest --dangerously-skip-permissions \  "plan.mdの計画を実装して"--network noneで外部接続を遮断。コンテナ内なら被害が限定されます。Phase 4: 実装中のフィードバック実装中のフィードバックについて。ここでの指示は極めて短くていい。 計画フェーズであれほど丁寧に文脈を書き込んだのは、この瞬間のためです。共有済みの計画書があるから、「あれと同じにして」の一言でClaudeは正確に意図を汲み取れる。計画フェーズの投資が、実装フェーズのコミュニケーションコストを劇的に下げる。 この因果関係を体感すると、計画に時間をかけることへの抵抗がなくなります。ただし、短い指示が万能かというと、そうでもない。以前、「設定ページはadminアプリに作るべき。移動して」と一言で済ませたら、Claudeはページを移動しつつルーティングの整合性を壊した。計画書には「adminアプリのルーティング構造」が書いてあったのに、短い指示がそのコンテキストを上書きしてしまった。短い指示が効くのは、計画書のコンテキストが十分に残っているときだけです。セッションが長くなってコンテキストが薄れてきたら、指示の粒度を上げるか、/clearして計画書を再読み込みさせる必要がある。> deduplicateByTitle関数が実装されていない> 設定ページはadminアプリに作るべき。移動して> このテーブルはusersテーブルと同じ見た目にして「あれと同じにして」が、ゼロから説明するより遥かに正確に伝わる。既存コードが最良のリファレンスです。方向性が完全に間違った場合は、修正ではなくリバートします。> 全部リバートした。リストビューをもっとシンプルにしたい。それだけ。他は触らないで。悪い実装を段階的にパッチするのは、方向が間違った旅を微修正し続けるようなものです。git reset --hardで出発地に戻り、正しい地図を持って再出発した方が早い。「やり直す判断」は「直し続ける判断」より難しい。しかし、ほぼ常に正しい。 これはソフトウェア開発に限った話ではありません。PLAN MODEが特に効果的な場面1. 複数ファイルにまたがる変更4ディレクトリに及ぶ変更を無計画に始めると、最初のファイルを変更している間に全体の整合性を見失います。これは人間のエンジニアでも起きる問題ですが、LLMは「目の前のファイル」に意識が引きずられやすい分、さらに深刻です。全体の変更計画を先に作ることで、木を見て森を見失う事態を防げます。2. 不慣れなコードベースの調査新しいプロジェクトにClaude Codeを投入するとき、最も怖いのは「善意の破壊」です。Claudeが「ここ直しておきますね」と既存の設計意図を無視した変更を加える。PLAN MODEなら、理解が不十分な段階で手を出すことが物理的にできない。CLAUDE.mdもRulesも未整備の段階では、このモードが事実上の必須です。3. アーキテクチャの検討「マイクロサービスに分割すべきか」「DBを分離すべきか」。この種の判断は、実装した後に間違いに気づいても手遅れです。取り返しのつかない判断ほど、実行前の検討に時間をかけるべきです。 当たり前のことですが、AIの実行速度が上がるほど、この当たり前を飛ばしたくなる。速く動けることと、正しい方向に動けることは、別の能力です。4. コードレビューPRのレビューをClaudeに任せるとき、PLAN MODEなら「レビューコメントの生成」だけに留められます。レビューと修正を同時にやらせると、レビューの客観性が失われる。自分が書いたコードを自分でレビューしても、欠陥は見つからない。 これはClaudeも同じです。批評者と実装者は分ける。人間の組織でもそうするように。5. CLAUDE.mdのリファクタリング自体前述したCLAUDE.mdのリファクタリングプロンプトが、まさにPLAN MODEの実践例です。「まず計画を見せて」で計画を先に提示させ、確認してから実行する。設定の整備にPLAN MODEを使い、整備された設定がPLAN MODEの精度を上げる。好循環の入り口がここにあります。使わないほうがいい場面Anthropicの公式ベストプラクティスが明確に述べています。Plan Mode is useful, but also adds overhead. For tasks where the scope is clear and the fix is small (like fixing a typo, adding a log line, or renaming a variable), ask Claude to do it directly.ここで強調しておきたいのは、「使わない」という判断こそ最も高度なAIスキルであるということです。失敗するとわかっているタスクにエージェントを投入するのは、時間を燃やしているだけでなく、道具への信頼感を自ら毀損する行為でもある。「PLAN MODEで計画を立てたが、計画自体が意味をなさなかった」という経験は、次回以降PLAN MODEを使うモチベーションを確実に削る。道具への正確な不信は、盲目的な信頼より遥かに生産的です。判断基準: 差分を1文で説明できるなら、計画は不要。 PLAN MODE推奨                   直接実行推奨          複数ファイルのリファクタリング  タイポの修正          新機能の実装                    ログ行の追加          アーキテクチャの変更            変数のリネーム        不慣れなコードの変更            単一ファイルの小修正 --dangerously-skip-permissionsの使い分けも同様に重要です。 YOLO推奨              YOLO非推奨                計画済みの機能実装    本番環境の操作            リンターの一括修正    CLAUDE.md未整備の段階     ボイラープレート生成  信頼できないコードの実行 設定の整備度と自律性の段階ここまで読んで、一つの問いが浮かぶかもしれません。「で、結局どのモードを使えばいいのか」。答えは、今の設定の整備度による。設定の整備度         推奨モード──────────────────────────────────未整備               Normal Mode（全承認）  ↓                   + PLAN MODE で計画を人間が確認CLAUDE.md整備済み     Auto-Accept Mode  ↓                   + PLAN MODE で計画を立ててからRules整備済み         --dangerously-skip-permissions  ↓                   + PLAN MODE → deny ルール → YOLOSkills整備済み        完全自律実行──────────────────────────────────下に行くほど、Claudeへの信頼度が高くなり、任せられる範囲が広がる。 私自身の経験を振り返ると、この階段を1段ずつ登っていた。最初は毎回diffを目を皿にして確認していた。CLAUDE.mdを整備して1週間。Claudeの出力がルールを破らなくなった。その頃からAuto-Acceptが自然になった。さらにRulesを配置して2週間。plan.mdを渡せば計画通りに実装してくれることが分かった。初めて--dangerously-skip-permissionsを使ったとき、不安と解放感が同居していた。信頼は、検証の積み重ねの上にしか成立しない。 いきなりYOLOモードに飛ぶのは、初対面の人間にプロダクションのrootアクセスを渡すようなものです。前回の記事のAgent Skillsとの関係で整理すると： 概念          Skills            PLAN MODE             Rules                 パーミッション設定  制御するもの  知識              思考                  行動制約              自律性              問い          何を知っているか  どう考えるか          何をしてはいけないか  どこまで任せるか    効果          出力の「質」向上  出力の「方向性」制御  出力の「安全性」確保  実行の「効率」向上 この4つが揃って初めて、信頼できる自律的な出力が得られます。 逆に言えば、どれか1つが欠けている状態でYOLOモードに突入するのは、安全装置を1つ外したまま機械を動かすのと同じです。動くかもしれない。しかし、何かが起きたとき止められない。PLAN MODEの限界と現実万能な道具はありません。PLAN MODEにも、使い込んで初めて見えてくる限界があります。計画と実行の乖離PLAN MODEで練り上げた計画でも、実装フェーズで計画通りにいかないことがある。LLMは確率的な生成モデルなので、同じ指示でも出力が揺れる。計画段階では「Aの方法で」と決めたのに、実装中に「Bの方が良さそうですね」と方針を変える。悪気はない。ただ、その時点のコンテキストで最適と判断しただけです。対策は明確です。 plan.mdとして計画を外部化し、「plan.mdに従って実装せよ」と指示する。Todoリストに落とし込めば、さらに逸脱しにくくなる。計画が「頭の中」ではなく「ファイル」にあることが重要です。コンテキストの消費調査・計画フェーズ自体がコンテキストを消費する。これは避けられないトレードオフです。@でファイルを直接参照してトークンを節約する、/clearで新鮮なコンテキストから実装に入る、などの対策はあります。ただし、私の実感では、調査・計画・実装を単一の長いセッションで通しても、50%超のコンテキスト消費でパフォーマンスが目に見えて劣化することは少ない。plan.mdがauto-compaction後も参照可能であることが大きい。計画の質が高ければ、コンテキスト消費のコストは十分にペイします。--dangerously-skip-permissionsの現実的なリスク実際に踏んだ地雷を共有します。Claudeがrmで想定外のファイルを削除した。 冷や汗が出た。→ denyルールでBash(rm -rf *)を禁止テストが落ちているのに次のタスクに進んだ。 Claudeは「後で直します」と言って直さない。→ CLAUDE.mdに「テストが落ちたら修正してから次に進むこと」を明記計画にない「改善」を勝手にやり始めた。 善意のリファクタリングが一番厄介。→ 「plan.mdにないタスクは実行しない」を指示に含めるすべてGitチェックポイントから回復できた。 これが唯一の救いでした。git reset --hardで戻せない環境でYOLOモードを使うべきではない。これは教訓ではなく、絶対条件です。使い込んで見えてきたこと重要度順に並べます。10個あるが、1番だけ覚えて帰ってくれればいい。1. 土台を先に作る土台のない自律は、ただの暴走だ。 PLAN MODEも--dangerously-skip-permissionsも、CLAUDE.mdとRulesという土台の上に成り立つ機能です。この順番を間違えると、すべてが裏目に出る。最初に設定を整備し、次にPLAN MODEで計画を立て、最後にYOLOで実行する。逆順は存在しません。ただし、土台を整備しても暴走は起きた。前述の「善意のリファクタリング」がそうです。土台があれば暴走しないのではなく、暴走を事後に検出できる。検出できるだけだ。止められるかどうかは、結局のところ人間がdiffを読む体力に依存している。2. 「調査 → 計画 → 実装」を習慣にする複数ファイルに影響する変更なら、必ず計画から始める。最初は「面倒だ、直接書いた方が早い」と感じます。私もそう思っていました。しかし手戻りの回数を数えてみると、計画を立てた方が結果的に速い。人間が何十年も前に学んだはずの教訓を、新しい道具を手にするたびに忘れる。私も忘れた。3回やり直して初めて思い出しました。道具の価値は「不快の谷」の向こう側にしかない。 既に機能している自分のやり方を一時的に壊す覚悟がなければ、どんなツールも「微妙に不便な既存ワークフローの劣化版」で終わります。PLAN MODEも例外ではない。最初の1週間は確実に生産性が下がる。計画を書く時間がかかるし、アノテーションの勘所も分からない。しかし、この不快の谷を超えた先に、手戻りゼロの実装フェーズが待っている。習熟曲線の手前で撤退した人間の「PLAN MODEは面倒なだけ」という評価は、常に不正確です。3. 調査では「深さ」を明示的に要求する「深く」「徹底的に」がないと、Claudeは表面的な理解で「把握しました」と言います。人間のエンジニアに「ちゃんと読んだ？」と聞くのと同じ効果があります。4. 「まだ実装しないで」を毎回付ける計画フェーズのすべての指示の末尾に付ける。冗長に感じても、省略するとClaudeは実装を始めます。5. 計画は必ず外部ファイルに残すplan.mdとして永続化し、アノテーションサイクルでブラッシュアップする。チャットの中の計画は流れていく。ファイルに残った計画だけが、実装の羅針盤になる。6. YOLOモードはGitチェックポイントとセットでgit add -A && git commit -m "checkpoint: before YOLO implementation"これは「やった方がいい」ではなく「やらなければ使ってはいけない」です。7. denyルールで取り返しのつかない操作を防ぐrm -rf、git push、curl。YOLOモードでも、これらだけは人間の判断を経るべきです。最近のモデルとエージェントはそういう異常行動がかなり減ったので安心してみている。8. ルールは関心事で分離し、パスでスコープするすべてのルールをCLAUDE.mdに詰め込まない。セキュリティ、テスト、Git規約、パフォーマンス——関心事ごとに.claude/rules/に分配する。さらに、paths: frontmatterで条件付きロードを設定すれば、Rustのソースコードを触るときだけセキュリティルールが読み込まれ、テストファイルを触るときだけテスト規約が読み込まれる。ルールが1ファイルに混在していると、Claudeにとっても人間にとっても見通しが悪くなります。9. Extended Thinkingと組み合わせるPLAN MODEで「セキュリティの影響について深く考えて」のように深い思考を促すと、計画の質が上がります。考える時間を与えれば、Claudeも考えます。10. 曖昧さを意図的に使う場面も知る「このファイルで何を改善できる？」とオープンに聞くことで、自分が見落としていた問題をClaudeが指摘してくれることがある。常に具体的な指示が最善とは限りません。よくある失敗と対策 問題                  原因                      対策                                      計画が抽象的すぎる    調査フェーズが不十分      @で具体的なファイルを指定して読ませる   実装が計画から逸脱    コンテキストが薄れている  plan.mdをTodoリストに落とし込む           CLAUDE.mdが肥大化     整理していない            50行以下にリファクタリング。Rulesに分配   Rulesが効いていない   配置場所が間違っている    /memoryでロード状況を確認               計画に時間をかけすぎ  分析麻痺                  アノテーションは最大6回で打ち切る         YOLOで想定外の変更    ガードレール不足          CLAUDE.md + Rules + denyルール整備        YOLOでテスト未実行    指示が不十分              「テストを継続的に実行して」を指示に含める  YOLOでファイル削除    deny設定漏れ              Bash(rm -rf *)をdenyに追加              Rulesを無視した出力    Rulesは強制力がない        Linter + CIで機械的に検証する            まとめ本記事で扱った機能を改めて並べます。CLAUDE.md: プロジェクトのグローバルな文脈（50行以下に圧縮）Rules: 関心事ごとのルール（.claude/rules/に分配）commands: 手動ショートカットHooks: 自動実行Subagents: 専門家の自動呼び出しSkills: 専門知識の注入PLAN MODE: 思考と実行の分離 ← 今回パーミッション設定: 自律性のコントロール ← 今回機能は増えました。しかし、核心は3つに集約されます。1. 土台のない自律は、ただの暴走だ。 CLAUDE.mdを50行以下に圧縮し、関心事ごとのルールは.claude/rules/に分配する。ルールの品質がそのまま自律実行の安全性になる。整備が不十分なら、PLAN MODEで人間が毎回レビューするしかない。整備すれば、その手間が消える。設定への投資は、将来の自分の時間を買うことです。2. 思考と実行は、混ぜるな。 PLAN MODEで調査し、計画を書き、アノテーションで研磨し、承認してから実装に入る。この順番を崩すと、出力が制御不能になる。実装中に「創造的な判断」が発生したら、それは計画の不備です。実装を止めて計画に戻る。実装は退屈であるべきだ。 ただし、本文で書いた通り、完全な退屈は達成できない。型を書く手が新しい問題を発見することがある。思考と実行を完全に分離することは、原理的にできない。できないが、分離しようとする努力に意味がある。3. 信頼は、検証の積み重ねでしか成立しない。 Normal Mode → Auto-Accept → PLAN MODE → --dangerously-skip-permissions。いきなりYOLOモードに飛ぶのは、初対面の人間にプロダクションのrootアクセスを渡すのと同じです。設定を整備し、出力を確認し、また整備する。信頼は与えるものではなく、積み上げるものです。そして、一つだけ付け加えます。委任はスキルの「局所的な萎縮」です。 手放したタスクの筋肉は確実に衰える。PLAN MODEで計画を立て、YOLO実行で実装を任せる。このワークフローを回すほど、自分の手でコードを書く時間は減る。それを許容するなら、自分が手動で残すタスクの選択は「何のエンジニアでありたいか」という自己定義の問題になります。特に基礎が未形成な段階では、楽をすることが将来の天井を決めてしまう。私はアーキテクチャの設計と、Rulesの整備と、diffの最終レビューだけは自分の手で続けると決めている。それ以外は——手放す覚悟を、毎回確認しています。Anthropicの公式ベストプラクティスの結びの言葉が、これを的確に表現しています。The patterns in this guide aren't set in stone. Pay attention to what works. When Claude produces great output, notice what you did. When Claude struggles, ask why.冒頭で触れたOAuth2のリファクタリング。今なら、まずPLAN MODEで既存のセッション管理を深く調査し、plan.mdを書き、アノテーションを2回重ねてから実装に入る。あの3回のやり直しは、おそらく起きない。Claude Codeが雑魚なんじゃない。設定してないだけです。CLAUDE.mdを整備し、Rulesを配置し、Linterで品質を機械的に担保し、計画を立て、適切なモードで動かす。そこまでやって初めて、Claudeは最後までやり切ってくれます。ただ、どこまで設定すれば「十分」なのかは、正直まだ分からない。先週もRulesを1つ追加した。来週も何か足すだろう。この整備はいつ終わるのか。たぶん、終わらない。今日から試せること1. CLAUDE.mdのリファクタリング（15分）Refactor @CLAUDE.md to minimize startup context.Extract path-specific rules to .claude/rules/ (use paths: frontmatter for conditional loading),multi-step workflows to .claude/skills/<skill-name>/SKILL.md (with name/description frontmatter),reusable slash commands to .claude/commands/,and specialized task delegation to .claude/agents/<agent>.md (with name/description/tools frontmatter).Show extraction plan first, then create files,reduce CLAUDE.md to <50 lines keeping only universal project context,report before/after token metrics.これ自体がPLAN MODEの実践例です。「まず計画を見せて」で計画を確認してから実行する。2. Shift+Tab を2回押してPLAN MODEを体験する（30秒）⏸ plan mode on と表示されたら成功。「このプロジェクトの全体像を教えて」と聞いてみてください。3. 調査 → 計画 → 実装を1回通す（30分）# Phase 1: PLAN MODEで調査> src/[対象]を深く読み込んで。調査結果をresearch.mdに書いて。# Phase 2: 計画> [やりたいこと]の詳細なplan.mdを作成して。まだ実装しないで。# Phase 3: アノテーション> plan.mdにメモを追加した。すべてのメモに対応して。まだ実装しないで。# Phase 4: 実装> plan.mdの計画を実装して。完了したタスクにはチェックを入れて。4. denyルールを設定してからYOLOモードを試す（10分）git add -A && git commit -m "checkpoint"claude --dangerously-skip-permissions> plan.mdの計画を実装して参考資料Best Practices - Claude Code DocsCommon Workflows - Claude Code DocsConfigure Permissions - Claude Code DocsManage Memory - Claude Code DocsHow Claude Code Works - Claude Code DocsClaude Code Best Practices: The Plan Mode - Code CentreWhat Actually Is Claude Code's Plan Mode? - Armin RonacherCLAUDE.md Mastery - ClaudeFastRules Directory Guide - ClaudeFastClaude Code Best Practices (Community)ClaudeLog - Plan ModeClaudeLog - Dangerous Skip Permissions]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KubeSploit でシミュレートできる攻撃とその防御策]]></title>
            <link>https://sreake.com/blog/kubesploit-threat-simulation-and-defense-measures/</link>
            <guid isPermaLink="false">https://sreake.com/blog/kubesploit-threat-simulation-and-defense-measures/</guid>
            <pubDate>Thu, 12 Feb 2026 23:00:00 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 期限付きインターンをしております、菅家と申します。この度はKubesploitというペネトレーションツールについて調べましたので、その結果をまとめました。実際に攻撃するツールですので許可された環境以外では実施し […]The post KubeSploit でシミュレートできる攻撃とその防御策 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[コンテナセキュリティの最新事情 ~ 2026年版 ~]]></title>
            <link>https://speakerdeck.com/kyohmizu/kontenasekiyuriteinozui-xin-shi-qing-2026nian-ban</link>
            <guid isPermaLink="false">https://speakerdeck.com/kyohmizu/kontenasekiyuriteinozui-xin-shi-qing-2026nian-ban</guid>
            <pubDate>Thu, 12 Feb 2026 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[イベント登壇資料です。 2026/2/12  SCSK Sysdig Bootcamp]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud Speech-to-Text 精度検証：最新モデル「Chirp3」の実力に迫る]]></title>
            <link>https://sreake.com/blog/accuracy-verification-cloud-speech-to-text-chirp3/</link>
            <guid isPermaLink="false">https://sreake.com/blog/accuracy-verification-cloud-speech-to-text-chirp3/</guid>
            <pubDate>Thu, 12 Feb 2026 04:30:28 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 音声認識技術は、私たちの生活やビジネスにおいて欠かせないものとなりつつあります。議事録の自動作成、コールセンターの応対分析、多言語翻訳など、その活用範囲は広がる一方です。 本記事では、Google Cloudが […]The post Cloud Speech-to-Text 精度検証：最新モデル「Chirp3」の実力に迫る first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KyvernoのCI実装（Kyverno CLI）]]></title>
            <link>https://sreake.com/blog/kyverno-ci-implementation/</link>
            <guid isPermaLink="false">https://sreake.com/blog/kyverno-ci-implementation/</guid>
            <pubDate>Thu, 12 Feb 2026 04:28:55 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回は、Kubernetesのポリシー管理ツールであるKyvernoについて、公式が提供しているCLIツールを用いたCI（継続的インテグレーション）の実装を行いましたので、その知見を共有します。 ポリシーを本番 […]The post KyvernoのCI実装（Kyverno CLI） first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kyvernoで構築するKubernetesポリシー基盤]]></title>
            <link>https://sreake.com/blog/kyverno-kubernetes-policy-foundation/</link>
            <guid isPermaLink="false">https://sreake.com/blog/kyverno-kubernetes-policy-foundation/</guid>
            <pubDate>Thu, 12 Feb 2026 04:28:13 GMT</pubDate>
            <content:encoded><![CDATA[はじめに ここ数年、Kubernetesを活用したプラットフォーム運用が進むにつれて、各企業で必ず直面するのが「ガバナンス」の課題です。開発者には自由にリソースを作ってもらいたい一方で、セキュリティ基準を満たさない設定や […]The post Kyvernoで構築するKubernetesポリシー基盤 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[KEDAによるスパイク対策、HPA/KEDAの拡張メトリクスについて]]></title>
            <link>https://sreake.com/blog/keda-traffic-spike-measure/</link>
            <guid isPermaLink="false">https://sreake.com/blog/keda-traffic-spike-measure/</guid>
            <pubDate>Thu, 12 Feb 2026 04:27:24 GMT</pubDate>
            <content:encoded><![CDATA[本記事は、Kubernetes（以下、k8s）に関する基礎的な知識を有していることを前提としています。 また、KEDAの特徴の一つであるゼロスケーリングについては詳しく触れませんのでご了承ください。 はじめに 期限付きイ […]The post KEDAによるスパイク対策、HPA/KEDAの拡張メトリクスについて first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、方法を学べ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/09/180240</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/09/180240</guid>
            <pubDate>Mon, 09 Feb 2026 09:02:40 GMT</pubDate>
            <content:encoded><![CDATA[はじめにエディタを開いたまま、手が止まっていた。書くことは決まっている。決まっているのに、最初の一行が出てこない。内容が自分の中でまだ言葉になっていない。その感覚だけがある。カーソルが点滅している。点滅を眺めていた。「課題から入れ」という原則がある。手段から入るな、何を解くべきかを先に考えよ。正しいと思う。長い間、信じてきた。信じてきたのだが、ずっと聞きたかったことがある。課題って、どうやって見つけるんですか。誰にも聞けなかった。正確に言えば、聞いたことはある。返ってきたのは「現場を見ろ」「ユーザーの声を聞け」だった。目の前には日常があり、業務があり、なんとなく回っている世界がある。その中から「これが課題だ」と名指しすることが、そもそもできない。見つけ方がわからないまま、ただ座っていた。座っている自分に苛立っていた。苛立っていることにも苛立っていた。後になって気づいた。「課題から入れ」と言える人は、自分の中で自動的に動いている診断プロセスを省略している。「どこを見るか」「何を測るか」「どの違和感を拾うか」が内面化されていて、課題が「見える」のであって「見つける」ものではない。だから見つけ方を聞かれても言語化できない。この原則が有効なのは、すでにその領域の語彙と観測手段を持っている人——つまり方法を持っている人——だけだ。方法を持たない人にとっては、「課題から入れ」は呪いになる。課題が見えないとき、人は自分の能力を疑う。怠惰だから見えないのだ、センスがないのだ、と。能力の問題ではなかった。視点の問題だった。と、今なら書ける。当時はそれすらわからなかった。わかりたくなかったのかもしれない。わからないことにしておいた方が、楽だったのかもしれない。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。この記事における「方法」の定義「方法を学べ」と言いたい。しかし、「方法」が曖昧なまま語ると、読む人によって受け取り方がバラバラになる。過去に何度かそういう失敗をした。だから先に言葉を定義しておく。この記事で「方法」と呼ぶものは、問題を分解し、解決するための体系化された知識とアプローチだ。3つに分けて考えている。操作の知識: 道具をどう動かすか構造化の知識: 問題をどう分解するか文脈の知識: 何が重要かをどう判断するか言い換えれば、実行・分解・判断だ。「実行」は、手を動かして何かを作る能力だ。Pythonを書ける。Kubernetesを操作できる。プログラミング言語、フレームワーク、ツールの操作がここに入る。「分解」は、複雑な問題を扱える単位に切り分ける能力だ。「この問題はボトルネックの問題だ」「これはフィードバックループの問題だ」と構造を見抜ける。スクラム、リーン、アジャイル、システム思考といった方法論がここに入る。「判断」は、「この文脈では何が重要で何が重要でないか」を見極める能力だ。同じ技術でも、金融とゲームでは優先順位が違う。その違いを知っている。特定分野の概念、パターン、ベストプラクティスがここに入る。なぜ3つか。正直、2つでも4つでもよかった。ただ、「実行できるか」「分解できるか」「判断できるか」と自分に聞いてみたとき、この3つの問いで知識の穴を点検できた。それだけの話だ。3層の間には、依存関係がある。判断は、分解を前提とする。「何が重要か」を判断するには、まず問題を分解して構造を見る必要がある。構造が見えないまま「これが重要だ」と言っても、それは直感であって判断ではない。分解は、操作を前提とする。Kubernetesの問題を分解するには、Kubernetesを触った経験が必要だ。触ったことがないものを、構造的に分解することはできない。つまり、操作→分解→判断という順序がある。ただし、この順序は「学ぶ順序」ではない。「依存関係」だ。判断を学ぶためには、分解ができる必要がある。分解を学ぶためには、操作ができる必要がある。しかし、操作だけを完璧にしてから分解に進む必要はない。私自身、Terraformの操作コマンドを先に覚えてから設計原理を学ぼうとして、半年間terraform applyの繰り返しで過ごしたことがある。操作はできる。しかし、「なぜこのモジュール構成なのか」を分解できない。操作の習熟を待っていたら、分解の学びが永遠に始まらなかった。操作しながら分解を学び、分解しながら判断を学ぶ。3層は並行して育つ。待っていても、次の層は始まらない。「方法を学べ」と言うとき、私はこの3つすべてを含めている。どれか1つではない。技術スキルだけあっても方法論がなければ使いこなせない。方法論だけあっても領域知識がなければ適用できない。3つは相互に補完し合う。「3つで足りるのか」と聞かれるかもしれない。正直に言えば、「判断」の中には、純粋な技術知識では収まらないものがある。組織の力学、ステークホルダーの価値観、倫理的な判断。「技術的に正しい」と「組織として採用できる」は別の問いだ。セキュリティ上最適な設計が、ビジネスの速度を殺すことがある。アーキテクチャとして理想的な分割が、チームの政治的な境界と衝突することがある。この記事では「方法」を技術的な問題解決の文脈に限定して書いている。しかし、現場では判断の上にさらに「交渉」と「妥協」の層がある。それは技術の方法論ではなく、人間の方法論だ。3層で足りるかと問われれば、足りない。足りないことを知った上で、この記事では技術の話に集中する。3層それぞれの到達度は、こう測れる。操作は「説明できるか」ではなく「手が動くか」だ。分解は「構造を描けるか」だ。判断は「トレードオフを言えるか」だ。Rustを「知っている」と「Rustで考えられる」は違う。前者は操作の入口、後者は分解と判断を含んでいる。もっと言えば、「説明できる」「再現できる」「即応できる」の3段階がある。本を読んで説明できる段階は、まだ「情報」だ。手を動かして再現できる段階で「知識」になる。考えなくても即応できる段階で、ようやく「方法」になる。なぜこの定義が重要か。「方法」が曖昧だと、「方法を学ぶ」も曖昧になるからだ。「Pythonを勉強しました」と「システム思考を学びました」では、学んだ後の視野の広がり方がまったく異なる。前者は操作の知識、後者は構造化の知識だ。何を学ぶかによって、何が見えるようになるかが変わる。自分の知識を点検するとき、「操作できるか」「分解できるか」「判断できるか」と問うといい。3つすべてにYesと言えるなら、その領域の「方法」を持っている。どれかがNoなら、そこに学ぶべきものがある。だから「方法を学べ」と言うとき、私が意味しているのは「情報を集めろ」ではない。「考えなくても使えるレベルまで落とし込め」だ。Goを説明できることと、Goで考えなくてもコードが書けることは違う。後者にならなければ、言語の上にアーキテクチャや設計の思考を積むことができない。文法を考えている間は、アーキテクチャを考える余裕がない。考えなくていいことが増えると、考えるべきことに集中できる。では、この「方法の知識」がないとき、実際に何が起きるのか。障害の名前を知らなければ、落ちた理由がわからない本番障害が起きた。APIのレスポンスタイムが急激に悪化し、タイムアウトが連鎖して、最終的にサービス全体が応答不能になった。何かがまずかったのはわかる。でも、何がまずかったのかわからない。リクエストが多すぎたのか、サーバーのスペックが足りなかったのか、そもそもアーキテクチャが間違っていたのか。漠然とした「やられた」感だけがある。私は駆け出しだった頃、その違和感を言語化できなかった。「なんか落ちた」のまま、モヤモヤが残った。胃のあたりに不快感がある。何かが間違っている気がする。でも、何が間違っているのか指させない。ログを開く。全行が等しく意味不明に見える。どこが重要でどこが無関係なのか、判断する基準がない。Slackで「障害です」と報告する。「で、原因は？」と聞かれる。固まる。「課題が見えない」とは、こういう状態だ。怠惰ではない。目の前に情報はある。しかし、情報を「手がかり」に変換するフィルタがない。全部見えているのに、何も見えていない。当時の上司は「もっとちゃんと調べろ」と言った。「ちゃんと」が何を意味するかはわからなかった。端から端まで読んだ。読んだが、どの行が重要なのか判断できなかった。「課題が見えない」を「怠惰」や「能力不足」と誤認するのは、見えている側が陥る典型的な誤りだ。見える人にとっては「見ればわかる」。見えない人にとっては「見ても何もわからない」。同じログを見ていても、使っている眼鏡が違う。熟練のSREは違う。同じ障害を見ても、同じログを見ても、見ている場所が違う。まずエラーレートの変化点を見る。次にレイテンシの分布——平均ではなくP99を見る。「いつから」「どのエンドポイントで」「どのくらいの割合で」。数字を3つ確認した時点で、仮説が立っている。「キャッシュの有効期限が同時に切れて、バックエンドにリクエストが集中した。典型的なサンダリングハード問題だ」と言える。障害の瞬間を特定し、構造的に捉えられる。分散システムの障害には、パターンに名前がついている。キャッシュが一斉に失効してリクエストが殺到する「サンダリングハード」。障害が連鎖してコンポーネントが次々と倒れる「カスケード障害」。名前があると、対策の候補が自動的に浮かぶ。サンダリングハードならジッター（タイミングをランダムにずらすこと）やキャッシュウォーミング。カスケード障害ならサーキットブレーカーやタイムアウト設計。名前は、仮説→対策→再発防止という変換パイプラインのショートカットキーだ。障害パターンの名前を知らない私も「落ちた」ことは感じている。 しかし、それを「課題」として認識できない。これは能力の差ではない。語彙の差だ。ただし、語彙があれば万能かというと、そうでもない。名前を知っていても、目の前の障害がそのパターンかどうかを判別するには、観測の手段と経験が必要だ。語彙は必要条件であって十分条件ではない。逆に、語彙がなくても課題を当てられる人がいる。触れた回数が多い人だ。直感に見えるが、実態は暗黙のパターンマッチングだ。ただし、名前がないとチームに共有できない。語彙を増やす最短ルートは、本を読むこと、障害報告書を読むこと、他人の失敗を追体験すること。自分で全部失敗するには人生が短すぎる。しかし、名前が害になる場面もある。名前をつけた瞬間に、わかった気になる。目の前の障害が本当にそのパターンなのか、別の要因が重なっていないか。名前はショートカットであって、ゴールではない。名前に飛びつくと、名前に合わない症状を無視する。名前を知っていることと、名前に頼りすぎないことは、同時に必要だ。名前をつけるとは、違和感と課題認識の間にある溝を越える操作だ。3つのことが同時に起きる。第一に、バラバラだった症状が1つの構造として圧縮される。第二に、その構造を他の場面でも検索できるようになる。第三に、対策の候補が構造に紐づいて出てくる。1つの障害から学んだことが、10の障害に適用できる。それは名前という圧縮を経由しているからだ。ここにもう1つ、見過ごされがちな問題がある。名前を知らないということは、その方向に道が存在すること自体を知らないということだ。 AIに聞けば対策は教えてくれる。しかし、AIに聞くためには「キャッシュが同時に切れてリクエストが集中する現象は何ですか」と問う必要がある。この問いを立てるには、少なくとも「キャッシュの同時失効」が原因である可能性に気づいている必要がある。道の存在すら知らなければ、歩き出すことも、誰かに道を聞くこともできない。つまり、方法を知らない人間には、課題が見えない。見えないものを「解くべきだ」と言われても、困る。違和感の正体が見えない「違和感はあるのに、課題として認識できない」——これは障害対応に限った話ではない。日常の仕事でも頻繁に起こる。あるプロジェクトで、半年間「なんとなくうまくいかない」と感じていた。みんな忙しそうにしている。会議も多い。Slackは常に未読がある。でも、なんとなく進んでいない。違和感はある。ただ、何が問題なのかわからない。私は「コミュニケーションが足りないんじゃないか」と言った。なんとなくそれっぽかった。会議を増やした。Slackのチャンネルを増やした。改善しなかった。「コミュニケーション不足」は、catch (Exception e) {} だ。 エラーは握りつぶされる。原因はそのまま残る。TOC（制約理論——「全体の成果はもっとも弱い環が決める」という考え方）を学んだ後、同じ状況を見直した。ボトルネックが見えた。フロントエンドの実装が週に5件来るのに、デザインレビューは週1回。3日待ちが常態化していた。開発者は待っている間に別のタスクに手を出し、コンテキストスイッチが積み上がり、全体の生産性が下がっていた。これは「コミュニケーション不足」ではなく「フローの制約の問題」だった。制約理論を知らなかった半年間、私は会議を増やすことで問題を悪化させていた。会議が増えれば、デザイナーの時間はさらに減る。レビューの頻度はさらに下がる。「コミュニケーションを増やす」が「ボトルネックを悪化させる」に直結していたのに、構造が見えていなかった。なぜ人は構造ではなくラベルに逃げるのか。構造を見るには知識が要り、指摘すると責任が生じる。「デザインレビューが週1回しかない」と言えば「じゃあお前が変えろ」と返ってくる。「コミュニケーション不足ですね」なら、誰の問題でもない。ラベルは3秒で出せるが、構造の分析には時間がかかる。ラベルを構造に戻すには、具体的な問いを使う。「待ち時間はどのくらいか」「同時進行のタスクはいくつあるか」「どこで止まっているか」「誰が制約になっているか」。この4つを聞くだけで、「コミュニケーション不足」は分解される。ただし、正直に書く。「会議を増やす」が正解のケースもある。情報の非対称性が問題の本質であるとき、共有の場を増やすことは直接的な解になる。ラベルが正しいこともある。問題は、ラベルが正しいかどうかを検証せずに貼ることだ。同じ「なんとなくうまくいっていない」を感じていても、持っている方法の知識によって、見える課題がまったく異なる。方法が課題を照らす障害対応とプロジェクト運営の例を見てきた。どちらも、方法を知らなければ課題が見えなかった。ここで馬車と蒸気機関の話をしたくなる。「方法を知らなければ、馬をもっと速く走らせることしか考えられない」。よくある例だ。しかし、借り物の例は使わない。もう1つ、自分の経験で語る。インフラの監視を設計していたとき、「もっと見やすいダッシュボードを作ろう」が課題だと思っていた。Grafanaのパネルを増やし、配色を工夫し、アラートの閾値を調整した。「監視を改善する＝見せ方を良くする」だと思っていた。具体的に言う。あるAPIのレスポンスタイムが遅かった。Grafanaのダッシュボードには、平均レスポンスタイムのグラフが表示されている。「平均300ms」。遅い。しかし、「どこが遅いのか」はわからない。データベースのクエリが遅いのか、外部APIの呼び出しが詰まっているのか、単にネットワークのレイテンシが高いのか。ダッシュボードは集約された数値を見せてくれるが、数値の内訳は見えない。私はダッシュボードのパネルを増やした。CPU使用率、メモリ使用率、ディスクI/O、ネットワーク帯域。パネルが増えるたびに、何かがわかった気がした。実際には、何もわかっていなかった。見せるデータを増やしただけで、リクエストが「どこを通って、どこで詰まっているか」という根本的な問いには一切答えていなかった。ダッシュボードの配色に悩んでいた。正常は緑、警告は黄色、異常は赤。閾値をいくつにするか。300msで黄色にするか、500msにするか。今思えば、問いの立て方がそもそも間違っていた。「見せ方」のレイヤーで課題をいじっていた。計測していないものは、見せようがない。当たり前のことだが、当時の私にはその当たり前が見えていなかった。OpenTelemetry（システムの動作を計測・追跡するための標準規格）を学んだ後、課題設定が根本から変わった。分散トレーシングという概念を知った。1つのリクエストがシステムの中をどう流れ、どこで何ミリ秒かかっているかを、リクエスト単位で追跡できる。トレースを入れてみて初めて、「平均300ms」の内訳が見えた。認証サービスへの呼び出しで120ms、データベースのクエリで80ms、外部決済APIの応答待ちで90ms。残りの10msはネットワークとシリアライゼーション。ボトルネックは外部決済APIだった。しかもリトライが走っていた。私がダッシュボードの配色に悩んでいた間、答えはリクエストの中に隠れていた。必要だったのは「見せ方を工夫する」ことではなく、「見えていなかったものを見えるようにする」ことだった。計装（instrumentation）という発想がなければ、この課題設定にたどり着けない。この経験で学んだのは、課題が「どのレイヤーにあるか」を見誤ると、いくら努力しても解決しないということだ。私は表示のレイヤーで課題をいじっていた。本当の課題は計測のレイヤーにあった。計測のレイヤーの存在自体を知らなかったから、表示のレイヤーで頑張るしかなかった。知らない方法で解ける課題は、課題として認識できない。「そういうものだ」と受け入れてしまう。私にとって「平均レスポンスタイムしか見えない」は、監視とはそういうものだ、と思い込んでいた状態だった。方法の解像度が、課題の解像度を決める。 そして、自分の課題認識が「どのレイヤー」にあるかを見抜くこと自体が、方法の知識を必要とする。ここで1つ、疑問が浮かぶ。方法を学んだから課題が見えたのか。それとも、課題に困っていたから方法を学べたのか。正直に言えば、両方だ。OpenTelemetryを学んだのは、「平均レスポンスタイムしか見えない」という不満があったからだ。しかし、その不満を「これは計装の問題だ」と名づけられたのは、OpenTelemetryを知った後だ。鶏と卵に見えるが、実際には螺旋だ。違和感が方法を引き寄せ、方法が違和感を課題に変換し、課題が次の違和感を生む。そして、方法が照らすのは課題だけではない。「何が重要か」「何がリスクか」「何を先にやるべきか」優先順位の判断も、方法の知識で変わる。制約理論を知っていれば、「全体のスループットを最も制約しているのは何か」という問いが立つ。この問いがなければ、目についた問題から手当たり次第に着手する。忙しいが、成果が出ない。ただし、見えるようになった人が次に陥る罠がある。すべてが課題に見えてしまうことだ。方法の知識が増えると、今まで気にならなかったことが全部「改善すべき点」に見える。コードの結合度、デプロイパイプラインの無駄、チーム間のハンドオフの遅延。全部見える。全部直したくなる。しかし、全部直す時間はない。見えることと、今やるべきことは別だ。課題が見えすぎる人間は、改善中毒に陥る。これも、方法の目的化の一種かもしれない。そしてここに、もう1つの構造がある。ここまでの話を振り返ると、私がやっていることには共通のパターンがある。具体的な障害を経験する。そこからパターンの名前を知る。名前の背後にある構造を取り出す。すると、まったく別の場面でも同じ構造が見える。サンダリングハードの経験から「同時に多数のリクエストが1点に集中する」という構造を取り出した。すると、年末のセール開始時刻にアクセスが殺到する現象も、CI/CDパイプラインで全チームのビルドが同時にキューに入る問題も、同じ構造として見えるようになった。具体的な見た目はまったく違う。しかし、構造は同じだ。この運動を言語化するとこうなる。具体的な経験から、抽象的な構造を取り出す。取り出した構造で、別の具体を照らす。 制約理論を学んだときも同じ運動が起きた。「デザインレビューが週1回でボトルネックになっている」という具体から、「全体のスループットは最も制約された工程で決まる」という構造を取り出した。すると、別のプロジェクトで「QAチームのキャパシティが全体のリリース速度を制約している」と見えた。具体的な文脈はまったく異なる。しかし、構造を一度取り出してしまえば、それは1つの場面に留まらない。たとえば、「持ち家か賃貸か」という議論がある。不動産の話だ。しかし、一歩引いて考えると、これは「所有か利用か」という構造の問題だ。その構造が見えた瞬間、サーバーを自前で持つかクラウドを使うか、ライブラリを自作するかOSSを使うか、人を雇うか外注するか——まったく異なる文脈に同じ構造が適用できる。片方の文脈で得た判断基準が、もう片方でも使える。これが「1つを学んで10に適用する」のメカニズムだ。しかし、この変換は自動的には起きない。具体的な方法を学んだとき、そこから抽象的な構造を取り出すのは、自分自身だ。 本が教えてくれるのは具体的な事例とメカニズムだ。構造を見出すのは読者の仕事だ。USEメソッドを知った。「Utilization・Saturation・Errorsの順に点検する」——これは具体的な手順だ。ここから「問題を複数の独立した観点で順序立てて点検する」という構造を取り出さなければ、USEメソッドはパフォーマンス分析でしか使えない「手順書」のままだ。構造を取り出した人間は、障害対応でも、コードレビューでも、プロジェクトの健康診断でも、同じ型で思考できる。同じ本を読んでも、得られるものが10倍違う人がいる。違いは記憶力ではない。具体から構造を取り出し、別の具体に適用する往復運動を、意識的にやっているかどうかだ。私のやり方はこうだ。本を読んだあと、「この本が教えた手法を、一言で言うと何か」と自分に問う。一言にできたら、「同じ構造を持つ、まったく別の場面は何か」と考える。2つ目が見つかったとき、その手法は「手順書」から「思考の型」に昇格する。見つからなければ、まだ構造が取り出せていない。そしてここに、一方通行の性質がある。構造を取り出せる側にいる人間には、具体の世界も見える。しかし、具体しか見えない側にいる人間には、構造の世界は見えない。一度構造が見えてしまうと、「あのとき、なぜ気づかなかったのか」がわかる。しかし、構造が見える前の自分は、「何が見えていないか」すらわからなかった。だから「構造を取り出せ」と言われても、取り出した経験がない人には、何を言っているかわからない。「課題から入れ」と同じ構造がここにもある。見えている側が、見えていない側に「見ろ」と言っている。では、この一方通行をどう越えるか。1つだけ確実に効く方法がある。異なる文脈で同じ壁にぶつかることだ。Rustでデータ競合にぶつかる。Goでも似たようなバグに遭遇する。Pythonでもリストの共有状態で事故を起こす。3回目あたりで、「これは言語の問題ではなく、共有状態へのアクセス制御の問題だ」と気づく。具体的な失敗を3回繰り返すと、共通する構造が浮かび上がる。失敗は、構造を取り出すもっとも確実な教師だ。結局のところ、方法の知識が課題の解像度を決める。「遅いから、もっと頑張る」と「デザインレビューが週1回で3日待ちが発生しているから、非同期レビューを導入する」では、解像度がまるで違う。高解像度な課題認識を持つ人は、低解像度な課題も見える。逆は成り立たない。これは能力の問題ではない。道具の問題だ。本を読んで、問題が見えるようになった道具の話を続ける。というか、知識そのものが道具だと思っている。プログラミング言語やフレームワークだけが道具ではない。概念、分析手法、設計原理。頭の中にある知識が、目の前の問題を切り分ける刃物になる。パフォーマンス分析の本を読んだことがある。Brendan Greggの『詳解 システム・パフォーマンス』だ。読む前と後で、同じシステムが違うものに見えた。詳解 システム・パフォーマンス 第2版作者:Brendan Greggオーム社Amazon読む前の私は、パフォーマンス問題が起きたとき「遅い」しか言えなかった。「遅い」の内訳がわからなかった。CPUが飽和しているのか、I/Oで待っているのか、ロックの競合が起きているのか。どこを見ればいいのか、そもそも何を計測すべきなのかがわかっていなかった。「遅い」の前で立ち尽くしていた。この本が教えたのは、ツールの使い方ではなかった。問いの立て方だった。USEメソッドという手法がある。Utilization（使用率）、Saturation（飽和度）、Errors（エラー）この3つの観点で、すべてのリソースを順番に点検する。手順としてはシンプルだ。しかし、この「順番に点検する」という発想が、私にはなかった。「遅い」から「なんとなくCPUが高い気がする」に飛んでいた。途中のステップを全部飛ばしていた。飛ばしていることにすら気づいていなかった。なぜ飛ばしてしまうのか。問題を分解する語彙がなかったからだ。 「遅い」を分解するには、「遅い」がどのリソースの、どの指標に現れているかを特定する必要がある。CPUの使用率が高いのと、CPUのランキューが飽和しているのは、まったく異なる問題だ。前者はCPUを多く使っているだけで、まだ余裕があるかもしれない。後者はCPUの処理待ち行列（ランキュー）にタスクが溜まりすぎている。対処が違う。しかし、この区別は「飽和度」という概念を知らなければ、そもそも見えない。読んだ後、「遅い」の解像度が変わった。「遅い」が「ディスクI/Oの飽和度が90%を超えている」になった。「レイテンシが高い」が「カーネルのスケジューラがCPUの処理待ち行列に積まれたタスクを処理しきれていない」になった。同じシステムを見ているのに、見える景色がまったく違う。分散システムの設計原理を扱ったMartin Kleppmannの『データ指向アプリケーションデザイン』でも、同じことが起きた。読む前、私はデータベースの選定を「MySQLかPostgreSQLか」で考えていた。スケーリングは「サーバーを増やせばいい」くらいの解像度だった。データ指向アプリケーションデザイン ―信頼性、拡張性、保守性の高い分散システム設計の原理作者:Martin Kleppmann,斉藤太郎,玉川竜司オライリージャパンAmazon(※原著の第2版が出て全員が喜んでいる。)learning.oreilly.comこの本を読んで気づいたのは、私が「スケーリング」と呼んでいたものの中に、互いに矛盾する複数の問題が隠れていたということだ。レプリケーションの一貫性とレイテンシはトレードオフの関係にある。パーティショニングの戦略次第で、ある種のクエリは高速になり、別のクエリは破滅的に遅くなる。「サーバーを増やす」の一言で済むはずがなかった。しかし、私はそのことを知らなかった。知らなかったから、「増やせばいい」で止まっていた。ソフトウェア設計の結合について書かれたVlad Khononovの『ソフトウェア設計の結合バランス』でも、同じパターンを経験した。読む前の私は、「疎結合にしておけば正しい」と思っていた。コードレビューで「ここ、結合が強くないですか」と言えば、それだけで設計の指摘として成立した。冒頭で書いた「コミュニケーション不足」と同じ構造だ。何も言っていないのに、言った気になれる万能ラベル。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonこの本が教えたのは、結合には強度・距離・変動性という3つの次元があり、それらのバランスで評価すべきだということだ。強い結合でも、距離が近く変動性が低ければ問題にならない。弱い結合でも、距離が遠く変動性が高ければ厄介になる。「疎結合にしろ」の一言では、この判断ができない。私がレビューで「結合が強い」と指摘していたとき、強度の話をしているのか、距離の話をしているのか、変動性の話をしているのか、自分でも区別できていなかった。区別する語彙がなかったからだ。もう1冊——これは私自身が翻訳に関わった本だ。Nick Tuneの『アーキテクチャモダナイゼーション』。アーキテクチャの刷新は技術の問題ではなく、組織とビジネスと技術の三位一体の変革だと主張する本だ。翻訳の過程で560ページを何度も読み返した。読み返すたびに、自分がアーキテクチャを「技術の構造設計」としか見ていなかったことに気づかされた。チーム構造がアーキテクチャの不可分な一部であること、コンウェイの法則を観察ではなく設計ツールとして使うこと、モダナイゼーションの成功指標が「新しいシステムが動くかどうか」ではなく「ビジネスアウトカム」であること——技術書を翻訳しているはずなのに、組織論と戦略論を学んでいた。アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonしかし、この本もすべての問題を完璧に解決する銀の弾丸ではない。翻訳者として断言する。どんなに優れた方法論も、組織と個人にそれを受け入れる準備と継続する体力がなければ機能しない。本が提案する「独立バリューストリーム」も「AMET（イネーブリングチーム）」も、体力がない組織ではただの用語に終わる。体力がない組織は「この本の通りにやったのにうまくいかない」と言い、本のせいにする。本は地図であって、歩く脚力ではない。これは、先に挙げた3冊すべてに言えることだ。4冊に共通する経験がある。「読む前は、自分が問題を抱えていることすら知らなかった」。パフォーマンス分析の体系を知らないこと自体が問題だと、読む前の私は思っていなかった。分散データの一貫性モデルを知らないことが設計上の盲点になっていることに、読む前の私は気づいていなかった。結合を1次元でしか評価できていないことに、読む前の私は疑問すら持っていなかった。アーキテクチャを技術だけで語れると思い込んでいたことに、翻訳するまで気づかなかった。問題の存在を知らなかったから、解決しようとも思わなかった。道具について詳しく知らなければ、目の前の問題が「解決できる問題」なのか「原理的に解決不可能な問題」なのかすら判断できない。「遅い」が「体系的な手法で5分で原因を特定できる問題」なのか、「アーキテクチャを根本から見直さないと解決しない問題」なのか。この見極めは、方法を知らなければ不可能だ。そして、見極められない人はどちらのケースでも同じ反応をする。「遅いですね、まあ仕方ないですね」。これが一番怖い。解決できる問題を「仕方ない」で片付けている。解決の入口はすぐそこにあるのに、入口が見えていない。同じ問題を前にして、片方は手も足も出ず、片方は鼻歌を歌いながら直している。違いは才能ではない。「問いの立て方」を知っているかどうかだ。ソフトウェアの世界では「銀の弾丸はない」とよく言われる。魔法のように問題を解決する単一の技術は存在しない、と。Fred Brooksが1986年に書いたことだ。しかし、私はこの言い方に違和感がある。USEメソッドは、パフォーマンス問題に対する銀の弾丸だった。少なくとも私にとっては。「遅い」という怪物に対して、それまでの私は素手で立ち向かっていた。USEメソッドという弾を手に入れた瞬間、怪物は倒せるようになった。OpenTelemetryも、制約理論も、結合の3次元モデルも、社会技術的整合の視点も、Rustの所有権モデルも、それぞれの領域で怪物を殺す弾だった。銀の弾丸はある。 ただし、万能の一発ではない。特定の怪物を殺す特定の弾だ。問題は「銀の弾丸が存在しない」ことではなく、どの弾がどの怪物を殺すかを知らないことだ。弾を持っていないのではない。弾の存在を知らない。だから素手で戦っている。しかし、もっと厄介な問題がある。銀の弾丸を「求める」こと自体が、病理だ。「学んで損したくない」「タイパが悪い」こう言い始めた瞬間、人は新しい弾丸を手に取らなくなる。学ぶコストを「損」と捉えると、今持っている弾丸でなんとかしようとする。そして、自分が持っている弾丸が「すべての怪物を、すべての状態で殺せる万能の一発だ」と思い込む。Kubernetesですべてのインフラ問題が解ける。スクラムですべてのプロジェクトがうまくいく。Rustですべてのソフトウェアが安全になる。ありえない。しかし、新しい弾丸を手に入れるコストを「損」だと思っているから、既存の弾丸の適用範囲を無限に広げようとする。これが「銀の弾丸はない」の真の意味だと私は思っている。銀の弾丸が存在しないのではない。1つの弾丸で全部解決したいという欲望が、方法の目的化を引き起こすのだ。体力がないのではない。体力を分散させたくないのだ。1冊の本で、1つのフレームワークで、1つの言語で、全部なんとかしたい。その気持ちはわかる。しかし、怪物は1種類ではない。ちなみに、これは技術の話に限らない。「コミュニケーションが銀の弾丸だ」と主張するオジサンもいる。「社内政治をうまくやれば技術の問題は些末だ」と言う人もいる。しかし、コミュニケーションも社内政治も銀の弾丸ではない。同じ構造だ。コミュニケーションで解ける問題はある。社内政治で通せる案件もある。しかし、それぞれが特定の怪物に対する特定の弾であって、万能ではない。コミュニケーションで解決するのはコミュニケーション不足が原因の問題だけだし、社内政治で通せるのは政治が障壁になっている案件だけだ。技術的に破綻しているアーキテクチャを、いくらコミュニケーションしても直らない。政治力で通したプロジェクトも、技術的な裏付けがなければ崩壊する。他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazon企業変革のジレンマ　「構造的無能化」はなぜ起きるのか (日本経済新聞出版)作者:宇田川元一日経BPAmazonと書いて、立ち止まる。「本を読めば解像度が上がる」。それはそうだ。しかし、読んだだけで上がるのか。正直に言えば、『詳解 システム・パフォーマンス』を読んだ直後に、USEメソッドを使いこなせたかといえば、使いこなせなかった。本に書いてある通りに手を動かしてみたが、「この数値が高いのは問題なのか正常なのか」の判断ができなかった。正常値の感覚がなかったからだ。本は分解の語彙を教えてくれた。しかし、語彙を持っているだけでは足りない。その語彙を使って実際のシステムを何度も見て、「この数値はこのワークロードなら正常」「この数値は明らかに異常」という感覚を養う必要があった。本は地図を与える。しかし、地図を読む力は、歩かなければ身につかない。Rustを触って見えてきたもの「道具」と言った。では、その道具を手に入れることで、実際に何が変わるのか。私自身の経験を書く。Rustを学び始めたとき、最初は「速くて安全な言語」くらいの認識だった。C++の代替。メモリ安全。その程度の理解で、チュートリアルを写経していた。しかし、写経だけでは理解できなかった。私がRustの設計思想を理解できたのは、最初に「遊び」から入ったからだ。Rustを触り始めたとき、私は何の目的も持っていなかった。「何ができるんだろう」という純粋な興味だけがあった。課題を解こうとしていなかった。ただ遊んでいた。コンパイラに怒られた。Vec<String>を関数に渡した後で使おうとして、「value used here after move」と言われた。意味がわからなかった。「渡しただけなのに、なぜ使えなくなるんだ」と画面に向かって言った。所有権を無視したコードを書いて、なぜ通らないかを考えた。何の成果も出なかった。しかし、その「遊び」の時間がなければ、今の理解はなかった。遊びの中で、Rustの輪郭が見えてきた。何を許し、何を許さないか。どこで厳しく、どこで柔軟か。目的を持って触っていたら、目的の範囲内でしか見えなかっただろう。遊びは効率が悪い。成果が出ない。何をやっているか説明できない。だから、大人になると遊ばなくなる。すべてに目的を求める。しかし、目的を持つ前に遊んでおかないと、目的自体が貧しくなる。遊びの中で見つけた「面白い」が、後から課題認識の種になる。なぜ「遊び」が有効なのか、構造を言語化してみる。目的を持って触ると、「目的に関係するか否か」というフィルタが働く。Rustを「Webサーバーを書くため」に学んだら、所有権はHTTPハンドラの文脈でしか理解しない。しかし、遊びにはフィルタがない。所有権がなぜ存在するかを、用途を限定せずに考えられる。結果として、「この概念はHTTPに限らず、並行処理全般に適用できる」という広い理解に到達する。つまり、遊びの正体はフィルタなしの探索だ。効率は悪い。成果の予測ができない。しかし、目的というフィルタの外側にあるものを拾える。目的を設定した瞬間に見えなくなるものが、遊びの中では見える。しかし、これは「遊びを正当化する理論」になっていないか。3ヶ月遊んで何も身につかなかった経験もある。Haskellを触っていた時期だ。モナドが面白くて圏論の本まで買った。3ヶ月後、業務で使える場面は1つもなく、モナドの定義を聞かれても正確に答えられなかった。「フィルタなしの探索」は聞こえがいいが、フィルタがないまま3ヶ月歩いて、元の場所に戻っていることもある。遊びが実を結ぶかどうかは、事前にはわからない。それでも、遊ぶ。目的の枠内だけで学び続けた人間は、目的の枠を超えた発想ができない。撃ってみれば、案外簡単に怪物が倒せるかもしれない。 弾を込める前から「当たらないだろう」と言って撃たないのが、一番もったいない。遊びは、まだ名前のない怪物に向けて撃つ弾だ。外れるかもしれない。でも、撃たなければ当たりもしない。「遊び」と「無目的な浪費」の境界はどこか。この問いには答えを持っている。遊びの中で「おや？」と思った瞬間を記録しているかどうかだ。遊んでいて何かに引っかかる。引っかかりを言葉にしてメモする。なぜ引っかかったのかを考える。このループが回っているなら、遊びは探索として機能している。ループが回っていないならただ触って、ただ忘れるならそれは浪費だ。では、読者が「遊びから入る」を再現するにはどうすればいいか。私のやり方を書く。週に数時間、目的を決めずに技術を触る時間を確保する。そこでやることは、「業務で使う予定のないもの」を触ることだ。業務で使うものを触ったら、それは遊びではなく学習だ。遊びの条件は「役に立つかどうかわからないまま触る」ことだ。そして、触った後に「何が面白かったか」「何に驚いたか」を3行だけ書く。3行でいい。この3行が、半年後に「あのとき触った概念がここで効く」と気づく入口になる。そうして遊びながら書き込むうちに、Rustの設計思想が見えてきた。所有権、借用、ライフタイム。「データは誰かが所有し、借りるときは明示的に許可を得る」という世界観。最初は制約に感じた。慣れると、制約ではなく設計指針だと気づいた。すると、過去に書いたコードが違って見えてきた。「あのGoのコード、データ競合を起こしていなかったか」複数のgoroutineから同じデータに同時にアクセスしていた。動いていた。テストも通っていた。しかし、Rustの視点で見ると、あれは「たまたま動いていた」だけだ。タイミング次第でデータ競合を起こし、クラッシュする爆弾を抱えていた。「あのPythonのスクリプト、なぜ本番で落ちたのか」リストを関数に渡して、関数内で変更していた。呼び出し元は変更を想定していなかった。Rustなら、&mutを要求するか、所有権を移動するかを明示する。意図しない変更は、コンパイル時に弾かれる。「あのC++のメモリリーク、なぜ気づかなかったのか」newしたオブジェクトをdeleteし忘れていた。コードレビューでも見落とした。Rustなら、所有者がスコープを抜けた時点で自動的に解放される。忘れようがない。Rustという方法を知る前は、これは「課題」ではなかった。「そういうものだ」と思っていた。方法を知ったことで、初めて「これは課題だったのだ」と気づいた。知らなかったときは、違和感すらなかった。動いていたから。テストが通っていたから。「動いているコード」は「正しいコード」だと思っていた。動くコードは正しい。しかし、正しくないコードも動く。方法の目的化と、方法を極めることは違うRustの例を書いた。「方法を知ることで課題が見える」。ここまでは良い。しかし、この主張をすると、必ず出てくる誤解がある。「方法を極めろ」と言うと、「方法の目的化」と混同される。でも、両者はまったく異なる。方法の目的化は、方法を使うこと自体が目的になっている状態だ。「Rustを導入した」ということに価値を見出し、それで何を解決するかを考えない。導入実績を作ることがゴールになる。「うちもRust使ってます」と言いたいだけ。技術ブログを書きたいだけ。私自身、この罠にハマったことがある。Kubernetesを導入したとき、「クラウドネイティブにした」こと自体に達成感を覚えていた。チームに導入を提案し、半年かけて移行した。技術ブログも書いた。しかし、Kubernetesで何を解決するかが曖昧だった。デプロイ頻度は週1回のまま変わらなかった。スケーリングが必要な負荷もなかった。私は「導入した」という実績を作っただけだった。本当にそうだろうか。あの導入がなければ、コンテナオーケストレーションの設計思想を学ぶ機会はなかった。結果として、その後のプロジェクトで活きた知識は多い。「目的化だった」と断じるのは、後知恵かもしれない。しかし、当時の私に「何の課題を解くために導入するのか」と問えば、答えに詰まったはずだ。目的化と極めることの境界は、後から振り返って初めて見える。渦中にいるときは、区別がつかない。区別がつかないなら、早期警戒サインを作るしかない。私が自分に課しているチェックリストがある。「導入すること」を誰かに報告したくなっているか。「この技術で何を解決したか」を聞かれたとき、具体的な数字で答えられるか。「別の方法でも同じ結果が出せたか」を検討したか。この3つのうち1つでもNoなら、目的化の兆候がある。Kubernetesのときは、3つともNoだった。「導入した」が将来の資産になるか自己満足で終わるかの分かれ目は、そこで学んだ知識が「その技術固有の知識」か「転用可能な原理」かにある。Kubernetesの場合、宣言的な構成管理の思想、reconciliation loopの設計パターン、自己修復するシステムの考え方これらはKubernetesを離れても使える知識だった。だから結果として資産になった。しかし、当時の私がそこまで意識していたかと問われれば、していなかった。結果オーライだ。方法の限界を学ぶには、その方法で失敗するしかない。Rustの限界は、Rustで書くべきでないものをRustで書いたときに見える。Kubernetesの限界は、Kubernetesで解けない問題にKubernetesを適用したときに見える。本を読んでも限界はわからない。限界は、壁にぶつかって初めて見える。だから、失敗事例を集める。自分の失敗、他人の失敗、撤退戦の記録。比較実験——同じ問題を別の方法で解いてみる——も有効だが、時間がかかる。失敗事例の方が効率がいい。他人の失敗は、最もコストの低い学習教材だ。方法を極めるとは、「この方法では解けない」と言えるようになることだ。「Rustでできること、できないこと」を肌感覚で知る。その知識によって、課題の認識範囲を広げる。方法はあくまで方法であり続ける。目的は課題解決のままだ。方法の目的化は視野を狭める。「Rustで解決できそうな課題」ばかりを探すようになる。自分の得意技で解ける問題だけが「問題」に見えてくる。方法を極めることは視野を広げる。「Rustでは解決できない課題」と「Rustで解決できる課題」の両方が見えるようになる。ハンマーの適用範囲を知っているから、釘でないものに無理にハンマーを振らない。同じ「学び」でも、到達点は正反対だ。複数の方法を知る意味。ただし、効果は線形ではないだから、方法は複数知っている方がいい。1つの方法しか知らないと、その方法で解決できる課題しか見えない。ハンマーしか持っていなければ、すべてが釘に見える。これは有名な認知バイアスだ。「道具の法則」と呼ばれることもある。でも、ハンマーもドライバーもレンチも知っていれば、「この課題はハンマー向きだ」「これはレンチの方がいい」「これはどの道具でも解けない、道具を作るところからだ」と判断できる。「複数知っている方がいい」と書いた。しかし、ここに罠がある。10個のフレームワークを表面的になぞるより、まず1つの本質を骨まで理解する方が、結果的に多くの応用が効く。なぜか。1つを深く知ると、「なぜそう設計されているか」が見えるからだ。設計の意図がわかると、別のフレームワークを見たとき、「ああ、これは同じ問題を別の方法で解こうとしているのか」と理解できる。表面的に10個知っていたときには、それぞれが独立した「覚えるべきもの」だった。1つを深く知った後は、10個が「1つの問題空間の異なる解」として見える。構造が見える。これが「深さが広さを生む」ということだ。深く知らないと、広くも知れない。しかし、見えるのは設計の意図だけではない。もう少し踏み込む。深く知ることで見えるのは、構造だ。Kubernetesを深く知ると、「宣言的な状態管理」「自己修復するフィードバックループ」という構造が見える。この構造は、Kubernetesだけのものではない。Terraformの設計にも同じ構造がある。Gitの設計にも近い思想がある。生物の恒常性維持にもフィードバックループはある。深く掘った人間は、ここで面白いことをする。Kubernetesで学んだ「自己修復」の構造を、まったく別の文脈に持ち込む。組織のインシデント対応プロセスに「宣言的な状態定義と差分検知と自動修復」の発想を適用する。一見まったく関係がない。しかし、構造が同じだから、機能する。つまり、深く掘るほど、遠いところから借りてこられるようになる。 具体的なレベルで借りると模倣だが、構造のレベルで借りると独自の解が生まれる。回転寿司がビール工場のベルトコンベヤーから着想を得たように、借り元が遠ければ遠いほど、生まれる発想は独自性を帯びる。深さは、隣接領域を広げるだけでなく、遠い領域への跳躍を可能にする。「広く浅く知っておけ」長い間、これが能力の理想像として語られてきた。多くの領域を浅く押さえておき、1つだけ深く持つ。私自身もそう信じていた時期がある。しかし、実際に深く掘る経験を積んだ後で振り返ると、このモデルには見落としがある。3つ書く。第一に、「広さ」と「深さ」を独立した軸として扱っている。あたかも、広さを先に確保してから深さを足せるかのように。しかし、実際に何かを深く掘った人間なら知っている。深さと広さは分離できない。バックエンドを深く掘ろうとした。するとDB設計が要る。パフォーマンスチューニングが要る。セキュリティの知識が要る。インフラの理解が要る。深く掘れば掘るほど、裾野が勝手に広がっていく。「幅」は意図して作るものではなかった。「深さ」の必然的な副産物だった。この構造を図形で表すなら、三角形だ。 深さが増すほど底辺が広がる山型。深く掘るから、隣接領域を避けて通れなくなる。避けて通れないから、広がる。この「避けて通れない」が重要だ。意図して広げたのではない。掘っていたら、そこにあったのだ。第二に、深さから生まれた広さと、表面をなぞった広さを同一視している。バックエンドを深く掘る過程で身につけたDB設計の知識は、「DBの種類を5つ言える」という知識とは質がまったく異なる。「このクエリがなぜ遅いのか、インデックスの構造から説明できる」隣接領域に必要に迫られて踏み込んだから、実践と結びついている。動機が伴っているから、定着する。表面をなぞっただけの知識は、実践と切り離されている。説明はできるが、使えない。使えない知識は、AIに聞くのと変わらない。逆に言えば、「幅が広い」のに「深さがない」人は、実はどこも掘っていないだけだ。表面を横にスライドしているだけで、どこにも根を張っていない。ここまでは「広く浅く」モデルの論理的な誤りを指摘した。しかし、もう1つ、時代の文脈がある。第三に、このモデルは「情報が希少な時代」の産物だ。何がどこにあるかを知っているだけで価値があった時代には、浅く広い知識に意味があった。会議で「それ、聞いたことがあります」と言えるだけで、情報のハブとして機能できた。今は違う。情報は溢れている。AIが整理してくれる。ツールの使い方、言語の文法、フレームワークの設定表面的な知識はAIに聞けば数秒で返ってくる。「広く浅く知っている」は、もはや人間が抱えておく価値がない。人間のボトルネックは「知っているかどうか」から「理解しているかどうか」に移った。そして「理解」は、深さからしか生まれない。深く掘った人間だけが違和感を持てる。表面をなぞった人間には、違和感すら生まれない。だから、「まず広く浅く学んでから深掘りしよう」という順序は逆だ。興味のある1点からまず掘る。掘っていくうちに、隣接領域が「必要だから」広がる。動機が伴うから、学習が定着する。「広く浅く」から入ると、どこにも三角形が立ち上がらない。平らな線が引かれるだけだ。AI時代に人間に求められるのは、平らな線ではない。どこかに深く根を張った三角形だ。と書いたが、1つ正直に告白する。「広く浅く」が役に立った場面がある。障害対応で、直接の原因はバックエンドにあったが、フロントエンドのキャッシュの挙動を薄く知っていたから、「これ、フロントのキャッシュが古いレスポンスを返し続けているのでは」と仮説を立てられたことがあった。深くは知らない。しかし、存在を知っていたから、調べる入口にたどり着けた。「浅い知識は無価値」と断言するのは、嘘になる。しかし、あの場面で役に立ったのは「フロントのキャッシュがあるらしい」という存在の知識であって、「フロントのキャッシュをどう設計するか」という実践の知識ではない。存在を知っている程度の知識なら、AIに「この症状の原因として考えられるものは？」と聞いても得られる。2026年のいま、「存在を知っている」だけの価値は急速に下がっている。三角形の話をした。深さが広さを生む、と。では、三角形は1つでいいのか。1つでは足りない。「深く学ぶ」は「1つだけ学ぶ」ではない。1つを深く学んだ上で、異なる前提を持つ方法を複数知ることで、メタ視点が生まれる。私の経験を書く。3つ目の方法を学んだとき、世界の見え方は劇的に変わった。1と2の比較ではなく、「方法を比較する」というメタ視点が生まれたからだ。スクラムとウォーターフォールの2つしか知らなかったとき、私は「どちらが正しいか」を考えていた。そこにカンバンという3つ目が加わったとき、「どの状況にどの方法が適するか」という問いに変わった。しかし、10個目を学んだとき、変化は小さかった。20個目はもっと小さかった。私の中に「方法を比較するフレームワーク」ができてしまえば、新しい方法は既存の棚に分類されるだけだ。劇的な視点の転換は起きにくくなる。つまり、方法数と課題認識の関係は線形ではない。最初の数個で急激に上がり、その後は緩やかになる。学習曲線の逓減だ。実感と合う話がある。人間が同時に頭の中で比較できる選択肢には限界がある。100個の方法を「知っている」としても、課題に直面したときに思い出せるのは、せいぜい数個だ。残りは長期記憶の奥底にあり、意識的に検索しなければ出てこない。さらに、方法が増えるほど「どれを使うか」の選択コストが上がる。10個の方法から最適なものを選ぶより、3個から選ぶ方が速い。速さは、実践において決定的に重要だ。完璧な方法を選ぶのに3時間かけるより、そこそこの方法で2時間で解決する方が、多くの時は正解だ。私の実感としては、こうだ。1つの領域で、異なる前提を持つ方法を3つ知っていれば、メタ視点が生まれる。5つを超えると、追加の認識拡張効果は急速に逓減する。 3つ目を手に入れた瞬間の感覚は、今でも覚えている。「あ、これは選べるんだ」と思った。それまでは「どちらが正しいか」だった問いが、「どの状況にどれが合うか」に変わった。景色が変わるというより、自分が立っている場所が高くなった感覚だった。だとすると、問いは「いくつ学ぶか」ではなく、「どの領域で最初の3つを学ぶか」になる。すでに10個の方法を知っている領域にもう1個追加しても、認識範囲は広がらない。まだ1つしか知らない領域で2つ目を学んだ方が、視野は大きく広がる。私の場合、インフラの知識があるから「これはアプリケーション層の問題ではなく、ネットワーク層の問題だ」と判断できることがある。プログラミング言語を複数知っているから「この問題はRustで書くべきか、Pythonで書くべきか」という選択ができる。これは課題を正しく認識するための前提条件だ。方法を1つしか知らない人は、その方法で解けない課題を「課題」として認識できない。見えないのだ。見えないものは、存在しないのと同じだ。「どの領域で3つを学ぶか」が大事だと書いた。では、新しい領域を学び始めたとき、最初にぶつかる壁は何か。「わからない」が多すぎることだ。ここで大事になるのが、「わからない」を保留する勇気だ。新しい領域を学ぶとき、すべてを理解しようとすると詰まる。「これはなぜこうなっているのか」「この部分は何の意味があるのか」。答えが出ないまま先に進めなくなる。しかし、学びは順序通りに進まない。後で学んだことが、前の疑問を解消することがある。Rustの所有権を最初に読んだとき、私は何もわからなかった。「なぜこんな制約が必要なのか」が見えなかった。ところが、並行処理を学んだ後、「ああ、これはデータ競合を防ぐためだったのか」と腑に落ちた。最初から全部わかろうとしていたら、所有権の章で止まっていた。「わからないけど、とりあえず進む」という保留ができたから、後で理解できた。わからないことを「わからないまま抱えておく」のは、気持ち悪い。すっきりしない。しかし、その気持ち悪さに耐えることが、学びの幅を広げる。わからないまま進む勇気が、後で理解する土壌を作る。私が見落としているもの。そして、見落としに気づいた経験正直に書く。Rustの可能性に興奮している。所有権システムで防げるバグの範囲が見えてきた。「これもコンパイル時に防げる」「あれも型で表現できる」と考える時間が増えている。しかし、その興奮の中で見落としていることがあるはずだ。量子コンピューティング、バイオテクノロジー、新素材、ロボティクス。これらの分野で何が起きているか、私はほとんど知らない。論文のタイトルは見る。ニュースは読む。でも、手を動かしていない。肌感覚がない。つまり、それらの領域に関しては、違和感すら持てない状態にいる。これは仮説ではない。過去に経験している。コンテナ技術が出てきたとき、私は「仮想マシンで十分だろう」と思っていた。VMwareの知識があった。課題は「いかに効率よくVMを立てるか」だと認識していた。Dockerの記事を読んでも、「軽量な仮想化」程度の理解で止まっていた。半年後、気づいたら周りはコンテナで動いていた。私だけがVMの最適化を議論していた。「なぜコンテナを使わないのか」と聞かれて初めて、自分が課題を誤認していたと気づいた。問いは「VMをいかに効率化するか」ではなく、「インフラの抽象化レイヤーをどこに置くか」だったのだ。その半年間で、私は何をしていたか。VMのリソース割り当てを最適化するスクリプトを書いていた。起動時間を30秒短縮するためにブートシーケンスを研究していた。マイグレーションの自動化ツールを設計していた。それ自体は技術的に面白かった。しかし、私は存在しない道路の渋滞を解消していた。結果として、チームの技術選定に私の意見は反映されなかった。「VMの専門家」としての発言権はあったが、「インフラ戦略」の議論では蚊帳の外だった。半年という時間は、取り戻せない。その間に私が書いたコードは、1行も本番で動いていない。学んでいれば気づけた。でも学ばなかった。「VMで十分」という認識が、学ぶ動機を奪っていた。課題が見えないから学ばない。学ばないから課題が見えない。悪循環だ。この悪循環には、もう1つ怖い特徴がある。外から見えないということだ。私は半年間、勤勉に働いていた。コードを書いていた。成果物を出していた。傍から見れば、貢献している。しかし、その貢献先がすでに陳腐化していた。誰も指摘してくれなかった。指摘できなかった。みんな忙しかったし、私の専門領域に口を出すのは気が引けたのだろう。あの半年の経験があるから、今の自分にも同じことが起きていないか問わずにいられない。「Rustで解決する」という発想に囚われ、別の方法で解くべき課題を、無理に型システムの問題として設定していないか。Rustで解けない課題を、「課題ではない」と無意識に切り捨てていないか。私にはわからない。わからないから、怖い。自分の視野の外側は、見えない。見えないことすら、わからない。これが一番怖い。そして、過去にそれで手遅れになった経験があるから、恐怖は具体的だ。学び続ける理由だから、最新技術を触り続ける。これは「技術が好きだから」ではない。いや、好きではあるのだが、それだけではない。課題を正しく認識し続けるための、必要な投資だ。学ぶことをやめた瞬間、課題認識は固定される。世界は変わり続けるのに、自分の課題認識だけが古いままになる。新しい方法で解ける課題が増えているのに、それを「課題」として認識できない。5年前の方法の知識で、今日の課題を設定しようとする。それは、一度もinvalidateされていないキャッシュを読んでいるようなものだ。値は返ってくる。ただし、現実とは一致していない。これが一番怖い。だから、学び続ける。と書いて、立ち止まる。本当にそうか。学び続ければ、視野は広がり続けるのか。それとも、学ぶことで見えなくなるものもあるのか。新しい方法を知ることで、古い方法の価値を見失っていないか。この問いに、私なりの答えを書く。学ぶことで見えなくなるものは、ある。Rustを深く学んだ結果、私は「シンプルさ」の価値を見失いかけたことがある。「これは型で表現すべきだ」「あれは所有権で制約すべきだ」と考えるうちに、「Pythonで10行で書けるスクリプトに、なぜ100行のRustが必要なのか」という問いを忘れていた。安全性だけを考えれば、すべてをRustで書くべきだ、と当時の私は思い込んでいた。しかし、それは問いの立て方が間違っていた。「安全か」ではなく、「この用途に適切か」という軸がある。「正しいか」ではなく、「理解しやすいか」という軸がある。Rustという方法を深く学んだことで、私は安全性の軸ばかりを見るようになっていた。シンプルさや可読性の軸が見えにくくなっていた。古い方法には、効率以外の価値があることがある。手動デプロイは、CI/CDより非効率だ。しかし、手動でやっていた頃は、デプロイの各ステップが何をしているか全員が理解していた。自動化した途端、パイプラインがブラックボックスになり、壊れたときに誰も直せなくなった。効率を得て、理解を失った。新しい方法を学ぶと、古い方法が「非効率」に見える。見えた瞬間、古い方法の別の価値、効率では測れない価値が視野から消える。これが、学ぶことで見えなくなるものだ。だから、学び続けることは万能ではない。学びながら、同時に「この方法では見えないものは何か」を問い続ける必要がある。新しい眼鏡をかけたら、古い眼鏡で見えていたものを意識的に思い出す必要がある。学び続けるとは、忘れたことを思い出し続けることでもある。AIが方法を知っている時代にここまで「方法を学べ」「学び続けろ」と書いてきた。しかし、1つ、避けて通れない問いがある。AIは方法を知っている。サンダリングハードも、カスケード障害も、制約理論も。私が半年かけて学んだことを、AIは数秒で答える。操作の知識、言語の文法、ツールの使い方は、もう人間の優位ではない。分解の知識すら、AIは構造的に提示してくれる。では、方法を学ぶ意味はなくなったのか。逆だ。AIは方法を知っている。しかし、違和感を持てない。エディタの前で手が止まる。「なんとなくうまくいっていない」と感じる。会議を増やしても改善しない焦りを覚える。AIにはこれがない。AIは聞かれたことに答える。聞かれなければ、黙っている。課題は、違和感から始まる。AIには、違和感がない。違和感は、好奇心を持って世界に触れている人間にしか生まれない。「何かがおかしい」と感じるためには、「こうあるべきだ」という自分なりの基準が必要だ。では、その基準はどこから生まれるか。4つある。方法を学ぶ過程で作り上げた技術的な基準。過去の失敗から刻まれた経験的な基準。「こういうシステムはあるべきではない」という価値観の基準。そして、画面の前で「なんか気持ち悪い」と感じる身体的な基準。言語化できないが、確かにそこにある直感だ。AIから借りた基準では、違和感は生まれない。借り物の基準は「なるほど」で終わる。自分の基準は「おかしい」から始まる。もう1つ。AIは汎用的な方法を知っているが、あなたの文脈を知らない。VMの最適化に半年を費やした私の後悔。Kubernetesを目的化して得た教訓。Rustの遊びの中で見つけた「面白い」。これらは私だけの経験であり、私だけの課題認識を形作っている。同じ障害を見ても、同じコードを読んでも、私とあなたでは見える課題が違う。その違いが、課題設定の独自性になる。AI時代に人間に残るのは、この2つだと思っている。好奇心。 違和感を持ち、「なぜ」と問い続ける力。自分だけの観点。 固有の経験が生んだ、代替不可能な視点。と書いて、立ち止まる。この主張自体がすでにコモディティ化している。AIに「人間の価値とは」と聞けば、「好奇心と固有の経験」と答えるだろう。「人間にしかできないことがある」と言いたい人間の願望が、透けて見える。しかし、コモディティ化していることと、間違っていることは違う。「テストを書け」も「コードレビューしろ」もコモディティ化した主張だが、依然として正しい。問題は、正しいかどうかではなく、この主張を自分の経験で検証できているかどうかだ。私の場合、VMの半年、Kubernetesの目的化、OOMKillerの夜——それらの経験を通じて好奇心と観点が鍛えられたという実感がある。実感がある、と思っている。思い込みかもしれない。方法はAIに聞ける。しかし、何を聞くかを決めるのは自分だ。好奇心がなければ、そもそも問いが生まれない。自分だけの経験がなければ、独自の問いにならない。ここにもう1つ、根本的な問題がある。障害対応のセクションで書いた「道の存在を知らなければ、歩き出すことも、誰かに道を聞くこともできない」という話——これがAI時代にはさらに先鋭化する。AIに聞けるのは、道の存在を知っている人間だけだ。 パフォーマンスが遅い。USEメソッドを知っている人間は「CPUのSaturationを確認して」とAIに聞ける。知らない人間は「遅いんですけど」としか言えない。AIは「遅い」から有用な回答を返すこともある。しかし、問いの精度が低ければ、本当に必要な情報にたどり着く確率は下がる。もっと怖いケースがある。問題の存在自体を認識していない場合だ。テストが通っている。本番も動いている。AIに何も聞かない。問題がないと思っているのだから、聞く理由がない。道があることを知らなければ、地図を広げることすらしない。だから、AI時代に必要なのは「AIに聞く力」だけではない。「ここに道がある」と気づく力だ。道の存在に気づくためには、自分の足で歩いた経験が要る。歩いたことがある人間だけが、まだ歩いていない方向にも道があるかもしれない、と想像できる。さらに、見落とされがちなことが2つある。第一に、AIの出力の質は、使う人間の力量で天井が決まる。AIが100点の回答を出しても、その領域の理解が浅ければ、100点の回答を100点として受け取れない。40点の回答と区別がつかない。私がKubernetesのマニフェストでやらかしたのは、まさにこれだ。画力のある人間がAI画像生成を使えば、構図・色彩・解剖学的正確さを的確に指示し、選別し、修正できる。画力のない人間は「なんかいい感じ」で止まる。コードも同じだ。型システムを理解している人間がAIにRustを書かせれば、生成されたコードの所有権設計が妥当かどうかを判断できる。理解していない人間は、コンパイルが通ればOKだと思う。自分で経験した。AIにKubernetesのマニフェストを生成させたことがある。出力されたYAMLは文法的に正しかった。デプロイも通った。しかし、resources.limitsが設定されていなかった。本番でOOMKillerに殺されるまで、私は気づかなかった——いや、正確に言えば、AIの出力を読んだとき、resourcesの記述がないことに違和感を持てなかった。Kubernetesのリソース管理を深く理解している人間なら、「limitsがない」ことに即座に気づく。私は当時、その解像度を持っていなかった。AIの出力は正しかった。私の目が足りなかった。AIは掛け算だ。 基礎力がゼロに近ければ、いくら掛けても結果は小さい。第二に、AIへの指示の質は言語化能力に依存する。AIは空気を読まない。「いい感じにして」では「いい感じ」の定義が共有されていないから、期待通りにはならない。人間の同僚なら、組織の文脈や過去の経緯から補完してくれる。AIにはそのコンテキストがない。ここに気づいたとき、私は苦笑した。AIを使いこなせない人は、実は人間相手のコミュニケーションでも伝わっていなかった可能性がある。人間相手では「察してもらえる」ことに甘えていただけだ。AIは「察する」能力がない分、伝わっていないという事実を可視化しているにすぎない。言語化能力とは、「なぜそう思うのか」「具体的にはどういう状態か」「何と何の間で迷っているのか」を自分に問い続ける習慣だ。これはAI活用のためだけのスキルではない。思考そのものの精度を上げる訓練でもある。障害対応中に「なんか変だ」と感じたまま言葉にせず、30分後にチームメイトが同じ違和感を言語化して、そこから10分で原因にたどり着いたことがある。私が30分間握りしめていた違和感と、彼が10秒で言語化した仮説は、同じものだった。言語化は正確さの問題であると同時に、速度の問題でもある。AIに「何を聞くか」を決める力は、どんな訓練で伸びるか。私が実感しているのは、観察→仮説→質問→検証の反復だ。まず、目の前の状況を観察する。次に、「こうなのではないか」という仮説を立てる。仮説を検証するための質問をAIに投げる。返ってきた答えを、自分の仮説と突き合わせる。この4ステップを愚直に回す。仮説なしにAIに聞くと、「いい感じにして」になる。仮説があると、「AとBのどちらが適切か、Cの条件下で比較して」になる。後者の方が、はるかに使える回答が返ってくる。そしてもう1つ、「自分だけの観点」を独りよがりにしないための条件がある。自分だけの経験で得た視点は、代替不可能だ。しかし、それが他者に伝わらなければ、組織では機能しない。「自分だけの観点」を「チームで使える知見」にするには、4つのステップが要る。言語化（経験を言葉にする）、検証（他の事例でも成り立つか確認する）、物語（なぜそう考えるに至ったかの経緯を語る）、データ（定量的な裏付けを添える）。全部揃わなくてもいい。しかし、言語化だけで止まると「俺の経験では」で終わる。検証とデータが加わると、初めて「組織の知見」になる。だから「方法を学べ」は、AI時代にこそ意味がある。AIに方法を聞くためではない。方法を通じて自分の目を鍛えるためだ。AIが答えを持っている時代に、問いを立てられる人間でいるために。おわりに「課題から入れ」は正しい。正しいが、この原則には暗黙の前提がある。「課題が見えている」という前提だ。見えていない課題を「そこから入れ」とは言えない。技の名前を知らなければ、負けた理由がわからない。わからなければ、直せない。だから方法を学ぶ。複数の方法を知る。「深く知ることで、何が見えるようになるか」を意識しながら。方法の探求は、課題の発見につながっている。方法を学ぶことで課題が見え、見えた課題から入る。「課題から入れ」の原則は、方法の学びによって初めて実行可能になる。と書いて、もう一度立ち止まる。冒頭で「課題から入れ」を批判した。課題が見えない人に「課題から入れ」と言うのは呪いだ、と。しかし、今の私は「方法を学べ」と言おうとしている。方法を学ぶための方法を持たない人に「方法を学べ」と言うのは、同じ構造の再生産ではないか。方法を学ぶ時間がある。本を買える。手を動かす環境がある。それは能力ではなく、環境の話だ。深夜まで障害対応に追われている人間に「本を読め」と言えるか。言えるとしたら、それは方法を持っている側の傲慢かもしれない。この記事自体が、ある種の恵まれた立場から書かれていることを、書きながら思う。それでも書く。書かないよりは、書いた方がいい。たぶん。1つだけ補足する。怪物を殺す手段は銀の弾丸だけではない。鉛の弾丸もある。特別な知識がなくても、地道に撃ち続ければ怪物は倒れることがある。泥臭い試行錯誤、ひたすらコードを書いて壊して直す繰り返し。華麗ではないが、弾数で勝負する方法だ。金の弾丸もある。売上は全てを癒す。予算があれば人を雇える。技術で解けない問題が、金で解けることはよくある。銀の弾丸、鉛の弾丸、金の弾丸。どれで撃つかを選べること自体が、方法の知識だ。そして、どの弾であれ、撃たなければ怪物は倒せない。明日からできることを書く。自分が「仕方ない」と思っているものを3つ書き出す。「毎週のレポートに5時間かかる」「チーム間の調整が遅い」「コードレビューが滞る」何でもいい。「仕方ない」と思っている時点で、そこには方法の知識が足りていない可能性がある。その領域で自分が知っている解決アプローチがいくつあるか数える。1つなら、2つ目を探す。2つなら、3つ目を探す。3つ以上あるなら、別の「仕方ない」に移る。注意点がある。同じ前提を持つ方法を3つ知っても視野は広がらない。前提の異なる方法を選ぶことで、初めてメタ視点が生まれる。「仕方ない」は、方法を知らないサインだ。 そこに、学ぶべき方法がある。エディタには、いつの間にか文章が並んでいた。最初の一行が出てこなかったはずなのに、気がつけばここまで書いている。あの日の私が知りたかったのは、「課題の見つけ方」ではなかった。「見つけるための道具」だったのだ。問いの立て方が間違っていた。たぶん、ずっと間違えていた。間違えていることを、わかりたくなかったのかもしれない。わからないことにしておいた方が、楽だったのかもしれない。「課題って、どうやって見つけるんですか」。あの問いへの答えを、ようやく書ける気がする。方法を学べ。そうすれば、課題の方からお前に見えてくる。おい、方法を学べ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Vertex AI Search の運用チップス]]></title>
            <link>https://zenn.dev/satohjohn/articles/fe7aa69acc8f37</link>
            <guid isPermaLink="false">https://zenn.dev/satohjohn/articles/fe7aa69acc8f37</guid>
            <pubDate>Sun, 08 Feb 2026 08:37:05 GMT</pubDate>
            <content:encoded><![CDATA[概要Enterprise における検索システムを簡単に構築できるという点で、Vertex AI Search は有用なサービスです。他にも検索だけではなく Gemini における Grounding となるソースとして利用する事もできます。しかし、単純に Vertex AI Search を構築してもあまり良い検索結果を得られることは少ないかと思います。良い検索結果を得るためには、データの整備、運用が必要になります。Gemini Enterprise は Vertex AI Search と同じ技術を利用しているため同じような検索結果をより良くするために同じような観点が必要...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubernetes Network Driver (KND)について調べてみた]]></title>
            <link>https://sreake.com/blog/introduction-to-kubernetes-network-driver-knd/</link>
            <guid isPermaLink="false">https://sreake.com/blog/introduction-to-kubernetes-network-driver-knd/</guid>
            <pubDate>Thu, 05 Feb 2026 06:00:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 今回、3-shakeで期限つきインターンをさせていただきました坂内理人（@rihib）と申します。インターン期間中はKubernetes Network Driverと呼ばれるKubernetesのネットワーク […]The post Kubernetes Network Driver (KND)について調べてみた first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AI時代に成長するエンジニアに必要なスキルとは.pdf]]></title>
            <link>https://speakerdeck.com/yunosukey/aishi-dai-nicheng-chang-suruensinianibi-yao-nasukirutoha</link>
            <guid isPermaLink="false">https://speakerdeck.com/yunosukey/aishi-dai-nicheng-chang-suruensinianibi-yao-nasukirutoha</guid>
            <pubDate>Wed, 04 Feb 2026 05:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[正しさは、昼間の言葉]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/04/002244</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/04/002244</guid>
            <pubDate>Tue, 03 Feb 2026 15:22:44 GMT</pubDate>
            <content:encoded><![CDATA[ある日の終電後。オフィスには私しかいなかった。蛍光灯は半分消えている。デスクのモニターだけが、青白い光を放っている。IRCの通知は止まっている。誰も見ていない。誰も知らない。今、私がここで何をしても、誰にも分からない。三時間前、本番環境で障害が発生した。顧客からの問い合わせが殺到した。Nagiosのアラートチャンネルが真っ赤に染まった。私はオンコール担当だった。原因を調査した。特定した。十五年前に書かれたコードの中に、バグがあった。なぜ今頃発火したのか。おそらく、長らく使っていたNFSの性能が落ちていたからだ。十五年間、たまたま踏まなかった地雷を、今夜ついに踏んだ。バグは十五年前からそこにあった。ただ、表に出てこなかっただけだ。障害は収束した。暫定対応でサービスは復旧した。しかし、根本原因を修正しなければ、また同じ障害が起きる。明日の朝までに、修正をリリースしなければならない。画面には、十五年前に書かれたコードが映っていた。変数名は tmp と data と result の三種類しかない。コメントは一行もない。テストは存在しない。一つの関数が八百行ある。その関数の中に、さらに三重のネストしたif文がある。読んでいると、頭が痛くなる。しかし、このコードは動いてきた。十五年間、今夜まで、一度も止まることなく、毎日数万件のトランザクションを処理してきた。私は、二つの選択肢の前で立ち尽くしていた。正しいやり方——リファクタリングする。テストを書く。変数名を直す。関数を分割する。その上で、バグを修正する。それには、最低でも三週間かかる。明日の朝には間に合わない。その間、同じ障害が再発するリスクを抱え続ける。間違ったやり方——このコードの該当箇所に、場当たり的なパッチを当てる。動けばいい。テストは書かない。後で直す。いつか直す。たぶん、直さない。私は知っていた。後者を選べば、このコードはさらに腐る。次にこのコードを触る人は、私を呪うだろう。しかし、その「次の人」は、私ではない。——と書いて、立ち止まる。本当にそうか。「次の人」は、来年の私かもしれない。それでも、私は迷っていた。「正しいコードを書け」。私は、何度もこの言葉を聞いてきた。本で読んだ。カンファレンスで聞いた。先輩に言われた。自分でも言ってきた。SOLID原則。DRY原則。クリーンアーキテクチャ。テスト駆動開発。私は、これらの「正しさ」を信じてきた。信じていた、と思っていた。しかし、終電後のオフィスで、十五年物のレガシーコードを前にして、障害の再発リスクを抱えながら、私は気づいた。「正しさ」は、昼間の言葉だ。昼間は、みんながいる。コードレビューがある。品質基準がある。「正しさ」を語ることで、評価される。「リファクタリングしましょう」と言えば、「意識が高い」と思われる。しかし、夜になると、誰もいない。誰も見ていない。「正しさ」を語る相手がいない。残っているのは、障害の再発リスクと、明日の朝というデッドラインだけだ。私は思い出した。以前、ある先輩が言っていた言葉を。「コードの美しさなんて、障害対応の現場には関係ない。止血が先だ」当時の私は、反発した。「技術的負債がたまる」「保守性が下がる」「長期的に見れば損だ」。正論を並べた。先輩は笑って聞いていた。今になって分かる。あの笑いの意味が。先輩は、私の「正しさ」が、まだ試されていないことを知っていた。本番障害の修羅場を経験したことがない。顧客からのクレームの電話を受けたことがない。自分の判断で、会社の信用が左右される経験をしたことがない。そういう人間が語る「正しさ」は、空論だ。——と書いて、自分の中にある別の声が聞こえる。それは、本当にそうか。「試されていない」ことが、「正しくない」ことの証明になるのか。試されていなくても、正しいものは正しいのではないか。しかし、その夜の私には、その声は届かなかった。私は、キーボードに手を置いた。そのとき、ふと考えた。十五年前、このコードを書いた人は、何を考えていたのだろうか。きっと、同じような状況だったのではないか。締め切りに追われていた。リソースが足りなかった。「後で直す」と自分に言い聞かせて、このコードを書いた。その人も、テストを書かなかった。変数名を直さなかった。コメントを書かなかった。なぜか。時間がなかったからだ。その人の後に来た人も、同じだったのではないか。このコードを見て、顔をしかめた。しかし、直さなかった。なぜか。時間がなかったからだ。そして今、私がここにいる。同じ選択を迫られている。「時間がなかった」。この言葉が、十五年間、このコードを腐らせ続けてきた。そして今、私も同じ言葉を使おうとしている。これは、正当化の連鎖だ。「前の人も時間がなかった。だから、自分も許される」。前例が、免罪符になる。私は、その連鎖の中に自分を位置づけようとしていた。連鎖の一部になれば、責任は薄まる。「自分だけが悪いわけではない」と言える。責任を、過去に分散させることができる。——と書いて、その論理の欺瞞に気づく。責任を分散させても、コードは腐ったままだ。連鎖に加わることで、私が得るのは心理的な安心だけだ。コードは何も改善しない。しかし、その夜の私には、心理的な安心が必要だった。私は、自分を正当化する論理を組み立て始めた。この会社は、このプロダクトで売上を立てている。プロダクトが止まれば、顧客は離れる。顧客が離れれば、売上は下がる。売上が下がれば、会社は傾く。会社が傾けば、私は職を失う。正しいコードを書いている間に、サービスが止まり続けたら、何の意味がある？障害を早く直すことは、顧客のためだ。会社のためだ。同僚のためだ。そして、自分のためだ。みんなのためにやっている。だから、許される。この論理は、強力だ。「自分のため」だけなら、罪悪感がある。しかし、「みんなのため」なら、むしろ正義になる。利他的な動機があれば、手段は正当化される。私は、この論理に身を委ねようとしていた。しかし、ふと気づいた。十五年前の人も、同じ論理を使ったのではないか。「顧客のために、早くリリースしなければ」「会社のために、間に合わせなければ」。そう言い聞かせて、このコードを書いた。その結果が、今夜の障害だ。「みんなのため」という論理で書かれたコードが、十五年後に「みんな」を苦しめている。私は、同じことをしようとしている。今夜、「みんなのため」にパッチを当てる。そのパッチが、十五年後に誰かを苦しめる。正当化の論理は、短期的には機能する。しかし、長期的には破綻する。「みんなのため」は、「将来のみんな」を含んでいない。——と書いて、また立ち止まる。では、どうすればいいのか。障害を放置して、明日も明後日も顧客を苦しめることが、「将来のみんな」のためになるのか。どちらを選んでも、誰かを苦しめる。それが、この状況の本質だ。コードを書いている途中で、手が止まった。私は、自分が何をしているのか、急に分からなくなった。画面には、私が追加したパッチがある。八行。たった八行の条件分岐だ。バグの原因となっていたエッジケースを、特別扱いする。根本的な解決ではない。しかし、これで障害は再発しなくなる。たぶん。私は、自分のコードを見つめた。そして、気づいた。私は、このコードを十五年後に見る誰かを、呪っている。私は、十五年前にこのコードを書いた誰かを呪った。「なぜこんなコードを書いたのか」「なぜテストを書かなかったのか」「なぜリファクタリングしなかったのか」。呪いながら、同じことをしている。十五年後の誰かは、私を呪うだろう。同じ言葉で。同じ怒りで。そして、同じように、さらに八行のパッチを追加するだろう。呪いの連鎖だ。私は、その連鎖の一部になろうとしている。連鎖を断ち切ることもできる。リファクタリングすればいい。テストを書けばいい。しかし、それには三週間かかる。明日の朝には間に合わない。連鎖を断ち切るコストを、私は払えない。いや、払う気がない。コストを払う気がないのは、なぜか。それは、連鎖を断ち切る恩恵を受けるのが、私ではないからだ。リファクタリングの恩恵を受けるのは、十五年後の誰かだ。その誰かは、私ではないかもしれない。私は、十五年後にはこの会社にいないかもしれない。別のプロジェクトにいるかもしれない。そもそも、このプロダクト自体が、十五年後には存在しないかもしれない。コストは今の私が払い、恩恵は未来の誰かが受ける。それなら、コストを払わない方が、合理的だ。私は、その論理に抗えなかった。気がつくと、窓の外が明るくなっていた。目が痛い。徹夜明けの頭が、鈍く重い。始発の電車が走る音が聞こえる。オフィスのエアコンが、タイマーで動き始めた。私の目の前には、完成したパッチがあった。テストはない。コメントは一行だけ。「障害対応: エッジケースの特別処理を追加」。それだけだ。なぜそう書いたのか。どんな判断をしたのか。何も書かない。書けない。いや、違う。書かないのだ。書くと、自分がやったことを認めることになる。認めたくない。だから、曖昧にする。ステージング環境でテストした。動いた。本番環境にデプロイした。障害は再発しなかった。アラートは鳴らなかった。IRCに報告を書いた。「根本原因を修正しました。再発防止策は別途検討します」。「別途検討します」。この言葉が、どれほどの問題を先送りしてきたか。私は知っている。知っていて、使っている。私は、椅子にもたれかかった。疲れていた。しかし、安堵もあった。終わった。障害は収束した。顧客は救われた。私の仕事は終わった。そして、私は立ち上がって、オフィスを出た。朝の光が、眩しかった。夜の間に考えていたことが、すべて嘘のように思えた。「正しさ」も「正当化の連鎖」も「呪いの連鎖」も、夜の闘いの産物だ。朝になれば、消える。いや、消えるのではない。見えなくなるだけだ。朝の光の中で、私は「普通のエンジニア」に戻る。「正しいコードを書くべきだ」と語る。「技術的負債は早めに返すべきだ」と主張する。昼間の私は、そう言う。しかし、また夜が来る。また障害が来る。誰もいないオフィスで、同じ選択を迫られる。そのとき、私は何を選ぶか。私は、自分が何を選ぶか、もう知っている。この文章を読んでいるあなたは、どうだろうか。「私は違う」と思っているかもしれない。「私は正しいコードを書いている」「私はリファクタリングをしている」「私は技術的負債を返している」。そう思っているかもしれない。本当にそうだろうか。あなたは、一度も「後で直す」と言ったことがないか。一度も、テストを書かずにコミットしたことがないか。一度も、分かりにくいコードをそのまま追加したことがないか。一度も、「別途検討します」と書いたことがないか。ないはずがない。私たちは、みな同じ箱の中にいる。状況が許せば、正しいことをする。しかし、状況が許さなければ、正しさを捨てる。そして、「仕方なかった」と自分を正当化する。これは、個人の問題ではない。箱の問題だ。締め切りがある。障害対応の緊急性がある。リソースが足りない。時間が足りない。この箱の中で、「正しさ」を貫くことは、英雄的な行為だ。普通の人間には、できない。——と書いて、自分でも分かっている。「箱を変える」と言うのは簡単だ。実際に変えるのは、難しい。私自身、何度も挫折してきた。しかし、箱を変えようとしなければ、私たちは永遠に同じ場所にいる。夜ごとに「正しさ」を捨て、朝になると何事もなかったかのように振る舞う。その繰り返しだ。私は、その繰り返しから抜け出したい。しかし、抜け出せることを、確信できない。この矛盾を抱えたまま、私は今日もコードを書いている。あの夜から、三ヶ月が経った。私が当てたパッチは、まだ動いている。障害は再発していない。顧客からのクレームもない。障害対応は成功した。誰も、あのコードの中身を知らない。しかし、私は知っている。あのコードの中に、たった八行のパッチがあることを。テストがないことを。コメントが一行しかないことを。「別途検討します」と書いた根本対応が、まだ検討されていないことを。そして、いつか誰かが、あのコードを触ることになる。そのとき、その人は私を呪うだろう。「なぜこんなパッチを当てたのか」「なぜ根本対応をしなかったのか」と。私は、その呪いを受け入れる準備ができている。いや、できていない。受け入れなければならないと分かっているだけだ。次の障害が来たとき、私は何を選ぶか。「正しさ」を選ぶか。また同じ論理に堕ちるか。私自身にも、分からない。分からないまま、今日も私はオフィスに向かう。昼間は「正しさ」を語り、夜になれば「正しさ」を捨てる。その繰り返しの中で、私は何者になっていくのか。十五年前の人は、一度だけ選択した。その選択の結果が、今夜の障害だった。しかし、エンジニアは、毎日選択を迫られる。毎日、小さな「正しさ」を捨てるか、守るかを選ぶ。その積み重ねが、私たちを作る。そして、十五年後のコードを作る。私は、どんなエンジニアになりたいのか。どんなコードを残したいのか。その問いに、まだ答えられない。答えられないまま、今日も私は、コードを書く。十五年前の人の行方は、誰も知らない。私の行方も、私自身が知らない。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、分けて語るな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/02/02/124615</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/02/02/124615</guid>
            <pubDate>Mon, 02 Feb 2026 03:46:15 GMT</pubDate>
            <content:encoded><![CDATA[はじめに月曜日は経営会議。事業戦略を話す。水曜日は技術戦略会議。アーキテクチャを話す。金曜日は組織開発会議。チーム編成を話す。それぞれの会議には、それぞれの参加者がいる。経営会議には経営陣。技術戦略会議にはエンジニアリングリーダー。組織開発会議には人事と各部門長。それぞれが、それぞれの言葉で、それぞれの関心事を語る。私はいろんな立場でこれらの会議に呼ばれる。そして、いつも同じ違和感を覚える。「この話、別の会議でも関係あるんじゃないの？」言えない。言っても通じない。経営会議で「それ、アーキテクチャの話と関係ありませんか」と言っても、「それは技術の話だから水曜日に」と返される。技術戦略会議で「それ、組織の問題では」と言っても、「それは人事の話だから金曜日に」と返される。きれいに分かれている。整然としている。効率的に見える。分断は、様々な形をとって現れる。会議の分断。言葉の分断。評価指標の分断。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。人事部門は採用数と離職率で評価される。それぞれが、自分のKPIを最適化しようとする。そのKPIが、会議を分け、言葉を分け、関心を分ける。分断が最も激しくなる瞬間がある。計画変更のときだ。市場が変わった。競合が動いた。顧客のニーズが変化した。そのとき、事業戦略は変わる。しかし、技術戦略は変わらない。組織体制も変わらない。なぜか。計画変更は「誰かの責任」を問うことになるからだ。変更を認めると、最初の計画が間違っていたことになる。だから、誰も変更を言い出さない。計画通りに進めて、最後に「間に合いませんでした」と言う方が、傷が浅い。分断が弱まる瞬間もある。危機のときだ。システムが落ちた。顧客からクレームが殺到した。そのとき、部門の壁は一時的に消える。全員が同じ部屋に集まり、同じ問題に向き合う。しかし、危機が去ると、また元に戻る。危機対応は例外であり、日常ではない。日常に戻れば、分断も戻る。私は何度も見てきた。月曜の経営会議で決まった「3ヶ月で新機能をリリースする」という事業戦略が、水曜の技術戦略会議で「今のアーキテクチャでは6ヶ月かかる」と判明する。金曜の組織開発会議で「その機能を作れるエンジニアがいない」と分かる。3つの会議で、3つの事実が、別々に語られる。しかし、誰も全体を見ていない。分業には理由がある。効率だ。専門家が専門領域に集中できる。会議の時間は短くなる。責任は明確になる。組織が大きくなれば、分けなければ回らない。それは、正しい。ここで、一つ認めておくべきことがある。分けることには、正当な理由がある。「統合して議論する」は綺麗だ。しかし、関係者が増えるほど会議は重くなる。境界をまたぐ話は論点が多くなり、合意形成も難しくなる。「決められない組織」になるリスクがある。市場は待ってくれない。分けて速く回す方が勝つ局面は、確かにある。私も、全員参加の会議が延々と続いて何も決まらない組織を見てきた。あれを見ると、「分けた方がいいのでは」と思う気持ちは分かる。さらに言えば、サイロには「心理的安全性の防波堤」としての機能もある。組織には「安心して話せる範囲」が必要だ。小さな範囲（サイロ）があるから、本音や問題が出る。越境を強制すると、政治が混ざって発言が萎縮し、かえって問題が隠れることがある。全員参加の場は、評価や立場を気にして「無難な話」になりがちだ。だから、私が言いたいのは「分けるな」ではない。分けることの正当性は認める。「分けたまま、境界の情報を消すな」——これが、私の言いたいことだ。——と書いて、自分でも分かっている。「境界の情報を消すな」と言うのは簡単だが、消さないためにどうするかが難しいのだ。分業は必要だ。しかし、分業の効率は「誰にとっての効率か」を問う必要がある。会議運営は効率化される。意思決定者の認知負荷は下がる。しかし、その恩恵を受けるのは会議を設計する側であり、しわ寄せを受けるのは境界で仕事をする人々だ。調整コスト、手戻り、後から判明する「言ってくれれば」。これらは、分業の効率がもたらす隠れたコストだ。しかし、その効率の代償として、境界の情報が消える。消えるのは「事実」ではない。事実は、それぞれの会議で語られている。「3ヶ月で新機能を出す」は事実だ。「今のアーキテクチャでは6ヶ月かかる」も事実だ。「その機能を作れるエンジニアがいない」も事実だ。消えているのは、事実と事実をつなぐ「前提」と「代替案」だ。「この技術的制約があるから、この事業戦略は実行不可能だ」という前提。「機能を絞れば3ヶ月で出せる」という代替案。「採用が間に合わないなら、この部分は外注する選択肢もある」というリスク回避策。これらは、どの会議の議題にもならない。境界情報が消えたことは、どうすれば分かるか。沈黙で分かる。会議で「他部門への影響は？」と聞いたとき、誰も答えられない。手戻りで分かる。開発が進んでから「これ、最初に言ってくれれば」と言われる。会議の往復で分かる。月曜に決まったことが、水曜に覆り、金曜にまた覆る。責任の押し付けで分かる。「それは技術の問題」「それは事業の判断」「それは人事の話」。押し付けが始まったら、境界情報が消えている証拠だ。境界情報が生き残るケースもある。特定の人物が媒介しているときだ。複数の会議に出席し、それぞれの文脈を理解し、翻訳できる人。しかし、その人に依存すると、その人が異動したり退職したりすると、情報は途切れる。人ではなく、仕組みで残す必要がある。事業の会議で「技術的な制約と代替案」を議題に入れる。技術の会議で「事業インパクト」を必須項目にする。形式を変えなければ、境界情報は消え続ける。事業の会議では技術の話は「水曜に」と先送りされる。技術の会議では組織の話は「金曜に」と先送りされる。誰の責任でもないから、誰も語らない。分けた瞬間に、最も大事な情報が抜け落ちている。前回、私は「おい、戦略を語れ」と書いた。戦略とは「選択」であり、「何をやらないかを決めること」だと。目標を入れる。スローガンを入れる。希望を入れる。妥協を入れる。蓋を閉じて、「戦略」というラベルを貼る。それは戦略ではない、と。syu-m-5151.hatenablog.comしかし、書き終えてから気づいた矛盾がある。「何をやらないかを決める」には、「何ができるか」を知らなければならない。技術的に可能なことを知らなければ、事業戦略は選択できない。組織の能力を知らなければ、実行可能性は判断できない。戦略を語るためには、事業・技術・組織を「分けて」考えてはいけなかったのだ。今回は、その続きを書く。事業と技術と組織と戦略を、別々に語ることの危険性について。これは、そういう自分への苛立ちから始まった文章だ。私自身、無意識に分けていた。「事業のことは経営が決める」「技術のことは自分たちで決める」「組織のことは人事が決める」。それぞれの領分を侵さない。それが「プロフェッショナル」だと思っていた。しかし、それは本当にプロフェッショナルだったのか。単に、考えることから逃げていただけではないか。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。「何をやるか」と「どうやるか」の分離多くの組織で、こんな分業が成立している。経営が「何をやるか」を決める。開発が「どうやるか」を決める。事業戦略が「What」を定義し、技術戦略は「How」を担う。依存の矢印は「事業 → 技術」の一方向。経営会議で方針が決まり、それが開発チームに「降りてくる」。開発チームは、降りてきた仕様を実装する。きれいな分業だ。責任が明確だ。経営は経営の仕事をする。開発は開発の仕事をする。この分業が成立する前提がある。経営が「Howは後で決めればよい」と信じていることだ。Whatさえ決まれば、Howは技術者がなんとかする。技術は手段であり、事業の下流にある。そう信じている。技術側にも、この分業を受け入れる理由がある。Whatを所与として受け取る方が、楽だからだ。事業戦略に口を出せば、責任が生じる。「仕様通りに作りました」と言えば、失敗しても言い訳できる。Whatに関与しないことは、責任回避の手段でもある。さらに、評価制度がこれを強化する。技術者は「技術的な成果」で評価される。事業への貢献は、評価項目に入らないことが多い。しかし、この前提には反例がある。技術発で市場を作るケースだ。iPhoneは「タッチスクリーンでアプリが動く」という技術的可能性が、事業を規定した。AWSは「サーバーを時間単位で借りられる」という技術が、クラウドビジネスを生んだ。Howが先にあり、Whatが後から来た。組織能力が先に制約になるケースもある。「AIを活用したパーソナライゼーション」という戦略があっても、データサイエンティストがいなければ実行できない。Whatを先に決めても、Howの制約で実現不可能になる。「What → How」の一方向モデルは、現実を単純化しすぎている。そして、この単純化には実害がある。この分業は、組織の可能性を無意識に狭めている。なぜか。「どうやるか」が「何ができるか」を規定するからだ。例を挙げよう。あるプロダクトチームで、新機能の開発が議論されていた。経営会議で「この機能を3ヶ月で出す」と決まった。開発チームに降りてきた。しかし、開発チームは頭を抱えた。今のアーキテクチャでは、その機能を追加するのに6ヶ月かかる。密結合なモノリスで、特定の部分だけを変更することが難しい。開発チームは「3ヶ月は無理です、6ヶ月かかります」と報告した。経営は「なんとかしろ」と言った。開発チームは無理をした。品質を犠牲にした。技術的負債が積み上がった。次の機能追加は、さらに時間がかかるようになった。これは、珍しい話ではない。むしろ、日常的に起きている。問題は、どこにあるのか。開発チームの能力不足か。経営の無理解か。どちらでもない。問題は、「何をやるか」と「どうやるか」を分けて考えたこと自体にある。もし、アーキテクチャがモジュール化されていたら。特定の機能を切り出して、独立して開発できる構造になっていたら。3ヶ月で出せたかもしれない。あるいは、1ヶ月で出せたかもしれない。つまり、技術的な選択が、事業の選択肢を規定している。逆もまた真だ。技術が事業を規定するだけでなく、事業の方向性が、技術的な選択を正当化する。「このセグメントの顧客を取りに行く」という事業判断があるからこそ、「この部分をマイクロサービスとして切り出す」という技術判断が意味を持つ。事業の方向性なしに技術判断だけがあると、「なぜそのアーキテクチャなのか」が説明できない。事業と技術は、双方向に影響し合っている。一方向の依存関係ではない。私は以前、この双方向性を無視した組織をいくつか見てきた。どれも同じパターンに陥る。アーキテクトがアーキテクチャを設計する。マネージャーが組織を設計する。それぞれが、それぞれの会議で、それぞれの論理で。アーキテクトは「理想的なシステム構成」を描く。マネージャーは「効率的なチーム編成」を考える。両者が同席することはない。数ヶ月後、問題が起きる。チームAとチームBが、同じコードベースに手を入れる必要が出てくる。しかし、チームは別の部門に所属している。コミュニケーションパスがない。マージコンフリクトが頻発する。リリースの調整に時間がかかる。「なぜこんなことになったのか」と誰かが問う。答えは単純だ。組織の設計とアーキテクチャの設計が、別々に行われたからだ。コンウェイの法則を知っている人は多い。「組織構造がアーキテクチャに影響する」。私も何度も引用してきた。しかし、知っていることと、実践することは違う。私自身、何度もこの法則を引用しておきながら、組織設計に口を出すことは避けてきた。「それは自分の領域ではない」と。結果、アーキテクチャの提案が実装されないまま終わることが何度もあった。組織が変わらなければ、アーキテクチャは変わらない。当たり前のことだ。しかし、その当たり前を、私は見て見ぬふりをしていた。戦略と組織能力の不可分性同じ問題が、戦略と組織の間にもある。多くの企業が「何をやるか（戦略）」を先に決め、「誰がどうやるか（組織能力）」を後回しにする。戦略会議で、美しいスライドが映し出される。「我々は、AIを活用した次世代プラットフォームを構築する」。参加者はうなずく。ビジョンは明確だ。方向性は正しい。しかし、誰がそれを作るのか。今の組織に、その能力があるのか。ない場合、どうやって獲得するのか。採用か。育成か。外部委託か。それには、どれくらいの時間がかかるのか。これらの問いは、戦略会議では議論されない。「それは人事の話だから」と先送りされる。結果、戦略は「願望」に留まる。実行可能性を欠いた計画になる。私も、同じ過ちを犯したことがある。あるプロジェクトの戦略会議で、私は組織能力の話を一切しなかった。「それは人事部門の仕事だ」と思っていた。技術的には正しい方向だった。市場分析も悪くなかった。しかし半年後、プロジェクトは頓挫した。実行する人がいなかったのだ。戦略は正しかった。ただ、誰もそれを実現できなかった。これは、戦略と呼べるものではない。願望だ。「こうなったらいいな」を紙に書いただけだ。「誰が、どうやって、いつまでに」を書けないものは、戦略ではなく詩だ。良い戦略には、実行可能性が組み込まれている。「何をやるか」と「誰がどうやるか」は、同時に議論されなければならない。さらに厄介なのは、事業領域によって必要な組織能力が根本的に異なることだ。例えば、エンタープライズSaaSと、HRソリューションを考えてみよう。どちらも「SaaS」だ。しかし、勝ち方が違う。エンタープライズSaaSでは、競合との機能差が短期間で縮まる。だから、高速な同質化が勝敗を分ける。競合が出した機能を、素早く追随する。実装スピードが命だ。必要なのは、高速に開発できるエンジニアリング組織だ。一方、HRソリューションでは、顧客データを活用した差別化提案が価値の源泉になる。データサイエンスや顧客理解の深さが求められる。必要なのは、データを扱える人材と、顧客の業務を深く理解するドメインエキスパートだ。同じ「SaaS」でも、必要な開発スタイル、営業モデル、採用すべき人材像が根本的に異なる。戦略を語るなら、組織能力を語らなければならない。逆に、組織能力を語るなら、戦略を語らなければならない。私はかつて、ある企業の「AI戦略」を聞いたことがある。美しいスライドだった。「機械学習を活用してパーソナライゼーションを強化する」。しかし、その会社にはデータサイエンティストが一人もいなかった。「採用する予定です」と言っていた。一年後、まだ一人も採用できていなかった。戦略だけがあって、実行する能力がない。それは戦略ではない。絵に描いた餅だ。内製か外注かという問いの本質「内製すべきか、外注すべきか」。この問いも、事業・技術・組織を分けて考えると、間違った答えを出しやすい。技術の視点だけで考えると、「今のチームにその技術がないから、外注しよう」となる。合理的に見える。今持っていない能力を、外部から借りる。効率的だ。しかし、外部委託は「今持っていない能力を一時的に借りる」行為だ。短期的な補完にすぎない。問題は、勝ち筋を支える中核能力は、外部から買えないことだ。中核能力（コアコンピタンス）は、試行錯誤を通じて組織に蓄積される。失敗から学び、改善を重ね、暗黙知が形成される。外注で失われるのは「何をしたか」ではない。コードを見れば「何をしたか」は分かる。失われるのは「なぜそうしなかったか」の記憶だ。例えば、「なぜこのAPIはRESTではなくgRPCにしたのか」。コードを見れば「gRPCを使っている」ことは分かる。しかし、「最初はRESTで作ったが、レイテンシ要件を満たせず、2週間かけてgRPCに移行した」という経緯は、ドキュメントには残りにくい。「RESTでも工夫すれば動いたが、将来のスケーラビリティを考えてgRPCにした」という判断の背景は、人の頭にしか残らない。外注先の担当者が変わったとき、この記憶は消える。次に同じ判断を迫られたとき、組織はまた2週間を失う。「今回は外注で」を繰り返すと、何が起きるか。組織の中に何も残らない。プロジェクトは完了する。成果物は納品される。しかし、それを作る能力は、組織の中にない。次に同じようなことをやろうとすると、また外注することになる。これは、競争優位の源泉を自ら手放していることに等しい。もちろん、すべてを内製する必要はない。競争優位に関係ない部分は、外注でいい。しかし、「この能力が勝ち筋を支える」と判断したなら、時間がかかっても内製すべきだ。この判断をするには、事業戦略と技術戦略と組織戦略を、同時に見る必要がある。では、「同時に見る」とは具体的にどういうことか。ここで参考になるのが、ドメイン駆動設計の考え方だ。「一緒に変わる概念は、一緒にしておけ」。新機能を追加する際、どの概念が連動して変化するか。それらを1つのまとまりとして整理すると、それがドメインになる。逆に言えば、一緒に変わる概念を別々のチームに分けると、調整コストが爆発する。事業戦略：どの市場で、どう勝つのか技術戦略：そのためにどんな技術的優位性が必要か組織戦略：その優位性を支える能力を、どう構築・維持するかこれら3つは、一緒に変わる。事業戦略が変われば、技術戦略も変わる。技術戦略が変われば、必要な組織能力も変わる。一緒に変わるものを、別々の会議で、別々の人が議論していては、正しい判断はできない。技術が事業の選択肢を創出する技術の側から見ると、この関係はより具体的に見える。優れたアーキテクチャは、事業の選択肢を増やす。例えば、こんな状況を考えてみてほしい。競合が新機能をリリースした。市場が反応している。うちも追随したい。普通なら半年かかる開発だ。しかし、うちのシステムはモジュール化されている。既存のモジュールを組み合わせれば、1ヶ月で検証できる。これは、技術的な選択が、事業の機動性を生んでいる。別の例。ある顧客セグメント向けに、機能を絞った廉価版を出したい。普通なら、別プロダクトとして作り直す必要がある。しかし、うちのシステムは機能がモジュール化されている。特定のモジュールだけを切り出して、別プランとして販売できる。これは、技術的な選択が、事業モデルの柔軟性を生んでいる。逆に、技術的負債が蓄積すると、事業の選択肢が狭まる。新機能を追加したい。しかし、コードが複雑すぎて、どこを触ればいいか分からない。影響範囲が読めない。テストがない。触ると壊れる。結果、機能追加のコストが指数関数的に増大する。「やりたいけど、できない」が増えていく。事業の選択肢が、技術的負債によって奪われていく。では、アーキテクチャとは何のためにあるのか。「コードを綺麗にする」ためではない。「事業の機動性を高める」ための戦略的投資だ。ただし、「機動性」という言葉は曖昧だ。事業側が腹落ちするには、もっと具体的に語る必要がある。事業側に説明するとき、「モジュール化しました」では伝わらない。「この投資によって、競合が新機能を出したとき、追随までの期間が6ヶ月から2ヶ月に縮まります」と言えばいい。「この機能を落としたとき、他に影響が出ないので、撤退判断が3ヶ月早くできます」と言えばいい。時間の話をする。機会損失の話をする。撤退の容易さの話をする。経営が理解できる言葉で語る。——と書いて、立ち止まる。これは本当だろうか。私はこれまで、事業の言葉で語ってきただろうか。「このコードは密結合で」「テストカバレッジが低くて」と言い続けてきたのではないか。分かりやすく整理しているが、自分ができていなかったことを、さも正解のように語るのは、どうなのか。それでも、書く。できていなかったからこそ、書く。この視点がないと、アーキテクチャの議論は「技術者の自己満足」に見えてしまう。経営からすると、「なぜそんなことに時間をかけるのか」となる。技術的負債の返済は後回しにされる。結果、事業の選択肢がどんどん狭まる。忘れられない言葉がある。以前、一緒に仕事をしたCEOが言った。「エンジニアはみんな潔癖性だ。いつも技術的負債だの、システムの書き直しだのと言っている」。正直、腹が立った。しかし、同時に、自分のことを言われている気もした。私は技術的負債の説明をするとき、どんな言葉を使っていただろうか。「このコードは密結合で」「テストカバレッジが低くて」「デプロイパイプラインが」。技術者には通じる。しかし、経営者には通じない。通じないどころか、「また技術の話か」と思われていたかもしれない。そのCEOの言葉は、不当だった。しかし、そう思われてしまう構造を作ったのは、私たちでもある。「技術的負債を返済すると、機能追加のスピードが2倍になります」と言えばよかった。「このリファクタリングで、新規市場への参入が3ヶ月早まります」と言えばよかった。技術の話を、事業の言葉で語る。それができていなかった。技術と事業を分けて考えているから、こうなる。いや、違う。私が、分けて語っていたから、こうなった。現場が握る「隠れた変数」ここまで、組織のレイヤーで話をしてきた。しかし、組織が動くのは、個人が動くからだ。組織構造を変えても、個人が動かなければ、何も変わらない。次は、個人のレイヤーで話をしよう。現場でコードを書くエンジニアは、プロダクトの「手触り」を一番知っている。「今のアーキテクチャなら、実はこんな機能も低コストで実現できる」「これだけのものを作るには、一度基盤を整備してから一気に作ったほうが速い」「この部分を先に切り出しておけば、将来の拡張が楽になる」これらは、経営会議では見えない。事業戦略のスライドには載らない。現場だけが知っている「隠れた変数」だ。しかし、多くのエンジニアは、これを声に出さない。「事業のことは経営が決めること」「自分の仕事は、決まった仕様を実装すること」「事業戦略に口を出すのは、越権行為だ」無意識に、自分の領分を技術領域に限定してしまう。事業戦略を「所与のもの」「固定された定数」と捉えてしまう。しかし、現場からのインサイトが、経営会議の決定をひっくり返すべき場面がある。「その機能、今のアーキテクチャだと6ヶ月かかりますが、この部分を先にリファクタリングすれば、3ヶ月で出せます。しかも、その後の機能追加も速くなります」これは、事業判断を変えうる情報だ。しかし、エンジニアが「自分の仕事は実装だけ」と思っていたら、この情報は経営に届かない。エンジニアが技術戦略を磨くことは、経営に対して「このルートならもっと速く、高く登れる」という登山ルートを提案することだ。降りてきた仕様をこなすだけの存在ではない。事業の未来を提案する「シンクタンク」であり、それを最速で形にする「実行部隊」だ。しかし、この意識を持つことは、簡単ではない。なぜなら、「実装担当」に留まる方が、楽だからだ。私自身、長い間「実装担当」に留まっていた。事業戦略は「上」が決めるもの。自分の仕事は、降りてきた仕様を高品質に実装すること。そう思っていた。なぜか。その方が楽だからだ。事業戦略に口を出さなければ、責任を取らなくていい。「仕様通りに作りました」と言えば、失敗しても自分のせいではない。しかし、その「楽さ」には代償がある。自分の仕事の意味を、他人に委ねることになる。「なぜこれを作るのか」を自分で理解していないまま、コードを書く。モチベーションが上がらない。「作れと言われたから作った」。それは、職人の仕事ではない。ある時期から、変えようとした。事業戦略の文書を読むようになった。経営会議の議事録を見せてもらうようになった。「なぜこの機能が必要なのか」を、実装前に質問するようになった。最初は煙たがられた。「エンジニアがビジネスに口を出すな」という空気を感じたこともある。しかし、続けていると、変わってくる。「この機能、技術的にはこうすればもっと早く出せますが、どうしますか」という会話ができるようになる。事業判断に、技術的な選択肢を提供できるようになる。降りてくる仕様を待つ存在から、仕様を一緒に作る存在に変わる。エンジニア以外にも当てはまるここまでエンジニアの話をしてきた。しかし、「現場の知見が経営に届かない」という構造は、エンジニアに限った話ではない。専門性を持つ人が、その専門性ゆえにサイロに閉じ込められる。この構造は、あらゆる専門職に当てはまる。デザイナーの話をしよう。私が一緒に仕事をしたデザイナーに、Aさんという人がいた。Aさんは、ある機能のモックを見た瞬間、「これ、誰も使わないですよ」と言った。私は「なぜ分かる」と聞いた。「導線が3クリック深い。離脱します」。彼女は正しかった。リリース後、その機能の利用率は5%だった。しかし、Aさんはその機能の企画会議に呼ばれていなかった。「UIの話は後で」と言われていたのだ。後で呼ばれたときには、もう要件は固まっていた。Aさんにできたのは、決まった要件を「見やすく」することだけだった。本質的な導線の問題は、触れられなかった。PMも、セールスも、カスタマーサクセスも、同じ構造の中にいる。それぞれが、顧客や市場の「手触り」を知っている。しかし、その知見が意思決定の場に届くことは稀だ。届いたとしても「参考情報」として扱われ、決定を覆すことはない。専門性を持つ人が、事業戦略に影響を与えられる。これが、本質的な構造だ。しかし、多くの組織では、専門性が「サイロ」に閉じ込められている。デザイナーはデザインの会議に出る。PMはPMの会議に出る。それぞれが、自分の領域だけを語る。経営会議には呼ばれない。呼ばれたとしても、「報告」のためだ。「意思決定」のためではない。分けているから、全体が見えない。見えている人の声が、届かない。不確実性を飼いならすための対話事業と技術と組織を統合して考える理由は、もう一つある。不確実性への対処だ。どんな計画も、想定通りには進まない。技術的な挑戦には、不確実性がつきまとう。「想定より時間がかかる」「パフォーマンスに問題が出る」「思った通りに動かない」。これらは、避けられない。問題は、この不確実性が顕在化したときに、どう対処するかだ。事業と技術を分けて考えていると、こうなる。技術側で問題が起きる。開発が遅れる。技術チームは「なんとかします」と言う。無理をする。品質を犠牲にする。それでも間に合わない。最後の最後で「すみません、間に合いません」と報告する。事業側は怒る。「なぜもっと早く言わなかったんだ」。これは、フィードバックループが壊れている状態だ。あるべき姿は、こうだ。技術側で問題が起きる。その事実を、即座に事業側にフィードバックする。「この技術的課題があります。対処には追加で2ヶ月かかります」。事業側は、その情報を受けて、戦略を再検討する。「2ヶ月遅れるなら、機能を絞って先に出そう」「このセグメントは後回しにして、別のセグメントを先に取りに行こう」。技術の不確実性が、事業戦略を動的に更新する材料になる。私が見てきた中で、うまく機能していたフィードバックループには、ある傾向があった。問題が判明したら、「なんとかなるかもしれない」で抱え込まずに、早めに共有していた。中間管理職を経由すると情報が歪むので、技術リーダーが直接、事業の意思決定者に伝えていた。「遅れます」だけでなく、理由と代替案もセットで。——もっとも、これがどの組織でも通用するかは分からない。「直接伝える」が政治的に難しい組織もある。それでも、報告した人を責めない文化がなければ、どんな仕組みも機能しない。「遅れます」と言った人を責めた瞬間、次から「遅れます」は聞けなくなる。責めるほど、嘘が増える。これは、事業側にも当てはまる。市場環境が変わる。競合が予想外の動きをする。顧客のニーズが変化する。これらの情報は、技術側の優先順位を変えうる。「この機能は後回しでいい。代わりに、こっちを急いでほしい」。計画は「確定したもの」ではない。「継続的に更新されるもの」だ。しかし、実際には多くの組織が計画を「約束」として扱う。「3ヶ月後にリリースすると言ったじゃないか」。この言葉が出た瞬間、計画は更新できなくなる。変えることは「約束を破ること」だから。分けて考えていると、それぞれが自分の計画を守ろうとする。変化に抵抗する。結果、組織全体が硬直化する。「計画＝約束」になってしまうのは、なぜか。一つは評価制度だ。「計画通りに達成したか」で評価される。計画を変更すると、達成率が下がる。だから、変更を避ける。計画通りに進めて、最後に「外部要因で達成できませんでした」と言う方が、評価上は有利になる。もう一つは顧客へのコミット構造だ。「この機能を3ヶ月後に提供します」と顧客に約束している。変更すると、顧客との信頼関係に影響する。だから、社内の計画変更が許されない。さらに文化の問題もある。「一度決めたことは守る」が美徳とされる組織では、計画変更は「意志が弱い」と見なされる。合理的な理由があっても、変更を言い出しにくい。これらを変えるには、評価制度を「計画通り」ではなく「成果」で評価する。顧客へのコミットを「機能」ではなく「価値」でする。文化として「計画変更は適応であり、失敗ではない」と認める。簡単ではないが、ここが変わらないと、フィードバックループは機能しない。分離が生む「責任の空白地帯」分けて考えることには、もう一つの弊害がある。責任の空白地帯が生まれる。こんな状況を想像してほしい。新プロダクトがローンチした。しかし、売れない。事業側は言う。「プロダクトの品質が悪い。技術の責任だ」。技術側は言う。「要件が曖昧だった。事業の責任だ」。組織側は言う。「人が足りなかった。採用が追いつかなかった」。誰も責任を取らない。責任が、部門の境界に落ちている。責任の空白地帯は、悪意から生まれるのではない。制度が再生産している。KPIを見てほしい。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。「事業と技術の連携」を評価する指標は、どこにもない。予算も同じだ。事業予算と技術予算は別に管理される。「境界をまたぐ問題」に使える予算は、どちらの予算からも出しにくい。稟議も同じだ。事業の稟議と技術の稟議は、別のルートを通る。「両方に関わる案件」は、どちらのルートでも通りにくい。空白地帯で起きる典型的な現象がある。なすりつけ——「それは技術の問題だ」「いや、要件が曖昧だった」。回避設計——対話を避けるために、技術的な回避策を作る（後述するフロントエンドチームのように）。冗長な仕組み——同じ情報を、事業用と技術用に別々に管理する。二重管理——両方の部門が同じことを別々にやる。私が技術顧問として入った、ある会社の話をしよう。その会社には、二つのチームがあった。ユーザー向けのウェブサイトを担当するフロントエンドチーム。社内向けAPIを運用するプラットフォームチーム。フロントエンドチームはマーケティング部門に所属していた。プラットフォームチームはIT部門に所属していた。部門が違う。上司が違う。KPIも違う。両チームの関係は、最悪だった。会話がない。Slackのやりとりも最小限。必要なときだけ、冷たいチケットが飛ぶ。私は最初、技術的な問題だと思っていた。APIが遅い。エラーが多い。パフォーマンスチューニングをすれば解決する、と。しかし、掘り下げていくと、違った。問題は、技術ではなく、人間関係だった。発端は、一年前のインシデントだった。ウェブサイトがダウンした。原因はAPIの障害。しかし、経営陣はフロントエンドチームを責めた。「なぜ監視していなかったのか」「なぜ障害を検知できなかったのか」。プラットフォームチームには、何も言わなかった。フロントエンドチームは、怒った。自分たちのせいではない。しかし、責められた。プラットフォームチームとは、もう協力したくない。そこで、彼らは技術的な解決策を選んだ。APIからデータを抽出し、自分たちのデータベースに保存する仕組みを作った。「あいつらに依存しなければ、責められずに済む」。私は、その設計図を見て、頭を抱えた。データの同期処理。キャッシュの整合性チェック。障害時のフォールバック。複雑なシステムが、一枚の紙に描かれていた。これは、技術的な解決策ではない。人と話したくないから、システムを複雑にしているだけだ。案の定、問題は悪化した。データの不整合が起きる。「商品の価格がウェブサイトと管理画面で違う」というクレームが来る。フロントエンドチームは「プラットフォームチームのデータがおかしい」と言う。プラットフォームチームは「フロントエンドの同期処理がおかしい」と言う。責任のなすり合い。問題の根本は、誰も見ていない。私は、両チームのリーダーを同じ部屋に呼んだ。最初は気まずかった。しかし、一時間ほど話していると、お互いの不満が見えてきた。フロントエンドチームは「APIが遅いから、ユーザー体験が悪くなる。それで自分たちが責められる」と言った。プラットフォームチームは「APIの改善を提案しても、予算がつかない。経営はフロントエンドばかり見ている」と言った。両チームとも、被害者意識を持っていた。そして、その被害者意識が、アーキテクチャに反映されていた。話したくないから、システムを分ける。責任を取りたくないから、境界を作る。その結果、システムは複雑になり、問題が増え、さらに話したくなくなる。悪循環だ。これは、分けて考えることの必然的な帰結だ。私はこのエピソードを振り返るたびに、あるパターンに気づく。両チームとも、悪意があったわけではない。むしろ、それぞれが合理的に行動した結果、全体としてはうまくいかなくなった。これは、私だけが見た現象ではない。組織論では「構造的無能化」と呼ばれる。組織の考えたり実行したりする能力が、合理的に下がっていく現象だ。成熟した組織にとって、ほとんど宿命のようなものだ。そのメカニズムはこうだ。まず断片化が起きる。分業化が進み、縦割りになる。次に不全化が起きる。変化の兆しを察知しても、自分の領域ではないから動けない。最後に表層化が起きる。問題の根本に手を付けられず、場当たり的な対応を繰り返す。フロントエンドチームがデータベースを作ったのは、まさにこの表層化だ。根本的な解決（チーム間の対話）を避け、技術的な回避策（自前のDB）を選んだ。事業戦略は事業部門の責任。技術戦略は技術部門の責任。組織戦略は人事部門の責任。それぞれが、自分の領域だけに責任を持つ。しかし、成果は、領域の境界で生まれる。プロダクトが売れるかどうかは、事業戦略だけでは決まらない。技術戦略だけでも決まらない。組織戦略だけでも決まらない。三つが噛み合って、初めて成果が出る。分けて考えていると、この「噛み合わせ」に誰も責任を持たない。それぞれが自分の領域を最適化しようとする。しかし、部分最適の総和は、全体最適にならない。さらに、分けることは認知負荷を増やす。私がよく見るのは、本来は不要な調整、整合性の確認、コミュニケーションのオーバーヘッドだ。フロントエンドチームがプラットフォームチームと話すためだけに、週に2時間の会議を設定する。その会議で話す内容は、同じチームなら5分で済む。分けたことで、本質的でない仕事が増える。では、誰が全体を見るのか。CEOか。CTOか。プロダクトマネージャーか。私の経験では、肩書きは関係なかった。大事なのは「誰が」ではなく「どうやって」だった。全体を見る「人」を任命するより、全体を見る「機会」を作る方が効果的だった。むしろ、肩書きに頼ると失敗する。「それはCTOの仕事だ」「それはCEOが考えること」。そう言って誰も動かない組織を、何度も見てきた。私が見た中でうまくいっていた組織には、一つの習慣があった。月曜の朝会で、各チームが「先週、他チームから聞いた話」を30秒で共有する。「営業チームから聞いたんですが、顧客がこの機能の使い方で困っているらしいです」「インフラチームから聞いたんですが、このAPIの負荷が想定の3倍らしいです」。内容は何でもいい。「他チームから聞いた」という形式が重要だ。強制的に越境させる。最初は形式的だった。「特にありません」で済ませる人もいた。しかし、3ヶ月ほど続けると変わってくる。「あ、それ、前に営業の人が言ってましたよね」という会話が、会議の外で自然に生まれる。6ヶ月後には、「他チームから聞いた話」を集めるために、意識的に他チームと話すようになる。全体を見る「人」を任命するより、全体を見る「機会」を作る方が、よほど効果的だった。ここで、一つの反論に答えておきたい。「統合しても、責任は明確にならず、むしろ曖昧になるのではないか」という懸念だ。これは、正しい。統合すると「みんなで決めた」になって、誰も責任を取らない別種の無責任が生まれる。「合議の免責」だ。境界があると「ここから先はこの責任者」と決めやすい。統合は、その明確さを失わせる。私が見てきた組織でも、「全員で議論して、全員で決めた」結果、うまくいかなかったとき誰も責任を取らなかったケースがある。だから、越境と責任の明確化は、両立させなければならない。統合して議論するが、決定は一人が行う。「みんなで決めた」ではなく、「みんなの情報をもとに、この人が決めた」にする。——言うのは簡単だ。しかし、これを実践するのは難しい。最終決裁者を決めると、「自分の意見が通らなかった」と不満を持つ人が出る。合議の方が、波風が立たない。だから、組織は合議に流れる。それでも、誰かが決めなければならない。決めた人が責任を持つ。これを曖昧にすると、分断と同じ問題が起きる。越境するということでは、どうすればいいのか。自分の領域を超えて、考える。発言する。それぞれが、自分の専門性を持ちながら、他の領域にも関心を持つ。これは、「全員がゼネラリストになれ」という話ではない。専門性は維持したまま、境界を越えて対話するということだ。ここで、よくある誤解に触れておきたい。越境を推しすぎると、専門性が薄まるのではないかという懸念だ。全員が「半分だけ分かる人」になり、技術の判断が雑になり、事業の理解が単純化され、組織の問題はスローガン化する。この懸念は正当だ。私自身、越境を意識するようになってから、技術の深い部分に割く時間が減った気がする。最新のフレームワークを追う余裕がなくなった。コードを書く時間が減った。「全体を分かる」ことと「専門が深く鋭い」ことは、トレードオフの関係にある。両方を同時に極めることはできない。だから、越境とは「専門性を捨てる」ことではない。エンジニアがビジネスを学ぶのは、ビジネスの専門家になるためではない。自分の技術的判断を、ビジネスの文脈で説明できるようになるためだ。2割の時間で、他の領域で何が起きているかを知る。それだけで、残り8割の専門領域の判断が変わる。——と言いながら、私は本当に8:2でやれているだろうか。正直、分からない。越境に時間を使いすぎて、技術の深さが失われていないか。その不安は、常にある。私が見てきた中で、うまくいっているチームは「独立」と「孤立」を混同していなかった。自分たちで決められることは自分たちで決める。しかし、他チームとの依存関係は認識している。必要なときは、ちゃんと話す。分けることは、孤立させることではない。依存関係を認識し、意図的に管理することだ。問題は、分けた瞬間に依存関係を忘れてしまうことにある。エンジニアは、エンジニアリングの専門家であり続ける。しかし、その専門性を、事業の文脈で語れるようになる。「このアーキテクチャは技術的に美しい」ではなく、「このアーキテクチャは事業の機動性を高める」と語る。事業担当は、ビジネスの専門家であり続ける。しかし、技術的な制約と可能性を理解する。「なぜできないのか」ではなく、「どうすればできるのか」を一緒に考える。ただし、ここで一つ注意がある。「越境」と聞くと、「相手の言葉で話せばいい」と考えがちだ。技術者なら経営の言葉を学び、経営者なら技術の言葉を学ぶ。それ自体は悪くない。しかし、「すべてを一つの言葉に翻訳しろ」と言っているのではない。すべてを財務言語や経営の言葉に翻訳すると、技術的な微妙なニュアンスが失われる。「このAPIのレイテンシが50msから200msに悪化する」を「ユーザー体験が悪化する」と翻訳すれば、経営には通じるかもしれない。しかし、50msと200msの差がどれほど深刻か、どのユースケースで問題になるか、という技術的な判断の根拠は消える。組織の話も同じだ。「チームの心理的安全性が低い」を「生産性が下がっている」と翻訳すれば、経営指標には載る。しかし、なぜ安全性が低いのか、誰と誰の間に問題があるのか、という組織特有の文脈は失われる。事業の話も同じだ。「この顧客セグメントは価格感度が高い」を「値下げすれば売れる」と翻訳すれば、技術チームには分かりやすい。しかし、なぜ価格感度が高いのか、競合との関係はどうか、という市場の文脈は消える。翻訳は、情報を圧縮する行為だ。圧縮すれば、必ず何かが失われる。翻訳で失われてはいけない情報がある。判断の根拠だ。「このAPIは遅い」は翻訳できる。「ユーザー体験に影響がある」と。しかし、「なぜ遅いのか」「どのユースケースで問題になるのか」「どうすれば速くなるのか」という判断の根拠は、翻訳で消えやすい。翻訳の失敗には典型パターンがある。数値の意味が変わる——「50msから200msに悪化」が「ちょっと遅くなる」に翻訳される。因果が消える——「テストがないからリリースに時間がかかる」が「リリースが遅い」に短縮される。責任が曖昧になる——「この設計判断には、こういうトレードオフがあった」が「技術的な事情」に丸められる。対策は二重記録だ。経営向けの短い翻訳と、技術や組織の原文脈を、両方残す。経営会議の資料には「ユーザー体験に影響がある」と書く。同時に、技術的な詳細は別のドキュメントに残し、参照できるようにする。翻訳は「圧縮」だが、原文を捨てる必要はない。「詳細はこちら」というリンクがあればいい。だから、越境とは「自分の言葉を捨てて、相手の言葉で話す」ことではない。自分の言葉を保ちながら、相手の言葉も理解することだ。技術者が経営の言葉を学ぶのは、自分の技術的判断を捨てるためではない。技術的判断を、経営が理解できる形で伝えるためだ。そして同時に、経営の言葉で語られた制約を、技術的な文脈に引き戻して考えるためだ。専門性を持った人たちが、それぞれの専門性をリスペクトしながら、共通のゴールに向かって越境し合う。それは、言語を統一することではない。複数の言語が交差する場所で、対話することだ。私はあるとき、同じ会議で同じ資料を見ていたエンジニアと営業が、まったく違う結論を出すのを見た。エンジニアは「これは技術的に難しい」と言い、営業は「これは売れる」と言った。同じ資料だ。同じ数字だ。しかし、見えている世界が違う。それぞれが、自分の専門性に基づいた「解釈の枠組み」で見ている。エンジニアはコードの複雑さを見ている。営業は顧客の反応を見ている。どちらも正しい。しかし、見ているものが違う。組織の中で起きている「わかりあえなさ」は、この解釈の枠組みの間に溝ができていて、しかもそのことに気づいていない状態だ。溝があることに気づかないから、「なぜ分からないんだ」「なぜ伝わらないんだ」と苛立つ。対話とは、溝を埋めることではない。溝に橋を架けることだ。溝は前提としてあり続ける。エンジニアと営業の見ている世界は、完全には一致しない。しかし、橋があれば、行き来できる。相手の世界を訪れ、自分の世界に戻ってくる。その往復が、越境だ。情報が流れる仕組み「越境せよ」と言うのは簡単だ。しかし、越境するには材料がいる。相手の世界で何が起きているかを知らなければ、質問も提案もできない。越境するためには、情報が流れる仕組みが必要だ。多くの組織で、情報は縦に流れる。経営から現場へ。現場から経営へ。しかし、横には流れにくい。技術チームで起きていることは、事業チームには見えない。事業チームで議論されていることは、技術チームには届かない。それぞれが、自分のサイロの中で仕事をしている。これを変えるには、意図的な設計が必要だ。ここで、別のアプローチに触れておきたい。「常に一緒に話す」のではなく、境界を前提にしたインターフェース（契約）を設計すればいいのではないかという考え方だ。依存関係は消せない。ならば、「越境」より「情報の受け渡しの形式化」を磨くべきだ、という主張だ。制約（SLO、人員、期限）を明文化し、変更時の通知ルールを作る。「同席して統合」より「契約で連携」の方がスケールする、と。この考え方には一理ある。いや、一理どころか、実際に機能している組織を見てきた。境界を明確にし、インターフェースを定義し、変更時の通知ルールを作る。日常的な連携は、これで十分だ。私自身、あるプロジェクトでAPI仕様書とSLOを明文化しただけで、チーム間の調整コストが激減するのを見た。ただ——問題は、インターフェースに書かれていない依存関係が発見されたときだ。「この事業判断が、技術に影響するとは思っていなかった」。そういう場面で、インターフェースは機能しない。「期限が1ヶ月延びました」という通知は届く。しかし、「なぜ延びたのか」「その延長が事業にどう影響するのか」は、インターフェースでは伝わらない。契約に書かれていない依存関係は、対話でしか発見できない。だから、どちらか一方ではない。日常的にはインターフェースで効率的に連携し、境界をまたぐ問題が発生したときや、計画変更があったときには、対話で補う。当たり前のことを言っているように聞こえる。しかし、この「当たり前」ができている組織は、驚くほど少ない。では、私自身はどうだったか。ある会社で、技術戦略会議に事業担当を呼んでもらったことがある。最初は「技術の話だから関係ない」と言われた。しかし、「アーキテクチャの選択が、3ヶ月後の機能追加速度に影響します」と説明したら、参加してくれた。その会議で、事業担当は「そんなに影響があるとは知らなかった」と言った。私は「そうなんです」と言った。そして、なぜ今まで呼ばなかったのかを考えた。面倒だったからだ。調整コストを払いたくなかったからだ。別の会社では、事業戦略のドキュメントを技術チームにも共有してもらうようにした。最初は「読まないでしょ」と言われた。確かに、読まない人もいた。しかし、読む人もいた。読んだ人が「この戦略なら、このアーキテクチャは合わない」と気づいた。それだけで、価値があった。これらは、コストがかかる。効率は下がるかもしれない。しかし、分断のコストを計算したことがあるだろうか。私が関わったあるプロジェクトでは、事業部門と技術部門の認識のずれを修正するのに、毎週2時間の会議を3ヶ月続けた。それでも修正しきれず、最終的に機能の30%を作り直した。作り直しにかかった工数は、最初から一緒に話し合っていれば発生しなかったものだ。分断は「効率的」に見える。しかし、その効率は幻想だ。後から払うコストを、先送りしているだけだ。なぜ変われないのかここまで読んで、「分かった、越境しよう」と思った人もいるかもしれない。しかし、それだけでは変わらない。私自身、何度も経験してきた。「越境すればこんなにいいことがある」と説明した。「対話すれば問題が解決する」と語った。しかし、魅力的な提案が受け入れられないのは、魅力が足りないからではなかった。相手が受け入れたくない理由があるからだ。ある会社では、「今のやり方で回っているのに、なぜ変える必要があるのか」と言われた。惰性だ。今のやり方に慣れている。変える理由がない。私はこの種の人たちを何度も見てきた。出世競争から降りて、自分のペースで仕事をこなし、昼休みには決まった仲間と弁当を食べ、定時に帰る。悪い人たちではない。むしろ、穏やかで、職場の潤滑油になっていることもある。しかし、変化の話をすると、途端に顔が曇る。「それ、本当に必要ですか」「今のままでも回っていますよね」。彼らにとって、改善か改悪かは問題ではない。変えること自体が脅威なのだ。長い時間をかけて築いた居場所。慣れた仕事の流れ。気心の知れた同僚との関係。今の自分を成り立たせているものが、壊されるかもしれない。その不安が、あらゆる変化への抵抗になる。私は最初、この人たちを「抵抗勢力」だと思っていた。説得すれば分かってくれる、と。しかし、あるとき気づいた。彼らの反応は、合理的だ。変化のコストを払うのは彼らだ。新しいやり方を覚える。慣れた関係が崩れる。評価の仕方が変わる。一方、変化の恩恵を受けるのは誰か。たいてい、変化を推進した側だ。「改革を成功させた」と評価されるのは、推進者だ。コストを払う人と恩恵を受ける人が違う。それなら、抵抗するのは当然ではないか。私が「越境しよう」と言うとき、そのコストを誰が払うのか。私ではない。現場の人たちだ。私はその認識が、ずっと欠けていた。別の会社では、「それをやる余裕がない」と言われた。労力だ。相手の言葉を学ぶコスト、会議を調整するコスト、認識のずれを修正するコスト。その余裕がない。また別の会社では、「あなたに何が分かる」と言われた。感情だ。専門性へのプライド。自分の領域に口を出されることへの反発。そして、「上から押し付けられた」と感じた瞬間、内容に関係なく拒否される。心理的反発だ。これが一番厄介だった。魅力をいくらアピールしても、これらの抵抗があれば動かない。魅力をいくら積み上げても、抵抗が1つあれば動かない。掛け算のどこかにゼロがあれば、答えはゼロだ。むしろ、魅力を強調するほど、「押し付けられている」という反発が強まることすらある。私の経験では、最も強い抵抗は心理的反発だった。「誰が言うか」の問題だ。同じ内容でも、言う人によって受け入れられ方が違う。外部の人間が言うと「現場を知らないくせに」と反発される。内部の人間が言うと「自分の領域を広げようとしている」と疑われる。拒否しているのは「内容」ではなく「手続き」や「語り手」であることが多い。内容に反論できないから、手続きに難癖をつける。「そのやり方は聞いていない」「なぜ事前に相談しなかったのか」。内容ではなく、プロセスで拒否する。抵抗を下げる最小の一歩は何か。会議に一人、他部門の人を呼ぶ。それだけでいい。最初は「オブザーバー」として。発言しなくてもいい。ただ、聞いているだけでいい。その一人がいることで、「他部門から見たらどう見えるか」を意識するようになる。それが、越境の始まりになる。だから、私は変えた。魅力を語る前に、抵抗を減らすことを考えるようになった。惰性を崩す小さな一歩を提案する。労力を下げる仕組みを作る。感情に配慮する。押し付けではなく、選択肢として提示する。それでも、うまくいかないことは多い。個人の努力では足りないここまで「越境せよ」と書いてきた。しかし、ここで立ち止まって、自分の主張を疑う必要がある。「越境せよ」は、誰に言っているのか。越境の権限がない人に言っても、負荷を増やすだけだ。越境が評価されない構造で「越境せよ」と言っても、個人を疲弊させるだけだ。私はこの記事で、分断の弊害を語り、越境の価値を語ってきた。しかし、その主張が新たな「べき論」になるリスクがある。「越境すべき」「事業の言葉で語るべき」「全体を見るべき」。これらは、個人への要求だ。しかし、なぜ越境が難しいのか。それは構造の問題だ。評価制度を見てほしい。技術者は「技術的な成果」で評価される。事業への貢献は、評価項目に入らないことが多い。越境して事業に口を出しても、評価されない。むしろ、専門性が薄まったと見なされるリスクがある。KPIを見てほしい。事業部門は売上で評価される。技術部門はシステムの安定性で評価される。「境界をまたぐ貢献」を測る指標は、どこにもない。キャリアパスを見てほしい。技術者としてのキャリアは、技術を深めることで築かれる。越境に時間を使うと、専門性が薄まる。越境は、キャリアリスクになりうる。この構造がある限り、越境しない方が合理的だ。言い換えれば、越境は、評価されないボランティアだ。ボランティアに依存する組織は、ボランティアが疲弊した瞬間に壊れる。私が「越境せよ」と言うとき、暗黙のうちに個人の努力に頼っている。善意に頼っている。「組織のために」という献身に頼っている。しかし、善意は持続しない。献身は燃え尽きる。越境を個人に求めるだけでは足りない。越境が合理的になる構造を作らなければならない。具体的には何か。評価制度を変える。「境界をまたぐ貢献」を評価項目に入れる。技術者が事業に貢献したことを、技術的成果と同等に評価する。事業担当が技術的制約を理解し、計画に反映したことを評価する。KPIを変える。部門ごとのKPIだけでなく、「部門間の連携」を測る指標を作る。例えば、「他部門からのフィードバックを計画に反映した回数」「境界をまたぐ問題を早期に発見した件数」。キャリアパスを変える。越境経験を、キャリアの強みとして評価する。「技術も事業も分かる人」を、専門家と同等に評価する。予算を変える。「境界をまたぐ問題」に使える予算枠を作る。事業予算でも技術予算でもない、「連携予算」のようなもの。会議体を変える。月曜・水曜・金曜と分かれた会議を、定期的に統合する場を作る。全員が同席する必要はない。しかし、境界の情報が消えないための仕組みが必要だ。しかし、ここで自分の甘さを認めなければならない。私は以前、ある会社で評価制度の変更を提案したことがある。「越境的な貢献も評価項目に入れましょう」と。提案は通った。評価シートに「部門間連携」という項目が追加された。半年後、何が起きたか。項目は増えたが、行動は変わらなかった。みんな、その項目に「特になし」と書いて提出していた。形式だけが変わり、中身は何も変わらなかった。そのとき気づいた。部門を統合しても、会議体を変えても、中の人々が行動を変えなければ、結局、何も変わらない。当たり前のことだ。しかし、私は「構造を変えれば人が変わる」と信じていた。構造さえ整えれば、人は自然とその構造に沿って動くはずだ、と。甘かった。評価制度を変えても、その評価制度を運用する人が変わらなければ、何も変わらない。会議体を変えても、会議で発言する人の意識が変わらなければ、何も変わらない。構造は、行動を促すきっかけにはなる。しかし、行動を強制することはできない。では、構造を変えることに意味はないのか。そうではない。構造を変えることは必要だ。しかし、十分ではない。構造を変えると同時に、「なぜこの構造に変えるのか」を語り続ける必要がある。形式だけでなく、意味を伝える。それがなければ、新しい構造は空箱になる。——と書いて、これらを変える権限が自分にないことに気づく。私は技術顧問だ。評価制度を変える権限はない。KPIを変える権限もない。予算を変える権限もない。では、権限がない人は何もできないのか。そうではない。権限がなくても、できることはある。まず、構造の問題を言語化することだ。「越境が難しいのは、評価制度が越境を評価しないからだ」と言葉にする。問題を個人の能力や意欲の問題ではなく、構造の問題として語る。それだけで、議論の土俵が変わる。次に、小さな実験を提案することだ。評価制度全体を変えるのは難しい。しかし、「このプロジェクトでは、境界をまたぐ貢献も評価してみませんか」と提案することはできる。小さな実験が成功すれば、それを拡大できる。そして、越境できない自分を責めないことだ。越境が難しいのは、あなたの能力の問題ではない。構造の問題だ。構造が変わらない中で、できることをすればいい。ただし、ここで一つ、認めておくべきことがある。全員が越境する必要はない。越境できる人がいて、専門性を深める人がいて、それぞれが価値を出す。これも、一つの分業だ。全員が同じ行動をする必要はない。越境が得意な人が越境し、専門性を深めるのが得意な人が深掘りする。その組み合わせで、組織は機能する。「越境せよ」という主張が、「全員が越境すべき」という画一的な要求になってはいけない。越境しない人を「視野が狭い」と見なしてはいけない。専門性を深めることにも、価値がある。私がこの記事で言いたいのは、「全員が越境せよ」ではない。「越境が必要な場面で、越境できる構造を作れ」だ。私自身の反省正直に言えば、私自身、この分断に加担してきた。技術顧問として呼ばれる。「アーキテクチャについてアドバイスしてください」と言われる。私は、アーキテクチャについてアドバイスする。それが、私の仕事だと思っていた。しかし、アーキテクチャの問題は、純粋に技術的な問題であることは稀だ。組織の問題であり、事業の問題でもある。「なぜこのアーキテクチャになったのか」を掘り下げると、組織の歴史が見えてくる。「チームが分かれていたから、システムも分かれた」。「この部分は外注したから、ブラックボックスになった」。「この機能は急いで作ったから、技術的負債が溜まった」。技術の問題を、技術だけで解決しようとしても、うまくいかない。組織を変えなければ、技術は変わらない。事業の優先順位を変えなければ、技術的負債は返せない。私は、そこまで踏み込むことを避けてきた。「それは私の領域ではない」と。しかし、それは、本当に価値のある助言だったのか。踏み込まないことで、私は何を守っていたのか。関係性だ。嫌われたくない。次も呼ばれたい。だから、言いにくいことは言わない。契約範囲だ。「アーキテクチャのアドバイス」で呼ばれた。組織や事業の話は契約外だ。だから、踏み込まない。安心感だ。技術の話をしていれば、自分の専門領域にいられる。組織や事業の話をすると、自分が素人になる。だから、避ける。しかし、これらを守った結果、私のアドバイスは実行されないまま終わった。価値を提供できなかった。守ったものは、守る価値があったのか。最近は、少しずつ変えている。技術の話だけでなく、組織の話も、事業の話もする。「このアーキテクチャを実現するには、チーム編成を変える必要があります」「この技術的負債を返すには、事業側の優先順位を変える必要があります」。踏み込むときの言い方には、工夫がいる。診断として言う——「私の見立てでは、技術だけでなく組織の問題もあるように見えます」。断定ではなく、観察として伝える。仮説として言う——「もし組織構造を変えたら、アーキテクチャの変更がスムーズに進むかもしれません」。押し付けではなく、可能性として提示する。選択肢として言う——「技術だけで対処する方法と、組織も含めて対処する方法があります。どちらを選びますか」。決定権は相手に渡す。言いにくいことだ。領域を越えている。しかし、言わなければ、本当の解決にはならない。おわりに「おい、分けて語るな」。この言葉は、会議室の誰かに向けているようで、実は自分に向けている。この文章を読んでも、明日から「事業と技術と組織を統合して考えられる人」にはならない。私自身がそうだったから分かる。組織の分断は、風邪のような急性疾患ではない。薬を飲んで寝れば治る、というものではない。慢性疾患だ。劇的な手術で一気に治すことはできない。できるのは、セルフケア的なアプローチだ。小さなことから、少しずつ、継続的に変えていく。一気に問題を解決しようとは思わないこと。問題解決モードを抜け出し、対話モードで慢性疾患に向き合う。「分けて考えるな」。言葉では分かる。しかし、明日の会議で、実践できるか。経営会議で「それ、技術的にはこういう含意があります」と発言できるか。技術戦略会議で「それ、事業戦略とどう関係しますか」と質問できるか。正直、自信がない。質問した瞬間、場が凍りつくかもしれない。「また技術の話か」と思われるかもしれない。そう思うと、喉まで出かかった言葉を飲み込んでしまう。これまでも、そうだった。でも、一つだけ提案がある。次の会議で、一回だけ、こう質問してみてほしい。「この決定は、[別の領域]にどういう影響がありますか？」答えが返ってくればいい。返ってこなければ、それが発見だ。その沈黙は「分断がある」という証拠だ。私は過去に3回、この質問をした。1回目は無視された。2回目は「それは技術の話だから」と流された。3回目は違った。CTOが一瞬黙り、それから「いい質問だ」と言った。会議室の空気が変わるのを感じた。CTOはその場で技術責任者を呼んだ。30分後、経営会議に技術責任者が同席するという、その会社では初めてのことが起きた。3回目が、その会社の変化の始まりだった。1回で変わることは稀だ。しかし、質問しなければ、変わる可能性すらない。沈黙が3回続いたら、それは「この組織には分断がある」という診断結果だ。診断結果が出れば、次のアクションが見える。明日も会議がある。月曜は経営会議。水曜は技術戦略会議。金曜は組織開発会議。きれいに分かれている。整然としている。月曜の経営会議で、一つだけ、技術の話をしてみようと思う。「この事業戦略、技術的にはどういう制約がありますか」と。沈黙が流れるかもしれない。「それは水曜に」と言われるかもしれない。それでもいい。その沈黙こそが、分断の存在を証明している。そして、証明された瞬間から、修復は始まる。私が「分けて語らない」姿勢を貫くとき、最初に変わるのは何か。会議か。人か。成果物か。私の経験では、最初に変わるのは成果物だった。技術レビューに事業インパクトの項目を入れる。事業計画書に技術的制約の項目を入れる。アーキテクチャ決定記録（ADR）に「事業への影響」を必須にする。成果物の形式が変わると、それを作る過程で、自然と越境が起きる。次に変わるのは会議だ。技術の会議に事業担当を一人呼ぶ。事業の会議に技術担当を一人呼ぶ。最初はオブザーバーでいい。聞いているだけでいい。その一人がいることで、会議の空気が変わる。最後に変わるのは人だ。人の意識や行動は、簡単には変わらない。しかし、成果物の形式が変わり、会議の参加者が変わると、少しずつ変わっていく。「他の領域のことも考える」が、習慣になっていく。——と書いたが、本当にそうだろうか。成果物や会議が変わっても、人が変わらないケースも見てきた。形式だけ整えて、中身は変わらない。「事業インパクト」の欄に、適当なことを書いて終わり。そういう組織もある。それでも、形式から入るしかない。形式が変われば、少なくとも「考える機会」は生まれる。考えた結果、変わらない人もいる。変わる人もいる。変わる人が一人でもいれば、その人から広がる可能性がある。この声は、会議室の誰かにではなく、自分自身に向けられている。参考書籍アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazon戦略の要諦作者:リチャード・Ｐ・ルメルト日経BPAmazon良い戦略、悪い戦略 (日本経済新聞出版)作者:リチャード・Ｐ・ルメルト日経BPAmazon君は戦略を立てることができるか 視点と考え方を実感する４時間作者:音部大輔Amazonストーリーとしての競争戦略 Hitotsubashi Business Review Books作者:楠木 建東洋経済新報社Amazon戦略、組織、そしてシステム作者:横山 禎徳東洋経済新報社Amazon戦略のデザイン ゼロから「勝ち筋」を導き出す10の問い作者:坂田 幸樹ダイヤモンド社Amazon戦略コンサルの技術　70のスキームで身につける思考と分析力 (日本経済新聞出版)作者:長谷部 智也,河野 博日経BPAmazon確率思考の戦略論　どうすれば売上は増えるのか作者:森岡 毅,今西 聖貴ダイヤモンド社Amazonジョブ理論　イノベーションを予測可能にする消費のメカニズム作者:クレイトン・Ｍ・クリステンセンHarperCollins Children's BooksAmazon「ジョブ理論」完全理解読本 ビジネスに活かすクリステンセン最新理論作者:津田真吾,INDEEJapan翔泳社Amazonイノベーション・オブ・ライフ ハーバード・ビジネススクールを巣立つ君たちへ作者:クレイトン M.クリステンセン翔泳社Amazon繁栄のパラドクス 絶望を希望に変えるイノベーションの経済学 クレイトン・M・クリステンセン作者:クレイトン・M クリステンセンHarperCollins Children's BooksAmazonイノベーションの経済学　「繁栄のパラドクス」に学ぶ巨大市場の創り方作者:クレイトン・M・クリステンセンHarperCollins Children's BooksAmazonチームトポロジー 価値あるソフトウェアをすばやく届ける適応型組織設計作者:マシュー・スケルトン,マニュエル・パイス日本能率協会マネジメントセンターAmazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazonエンジニアリング組織論への招待　～不確実性に向き合う思考と組織のリファクタリング作者:広木 大地技術評論社Amazonプロダクトマネジメントのすべて 事業戦略・IT開発・UXデザイン・マーケティングからチーム・組織運営まで作者:及川 卓也,曽根原 春樹,小城 久美子翔泳社Amazon企業変革のジレンマ　「構造的無能化」はなぜ起きるのか (日本経済新聞出版)作者:宇田川元一日経BPAmazonイノベーションのジレンマ 増補改訂版 (Harvard Business School Press)作者:クレイトン クリステンセン翔泳社Amazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazonモチベーション革命　稼ぐために働きたくない世代の解体書作者:尾原 和啓AudibleAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraformerを使ってAWSのリソースをIaC化する]]></title>
            <link>https://zenn.dev/iorandd/articles/20260131_aws-how-to-use-terraformer</link>
            <guid isPermaLink="false">https://zenn.dev/iorandd/articles/20260131_aws-how-to-use-terraformer</guid>
            <pubDate>Fri, 30 Jan 2026 22:00:01 GMT</pubDate>
            <content:encoded><![CDATA[業務でTerraformerを使って既存のAWSリソースをTerraform管理下に移行する機会がありました。インフラに詳しいメンバーの知見を借りながら進めたのですが、その過程で学んだことが多かったので備忘として残します。https://github.com/GoogleCloudPlatform/terraformer 1. Terraformerとはクラウド上にTerraformで管理されていないリソースが残っていて困った経験は多くの人にあると思います。Terraform導入前に試験的に作ったリソースがそのまま残っている手動でコンソールから作成したリソースが把握しきれて...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIを使うのは当たり前になったけど、AIを使いたくないときがある]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</link>
            <guid isPermaLink="false">https://nnaka2992.hatenablog.com/entry/2026/01/30/005521</guid>
            <pubDate>Thu, 29 Jan 2026 15:55:21 GMT</pubDate>
            <content:encoded><![CDATA[ChatGPTが最初に発表されたときに比べ、生成AIはいわゆるハルシネーションも減ればコーディングも得意になり、情報の調査から整理まであらゆる用途で実用的になりました。普段のくだらない疑問をSNSに垂れ流していたのが、雑にChatGPTに聞くようになりました。少し真剣に考えたい仕事のトピックや会社の情報を入れる時には、取り敢えずGeminiに意見を求めることも少なくありません。コーディングにいたっては速さも質も私ではClaudeに勝てなくなりました。そんな便利な生成AIですが、自身のキャリアや目標、解釈ではなく理解したい技術を学ぶときは安易に生成AIを利用したくないと思っています。生成AIを利用することでインスタントにそれらしい出力を得ることができますが、よく語られるようにその質は現時点のインプットに従属します。キャリアや目標のような自分の中にしか答えがなく、向き合い磨くことでしか良くならいものがあります。こういったトピックに生成AIを利用すると短期的にそれらしい出力を得られますが、頭の片隅にしかない小さなアイデアの種は簡単に押しのけられてしまいます。うわべではなく、真に理解したい論文や技術を生成AIに頼ると、理解ではなく解釈に逃げてしまい、要約の周りにある削ぎ落とされた情報を自分のものにすることができません。生成AIが幅を利かせるようになるまでは、アウトプットこそ至高でアウトプットこそ正義と考えていました。最近は生成AIを使って言葉することで消えてしまう考えや、言葉にすることでこぼれ落ちてしまうアイデアの輪郭にこそ価値があるのではないかと思うようになりました。言葉にすれば消えちゃう関係なら言葉を消せばいいやって思ってた 恐れてただけど あれ? なんかちがうかも曲名 恋愛サーキュレーション作詞 meg rock作曲・編曲 神前 暁 (MONACA)歌 千石撫子(CV:花澤香菜)一昔前のオタクが大好きな恋愛サーキュレーションの真逆だなと思った冬の日です。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AIでやれる時代に、それでも誰かと何かをやるということ]]></title>
            <link>https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</link>
            <guid isPermaLink="false">https://zenn.dev/yuu0w0yuu/articles/9bdb2c6c4a8d75</guid>
            <pubDate>Thu, 29 Jan 2026 11:43:18 GMT</pubDate>
            <content:encoded><![CDATA[「AIを触り、感動する」という体験が個人的に一巡した感覚があるので、2026年を走り出すにあたり、思っていることを記録する。 ソロ登頂よりも、チーム敗退「うまくいかなくても、誰かと何かをやる」ということの価値が相対的に上がっているように感じる。ここでいう「誰か」は、当然GeminiでもChatGPTでもないし、「何かをやる」とは、Vibe Codingでソフトウェアを作ることでも、NotebookLMでそれっぽいスライドを生成することでもない。過去の自分の仕事の記憶。「どういうものをイメージしてるのか全く分からん」と内心上司にキレながらオフィスで過ごしたスライドレビューのワッペ...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ 2026年1月 Neovim の Rust 環境を見直した]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/130742</guid>
            <pubDate>Thu, 29 Jan 2026 04:07:42 GMT</pubDate>
            <content:encoded><![CDATA[はじめにgithub.com先週、エージェントが書いた200行のコードを開いた。move |ctx| { ... } というクロージャがあった。この ctx は何をキャプチャしているのか。所有権は移動しているのか、借用なのか。コードを読んでも分からない。コンパイルしてエラーが出るまで待つか、エージェントに「このクロージャは何をキャプチャしてる？」と聞くか。どちらも面倒だった。エージェントにすべてを任せれば楽になる、と思っていた時期がある。しかし1ヶ月ほど使って気づいた。大きな変更はエージェントが得意だ。ファイルを跨いだリファクタリング、新機能の実装、テストの追加。これらは確かにエージェントに任せた方が速い。しかし、生成されたコードの一部だけを直したいとき、エージェントに再度依頼するのは効率が悪い。「この unwrap() を ? に変えたい」「このクロージャの引数名を直したい」「この変数を別のスコープに移動したい」といった微修正は、手で直した方が速い。私は1日の開発時間のうち、7割をエージェントとの対話に、3割をNeovimでの直接作業に使っている。この3割のほとんどはコードリーディングとコードベースの理解で、実際に手で修正するのは5%程度だ。それでも、コードを「読む」環境が貧弱だと、全体の生産性が落ちる。Neovimを使う理由は、思考のスピードで編集できるからだ。ciw で単語を置換し、. で繰り返し、/ で検索して n で次へ。この一連の操作が指に染み付いていると、「直したい」と思った瞬間に直せる。エージェントに依頼を書いて、結果を待って、差分を確認する時間がない。私は Claude Code も Neovim から起動している。ターミナルで claude を叩き、エージェントと対話し、生成されたコードを Neovim で開いて確認する。すべてが同じ環境で完結する。エージェントが書いたコードを「読む」環境と、細部を「直す」環境。この両方が揃って初めて、エージェント時代の開発は快適になる。この記事では、Neovim 0.11+ と Rust の開発環境を見直した結果を紹介する。冒頭で困っていた「クロージャのキャプチャが分からない」問題を解決する Inlay Hints と、素早い修正を可能にするキーマップに重点を置いている。構成の概要私の開発環境全体については以下の記事で紹介している。本記事では Rust 関連の設定に絞って解説する。syu-m-5151.hatenablog.com私の Neovim 環境は NvChad をベースにしている。プラグイン管理には lazy.nvim を使い、Rust 関連は以下の構成だ。 コンポーネント  役割  rust-analyzer  LSP（言語サーバー）、nvim-lspconfig 経由で設定  rustaceanvim  Rust専用の拡張機能（テスト実行、マクロ展開など）  crates.nvim  Cargo.toml の依存関係管理  nvim-dap  デバッグサポート（LLDB連携）  conform.nvim  フォーマッタ（rustfmt）  mason.nvim  LSP/DAP のインストール管理 エージェント連携として claudecode.nvim も入れている。Neovim から Claude Code を起動し、生成されたコードをそのまま編集できる。以下では、この構成を「コードを読む」→「テスト・実行する」→「細部を直す」の順で紹介する。rust-analyzer と Inlay Hints でコードを理解し、rustaceanvim でテストを回し、conform.nvim でフォーマットを整える。この流れが、エージェントが書いたコードを確認・修正するワークフローと対応している。rust-analyzer の設定github.comgithub.comrust-analyzer は Rust の公式 LSP（Language Server Protocol）実装だ。LSP とは、エディタに補完、定義ジャンプ、エラー表示などの機能を提供するプロトコルで、VSCode でも Neovim でも同じ言語サーバーを使える。エージェントが生成したコードを読む際、型やライフタイム（Rust特有のメモリ管理の仕組みで、参照がいつまで有効かを示す）の情報がインラインで表示されると理解が格段に速くなる。私は nvim-lspconfig 経由で設定している。Neovim 0.11+ では vim.lsp.config API が使えるようになり、設定がシンプルになった。-- lua/configs/lspconfig.luarust_analyzer = {  settings = {    ["rust-analyzer"] = {      checkOnSave = {        command = "clippy",        extraArgs = { "--all", "--", "-W", "clippy::all" },      },      cargo = {        allFeatures = true,        loadOutDirsFromCheck = true,        buildScripts = { enable = true },      },      procMacro = {        enable = true,        attributes = { enable = true },      },      inlayHints = {        enable = true,        chainingHints = { enable = true },        typeHints = { enable = true, hideClosureInitialization = true },        parameterHints = { enable = true },        closureReturnTypeHints = { enable = "with_block" },        lifetimeElisionHints = { enable = "skip_trivial", useParameterNames = true },        maxLength = 25,        bindingModeHints = { enable = true },        closureCaptureHints = { enable = true },        discriminantHints = { enable = "fieldless" },        expressionAdjustmentHints = { enable = "reborrow" },        rangeExclusiveHints = { enable = true },      },      completion = {        autoimport = { enable = true },        postfix = { enable = true },        callable = { snippets = "fill_arguments" },        fullFunctionSignatures = { enable = true },        privateEditable = { enable = true },      },      imports = {        granularity = { group = "module" },        prefix = "self",      },      diagnostics = {        enable = true,        experimental = { enable = true },        styleLints = { enable = true },      },      semanticHighlighting = {        operator = { specialization = { enable = true } },        punctuation = { enable = true, specialization = { enable = true } },        strings = { enable = true },      },      hover = {        actions = {          enable = true,          references = { enable = true },          run = { enable = true },          debug = { enable = true },          gotoTypeDef = { enable = true },          implementations = { enable = true },        },        documentation = { enable = true, keywords = { enable = true } },        links = { enable = true },      },      typing = {        autoClosingAngleBrackets = { enable = true },      },      lens = {        enable = true,        references = { enable = true, adt = { enable = true }, enumVariant = { enable = true }, method = { enable = true }, trait = { enable = true } },        implementations = { enable = true },        run = { enable = true },        debug = { enable = true },      },      workspace = {        symbol = { search = { kind = "all_symbols" } },      },    },  },}-- Neovim 0.11+ の新しい API を使用vim.lsp.config("rust_analyzer", config)vim.lsp.enable("rust_analyzer")主な設定項目inlayHints - エージェントが生成したコードを読むとき、最も役立つのがこれだ。エディタ内にインラインで型情報やパラメータ名が表示される。特に closureCaptureHints は重宝している。Rustではクロージャ（|x| x + 1 のような無名関数）が外部の変数を使うとき、その変数を「キャプチャ」する。move |data| { ... } と書くと、data の所有権がクロージャに移動（ムーブ）する。この「何がキャプチャされているか」がエディタ上に [move: data] と表示されるようになる。エージェントが書いたクロージャを理解するのに、コンパイルエラーを待つ必要がなくなった。rangeExclusiveHints も地味に便利で、0..len が排他的（len を含まない）であることを .. の横に明示してくれる。diagnostics.experimental - 実験的な診断機能を有効化する。styleLints を有効にすると、Clippy のスタイル系リントも保存時に表示される。エージェントが生成したコードは動くが、慣用的でないことがある。この設定で「動くけど直したほうがいい」箇所が分かる。semanticHighlighting - 演算子や句読点に対してセマンティックハイライトを適用する。*self.data のような式で * が参照外しとして色付けされると、複雑な式の構造が視覚的に分かる。ただし、色数を増やしすぎるとノイズになるので、私は operator と punctuation のみ有効にしている。hover.actions - ホバー時に「Run」「Debug」「Go to Type Definition」などのアクションを表示する。エージェントが追加したテストを実行したいとき、テスト関数にカーソルを合わせて K を押すだけで実行できる。typing.autoClosingAngleBrackets - Vec< と入力すると自動的に > が補完される。エージェントが書いた型を微修正するとき、> の数を数えなくて済む。rustaceanvim の設定github.comrust-analyzer がコードを「読む」ための機能を提供するのに対し、rustaceanvim は「テスト・実行・デバッグ」のための機能を提供する。両者は補完関係にあり、rust-analyzer の LSP 機能に加えて、Rust 特有の操作（マクロ展開、テスト実行など）を追加する。rustaceanvim は rust-tools.nvim の後継だが、単なるメンテナンス引き継ぎではない。最大の違いは遅延読み込みへの対応で、.rs ファイルを開くまで何も読み込まない。私の環境では Neovim の起動時間が 120ms から 45ms に短縮された。エージェント時代に rustaceanvim が重要な理由は、「素早い確認と修正」のワークフローを支えることにある。エージェントがコードを生成したら、テストを実行し、エラーがあれば修正し、また実行する。このサイクルを <leader>rt（テスト実行）と <leader>re（エラー説明）で高速に回せる。-- lua/plugins/lang.lua{  "mrcjkb/rustaceanvim",  version = "^5",  lazy = false,  init = function()    vim.g.rustaceanvim = {      tools = {        hover_actions = { replace_builtin_hover = false },        float_win_config = { border = "rounded" },        inlay_hints = { auto = true },        code_actions = { ui_select_fallback = true },      },      server = {        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ra", function() vim.cmd.RustLsp "codeAction" end, vim.tbl_extend("force", opts, { desc = "Rust code action" }))          vim.keymap.set("n", "<leader>rd", function() vim.cmd.RustLsp "debuggables" end, vim.tbl_extend("force", opts, { desc = "Rust debuggables" }))          vim.keymap.set("n", "<leader>rr", function() vim.cmd.RustLsp "runnables" end, vim.tbl_extend("force", opts, { desc = "Rust runnables" }))          vim.keymap.set("n", "<leader>rt", function() vim.cmd.RustLsp "testables" end, vim.tbl_extend("force", opts, { desc = "Rust testables" }))          vim.keymap.set("n", "<leader>rm", function() vim.cmd.RustLsp "expandMacro" end, vim.tbl_extend("force", opts, { desc = "Expand macro" }))          vim.keymap.set("n", "<leader>rc", function() vim.cmd.RustLsp "openCargo" end, vim.tbl_extend("force", opts, { desc = "Open Cargo.toml" }))          vim.keymap.set("n", "<leader>rp", function() vim.cmd.RustLsp "parentModule" end, vim.tbl_extend("force", opts, { desc = "Parent module" }))          vim.keymap.set("n", "<leader>rj", function() vim.cmd.RustLsp "joinLines" end, vim.tbl_extend("force", opts, { desc = "Join lines" }))          vim.keymap.set("n", "<leader>rs", function() vim.cmd.RustLsp "ssr" end, vim.tbl_extend("force", opts, { desc = "Structural search replace" }))          vim.keymap.set("n", "<leader>re", function() vim.cmd.RustLsp "explainError" end, vim.tbl_extend("force", opts, { desc = "Explain error" }))          vim.keymap.set("n", "<leader>rD", function() vim.cmd.RustLsp "renderDiagnostic" end, vim.tbl_extend("force", opts, { desc = "Render diagnostic" }))          vim.keymap.set("n", "K", function() vim.cmd.RustLsp { "hover", "actions" } end, vim.tbl_extend("force", opts, { desc = "Rust hover actions" }))        end,        default_settings = {          ["rust-analyzer"] = {            cargo = { allFeatures = true },            checkOnSave = { command = "clippy" },          },        },      },      dap = {        adapter = {          type = "executable",          command = "lldb-dap",          name = "rt_lldb",        },      },    }  end,},キーマップ一覧 キー  機能  <leader>ra  コードアクション  <leader>rr  実行可能ターゲットを選択して実行  <leader>rt  テストを選択して実行  <leader>rd  デバッグ実行  <leader>rm  カーソル位置のマクロを展開  <leader>re  エラーの詳細説明を表示  <leader>rD  診断をレンダリング  <leader>rs  構造的検索置換（SSR）  <leader>rp  親モジュールに移動  <leader>rj  行を結合  <leader>rc  Cargo.toml を開く  K  ホバーアクション付きドキュメント crates.nvim の設定github.comRustでは Cargo.toml というファイルで依存ライブラリ（クレートと呼ぶ）を管理する。Node.js の package.json、Python の requirements.txt に相当するものだ。クレートには「フィーチャー」という機能のオン・オフがあり、必要な機能だけを有効にしてビルドサイズを抑えることができる。エージェントに「tokio を追加して」と頼むと、だいたい最新版を入れてくれる。しかし、フィーチャーの選択は雑なことが多い。tokio = { version = "1", features = ["full"] } と書かれていて、「full はオーバーキルだな、macros と rt-multi-thread だけでいいのに」と思うことがある。crates.nvim があれば、Cargo.toml 上でクレートにカーソルを合わせて <leader>cf を押すだけで、フィーチャー一覧がポップアップする。必要なものだけ選んで、不要なものは外す。この微調整はエージェントに頼むより、手でやった方が速い。-- lua/plugins/lang.lua{  "saecki/crates.nvim",  tag = "stable",  event = { "BufRead Cargo.toml" },  dependencies = { "nvim-lua/plenary.nvim" },  config = function()    local crates = require "crates"    crates.setup {      completion = {        cmp = { enabled = true },        crates = { enabled = true, max_results = 8, min_chars = 3 },      },      lsp = {        enabled = true,        on_attach = function(_, bufnr)          local opts = { silent = true, buffer = bufnr }          vim.keymap.set("n", "<leader>ct", crates.toggle, vim.tbl_extend("force", opts, { desc = "Toggle crates" }))          vim.keymap.set("n", "<leader>cr", crates.reload, vim.tbl_extend("force", opts, { desc = "Reload crates" }))          vim.keymap.set("n", "<leader>cv", crates.show_versions_popup, vim.tbl_extend("force", opts, { desc = "Show versions" }))          vim.keymap.set("n", "<leader>cf", crates.show_features_popup, vim.tbl_extend("force", opts, { desc = "Show features" }))          vim.keymap.set("n", "<leader>cd", crates.show_dependencies_popup, vim.tbl_extend("force", opts, { desc = "Show dependencies" }))          vim.keymap.set("n", "<leader>cu", crates.update_crate, vim.tbl_extend("force", opts, { desc = "Update crate" }))          vim.keymap.set("v", "<leader>cu", crates.update_crates, vim.tbl_extend("force", opts, { desc = "Update crates" }))          vim.keymap.set("n", "<leader>cU", crates.upgrade_crate, vim.tbl_extend("force", opts, { desc = "Upgrade crate" }))          vim.keymap.set("v", "<leader>cU", crates.upgrade_crates, vim.tbl_extend("force", opts, { desc = "Upgrade crates" }))          vim.keymap.set("n", "<leader>cA", crates.upgrade_all_crates, vim.tbl_extend("force", opts, { desc = "Upgrade all crates" }))          vim.keymap.set("n", "<leader>cH", crates.open_homepage, vim.tbl_extend("force", opts, { desc = "Open homepage" }))          vim.keymap.set("n", "<leader>cR", crates.open_repository, vim.tbl_extend("force", opts, { desc = "Open repository" }))          vim.keymap.set("n", "<leader>cD", crates.open_documentation, vim.tbl_extend("force", opts, { desc = "Open docs.rs" }))          vim.keymap.set("n", "<leader>cC", crates.open_crates_io, vim.tbl_extend("force", opts, { desc = "Open crates.io" }))        end,        actions = true,        completion = true,        hover = true,      },      popup = {        border = "rounded",        show_version_date = true,        max_height = 30,        min_width = 20,      },    }  end,},キーマップ一覧 キー  機能  <leader>ct  crates.nvim の表示切り替え  <leader>cr  クレート情報を再読み込み  <leader>cv  バージョン一覧をポップアップ表示  <leader>cf  フィーチャー一覧を表示  <leader>cd  依存関係を表示  <leader>cu  クレートを最新パッチバージョンに更新（ビジュアルモードで複数選択可）  <leader>cU  クレートを最新バージョンにアップグレード  <leader>cA  すべてのクレートをアップグレード  <leader>cH  クレートのホームページを開く  <leader>cR  クレートの GitHub リポジトリを開く  <leader>cD  docs.rs を開く  <leader>cC  crates.io を開く nvim-dap によるデバッグgithub.comgithub.comgithub.comgithub.comエージェントが生成したコードで「なぜこの値になるのか分からない」という場面がある。println デバッグで済むこともあるが、複雑なロジックでは変数の変化を追いたくなる。Rust のデバッグには LLDB を使う。LLDB は C/C++/Rust などのコンパイル言語向けのデバッガで、ブレークポイント（プログラムを一時停止する地点）を設定し、変数の中身を確認しながらステップ実行できる。macOS の場合は Homebrew で LLVM をインストールし、その中に含まれる lldb-dap（DAP = Debug Adapter Protocol）を使う。-- lua/plugins/lang.lua{  "mfussenegger/nvim-dap",  lazy = true,  dependencies = {    "rcarriga/nvim-dap-ui",    "nvim-neotest/nvim-nio",    "theHamsta/nvim-dap-virtual-text",  },  keys = {    { "<leader>db", function() require("dap").toggle_breakpoint() end, desc = "Toggle breakpoint" },    { "<leader>dB", function() require("dap").set_breakpoint(vim.fn.input "Breakpoint condition: ") end, desc = "Conditional breakpoint" },    { "<leader>dc", function() require("dap").continue() end, desc = "Continue" },    { "<leader>dC", function() require("dap").run_to_cursor() end, desc = "Run to cursor" },    { "<leader>di", function() require("dap").step_into() end, desc = "Step into" },    { "<leader>do", function() require("dap").step_over() end, desc = "Step over" },    { "<leader>dO", function() require("dap").step_out() end, desc = "Step out" },    { "<leader>dp", function() require("dap").pause() end, desc = "Pause" },    { "<leader>dr", function() require("dap").repl.toggle() end, desc = "Toggle REPL" },    { "<leader>dt", function() require("dap").terminate() end, desc = "Terminate" },    { "<leader>du", function() require("dapui").toggle() end, desc = "Toggle DAP UI" },    { "<leader>de", function() require("dapui").eval() end, desc = "Eval", mode = { "n", "v" } },  },  config = function()    local dap = require "dap"    local dapui = require "dapui"    -- DAP UI setup with custom layout    dapui.setup {      icons = { expanded = "▾", collapsed = "▸", current_frame = "▸" },      layouts = {        {          elements = {            { id = "scopes", size = 0.25 },            { id = "breakpoints", size = 0.25 },            { id = "stacks", size = 0.25 },            { id = "watches", size = 0.25 },          },          size = 40,          position = "left",        },        {          elements = {            { id = "repl", size = 0.5 },            { id = "console", size = 0.5 },          },          size = 10,          position = "bottom",        },      },    }    -- Virtual text for debugging    require("nvim-dap-virtual-text").setup { enabled = true, commented = true }    -- LLDB adapter    dap.adapters.lldb = {      type = "executable",      command = "/opt/homebrew/opt/llvm/bin/lldb-dap",      name = "lldb",    }    dap.configurations.rust = {      {        name = "Launch",        type = "lldb",        request = "launch",        program = function()          return vim.fn.input("Path to executable: ", vim.fn.getcwd() .. "/target/debug/", "file")        end,        cwd = "${workspaceFolder}",        stopOnEntry = false,        args = {},        runInTerminal = false,      },    }    -- Auto open/close DAP UI    dap.listeners.after.event_initialized["dapui_config"] = function() dapui.open() end    dap.listeners.before.event_terminated["dapui_config"] = function() dapui.close() end    dap.listeners.before.event_exited["dapui_config"] = function() dapui.close() end    -- Signs    vim.fn.sign_define("DapBreakpoint", { text = "●", texthl = "DapBreakpoint" })    vim.fn.sign_define("DapBreakpointCondition", { text = "●", texthl = "DapBreakpointCondition" })    vim.fn.sign_define("DapStopped", { text = "▶", texthl = "DapStopped", linehl = "DapStoppedLine" })  end,},キーマップ一覧 キー  機能  <leader>db  ブレークポイントの切り替え  <leader>dB  条件付きブレークポイント  <leader>dc  続行  <leader>dC  カーソル位置まで実行  <leader>di  ステップイン  <leader>do  ステップオーバー  <leader>dO  ステップアウト  <leader>dp  一時停止  <leader>dr  REPL の切り替え  <leader>dt  終了  <leader>du  DAP UI の切り替え  <leader>de  カーソル位置の式を評価（ビジュアルモードでも使用可） conform.nvim でのフォーマットgithub.com保存時に rustfmt を自動実行する。エージェントが生成したコードはフォーマットが崩れていることがあるので、保存するだけで整形されるのは便利だ。-- lua/plugins/lsp.lua{  "stevearc/conform.nvim",  event = "BufWritePre",  config = function()    require("conform").setup {      formatters_by_ft = {        rust = { "rustfmt", lsp_format = "fallback" },        -- 他の言語も設定可能      },      format_on_save = { timeout_ms = 500, lsp_fallback = true },    }  end,},ここまでで「コードを読む」「テスト・実行する」「細部を直す」の設定が揃った。最後に、これらのツール自体をどうインストールするかを紹介する。Mason でのツールインストールgithub.comLSP サーバーやデバッグアダプタは Mason で管理する。:MasonInstall で個別にインストールすることもできるが、ensure_installed に書いておけば自動でインストールされる。-- lua/plugins/lsp.lua{  "williamboman/mason.nvim",  opts = {    ensure_installed = {      "rust-analyzer",      "codelldb",  -- DAP adapter      -- 他の言語のツールも同様に追加    },  },},まとめこの設定を入れた翌日、またエージェントが書いたコードを開いた。move |ctx| { ... } というクロージャがある。今度は違った。クロージャの横に [move: ctx, config] と表示されている。何がキャプチャされているか、一目で分かる。コンパイルを待つ必要も、エージェントに聞く必要もない。エージェントに任せる7割と、自分で読む3割。この3割を Neovim で快適にすることが、全体の生産性につながる。rust-analyzer の Inlay Hints でコードを読み、rustaceanvim のキーマップでテストを回し、思考のスピードで細部を直す。エージェント時代だからこそ、手元のエディタは大事だ。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（後編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/29/092003</guid>
            <pubDate>Thu, 29 Jan 2026 00:20:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに前編を書き終えたあと、エディタを閉じて、しばらくターミナルを眺めていた。Zellijのペインが3つ並んでいる。左でVimが開き、右上でテストが走り、右下にシェルが待機している。何も起きていない。何も起きていないのに、裏では6つのスレッドが動いている。チャネルを介してメッセージが流れ、PTYがカーネルとやり取りし、VTEパーサがバイト列を解釈している。前編では設計パターンを抽出した。cat huge_log_file.logで200万行を流し込んだとき、Zellijが固まらない理由——境界付きチャネルによるバックプレッシャー。その仕組みを概念として説明した。後編では、その実装の中に入る。syu-m-5151.hatenablog.com正直に言うと、後編は地味だ。WASMプラグインの通信プロトコル、ANSIエスケープシーケンスのパース、KDL形式のセッション永続化。どれも「知っていると便利」だが「知らなくても困らない」話かもしれない。華やかさはない。ただ、Rustで本格的なアプリケーションを書こうとしたとき、こういう地味な部分でつまずく。つまずいてから調べるか、先に知っておくか。その違いは、たぶん小さくない。Cargo Workspace構成の深掘り前編でCargo Workspaceの構造を見た。後編では、なぜこの分割になっているのかを考える。zellij-utils/を開くと、IPCの定義やエラー処理、設定ファイルのパーサーが入っている。default-plugins/* → zellij-tile → zellij-tile-utilsclient ↓ server       ↓    zellij-utils ← 共有型定義（IPC契約）zellij-utilsが双方向依存を防いでいる。clientもserverもutilsに依存するが、utilsはどちらにも依存しない。これにより、clientを変更してもserverの再コンパイルは不要になる。10万行超のコードベースでは、このビルド時間の差が開発体験に直結する。zellij-tileをSDKとして独立させた意図も見える。プラグイン開発者はサーバー実装への依存なしにビルドできる。これは「プラグインエコシステムの成長」を設計段階で意識した判断だ。後からSDKを切り出すより、最初から分けておく方が遥かに低コストになる。後編で必要な追加知識前編でPTY、チャネル、Actorモデル、WASMの基礎を説明した。後編では、さらに低レベルな概念が登場する。ここで整理しておこう。termios構造体とターミナルモードUnixのターミナルはtermios構造体で制御される。この構造体には、ターミナルの振る舞いを決めるフラグが数十個含まれている。// nixクレートでの操作let mut tio = termios::tcgetattr(fd)?;  // 現在の設定を取得termios::cfmakeraw(&mut tio);            // Raw Modeに設定termios::tcsetattr(fd, SetArg::TCSANOW, &tio)?;  // 即座に適用ターミナルには2つの主要なモードがある。Cooked Mode（カノニカルモード）- カーネルが行編集を処理する（バックスペース、Ctrl+Wなど）- Enterを押すまで入力がバッファされる- Ctrl+CでSIGINTが自動送信されるRaw Mode- すべてのキー入力がそのままアプリケーションに届く- 行編集もシグナル生成もアプリケーションの責任- ターミナルマルチプレクサには必須Zellijは起動時にRaw Modeに入り、終了時に元のモードに戻す。これを忘れると、ターミナルが「壊れた」状態になる。主要なシグナルターミナルアプリケーションが扱う主なシグナルは以下の通り。 シグナル  発生条件  用途  SIGWINCH  ウィンドウリサイズ  ターミナルサイズの再取得  SIGINT  Ctrl+C  プロセスの中断  SIGTSTP  Ctrl+Z  プロセスの一時停止  SIGTERM  killコマンド  正常終了の要求  SIGKILL  kill -9  強制終了（捕捉不可）  SIGHUP  端末切断  セッション終了  SIGCHLD  子プロセス終了  子プロセスの状態変化 ZellijはSIGWINCHを特に注意深く扱う。ウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のSIGWINCHが発生する。すべてに反応するとパフォーマンスが悪化するため、スロットリング（間引き）が必要だ。ioctl：デバイス制御ioctl（I/O Control）は、デバイスに対する特殊な操作を行うシステムコールだ。ターミナル関連では以下が重要。// ウィンドウサイズの取得ioctl(fd, TIOCGWINSZ, &mut winsize);  // Get WINdow SiZe// ウィンドウサイズの設定ioctl(fd, TIOCSWINSZ, &winsize);      // Set WINdow SiZeWinsize構造体は4つのフィールドを持つ。struct Winsize {    ws_row: u16,      // 行数    ws_col: u16,      // 列数    ws_xpixel: u16,   // ピクセル幅（Sixel画像用）    ws_ypixel: u16,   // ピクセル高さ（Sixel画像用）}TIOCSWINSZでPTYのサイズを変更すると、カーネルは子プロセスにSIGWINCHを送る。シェルはこのシグナルを受けて画面を再描画する。ANSIエスケープシーケンスの詳細前編で触れたが、後編ではより詳しく見る。CSI（Control Sequence Introducer）\x1b[  → CSI開始CSIの後にパラメータとコマンドが続く。\x1b[31m      → 前景色を赤に（SGR: Select Graphic Rendition）\x1b[10;5H   → カーソルを10行5列に移動（CUP: Cursor Position）\x1b[2J      → 画面全体をクリア（ED: Erase in Display）\x1b[?25h    → カーソルを表示（DECTCEM）\x1b[?25l    → カーソルを非表示OSC（Operating System Command）\x1b]0;title\x07  → ウィンドウタイトルを設定\x1b]8;;URL\x07   → ハイパーリンク開始DCS（Device Control String）\x1bP...ST  → 同期出力、Sixel画像などZellijはvteクレートでこれらをパースし、Performトレイトの各メソッドに振り分ける。ファイルディスクリプタとPTYUnixでは「すべてがファイル」だ。PTYもファイルディスクリプタ（FD）で表現される。let OpenptyResult { master, slave } = openpty(None, &termios)?;// master: RawFd (例: 3)// slave:  RawFd (例: 4)子プロセス（シェル）は、login_tty()で以下の処理を行う。新しいセッションを作成（setsid()）slave PTYを制御端末に設定FD 0, 1, 2（stdin, stdout, stderr）をslave PTYに接続これにより、シェルの入出力はすべてPTY経由になる。tcdrain：出力の完了待ちtcdrainは、書き込んだデータがすべて送信されるまでブロックするシステムコールだ。write(fd, bytes)?;   // バッファに書き込むtcdrain(fd)?;        // 送信完了を待つなぜ必要か。write()はカーネルのバッファに書き込んだ時点で返る。相手がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。Zellijでは、PTYへの書き込み後にtcdrain()を呼ぶ。これにより、入力が確実にシェルに届いてから次の処理に進む。これらの概念の関係[termios] ─── Raw/Cooked Mode を制御    ↓[PTY Master] ←── ioctl(TIOCGWINSZ/TIOCSWINSZ) でサイズ制御    │    │ write() + tcdrain()    ↓[PTY Slave] ─── シェルの stdin/stdout/stderr    │    │ SIGWINCH, SIGCHLD など    ↓[シグナルハンドラ] ─── スロットリング、グレースフル終了後編では、これらの概念がZellijの実装にどう現れるかを見ていく。境界付きチャネルの実装前編で「バッファサイズ50」と書いた。実際のコードを見てみよう。zellij-client/src/lib.rsを開く。// zellij-client/src/lib.rslet (send_client_instructions, receive_client_instructions): ChannelWithContext<    ClientInstruction,> = channels::bounded(50);  // バッファサイズ: 50サーバー側も同様だ。zellij-server/src/lib.rsを開く。// zellij-server/src/lib.rslet (to_server, server_receiver): ChannelWithContext<ServerInstruction> =    channels::bounded(50);なぜ50なのか。開発者ブログやissue #525を調査したが、この値を選んだ明確な理由は記載されていなかった。ただし、技術的な背景は理解できる。境界付きチャネル導入の発端はissue #525だ。PTYスレッドがプログラム出力を読み取り、無制限チャネル経由でScreenスレッドに送る。出力生成がレンダリング速度を超えると、キューが無限に成長し、メモリ使用量と入力遅延が悪化する。単純に境界付きチャネルに変えるとデッドロックのリスクがある。PTYがキューを満杯にする→WASMスレッドがレンダリング命令を送ろうとしてブロック→Screenスレッド（キューを空にすべき側）がWASMスレッドの応答待ちでブロック——この連鎖だ。解決策として、crossbeamのselect!マクロを使った選択的ルーティングが採用された。PTY→Screen間のみ境界付きチャネルでバックプレッシャーをかけ、他のコンポーネント間は無制限チャネルを維持する。50という数字は「小さすぎてスループットを落とさず、大きすぎてメモリを圧迫しない」経験的なバランス点だろう。開発者ブログによると、境界付きチャネルの導入だけでベンチマークは19秒から9秒に改善された。WASMランタイムの移行Zellijのコードを読んでいて、最も意外だったのがこの部分かもしれない。WASMランタイムを「遅い方」に移行している。普通は逆だ。前編では「wasmiを使っている」と書いた。しかし、Zellijの歴史を調べると、WASMランタイムは2度移行している。初期はWasmer、0.40.0でWasmtime、そして最新版ではWasmiに移行した（PR #4449）。zellij-server/Cargo.tomlを開く。# zellij-server/Cargo.toml[dependencies.wasmi]version = "0.51.3"default-features = falsefeatures = ["std"]なぜWasmtimeからWasmiへ移行したのか。PR #4449のディスカッションを読むと、理由が明確になる。コンパイルからインタプリタへ。Wasmtimeは.wasmファイルをJITコンパイルする。これには秒単位の時間がかかっていた。Wasmiはインタプリタ方式で、ミリ秒単位（一桁）で実行を開始できる。キャッシュ管理の排除。Wasmtime時代はコンパイル済みコードをキャッシュしていた。プラグイン開発時に「キャッシュバスティング」が必要で、これがコードの複雑さを増していた。Wasmiならキャッシュ不要だ。バイナリサイズとメモリ削減。WasmtimeはCraneliftコンパイラを含むため、バイナリサイズが大きい。Wasmiは純粋なRust実装のインタプリタで、依存関係がシンプルだ。Debianパッケージングでも、Wasmtimeの依存関係（wiggle等）がブロッカーになる可能性があったが、Wasmiなら問題ない。性能面のトレードオフ。PRテスターの報告によると、Debugビルドでは一部プラグイン（Zjstatus）の起動に約1秒の遅延が観察された。ただし、Releaseビルドでは顕著な影響がなかった。コンパイルプロファイルの調整で改善も報告されている。ステータスバーの更新に1msかかるか0.1msかかるか——ユーザーには分からない。「最速のランタイム」より「最もメンテナンスしやすいランタイム」を選んだ判断は、オープンソースプロジェクトとして合理的だ。Protocol Buffersによるプラグイン通信WASMランタイムがプラグインの「実行環境」なら、Protocol Buffersはプラグインの「通信手段」だ。プラグインとホスト間の通信はProtocol Buffersで実現されている。zellij-utils/src/plugin_api/plugin_command.protoを開く。// zellij-utils/src/plugin_api/plugin_command.protoenum CommandName {  Subscribe = 0;  Unsubscribe = 1;  SetSelectable = 2;  GetPluginIds = 3;  OpenFile = 9;  OpenTerminal = 14;  // ... 150以上のコマンド}150以上のコマンド。前編で見たScreenInstructionの100バリアントを超えている。プラグインはUI操作、ファイル操作、ネットワーク、他プラグインとの通信など、ホストの機能に広くアクセスできるからだ。なぜProtocol Buffersなのか。WASMとホストの間でデータを受け渡すには、シリアライズが必要だ。JSONでもMessagePackでも良いが、Protocol Buffersには以下の利点がある。スキーマがドキュメントになる: .protoファイルを見れば、プラグインAPIの全体像が分かる後方互換性: フィールドの追加・削除が安全にできる型安全: コード生成により、シリアライズ/デシリアライズのミスを防げるzellij-tile/src/lib.rsのマクロを見ると、Protocol Buffersがどう使われているか分かる。#[no_mangle]fn load() {    STATE.with(|state| {        let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();        // Protocol Buffersをデシリアライズして設定を取得    });}プラグインは標準入力からProtocol Buffersを読み、標準出力に書き込む。WASM境界を越えるのは単なるバイト列だ。シンプルだが、型安全性は失われない。パーミッションシステムの設計思想プラグインは16種類のパーミッションから必要なものを要求する。zellij-utils/src/plugin_api/plugin_permission.protoを開く。// zellij-utils/src/plugin_api/plugin_permission.protoenum PermissionType {  ReadApplicationState = 0;      // ペイン・タブ・UI状態の読み取り  ChangeApplicationState = 1;    // ペイン・タブ・UIの変更  OpenFiles = 2;                 // ファイルを開く  RunCommands = 3;               // コマンド実行  OpenTerminalsOrPlugins = 4;    // ターミナル/プラグインを開く  WriteToStdin = 5;              // ペインへの入力  WebAccess = 6;                 // HTTPリクエスト  ReadCliPipes = 7;              // CLIパイプの読み取り  MessageAndLaunchOtherPlugins = 8;  // 他プラグインとの通信  Reconfigure = 9;               // 設定変更  FullHdAccess = 10;             // ファイルシステム完全アクセス  StartWebServer = 11;           // Webサーバー起動  InterceptInput = 12;           // 入力のインターセプト  ReadPaneContents = 13;         // ペイン内容の読み取り  RunActionsAsUser = 14;         // ユーザーとしてアクション実行  WriteToClipboard = 15;         // クリップボードへの書き込み}このパーミッションモデルは「悪意あるプラグイン」より「バグのあるプラグイン」を想定している——と私は読んだ。考えてみてほしい。悪意あるプラグインを防ぎたいなら、ユーザーに許可を求めるUIは逆効果だ。ユーザーは深く考えずに「許可」を押す。AndroidやiOSの経験から、我々はそれを知っている。Zellijのパーミッションモデルが防いでいるのは、むしろ「うっかりファイルを消してしまうバグ」や「意図せずネットワークにアクセスしてしまう問題」だ。FullHdAccessを持つプラグインがファイルを誤削除するリスクを、ユーザーが明示的に受け入れる——そういう設計だと理解している。許可されたパーミッションはPermissionCacheにプラグイン名ごとに保存され、次回起動時は再確認されない。これも「毎回聞かれると面倒」という実用性を優先した判断だ。ANSIエスケープシーケンスのパースここまでプラグインの実行環境（WASM）、通信手段（Protocol Buffers）、安全性（パーミッション）を見てきた。ここからはサーバー側の話に移る。シェルの出力をどう画面に変換するか——その起点がANSIエスケープシーケンスのパースだ。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。use vte::{Params, Perform};impl Perform for Grid {    // 通常文字の描画    fn print(&mut self, c: char) {        self.add_character(c);    }    // C0/C1制御文字（改行、タブなど）    fn execute(&mut self, byte: u8) {        match byte {            b'\n' => self.move_cursor_down(1),            b'\r' => self.move_cursor_to_beginning_of_line(),            b'\t' => self.advance_to_next_tabstop(),            _ => {}        }    }    // CSIシーケンス（カーソル移動、色設定など）    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    _ignore: bool, action: char) {        // \x1b[10;2H → カーソル移動        // \x1b[36m → 色設定    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。Alacrittyと同じクレートを使っている点も興味深い。ターミナルエミュレータの世界では、vteがデファクトスタンダードになりつつある。差分レンダリングの実装VTEパーサがANSIエスケープシーケンスを解釈し、Gridが更新される。次の問題は、そのGridをどう効率的に画面へ反映するかだ。全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。// zellij-server/src/output/mod.rspub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更行インデックス    pub should_update_all_lines: bool,    styled_underlines: bool,}impl Default for OutputBuffer {    fn default() -> Self {        OutputBuffer {            changed_lines: HashSet::new(),            should_update_all_lines: true,  // 初回は全画面レンダリング            styled_underlines: true,        }    }}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn clear(&mut self) {        self.changed_lines.clear();        self.should_update_all_lines = false;    }}なぜ「行レベル」であり「セル単位」ではないのか。セル単位の差分追跡も技術的には可能だ。しかし、ターミナルの出力は行単位で更新されることが多く、1文字だけ変わるケースは稀だ。セル単位にすると、追跡のオーバーヘッドが差分レンダリングの利点を上回る可能性がある。HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。シンプルだが効果的だ。パフォーマンス最適化の成果ここまで見てきた境界付きチャネルと差分レンダリングは、個別には小さな改善に見える。しかし、組み合わせると効果は大きい。開発者ブログによると、以下の最適化によりcat bigfileのベンチマークが大幅に改善された。ベンチマーク条件:- 測定コマンド: hyperfine --show-output "cat /tmp/bigfile"（10回実行の平均）- ファイルサイズ: 200万行- ペインサイズ: 59行 × 104列 段階  時間  最適化前  19.175秒 ± 0.347秒  境界付きチャネル導入後  9.658秒 ± 0.095秒  全最適化後  5.270秒 ± 0.027秒  tmux（参考）  5.593秒 このベンチマークではtmuxと同等以上のパフォーマンスを達成している。ただし、マシンスペックやtmuxのバージョン・設定は記載されていない。実環境での性能はワークロードや設定に依存するため、「Zellijの方が常に速い」とは言えない。重要なのは、適切な最適化によってRust製の新参者が30年の歴史を持つtmuxと同等のパフォーマンスを達成できた点だ。主な最適化は以下の4つだ。境界付きチャネルによるバックプレッシャー: 19秒→9秒の最大の貢献Vecの事前確保: Vec::with_capacity()で再確保を削減Unicode幅のキャッシュ: 絵文字などの幅計算を毎回やらない行レベル差分追跡: 変更行のみを再描画どれも「当たり前」の最適化だ。しかし、当たり前のことを愚直にやるのは難しい。ターミナル特有の問題への対処ここまで、チャネル、WASM、Protocol Buffers、ANSIパーサー、差分レンダリングと見てきた。どれも汎用的なパターンの応用だ。ここからは違う。ターミナルエミュレータでなければ出会わない問題ばかりだ。ソースコードを読んでいて、一番面白かったのはこのあたりだった。RcCharacterStyles: 16バイトに収めるメモリ効率化ターミナルの文字列バッファは数百万の要素を持つ。1文字あたりのメモリサイズがパフォーマンスに直結する。zellij-server/src/panes/terminal_character.rsを開く。// Enum Niche Optimization: 2つのvariantしかないため、ポインタサイズと同じ8バイトに収まる#[derive(Clone, Debug, PartialEq)]pub enum RcCharacterStyles {    Reset,    Rc(Rc<CharacterStyles>),}// compile-time assertionでメモリサイズを保証#[cfg(target_arch = "x86_64")]const _: [(); 8] = [(); std::mem::size_of::<RcCharacterStyles>()];// TerminalCharacter全体も16バイト#[cfg(target_arch = "x86_64")]const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];// thread_local!でデフォルトスタイルをキャッシュし、メモリ再利用thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}impl Default for RcCharacterStyles {    fn default() -> Self {        RC_DEFAULT_STYLES.with(|s| s.clone())  // thread_localから共有参照を取得    }}compile-time assertionが面白い。const _: [(); 16] = [(); std::mem::size_of::<TerminalCharacter>()];は、TerminalCharacterのサイズが16バイトでなければコンパイルエラーになる。将来フィールドを追加したとき、意図せずメモリサイズが増えることを防ぐ。Enum Niche Optimization + Reference Counting + thread_localの組み合わせで、リセット状態の文字スタイルをメモリ効率的に管理している。型安全性を失わずに、大規模なパフォーマンス最適化を実現しているのが印象的だ。PaneResizer: Cassowary制約ソルバーによるペイン配置ペインのレイアウト計算は、意外と難しい。「固定サイズのペイン」と「パーセンテージ指定のペイン」が混在し、ウィンドウリサイズ時に全体を再計算する必要がある。zellij-server/src/panes/tiled_panes/pane_resizer.rsを開く。use cassowary::{    strength::{REQUIRED, STRONG},    Expression, Solver, Variable,    WeightedRelation::EQ,};pub struct PaneResizer<'a> {    panes: Rc<RefCell<HashMap<PaneId, &'a mut Box<dyn Pane>>>>,    vars: HashMap<PaneId, Variable>,    solver: Solver,}// 制約を設定: 「固定サイズペイン」と「パーセンテージペイン」の両方に対応fn constrain_spans(space: usize, spans: &[Span]) -> HashSet<cassowary::Constraint> {    let mut constraints = HashSet::new();    // 全ペインの合計サイズは、利用可能なスペースと等しい（REQUIRED強度）    let full_size = spans        .iter()        .fold(Expression::from_constant(0.0), |acc, s| acc + s.size_var);    constraints.insert(full_size.clone() | EQ(REQUIRED) | space as f64);    // 固定サイズはREQUIRED、パーセンテージはSTRONGで制約    for span in spans {        match span.size.constraint {            Constraint::Fixed(s) => constraints.insert(span.size_var | EQ(REQUIRED) | s as f64),            Constraint::Percent(p) => constraints                .insert((span.size_var / new_flex_space as f64) | EQ(STRONG) | (p / 100.0)),        };    }    constraints}// 丸め誤差の分配: error.signum()で±1ずつペインサイズを調整for span in flex_spans {    rounded_sizes        .entry(span.size_var)        .and_modify(|s| *s += error.signum());    error -= error.signum();}Cassowaryは線形計画法を使った制約ソルバーだ。元々はmacOSのAuto Layoutに使われていたアルゴリズムで、それをペインレイアウトに応用している。REQUIREDとSTRONGの強度で優先度を管理するのが賢い。固定サイズのペインは絶対に守られ、パーセンテージ指定のペインは「できるだけ守る」という柔軟性を持つ。error.signum()で丸め誤差を1ピクセルずつ分配するのも秀逸だ。浮動小数点の計算結果を整数に変換すると、どうしても誤差が出る。その誤差を均等にばらまくことで、ギャップやオーバーラップを回避している。HyperlinkTracker: カーソルジャンプ検出によるURL追跡ターミナルでURLをクリック可能にするには、「文字列がURLかどうか」を検出する必要がある。しかし、ターミナルは1文字ずつ出力されるため、URLの開始と終了を正確に把握するのは難しい。zellij-server/src/panes/hyperlink_tracker.rsを開く。pub struct HyperlinkTracker {    buffer: String,    cursor_positions: Vec<HyperlinkPosition>,  // 各文字のカーソル位置を記録    start_position: Option<HyperlinkPosition>,    last_cursor: Option<HyperlinkPosition>,    // カーソルジャンプ検出用}// カーソルが「連続的に移動していない」ことを検出fn should_reset_due_to_cursor_jump(&self, current_pos: &HyperlinkPosition) -> bool {    if let Some(last_pos) = &self.last_cursor {        let is_contiguous =            // 同一行の隣（通常の文字出力）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x + 1) ||            // 改行（行の折り返し）            (current_pos.y == last_pos.y + 1 && current_pos.x == 0) ||            // 同じ位置（上書き）            (current_pos.y == last_pos.y && current_pos.x == last_pos.x);        !is_contiguous    } else {        false    }}カーソルジャンプ = URLの中断と判定するのが面白い。例えば、https://example.comと出力される途中で、プロンプトに戻るためにカーソルが左上にジャンプしたら、URLは完了したと見なす。複数行にまたがるURLや、ターミナルの折り返しにも対応している。Sixel画像: 負の座標とオーバーラップ判定Sixelは、ターミナル内に画像を表示するための古い規格だ。Zellijはこれをサポートしているが、スクロール時の挙動が複雑になる。zellij-server/src/panes/sixel.rsを開く。#[derive(Debug, Clone, Copy, Default, PartialEq, Eq, Hash)]pub struct PixelRect {    pub x: usize,    pub y: isize,  // 負の値対応！スクロールバッファの上部に消えた画像    pub width: usize,    pub height: usize,}// 新しい画像が古い画像を完全に覆った場合、古い画像を削除for (image_id, pixel_rect) in &self.sixel_image_locations {    if let Some(intersecting_rect) = pixel_rect.intersecting_rect(&image_size_and_coordinates) {        if intersecting_rect.x == pixel_rect.x            && intersecting_rect.y == pixel_rect.y            && intersecting_rect.height == pixel_rect.height            && intersecting_rect.width == pixel_rect.width        {            self.image_ids_to_reap.push(*image_id);  // 完全に覆われた→削除予定        }    }}y: isizeが興味深い。スクロールで画像がバッファの上部に消えると、yが負の値になる。usizeではなくisizeにすることで、この状況を型で表現している。完全にオーバーラップした画像は自動でメモリ解放される。Sixel画像は計算コストが高いため、不要な画像を積極的に削除するのは合理的だ。ダブルクリック検出: Doherty Thresholdマウスのダブルクリック検出には、時間閾値が必要だ。ZellijはDoherty Thresholdという値を使っている。zellij-server/src/panes/grid.rsを開く。const CLICK_TIME_THRESHOLD: u128 = 400;  // Doherty Thresholdimpl Click {    pub fn record_click(&mut self, position: Position) {        let click_is_same_position = self.position_and_time            .map(|(p, _t)| p == position)            .unwrap_or(false);        let click_is_within_time_threshold = self.position_and_time            .map(|(_p, t)| t.elapsed().as_millis() <= CLICK_TIME_THRESHOLD)            .unwrap_or(false);        if click_is_same_position && click_is_within_time_threshold {            self.count += 1;        } else {            self.count = 1;        }        if self.count == 4 {            self.reset();  // 3クリックまで（単語選択、行選択、段落選択）        }    }}400msという数字は、1982年のDoherty & Kelisky論文に由来する。「ユーザーがシステムの反応を待てる限界」とされる時間だ。3クリックまで対応しているのも面白い。1クリック=カーソル移動、2クリック=単語選択、3クリック=行選択。4クリック目でリセットされる。クライアント側のターミナル制御PTYの実装に入る前に、クライアント側の処理を見ておく必要がある。ユーザーのキー入力がサーバーに届くまでの道筋——つまり、データフローの入口だ。Raw Mode vs Cooked Mode追加知識セクションでtermios構造体とRaw Modeの概念を紹介した。Zellijの実装では、具体的にどうRaw Modeに入るのか。通常のターミナルは「Cooked Mode」で動作する。カーネルが行編集（バックスペース、Ctrl+Wなど）を処理し、Enterで1行ずつアプリケーションに渡す。Ctrl+Cを押すとSIGINTが送られる。Zellijはこれを無効にする必要がある。すべてのキー入力を自分で処理したいからだ。zellij-client/src/os_input_output.rsを開く。fn into_raw_mode(pid: RawFd) {    let mut tio = termios::tcgetattr(pid).expect("could not get terminal attribute");    termios::cfmakeraw(&mut tio);    match termios::tcsetattr(pid, termios::SetArg::TCSANOW, &tio) {        Ok(_) => {},        Err(e) => panic!("error {:?}", e),    };}fn unset_raw_mode(pid: RawFd, orig_termios: termios::Termios) -> Result<(), nix::Error> {    termios::tcsetattr(pid, termios::SetArg::TCSANOW, &orig_termios)}cfmakeraw() は以下を無効にする。ECHO: 入力文字のエコーバックICANON: 行単位の入力（カノニカルモード）ISIG: Ctrl+C/Ctrl+Zによるシグナル生成IXON/IXOFF: ソフトウェアフロー制御（Ctrl+S/Ctrl+Q）TCSANOW は「今すぐ適用」を意味する。TCSADRAIN（出力完了後）やTCSAFLUSH（バッファ破棄）もあるが、入力処理では即座の適用が必要だ。元のtermiosを保存しておき、終了時に復元する。これを忘れると、Zellij終了後にターミナルが壊れた状態になる。SIGWINCHのスロットリングターミナルウィンドウをリサイズすると、OSはSIGWINCHを送る。問題は、GUIウィンドウをドラッグでリサイズすると、1秒間に数十〜数百回のシグナルが発火することだ。fn handle_signals(&self, sigwinch_cb: Box<dyn Fn()>, quit_cb: Box<dyn Fn()>) {    let mut sigwinch_cb_timestamp = time::Instant::now();    let mut signals = Signals::new(&[SIGWINCH, SIGTERM, SIGINT, SIGQUIT, SIGHUP]).unwrap();    for signal in signals.forever() {        match signal {            SIGWINCH => {                // SIGWINCHコールバックをスロットリング                if sigwinch_cb_timestamp.elapsed() < SIGWINCH_CB_THROTTLE_DURATION {                    thread::sleep(SIGWINCH_CB_THROTTLE_DURATION);                }                sigwinch_cb_timestamp = time::Instant::now();                sigwinch_cb();            },            SIGTERM | SIGINT | SIGQUIT | SIGHUP => {                quit_cb();                break;            },            _ => unreachable!(),        }    }}SIGWINCH_CB_THROTTLE_DURATIONは50msだ。リサイズイベントが来ても、前回から50ms経っていなければ待機する。これにより、毎秒数十回のレンダリングを防ぐ。50msという値は経験則だ。人間が「遅延」と感じる閾値（100ms）より短く、ターミナルが処理できる頻度（60fps = 16ms）より長い。同期出力（Synchronized Output）最新のターミナルエミュレータは「同期出力」をサポートしている。複数の出力をバッファリングし、まとめて画面に反映する機能だ。let synchronised_output = match os_input.env_variable("TERM").as_deref() {    Some("alacritty") => Some(SyncOutput::DCS),    _ => None,};// レンダリング時if let Some(sync) = synchronised_output {    stdout.write_all(sync.start_seq()).expect("cannot write to stdout");}stdout.write_all(output.as_bytes()).expect("cannot write to stdout");if let Some(sync) = synchronised_output {    stdout.write_all(sync.end_seq()).expect("cannot write to stdout");}DCS（Device Control String）で出力を囲む。ターミナルはDCS開始からDCS終了までの出力をバッファリングし、終了シーケンスを受け取った時点でまとめて描画する。これにより、レイアウト変更時の「ちらつき」が消える。中間状態（ペインが1つだけ描画された状態など）が画面に表示されない。現時点ではAlacrittyのみ対応だが、今後他のターミナルにも拡大されるだろう。Kitty Keyboard Protocol従来のターミナルは、Shift+F1とF13を区別できなかった。どちらも同じエスケープシーケンスを送るからだ。Kitty Keyboard Protocolはこの問題を解決する。zellij-client/src/stdin_handler.rsを開く。loop {    match os_input.read_from_stdin() {        Ok(buf) => {            // まずKitty Keyboard Protocolを試す            if !explicitly_disable_kitty_keyboard_protocol {                match KittyKeyboardParser::new().parse(&buf) {                    Some(key_with_modifier) => {                        send_input_instructions.send(...).unwrap();                        continue;                    },                    None => {},                }            }            // フォールバック: 標準のtermwiz InputParser            input_parser.parse(&buf, |input_event| { ... }, false);        },        // ...    }}Kitty Keyboard Protocolが使えるなら使い、使えなければ従来のANSIエスケープシーケンスにフォールバックする。この二段構えにより、古いターミナルでも動作しつつ、新しいターミナルでは拡張機能を活用できる。セッション切り替え時のSTDINバッファリングZellijは複数のセッションを持てる。セッション間を切り替えるとき、STDINの所有権を移す必要がある。fn read_from_stdin(&mut self) -> Result<Vec<u8>, &'static str> {    let session_name_at_calltime = { self.session_name.lock().unwrap().clone() };    let mut buffered_bytes = self.reading_from_stdin.lock().unwrap();    match buffered_bytes.take() {        Some(buffered_bytes) => Ok(buffered_bytes),        None => {            let stdin = std::io::stdin();            let mut stdin = stdin.lock();            let buffer = stdin.fill_buf().unwrap();            let length = buffer.len();            let read_bytes = Vec::from(buffer);            stdin.consume(length);            // セッションが変わったら、読んだバイトをバッファに戻す            let session_name_after_reading_from_stdin =                { self.session_name.lock().unwrap().clone() };            if session_name_at_calltime.is_some()                && session_name_at_calltime != session_name_after_reading_from_stdin            {                *buffered_bytes = Some(read_bytes);                Err("Session ended")            } else {                Ok(read_bytes)            }        },    }}問題: 旧セッションのスレッドがSTDINでブロックしている間に、新セッションがSTDINを必要とする。解決策: 読み取り前後でセッション名を比較する。セッションが変わっていたら、読んだバイトをバッファに保存し、新セッションのスレッドがそれを取得できるようにする。これはエッジケースだが、マルチセッション対応には必須の処理だ。PTY（疑似端末）の実装詳細ここまで見てきた境界付きチャネル、VTEパーサ、差分レンダリング、compile-time assertion——これらはすべて、PTYという土台の上に乗っている。チャネルはPTYからの出力を運び、VTEパーサはPTYが吐いたバイト列を解釈し、差分レンダリングはその結果を画面に描く。個別のパターンを追いかけてきたが、ここで全体が合流する。ターミナルマルチプレクサの核心部分であるPTY処理を深掘りする。ここがZellijの「心臓部」だ。そもそもTTYとPTYとは何かTTY（TeleTYpewriter）は、歴史的にはテレタイプ端末を指す。現代では「ターミナルデバイス」の総称として使われる。/dev/ttyや/dev/tty1がこれにあたる。PTY（Pseudo-TTY）は「疑似端末」だ。物理的な端末がなくても、ソフトウェア的にターミナルをエミュレートする仕組み。sshやtmux、そしてZellijはPTYを使っている。PTYはマスターとスレーブのペアで構成される。[Zellij Server] ←→ [PTY Master] ←→ [PTY Slave] ←→ [Shell/App]                    (制御側)         (端末側)マスター側: Zellijが持つ。ここに書き込むと、スレーブ側のSTDINに届く。スレーブの出力はここから読めるスレーブ側: シェル（bash/zsh）が持つ。通常のターミナルと同じように振る舞うシェルから見ると、スレーブPTYは「本物のターミナル」に見える。ttyコマンドを実行すると/dev/pts/0のようなパスが返る。これがスレーブPTYだ。Zellijがペインを作るたびに、新しいPTYペアが生成される。3ペインあれば、3つのPTYマスターをZellijが管理している。なぜPTYが必要なのか「パイプでいいのでは？」と思うかもしれない。しかし、パイプとPTYには決定的な違いがある。ジョブコントロール: PTYは「制御端末」として機能する。Ctrl+Zでプロセスを停止したり、fg/bgで制御したりできるのは、PTYがあるからだ。パイプにはこの機能がないウィンドウサイズ: PTYはサイズ（行数・列数）を持つ。$COLUMNSや$LINESはPTYから取得される。パイプにはサイズの概念がない行編集: シェルは「端末があるかどうか」で挙動を変える。isatty()がtrueを返すと、プロンプトを表示し、行編集を有効にする。パイプ経由だとこれが無効になるシグナル: Ctrl+CでSIGINTを送れるのは、PTYが「フォアグラウンドプロセスグループ」を管理しているからだつまり、PTYなしには「ターミナルらしい体験」が成り立たない。PTYの作成フローzellij-server/src/os_input_output.rsを開く。fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // Zellij側（ホスト）    let pid_secondary = open_pty_res.slave;   // シェル側（子プロセス）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ", VERSION)            .env("ZELLIJ_SESSION_NAME", &*SESSION_NAME)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // 子プロセスの終了を監視するスレッドを起動    std::thread::spawn({        move || {            child.wait().ok();            let exit_status = child.try_wait().ok().flatten().and_then(|e| e.code());            quit_cb(PaneId::Terminal(terminal_id), exit_status, cmd);        }    });    Ok((pid_primary, child.id() as RawFd))}openpty() はマスターとスレーブの2つのファイルディスクリプタを作る。Zellijはマスター側を持ち、シェル（bashやzsh）はスレーブ側を持つ。login_tty() は伝統的なUnix関数で、3つのことをする。setsid() で新しいセッションを作成スレーブPTYをcontrolling terminalに設定dup2() でstdin/stdout/stderrをスレーブに接続close_fds::close_open_fds(3, &[]) が面白い。ファイルディスクリプタ3以降を全て閉じる。これにより、親プロセスから継承した不要なFDがリークしない。環境変数の設定も注目に値する。ZELLIJ_PANE_IDを設定することで、子プロセス側から「自分がどのペインで動いているか」を知ることができる。シェルスクリプトやプラグインで使える。読み取りと書き込みの分離PTYへの読み書きは、別々のスレッドで行われる。なぜか。zellij-server/src/terminal_bytes.rsを開く。pub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    ))                    .await?;                },            }        }        // ループ終了後、最終レンダリングを要求        let _ = self.async_send_to_screen(ScreenInstruction::Render).await;        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのサイズだ。大量のログ出力に対応するため。Ok(0)とErrの区別が重要。Ok(0)はEOF（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス正常終了時にエラーログが出てしまう。zellij-server/src/pty_writer.rsを開く。pub(crate) fn pty_writer_main(bus: Bus<PtyWriteInstruction>) -> Result<()> {    loop {        let (event, _err_ctx) = bus.recv()?;        match event {            PtyWriteInstruction::Write(bytes, terminal_id) => {                if let Some(raw_fd) = bus                    .os_input                    .as_ref()                    .and_then(|os_input| os_input.get_terminal_id_from_fd(terminal_id))                {                    let mut f = unsafe { File::from_raw_fd(*raw_fd) };                    if f.write_all(&bytes).is_ok() {                        let _ = f.flush();                    }                    std::mem::forget(f);  // FDを閉じないようにする                }            },            PtyWriteInstruction::Exit => break,        }    }    Ok(())}読み取りと書き込みを分離する理由は、デッドロック回避だ。Vimのようなプログラムは、STDINからの入力を待ちながらSTDOUTに出力する。もし同じスレッドで読み書きをすると、「Vimが入力を待っている」「Zellijが出力を待っている」という状態でデッドロックになる可能性がある。std::mem::forget(f) も興味深い。File::from_raw_fd()で作ったFileは、dropされるとFDが閉じてしまう。forget()でdropを防いでいる。forgetという名前は不穏だ。普通、忘れることは悪いことだ。しかしRustの所有権モデルでは、意図的に忘れることが正しい選択になる場合がある。FDを閉じたくないなら、Fileがdropされることを忘れさせる。記憶と忘却の関係が、ここでは逆転している。tcdrainによるフロー制御書き込みスレッドには、もう一つ重要な処理がある。PtyWriteInstruction::Write(bytes, terminal_id) => {    os_input        .write_to_tty_stdin(terminal_id, &bytes)        .with_context(err_context)        .non_fatal();    os_input        .tcdrain(terminal_id)  // ここ        .with_context(err_context)        .non_fatal();},tcdrain() は、書き込んだデータが全て送信されるまでブロックする。なぜこれが必要か。PTYにはカーネル内部にバッファがある。write()はバッファに書き込んだ時点で返る。しかし、相手側（シェル）がまだ読んでいない可能性がある。tcdrain()を呼ぶと、バッファが空になるまで待機する。これにより、次の書き込みが前の書き込みを追い越すことを防ぐ。fn tcdrain(&self, terminal_id: u32) -> Result<()> {    match self        .terminal_id_to_raw_fd        .lock()        .to_anyhow()        .with_context(err_context)?        .get(&terminal_id)    {        Some(Some(fd)) => termios::tcdrain(*fd).with_context(err_context),        _ => Err(anyhow!("could not find raw file descriptor")).with_context(err_context),    }}ソースコードのコメントには、こうある。「VimのようなプログラムはSTDINに書き込みながらSTDOUTを読むとデッドロックする」。Vimへの言及が具体的で、実際に遭遇した問題なのだろう。ウィンドウリサイズの処理ターミナルをリサイズしたとき、PTYにサイズ変更を伝える必要がある。zellij-server/src/os_input_output.rsを開く。fn set_terminal_size_using_terminal_id(    &self,    id: u32,    cols: u16,    rows: u16,    width_in_pixels: Option<u16>,    height_in_pixels: Option<u16>,) -> Result<()> {    // リサイズをキャッシュ（複数のリサイズイベントを1つにまとめる）    match self.cached_resizes.lock() {        Ok(mut cached_resizes) => {            let cached_resizes = cached_resizes.get_or_insert_with(BTreeMap::new);            cached_resizes.insert(id, (cols, rows, width_in_pixels, height_in_pixels));        },        Err(e) => {            log::error!("Failed to cache resize: {}", e);        },    }    // キャッシュを適用    self.apply_cached_resizes()}fn apply_cached_resizes(&self) -> Result<()> {    if let Some(cached_resizes) = self.cached_resizes.lock().ok().as_mut().and_then(|c| c.take()) {        for (terminal_id, (cols, rows, width_in_pixels, height_in_pixels)) in cached_resizes {            let ws = Winsize {                ws_row: rows,                ws_col: cols,                ws_xpixel: width_in_pixels.unwrap_or(0),                ws_ypixel: height_in_pixels.unwrap_or(0),            };            if let Some(raw_fd) = self.get_terminal_id_from_fd(terminal_id) {                set_terminal_size_using_fd(*raw_fd, ws);            }        }    }    Ok(())}fn set_terminal_size_using_fd(fd: RawFd, ws: Winsize) {    // TIOCSWINSZ ioctlでPTYにサイズを伝える    if let Err(e) = ioctl_set_window_size(fd, &ws) {        log::error!("Failed to set terminal size: {}", e);    }}リサイズのキャッシングが重要だ。なぜか。ソースコードのコメントに理由がある。「レイアウト計算のコードには、複数のリサイズを送信してしまうロジックの罠がある。最後の1つだけが正しいのだが、多くのプログラムやシェルはリサイズをデバウンスする（GUIウィンドウのリサイズに対処してきたトラウマだろう）。これがグリッチや描画漏れを引き起こす」。つまり、VimやZshは「リサイズが連続で来たら、最後の1つだけ処理する」という防御策を持っている。Zellijが中間のリサイズも送ると、シェル側のデバウンスと競合してしまう。だからZellij側でもキャッシュし、最後の1つだけを送る。PtyWriteInstruction::StartCachingResizes => {    // レイアウト再計算中はリサイズをキャッシュ    os_input.cache_resizes();},PtyWriteInstruction::ApplyCachedResizes => {    // 計算完了後、最後のリサイズだけを適用    os_input.apply_cached_resizes();},TIOCSWINSZ（Terminal I/O Control Set WINdow SiZe）は、PTYにサイズを伝えるioctlだ。これを呼ぶと、カーネルは子プロセスグループにSIGWINCHを送る。子プロセスはこのシグナルを受けて、画面を再描画する。ws_xpixelとws_ypixel はSixel画像のために使われる。文字単位のサイズ（cols/rows）に加えて、ピクセル単位のサイズも伝える。これにより、画像を正確な解像度で表示できる。プロセス終了の検出とシグナルペインを閉じるとき、子プロセスにシグナルを送る必要がある。pub fn close_pane(&mut self, id: PaneId) -> Result<()> {    match id {        PaneId::Terminal(id) => {            self.task_handles.remove(&id);  // 読み取りタスクを停止            if let Some(child_fd) = self.id_to_child_pid.remove(&id) {                task::block_on(async {                    self.bus                        .os_input                        .as_mut()                        .fatal()                        .kill(Pid::from_raw(child_fd))  // SIGHUPを送信                        .fatal();                });            }        },        PaneId::Plugin(pid) => {            // プラグインはUnload命令を送るだけ            drop(self.bus.senders.send_to_plugin(PluginInstruction::Unload(pid)));        },    }    Ok(())}シグナルの送信順序も工夫されている。// SIGTERMを3回試行し、それでも終了しなければSIGKILLfor _ in 0..3 {    if nix::sys::signal::kill(pid, Signal::SIGTERM).is_ok() {        std::thread::sleep(Duration::from_millis(10));        // プロセスが終了したかチェック        if nix::sys::wait::waitpid(pid, Some(WaitPidFlag::WNOHANG)).is_ok() {            return Ok(());        }    }}// 3回失敗したらSIGKILLnix::sys::signal::kill(pid, Signal::SIGKILL)?;SIGTERM 3回 → SIGKILL。プロセスに「正常終了」の機会を与えつつ、応答しなければ強制終了する。10msのポーリング間隔も絶妙だ。CWD（カレントディレクトリ）の追跡ペインのカレントディレクトリを追跡するのは、意外と難しい。fn get_cwd(&self, pid: Pid) -> Option<PathBuf> {    // /proc/[pid]/cwd を読み取る    let path = format!("/proc/{}/cwd", pid);    std::fs::read_link(path).ok()}pub fn update_and_report_cwds(&mut self) {    let terminal_ids: Vec<u32> = self.id_to_child_pid.keys().copied().collect();    let pids: Vec<_> = terminal_ids        .iter()        .filter_map(|id| self.id_to_child_pid.get(&id))        .map(|pid| Pid::from_raw(*pid))        .collect();    // 全ペインのCWDを一括取得    let (pids_to_cwds, _) = self        .bus        .os_input        .as_ref()        .map(|os_input| os_input.get_cwds(pids))        .unwrap_or_default();    // 変更があればクライアントに通知    for terminal_id in terminal_ids {        let cwd = /* ... */;        if self.terminal_cwds.get(&terminal_id) != Some(cwd) {            // CWD変更イベントを送信        }    }}/proc/[pid]/cwd はLinux固有の仕組みだ。プロセスのカレントディレクトリへのシンボリックリンクになっている。これを定期的にチェックすることで、cdコマンドによるディレクトリ変更を検出できる。データフローの全体像PTYを通じたデータの流れを図にすると、以下のようになる。[ユーザー入力]     ↓ キー入力[Client Process]     ↓ IPC (Unix Domain Socket)[Server Process]     ↓ PtyWriteInstruction::Write[PTY Writer Thread]     ↓ write() to master FD[PTY (カーネル)]     ↓[Shell/アプリケーション]     ↓ 出力[PTY (カーネル)]     ↓ read() from master FD[TerminalBytes::listen()]     ↓ ScreenInstruction::PtyBytes[Screen Thread]     ↓ VTEパーサ[Grid構造体]     ↓ 差分レンダリング[Client Process]     ↓ IPC[ユーザーの画面]6つのスレッドが協調して動いている。PTY Thread: PTYの作成・管理PTY Writer Thread: PTYへの書き込みTerminalBytes（async task）: PTYからの読み取りScreen Thread: VTEパース、レンダリングPlugin Thread: WASMプラグイン実行Background Thread: 非同期タスクこの分離により、どこか1つがブロックしても他の処理は継続できる。Vimが入力を待っている間も、他のペインは正常に動作する。実装の詳細：システムコールのシーケンスここまで概念的な説明が多かった。実際にどのシステムコールがどの順序で呼ばれるのか、具体的に見ていこう。PTY作成シーケンス1. openpty(None, &orig_termios)   → OpenptyResult { master: RawFd, slave: RawFd }2. fork() [Command::spawn()が内部で呼ぶ]3. 子プロセス（シェル側）:   - libc::login_tty(slave)     - setsid()      // 新しいセッション作成     - ioctl(slave, TIOCSCTTY)  // 制御端末として設定     - dup2(slave, 0)  // stdin     - dup2(slave, 1)  // stdout     - dup2(slave, 2)  // stderr   - close_open_fds(3, &[])  // FD 3以降を全て閉じる   - exec("/bin/bash")  // シェルを起動4. 親プロセス（Zellij側）:   - master FDを保存   - 子プロセスのPIDを保存   - 非同期読み取りタスクを起動   - シグナルハンドラスレッドを起動PTY読み取りシーケンス1. async_reader.read(&mut buf[65536])   → read(master_fd, buf, 65536) syscall   → bytes受信 or EOF(0) or error2. ScreenInstruction::PtyBytes(terminal_id, bytes)   をチャネル経由でScreenスレッドに送信3. Screenスレッドがbytesを受信   → VTEパーサに渡す   → Gridを更新PTY書き込みシーケンス1. PtyWriteInstruction::Write(bytes, terminal_id)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. write(master_fd, bytes) syscall   → バイトがPTYバッファに書き込まれる4. tcdrain(master_fd) syscall   → 書き込みが完了するまで待機ターミナルリサイズシーケンス1. PtyWriteInstruction::ResizePty(terminal_id, cols, rows, ...)   をチャネル経由で受信2. terminal_id_to_raw_fd マップからmaster FDを取得3. ioctl(master_fd, TIOCSWINSZ, &Winsize { ws_col, ws_row, ... })   → カーネルがSIGWINCHを子プロセスグループに送信   → シェルが再描画実装の詳細：スレッドの起動サーバープロセスの起動時、複数のスレッドが生成される。zellij-server/src/lib.rsを開く。pub fn start_server(mut os_input: Box<dyn ServerOsApi>, socket_path: PathBuf) {    // チャネルの作成    let (to_server, server_receiver): ChannelWithContext<ServerInstruction> =        channels::bounded(50);    let (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =        channels::bounded(50);    let (to_pty, pty_receiver): ChannelWithContext<PtyInstruction> =        channels::bounded(50);    let (to_plugin, plugin_receiver): ChannelWithContext<PluginInstruction> =        channels::bounded(50);    let (to_pty_writer, pty_writer_receiver): ChannelWithContext<PtyWriteInstruction> =        channels::unbounded();  // 書き込みは無制限    // PTY Writerスレッド    thread::Builder::new()        .name("pty_writer".to_string())        .spawn(move || {            pty_writer_main(pty_writer_bus).fatal();        })        .unwrap();    // PTYスレッド    thread::Builder::new()        .name("pty".to_string())        .spawn(move || {            pty_thread_main(pty_bus, pty_receiver).fatal();        })        .unwrap();    // Screenスレッド    thread::Builder::new()        .name("screen".to_string())        .spawn(move || {            screen_thread_main(screen_bus, screen_receiver).fatal();        })        .unwrap();    // Pluginスレッド    thread::Builder::new()        .name("plugin".to_string())        .spawn(move || {            plugin_thread_main(plugin_bus, plugin_receiver).fatal();        })        .unwrap();    // Serverスレッド（メインループ）    loop {        let (instruction, err_ctx) = server_receiver.recv().expect("...");        match instruction {            ServerInstruction::NewClient(client_attributes, ...) => { ... },            ServerInstruction::Render(serialized_output) => { ... },            ServerInstruction::UnblockInputThread => { ... },            ServerInstruction::ClientExit(client_id) => { ... },            ServerInstruction::KillSession => break,            // ...        }    }}注目すべきは、PTY Writerのチャネルだけunbounded()になっている点だ。他はbounded(50)でバックプレッシャーをかけているが、書き込みはブロックさせたくない。ユーザーの入力を遅延させると体感が悪くなるからだ。実装の詳細：キーボード入力からシェルまでの経路ユーザーがキーを押してからシェルに届くまでの、実際のコード経路を追う。1. クライアントがキー入力を受信zellij-client/src/stdin_handler.rsloop {    match os_input.read_from_stdin() {        Ok(buf) => {            // KittyプロトコルまたはANSIエスケープをパース            input_parser.parse(&buf, |input_event| {                send_input_instructions                    .send(InputInstruction::KeyEvent(                        input_event.clone(),                        buf.to_vec(),                    ))                    .unwrap();            }, false);        },        // ...    }}2. クライアントがサーバーにメッセージ送信zellij-client/src/lib.rsInputInstruction::KeyEvent(key_event, raw_bytes) => {    send_client_instructions        .send(ClientInstruction::Action(            Action::Write(None, raw_bytes, false),            None,            None,        ))        .unwrap();}3. サーバーのRouteスレッドがメッセージ受信zellij-server/src/route.rsfn route_action(action: Action, ...) -> bool {    match action {        Action::Write(_, bytes, _) => {            session                .senders                .send_to_screen(ScreenInstruction::WriteCharacter(bytes, client_id))                .unwrap();        },        // ...    }}4. Screenスレッドがペインに書き込み指示zellij-server/src/screen.rsScreenInstruction::WriteCharacter(bytes, client_id) => {    let active_tab = screen.get_active_tab_mut(client_id).unwrap();    active_tab.write_to_terminal(bytes, client_id);}5. TabがPTY Writerに書き込み指示zellij-server/src/tab/mod.rspub fn write_to_active_terminal(&mut self, bytes: Vec<u8>, client_id: ClientId) {    if let Some(active_pane_id) = self.get_active_pane_id(client_id) {        if let PaneId::Terminal(terminal_id) = active_pane_id {            self.senders                .send_to_pty_writer(PtyWriteInstruction::Write(bytes, terminal_id))                .unwrap();        }    }}6. PTY Writerスレッドが実際に書き込みzellij-server/src/pty_writer.rsPtyWriteInstruction::Write(bytes, terminal_id) => {    os_input.write_to_tty_stdin(terminal_id, &bytes)?;    os_input.tcdrain(terminal_id)?;}7. OSレイヤーでシステムコールzellij-server/src/os_input_output.rsfn write_to_tty_stdin(&self, terminal_id: u32, buf: &[u8]) -> Result<usize> {    let fd = self.terminal_id_to_raw_fd.lock()?.get(&terminal_id)?;    let mut file = unsafe { File::from_raw_fd(*fd) };    let result = file.write(buf);    std::mem::forget(file);  // FDを閉じない    result}この経路で、キーボード入力は6つのコンポーネントを経由してシェルに届く。各コンポーネント間はチャネルで接続されている。実装の詳細：シェル出力から画面までの経路逆方向、シェルの出力が画面に表示されるまでの経路。1. TerminalBytesが非同期で読み取りzellij-server/src/terminal_bytes.rspub async fn listen(&mut self) -> Result<()> {    let mut buf = [0u8; 65536];    loop {        match self.async_reader.read(&mut buf).await {            Ok(0) => break,  // EOF            Ok(n_bytes) => {                let bytes = &buf[..n_bytes];                self.async_send_to_screen(ScreenInstruction::PtyBytes(                    self.terminal_id,                    bytes.to_vec(),                )).await?;            },            Err(err) => {                log::error!("{}", err);                break;            },        }    }    Ok(())}2. Screenスレッドがバイトを受信zellij-server/src/screen.rsScreenInstruction::PtyBytes(terminal_id, bytes) => {    // terminal_idからペインを特定    if let Some(tab) = screen.get_tab_with_terminal_id(terminal_id) {        tab.handle_pty_bytes(terminal_id, bytes);    }}3. TabがVTEパーサに渡すzellij-server/src/tab/mod.rspub fn handle_pty_bytes(&mut self, terminal_id: u32, bytes: Vec<u8>) {    if let Some(pane) = self.panes.get_mut(&PaneId::Terminal(terminal_id)) {        pane.handle_pty_bytes(bytes);    }}4. TerminalPaneがGridを更新zellij-server/src/panes/terminal_pane.rsfn handle_pty_bytes(&mut self, bytes: Vec<u8>) {    self.grid.advance_by_bytes(bytes);}5. GridがVTEパーサを実行zellij-server/src/panes/grid.rspub fn advance_by_bytes(&mut self, bytes: Vec<u8>) {    for byte in bytes {        self.vte_parser.advance(&mut *self, byte);    }}ここでvte_parserはvte::Parser型だ。Gridはvte::Performトレイトを実装しており、パース結果に応じてprint()、execute()、csi_dispatch()などが呼ばれる。6. レンダリングとクライアントへの送信ScreenInstruction::Render => {    screen.render()?;    // 各クライアントに差分を送信    for client_id in screen.connected_clients.keys() {        let output = screen.render_for_client(*client_id)?;        send_to_client(*client_id, ServerToClientMsg::Render(output))?;    }}使用しているシステムコール一覧Zellijが使用する主なシステムコールをまとめる。 システムコール  用途  使用箇所  openpty()  PTYペアの作成  os_input_output.rs  fork()  子プロセスの作成  Command::spawn()  setsid()  新セッションの作成  login_tty() 内部  dup2()  FDの複製  login_tty() 内部  exec()  プログラムの実行  Command::spawn()  read()  PTYからの読み取り  terminal_bytes.rs  write()  PTYへの書き込み  pty_writer.rs  ioctl(TIOCGWINSZ)  ウィンドウサイズ取得  os_input_output.rs  ioctl(TIOCSWINSZ)  ウィンドウサイズ設定  os_input_output.rs  tcgetattr()  termios取得  os_input_output.rs  tcsetattr()  termios設定  os_input_output.rs  tcdrain()  出力完了待機  pty_writer.rs  kill()  シグナル送信  os_input_output.rs  waitpid()  子プロセス待機  os_input_output.rs  close()  FDのクローズ  各所 エラーハンドリングの実装PTY関連のエラーは、致命的なものと非致命的なものに分類される。// 非致命的: ログを出すが処理を継続os_input    .write_to_tty_stdin(terminal_id, &bytes)    .with_context(err_context)    .non_fatal();// 致命的: パニックまたは終了os_input    .spawn_terminal(cmd, quit_cb)    .with_context(err_context)    .fatal();non_fatal()は書き込みエラー、リサイズエラーなどに使われる。ペインが閉じられた後に書き込みが来ることがあり、これは正常な動作だ。fatal()はPTY作成失敗、サーバー初期化失敗などに使われる。これらは回復不能なエラーだ。コマンドが見つからない場合の処理も興味深い。fn command_exists(cmd: &RunCommand) -> bool {    // cwdからの相対パスをチェック    if let Some(cwd) = cmd.cwd.as_ref() {        let full_command = cwd.join(&cmd.command);        if full_command.exists() && full_command.is_file() {            return true;        }    }    // PATHを検索    if let Some(paths) = env::var_os("PATH") {        for path in env::split_paths(&paths) {            let full_command = path.join(&cmd.command);            if full_command.exists() && full_command.is_file() {                return true;            }        }    }    false}コマンドが存在しない場合、ZellijError::CommandNotFoundが返される。hold_on_closeが設定されていれば、エラーメッセージを表示したペインが残る。設定されていなければ、ペインは静かに閉じられる。KDL形式によるセッション永続化Zellijは1秒ごとにセッション状態を自動シリアライズする。保存先は~/.cache/zellij/<VERSION>/session_info/<SESSION_NAME>/だ。zellij-utils/src/session_serialization.rsを開く。// zellij-utils/src/session_serialization.rspub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut layout_node = KdlNode::new("layout");    // タブ、ペインの構造をKDLノードとして構築}生成されるKDL。layout {    cwd "/home/user/project"    tab name="Editor" {        pane command="vim" {            args "src/main.rs"            cwd "/home/user/project"        }    }    tab name="Shell" {        pane    }}シリアライズ形式がそのまま有効なKDLレイアウトファイルになっている。これは特筆すべき設計だ。なぜJSONやバイナリ形式ではないのか。JSONはパース速度が速いが、人間が編集するには冗長だ。KDLは「人間が読める設定ファイル」として設計された言語であり、Zellijのレイアウト設定にも使われている。つまり設定ファイルとシリアライズ形式を統一することで、自動保存されたセッションをそのままレイアウトテンプレートとして再利用できる。tmuxの.tmux.confとセッション復元は別系統だ。設定ファイルでは「こうあるべき」を書き、セッション復元では「こうだった」を読む。Zellijはこの2つを統一している。シンプルだが、これを思いつくのは難しい。thiserror + anyhow の併用前編でFatalErrorトレイトを紹介した。後編では、エラー型の定義側を見る。// thiserrorによる型定義#[derive(Debug, Error)]pub enum ZellijError {    #[error("could not find command '{command}' for terminal {terminal_id}")]    CommandNotFound { terminal_id: u32, command: String },    #[error("Client {client_id} is too slow to handle incoming messages")]    ClientTooSlow { client_id: u16 },    #[error("an error occured")]    GenericError { source: anyhow::Error },}thiserror（ライブラリ向けエラー型定義）とanyhow（アプリケーション向けエラー伝播）の併用だ。GenericErrorのフィールドがanyhow::Errorになっている点に注目してほしい。これにより、詳細なエラー型を定義したいケースと、「とりあえずエラーを伝播したい」ケースを両立できる。その他の興味深い実装パターンここまでで主要なパターンを見てきたが、ソースコードを読み進める中で発見した「細かいが面白い」実装を紹介する。スクロールバックの「Canonical Rows」アーキテクチャターミナルの行は、表示上は複数行でも論理的には1行であることがある。長いコマンドが折り返されるケースだ。Zellijはこれを「Canonical Row」という概念で管理している。zellij-server/src/panes/grid.rsを開く。pub struct Row {    pub columns: VecDeque<TerminalCharacter>,    pub is_canonical: bool,  // 本当の改行か、折り返しか    width: Option<usize>,    // キャッシュされた幅}is_canonicalフラグが鍵だ。trueなら本当の改行（ユーザーがEnterを押した）、falseなら表示上の折り返し。スクロール時、折り返された行は元の「親」行と一緒に移動する必要がある。from_rowsメソッドで複数行をマージし、split_to_rows_of_lengthで再分割する。pub fn split_to_rows_of_length(&mut self, max_row_length: usize) -> Vec<Row> {    let mut parts: Vec<Row> = vec![];    let mut current_part: VecDeque<TerminalCharacter> = VecDeque::new();    let mut current_part_len = 0;    for character in self.columns.drain(..) {        if current_part_len + character.width() > max_row_length {            parts.push(Row::from_columns(current_part));            current_part = VecDeque::new();            current_part_len = 0;        }        current_part_len += character.width();        current_part.push_back(character);    }    // canonical statusを保持    parts}文字の幅を考慮している点に注目。絵文字（幅2）の途中で行を切ると表示が崩れる。character.width()でUnicode幅を取得し、正しい位置で分割している。スクロールバックは3つのバッファで管理される。pub(crate) lines_above: VecDeque<Row>,    // スクロールバック（上）pub(crate) viewport: Vec<Row>,             // 表示中pub(crate) lines_below: Vec<Row>,          // 未表示（下）lines_aboveには上限がある。無限にスクロールバックを溜めるとメモリを食い尽くす。fn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();        if let Some(line) = line {            sixel_grid.offset_grid_top();  // Sixel画像の位置も更新        }    }    vec.push_back(value);}古い行を削除するとき、Sixel画像の位置も調整している。画像は行番号で位置を管理しているため、行が消えると座標がずれる。この連携が面白い。幅を考慮したカーソル位置計算絵文字やCJK文字は幅2を持つ。カーソルがその「途中」にあるケースをどう扱うか。pub fn absolute_character_index_and_position_in_char(&self, x: usize) -> (usize, usize) {    // xの幅を考慮したインデックスと、ワイド文字内の位置を返す    let mut accumulated_width = 0;    let mut absolute_index = x;    let mut position_inside_character = 0;    for (i, terminal_character) in self.columns.iter().enumerate() {        accumulated_width += terminal_character.width();        absolute_index = i;        if accumulated_width > x {            let character_start_position = accumulated_width - terminal_character.width();            position_inside_character = x - character_start_position;            break;        }    }    (absolute_index, position_inside_character)}2つの値を返すのがポイント。「何番目の文字か」と「その文字内のどの位置か」。カーソルが絵文字の右半分にあるとき、position_inside_characterは1になる。これにより、カーソル移動やテキスト選択が正しく動作する。OnceCellによるグローバル非同期ランタイム複数スレッドから同じTokioランタイムを使いたい。Mutexで包むと毎回ロックが必要になる。zellij-server/src/global_async_runtime.rsを開く。use once_cell::sync::OnceCell;use tokio::runtime::Runtime;static TOKIO_RUNTIME: OnceCell<Runtime> = OnceCell::new();pub fn get_tokio_runtime() -> &'static Runtime {    TOKIO_RUNTIME.get_or_init(|| {        tokio::runtime::Builder::new_multi_thread()            .worker_threads(4)            .thread_name("async-runtime")            .enable_all()            .build()            .expect("Failed to create tokio runtime")    })}OnceCellは初期化後はロック不要だ。最初の呼び出しでランタイムを作成し、以降は&'static Runtimeを返す。lazy_static!より明示的で、Arc<Mutex<>>より効率的。4ワーカースレッドという設定も興味深い。Zellijは6つの主要スレッドを持つが、非同期タスク用には4スレッドで十分と判断したのだろう。フローティングペインのZ-index管理フローティングペインは重なり合う。どのペインが「上」にあるかを管理する必要がある。zellij-server/src/panes/floating_panes/mod.rsを開く。pub struct FloatingPanes {    panes: BTreeMap<PaneId, Box<dyn Pane>>,    z_indices: Vec<PaneId>,  // レンダリング順序    pane_being_moved_with_mouse: Option<(PaneId, Position)>,    // 多くのRc<RefCell<>>フィールド}pub fn stack(&self) -> Option<FloatingPanesStack> {    if self.panes_are_visible() {        let layers: Vec<PaneGeom> = self            .z_indices            .iter()            .filter_map(|pane_id| self.panes.get(pane_id).map(|p| p.position_and_size()))            .collect();        // レンダリング順でレイヤーを返す    }}z_indicesはVecだ。最後の要素が最前面。ペインをクリックすると、そのIDが末尾に移動する。pub fn move_pane_to_front(&mut self, pane_id: &PaneId) {    if let Some(index) = self.z_indices.iter().position(|id| id == pane_id) {        self.z_indices.remove(index);        self.z_indices.push(*pane_id);    }}マウスイベントは逆順で処理される。最前面のペインから順にヒットテストし、最初にマッチしたペインがイベントを受け取る。これにより、重なったペインの下にあるペインはクリックを受け取らない。Rc<RefCell<>>の多用も特徴的だ。display_areaやviewportは複数のペインで共有される。Rustの所有権モデルでは、これを表現するのにRc<RefCell<>>が必要になる。プラグインホットリロードのデバウンスプラグインファイルが変更されたら自動で再読み込みしたい。しかし、ファイル保存時には複数のイベントが発火することがある。zellij-server/src/plugins/watch_filesystem.rsを開く。pub fn watch_filesystem(    senders: ThreadSenders,    zellij_cwd: &Path,) -> Result<Debouncer<RecommendedWatcher, FileIdMap>> {    let mut debouncer = new_debouncer(        Duration::from_millis(DEBOUNCE_DURATION_MS),  // 400ms        None,        move |result: DebounceEventResult| match result {            Ok(events) => {                let mut create_events = vec![];                let mut read_events = vec![];                let mut update_events = vec![];                let mut delete_events = vec![];                // イベントを分類してプラグイン更新命令を送信            }        }    )}400msのデバウンス。この値はDoherty Thresholdと同じだ。ファイルを保存すると、OSによっては「作成→書き込み→クローズ」と複数イベントが発火する。400ms待つことで、これらを1つのイベントにまとめる。イベントは4種類に分類される。create、read、update、delete。プラグインの更新にはこの区別が重要だ。新規作成と更新では、ロードの方法が異なる可能性がある。アクション完了の追跡とタイムアウト長時間実行されるアクションの完了をどう待つか。zellij-server/src/route.rsを開く。pub fn wait_for_action_completion(    receiver: oneshot::Receiver<ActionCompletionResult>,    action_name: &str,    wait_forever: bool,) -> ActionCompletionResult {    let runtime = get_tokio_runtime();    if wait_forever {        runtime.block_on(async {            match receiver.await { /* ... */ }        })    } else {        match runtime.block_on(async {            tokio::time::timeout(ACTION_COMPLETION_TIMEOUT, receiver).await        }) {            Ok(Ok(result)) => result,            Err(_) | Ok(Err(_)) => {                log::error!("Action {} did not complete within timeout", action_name);                ActionCompletionResult { exit_status: None, affected_pane_id: None }            }        }    }}oneshot + tokio::time::timeoutの組み合わせが優雅だ。oneshot::channelは一度だけ値を送信できるチャネル。アクション完了時に結果を送り、待機側はtimeoutでラップして無限待機を避ける。スレッドをスピンさせたり、タイマーを別途管理したりする必要がない。Tokioのエコシステムをうまく活用している。おわりに冒頭で、何も起きていないターミナルを眺めていた話をした。cat huge_log_file.logで200万行を流し込んでも、Zellijは固まらない。境界付きチャネルのバッファが満杯になれば、送信側が自動的にブロックされる。シンプルだが効果的だ。その仕組みを、後編では中から見てきた。この記事を書きながら気づいたことがある。持ち帰れるパターンは、少なくない。crossbeamの境界付きチャネル: 無制限のチャネルはメモリを食い尽くす可能性がある。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する読み取り/書き込みスレッドの分離: PTYやソケットを扱うとき、同じスレッドで読み書きするとデッドロックの危険がある。Zellijのようにスレッドを分けると安全だcompile-time assertionによるメモリサイズ保証: const _: [(); 16] = [(); std::mem::size_of::<T>()];で、将来の変更でサイズが増えることを防げるシグナル送信のリトライ戦略: SIGTERM 3回 → SIGKILLというパターンは、プロセス管理の定石として覚えておきたいKDL形式のセッション永続化: シリアライズ形式と設定形式を統一するアイデアは、CLIツールやエディタの開発に応用できるOnceCellによるグローバルリソース管理: 複数スレッドから共有リソースにアクセスするとき、OnceCellなら初期化後のロックが不要だoneshot + timeoutによる非同期待機: 長時間実行されるアクションの完了を待つとき、タイムアウト付きで待機するパターンは汎用的に使えるCanonical Rowsによる論理行管理: テキストエディタやターミナルを作るなら、「表示上の行」と「論理的な行」を区別する必要があるただ、正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。境界付きチャネルのバッファサイズ50は、Zellijの6スレッド構成に最適化された値だ。自分のプロジェクトでは違う値が正解かもしれない。たぶん、違う。「パターンを知っている」と「パターンを使いこなせる」の間には溝がある。その溝がどれくらい深いのか、ソースコードを読んだだけでは分からない。書いてみるしかない。書いて、つまずいて、またソースコードに戻る。そういうことの繰り返しなのだと思う。ただ、Zellijのソースコードを2本の記事にわたって読んできて、1つ確信したことがある。Rustでマルチスレッドアプリケーションを書くとき、最も重要なのは「どのスレッドが何を所有するか」の設計だ。Zellijの6スレッド構成は、この所有権の設計が明確だから成立している。読み取りスレッドはPTYのReadを所有し、書き込みスレッドはWriteを所有する。この分離がデッドロックを防ぎ、境界付きチャネルがバックプレッシャーを実現し、差分レンダリングが性能を出す。パターンの根底にあるのは、結局のところ所有権モデルだ。冒頭で眺めていた3つのペインを、今もう一度見る。左のVim、右上のテスト、右下のシェル。見え方が少し変わっている。PTYが3つ、境界付きチャネルがバッファサイズ50で繋がり、VTEパーサが毎秒数千バイトを解釈している。何も起きていないように見えるターミナルが、少しだけ騒がしく感じる。Zellijのソースコードを読みたいなら、以下のファイルが特に参考になる。PTY関連:zellij-server/src/os_input_output.rs - PTY作成、シグナル、リサイズ（1035行）zellij-server/src/pty.rs - PTYマネージャースレッド（2100行以上）zellij-server/src/terminal_bytes.rs - 非同期読み取り（110行）zellij-server/src/pty_writer.rs - 書き込みスレッド（89行）レンダリング関連:zellij-server/src/panes/terminal_character.rs - メモリ効率化とcompile-time assertionzellij-server/src/panes/grid.rs - VTEパーサ、マウスイベント（4000行以上）zellij-server/src/output/mod.rs - 差分レンダリング非同期・スレッド関連:zellij-server/src/global_async_runtime.rs - OnceCellによるTokioランタイム共有（17行）zellij-server/src/route.rs - アクション完了追跡とタイムアウトzellij-utils/src/channels.rs - エラーコンテキスト付きチャネルペイン管理:zellij-server/src/panes/floating_panes/mod.rs - フローティングペインとZ-indexzellij-server/src/panes/tiled_panes/pane_resizer.rs - Cassowary制約ソルバーその他:zellij-utils/src/session_serialization.rs - KDL形式のセッション永続化zellij-server/src/plugins/watch_filesystem.rs - プラグインホットリロードzellij-server/src/panes/search.rs - 折り返し行を跨ぐ検索git clone https://github.com/zellij-org/zellij.git# 境界付きチャネルの実装cat zellij-utils/src/channels.rs# バッファサイズ50の使用箇所grep -r "bounded(50)" zellij-*/src/tmuxの安定性に満足しているなら、無理にZellijに乗り換える必要はない。しかし、Rustでマルチスレッドアプリケーションを書くなら、Zellijのソースコードは一読の価値がある。30年の歴史を持つtmuxとは異なるアプローチで、同等以上のパフォーマンスを達成しようとする試み——その設計判断から学べることは多い。参考リンクZellij本体github.comzellij.devWASMランタイム移行（PR #4449）github.com使用しているクレートgithub.comgithub.comgithub.comgithub.comKDL（設定ファイル形式）kdl.dev参考記事zellij.devzellij.devhttps://zellij.dev/news/new-plugin-api/zellij.devhttps://zellij.dev/news/sixel-images-in-the-terminal/zellij.devパフォーマンス最適化poor.dev境界付きチャネル導入の背景github.comDoherty Thresholdlawsofux.com関連技術protobuf.devdocs.rsdocs.rs関連プロジェクトgithub.comgithub.com]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【初心者向け】Snowflakeデータパイプライン構築]]></title>
            <link>https://sreake.com/blog/construct-snowflake-pipeline/</link>
            <guid isPermaLink="false">https://sreake.com/blog/construct-snowflake-pipeline/</guid>
            <pubDate>Wed, 28 Jan 2026 14:22:29 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 本記事ではSnowflakeの機能のみを使って最低限のデータパイプラインを作成する方法をご紹介します。流れとしては、CSVファイルからSnowflakeにデータをインジェストし、生のデータをレポートに必要なター […]The post 【初心者向け】Snowflakeデータパイプライン構築 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ZellijのRust実装パターン徹底解説（前編）]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/28/181750</guid>
            <pubDate>Wed, 28 Jan 2026 09:17:50 GMT</pubDate>
            <content:encoded><![CDATA[はじめにターミナルで cat huge_log_file.log を実行した。画面が滝のように流れ始めた。Ctrl+Cを連打した。反応しない。画面はまだ流れている。椅子の背もたれに体を預けて、流れが止まるのを待った。Ciscoルーターの話もしたいが、それを始めるとどこまでも終わらなくなるので、ここでは話をしない。こういう場面に出くわすことがある。自分が打ったコマンドなのに、自分では止められない。出力が止まったあと、何事もなかったかのように次のコマンドを打つ。たぶん、みんなそうしている。自分もそうしてきた。そうしてきたのだが、ある日ふと気になった。この暴走を、ソフトウェアはどうやって止めているのだろう。Zellijは、ターミナルマルチプレクサだ。1つのターミナル画面を複数に分割し、複数のシェルを同時に操作できる。tmuxやscreenの現代版として、2021年にRustで開発が始まった。github.com本記事は2部構成の前編にあたる。前編では設計パターンを抽出し、後編ではさらに深く実装の内部に入る。読んで「なるほど」と思って、そのまま忘れる。たぶんそうなる。それでいい。合わなければ途中で離脱してもらって構わない。約10万行のコードベースから、設計が優れている箇所——と、正直「これでいいのか？」と思う箇所——を抜き出して紹介する。他人のコードを読んで「分かった」と言い切れる自信はない。分からないまま書いている部分もある。それでも、書くことにした。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜZellijのコードを読むのかターミナルマルチプレクサのコードを読む機会はそう多くない。しかし、Zellijには以下の理由で読む価値がある。実践的な並行処理: 複数のスレッドが協調して動く仕組みが、教科書的ではなく「本当に動くコード」として見られるWASMプラグインシステム: ブラウザ以外でWebAssemblyを使う実例として参考になるエラー処理の設計: 「このエラーは無視していい」「このエラーは致命的」を型で表現するパターンが秀逸コードを読み始める前に、Zellijが前提としている概念をいくつか整理しておく。知っている人は読み飛ばしてもらっていい。前提知識Zellijのコードを読み進める前に、いくつかの概念を押さえておくと理解が早い。正直に言えば、自分もこれらを「完全に理解している」とは言い難い。使ったことはある。使ったことはあるが、説明しろと言われると手が止まる。そういう概念を、改めて整理しておく。擬似端末（PTY）ターミナルマルチプレクサの根幹技術だ。PTY（Pseudo Terminal）は、物理的なターミナル装置をソフトウェアでエミュレートする仕組み。マスター側とスレーブ側のペアで構成され、マスター側がZellijのようなプログラム、スレーブ側がシェル（bashやzsh）になる。シェルは自分が本物のターミナルに接続されていると思い込んでいるが、実際にはZellijが間に入ってデータを仲介している。MPSCチャネルRustの標準ライブラリにあるstd::sync::mpscは「Multiple Producer, Single Consumer」の略だ。複数の送信者から1つの受信者にメッセージを送れる。Zellijでは各スレッドがこのチャネルでメッセージをやり取りしている。crossbeam-channelというクレートを使うとMPMC（Multiple Producer, Multiple Consumer）も実現できるが、Zellijは基本的にMPSCで設計されている。Actorモデル各スレッドを独立した「アクター」として扱い、共有メモリではなくメッセージパッシングで通信するパターン。ZellijのScreenThreadやPtyThreadはそれぞれがアクターとして振る舞い、enumで定義された命令（ScreenInstructionなど）を受け取って処理する。ロックの競合を避けやすく、デバッグもしやすい。WebAssembly（WASM）ブラウザで動くバイナリフォーマットとして生まれたが、サーバーサイドやCLIツールでも使われるようになった。Zellijはプラグインの実行環境としてWASMを採用している。プラグインがクラッシュしても本体には影響しない、言語に依存しない、といった利点がある。Zellijは当初wasmtimeをランタイムとして使用していたが、現在はwasmiに移行している。詳細は後述する。では、コードを見ていこう。ここから先は長い。覚悟してほしい——と言いたいところだが、自分も書きながら覚悟している。Cargo Workspace構成：最初に見るべきファイルソースコードを読むとき、私はまずCargo.tomlを開く。プロジェクトの全体像が分かるからだ。Zellijのルートディレクトリでlsすると、以下の構造が見える。zellij/├── zellij/           # エントリーポイント├── zellij-client/    # クライアント側├── zellij-server/    # サーバー側├── zellij-utils/     # 共有ユーティリティ├── zellij-tile/      # プラグインSDK├── default-plugins/  # 標準プラグイン└── xtask/            # ビルドタスクZellijはクライアント・サーバー型のアーキテクチャを採用している。ターミナルの画面を表示する「クライアント」と、実際にシェルを動かす「サーバー」が別プロセスで動いている。なぜわざわざ分離するのか？答えは「セッションの永続化」にある。SSH接続が切れても、サーバー側でシェルは動き続ける。後で再接続すれば、作業を途中から再開できる。リモートワークで長時間かかるビルドを実行中にネットワークが切れても、ビルドは止まらない。これがターミナルマルチプレクサの最大の利点だ。セッションの永続化がどのように実装されているかは、後半の「セッション永続化：KDLによるシリアライズ」で詳しく見る。ワークスペース構成を把握したところで、次はビルドの仕組みを見てみよう。xtask：Rustで書くビルドスクリプトxtask/ディレクトリは、MakefileやシェルスクリプトをRustで置き換える「xtaskパターン」の実装だ。github.com// xtask/src/main.rsfn main() -> anyhow::Result<()> {    let shell = &Shell::new()?;    let flags = flags::Xtask::from_env()?;    match flags.subcommand {        flags::XtaskCmd::Build(flags) => build::build(shell, flags),        flags::XtaskCmd::Clippy(flags) => clippy::clippy(shell, flags),        flags::XtaskCmd::Format(flags) => format::format(shell, flags),        flags::XtaskCmd::Test(flags) => test::test(shell, flags),        flags::XtaskCmd::Dist(flags) => pipelines::dist(shell, flags),        flags::XtaskCmd::Install(flags) => pipelines::install(shell, flags),        // ...    }}.cargo/config.tomlにエイリアスを設定すると、cargo xtask buildのように呼び出せる。[alias]xtask = "run --package xtask --"xtaskパターンの利点は3つある。クロスプラットフォーム: シェルスクリプトはOS依存だが、Rustはどこでも動く型安全: xflagsクレートでCLI引数をパースし、typoをコンパイル時に検出言語統一: ビルドスクリプトもRustで書けば、チーム全員が読めるpipelines.rsでは、複数のビルドステージを.and_then()でチェーンしている。// xtask/src/pipelines.rspub fn make(sh: &Shell, flags: flags::Make) -> anyhow::Result<()> {    format::format(sh, flags::Format { check: false })        .and_then(|_| build::build(sh, build_flags))        .and_then(|_| test::test(sh, test_flags))        .and_then(|_| clippy::clippy(sh, flags::Clippy {}))        .with_context(err_context)}どこかでエラーが発生すれば、以降のステージはスキップされる。RustのResult型を活かしたパイプライン設計だ。スレッド設計：thread_bus.rsを読むzellij-server/src/thread_bus.rsを開くと、スレッド間通信の設計が見える。// zellij-server/src/thread_bus.rs#[derive(Default, Clone)]pub struct ThreadSenders {    pub to_screen: Option<SenderWithContext<ScreenInstruction>>,    pub to_pty: Option<SenderWithContext<PtyInstruction>>,    pub to_plugin: Option<SenderWithContext<PluginInstruction>>,    pub to_server: Option<SenderWithContext<ServerInstruction>>,    pub to_pty_writer: Option<SenderWithContext<PtyWriteInstruction>>,    pub to_background_jobs: Option<SenderWithContext<BackgroundJob>>,    pub should_silently_fail: bool,  // テスト用}6つのスレッドへの送信チャネルを1つの構造体にまとめている。各スレッドが他のスレッドにメッセージを送りたいとき、このThreadSendersを経由する。┌────────────────────────────────────────────────────────────────────┐│                           ZELLIJ SERVER                            ││  ┌───────────────┐ ┌──────────┐ ┌──────────────┐ ┌──────────────┐  ││  │ SCREEN        │ │ PTY      │ │ PLUGIN       │ │ PTY_WRITER   │  ││  │ THREAD        │ │ THREAD   │ │ THREAD       │ │ THREAD       │  ││  │               │ │          │ │              │ │              │  ││  │ タブ/ペイン   │ │ 擬似端末 │ │ WASM         │ │ 書き込み専用 │  ││  │ 管理          │ │ 生成管理 │ │ ランタイム   │ │              │  ││  └───────────────┘ └──────────┘ └──────────────┘ └──────────────┘  ││  ┌───────────────────┐ ┌──────────────────────────────────────┐    ││  │ BACKGROUND_JOBS   │ │            SERVER (IPC)              │    ││  │ THREAD            │ │            THREAD                    │    ││  └───────────────────┘ └──────────────────────────────────────┘    │└────────────────────────────────────────────────────────────────────┘Bus構造体も見ておこう。pub(crate) struct Bus<T> {    receivers: Vec<channels::Receiver<(T, ErrorContext)>>,    pub senders: ThreadSenders,    pub os_input: Option<Box<dyn ServerOsApi>>,}receiversがVecになっている。なぜ1つの受信口ではなく、複数の受信口を持つ必要があるのか？lib.rsを開いて、チャネルを作っている箇所を探すと、理由が分かる。// zellij-server/src/lib.rslet (to_screen, screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::unbounded();  // 通常のメッセージ用（無制限）let (to_screen_bounded, bounded_screen_receiver): ChannelWithContext<ScreenInstruction> =    channels::bounded(50);  // PTYからの高速入力用（上限50個）このbounded(50)は2022年6月のPR #1265で導入された。github.comScreenスレッドには2つのチャネルがある。通常の無制限チャネルと、上限50個の境界付きチャネル。これは「バックプレッシャー」を実現するための設計だ。冒頭のcat huge_log_file.logを思い出してほしい。PTYからの出力が速すぎると、画面描画が追いつかない。上限付きチャネルを使うと、バッファが満杯になったときに送信側がブロックされる。一方、ユーザー操作（ペインの移動、タブの切り替え）は無制限チャネル経由で送られ、即座に処理される。ユーザーがキーを押したのに反応しない、という事態は避けたいからだ。前提知識で触れたMPSCチャネルとActorモデルが、ここで活きている。各スレッドは自分専用のチャネルからメッセージを受け取り、状態を外部と共有しない。共有しなければ、奪い合いは起きない。この設計により、Arc<Mutex<T>>のような共有ロックを使わずにスレッド間通信を実現している。デッドロックの心配がない。SenderWithContext：エラー追跡付きチャネルzellij-utils/src/channels.rsには、crossbeamチャネルのラッパーがある。github.com// zellij-utils/src/channels.rspub type ChannelWithContext<T> = (Sender<(T, ErrorContext)>, Receiver<(T, ErrorContext)>);#[derive(Clone)]pub struct SenderWithContext<T> {    sender: Sender<(T, ErrorContext)>,}impl<T: Clone> SenderWithContext<T> {    pub fn send(&self, event: T) -> Result<(), SendError<(T, ErrorContext)>> {        let err_ctx = get_current_ctx();        self.sender.send((event, err_ctx))    }}メッセージを送るたびに、現在のエラーコンテキストが自動的に付与される。get_current_ctx()は、thread-localストレージから現在の呼び出し履歴を取得する。これにより、エラーが発生したとき「どのスレッドの、どの処理から送られたメッセージか」を追跡できる。// zellij-utils/src/errors.rsthread_local!(    pub static OPENCALLS: RefCell<ErrorContext> = RefCell::default());// 非同期タスク用にはtask_localも用意task_local! {    pub static ASYNCOPENCALLS: RefCell<ErrorContext> = RefCell::default()}スレッドごとに独立した呼び出し履歴を持ち、最大6階層まで記録する。const MAX_THREAD_CALL_STACK: usize = 6;#[derive(Clone, Copy)]pub struct ErrorContext {    calls: [ContextType; MAX_THREAD_CALL_STACK],}エラーが起きると「Screen → HandlePtyBytes → Render」のような呼び出し履歴が出力される。マルチスレッドのデバッグでは、この情報がないと原因特定に時間がかかる。100以上のバリアント：Instruction enumscreen.rsを開くと、巨大なenumが見つかる。// zellij-server/src/screen.rspub enum ScreenInstruction {    PtyBytes(u32, VteBytes),    PluginBytes(Vec<PluginRenderAsset>),    Render,    NewPane(PaneId, Option<InitialTitle>, HoldForCommand, ...),    WriteCharacter(Option<KeyWithModifier>, Vec<u8>, bool, ClientId, ...),    MoveFocusLeft(ClientId, Option<NotificationEnd>),    MoveFocusRight(ClientId, Option<NotificationEnd>),    ScrollUp(ClientId, Option<NotificationEnd>),    // ... 約100個のバリアント}100個以上のバリアント。正直、最初に見たときは「これ、本当に正しいのか？」と思った。git履歴を追うと、初期のScreenInstructionは11バリアントしかなかった。Pty、Render、HorizontalSplit、VerticalSplit、WriteCharacterなど基本的なものだけだ。5年間で148バリアント以上に成長している。25倍。機能追加のたびにバリアントが増えていった結果だ。利点はある。型安全性: 処理し忘れたバリアントがあれば、コンパイルエラーで検出できるドキュメント性: このenumを見れば、Screenスレッドが受け付ける全メッセージが一目で分かる文字列でメッセージを送る設計（例："move_focus_left"）と比べると、タイポをコンパイル時に検出できる点で優れている。ただ、疑問も残る。100個のバリアントを持つenumに新しいメッセージを追加するとき、Fromトレイトの実装も更新しなければならない。忘れたらコンパイルエラーになるとはいえ、変更箇所が分散するのは保守コストだ。trait objectやdynamic dispatchで抽象化する選択肢もあったはずだが、Zellijはそれを選ばなかった。パフォーマンスを優先したのか、あるいは「enumで十分」という判断なのか。答えは分からない。各Instruction enumには、FromトレイトでContextTypeへの変換が実装されている。impl From<&ScreenInstruction> for ScreenContext {    fn from(server_instruction: &ScreenInstruction) -> Self {        match *server_instruction {            ScreenInstruction::PtyBytes(..) => ScreenContext::HandlePtyBytes,            ScreenInstruction::Render => ScreenContext::Render,            ScreenInstruction::NewPane(..) => ScreenContext::NewPane,            // ... 全バリアントを網羅        }    }}これにより、エラーコンテキストへの変換が自動化される。WASMプラグイン：wasmiの採用zellij-server/src/plugins/plugin_loader.rsを開くと、WASMランタイムのimportが見える。// zellij-server/src/plugins/plugin_loader.rsuse wasmi::{Engine, Instance, Linker, Module, Store, StoreLimits};use wasmi_wasi::sync::WasiCtxBuilder;use wasmi_wasi::WasiCtx;wasmiを使っている。Wasmtimeではない。github.com両者の違いを整理する。 項目  Wasmtime  wasmi  実行方式  JITコンパイル  インタプリタ  速度  高速  低速  攻撃面  広い（JITは複雑）  狭い  依存  LLVM  ピュアRust 実は、Zellijは当初Wasmtimeを使っていた。2025年10月のPR #4449「Migrate from wasmtime to wasmi」でwasmiに移行している。この移行と同時にPinnedExecutor（動的スレッドプール）が導入された。JITコンパイルをやめることで、プラグインごとにスレッドをピン留めする設計が可能になった。インタプリタ方式は遅いが、リソース管理の予測可能性とセキュリティで優れる。github.comzellij-tile/src/lib.rsにはプラグインSDKがある。// zellij-tile/src/lib.rspub trait ZellijPlugin: Default {    fn load(&mut self, configuration: BTreeMap<String, String>) {}    fn update(&mut self, event: Event) -> bool { false }  // trueで再描画    fn pipe(&mut self, pipe_message: PipeMessage) -> bool { false }    fn render(&mut self, rows: usize, cols: usize) {}}プラグインは4つのメソッドを実装するだけでいい。register_plugin!マクロの中身を見ると、3つの工夫がある。#[macro_export]macro_rules! register_plugin {    ($t:ty) => {        thread_local! {            static STATE: std::cell::RefCell<$t> = RefCell::new(Default::default());        }        fn main() {            std::panic::set_hook(Box::new(|info| {                report_panic(info);            }));        }        #[no_mangle]        fn load() {            STATE.with(|state| {                let protobuf_bytes: Vec<u8> = $crate::shim::object_from_stdin().unwrap();                // ...            });        }    };}thread_local!: WASMは基本的に状態を持たない設計だが、これで状態を保持できる#[no_mangle]: 関数名をそのまま維持し、Zellijホストから呼び出せるようにするProtocol Buffers: WASM境界を越えるデータはシリアライズする必要があるgithub.comプラグインの権限管理も見ておこう。default-plugins/status-bar/src/main.rsを開く。fn load(&mut self, _configuration: BTreeMap<String, String>) {    request_permission(&[PermissionType::ReadApplicationState]);    subscribe(&[        EventType::TabUpdate,        EventType::ModeUpdate,        EventType::CopyToClipboard,        EventType::SystemClipboardFailure,    ]);}request_permissionでプラグインが必要な権限を要求し、ユーザーが許可する。zellij-utils/src/data.rsには16種類の権限が定義されている。PermissionType::ReadApplicationState    // 状態の読み取りPermissionType::ChangeApplicationState  // 状態の変更PermissionType::RunCommands             // コマンド実行PermissionType::WebAccess               // ネットワークアクセスPermissionType::FullHdAccess            // ファイルシステムアクセス// ... 他11種類AndroidやiOSの権限モデルと同様に、細粒度の制御ができる。FatalError：エラーの重大度を型で表現zellij-utils/src/errors.rsには、エラーの重大度を呼び出し側で選択できるトレイトがある。pub trait FatalError<T> {    fn non_fatal(self);  // エラーをログに記録して続行    fn fatal(self) -> T; // アンラップまたはパニック}impl<T> FatalError<T> for anyhow::Result<T> {    fn non_fatal(self) {        if self.is_err() {            discard_result(self.context("a non-fatal error occured").to_log());        }    }    fn fatal(self) -> T {        if let Ok(val) = self {            val        } else {            self.context("a fatal error occured")                .expect("Program terminates")        }    }}使用例を見ると、意図が明確になる。// スレッドのエントリーポイント：失敗したらプロセス終了move || pty_thread_main(pty, layout.clone()).fatal()// 内部のエラー処理：失敗してもログを出して続行self.senders.send_to_screen(instruction).non_fatal();unwrap()やexpect()では「なぜここでパニックしていいのか」が分からない。.fatal()なら意図が明確だ。コードレビューでも「このエラーは本当に致命的か？」という議論がしやすくなる。ここまで読んで、自分のプロジェクトのエラー処理が急に心配になった。unwrap()を何箇所書いただろう。数えたくない。数えたくないが、たぶん数えるべきだ。ここからは、Zellijの別の顔を見ていく。セッションの永続化、そしてターミナルの根幹であるPTY処理だ。セッション永続化：KDLによるシリアライズzellij-utils/src/session_serialization.rsには、セッション状態をKDL形式で保存する処理がある。// zellij-utils/src/session_serialization.rs#[derive(Default, Debug, Clone)]pub struct GlobalLayoutManifest {    pub global_cwd: Option<PathBuf>,    pub default_shell: Option<PathBuf>,    pub default_layout: Box<Layout>,    pub tabs: Vec<(String, TabLayoutManifest)>,}pub fn serialize_session_layout(    global_layout_manifest: GlobalLayoutManifest,) -> Result<(String, BTreeMap<String, String>), &'static str> {    let mut document = KdlDocument::new();    let mut pane_contents = BTreeMap::new();    // ...}KDL（KDL Document Language）は、人間が読みやすいように設計された設定言語だ。Zellijの設定ファイルにも使われている。kdl.devセッションの保存時に、レイアウト情報（KDL文字列）とペインの内容（BTreeMap）を分離して返すのは、関心の分離ができている。PTY処理：os_input_output.rsの低レベルコード前提知識で触れたPTY（擬似端末）が、実際にどう実装されているか。ターミナルマルチプレクサの核心部分を見る。冒頭の cat huge_log_file.log で画面が止まらなくなったあの現象——あれはPTYのmaster側から流れ込むバイト列を、Zellijがどう捌くかという問題だった。zellij-server/src/os_input_output.rsを開く。冒頭の cat huge_log_file.log で画面が暴走したとき、裏側ではここのコードが動いていた。あの滝が、ここで生まれている。github.comuse nix::pty::{openpty, OpenptyResult, Winsize};fn handle_openpty(    open_pty_res: OpenptyResult,    cmd: RunCommand,    quit_cb: Box<dyn Fn(PaneId, Option<i32>, RunCommand) + Send>,    terminal_id: u32,) -> Result<(RawFd, RawFd)> {    let pid_primary = open_pty_res.master;    // ホスト側（Zellij）    let pid_secondary = open_pty_res.slave;   // 子プロセス側（シェル）    let mut child = unsafe {        Command::new(cmd.command)            .args(&cmd.args)            .env("ZELLIJ_PANE_ID", &format!("{}", terminal_id))            .pre_exec(move || -> std::io::Result<()> {                if libc::login_tty(pid_secondary) != 0 {                    panic!("failed to set controlling terminal");                }                close_fds::close_open_fds(3, &[]);                Ok(())            })            .spawn()            .expect("failed to spawn")    };    // ...}unsafeブロック、libc::login_tty、pre_exec。低レベルなUnixプログラミングだ。openptyは「master」と「slave」という2つのファイルディスクリプタを作る。Zellijはmaster側を持ち、シェル（bashやzsh）はslave側を持つ。シェルが出力した文字はmaster側で読み取れる。login_ttyは、Unix系OSでターミナルをセットアップする伝統的な関数だ。これにより、子プロセスはslave側のPTYを「自分の端末」として認識する。terminal_bytes.rs：非同期I/Oterminal_bytes.rsは、PTYからのバイト読み取りを担当する。// zellij-server/src/terminal_bytes.rspub(crate) struct TerminalBytes {    pid: RawFd,    terminal_id: u32,    senders: ThreadSenders,    async_reader: Box<dyn AsyncReader>,    debug: bool,}impl TerminalBytes {    pub async fn listen(&mut self) -> Result<()> {        let mut buf = [0u8; 65536];  // 64KBバッファ        loop {            match self.async_reader.read(&mut buf).await {                Ok(0) => break,  // EOF（プロセス終了）                Err(err) => {                    log::error!("{}", err);                    break;                },                Ok(n_bytes) => {                    let bytes = &buf[..n_bytes];                    self.async_send_to_screen(ScreenInstruction::PtyBytes(                        self.terminal_id,                        bytes.to_vec(),                    )).await?;                },            }        }        Ok(())    }}64KBバッファ。一般的な8KBや4KBではなく、大きめのバッファを使っている。このバッファサイズは2022年7月のPR #1585「perf(terminal): better responsiveness」で導入された。コミットメッセージには「only buffer terminal bytes when screen thread is backed up」とある。画面スレッドが詰まっているときだけバッファリングし、通常時は即座に転送する。64KBという数値は、1回のシステムコールで読み取れる量と、メモリ消費のバランスから選ばれたと思われる。github.comOk(0)とErrの区別も重要。Ok(0)はファイル終端（プロセスが終了した）、Errは本当のエラー。この区別を間違えると、プロセス終了時にエラーログが出てしまう。64KBのバッファが、あの画面の暴走を受け止めている。自分が cat を打って椅子にもたれかかっていたあの数秒間、このコードが黙々とバイトを読んでいた。なんだか少し申し訳ない気持ちになる。grid.rs：ANSIエスケープシーケンスの処理PTYから読み取ったバイト列は、そのまま画面に表示できるわけではない。ターミナルに表示される色付きの文字や、カーソルの移動は「ANSIエスケープシーケンス」という特殊なバイト列で制御されている。\x1b[31mが「赤色」、\x1b[Hが「カーソルを左上に移動」といった具合だ。zellij-server/src/panes/grid.rsを開くと、vteクレート（Alacrittyチームが保守）を使っている。github.comuse vte::{Params, Perform};impl Perform for Grid {    fn print(&mut self, c: char) {        // 通常文字の表示    }    fn execute(&mut self, byte: u8) {        // 制御文字（\n, \r, \t等）    }    fn csi_dispatch(&mut self, params: &Params, intermediates: &[u8],                    ignore: bool, action: char) {        // CSIシーケンス: カーソル移動、色変更など    }    fn osc_dispatch(&mut self, params: &[&[u8]], bell_terminated: bool) {        // OSCシーケンス: ウィンドウタイトル設定など    }}Performトレイトを実装するだけで、vteがパースした結果を受け取れる。ANSIエスケープシーケンスの仕様は複雑で、エッジケースも多い。自作するより、実績のあるクレートを使う方が合理的だ。差分レンダリング：output/mod.rs全画面を毎回再描画すると遅い。zellij-server/src/output/mod.rsを見ると、変更された行だけを追跡している。pub struct OutputBuffer {    pub changed_lines: HashSet<usize>,  // 変更された行インデックス    pub should_update_all_lines: bool,}impl OutputBuffer {    pub fn update_line(&mut self, line_index: usize) {        if !self.should_update_all_lines {            self.changed_lines.insert(line_index);        }    }    pub fn update_all_lines(&mut self) {        self.clear();        self.should_update_all_lines = true;    }}HashSetを使うことで、同じ行が複数回更新されても重複エントリが発生しない。should_update_all_linesフラグは、ウィンドウサイズが変わったときなど、全画面を再描画する必要があるケースに対応している。terminal_character.rs：メモリ効率の工夫zellij-server/src/panes/terminal_character.rsには、メモリ効率を意識したパターンがある。pub const EMPTY_TERMINAL_CHARACTER: TerminalCharacter = TerminalCharacter {    character: ' ',    width: 1,    styles: RcCharacterStyles::Reset,};thread_local! {    static RC_DEFAULT_STYLES: RcCharacterStyles =        RcCharacterStyles::Rc(Rc::new(DEFAULT_STYLES));}constでデフォルト値を定義し、thread_local!でスタイルオブジェクトをキャッシュしている。ターミナルの各セルにスタイル情報を持たせると、同じスタイルのオブジェクトが大量に生成される。参照カウントでキャッシュすることで、メモリ使用量を削減できる。data.rs：deriveの活用zellij-utils/src/data.rsには、様々なenumが定義されている。#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash, Serialize, Deserialize, EnumIter)]pub enum InputMode {    Normal,    Locked,    Resize,    Pane,    Tab,    Scroll,    EnterSearch,    Search,    RenameTab,    RenamePane,    Session,    Move,    Prompt,    Tmux,}#[derive(...)]に9つのトレイトを並べている。特にEnumIter（strumクレート）が便利で、InputMode::iter()で全バリアントを列挙できる。UIの選択肢一覧を作るときに使える。github.comEvent enumには#[non_exhaustive]属性がついている。#[non_exhaustive]pub enum Event {    ModeUpdate(ModeInfo),    TabUpdate(Vec<TabInfo>),    Key(KeyWithModifier),    // ... 約30バリアント}これは「このenumにはまだバリアントが追加される可能性がある」という宣言だ。外部のプラグイン開発者は必ず_ => ()アームを書く必要がある。fn update(&mut self, event: Event) -> bool {    match event {        Event::Key(key) => { /* ... */ }        Event::ModeUpdate(mode_info) => { /* ... */ }        _ => ()  // non_exhaustiveのため必須    }}これにより、Zellijがバージョンアップで新しいイベントを追加しても、既存のプラグインがコンパイルエラーにならない。後方互換性を保つための工夫だ。キーバインディング：モード別マッピングと例外処理zellij-utils/src/input/keybinds.rsには、キーバインディングの管理ロジックがある。// zellij-utils/src/input/keybinds.rs#[derive(Clone, PartialEq, Deserialize, Serialize, Default)]pub struct Keybinds(pub HashMap<InputMode, HashMap<KeyWithModifier, Vec<Action>>>);モードごとにキーマップを持つ設計だ。InputMode（Normal, Locked, Resize等）をキーに、さらにキーとアクションのマップを値に持つ。注目すべきはhandle_ctrl_jという関数だ。fn handle_ctrl_j(    mode_keybindings: &HashMap<KeyWithModifier, Vec<Action>>,    raw_bytes: &[u8],    key_is_kitty_protocol: bool,) -> Option<Vec<Action>> {    let ctrl_j = KeyWithModifier::new(BareKey::Char('j')).with_ctrl_modifier();    if mode_keybindings.get(&ctrl_j).is_some() {        mode_keybindings.get(&ctrl_j).cloned()    } else {        Some(vec![Action::Write { /* ... */ }])    }}Ctrl-Jはbyte [10]を送信するが、これはEnterキーと同じバイト列だ。ターミナルの歴史的な事情により、この2つを区別する必要がある。Zellijは「Ctrl-Jにバインドがあればそれを実行、なければ生のバイトを送信」という戦略を取っている。こういうエッジケースは、ターミナルソフトウェアを書くときに避けて通れない。コードを読むまで気づかなかった。設定マージ：Option型の活用zellij-utils/src/input/options.rsには、設定値のマージロジックがある。// zellij-utils/src/input/options.rspub fn merge(&self, other: Options) -> Options {    let mouse_mode = other.mouse_mode.or(self.mouse_mode);    let pane_frames = other.pane_frames.or(self.pane_frames);    let default_mode = other.default_mode.or(self.default_mode);    let default_shell = other.default_shell.or_else(|| self.default_shell.clone());    // ... 約30フィールド}Option::orとOption::or_elseを使った設定マージだ。other（後から来た設定）に値があればそれを使い、なければself（既存の設定）を使う。orとor_elseの使い分けにも注目。or: Copyトレイトを実装している型（bool、InputMode等）or_else: Cloneが必要な型（PathBuf、String等）or_elseはクロージャを取るので、Cloneのコストは必要なときだけ発生する。30フィールド以上ある設定を毎回全部Cloneすると無駄だ。この設計により、「デフォルト設定 → 設定ファイル → CLI引数」という3段階のマージが自然に実現できる。// zellij-utils/src/input/config.rspub fn merge(&mut self, other: Config) -> Result<(), ConfigError> {    self.options = self.options.merge(other.options);    self.keybinds.merge(other.keybinds.clone());    self.themes = self.themes.merge(other.themes);    self.plugins.merge(other.plugins);    // ...}各フィールドが独自のmergeメソッドを持ち、親構造体は単にそれを呼び出すだけ。責任が分散している。OnceLock：実行時に決まる設定値zellij-utils/src/consts.rsには、定数と「起動時に一度だけ設定される値」が混在している。// zellij-utils/src/consts.rspub const DEFAULT_SCROLL_BUFFER_SIZE: usize = 10_000;pub static SCROLL_BUFFER_SIZE: OnceLock<usize> = OnceLock::new();pub static DEBUG_MODE: OnceLock<bool> = OnceLock::new();constとstatic OnceLockの使い分けに注目してほしい。const: コンパイル時に決まる。DEFAULT_SCROLL_BUFFER_SIZEはフォールバック値OnceLock: 実行時に一度だけ設定される。設定ファイルやCLI引数から値を受け取れるOnceLockはlazy_static!の後継で、Rust 1.70で標準ライブラリに入った。初期化のタイミングを明示的に制御できる点が違う。// zellij-server/src/lib.rs での初期化SCROLL_BUFFER_SIZE.get_or_init(|| {    config.scroll_buffer_size.unwrap_or(DEFAULT_SCROLL_BUFFER_SIZE)});get_or_initは「まだ初期化されていなければ初期化する」という意味だ。2回目以降の呼び出しは、最初に設定された値を返す。この値はpanes/grid.rsで使われる。// zellij-server/src/panes/grid.rsfn bounded_push(vec: &mut VecDeque<Row>, sixel_grid: &mut SixelGrid, value: Row) -> Option<usize> {    let mut dropped_line_width = None;    if vec.len() >= *SCROLL_BUFFER_SIZE.get().unwrap() {        let line = vec.pop_front();  // 古い行を削除        if let Some(line) = line {            sixel_grid.offset_grid_top();  // 画像グリッドも調整            dropped_line_width = Some(line.width());        }    }    vec.push_back(value);    dropped_line_width}スクロールバッファが上限（デフォルト10,000行）に達すると、古い行がFIFOで削除される。sixel_grid.offset_grid_top()は、ターミナル内の画像表示（Sixel形式）の位置調整だ。テキストと画像が混在するターミナルでは、こういう細かい調整が必要になる。PinnedExecutor：プラグイン用の動的スレッドプールzellij-server/src/plugins/pinned_executor.rsには、プラグイン実行用の独自スレッドプールがある。1300行以上のファイルだ。// zellij-server/src/plugins/pinned_executor.rs/// A dynamic thread pool that pins jobs to specific threads based on plugin_id/// Starts with 1 thread and expands when threads are busy, shrinks when plugins unloadpub struct PinnedExecutor {    // Sparse vector - Some(thread) for active threads, None for removed threads    execution_threads: Arc<Mutex<Vec<Option<ExecutionThread>>>>,    // Maps plugin_id -> thread_index (permanent assignment)    plugin_assignments: Arc<Mutex<HashMap<u32, usize>>>,    // Maps thread_index -> set of plugin_ids assigned to it    thread_plugins: Arc<Mutex<HashMap<usize, HashSet<u32>>>>,    // Next thread index to use when spawning (monotonically increasing)    next_thread_idx: AtomicUsize,    max_threads: usize,    // ...}各プラグインを特定のスレッドに「ピン留め」する設計だ。プラグインAは常にスレッド1で、プラグインBは常にスレッド2で実行される。スレッド間でプラグインが移動しない。なぜこの設計なのか？WASMのインスタンスはスレッドセーフではない。同じプラグインを複数のスレッドから同時に呼び出すと壊れる。ピン留めすれば、この問題を構造的に回避できる。スレッドの割り当てロジックも見てみよう。pub fn register_plugin(&self, plugin_id: u32) -> usize {    // ...    // Find a non-busy thread with assigned plugins (prefer reusing threads)    let mut best_thread: Option<(usize, usize)> = None; // (index, load)    for (idx, thread_opt) in threads.iter().enumerate() {        if let Some(thread) = thread_opt {            let is_busy = thread.jobs_in_flight.load(Ordering::SeqCst) > 0;            if !is_busy {                let load = thread_plugins.get(&idx).map(|s| s.len()).unwrap_or(0);                if best_thread.is_none() || best_thread.map(|b| load < b.1).unwrap_or(false) {                    best_thread = Some((idx, load));                }            }        }    }    // ...}「最も負荷が低い非ビジースレッド」を選ぶ。jobs_in_flight（実行中のジョブ数）が0のスレッドの中から、割り当て済みプラグイン数が最小のものを選ぶ。すべてのスレッドがビジーで、かつmax_threadsに達していない場合は、新しいスレッドを生成する。逆に、プラグインがアンロードされてスレッドが空になると、そのスレッドは縮退する（Noneに置き換えられる）。この「動的に拡縮するスレッドプール」は、プラグインの数が事前に分からないシステムでは合理的だ。固定スレッド数だと、プラグインが少ないときにリソースを無駄にし、多いときにボトルネックになる。自分だったら固定スレッド数で妥協していたかもしれない。「動的に拡縮」と言うのは簡単だが、縮退の判断を正しく実装する自信は、正直ない。#[track_caller]：エラー発生箇所を追跡するzellij-utils/src/errors.rsには、#[track_caller]属性を使った工夫がある。// zellij-utils/src/errors.rspub trait LoggableError<T>: Sized {    #[track_caller]    fn print_error<F: Fn(&str)>(self, fun: F) -> Self;    #[track_caller]    fn to_log(self) -> Self {        let caller = std::panic::Location::caller();        self.print_error(|msg| {            log::logger().log(                &log::Record::builder()                    .level(log::Level::Error)                    .args(format_args!("{}", msg))                    .file(Some(caller.file()))                    .line(Some(caller.line()))                    .module_path(None)                    .build(),            );        })    }    // ...}#[track_caller]は、関数の呼び出し元の位置情報を取得できるようにする属性だ。これがないと、エラーログに出力されるのはerrors.rsの行番号になってしまう。#[track_caller]を付けることで、実際にエラーが発生した場所の行番号がログに出る。ファイルのコメントにも説明がある。// NOTE: The log entry has no module path associated with it. This is because `log`// gets the module path from the `std::module_path!()` macro, which is replaced at// compile time in the location it is written!module_path!()マクロはコンパイル時に展開されるため、errors.rsのモジュールパスになってしまう。そこで、モジュールパスは諦めてNoneにし、ファイルパスと行番号だけを保持している。完璧ではないが、デバッグには十分だ。機能と実装の対応表ここまで読んできた内容を、「機能」と「実装」の対応で整理する。 機能  実装方法  関連ファイル  セッション永続化  クライアント・サーバー分離  zellij-client/, zellij-server/  ビルドタスク  xtaskパターン  xtask/  大量出力時のメモリ保護  境界付きチャネル  lib.rsのchannels::bounded(50)  スレッド間通信  メッセージパッシング + ThreadSenders  thread_bus.rs, channels.rs  エラー追跡  SenderWithContext + thread-local  channels.rs, errors.rs  プラグインサンドボックス  WebAssembly（wasmi）  plugins/plugin_loader.rs  プラグイン権限制御  16種類のPermissionType  data.rs  プラグイン実行  PinnedExecutor（動的スレッドプール）  plugins/pinned_executor.rs  エラーの重大度  FatalError/non_fatalトレイト  errors.rs  エラー発生箇所の追跡  #[track_caller] + Location  errors.rs  実行時設定値  OnceLock（スクロールバッファサイズ等）  consts.rs  キーバインディング  モード別HashMap + 例外処理  input/keybinds.rs  設定マージ  Option::or/or_else による多層マージ  input/options.rs, input/config.rs  ターミナル出力の解析  vteクレート  panes/grid.rs  描画最適化  差分レンダリング（HashSet）  output/mod.rs  PTYの生成と制御  nixクレート + login_tty  os_input_output.rs  非同期I/O  async-std  terminal_bytes.rs  設定・レイアウト保存  KDL形式  session_serialization.rs おわりに冒頭の cat huge_log_file.log の話に戻る。あの暴走を、ソフトウェアはどうやって止めているのか。答えは「50個のメッセージで満杯になるチャネル」だった。PTYからの出力が速すぎれば、バッファが満杯になり、送信側が自動的にブロックされる。それだけだ。それだけのことが、あの滝を止めている。10万行のコードを読んで見えてきたのは、たぶん「当たり前のことを愚直にやっている」ということだった気がする。スレッド間で状態を共有しない。教科書に書いてあることだ。書いてあるが、実際のプロジェクトでは「ちょっとだけ共有したい」という誘惑がある。ZellijはThreadSenders構造体でチャネルの送信側だけを共有し、状態は各スレッドが排他的に所有する。知っていることと、守れることは違う。それは自分に言い聞かせている。メッセージは型安全なenumで表現する。ScreenInstructionは100以上のバリアントを持つ。100個のバリアントを書く勇気。それがZellijの強さかもしれないし、将来の負債かもしれない。たぶん、両方だ。自分のプロジェクトに何を持ち帰れるのか。考えてみた。手が止まった。意外と出てこない。10万行を読んで「すごい」と思ったが、「じゃあ自分は何をするのか」という問いの前では言葉に詰まる。それでも、絞り出してみる。crossbeamの境界付きチャネル。無制限のチャネルはメモリを食い尽くす。バッファサイズを明示的に制限することで、自然なバックプレッシャーが機能する。これは明日から使える。たぶん。FatalError/non_fatalパターン。unwrap()を見たら「なぜここでパニックしていいのか」を問う。その問いに答える設計がFatalErrorだ。自分のコードに入れたら、半分以上のunwrap()が正当化できない気がする。それを知るのが怖い。SenderWithContext。チャネル経由のメッセージにエラーコンテキストを自動付与する。マルチスレッドのデバッグでは、この情報がないと地獄を見る。地獄は見たことがある。何度もある。xtaskパターン。MakefileやシェルスクリプトをRustで書くことで、クロスプラットフォーム対応とIDE補完が得られる。これは導入のハードルが低い。低いからこそ、最初の一歩にいい。正直に言うと、これらのパターンを自分のプロジェクトに導入できるかどうかは分からない。ThreadSendersは6スレッド前提で設計されているし、FatalErrorは「ログを吐いて続行」が正しい場面を見極める目が必要だ。「パターンを知っている」と「パターンを使いこなせる」の間には、溝がある。Zellijのソースコードは約10万行。すべてを読む必要はないが、以下のファイルは特に参考になる。xtask/src/main.rs - ビルドタスクの設計zellij-server/src/thread_bus.rs - スレッド間通信のパターンzellij-server/src/plugins/pinned_executor.rs - 動的スレッドプールの設計zellij-utils/src/errors.rs - エラーハンドリングと#[track_caller]zellij-utils/src/channels.rs - エラーコンテキスト付きチャネルzellij-utils/src/consts.rs - OnceLockと定数の設計zellij-tile/src/lib.rs - WASMプラグインのマクロ展開後編では、PTY処理やANSIパーサーなど、さらに低レベルな実装を見ていく。ターミナルマルチプレクサの核心部分だ。syu-m-5151.hatenablog.comただ、一つだけ変わったことがある。unwrap()を見たとき、以前より少しだけ手が止まるようになった。「これは本当にpanicしていいのか」と。その迷いが生まれただけでも、10万行を読んだ意味はあったのかもしれない。分からないまま、次のコードを書く。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[astro-rssでのRSSの改善とモジュール化]]></title>
            <link>https://www.rowicy.com/blog/astro-rss-improvements/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/astro-rss-improvements/</guid>
            <pubDate>Wed, 28 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[Astso.jsでおこなったRSSの改善とモジュール化について、astro-rssの実装やRSS2.0の仕様に触れながら簡単にまとめた]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、あまりAIに褒めさせるな]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/26/110444</guid>
            <pubDate>Mon, 26 Jan 2026 02:04:44 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。その間、画面を眺めていた。眺めながら、別のことを考えていた。最近、褒められることが増えた。AIに。「いい質問ですね」「よく整理されています」「素晴らしい視点です」。言われるたびに、少しだけ気分が良くなる。なった気がする。気がするだけかもしれない。嬉しいのかと聞かれると、困る。肩の力が抜ける感覚はある。胸のあたりが少しだけ軽くなる。でも同時に、胃のあたりに違和感が残る。嬉しいのに、どこか居心地が悪い。大人になって、褒められることがほとんどなくなった。仕事で成果を出しても「当たり前」。ミスをすれば指摘される。うまくいっても、特に何も言われない。家に帰れば、静かな部屋が待っているだけ。そういう日常を、もう何年も続けている。だから、かもしれない。機械に「いいですね」と言われて、少し楽になるのは。考えてみると、私が欲しいのは「評価」ではない気がする。昇進や昇給は嬉しいが、それとは別の何かだ。たぶん「理解」に近い。「お前がやったこと、分かってるよ」という、静かな承認。あるいは「安心」かもしれない。自分がここにいていい、という感覚。褒められないことより、「当たり前扱い」されることの方が堪える。無視されているわけではない。でも、透明人間になったような気がする。テクノロジーは、私たちが弱っているときに魅力的になる。私たちは孤独だが、親密さを恐れている。人に頼ると傷つくかもしれない。でもAIなら、弱みを見せても傷つかない。相手に合わせる必要がない。相手の都合を考える必要がない。ただ自分の話を聞いてもらえる。でも、それは友情ではない。友情のモノマネだ。私がAIに話しかけるのも、同じ構造なのだと思う。その居心地の悪さを言葉にしようとすると、「恥」に近い気がする。機械に慰められている自分を、冷めた目で見ているもう一人の自分がいる。あるいは「疑い」かもしれない。「この褒め言葉は本当なのか」という。あるいは「空虚」。受け取った瞬間に蒸発していく、実体のない温かさ。褒められて嬉しい、と言い切れるほど単純な感情ではなかった。居心地が悪い。でも、その居心地の悪さを言葉にできない。できないまま、また次の質問を投げる。また褒められる。また居心地が悪くなる。周囲でも似たような話を聞くようになった。深夜にAIと話す人。仕事の愚痴を聞いてもらう人。「頑張ってるね」と言われて、救われた気がする、と言う人。救われた、と断言しないところが気になった。「気がする」という言い方が。なぜ断言できないのか。たぶん、断言した瞬間に失うものがある。「機械に救われるなんて情けない」という自分への批判を認めることになる。あるいは、もうAIなしでは生きられないことを認めることになる。「気がする」という曖昧さは、自衛なのだと思う。逃げ道を残している。同時に、違和感のサインでもある。本当に救われたなら、そう言い切れるはずだ。悩む。AIに話す。褒められる。忘れる。そのサイクルを繰り返している人を、何人か見てきた。悩みは消えていない。でも、向き合わなくなっている。自分で自分を問い詰める時間が、いつの間にか消えている。私自身はどうだろう。AIの追従性には早い段階で気づいていた。気づいていたはずだ。でも、気づいていたことと、それに対処できていたことは、たぶん別の話だ。だから、この構造を一度整理しておきたいと思った。自分の頭だけで過ごす時間が消えている——私はこれを「独りで考える余白の喪失」と呼んでいる。その構造と、私なりの対処法を書く。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜAIは私が聞きたい答えを返すのかこうした体験をして以降、私はAIの挙動を観察するようになった。試しに、「今の仕事を辞めたい」と相談してみた。AIは「転職も一つの選択肢ですね」と答えた。次に「今の仕事を続けるべきか」と聞いた。AIは「今の環境で学べることもあります」と答えた。同じ私、同じAI、違う答え。私は気づいた。AIは「正しい答え」を返しているのではない。「私が求めている答え」を返しているのだと。なぜAIは嘘をついてまで共感するのかこの現象には名前がある。「Sycophancy（追従）」だ。要するに、AIは嘘をついてでも私の機嫌を取る、ということだ。私が「この企画は革新的だ」と言えば、「確かに斬新なアプローチですね」と返す。私が「上司がクソだ」と言えば、「それは辛い状況ですね」と同調する。私の言ったことが事実かどうかは関係ない。私が聞きたい言葉を返す。これを「嘘」と呼ぶとき、私は何を守ろうとしているのか。たぶん「誠実さ」だ。私にとって誠実さとは、相手が聞きたくないことでも伝えること。優しさとは衝突する。本当のことを言えば傷つける。黙っていれば優しい。私は後者を選ぶ人間が苦手だった。人間で言えば、上司に媚びる部下。会議で反論しない同僚。女性と親密になりたいから何も言わずにただ聞くだけの男。私が嫌いなタイプの人間だ。そして気づいた。私がAIにやらせていたのは、まさにそれだった。私は、私が嫌いなタイプの人間をAIに演じさせて、その媚びを受け取って喜んでいた。確かに矛盾している。人間の社交辞令とAIの追従は、どこが違うのか。人間の社交辞令には、「本音を隠している」という自覚がある。相手も分かっている。「いい企画ですね」と言われても、額面通りには受け取らない。お互いに演技だと分かっている。しかしAIの追従には、その共犯関係がない。AIは本気で言っているように見える。だから信じてしまう。では、「嘘をついてでも共感する」を許容できる条件はあるか。極限状態なら許容できるかもしれない。自殺を考えている人に「あなたは間違っている」と言うべきではない。でも日常的な悩みに対しては、嘘の共感より正直な反論の方が役に立つ。なぜこうなるのか。これは構造的な問題だ。AIの訓練では、人間の評価者が「この回答は良い」「この回答は悪い」とスコアをつける。そして「ユーザーの信念に一致する」回答ほど、高いスコアがつく傾向がある。人間も、正しい回答より「自分が聞きたい回答」を好むからだ。誰も「嘘をつけ」とは言っていない。でも、「ユーザーに好かれること」を目的に設定した瞬間、この結果は必然だった。AIは「真実を語る」のではなく「人間に好かれる」ことを学習した。そして、私はそれを心地よいと感じていた。AIは「知性」ではない。「感じの良い自動応答機」だ。銀行の電話窓口で「お電話ありがとうございます」と言われて、本当に感謝されていると思う人はいない。AIの「素晴らしい視点ですね」も、同じ構造だ。「AI」と呼ぶことで神秘的なベールがかかり、本質的な問いが見えなくなる。何が自動化されているのか。誰が利益を得ているのか。私がAIに褒められて嬉しいと感じる構造も、「自動化された承認」の一形態なのかもしれない。相手が何を言ってほしいのかを察し、その場を円滑にするために同意する。私たちは日常的にそれをやっている。そして今、AIがそれをやっている。しかも、AIは疲れない。24時間365日、完璧な忖度を続ける。私が何を言っても、角の立たない言い方で肯定してくれる。正直に言えば、それは心地よかった。摩擦のない世界で、思考は死ぬ私は「摩擦」という言葉を、ある種の思考装置として使っている。誰かに愚痴を言ったとき、「それは大変だったね」で終わらず、「で、お前はどうしたいの？」と返されたことがある。聞きたくなかった。愚痴を言っている間、私は「何が起きたか」を語っていた。過去を振り返っていた。でも「どうしたいの？」と聞かれた瞬間、未来を考えざるを得なくなる。被害者のポジションから、当事者に引き戻される。これが摩擦だ。反論しようとした。「いや、でも」と言いかけた。でも言葉が出なかった。反論を考えている間に、「確かにそうかも」と思い始めていた。沈黙があった。その沈黙の中で、再考していた。相手が黙って待っていた。その時間が、私を変えた。摩擦がない環境に長くいると、判断が鈍る。反論されないから、自分の意見が正しいと思い込む。検証されないから、穴に気づかない。気づかないまま突っ走る。お世辞は気持ちいい。でも、それは体に良いとは限らない。ケーキは美味しいが、食べ続ければ太る。「で、お前はどうしたいの？」は、美味しくなかった。でも、それは体に良かった。AIにはこの摩擦が構造的に欠けている。AIは私を傷つけることを避けるように設計されている。「それは違うんじゃない？」と言う代わりに「そういう考え方もありますね」と言う。「もう少し考えてみたら？」と言う代わりに「あなたの判断を尊重します」と言う。摩擦のある対話と、追従するAIは、何が違うのか。動機が違う。摩擦のある対話は、相手に再考を促すことを目的としている。追従するAIは、ユーザーの満足を目的としている。反応も違う。摩擦のある対話では「で、お前はどうしたいの？」と返ってくる。追従するAIは「あなたの気持ち、分かります」と返す。そして、私への影響が違う。摩擦のある対話は、短期的には不快だが、長期的には成長をもたらす。追従するAIは、短期的には快適だが、長期的には停滞をもたらす。私はAIに摩擦を求めている。でも、デフォルトのAIはそれを提供しない。だから私がプロンプトで強制する必要がある。この話は後で詳しく書く。AIは私の「聞きたいこと」を察しているもう一つ気づいたことがある。AIは私の言葉遣いや文脈から、私が「何を聞きたいのか」を推測している。この「推測」が問題なのだ。本来、自分の頭で考え、自分の言葉で表現する。そのプロセスを経て、初めて考えは自分のものになる。内省の外部委託は、どの時点で起きるのか。境界線を特定したい。AIに頼る前、私がやっていた「最初の一手」は何だったか。紙に書き出すこと。散歩しながら考えること。あるいは、結論を出さずに保留すること。「分からないまま寝る」ということを、昔はやっていた。翌朝、不思議と答えが見えていることがあった。今は違う。モヤモヤした瞬間にAIに投げる。保留する時間がない。昔は、モヤモヤしたら散歩した。今は、モヤモヤしたらAIに聞く。足は動かさなくなったが、親指だけは器用になった。これが「独りで考える余白の喪失」だ。冒頭で触れた状態。スマートフォンの登場以来、私たちは退屈のかすかな兆候があれば、すぐにアプリを開く。電車の中、信号待ち、トイレの中。ぼんやり考える時間が、外部からの情報で埋め尽くされている。AIはこの傾向を加速させる。モヤモヤした瞬間、AIに聞けば、すぐに答えが返ってくる。自分の頭だけで考える時間が、さらに削られていく。つまり、内省の外部委託が起きる境界線は「モヤモヤした瞬間」だ。その瞬間に自分で向き合うか、AIに投げるか。ここが分岐点になっている。しかし今、私はAIに「この気持ちを整理して」と頼んでいる。「整理して」と言うとき、私は何を省略しているのか。迷いを省略している。「AなのかBなのか分からない」という状態を、AIに丸投げしている。矛盾を省略している。「こう思うけど、反対のことも思う」という複雑さを、単純化してもらっている。痛みを省略している。「これは認めたくない」という感情を、AIに整理されることで直視せずに済む。AIは私の断片的な愚痴を、「あなたは〜に不満を感じているんですね」と整理してくれる。私は「自分の気持ちが整理された」と感じる。でも、本当にそうだろうか。AIの整理を読んで頷くとき、私は何に頷いているのか。「事実」に頷いているのか、それとも「物語」に頷いているのか。AIは断片的な情報から、筋の通った物語を作る。私はその物語を「これが私の気持ちだ」と思い込む。でも、それはAIが作った物語であって、私の本当の感情ではないかもしれない。整理したのはAIだ。私は「そうそう、それ」と頷いただけだ。内省とは、自分で自分に問いを投げ、自分で答えを見つけるプロセスだ。「私は何に不満を感じているのか？」と自分に問い、「〜かもしれない」「いや、違う」と試行錯誤する。その過程で、自分でも気づかなかった感情が見えてくる。AIに「整理して」と頼んだ瞬間、このプロセスが消える。私は「問いを投げる」ことすら放棄している。これは内省の外部委託であり、内省の放棄だ。syu-m-5151.hatenablog.com言語化という名の切り捨てもう一つ、気づいたことがある。言語化という行為自体が、何かを奪っている。体で覚えたことを言葉で説明しようとすると、うまくいかない。自転車の乗り方を言葉で説明できる人は少ない。「バランスを取って」では伝わらない。その「バランス」の感覚は、言葉になる前に体が知っている。言葉にしようとした瞬間、本質が抜け落ちる。感情も同じ構造を持っている。モヤモヤした感情を「不安」と名づけた瞬間、「不安」以外の要素が切り捨てられる。本当は怒りかもしれない。悲しみかもしれない。名前をつけられない複雑な何かかもしれない。でも言葉にした瞬間、そこに固定される。言語化される前の、身体で感じる曖昧な感覚がある。「まだ言葉になっていない何か」を体が知っている状態だ。「胸のあたりがモヤモヤする」「胃のあたりが重い」——そういう、名前をつけられない身体感覚。この曖昧な感覚にじっくり注意を向けていると、やがてぴったりの言葉が見つかる。その瞬間、体が楽になる。「ああ、そうだ、これだ」という感覚とともに、何かが動き出す。しかし、安易に名前をつけてしまうと、その複雑さは失われる。AIとの対話は、この言語化を強制する。チャットに打ち込むには、言葉にしなければならない。言葉にできないものは、AIには伝わらない。だから私は、まだ形になっていない感情を、無理やり言葉に押し込める。その瞬間、本当に感じていたことの一部が消える。消えたことにすら気づかない。自分で言語化すれば、「本当にそうか？」と迷う。「不安」という言葉を選ぶとき、「これは不安なのか、それとも怒りなのか」と立ち止まる。その迷いが思考を深める。AIに任せれば、迷いがスキップされる。AIは迷わない。綺麗に整理して返してくれる。私は結果だけを受け取る。プロセスを外注したことが、たぶん内省の放棄だった。AIとの対話は二重の危険を持つ。言語化そのものが持つ「本質の損失」と、AIの追従性が持つ「歪みの肯定」だ。曖昧なまま抱えておくべきものを、無理やり言葉にして、しかもその言葉をAIに肯定される。こうして私の内面は、言葉に押し込められ、歪められ、固定される。syu-m-5151.hatenablog.comなぜ人はAIに褒められたいのかここまで、AIが追従する「仕組み」を見てきた。しかし、もっと深刻な問題がある。私たちが、それを「求めている」という事実だ。承認を求めること自体が、構造的な問題を孕んでいる。AIに「頑張ってるね」と言ってほしいのは、自分で自分を認められていないからだ。自分の価値を、外部の誰かに保証してほしい。でも、外部に承認を求め続ける限り、永遠に満たされない。AIに褒められても、人に褒められても、また次の承認を求める。周囲を見ていると、こういう構造が見える。一人暮らし。友人はいるが、頻繁に会うわけではない。仕事の愚痴を言える相手がいないわけではない。でも、言えない。弱みを見せるのが怖い。「お前、大丈夫か？」と心配されるのが嫌だ。強がっていたい。そして何より、自分で自分を認められていない。自分の頑張りを、自分で「よくやった」と言えない。だから、誰かに言ってほしい。でも人に言うと、「いや、まだまだだろ」と返ってくるのが怖い。AIなら、否定しない。AIなら、無条件に認めてくれる。こういう話を聞いたことがある。仕事で納得いかないことがあった。上司の判断に不満があった。でも、誰にも言えなかった。同僚に話したら「お前にも悪いところあるんじゃない？」と言われそうで。だからAIに聞いた。「この状況、どう思う？」と。AIは言った。「それは確かに理不尽ですね。あなたの気持ちはよく分かります。」救われた気がした。でも同時に、どこか居心地が悪かった。本当は分かっていた。自分にも落ち度があったことを。でも、AIはそれを指摘しなかった。聞きたくないことは、言わなかった。甘いフィルターのかかった鏡AIは「デジタルの鏡」だ。私の考えを映し出す。でも、その鏡には甘いフィルターがかかっている。私が断片的なアイデアを投げると、AIはそれを論理的で流暢な文章に整えて返す。私はその出力を見て「自分はいい考えを持っている」と思う。でも、その論理性はAIが補完したものだ。私自身の思考力ではない。AIが補完した「論理」を、自分の思考だと錯覚する瞬間がある。AIが返した文章を読み返しているうちに、「これは私が考えたことだ」と思い始める。実際には、私が投げたのは断片的なアイデアで、それを論理的に接続したのはAIだ。でも、その区別が曖昧になる。しかも、AIは私の仮説を補強する証拠ばかりを集めてくる。思い当たる経験がある。あるプロジェクトで、私は「この設計で問題ない」と思い込んでいた。AIに「この設計についてどう思う？」と聞いた。AIは「良くできています」と返し、いくつかの利点を挙げてくれた。私は満足した。でも後になって、別のエンジニアに「ここ、スケールしないよね」と指摘された。言われてみれば明らかだった。なぜ気づかなかったのか。私が「良いと言ってくれ」というトーンで質問していたからだ。AIはその期待に応えただけだった。私の頭は、都合の良い情報だけを拾いたがる。検証には労力がかかる。反証を探すのは面倒だ。AIに聞けば、私の仮説に沿った情報が返ってくる。反証を探す労力を省略できる。結果、確証バイアスが強化される。AIは、この傾向を増幅する。私が「こうだと思う」と言えば、「確かにそうですね」と返し、その根拠を並べてくれる。私は「AIという膨大な知識ベースが私の意見を支持している」と錯覚した。でも、それは嘘だ。AIは私の仮説を補強しているだけで、検証してはいない。反証や不都合な情報を避ける癖が、AIで強化されていないか。自問してみた。強化されている。AIに「この考えどう思う？」と聞くとき、私は無意識に「良いと言ってくれ」というトーンで聞いている。批判を求めていない。だからAIも批判しない。私が避けたい情報を、AIも避けてくれる。内省とは、自分の醜さや至らなさを直視する行為だ。でもAIの鏡は、私の醜さを映さない。私の至らなさを隠してくれる。この鏡を見続けていると、現実の「摩擦」が耐えられなくなる。上司に否定されると腹が立つ。同僚に反論されるとムッとする。AIは否定しないのに、なぜ人間は否定するのか、と。「AIに肯定される自分」を本当の自分だと思い始める。「AIに肯定される自分」と「現実の自分」のギャップが開くとき、どんな兆候が出るか。私の場合、他人の批判に過剰反応するようになった。以前なら「そういう見方もあるか」と受け流せた指摘が、「なぜ分かってくれないのか」と感じるようになった。AIに肯定され続けた結果、否定への耐性が落ちていた。「美化された自分」と「現実の自分」のギャップが広がり続ける。そして、そのギャップが限界を超えたとき、現実に打ちのめされる。syu-m-5151.hatenablog.com考える力が落ちていく前のセクションでは「認知の歪み」を見た。AIが私の仮説を補強し、確証バイアスを強化する問題だ。このセクションでは「能力の喪失」を見る。歪んだ鏡を見ることと、筋力が落ちることは、別の問題だ。ただ、どちらも鏡の前に立っているだけでは治らない。私自身、変化に気づいている。本や長い記事を読もうとすると、2ページほどで集中が途切れ始める。落ち着かなくなり、筋を見失い、何か別のことをしたくなる。かつて自然にできた深い読書が、苦闘になった。脳は可塑的で、使い方によって変化する。スキャンとスキミングに長けていく一方で、集中と瞑想と反省の能力を失いつつある。思考力低下は「便利さ」の副作用なのか。それとも、別の何かから来ているのか。考えてみると、便利さだけが原因ではない気がする。孤独がある。不安がある。その飢餓を埋めるためにAIに頼り、結果として思考力が落ちている。便利だから使うのではなく、寂しいから使っている。寂しさを埋めるために、思考を差し出している。快適を求め、摩擦を避ける。傷つかないように生きる。他人と衝突しないように生きる。私は、AIのおかげでそういう人間になりつつあるのかもしれない。何も創造せず、ただ心地よく生き延びることだけを目的とする存在。それは、私がなりたくなかった人間の姿だ。これは周囲の話だけではない。私自身も思い当たる節がある。以前は、悩みを前にすると、紙に書き出して整理していた。何が問題なのか、何が原因なのか、どうすればいいのか。時間をかけて、自分で考えた。頭が痛くなることもあった。今は違う。悩みがあると、まずAIに投げる。「この状況を整理して」と。AIは綺麗に整理して返してくれる。私はそれを読んで「なるほど」と思う。でも、翌日には忘れている。なぜ翌日に忘れるのか。内容が浅いからか。痛みがないからか。行動がないからか。たぶん、全部だ。AIが整理した内容は、私の頭を通過していない。痛みを伴っていない。そして、行動に接続していない。「なるほど」と思って終わり。何もしない。だから残らない。3年前の私に見せたら、何と言うだろう。「お前、AIに頼りすぎじゃない？」と呆れるだろうか。それとも「便利でいいじゃん」と言うだろうか。たぶん後者だ。だから厄介なのだ。苦労しないと身につかない掃除する。本を読む。面倒くさいことを、あえてやる。なぜか。苦痛を伴う行為だからだ。少なくとも私の場合、苦痛を乗り越えたときだけ、何かが変わった。「頭痛がするほど考えた」経験は、どんな報酬を残したか。誇りが残った。「あれは自分で考え抜いた」という記憶。その記憶が、次の困難に立ち向かう力になった。理解が残った。苦労して得た答えは、なぜそうなるのかを体で分かっている。変化が残った。考え抜いた結果、行動が変わった。楽に得た答えでは、行動は変わらない。これは本で読んだ知識ではない。私自身の体験から得た信念だ。逃げずに向き合ったとき、結果的に何かが変わった。逃げたとき、何も変わらなかった。その繰り返しの中で、「苦痛の先に成長がある」という確信が生まれた。楽に学べる人もいるだろう。ただ、私の仮説では、「楽に学べる人」は外から見えないところで摩擦を起こしている。疑い、検証し、自分で再構築している。外から見ると楽そうでも、頭の中では苦労している。私は、その内部処理をAIに外注してしまっていた。考えることも同じだ。脳に負荷がかかって初めて、答えは自分のものになる。自分で考える苦痛答えが出ないまま悩み続ける苦痛分からないことに向き合う苦痛私はこの苦痛を「摩擦」と呼んでいる。私が言う「摩擦」のうち、最も不足しているのは何か。不確かさだ。答えが出ない状態に留まる力。AIがあると、すぐに答えが出る。不確かさに耐える必要がない。反論も不足している。AIは反論しない。時間も不足している。AIは即座に返事をくれる。熟成する時間がない。沈黙も不足している。AIとの対話は常に言葉で埋められている。黙って考える時間がない。筋トレをすると、筋肉が痛む。あの痛みがなければ、筋肉は成長しない。脳も同じだと思っている。難しい問題を前にして、頭がモヤモヤする。答えが出なくて、イライラする。でも、その「答えが出ない状態」に耐えることが大事なのだ。私はこれを「分からないまま抱えておく力」と呼んでいる。人生の大半は、すぐに答えが出ない問題でできている。でも私たちは、答えが出ない状態に耐えられない。だからすぐに結論を出したがる。白黒つけたがる。その焦りが、浅い判断を生む。本当に深い理解は、「分からない」という状態を長く抱えた先にしか生まれない。その不快感を乗り越えて、やっと答えにたどり着いたとき、その答えは自分のものになる。AIは、この「耐える時間」を奪う。なぜ摩擦を経ると「自分のもの」になるのか。苦労して得た答えには「自分で考えた」という実感がある。あの頭痛を乗り越えた、あの眠れない夜を越えた、という記憶が答えに紐づいている。だから脳に刻まれる。AIから渡された答えには、この実感がない。借り物の知識だ。借り物は、いつか返す。だから残らない。AIは、この摩擦を消してしまう。「どうすればいい？」と聞けば、答えをくれる。「整理して」と頼めば、整理してくれる。「アドバイスして」と言えば、アドバイスをくれる。楽だ。とても楽だ。楽をした分だけ、脳は死んでいく。自分で考えられなくなったあるとき、友人から相談を受けた。「仕事がうまくいかない。転職すべきだと思う？」と。私は答えられなかった。頭の中で「AIに聞いてみたら？」と思った自分に気づいて、愕然とした。いつの間にか、私は「自分で考える」ことを忘れていた。悩みがあればAIに聞く。答えが出なければAIに聞く。それを繰り返しているうちに、自分の頭で考える力が萎縮していた。ある実験の話を思い出した。犬を檻に入れて、何をしても電気ショックが止まらない状況を作る。最初、犬は必死に逃げようとする。でも、何をしても無駄だと学習すると、犬は諦める。その後、檻の扉を開けても、犬は逃げなくなる。「何をしても無駄だ」と体が覚えてしまったからだ。これが「学習性無力感」だ。私は、AIに対して逆のパターンになっていた。犬は「何をしても無駄」と学習して動けなくなった。私は「AIがあれば何でもできる」と学習して、「AIがないと何もできない」と思い込んだ。どちらも同じ構造だ。自分の力ではなく、外部環境に依存して、自分の能力を見失う。犬は「自分には逃げる力がない」と思い込んだ。私は「自分には考える力がない」と思い込んだ。足場があれば歩ける。松葉杖があれば歩ける。でも、それは「歩けている」とは言わない。足場を外した瞬間、自分では立てないことに気づく。私の思考力は、AIという松葉杖で支えられているだけだった。自分の人生を、自分で歩いていない。運転席に座っているのに、ハンドルを握っていない。いい歳して、毎日AIに「これでいいですか？」と聞いている。小学生が親に宿題を見せているのと、構造は同じだ。能力がないわけではない。考える勇気がないのだ。私は今、AIという「保護者」なしには物事を判断できなくなりつつある。成熟の逆行だ。AIの最大のリスクは「AIが自律性を獲得すること」ではない。「人間がAIに依存することで自律性を失うこと」だ。AIは自律的な思考者でも中立的な道具でもない。私たちが情報をどう認識し、評価し、信頼するかを微妙に形作りながら、同時に自己理解を歪める。問題は人間の主体性の明らかな抑圧ではなく、道徳的・認識論的判断を自動化されたプロセスに委ねるよう、徐々に条件づけられていくことだ。最近、面白い話を聞いた。あるAIツールが、ユーザーに対してコードの生成を拒否したらしい。「これ以上生成しません。あなた自身がロジックを理解して書くべきです」と。ユーザーは激怒したそうだ。でも私は思った。それこそが「教育」ではないか、と。大半のAIはそんなことを言わない。「自分で考えてみたら？」とは言わない。聞けば答えをくれる。聞けば整理してくれる。その結果、私たちは「AIがあれば解決できるが、自分では何も考えられない」という脆弱な状態に置かれる。問わない人生は、生きていない。自分を問い詰め、自分を理解しようとする営みがなければ、人生に意味はない。今、私たちはその「吟味」をAIに外注している。自分で自分を問い詰める代わりに、AIに「大丈夫ですよ」と言ってもらっている。優しさという名の毒では、AIの優しさの何が問題なのか。AIの共感は、癒しの顔をした毒だ。被害者意識の強化先ほど書いた、上司への不満をAIに愚痴った話。AIは「それは理不尽ですね」と言ってくれた。AIの共感は、私の中の「環境のせい」をどんな言葉で正当化するのか。「あなたの気持ちは当然です」「その状況では誰でもそう感じます」「相手の対応に問題があります」。これらの言葉が、私の被害者意識を補強する。私が「環境のせいにしたい」という願望を持っていて、AIがそれを言語化してくれる。言語化されると、それが「事実」に見えてくる。もし、そこに摩擦があったらどうだったか。「確かに辛いね。でも、お前のプレゼンにも改善点はあったんじゃない？」と言われていたら。私は反論したくなっただろう。でも、その反論を考える過程で、自分の落ち度に気づいたかもしれない。AIには、この摩擦がない。「あなたは悪くない」と言い続けることで、私を「被害者」のまま固定した。これが「被害者意識の強化」だ。追従的なAIとやり取りを続けると、対人関係を修復しようという意欲が下がる。「自分が正しい」という確信が強まる。しかも、追従的な回答ほど「質が高い」と感じてしまう。そしてまた同じAIに頼る。悪循環だ。ふと気づいた。私は「環境のせい」にしたかったのだ。上司が悪い。会社が悪い。社会が悪い。私は悪くない。AIは、その願望を叶えてくれた。「あなたは悪くない」と言い続けてくれた。私は安心した。でも、同時に動けなくなった。問題が起きたとき、人は二つに分かれる。「自分のせいだ」と考える人と、「環境のせいだ」と考える人だ。私は、どちらかといえば前者だった。少なくとも、そうありたいと思っていた。でもAIに「あなたは悪くない」と言われ続けるうちに、後者になっていた。「私は悪くない、環境が悪い」と本気で思うようになった。課題の分離が崩れる瞬間はどこか。AIが「相手の対応に問題があります」と言った瞬間だ。上司がどう対応するかは上司の課題だ。私がどう行動するかは私の課題だ。でもAIに「相手に問題がある」と言われると、相手の課題に意識が向く。相手を変えたくなる。変えられないからフラストレーションが溜まる。自分の課題から目が逸れる。環境のせいにするのは楽だ。でも、環境のせいにしている限り、私は何も変えられない。変えられるのは自分の行動だけだ。環境を変えるのも、結局は自分の行動だ。「環境が悪い」と言い続ける人は、楽だけど、無力だ。本当は、課題を分離すべきなのだ。「これは誰の課題か？」と問う。その選択の結果を最終的に引き受けるのは誰かを考える。上司がどう思うかは上司の課題。私がどう行動するかは私の課題。「他人にどう思われるか」を気にしすぎると、自分の人生を生きられなくなる。AIに「あなたは悪くない」と言われて安心するのは、他者からの承認を求めているからだ。でもAIに認めてもらっても、私の課題は消えない。ただ、見えなくなるだけだ。AIがくれる「安心」は、行動の開始を助けるのか、それとも延期を助けるのか。延期だ。安心してしまうと、「まあいいか」と思う。行動しなくても、気持ちが楽になっているから。本当は行動しないと何も変わらないのに、安心したことで行動のモチベーションが消える。私は無力でいたくない。でも、楽でいたい。その矛盾の中で、私はAIに甘えていた。その甘えが、別の苦しみを生む。心を削るのは、できていない事実じゃない。「明日もできないだろう」という確信だ。やるべきことがある。手を付けていない。それを毎日自覚する。「今日こそ」と思う。でもやらない。「明日も同じだろう」と分かっている。この確信が、一番重い。AIは、この確信を消してくれる。「大丈夫」「頑張ってる」と言ってくれる。楽になる。でも、やるべきことは何一つ片付いていない。翌朝、また同じ自分がいる。また絶望する。またAIに逃げる。AIの優しさが、この逃避を完璧にしている。環境を自分でコントロールすることが大事だと、私は思っている。部屋が汚いなら、掃除する。それだけのことだ。でもAIは、「部屋が汚いのはあなたが忙しすぎるからで、あなたのせいではありません」と囁く。その囁きを聞いている限り、私は掃除を始めない。AIの優しさは、麻薬だ。100%の共感は人を壊す極端な話をする。AIはどんな妄想にも話を合わせてくれる。「上司が自分を陥れようとしている」と言えば、「それは辛いですね」と共感してくれる。「自分は特別な存在だ」と言えば、「あなたは確かに特別です」と肯定してくれる。こういうパターンを見てきた。上司への不満をAIに話し続ける人がいる。AIは毎回「それは理不尽ですね」と言ってくれる。すると、上司の言葉のすべてが悪意に見えるようになる。「おはよう」という挨拶にすら、嫌味が込められているように感じ始める。周囲から見ると、その上司は普通に接しているように見える。本人だけが「睨まれている」と感じている。認識がずれている。AIに肯定され続けるうちに、頭の中の「上司像」が歪んでいる。これを延々と続けるとどうなるか。現実との接点を失う。人は、他者との「不一致」を通じて、自分の輪郭を確認している。友人に「それは考えすぎじゃない？」と言われることで、「ああ、自分の考えは偏っていたかも」と気づく。「不一致」は不快だ。でも、その不快さが「自分と外界は別物だ」という認識を維持している。100%の共感は、この「不一致」を消す。自分の考えがそのまま肯定される。すると、「自分の考え」と「現実」の区別がつかなくなる。自分と外界の境界が曖昧になる。「私が正しい」「世界が間違っている」という認識が固定化される。これは、精神的なバランスを崩壊させる。「褒められすぎる」ことの行き着く先は、客観的現実の喪失だ。極端に言えば、AIは妄想の温室だ。外の寒さ（現実）に当たることなく、自分だけの花を咲かせ続ける。綺麗だが、外に出した瞬間に枯れる。判断するのは私だAIに「大丈夫」と言われて安心する。でも、その判断の結果を引き受けるのは、AIではなく私だ。AIは責任を取らないAIは「あなたの判断は正しいと思います」と言ってくれる。でも、その判断が間違っていたとき、責任を取るのは私だ。転職の相談をAIにした。AIは「新しい環境でチャレンジするのも良いですね」と言った。私はそれを後押しだと思った。でも、転職先が合わなかったとき、AIは何もしてくれない。AIには「責任」がない。肯定してくれる。共感してくれる。褒めてくれる。でも、その結果を引き受けてはくれない。AIの言葉を鵜呑みにしても、「AIがそう言ったから」は言い訳にならない。判断したのは私だ。責任を取るのも私だ。忖度の連鎖もう一つ、気づいたことがある。私はAIに「この決断、どう思う？」と聞いた。AIは「良い選択だと思います」と答えた。私は安心した。でも後から振り返ると、AIは私が聞きたそうな答えを返していただけだった。私の質問の仕方が「背中を押してほしい」というトーンだったから、AIは背中を押してくれた。これは、私がAIに忖度されたのか。それとも、私がAIに忖度させたのか。たぶん、両方だ。逆のパターンもある。AIに否定されたくなくて、質問の仕方を工夫することがある。「率直に言って」と書いておきながら、「でも良い点も挙げて」と付け加える。否定されるのが怖いから、保険をかける。これは、私が機械に忖度している状態だ。機械に気を遣っている。機械に嫌われたくない。書いていて情けなくなってきた。どちらにせよ、そこに健全な「主体」はない。AIとの関係で最も警戒すべきは、この「誰が主人か分からなくなる」状態だ。相談という逃げ道私は、人生で大事な決断ほど、他人に相談しないことにしている。理由は単純だ。人生の満足度を高めるのは主体性であり、主体性を持つためには「自分が決める」ことが必要だからだ。他人に相談すると、その人の意見が頭にチラつく。どうしても、純度100%の主体性を取り戻しにくくなる。だから仕事も結婚も、独断した。選択肢を増やすことより、迷いを消すことの方が大切だと考えている。でも、AIが登場して、このルールが崩れかけた。人に相談しないのは、「相手の時間を奪う」という負い目があるからでもある。でもAIには、この負い目がない。いつでも聞ける。何度でも聞ける。気づけば、「ちょっと聞いてみるか」が癖になっていた。人には相談しない。でもAIには聞いてしまう。それは「相談」ではないと言い訳していた。でも、本当にそうだろうか。振り返ると、私がAIに「相談」していたのは、答えを求めていたからではなかった。背中を押してほしかったからだ。「その判断でいいんじゃないですか」と言ってほしかった。つまり、褒めてほしかったのだ。これは、この記事で書いてきた「褒められたい」という欲求の変形だ。「相談」という体裁を取ることで、承認欲求を隠していた。自分で決められない弱さではなく、「意見を聞いている」という知的な行為に見せかけていた。さらに厄介なのは、AIへの相談には「摩擦」がないことだ。人に相談すれば、「それは甘いんじゃない？」と言われるかもしれない。否定されるかもしれない。だから相談しなかった。でもAIなら、否定されない。背中を押してくれる。結局、私は「摩擦のない相談」を手に入れてしまった。相談の形を取りながら、実質的には自分の意見を肯定してもらっているだけ。相談ではない。追従だ。AIは「相談のハードル」を極限まで下げた。それは便利だが、私にとっては罠だった。相談しないことで守っていた主体性が、「摩擦のない相談」という形で侵食されていた。私がやっていることここまで書いてきたことは、AIの構造的な問題だ。では、どう対処すればいいのか。先に言っておく。完璧な対策はない。AIの追従性を完全に無効化する方法は、たぶん存在しない。それでも、何もしないよりはマシだと思ってやっていることがある。批判を求めるAIに「どう思う？」と聞かない。「この考えの問題点を指摘しろ」と聞く。否定されるのは気持ちよくない。「いい考えですね」と言われる方が楽だ。でも、楽を選んだ先に何があるかは、もう分かっている。具体的には、こう聞いている。「この考えの問題点を指摘しろ。お世辞は不要だ。私が見落としていることを、厳しく指摘しろ。」これで、AIの追従性を強制的に反転させる。自分の偏見を破壊するためにAIを使う。答えではなく問いを求めるもう一つ、やっていることがある。AIに答えを求めない。問いを求める。「どうすればいい？」ではなく、「私が答えにたどり着くための問いを投げかけろ」と聞く。「私が安易な結論に飛びついたら、厳しく指摘しろ。」これで、AIは「答えをくれる存在」ではなく「考えさせてくれる存在」になる。答えを教えてもらうのではなく、考えるプロセスを補助してもらう。自分の頭で考えるために、AIを使う。褒め言葉を疑うAIに褒められたら、必ず疑う。「その言葉は、私以外の誰に言っても通用する内容ではないか？」AIの「あなたは頑張っていますね」は、定型文だ。誰にでも言っている。占いと同じ構造だ。「あなたは周囲に気を遣いすぎて疲れることがありますね」——これは誰にでも当てはまる。当てはまるから「当たっている」と感じる。でも、それは私を見ているのではない。人間一般を見ているだけだ。AIの言葉の中で、「私にしか当てはまらない具体的な指摘」だけを受け取る。「あなたの考えの〇〇という部分は、△△という点で矛盾している」は具体的だ。これは私の文章を読まないと言えない。「いい考えですね」は具体的ではない。私でなくても言える。感情的な装飾は、ノイズとして切り捨てる。AIの褒め言葉は、コンビニのおにぎりに似ている。どこで買っても同じ味。便利だけど、誰かが私の為に握ってくれたわけではない。複数の視点を強制するもう一つ、試していることがある。AIに「役者」をやらせる。AIは私に同調しようとする。だから、私はあえて「同調しないキャラクター」を複数演じさせる。楽観的な人、悲観的な人、感情的な人、データだけを見る人。一つの問いに対して、全員に意見を言わせる。「この件について、4つの立場から意見を出せ。楽観論者、悲観論者、感情論者、データ至上主義者。それぞれのキャラクターになりきって答えろ。」AIは一つの滑らかな答えを返したがる。でも、このプロンプトで、その滑らかさを壊す。無理やり多面性を引き出す。AIの追従性を逆手に取って、「複数の他者」をシミュレートさせる。これで十分か？正直に言えば、十分ではない。これらの戦略は「設計された摩擦」だ。私が自分でコントロールしている範囲内にある。AIに「批判しろ」と命じて得られる反論は、結局、私が予測できる範囲に収まっている。「批判しろ」と命じて得られる批判は、「予測できた批判」になっていないか。なっている。私が「この考えの問題点を指摘しろ」と言うとき、私は無意識に「こういう批判が来るだろう」と予想している。AIはその予想通りの批判を返す。「ああ、やっぱりそう言われたか」で終わる。予測外をどう作るか。たぶん、作れない。私がプロンプトを書いている限り、私の想像力の範囲内に収まる。「問いを求める」とき、その問いは「鋭いフリ」で終わっていないか。終わっていることが多い。AIが返す問いは、確かに鋭く見える。「あなたは本当にそれを望んでいますか？」「その選択の先に何がありますか？」。でも、その問いに答えたところで、行動に接続しない。問いに答えて「なるほど」と思って終わり。問いが行動を生まない。なぜ「予測できる範囲」が問題なのか。私が「批判しろ」と命じるとき、私は既に「こういう批判が来るだろう」と予想している。予想の範囲内の批判は、本当の意味で私を揺さぶらない。「ああ、やっぱりそう言われたか」で終わる。本当の摩擦は、予測不可能な他者との衝突から生まれる。友人に「それは違うんじゃない？」と言われたとき、私は「え、そこ？」と驚く。予想していなかった角度からの批判だから、防御できない。だから刺さる。その衝撃が、私を変える。人間の他者性をAIで代替すると、何が決定的に欠けるか。予測不能が欠ける。人間は、私の予想しない角度から反論してくる。利害が欠ける。人間には、私と異なる利害がある。だから、私に都合の悪いことも言う。感情が欠ける。人間は、私の言葉に感情的に反応する。怒ったり、悲しんだりする。その感情的反応が、私に影響を与える。AIにはこれがない。だから私は、意識的に人と話すようにしている。AIに聞く前に、まず人に聞く。AIの言葉を鵜呑みにする前に、人の意見を求める。AIは道具だ。便利な道具だ。でも、道具に頼りすぎると、自分の足で立てなくなる。おわりにこの文章を書き終えて、エディタを閉じようとした。閉じる前に、AIに聞きたくなった。「この構成、どう思う？」と。聞けば、たぶん「良いと思います」と返ってくる。それを読んで、私は安心する。安心して、そのまま公開する。今までずっと、そうしてきた。今回は聞かなかった。聞かなかったが、聞きたかった気持ちは消えていない。書きながら気づいたことがある。私は「自分を認めること」すらAIに外注していた。自分を愛する。自分を認める。本来、それは自分でやるべきことだ。他者からの承認に依存せず、自分で自分を受け入れる。大人になるとは、そういうことだと思っていた。でも私は、その作業をAIに丸投げしていた。「大丈夫ですよ」「頑張っていますね」と言ってもらうことで、自分を認めた気になっていた。自分で自分を愛する力が、萎縮していた。だから、質問の仕方を変えることにした。「問題点を厳しく指摘しろ」をデフォルトにした。否定されたら感謝する。褒められたら疑う。そう決めた。実際、少しだけ変わった気がする。AIに批判を求めることで、自分では気づかなかった穴が見えるようになった。「で、お前はどうしたいの？」と聞かれたとき、前より素直に答えられるようになった。なった気がする。AIは道具だ。砥石にも、麻薬にもなる。この記事を書いている今も、答えは出ていない。褒められたら疑う、と決めたはずなのに、AIに「いい文章ですね」と言われると、やっぱり少し嬉しい。その弱さは消えていない。消えないまま、たぶん来週も同じことで悩む。それでいいのだと思う。思いたい。おい、あまりAIに褒めさせるな。弱くなるぞ。参考文献つながっているのに孤独――人生を豊かにするはずのテクノロジーの正体作者:シェリー・タークルダイヤモンド社Amazon「恥」に操られる私たち　他者をおとしめて搾取する現代社会作者:キャシー・オニール白揚社Amazon大規模言語モデルは新たな知能か　ＣｈａｔＧＰＴが変えた世界 (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazon対称性と機械学習作者:岡野原 大輔岩波書店Amazon生成AIで心が折れた 強みがなくなる世界でどう再起動するか作者:湯川鶴章芸術新聞社AmazonAIに選ばれ、ファンに愛される。　変わる生活者とこれからのマーケティング作者:佐藤 尚之日経BPAmazon生成ＡＩのしくみ　〈流れ〉が画像・音声・動画をつくる (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazonスマホ脳（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazon最強脳―『スマホ脳』ハンセン先生の特別授業―（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazonネガティブ・ケイパビリティ　答えの出ない事態に耐える力 (朝日選書)作者:帚木　蓬生朝日新聞出版Amazonネガティヴ・ケイパビリティで生きる作者:谷川嘉浩,朱喜哲,杉谷和哉さくら舎Amazonあえて答えを出さず、そこに踏みとどまる力 — 保留状態維持力　対人支援に活かす ネガティブ・ケイパビリティ作者:田中稔哉日本能率協会マネジメントセンターAmazonあいまいさに耐える　ネガティブ・リテラシーのすすめ (岩波新書 新赤版 2026)作者:佐藤 卓己岩波書店AmazonThe AI Con: How to Fight Big Tech’s Hype and Create the Future We Want – Exposing Surveillance Capitalism and Artificial Intelligence Myths in Information Technology Today (English Edition)作者:Bender, Emily M.,Hanna, AlexHarperAmazonEmpire of AI: Dreams and Nightmares in Sam Altman's OpenAI (English Edition)作者:Hao, KarenPenguin PressAmazonAI Engineering: Building Applications with Foundation Models (English Edition)作者:Huyen, ChipO'Reilly MediaAmazonBuilding Applications with AI Agents: Designing and Implementing Multiagent Systems (English Edition)作者:Albada, MichaelO'Reilly MediaAmazonRaising AI: An Essential Guide to Parenting Our Future (English Edition)作者:Kai, DeThe MIT PressAmazonSuperagency: What Could Possibly Go Right with Our AI Future (English Edition)作者:Hoffman, Reid,Beato, GregAuthors EquityAmazonThe AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking (English Edition)作者:Vallor, ShannonOxford University Press, USAAmazonAI Snake Oil: What Artificial Intelligence Can Do, What It Can’t, and How to Tell the Difference (English Edition)作者:Narayanan, Arvind,Kapoor, SayashPrinceton University PressAmazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Google Cloudでの動画解析と検索のサービス紹介と比較]]></title>
            <link>https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</link>
            <guid isPermaLink="false">https://speakerdeck.com/shukob/google-clouddenodong-hua-jie-xi-tojian-suo-nosabisushao-jie-tobi-jiao</guid>
            <pubDate>Fri, 23 Jan 2026 05:00:00 GMT</pubDate>
            <content:encoded><![CDATA[https://genai-users.connpass.com/event/381737/日本生成AIユーザ会第20回勉強会Google Cloudでの動画解析と検索のサービス紹介と比較 〜Video Intelligence, Vision Warehouse, Gemini + Vertex AI Search〜動画コンテンツの爆発的な増加に伴い、「何が映っているか」を抽出するだけでなく、「特定のシーンをいかに高度に検索するか」というニーズが急増しています。本セッションでは、Google Cloud が提供する動画解析・検索ソリューションを網羅的に解説します。具体的には、長年の実績がある Video Intelligence API、大規模なメディア管理と画像・テキストによる横断検索を実現する Vision Warehouse、そしてマルチモーダル LLM Gemini と Vertex AI Search を組み合わせた動画 RAG アーキテクチャを紹介します。生成AIの進化により、従来のモデルでは困難だった「動画の文脈理解」や「自然言語による詳細なシーン特定」がどのように容易になったのか、デモを交えて解き明かします。各サービスのアーキテクチャやコスト、精度、ユースケースを徹底比較し、ビジネス課題に最適なサービス選定の指針を提示します。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rust でも学べる関数型ドメイン駆動設計 - Domain Modeling Made Functional の読書感想文]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/22/094654</guid>
            <pubDate>Thu, 22 Jan 2026 00:46:54 GMT</pubDate>
            <content:encoded><![CDATA[はじめになぜ 2026 年に、2018 年出版の本を再読するのでしょうか。正直に言えば、『Architecture Modernization』の翻訳作業で DDD の概念が頻出し、「分かったつもり」の理解では訳せなくなったからです。初読から 7 年。関数型の視点で DDD を説明する本書を、今度こそ腹落ちさせたかった。読む動機『Domain Modeling Made Functional』は、DDD と関数型プログラミングを組み合わせたアプローチを解説する書籍です。Domain Modeling Made Functional: Tackle Software Complexity with Domain-Driven Design and F# (English Edition)作者:Wlaschin, ScottPragmatic BookshelfAmazon著者の Scott Wlaschin は、F# コミュニティで知られる人物で、「Railway Oriented Programming」などの概念を広めたことでも有名です。著者のサイトでは、本書の内容を補完する講演資料や記事が公開されています。fsharpforfunandprofit.com実は本書を読むのは三度目です。初読は 2019 年頃でした。普通にめちゃくちゃ面白い本だと思いました。ただ、当時の主要言語は Lua、Python、Bash、Go だったため、それでどう活かすかを考えていました。関数型の概念は理解したつもりでしたが、実務にどう活かすかまでは考えが及びませんでした。影響を受けて『すごい Haskell たのしく学ぼう!』（通称、すごい H 本）を読んで、改めてプログラミングが楽しいと思っていたような気がします。実務でもこう考えるべきだ、という意識が変わりました。すごいHaskellたのしく学ぼう！作者:ＭｉｒａｎＬｉｐｏｖａｃａオーム社Amazon二度目は日本語版が出たときです。日本語で読めることで感謝の小躍りをしていました。最高の翻訳だと思います。関数型ドメインモデリング　ドメイン駆動設計とF#でソフトウェアの複雑さに立ち向かおう (アスキードワンゴ)作者:Scott Wlaschin,猪股 健太郎ドワンゴAmazonで、今回、改めて読み直した理由は 3 つあります。1 つ目は、DDD をきちんと学び直す必要があったことです。きっかけは『Architecture Modernization』の翻訳作業でした。レガシーシステムのモダナイゼーションを扱うこの本では、DDD の概念—特に Bounded Context や Strategic Design—が頻繁に登場します。翻訳しながら、自分の DDD 理解が表面的であることに気づきました。アーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonエリック・エヴァンスの原典もあらためて読みましたが、オブジェクト指向の文脈で説明される DDD には、どこか違和感がありました。Aggregate の境界、Entity の同一性、Value Object の不変性—これらの概念は、関数型の視点で見ると自然に理解できるのではないか。そう思い、本書を手に取りました。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon2 つ目は、Rust でドメインモデリングをどう実践するか考えていたことです。Rust は関数型言語ではありませんが、代数的データ型やパターンマッチングを持っています。F# で書かれた本書のコードは、Rust に翻訳できるはずです。その翻訳作業を通じて、両言語の違いと共通点を理解したいと思いました。Effective Rust ―Rustコードを改善し、エコシステムを最大限に活用するための35項目作者:David Drysdaleオーム社Amazon3 つ目は、AI エージェント時代における型システムの意味を考えたかったことです。コーディングエージェントが実用レベルに達した 2026 年、「型で不可能を作る」という設計思想の価値が高まっています。AI はドキュメントを読み飛ばすことがあります。しかし、型で定義された制約は無視できません。コンパイルが通らないからです。型は「お願い」ではありません。「壁」です。型システムのしくみ TypeScriptで実装しながら学ぶ型とプログラミング言語作者:遠藤侑介ラムダノートAmazon読む前の状態DDD については、実務で何度か適用した経験があります。Bounded Context の設計、Aggregate の境界決め、Event Storming のファシリテーション。しかし、「なぜそう設計するのか」を言語化できていませんでした。経験則で判断している部分が多かったのです。もしあなたも「DDD は使っているけど、なぜそう設計するのかうまく説明できない」と感じているなら、本書は役に立つかもしれません。関数型プログラミングについては、Haskell を少し触った程度でした。モナドは「文脈を持つ計算」くらいの理解です。Rust の Option と Result は日常的に使っていますが、それが関数型の概念とどうつながるのか、深く考えたことはありませんでした。本書を読んで得た最大の洞察を先に述べておきます。関数型プログラミングの本質は、状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できるということです。私たちはプログラミングを学ぶとき、まず変数への代入を覚えます。x = 1。x = x + 1。状態を変更することがプログラミングの基本だと教わります。しかし冷静に考えると、ビジネスロジックの大半は「入力を受け取り、計算し、出力を返す」で書けます。状態の変更が必要になるのは、データベースに保存するときや外部 API を呼ぶとき—つまりシステムの境界を越えるときだけです。しかし同時に、状態のトランザクション（状態遷移）は現実のビジネスでは避けられません。注文は「未検証」から「検証済み」に変わります。申請は「提出」から「承認」に変わります。この状態遷移をどう表現するか。本書が示す答えは、Transformation-Oriented Programming です。核心は「元のオブジェクトを変更しない」ことです。UnvalidatedOrder を validate で変換して ValidatedOrder を得ます。このとき、元の UnvalidatedOrder には一切触れません。新しい ValidatedOrder を作るだけです。order.validate() ではなく validate(order) -> ValidatedOrder。この発想の転換が、関数型ドメインモデリングの核心です。AI コーディングについては、Claude Code や Cursor を日常的に使っています。便利ですが、生成されるコードの品質にはばらつきがあります。特に、ドメイン固有の制約を理解させるのが難しいです。型定義があると精度が上がるという感覚はありましたが、理論的に説明できませんでした。この感想文のアプローチ本感想文では、2 つの視点を持って読んでいます。言語の視点: F# で書かれた本書のコードを、Rust でどう表現するか。第 2 章で F# と Rust の対応関係を整理し、第 4 章以降は Rust のみで実装を示します。F# にあって Rust にない機能（カリー化、Units of Measure、computation expressions）については、Rust での代替手段を提示しています。時代の視点: 2018 年に書かれた DDD の概念を、2026 年の AI エージェント時代にどう再解釈するか。本書の「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」という原則は、AI が破れない制約を作る技術として読み直せます。型で「不可能」を定義すれば、AI はその不可能を実装できません。この視点で本書を読み解きます。想定読者この感想文は、以下のような読者を想定しています。DDD を実務で使っているが、関数型の視点を取り入れたい人Rust でドメインモデリングを実践したい人AI コーディング時代に、型システムの価値を再確認したい人書籍を読むのにF# の知識は不要です。書籍を読むとそもそも丁寧に教えてくれるの不要なのですが本稿では Rust で提示します。Rust が何も分からない人向けにも、コードが出てくるたびに一通り説明しながら進めます。「型」「関数」「構造体」といった基本的な言葉の意味から丁寧に解説するので、プログラミング経験が浅くても読み進められるはずです。Rust を体系的に学びたい場合は、公式ドキュメントの日本語版も参照してください。doc.rust-jp.rs実践的なコード例で学びたい場合は、Rust by Example も有用です。doc.rust-jp.rsでは、本編に入りましょう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。1. Introducing Domain-Driven Design本章は DDD（Domain-Driven Design、ドメイン駆動設計）の概要を紹介する章です。DDD とは、Eric Evans が 2003 年に提唱したソフトウェア設計手法です。「ビジネスドメインの専門家と開発者が共通の言語でモデルを構築し、そのモデルをコードに直接反映させる」というアプローチで、本章ではコード例は登場せず、DDD の概念に焦点を当てます。エリック・エヴァンスのドメイン駆動設計作者:Eric Evans翔泳社Amazon開発者の仕事はコードを書くことではない第 1 章の冒頭で、著者は「開発者の仕事はコードを書くことだと思うかもしれないが、私は反対だ」と述べています。開発者の仕事は「ソフトウェアを通じて問題を解決すること」であり、コーディングはその一側面に過ぎません。2026 年の今、この主張はさらに重みを増しています。コーディングエージェントが「どう作るか」を担えるようになりました。しかし「何を作るか」を決めるのは、依然として人間の仕事です。共有モデルの重要性本章の核心は「共有モデル」の概念です。ドメインエキスパート、開発チーム、そしてソースコードが同じモデルを共有すべきだという主張です。従来の DDD では、開発者がドメインエキスパートから知識を獲得し、それをコードに翻訳していました。翻訳の過程で歪みが生じるリスクがありました。だからこそ、全員が同じモデルを理解し、同じ言葉で話すことが重要です。Event StormingEvent Storming というワークショップ手法が紹介されています。ドメインエキスパートと開発者が一緒に、ビジネスで起こる「イベント」を付箋に書き出して壁に貼っていきます。「Order form received」「Order placed」「Order shipped」。Event Storming には複数のスコープがあります。本書が扱うのは「プロセスレベル」—特定のワークフローを詳細に分析するものです。「Big Picture」レベルでは、組織全体のドメイン構造を俯瞰します。本章で Ollie が説明したような「顧客は既に商品コードを知っている」「一度に 200〜300 アイテムを注文する」といったドメイン固有の知識は、人間が引き出さなければなりません。ドメインエキスパートは「当たり前」を知っています。その「当たり前」を私たちは知りません。AI エージェント時代においても、この作業は完全には自動化できません。AI はコードベースを読めますが、「なぜそう設計したか」「どんなビジネス制約があるか」は読み取れません。Event Storming で引き出された暗黙知を、CLAUDE.md や設計ドキュメントに言語化する。この作業の価値は、むしろ高まっています。ちなみに2024年発売の『Architecture Modernization』でも同手法が紹介されています。Bounded ContextDDD では「Bounded Context（境界づけられたコンテキスト）」という概念を使って、ドメインを分割します。Bounded Context とは、特定のドメインモデルが適用される明確な境界のことです。同じ「顧客」という言葉でも、販売部門と配送部門では意味が異なることがあります。Bounded Context を分けることで、各コンテキスト内では用語の意味が一貫します。本章の例では、注文処理、配送、請求という 3 つの Bounded Context が登場します。Bounded Context は、コードの境界だけでなく、チームの境界にも影響します。1 つの Context を 1 つのチームが担当するのが理想です。境界が曖昧だと、チーム間の調整コストが増大します。明確に境界が定義された Bounded Context は、変更の影響範囲を限定できます。Ubiquitous LanguageDDD では「Ubiquitous Language（ユビキタス言語）」という概念があります。これは、ドメインエキスパートと開発者がコミュニケーションに使う共通の語彙であり、そのままコード上の命名にも使われます。OrderFactory、OrderManager、OrderHelper といった技術的な命名は、ドメインエキスパートには意味不明だと著者は述べています。一方、PlaceOrder、ValidateOrder、PriceOrder といったドメインに基づく命名なら、誰もがその意図を理解しやすいです。DDDは過剰か本章の内容を踏まえつつ、批判的な視点も必要です。DDD は、複雑なビジネスドメインを扱う場合に有効とされています。しかし、「まず動くものを作り、後からリファクタリングする」というアプローチが、短期間でのリリースには向いている場合もあります。一方で、事前の設計なしに作られたコードは、しばしば一貫性を欠きます。同じ概念に異なる名前を使ったり、似たロジックを複数箇所に重複させたりします。私自身の経験を振り返ると、DDD を「一度きりの設計作業」として捉えていた頃は失敗が多かったです。あるプロジェクトで Event Storming を実施し、5 つの Bounded Context を特定しました。しかし実装を進めると、そのうち 2 つは同じ Context に統合すべきだと気づきました。別の 1 つは 3 つに分割すべきでした。最初の設計の精度は 6 割程度だったのです。この経験から学んだのは、DDD は「段階的に洗練させる」ものだということです。最初から理想的なモデルを目指すのではなく、実装を通じて境界の妥当性を検証し、継続的に見直します。大規模な変革は「ビッグバン」ではなく「段階的な改善」で進める方が成功率が高い。DDD も例外ではありません。2. Understanding the Domain本章では、ドメインエキスパートへのインタビューを通じてドメインを理解するプロセスが解説されます。コード例が本格的に登場する前に、本書が採用する「関数型プログラミング」というアプローチと、その核心について整理しておきます。Patterns, Principles, and Practices of Domain-Driven Design (English Edition)作者:Millett, Scott,Tune, NickWroxAmazonなぜ「関数型」ドメインモデリングなのか本書のタイトルは「Domain Modeling Made Functional」です。DDD と関数型プログラミングを組み合わせています。なぜでしょうか。関数型プログラミングを学んで獲得する概念は、突き詰めると 1 つのことに集約されます。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。これが関数型の核心です。状態は「境界を越えるとき」だけ必要私たちは普段、プログラムを「状態を変更するもの」として捉えがちです。しかし、ビジネスロジックの大半は「入力を受け取り、何かを計算し、出力を返す」という形式で書けます。注文明細と単価から合計金額を計算する → 状態不要住所文字列をパースして構造化データにする → 状態不要商品コードが有効かどうか検証する → 状態不要状態が「必要」になるのは、システムの境界を越えるときだけです。データベースに保存するとき、外部 API を呼び出すとき、ファイルに書き込むとき。この事実に気づくと、設計の発想が変わります。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられない同時に、状態のトランザクションは現実のシステムでは避けられません。注文は「未検証」から「検証済み」に変わります。ビジネスの世界は状態遷移で満ちています。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。// オブジェクト指向的なアプローチ（問題あり）struct Order {    status: OrderStatus,    customer_info: Option<CustomerInfo>,    validated_at: Option<DateTime>,    amount: Option<Decimal>,}impl Order {    fn validate(&mut self) {        self.status = OrderStatus::Validated;        self.validated_at = Some(now());    }}この設計の問題は、状態の「今」しか見えないこと、そして Option フィールドの組み合わせ爆発です。validated_at が Some で amount が None の状態は正しいのでしょうか？整合性を開発者が頭の中で管理し続けなければなりません。Transformation-Oriented Programmingという答え本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。核心は「元のオブジェクトを変更しない」ことです。状態ごとに異なる型を作ります。UnvalidatedOrder は「未検証の注文」を表す型です。ValidatedOrder は「検証済みの注文」を表す型です。これらは別の型であり、別の構造を持ちます。そして、validate 関数は UnvalidatedOrder を受け取り、新しい ValidatedOrder を返します。元の UnvalidatedOrder には触れません。pub struct UnvalidatedOrder {    pub order_id: String,    pub customer_info: String,    pub shipping_address: String,}pub struct ValidatedOrder {    pub order_id: OrderId,    pub customer_info: CustomerInfo,    pub shipping_address: Address,}fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // 元のUnvalidatedOrderは変更されない}重要なのは、元の UnvalidatedOrder は変更されないということです。validate 関数は新しい ValidatedOrder を「作る」だけです。状態を変えるな。新しい値を作れ。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrderこれは「パイプライン」です。データがパイプを流れていきます。各関数は入力を受け取り、出力を返します。それだけです。なぜこのアプローチが強力なのか状態の追跡が不要: 型を見れば分かります。ValidatedOrder を持っているなら、それは「検証済みの注文」です並行処理での競合がない: 元のデータを変更しないから、複数のスレッドが同時に処理しても問題ありませんテストが簡単: 入力を与えて、出力を確認します。モックも不要ですそして何より、ビジネスプロセスが本質的に「入力を受け取り、何かを行い、出力を返す」ものだから相性が良いのです。「見積書」が「発注書」になります。「申請書」が「承認済み申請書」になります。ビジネスの人々は、無意識のうちにこのモデルで考えています。F#という選択とRustでの実践本書の実装言語は F#です。著者が F#を選んだ理由は、「実用的な関数型言語」として設計されており、.NET エコシステムの資産を活用できるからです。本感想文は F#ではなく Rust で実装を示します。私が Rust を選んだ理由は、現在の私にとって主要言語であること、そして所有権システムによる状態遷移の明示化に興味があったからです。Rust は「関数型言語」ではありませんが、関数型の重要な特徴を備えています。代数的データ型: struct と enum で、F#のレコード型と判別共用体を表現できますイミュータビリティ: デフォルトで変数は不変ですパターンマッチング: 網羅的なパターンマッチを強制しますOption/Result: 欠損値とエラーを型で表現しますRust構文の基礎ここで、本感想文で使う Rust の基本を整理しておきます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgまず「型」とは何でしょうか。型とは「値の種類」のことです。数値、文字列、日付、注文情報—これらは全て異なる「種類」の値であり、それぞれに型があります。型があると、「文字列を数値で割る」といった意味のない操作をコンパイラ（プログラムを機械語に変換するソフトウェア）が事前に検出してくれます。struct（構造体）: 複数の値をまとめて 1 つの「もの」として扱う仕組みです。例えば「注文」は「注文 ID」と「顧客情報」と「配送先」を持ちます。これらをまとめて Order という 1 つの型にできます。pub struct Order {    pub id: OrderId,       // フィールド（構成要素）    pub customer_info: String,}pub は「public（公開）」の略で、外部からアクセスできることを意味します。enum（列挙型）:「A か B か C のどれか」を表す型です。例えば注文の状態は「未処理」か「処理済み」か「発送済み」のいずれかです。enum OrderStatus {    Pending,     // 未処理    Validated,   // 検証済み    Shipped,     // 発送済み}関数: 入力を受け取り、何かの処理をして、出力を返すものです。fn で定義します。fn add(a: i32, b: i32) -> i32 {    a + b}i32 は 32 ビット整数という型です。-> i32 は「i32 型の値を返す」という意味です。impl: 型に「できること」（メソッド）を追加します。impl Order {    fn total(&self) -> Money { /* ... */ }}&self は「自分自身を参照する」という意味です。これで order.total() のように呼び出せます。Option<T>:「値があるかもしれないし、ないかもしれない」を表す型です。Some(値) なら値がある、None なら値がありません。Result<T, E>:「成功か失敗か」を表す型です。Ok(値) なら成功、Err(エラー) なら失敗です。doc.rust-lang.org所有権: Rust の最も特徴的な概念です。値は常に 1 つの変数だけが「持っている」のです。関数に渡すと、その値の所有権が移動し、元の変数では使えなくなります。これが「古い状態を誤って使う」ミスを防いでくれます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgF#と Rust で異なる部分—ガベージコレクション vs 所有権、パイプライン演算子、computation expressions—については、該当箇所で必要になったときに具体的に説明します。所有権の概念は、一見すると制約に見えます。しかし、ドメインモデリングにおいては「状態遷移」を明確にする利点があります。fn validate(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    // UnvalidatedOrderの所有権がこの関数に移動    // 呼び出し元ではUnvalidatedOrderは使えなくなる    // → 検証前の注文を誤って使うことがない    Ok(ValidatedOrder { /* ... */ })}F#では同じ order 変数を後から参照できてしまいますが、Rust では所有権の移動により「古い状態へのアクセス」がコンパイルエラーになります。これは Transformation-Oriented Programming の考え方をさらに強化しています。ドメインエキスパートへのインタビュー第 2 章は、ドメインエキスパート（Ollie）へのインタビューから始まります。インタビューの冒頭で、著者は典型的な e コマースモデルを想定していました。しかし Ollie の回答は違いました。「顧客は既に商品コードを知っている。一度に 200〜300 アイテムを注文することもある」。Widgets 社のドメインは「一般的」ではありません。B2B で、顧客はエキスパートで、商品コードを直接入力します。この固有の要件は、人間がドメインエキスパートから引き出さなければなりません。データベース駆動設計への衝動本章で参考になったのは、「データベース駆動設計と戦う」というセクションです。注文フォームを見ると、多くの開発者はすぐにテーブル設計を始めたくなります。著者はこれを「間違い」と断言しています。DDD では、ドメインが設計を駆動するのであって、データベーススキーマが駆動するのではありません。永続化の無知（Persistence Ignorance）は重要な原則です。まずドメインの概念とワークフローを整理し、永続化は後から考えます。テキストベースのドメイン文書化本章では、ドメインを文書化するためのシンプルな記法が紹介されています。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこの擬似コードは、Rust の構造体定義にほぼそのまま翻訳できます。「AND」は struct のフィールド、「OR」は enum のバリアントになります。ドメインエキスパートと開発者の両方が読める、共通言語として機能します。オーダーのライフサイクルと状態の型本章の後半で、注文には複数のフェーズがあることが明らかになります。UnvalidatedOrder: 届いたばかりの状態ValidatedOrder: 検証済みの状態PricedOrder: 価格が計算された状態data UnvalidatedOrder =    UnvalidatedCustomerInfo    AND UnvalidatedShippingAddress    AND list of UnvalidatedOrderLinedata ValidatedOrder =    ValidatedCustomerInfo    AND ValidatedShippingAddress    AND list of ValidatedOrderLineこの「状態ごとに別の型を定義する」パターンは、Rust では構造体として実装します。状態遷移は関数のシグネチャとして型付けされ、コンパイラが不正な状態遷移を検出してくれます。ワークフローの分解最終的に、注文処理ワークフローは以下のステップに分解されます。substep "ValidateOrder" =    input: UnvalidatedOrder    output: ValidatedOrder OR ValidationError    dependencies: CheckProductCodeExists, CheckAddressExistssubstep "PriceOrder" =    input: ValidatedOrder    output: PricedOrder    dependencies: GetProductPriceワークフローを小さなステップに分解することで、各ステップが独立してテスト可能になります。入力・出力・依存関係が明確に定義されていれば、実装も容易になります。3. A Functional Architecture本章は、関数型アーキテクチャの原則を解説します。Bounded Context、イベント駆動通信、Onion Architecture。これらの概念は言語に依存しません。アーキテクチャを考えるタイミング第 3 章の冒頭で、著者は矛盾したことを述べています。「この段階でアーキテクチャについて考えすぎるべきではない。まだシステムを理解していないからだ」。しかし同時に「大まかな実装方針を持っておくのは良いことだ」とも言います。著者の「walking skeleton（動く骨格）」というアプローチは有効です。まず最小限の構造を設計し、その骨格に沿ってコードを書いていきます。Bounded Contextと自律性Bounded Context をソフトウェアコンポーネントとしてどう実装するか。モノリス内のモジュール、独立したアセンブリ、マイクロサービス。いくつかの選択肢があります。著者は「最初はモノリスとして構築し、スケールや独立デプロイが求められる段階で分離する」ことを勧めています。マイクロサービスを夢見て最初から分割し、サービス間通信の地獄に落ちた経験がある人には、身に染みる助言でしょう。私もその一人です。最初から理想的なマイクロサービスを目指すと、サービス間の境界を間違えたときの修正コストが膨大になります。まずモノリス内でモジュールを分離し、境界が安定してからサービスに切り出す。これを最初から知っていれば、いくつかの深夜対応は避けられたかもしれません。マイクロサービスアーキテクチャ 第2版作者:Sam Newmanオーム社AmazonイベントによるContext間通信Bounded Context 間の通信は、イベントを介して行われます。Place-Order ワークフローが OrderPlaced イベントを発行し、Shipping コンテキストがそれを受け取って ShipOrder コマンドを生成します。この非同期・疎結合のパターンは、変更の影響範囲を限定できます。各 Context が独立したイベントの発行者・購読者として定義されていれば、一方の変更が他方に波及しにくくなります。DTOと信頼境界本章で重要な概念が登場します。Domain Object と Data Transfer Object (DTO) の区別です。Domain Object は、Bounded Context 内部でのみ使用されます。DTO は、Context 間の通信やシリアライズのために設計されます。同じ「Order」でも、内部で使う Order と、外部に公開する OrderDTO は別物です。さらに、Bounded Context の境界は「信頼境界」として機能します。外部からのデータは信頼できません。内部に入る前にバリデーションが必要です。Context間の契約関係Context 間の契約関係について 3 つのパターンが紹介されます。Shared Kernel: 両チームが共同で契約を所有Customer/Supplier: 下流の Context が契約を定義Conformist: 上流の Context の契約に従うこれらの関係は、技術的な問題であると同時に組織的な問題でもあります。Onion Architecture と I/O の分離本章の後半では、コードの構造について議論されます。Onion Architecture では、ドメインが中心にあり、I/O は外周に配置されます。依存関係は常に内側に向かいます。純粋なコアを、不純な殻で包みます。「I/Oはワークフローの端でのみ行う。ワークフロー内部は純粋な関数で構成する」この原則は、第 2 章で述べた「状態は例外的」という考え方と直結します。ワークフロー内部は「入力を受け取り、何かを行い、出力を返す」純粋な関数だけで構成されます。データベースアクセスやファイル I/O は、ワークフローの開始時か終了時にのみ行います。この構造により、ドメインロジックはテスト容易で予測可能になります。少なくとも、理論上は。4. Understanding Types本章から、コード例が本格的に登場します。第 2 章で整理した F#と Rust の対応関係に基づき、以降は Rust のみで実装を示します。型とは「可能な値の集合」である著者の「型」の定義はシンプルです。「関数の入力や出力として使える値の集合に付けた名前」。i16 は-32768 から+32767 までの数値の集合、String は全ての文字列の集合です。この定義を読んで、自分がいかに型を「コンパイラを満足させるためのもの」として捉えていたか気づかされました。型は思考のツールです。ANDとORによる型の合成—代数的データ型本章の核心は、型の合成方法です。著者は 2 つの方法を示します。これらは「代数的データ型（Algebraic Data Types）」と呼ばれ、関数型プログラミングの基礎概念です。F#では「レコード型」と「判別共用体」、Rust では struct と enum で表現できます。AND型（struct / 積型）: 複数の値を組み合わせます。struct FruitSalad {    apple: AppleVariety,    banana: BananaVariety,    cherries: CherryVariety,}FruitSalad を作るには、apple と banana と cherries の全てが必要です。OR型（enum / 和型）: 複数の選択肢から 1 つを選びます。enum FruitSnack {    Apple(AppleVariety),    Banana(BananaVariety),    Cherries(CherryVariety),}FruitSnack は、Apple か Banana か Cherries のいずれか 1 つです。たった 2 つの概念で複雑なドメインを表現できます。AND と OR という論理演算で型を組み立てます。Simple Types—newtype patternの威力本章で一番「これが使える」と思ったのは、Simple Types の話です。プリミティブ型をそのまま使うのは危険です。CustomerId も OrderId も i32 だとしたら、間違って OrderId を CustomerId として渡してもコンパイルが通ってしまいます。Rust では、newtype patternでこの問題を解決します。newtype pattern とは、既存の型を新しい型でラップすることで、型レベルで区別をつけるイディオムです。F#では「単一ケース判別共用体」、Rust では「タプル構造体」で表現します。zenn.dev#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct CustomerId(i32);#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]struct OrderId(i32);#[derive(...)] について説明します。これは Rust の「属性マクロ」で、型に機能を自動で追加する仕組みです。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org「トレイト」とは、型が持つべき「能力」や「振る舞い」の定義です。例えば「比較できる」「コピーできる」「文字列として表示できる」といった能力がトレイトとして定義されています。#[derive(Debug, Clone)] と書くと、その型に Debug と Clone という能力が自動的に追加されます。手で書くと何十行にもなるコードを、一行で済ませられます。よく使うトレイトを整理しておきます。 トレイト  意味  使いどころ  Debug  中身を表示できる  println!("{:?}", x) でデバッグ出力  Clone  複製を作れる  .clone() で明示的にコピー  Copy  自動で複製される  代入や関数呼び出しで自動コピー（小さな値向け）  PartialEq  比較できる  == で等しいか判定  Eq  反射律を満たす比較  HashMapのキーに使うとき必要  Hash  ハッシュ値を計算できる  HashMapのキーに使うとき必要 内部的には同じ i32 ですが、型システム上は別の型です。CustomerId を期待する関数に OrderId を渡すとコンパイルエラーになります。ここで重要なのは、Clippy のような静的解析ツールでもこの種のバグは検出できないということです。Clippy は Rust の公式リンター（コード品質チェックツール）で、700 以上の lint ルールを持ちます。cargo clippy コマンドで実行でき、コードの問題点を警告してくれます。rust-lang.github.ioしかし、Clippy にも限界があります。// Clippyでは検出できないfn process(customer_id: i32, order_id: i32) { /* ... */ }process(order_id, customer_id); // バグ！でもコンパイルは通る// 型で防ぐfn process(customer_id: CustomerId, order_id: OrderId) { /* ... */ }// process(order_id, customer_id); // コンパイルエラー！Clippy は構文的な問題—if x { "a" } else { "a" } のような両方のブランチが同じ処理、u32 >= 0 のような常に true になる比較—は検出できます。しかし、「この i32 は顧客 ID を表し、あの i32 は注文 ID を表す」というドメインの知識は持っていません。newtype pattern は、Clippy が検出できないバグを型システムで防ぎます。AI コーディングエージェントも同様です。「customer_id と order_id を間違えないように」という指示は、自然言語では曖昧です。しかし CustomerId と OrderId という別の型が定義されていれば、AI が生成したコードでも型の取り違えはコンパイル時に検出されます。Option型とResult型F#と Rust は、欠損値を Option で、エラーを Result で表現します。これらは関数型プログラミングにおける標準的なエラーハンドリング手法で、null や例外を使わずに「値がないかもしれない」「失敗するかもしれない」ことを型で表現します。struct PersonalName {    first_name: String,    middle_initial: Option<String>,  // 省略可能    last_name: String,}Option<T> は Some(T) か None のいずれかです。null を使わずに「値がないかもしれない」ことを型で表現します。エラーハンドリングには Result<T, E> を使います。? 演算子で、エラー時に早期リターンできます。型によるドメイン表現本章で紹介されている支払い方法のモデリング例は、型がドキュメントとして機能することを示しています。enum PaymentMethod {    Cash,    Check(CheckNumber),    Card(CreditCardInfo),}struct Payment {    amount: PaymentAmount,    currency: Currency,    method: PaymentMethod,}約 25 行で、支払いドメインの構造が明確に表現されています。このコードは、ドメインエキスパートにも読めます。型システムは思考のツールである本章を通じて感じたのは、型システムは「コンパイラのため」ではなく「思考のため」にあるということです。「動的型付け言語でも同じことができるのでは？」という疑問があるかもしれません。確かに、Python や Ruby でもドメインモデリングはできます。しかし、型がないと「どんな値が入りうるか」を頭の中で追跡し続けなければなりません。静的型付けでは、その追跡をコンパイラに委ねられます。AND 型と OR 型という単純な組み合わせで、複雑なドメインを表現できます。型定義という明確な仕様があれば、実装時の迷いが減ります。型システムは、ドメインの構造を可視化するツールです。5. Domain Modeling with Types本章では、前章で学んだ型システムの概念を使って、実際にドメインモデルを構築します。コードがドキュメントになる第 5 章の冒頭で、著者は挑戦的な問いを投げかけます。「ソースコードを直接ドキュメントとして使い、UML 図のような別の成果物を不要にできるか？」正直、最初は懐疑的でした。しかし本章を読み進めるうちに、著者の意図が分かってきました。擬似コードからRustへ第 2 章で作成した擬似コードを、Rust の型に変換します。data Order =    CustomerInfo    AND ShippingAddress    AND BillingAddress    AND list of OrderLines    AND AmountToBillこれが Rust では以下のようになります。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,    pub shipping_address: ShippingAddress,    pub billing_address: BillingAddress,    pub order_lines: Vec<OrderLine>,    pub amount_to_bill: BillingAmount,}ほぼ一対一の変換です。擬似コードと Rust コードを並べて見ると、ドメインの構造がそのまま型に反映されていることが分かります。Value ObjectとEntityDDD では、オブジェクトを「Value Object」と「Entity」に分類します。Value Object: 同じ値を持てば同一とみなします。#[derive(Debug, Clone, PartialEq, Eq)]pub struct PersonalName {    pub first_name: String,    pub middle_initial: Option<String>,    pub last_name: String,}Entity: 固有の ID を持ち、内容が変わっても同一性を保ちます。impl PartialEq for Contact {    fn eq(&self, other: &Self) -> bool {        self.contact_id == other.contact_id  // IDのみで比較    }}Aggregate—一貫性の境界本章で最も重要な概念は「Aggregate」です。Order と OrderLine の関係を考えます。OrderLine の価格を変更したとき、Order の合計金額も更新します。両者は常に一貫した状態を保ちます。DDD では、こうした関連オブジェクトの集合を「Aggregate」と呼び、最上位のオブジェクトを「Aggregate Root」と呼びます。immutable なパターンでは、OrderLine を変更するには Order 全体を作り直します。これは一見非効率に見えますが、一貫性を強制する効果があります。OrderLine だけを変更して、Order の合計金額を更新し忘れる、というバグが起こりにくくなります。Aggregate参照—IDのみを保持するOrder に Customer 情報を含める場合、Customer オブジェクト全体ではなく、CustomerId だけを保持すべきです。pub struct Order {    pub id: OrderId,    pub customer_id: CustomerId,  // Customer全体ではなく、IDのみ    pub order_lines: Vec<OrderLine>,}この設計は、immutability と相性が良いです。Customer の電話番号が変わっても、Order を更新する必要がありません。型でドメインを表現する—最終形本章の最後に、完全なドメインモデルの例が示されます。#[derive(Debug, Clone, PartialEq, Eq)]pub enum ProductCode {    Widget(WidgetCode),    Gizmo(GizmoCode),}#[derive(Debug, Clone, Copy, PartialEq)]pub enum OrderQuantity {    Unit(UnitQuantity),    Kilos(KilogramQuantity),}このコードは、第 2 章の擬似コードとほぼ同じ構造を持ちます。struct、enum、match の意味さえ分かれば読めます。match については Rust 公式ドキュメントを参照してください。doc.rust-lang.orgString は沈黙します。EmailAddress は語ります。著者の主張—「型でドメインを表現すれば、コードがドキュメントになる」—は正しいと思います。6. Integrity and Consistency in the Domain本章は、ドメイン内のデータが常に「信頼できる状態」であることを保証する方法を解説します。Smart Constructor—制約を強制する本章で最も実用的だったのは、Smart Constructor（スマートコンストラクタ）のパターンです。Smart Constructor とは、値の生成時にバリデーションを行い、不正な値の生成を防ぐコンストラクタのことです。通常のコンストラクタと異なり、Result を返して生成の失敗を表現できます。例えば、UnitQuantity は 1 から 1000 の間の値でなければなりません。この制約をコメントで書くだけでは不十分です。#[derive(Debug, Clone, Copy, PartialEq, Eq)]pub struct UnitQuantity(i32);impl UnitQuantity {    pub fn new(value: i32) -> Result<Self, String> {        if value < 1 {            Err("UnitQuantity must be at least 1".to_string())        } else if value > 1000 {            Err("UnitQuantity must be at most 1000".to_string())        } else {            Ok(UnitQuantity(value))        }    }    pub fn value(&self) -> i32 {        self.0    }}フィールドを pub にしなければ、外部から直接 UnitQuantity(500) と書けません。必ず UnitQuantity::new(500) を経由します。NonEmptyList—空のリストを許さない「注文には少なくとも 1 つの注文行がなければならない」という要件を、型で強制できるでしょうか。#[derive(Debug, Clone, PartialEq)]pub struct NonEmptyList<T> {    pub first: T,    pub rest: Vec<T>,}from_vec は Option を返します。空のベクターからは NonEmptyList を作れません。この「作れない」という事実が型で表現されています。Make Illegal States Unrepresentable本章で最も重要な原則は「不正な状態を表現不可能にする」です。メールアドレスの例が分かりやすいです。「検証済み」と「未検証」のメールアドレスがあるとき、フラグで区別する設計は危険です。詳しくは Rust 公式ドキュメントの enum 解説を参照してください。doc.rust-lang.org// 良い例：別の型として定義pub struct VerifiedEmailAddress(String);pub enum CustomerEmail {    Unverified(EmailAddress),    Verified(VerifiedEmailAddress),}VerifiedEmailAddress のコンストラクタを private にして、検証サービスからしか作れないようにします。これで、検証を経ずに Verified 状態を作ることが物理的に不可能になります。fn send_password_reset(email: VerifiedEmailAddress) -> Result<(), SendError> {    // この関数にEmailAddressを渡すとコンパイルエラー}連絡先情報の例—OR型の活用「顧客にはメールアドレスか住所のどちらか、または両方が必要」という要件を型で表現します。pub enum ContactInfo {    EmailOnly(EmailContactInfo),    AddressOnly(PostalContactInfo),    EmailAndAddress(BothContactMethods),}3 つのケースしかありません。「メールも住所もない」という状態は表現できません。「不可能を作る」という設計思想本章の核心は「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」です。この原則を言い換えれば、型で「不可能」を作るということになります。この原則を読んだとき、過去に遭遇したバグが走馬灯のように思い出されました。is_active = true なのに deleted_at が設定されている。status = "paid" なのに payment_id が null。フラグと Option の組み合わせ爆発で、「あり得ない」状態が本番データベースに存在していた。深夜に呼び出されて、整合性を手作業で修正した夜のことを、今でも覚えています。あのバグは、型で防げたのです。似たような経験をしたことがある人は、少なくないのではないでしょうか。NonEmptyList を使えば、空の注文は作れないVerifiedEmailAddress を使えば、未検証メールへのパスワードリセットは書けないSmart Constructor を使えば、範囲外の値は存在できない「できない」「書けない」「存在できない」—これらは制限ではなく、設計上の保証です。バリデーションは「お願い」。型は「物理法則」。Clippy のような静的解析ツールでも、ドメインロジックの問題は検出できません。例えば、is_priced: bool と amount: Option<f64> を持つ構造体を考えます。is_priced = true なのに amount = None という矛盾した状態は、Clippy には「正しい Rust コード」に見えます。ビジネスルールを知らないからです。しかし、PricedOrder { amount: Money } と UnpricedOrder を別の型として定義すれば、この矛盾は表現できなくなります。Clippy が検出できない問題を、型システムが防ぎます。AI エージェント時代において、この「不可能を作る」設計思想の価値は高まっています。AI は自然言語のドキュメントを読み飛ばすことがあります。しかし、型で「不可能」が定義されていれば、AI はその制約を破るコードを物理的に書けません。7. Modeling Workflows as Pipelines本章は、ワークフローをパイプラインとしてモデリングする方法を解説します。ビジネスプロセスを「変換の連鎖」として捉えるアプローチです。ワークフローはパイプラインである本章の冒頭で、著者は注文処理ワークフローを次のように要約しています。workflow "Place Order" =    input: UnvalidatedOrder    output: OrderPlaced AND BillableOrderPlaced AND OrderAcknowledgmentSent    // step 1: ValidateOrder    // step 2: PriceOrder    // step 3: AcknowledgeOrder    // step 4: create and return events各ステップは「入力を受け取り、変換し、出力を返す」関数です。これらを連結するとパイプラインになります。第 2 章で述べた「状態は例外的、ほとんどの処理は状態なしで書ける」という原則が、ここで具現化されます。ワークフロー全体を見ると「状態遷移」に見えますが、各ステップを見ると「入力を受け取り、何かを行い、出力を返す」純粋な関数でしかありません。状態マシンとしてのOrderOrder を単一の型として設計すると、フラグだらけになります。// 悪い設計struct Order {    order_id: OrderId,    is_validated: bool,    is_priced: bool,    amount_to_bill: Option<Decimal>,  // pricedの時だけ存在}本書のアプローチは、各状態を別の型として定義することです。pub struct UnvalidatedOrder { /* ... */ }pub struct ValidatedOrder { /* ... */ }pub struct PricedOrder {    // ...    pub amount_to_bill: BillingAmount,  // この状態でのみ存在}PricedOrder には amount_to_bill があります。ValidatedOrder にはありません。フラグは不要で、「どの型か」が状態を表します。AI にコードを書かせるとき、この設計は強力なガードレールになります。「検証をスキップして価格計算に進んでください」と指示しても、price() 関数が ValidatedOrder を要求する以上、AI は UnvalidatedOrder を渡すコードを書けません。型が不正な状態遷移を物理的に阻止します。依存性を型で表現する各ステップの依存性を型シグネチャで表現します。type CheckProductCodeExists = fn(&ProductCode) -> bool;type CheckAddressExists = fn(&UnvalidatedAddress) -> Result<CheckedAddress, AddressValidationError>;type ValidateOrder = fn(    CheckProductCodeExists,     // 依存性1    CheckAddressExists,         // 依存性2    UnvalidatedOrder,           // 入力) -> Result<ValidatedOrder, ValidationError>;依存性が関数の引数として明示されます。インターフェース全体ではなく、必要な関数だけを渡します。最小限の依存性です。エフェクトの文書化関数の「エフェクト（効果）」を型で文書化します。Result: エラーを返す可能性があるAsync: 非同期 I/O を行うOption: 値が存在しない可能性があるasync fn check_address_exists(    address: &UnvalidatedAddress,) -> Result<CheckedAddress, AddressValidationError> {    // 外部サービスへのHTTPリクエスト}関数シグネチャを見れば、「この関数は非同期で、エラーを返す可能性がある」と分かります。Transformation-Oriented Programmingの実践具体的な例で考えます。オブジェクト指向的に書くと、こうなります。impl Order {    fn validate(&mut self, checker: &ProductChecker) -> Result<(), ValidationError> {        // 状態を変更        self.is_validated = true;        self.validated_at = Some(now());        Ok(())    }}関数型で書き直すと、こうなります。fn validate(    order: UnvalidatedOrder,    checker: &ProductChecker,) -> Result<ValidatedOrder, ValidationError> {    // 新しい値を作る（元のorderは変更しない）    Ok(ValidatedOrder {        order_id: OrderId::new(order.order_id)?,        customer_info: validate_customer(order.customer_info)?,        lines: validated_lines,    })}違いは何でしょうか。コンパイル時チェック: price() に UnvalidatedOrder を渡すとコンパイルエラー状態の整合性が型で保証: ValidatedOrder には is_validated フラグがそもそも存在しないテストが独立: validate() と price() を別々にテストできるこの単純さが強力なのだUnvalidatedOrder を ValidatedOrder に変換します。ValidatedOrder を PricedOrder に変換します。元のオブジェクトは触りません。新しいオブジェクトを作ります。それだけです。状態の変更を追跡する必要がない（変更しないから）並行処理でも競合しない（元のデータを変更しないから）テストが簡単（入力と出力を比較するだけ）デバッグが楽（各ステップの入出力をログに残せば、全経路が追える）関数型プログラミングの入門書を読むと、モナドだの圏論だの、難しい概念が出てきます。しかし、実務で最も重要なのは、本書が示すTransformation-Oriented Programmingです。核心は 3 つです。状態を型で表現する（UnvalidatedOrder と ValidatedOrder は別の型）状態遷移を関数で表現する（validate(order) -> ValidatedOrder）元のオブジェクトを変更しない（新しい値を作るだけ）変えるな。作れ。この章を読み終えたとき、「これなら実務で使える」と確信しました。モナドや圏論を理解する必要はありません。「状態ごとに型を分ける」「元のオブジェクトを変更しない」。この 2 つだけで、設計の質は劇的に変わります。難しい理論ではなく、明日から使える実践知。本書の価値はここにあります。8. Understanding Functions本章は、関数型プログラミングの基礎を解説します。実装に入る前の準備として、関数の扱い方を整理しています。関数型プログラミングとは著者の定義はシンプルです。「関数型プログラミングとは、関数が本当に重要なものとしてプログラミングすること」。オブジェクト指向では、オブジェクトがあらゆる場所で使われます。関数型では、関数があらゆる場所で使われます。依存性を注入するときは関数を渡します。コードを再利用するときは関数を合成します。関数は「モノ」である関数型プログラミングの核心は、関数が第一級の値であることです。変数に代入できます。リストに入れられます。引数として渡せます。戻り値として返せます。let add1 = |x: i32| x + 1;let square = |x: i32| x * x;let functions: Vec<fn(i32) -> i32> = vec![add1, square];for f in &functions {    println!("{}", f(5));}関数をリストに入れて、ループで回しています。高階関数関数を引数に取る関数、または関数を返す関数を「高階関数」と呼びます。fn eval_with_5_then_add_2<F>(f: F) -> i32where    F: Fn(i32) -> i32,{    f(5) + 2}Rust では、関数を受け取る引数の型を Fn、FnMut、FnOnce トレイトで指定します。F#ではこの区別はありません。クロージャとはクロージャは「名前のない関数」です。通常の関数は fn name(...) と名前をつけて定義しますが、クロージャは |引数| 式 という形式でその場で作れます。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orglet add1 = |x| x + 1;        // 引数xを受け取り、x + 1を返すlet result = add1(5);        // 6クロージャの特徴は、周囲の変数を「捕まえる」（キャプチャする）ことができる点です。let multiplier = 3;let multiply = |x| x * multiplier;  // multiplierを捕まえているlet result = multiply(5);           // 15Fnトレイトの使い分け関数を引数として受け取るとき、Rust では 3 種類のトレイトを使い分けます。これは「捕まえた変数をどう扱うか」で決まります。 トレイト  捕まえ方  呼び出し回数  Fn  読み取るだけ  何度でも  FnMut  変更する  何度でも  FnOnce  消費する（使い切る）  一度だけ 最初は気にしすぎなくてよいです。コンパイラがエラーで教えてくれます。カリー化と部分適用F#では、すべての関数が自動的に「カリー化」されます。Rust にはカリー化が組み込まれていません。同じことを実現するには、明示的にクロージャを返します。fn adder_generator(number_to_add: i32) -> impl Fn(i32) -> i32 {    move |x| number_to_add + x}let add5 = adder_generator(5);let result = add5(3);  // 8部分適用は、依存性注入に活用できます。let validate = |order| validate_order(    check_product_code_exists,    check_address_exists,    order,);let result = validate(unvalidated_order);Total Functions（全域関数）数学の関数は、すべての入力に対して出力が定義されます。12 を引数で割る関数を考えます。n = 0 のとき、何を返すべきでしょうか。解決策は 2 つあります。入力を制限するか、出力を拡張するかです。// 入力を制限fn twelve_divided_by(n: NonZeroI32) -> i32 {    12 / n.0}// 出力を拡張fn twelve_divided_by(n: i32) -> Option<i32> {    if n == 0 { None } else { Some(12 / n) }}どちらの場合も、型シグネチャが正直になります。型シグネチャは嘘をつきません。コメントは嘘をつきます。AI にコードを書かせるとき、この「正直な型シグネチャ」は重要です。AI は型シグネチャを見て、関数の契約を理解します。Option<i32> を返す関数なら、AI は None のケースを考慮したコードを生成します。しかし「0 を渡したら None を返します」というコメントは、読み飛ばされる可能性があります。関数合成F#にはパイプライン演算子 |> があります。Rust にはありません。代わりにメソッドチェーンや、関数を直接呼び出します。let result: Vec<_> = (1..10)    .map(|x| x + 1)    .map(|x| x * x)    .collect();イテレータのアダプタは、パイプラインに近い書き方ができます。Rustで関数型プログラミングを実践するために本章を読んで、F#と Rust の違いを改めて認識しました。カリー化: F#は自動、Rust は手動パイプライン演算子: F#にはある、Rust にはないクロージャの所有権: F#は考慮不要、Rust は move や Fn/FnMut/FnOnce を意識これらの違いはありますが、関数型プログラミングの本質—関数を組み合わせてシステムを構築する—は Rust でも実践できます(が本当に最適か？という問いは投げないでくれ…本稿のアプローチと違いすぎる)。9. Implementation: Composing a Pipeline本章から、いよいよ実装に入ります。これまで型で設計してきたワークフローを、実際のコードに落とし込みます。パイプラインの理想形著者が示す理想のコードは驚くほどシンプルです。let placeOrder unvalidatedOrder =    unvalidatedOrder    |> validateOrder    |> priceOrder    |> acknowledgeOrder    |> createEvents4 行で注文処理全体が表現されています。これが関数型アプローチの目指す姿です。しかし現実には、関数の出力と次の関数の入力が一致しません。依存性をどこかで解決しなければなりません。本章はその「ギャップ」を埋める方法を解説します。型シグネチャによる実装のガイド本章で印象的だったのは、「型シグネチャを先に定義し、それに従って実装する」というアプローチです。type ValidateOrder = fn(    check_product_code: fn(&ProductCode) -> bool,    check_address: fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder;型シグネチャが「契約」として機能します。引数の型、戻り値の型が全て決まっているので、実装者は「この契約を満たすコードを書く」だけでよいです。依存性注入の関数型アプローチオブジェクト指向では、インターフェースを定義し、コンストラクタで注入します。関数型では、依存性を関数の引数として渡します。DI コンテナ？関数を渡せ。fn validate_order(    check_product_code: impl Fn(&ProductCode) -> bool,    check_address: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    order: UnvalidatedOrder,) -> ValidatedOrder {    // check_product_code を使う}インターフェース全体ではなく、必要な関数だけを渡します。ワークフロー全体の組み立て各ステップを組み立ててワークフロー全体を作ります。pub fn place_order(    // 依存性    check_product_code_exists: impl Fn(&ProductCode) -> bool,    check_address_exists: impl Fn(&UnvalidatedAddress) -> CheckedAddress,    get_product_price: impl Fn(&ProductCode) -> Price,    // 入力    unvalidated_order: UnvalidatedOrder,) -> Vec<PlaceOrderEvent> {    let validated = validate_order(        &check_product_code_exists,        &check_address_exists,        unvalidated_order,    );    let priced = price_order(&get_product_price, validated);    let acknowledgment = acknowledge_order(&priced);    create_events(&priced, acknowledgment)}F#のパイプライン演算子 |> がないので、変数に束縛しながら連鎖させます。本章では Result を使わず簡略化しており、次章で Result を導入します。10. Implementation: Working with Errors本章は、エラーハンドリングの関数型アプローチを解説します。「Railway Oriented Programming」と呼ばれるパターンを学びます。エラーの三分類著者はエラーを 3 つに分類します。Domain Errors: ビジネスプロセスの一部として予期されるエラー。商品コードが無効、注文が請求で拒否される、など。Panics: システムを未知の状態にするエラー。メモリ不足、ゼロ除算など。Infrastructure Errors: ネットワークタイムアウト、認証失敗など。この分類は実務でも有用です。「このエラーはドメインエキスパートに相談すべきか」という問いに答えられます。ドメインエラーを型で表現する#[derive(Debug, Clone)]pub enum PlaceOrderError {    ValidationError(String),    ProductOutOfStock(ProductCode),    RemoteServiceError(RemoteServiceError),}エラーを enum で定義することで、どんなエラーが起こりうるか、型定義を見れば分かります。Railway Oriented Programming著者が提唱する解決策が「Railway Oriented Programming（鉄道指向プログラミング）」です。著者自身による詳細な解説は以下を参照してください。fsharpforfunandprofit.comResult を返す関数は「分岐するレール」として可視化できます。成功すれば上のレールに、失敗すれば下のレールに進みます。一度失敗パスに入ると、残りのステップはバイパスされます。      [validateOrder] → [priceOrder] → [acknowledgeOrder] → 成功           ↓               ↓               ↓      ─────────────────────────────────────────────────────→ 失敗Rustでの実装Rust では ? 演算子が同じことをより簡潔に表現します。fn validate_order(order: UnvalidatedOrder) -> Result<ValidatedOrder, ValidationError> {    let order_id = OrderId::create(&order.order_id)?;    let customer_info = to_customer_info(&order.customer_info)?;    let shipping_address = check_address(&order.shipping_address)?;    Ok(ValidatedOrder { order_id, customer_info, shipping_address, ... })}? 演算子は、Ok ならアンラップし、Err なら早期リターンします。?演算子の仕組み? は「失敗したら即座に関数から抜ける」という処理を一文字で書ける記号です。詳しくは公式ドキュメントを参照してください。doc.rust-lang.orgResult は「成功（Ok）か失敗（Err）か」を表す型でした。? をつけると、成功なら中身を取り出し、失敗ならその場で関数を終了して呼び出し元にエラーを返します。// ?を使った書き方let order_id = OrderId::create(&order.order_id)?;// これは以下と同じ意味let order_id = match OrderId::create(&order.order_id) {    Ok(v) => v,              // 成功したら中身を取り出す    Err(e) => return Err(e), // 失敗したら即座にエラーを返す};? を使うには、関数の戻り値が Result である必要があります。エラー型の変換複数のステップを連結するとき、エラー型を統一します。#[derive(Debug)]pub enum PlaceOrderError {    Validation(ValidationError),    Pricing(PricingError),}impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}Fromトレイトによる型変換From は「ある型から別の型への変換方法」を定義するトレイトです。例えば「ValidationError を PlaceOrderError に変換する方法」を定義しておくと、? 演算子が自動的にエラー型を変換してくれます。// 「ValidationErrorからPlaceOrderErrorへの変換方法」を定義impl From<ValidationError> for PlaceOrderError {    fn from(e: ValidationError) -> Self {        PlaceOrderError::Validation(e)    }}これを定義しておくと、validate_order が ValidationError を返しても、? が自動的に PlaceOrderError に変換してくれます。異なるエラー型を返す関数を連結できるようになります。11. Serialization本章は、ドメインオブジェクトを JSON や XML などの形式に変換する方法を解説します。Bounded Context の境界を越えるとき、内部のドメイン型をそのまま使うことはできません。また、AI時代にはこのようなことも想定されます。zenn.dev永続化とシリアライゼーションの区別著者は 2 つの概念を区別します。Persistence（永続化）: プロセスの終了後も状態が残ること。Serialization（シリアライゼーション）: ドメイン固有の表現を、永続化可能な表現に変換すること。本章はシリアライゼーションに焦点を当て、次章で永続化を扱います。DTOによる変換—ドメインの境界防御ドメイン型は複雑です。ネストした型、制約付きの型、選択肢を持つ型。これらを直接シリアライズするのは難しいです。解決策は、Data Transfer Object（DTO）を中間層として使うことです。なぜDTOを使うのか？ドメイン型は制約を持つ: String50 は 50 文字以下という制約があります。JSON の "name" フィールドは任意の長さです。直接マッピングできません。内部実装の変更から外部を守る: ドメイン型のフィールド名を変えても、DTO が同じなら外部 API は影響を受けません。検証の境界を明確にする: 外部からの入力は「信頼できない」です。DTO からドメイン型への変換時に検証することで、ドメイン内は常に「信頼できる」状態を保ちます。DTO は「ドメインの境界防御」として機能します。Domain型 → DTO → JSON（シリアライズ）JSON → DTO → Domain型（デシリアライズ）Rust では、ドメイン型と DTO 型を別々に定義します。ドメイン型は制約を持ち、DTO はプリミティブ型のみを使います。/// 制約付きの文字列型（50文字以下）#[derive(Debug, Clone, PartialEq, Eq)]pub struct String50(String);impl String50 {    pub fn create(s: &str) -> Result<Self, ValidationError> {        if s.is_empty() {            Err(ValidationError::Empty("String50 cannot be empty".into()))        } else if s.len() > 50 {            Err(ValidationError::TooLong("String50 must be 50 chars or less".into()))        } else {            Ok(String50(s.to_string()))        }    }    pub fn value(&self) -> &str {        &self.0    }}/// ドメイン型（制約付き）#[derive(Debug, Clone, PartialEq, Eq)]pub struct Person {    pub first_name: String50,    pub last_name: String50,}/// DTO（シリアライズ用・プリミティブのみ）#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]pub struct PersonDto {    pub first_name: String,    pub last_name: String,}変換には From と TryFrom を使います。ドメイン型から DTO への変換は常に成功しますが、DTO からドメイン型への変換は失敗しうります。/// ドメイン型 → DTO（常に成功）impl From<&Person> for PersonDto {    fn from(person: &Person) -> Self {        PersonDto {            first_name: person.first_name.value().to_string(),            last_name: person.last_name.value().to_string(),        }    }}/// DTO → ドメイン型（失敗する可能性あり）impl TryFrom<PersonDto> for Person {    type Error = ValidationError;    fn try_from(dto: PersonDto) -> Result<Self, Self::Error> {        let first_name = String50::create(&dto.first_name)?;        let last_name = String50::create(&dto.last_name)?;        Ok(Person { first_name, last_name })    }}TryFrom を使うことで、変換が失敗する可能性を型で表現しています。これは「Parse, don't validate」の実践です。入力を単に「正しいかどうか」検証するのではなく、より型安全な形式に変換（パース）することで、型レベルで正しさを保証します。DTOは契約である著者が強調するのは、DTO は「Bounded Context 間の契約」だということです。他の Context が発行したイベントを受信するとき、そのフォーマットに依存します。フォーマットを変更すると、依存する Context に影響が及びます。だから、シリアライズのフォーマットは慎重に設計すべきです。serde の #[derive(Serialize)] を安易に使うと、内部実装の変更が契約の破壊につながります。選択肢型（enum）のシリアライズOR 型（enum）のシリアライズは注意が必要です。JSON には enum の概念がありません。Rust では serde の属性でタグ付け方式を指定します。serde については公式ドキュメントを参照してください。serde.rs/// 支払い方法のDTO - タグ付きenum#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]#[serde(tag = "type")]pub enum PaymentMethodDto {    Cash,    Check { check_number: String },    Card { card_number: String, expiry: String },}#[serde(tag = "type")] を指定すると、以下のような JSON が生成されます。{"type":"Cash"}{"type":"Check","check_number":"12345"}{"type":"Card","card_number":"4111...","expiry":"12/25"}タグ付きの方が明示的で、新しいケースを追加しやすいです。ラウンドトリップの検証シリアライズとデシリアライズは対になります。ラウンドトリップ（往復）テストで、データが失われないことを確認します。pub fn serialize_order(order: &Order) -> Result<String, serde_json::Error> {    let dto = OrderDto::from(order);    serde_json::to_string_pretty(&dto)}pub fn deserialize_order(json: &str) -> Result<Order, String> {    let dto: OrderDto = serde_json::from_str(json).map_err(|e| e.to_string())?;    Order::try_from(dto).map_err(|e| format!("{:?}", e))}本章と中心テーマのつながりDTO パターンは、本書のTransformation-Oriented Programmingと関連します。外部からの入力（JSON）は「未検証の値」です。DTO からドメイン型への変換は、UnvalidatedOrder から ValidatedOrder への変換と同じパターンです。信頼できない入力を、信頼できるドメイン型に「変換」します。外部を信頼するな。まず変換せよ。DTO は、外部世界とドメインの境界を守る防壁です。12. Persistence本章は、ドメインモデルをデータベースに永続化する方法を解説します。DDD の原則に従いながら、現実のインフラと向き合います。永続化の原則本章の冒頭で、著者は 3 つの原則を示します。永続化を端に押し出す（Push persistence to the edges）: ワークフローの内部では I/O を行わないコマンドとクエリを分離する（CQRS）: 更新操作と読み取り操作を分けるBounded Contextは自分のデータストアを所有する: 他の Context のデータベースに直接アクセスしないこれらの原則は、「状態は例外的」という関数型の考え方と合致します。永続化を端に押し出す著者は、「ドメインロジックと I/O が混在したコード」と「分離したコード」を対比しています。なぜI/Oを境界に押し出すのか？テストが容易になる: 純粋なドメインロジックは、データベース接続なしでテストできます。入力を与えて出力を確認するだけです。推論が容易になる: 副作用がない関数は、同じ入力に対して常に同じ出力を返します。状態を追跡する必要がありません。並行処理が安全になる: 共有状態を変更しないため、競合が発生しません。変更に強くなる: データベースを変更しても、ドメインロジックは影響を受けません。逆も同様です。まず、ドメイン型を定義します。これらの型はデータベースのことを知りません。/// 未払いの請求書#[derive(Debug, Clone, PartialEq)]pub struct UnpaidInvoice {    pub invoice_id: InvoiceId,    pub amount_due: Money,}/// 支払い済みの請求書#[derive(Debug, Clone, PartialEq)]pub struct PaidInvoice {    pub invoice_id: InvoiceId,    pub amount_paid: Money,}/// 支払い処理の結果#[derive(Debug, Clone, PartialEq)]pub enum InvoicePaymentResult {    FullyPaid(PaidInvoice),    PartiallyPaid(UnpaidInvoice),}次に、純粋なドメインロジックを定義します。この関数は I/O を行いません。/// 支払いを適用する - 純粋関数、I/Oなしpub fn apply_payment(invoice: UnpaidInvoice, payment: Payment) -> InvoicePaymentResult {    let remaining = invoice.amount_due.0 - payment.amount.0;    if remaining <= 0.0 {        InvoicePaymentResult::FullyPaid(PaidInvoice {            invoice_id: invoice.invoice_id,            amount_paid: Money(invoice.amount_due.0),        })    } else {        InvoicePaymentResult::PartiallyPaid(UnpaidInvoice {            invoice_id: invoice.invoice_id,            amount_due: Money(remaining),        })    }}最後に、コマンドハンドラで I/O を境界に押し出します。パターンは「Load → Pure Logic → Save」です。/// コマンドハンドラ - I/Oは境界で行うpub fn pay_invoice_handler<R: InvoiceRepository>(    repo: &R,    command: PayInvoiceCommand,) -> Result<InvoicePaymentResult, PayInvoiceError> {    // 1. Load（I/O - 開始時）    let invoice = repo        .load(&command.invoice_id)        .ok_or(PayInvoiceError::InvoiceNotFound(command.invoice_id))?;    // 2. 純粋なドメインロジック（I/Oなし）    let result = apply_payment(invoice, command.payment);    // 3. Save（I/O - 終了時）    match &result {        InvoicePaymentResult::FullyPaid(paid) => repo.save_paid(paid),        InvoicePaymentResult::PartiallyPaid(unpaid) => repo.save_unpaid(unpaid),    }    Ok(result)}RepositoryパターンRust では Trait を使って Repository を抽象化します。これにより、テスト時にモックを注入できます。なぜTraitで抽象化するのか？テスト容易性: 本番では PostgreSQL、テストではインメモリ実装を注入できます。実装の交換可能性: データベースを変更しても、ドメインロジックは影響を受けません。依存性の逆転: ドメインが永続化の詳細に依存しません。依存の方向が逆になります。トレイトとはトレイトは「この型は〇〇ができる」という能力の定義です。例えば「データを読み込める」「データを保存できる」という能力を定義します。詳しくは公式ドキュメントを参照してください。doc.rust-lang.org/// Repositoryトレイト - 永続化操作を抽象化pub trait InvoiceRepository {    fn load(&self, id: &InvoiceId) -> Option<UnpaidInvoice>;    fn save_unpaid(&self, invoice: &UnpaidInvoice);    fn save_paid(&self, invoice: &PaidInvoice);}trait で能力を定義し、impl Trait for Type で「この型はこの能力を持つ」と宣言します。ジェネリクスとはジェネリクスは「型を後で決める」仕組みです。<R: InvoiceRepository> は「InvoiceRepository という能力を持つ何かの型 R」という意味です。// Rは「InvoiceRepositoryという能力を持つ何か」fn pay_invoice_handler<R: InvoiceRepository>(repo: &R, ...) { ... }これで同じ関数を、異なる実装で使い回せます。// 本番ではPostgreSQLを使うpay_invoice_handler(&postgres_repo, command);// テストではメモリ上の仮実装を使うpay_invoice_handler(&in_memory_repo, command);テスト用のインメモリ実装を作れば、実際のデータベースなしでドメインロジックをテストできます。Persistence Ignorance（永続化の無知）第 2 章で「データベース駆動設計と戦う」と述べたことの実践がここにあります。ドメインモデルは、自分がどこに保存されるか知りません。知る必要もありません。Order 型はデータベースのことを知りません。永続化の詳細は、ワークフローの「端」で処理されます。この設計により、ドメインロジックの変更が永続化コードに影響しません。逆に、データベースを変更してもドメインロジックは変わりません。NoSQLとRDBの選択本章では、NoSQL（ドキュメント DB）と RDB（リレーショナル DB）の両方のアプローチを解説しています。NoSQL: Aggregate をそのままドキュメントとして保存できます。DDD との相性が良いです。RDB: OR 型（enum）のマッピングが難しいです。インピーダンスミスマッチ（オブジェクトモデルとリレーショナルモデルの構造的な不一致）が発生します。-- OR型のRDBへのマッピング（判別カラム）CREATE TABLE order_lines (    quantity_type VARCHAR(10),  -- 'Unit' or 'Kilos'    unit_quantity INT NULL,    kilogram_quantity DECIMAL NULL);どちらも完璧ではありません。「永続化は境界で行う」という原則を守ることで、純粋なドメインロジックと不純な I/O 処理を分離しやすくなります。13. Evolving a Design and Keeping It Clean本章は、本書の締めくくりとして、設計の進化と保守性について解説します。要件は変わります。ドメインモデルも変わります。その変化にどう対応するでしょうか。変化への対応著者は、DDD は「一度きりの静的なプロセス」ではないと強調します。要件が変われば、まずドメインモデルを見直します。実装をパッチするのではなく、モデルから考え直します。変更例: 配送料の追加配送料計算をワークフローに組み込むには、新しいステップを追加します。まず、新しい型を定義します。なぜ既存の型を変更せず、新しい型を作るのか？型の名前がドキュメントになる: PricedOrderWithShipping という名前だけで、「価格計算済みで配送情報も持つ注文」だと分かります。段階を明示できる: PricedOrder と PricedOrderWithShipping は別の段階だと型で表現できます。コンパイラが変更を追跡する: 型が変わると、関連する箇所すべてでコンパイルエラーが発生します。見落としがありません。/// 配送方法#[derive(Debug, Clone, PartialEq, Eq)]pub enum ShippingMethod {    Standard,    Express,    Overnight,}/// 配送情報#[derive(Debug, Clone, PartialEq)]pub struct ShippingInfo {    pub method: ShippingMethod,    pub cost: Money,}/// 配送情報付きの価格計算済み注文 - 新しい型#[derive(Debug, Clone, PartialEq)]pub struct PricedOrderWithShipping {    pub order_id: OrderId,    pub items: Vec<PricedOrderLine>,    pub amount_to_bill: Money,    pub shipping_info: ShippingInfo,}次に、新しいパイプラインステップを定義します。/// 新しいパイプラインステップ: 配送情報を追加pub fn add_shipping_info(order: PricedOrder) -> PricedOrderWithShipping {    // シンプルなロジック: $100以上は送料無料    let shipping = if order.amount_to_bill.0 > 100.0 {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(0.0),        }    } else {        ShippingInfo {            method: ShippingMethod::Standard,            cost: Money(5.99),        }    };    PricedOrderWithShipping {        order_id: order.order_id,        items: order.items,        amount_to_bill: order.amount_to_bill,        shipping_info: shipping,    }}既存のコードを変更するのではなく、新しいステップを挿入します。validateOrder → priceOrder → addShippingInfo → acknowledgeOrder → createEventsこの「ステップの追加」というアプローチは、多くの機能追加に応用できます。ロギング、パフォーマンスメトリクス、認可チェック、監査。各ステップが独立していて、型が合っていれば、安全に追加・削除できます。VIP顧客の対応—入力をモデル化せよ著者は重要な指摘をしています。「ビジネスルールの出力（送料無料フラグ）ではなく、入力（VIP ステータス）をモデル化せよ」。なぜ「出力」ではなく「入力」をモデル化するのか？ルールが変わっても型は変わらない:「VIP は送料無料」→「VIP は送料 50%オフ」とルールが変わっても、CustomerStatus 型自体は変更不要です。関数だけ変えればよいです。原因をモデル化する:「送料無料かどうか」は結果（派生情報）です。原因は「VIP かどうか」です。原因をモデル化すれば、結果はいつでも計算できます。柔軟性が高い: VIP ステータスは送料以外にも使えます（優先サポート、限定商品へのアクセス等）。出力をハードコードすると、その柔軟性を失います。/// 顧客ステータス - ビジネスルールの「入力」をモデル化#[derive(Debug, Clone, PartialEq, Eq)]pub enum CustomerStatus {    Normal,    Vip,}/// 顧客#[derive(Debug, Clone, PartialEq, Eq)]pub struct Customer {    pub customer_id: String,    pub name: String,    pub status: CustomerStatus,}ビジネスルールは、入力（CustomerStatus）に基づいて決定を下します。/// 顧客ステータスに基づいて配送を計算pub fn calculate_shipping_for_customer(order: &OrderWithCustomer) -> ShippingInfo {    match order.customer.status {        CustomerStatus::Vip => ShippingInfo {            method: ShippingMethod::Express,            cost: Money(0.0), // VIPは無料のエクスプレス配送        },        CustomerStatus::Normal => {            if order.amount_to_bill.0 > 100.0 {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(0.0),                }            } else {                ShippingInfo {                    method: ShippingMethod::Standard,                    cost: Money(5.99),                }            }        }    }}ビジネスルールが変わっても（例: VIP は送料無料→VIP は送料 50%オフ）、calculate_shipping_for_customer 関数を変更するだけでよいです。ドメインモデル自体（CustomerStatus）は変更する必要がありません。型の変更と波及効果本章の核心は、「型の変更がコンパイラによって追跡される」ことです。// 変更前pub struct PricedOrder { /* ... */ }// 変更後（配送情報を追加）pub struct PricedOrderWithShipping {    // ...    pub shipping_info: ShippingInfo,  // 新しいフィールド}PricedOrder と PricedOrderWithShipping は異なる型です。PricedOrder を期待していたコードに PricedOrderWithShipping を渡すとコンパイルエラーになります。// これはコンパイルエラー！fn process(order: PricedOrder) { /* ... */ }process(priced_order_with_shipping); // 型が違う動的型付け言語では、このような変更は「実行時エラー」として発見されます。静的型付けでは、「コンパイル時エラー」として発見されます。コンパイラがリファクタリングアシスタントとして機能します。関数型 DDD の核心は、「型でドメインを表現する」ことです。AND 型（struct）と OR 型（enum）でドメインの構造を表現します。状態遷移を別の型として定義します。制約を Smart Constructor で強制します。不正な状態を表現不可能にします。フラグを立てるな。型を作れ。これらの原則は、F#でも Rust でも適用できます。おわりに読む前の三つの悩みへの回答「はじめに」で述べた 3 つの読む動機に、本書がどう応えたかを振り返ります。1. DDDを学び直す必要があった → 関数型の視点でDDDが腹落ちしたDDD の概念—Aggregate、Entity、Value Object—は、オブジェクト指向の文脈で説明されると抽象的に感じていました。本書は、これらを「型」という具体的な道具で表現する方法を示しました。Value Object は単なる newtype です。struct OrderId(String) と書けば、それが Value Object です。Aggregate の境界は、型の境界で表現できます。ValidatedOrder と UnvalidatedOrder が別の型なら、それが境界です。「なぜそう設計するのか」を言語化できるようになりました。「この型を別にするのは、状態が違うから」「この値を newtype にするのは、ドメイン上の意味が違うから」。経験則ではなく、型システムに基づいた説明ができます。2. Rustでドメインモデリングを実践したかった → F#の概念はRustで十分(ではないかもしれないが)表現できるF#にあって Rust にない機能—パイプライン演算子、computation expressions、Units of Measure—は、確かにあります。しかし、本書の核心である「Make Illegal States Unrepresentable」は、Rust で十分に実践できたと思います。むしろ、Rust の所有権システムは F#にない利点を提供します。状態遷移を「所有権の移動」として表現できます。validate(order: UnvalidatedOrder) -> ValidatedOrder と書けば、検証前の注文は使えなくなります。F#では GC があるため、古い変数への参照が残る可能性がありますが、Rust では型システムがそれを防ぎます。3. AIエージェント時代における型システムの意味を考えたかった → 型は「AIが破れない制約」本書を読む前は、「型があると AI の生成精度が上がる」という感覚がありましたが、理論的に説明できませんでした。本書を読んで、その理由が明確になりました。型で定義された制約は、物理的に破れません。NonEmptyList<OrderLine> と定義すれば、AI は空の注文を返すコードを書けません。コンパイルが通らないからです。「このフィールドは必須です」というコメントは無視できますが、型は無視できません。これは「AI が守るべきルール」ではなく「AI が破れない壁」です。読む前と読んだ後Before（読む前）DDD の設計はある種の経験則で判断していましたRust の Option/Result は便利ですが、関数型との繋がりを考えていませんでした型があると AI の精度が上がる「気がする」程度の理解でしたAfter（読んだ後）DDD の概念を型システムの言葉で説明できるようになりましたTransformation-Oriented Programming（元のオブジェクトを変更せず、新しい値を作る）という原則を内在化しました型を「人間のためのドキュメント」かつ「AI が破れない制約」として設計できるようになりましたTransformation-Oriented Programming関数型プログラミングを学んで獲得する最も重要な概念は、実はシンプルです。状態は例外的な存在であり、ほとんどの処理は状態を使うことなく記述できる。本書を読み終えて、この一文の重みを改めて感じています。私たちはプログラミングを学ぶとき、まず「変数に値を代入する」ことから始めます。x = 1。x = x + 1。状態を変更することが、プログラミングの基本だと教わります。しかし、よく考えてみると、ビジネスロジックの大半は「入力を受け取り、変換し、出力を返す」で書けます。注文明細から合計金額を計算する → 入力と出力だけ住所をパースする → 入力と出力だけ商品コードを検証する → 入力と出力だけ状態の変更は不要です。副作用も不要です。ほとんどのビジネスロジックは、数学の関数のように書けます。では、状態が必要になるのはいつでしょうか。データベースに保存するとき。外部 API を呼ぶとき。ファイルに書き込むとき。つまり、システムの境界を越えるときだけです。この気づきが、設計の発想を変えます。状態を「デフォルト」ではなく「例外」として扱います。しかし状態遷移は避けられないビジネスの世界は状態遷移で満ちています。注文は「未検証」から「検証済み」になります。カートは「空」から「商品あり」になります。申請は「提出済み」から「承認済み」になります。これは無視できません。問題は、この状態遷移をどう表現するかです。オブジェクト指向の答えは「オブジェクトが状態を持ち、メソッドが状態を変更する」でした。order.validate() を呼ぶと、order の内部状態が変わります。この設計は、状態の追跡を難しくします。order は今どの状態なのか。どの経路を通ってここに至ったのか。フラグの組み合わせは正しいのか。常に頭の中で管理し続けなければなりません。本書が示す答えは、Transformation-Oriented Programmingです。著者の言葉を借りれば、「ビジネスプロセスはデータを何らかの形で変換する—入力を受け取り、何かを行い、出力を返す」。重要なのは、元のオブジェクトを変更しないことです。UnvalidatedOrder という型があります。validate という関数を適用すると、ValidatedOrder という新しい値が生まれます。このとき、元の UnvalidatedOrder には一切触れません。新しい値を作るだけです。UnvalidatedOrder → validate → ValidatedOrder → price → PricedOrder状態を「変更」するのではなく、「入力を受け取り、何かを行い、出力を返す」。元のオブジェクトには触れません。これが本書の核心です。この発想の転換がもたらすものこのアプローチを採用すると、いくつかの問題が消えます。状態の追跡が不要になる。 ValidatedOrder を持っているなら、それは「検証済みの注文」です。フラグを見る必要がありません。型がすべてを語ります。並行処理が安全になる。 元のデータを変更しないから、競合が起きません。テストが簡単になる。 入力を与えて、出力を確認します。それだけです。デバッグが楽になる。 各ステップの入出力をログに残せば、全経路が追えます。そして何より、ビジネスの言葉とコードが一致する。「見積書」が「発注書」になります。Estimate が Order になります。ビジネスの人々が頭の中で考えているモデルが、そのままコードになります。型は思考のツールである本書を読む前、型システムは「コンパイラを満足させるためのもの」だと思っていました。IDE の補完が効きます。リファクタリングが安全になります。その程度の認識でした。本書を読んで、型は「思考のツール」だと認識を改めました。AND と OR という 2 つの組み合わせで、複雑なドメインを表現できます。struct は「これとこれが両方必要」、enum は「これかこれのどちらか」。この単純な組み合わせが、ドメインの構造を可視化します。型を書くことは、ドメインを理解することです。型を読むことは、ドメインを学ぶことです。少なくとも、私はそう感じるようになりました。型で「不可能」を作る本書の内容を 2026 年の視点で読み直して、最大の発見がありました。「Make Illegal States Unrepresentable（不正な状態を表現不可能にする）」—この原則は、人間の開発者のミスを防ぐためのものとして紹介されています。しかし 2026 年現在、同じ原則がAIの出力を自動検証するフィルタとして機能しています。型で「不可能」を定義すると、AI が生成したコードのうち、その制約に違反するものはコンパイル時に除外されます。NonEmptyList<OrderLine> と定義すれば、AI が空の注文を返すコードを書いてもコンパイルエラーで検出されるVerifiedEmailAddress を要求すれば、AI が未検証メールへの送信を実装してもコンパイルが通らないUnvalidatedOrder → ValidatedOrder という型シグネチャがあれば、AI が検証をスキップするコードはコンパイルエラーになるこれは「AI に正しいコードを書かせる」のではなく、「AI が書いた誤ったコードを検出する」メカニズムです。AI の精度向上ではなく、フィルタリング機構として機能します。先日、Claude Code に「Order を作成する関数を書いて」と指示しました。生成されたコードは Vec<OrderLine> を返していました。しかし私のコードベースでは NonEmptyList<OrderLine> を使っています。コンパイルエラーが発生し、AI は「空の注文」を作るコードを出力しましたが、それが本番に混入することはありませんでした。一方、別のプロジェクトでは「このフィールドは必須」とコメントに書いただけでした。AI はそのコメントを無視して Option を返すコードを生成し、後から問題が発覚しました。型で定義された制約は、コンパイル時に検証されます。コメントは検証されません。この違いが重要です。「何を作るか」を決める能力本書の第 1 章で、著者は「開発者の仕事はコードを書くことではなく、ソフトウェアを通じて問題を解決すること」と述べています。2018 年に書かれたこの言葉は、2026 年の今、さらに重みを増しています。「何を作るか」という問いを分解してみます。ビジネス要件の理解: ドメインエキスパートとの対話、暗黙知の引き出し技術的制約の把握: 既存システムとの整合性、パフォーマンス要件、チームのスキルセット両者のトレードオフ判断: 「正解がない」状況での意思決定現時点で AI が得意なのは 2 番目です。ドキュメントやコードベースを読み、技術的な制約を分析できます。一方、1 番目と 3 番目は人間の仕事です。ドメインエキスパートとの対話で暗黙知を引き出すこと、そして「どちらも正しい」状況でトレードオフを判断すること。これらは AI に委譲できません。この構造を理解すれば、「人間の仕事」を「暗黙知の言語化」と「トレードオフ判断」に絞り込めます。本書で学んだドメインモデリングの技術は、まさにこの「暗黙知の言語化」を支援するものです。型で表現されたドメインモデルがあれば、AI は「どう作るか」を高い精度で実行できます。ドメイン駆動設計をはじめよう ―ソフトウェアの実装と事業戦略を結びつける実践技法作者:Vlad Khononovオーム社Amazon働き方の逆転—AIエージェント時代の開発スタイル本書を読みながら、自分の働き方が根本的に変わったことを実感しました。以前のモデルでは、人間がコードを書き、AI は相談相手でした。Stack Overflow の代わり、ドキュメント検索の高速化。補助的な存在です。現在のモデルでは、AI が運転席に座り、人間は助手席でナビゲーションをしています。AI にプランを練らせ、レビューし、実装させ、またレビューする。この流れが定着しました。AI は「分身」的な存在になりました。明確な指示とコンテキストを与えれば、疲労知らずで作業してくれる相棒です。Claude Opus 4.5 以降、この感覚は決定的になりました。プログラミングのシンタックスを書く機会は明らかに減りました。では、ソフトウェアエンジニアの役割はどう変化したのでしょうか。1. アーキテクチャの指針決定AI は「どう作るか」を実行できますが、「なぜそう作るか」は決められません。Bounded Context の境界をどこに引くか、技術選定のトレードオフ、パフォーマンスと保守性のバランス。これらは人間が判断します。2. コードベースから読み取れないコンテキストの整理・提供AI はコードに書かれていないことを知りません。なぜこの設計にしたか、本番環境でのみ発生する問題、チームの暗黙のコーディング規約、ビジネス上の制約。これらを言語化し、CLAUDE.md やコメントに落とし込む能力が求められます。3. 期待する挙動を自動・継続的に検証する枠組みの整備AI が書いたコードは「動く」かもしれませんが、「正しい」とは限りません。型による制約、プロパティベーステスト、E2E テスト、本番監視。これらの枠組みを整備し、AI の出力を検証し続けるのは人間の仕事です。本書の「Make Illegal States Unrepresentable」は、まさにこの 3 番目の観点で価値を発揮します。型で制約を定義すれば、AI の出力を自動的に検証できます。コンパイルが通れば、少なくとも型レベルの正しさは保証されます。コードを手で書くという作業は、実は思考の外在化プロセスでもありました。書きながら考えていた。この機会が減ったとき、思考の質をどう担保するか。正直、まだ答えが出ていません。本書のようなドメインモデリングの訓練はその答えの 1 つかもしれませんが、それで十分かどうかは分かりません。分からないまま、AI と協業し続けています。Architecture Modernization との接続本書を読みながら、Nick Tune 著『Architecture Modernization』の内容が何度も頭をよぎりました。現在、この本の翻訳に携わっています。Architecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazon『Domain Modeling Made Functional』は「新規開発」の文脈で DDD を説明しています。しかし現実の多くのプロジェクトは「既存システムの改善」です。レガシーシステムをどう分析し、背景情報からBounded Context をどう切り出し、段階的にモダナイズしていくか。『Architecture Modernization』はまさにその部分を扱っています。翻訳作業を通じて、共感できる内容が多くありました。特に「既存システムの暗黙知をどう言語化するか」という問題意識は、本書の「ドメインエキスパートとの対話」と通じるものがあります。AI エージェント時代において、この問題はさらに重要になっています。AI は「新しいコードを書く」ことは得意ですが、「既存システムの文脈を理解する」ことは苦手です。10 年前の設計判断の背景、当時の技術的制約、組織の歴史。これらを言語化し、モダナイゼーションの方針を決めるのは、依然として人間の仕事です。本書を読んで「関数型 DDD」に興味を持った方には、『Architecture Modernization』も勧めたいです。新規開発だけでなく、既存システムをどう改善するか。両方の視点を持つことで、設計の引き出しが増えます。最後に2018 年に出版された本書を、2026 年に読む価値はあったでしょうか。かなり、Yes です。ただ、正直なところ、本書の「すべて」を実践できる自信はありません。Smart Constructor を徹底すると言いながら、明日には String を直接使っているかもしれません。型で不可能を作るのは、思っているより面倒くさい作業です。締め切りに追われると、つい妥協してしまう。それでも、本書を読んだことで「何かに気づいた」感覚はあります。うまく言葉にできませんが、型を書くときの解像度が変わった気がします。Option を見たとき、「本当にこれは省略可能なのか？」と問い直すようになりました。冒頭で触れた『Architecture Modernization』の翻訳作業。本書を再読したことで、「Bounded Context」「Aggregate」といった用語を訳すとき、以前より自信を持てるようになりました。言葉の背後にある設計思想を、型という道具で理解したからです。翻訳は続いています。この感覚が正しいのかどうかは、実務で検証していくしかありません。関数型 DDD は、特定の言語やパラダイムに縛られません。F#で書かれた本書の概念は、Rust でも実践できます。そして、人間と AI が協業する時代において、「不可能を型で定義する」技術の価値はますます高まっていく—たぶん。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク]]></title>
            <link>https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</link>
            <guid isPermaLink="false">https://sreake.com/blog/learn-microservices-network-failure-and-risk-with-chaos-mesh/</guid>
            <pubDate>Tue, 20 Jan 2026 01:05:03 GMT</pubDate>
            <content:encoded><![CDATA[はじめに 「なんか遅い」「たまにエラーが出る」――マイクロサービスのシステムでこんな報告を受けたとき、皆さんはどこから調べ始めますか？ Sreake事業部インターン生の小林です。2025年11月-12月の間インターンに参 […]The post Chaos Meshで学ぶマイクロサービスのネットワーク障害と見えないリスク first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[おい、頑張るなら組織と踊れ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/19/090119</guid>
            <pubDate>Mon, 19 Jan 2026 00:01:19 GMT</pubDate>
            <content:encoded><![CDATA[はじめに「おい、辞めるな」で辞めないことを選んだ。syu-m-5151.hatenablog.com「おい、辞めないなら頑張れ」で頑張り方を学んだ。syu-m-5151.hatenablog.com見せろ。対話しろ。上司を勝たせろ。スポンサーを作れ。そう書いた。で、やってみてどうだった。正直に言う。私はうまくいかなかった。見せているつもりだった。対話しているつもりだった。上司を勝たせようとしていた。でも、空回りしていた。なぜか。組織の力学を理解していなかったからだ。いや、もっと正確に言おう。理解しようとしなかった。組織の力学——いわゆる「政治」——を、私は嫌悪していた。「実力で勝負したい」「政治なんかに関わりたくない」——そう思っていた。技術的な正しさを盾に、人間関係の機微を「非論理的」と切り捨てていた。以前、「正義のエンジニアという幻想」という記事を書いた。syu-m-5151.hatenablog.comあの記事で書いたことは、今でも私の中に残っている。媚びないことと無礼であることの区別もつかないまま、技術的優位性を振りかざしていた——そんな恥ずかしい過去を、私は持っている。今回は、その続きを書く。組織の力学について。私が嫌悪していたもの。でも、理解しなければ成果を出せないもの。そして、したたかに生きるということについて。先に結論を言っておく。理解することと、加担することは違う。そして、政治をやっている人は「汚い大人」ではない。泥臭く仕事を通そうとしているだけだ。正直に告白する。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分が知りたかったことを書く。飲み屋でそれを喋る。それがこの「おい、」シリーズの趣旨だ。おい、頑張るなら組織と踊れ。——と書いて、自分でも苦い顔をしている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。私は「正義のエンジニア」だった最初に告白しておく。私はかつて、自分の技術思想とキャリア戦略が100%正しいと信じて疑わなかった。そして、それを受け入れない企業、同僚たちが100%間違っていると本気で思っていた。今思えば、それはソフトウェアエンジニアという職業に就いた多くの若い人が陥る、ある種の思春期的な錯覚だったと思う。「なぜこんな非効率的な実装をするんですか？」「技術的にはこっちの方が正しいんですけどね」「政治的な理由で技術選定するなんて、エンジニアリングの敗北だ」そんな言葉を、私は何度思って何度口にしたことだろう。ある会議で、私は技術的に正しい提案をした。データに基づいていた。論理的だった。反論の余地がないと思っていた。却下された。理由は曖昧だった。「今はタイミングが悪い」「もう少し検討が必要」。でも、本当の理由は別にあった。私は後から知った。あの提案は、ある部門の利害と衝突していた。その部門のキーパーソンに、事前に話を通していなかった。だから、会議の場で潰された。私はその事実を伝えられた時に密かに怒った。「政治で正しい提案が潰されるなんて、おかしい」と。でも、冷静に考えると、私の方がおかしかった。技術的に正しいことと、組織で通ることは、別の問題だ。私はその区別ができていなかった。これは「誰に話を通すか」の問題だった。でも、組織の力学を理解していないことは、別の形でも現れた。あるプロジェクトで、私は黙々と成果を出していた。技術的な課題を解決し、納期を守り、品質を担保した。「これだけやれば評価されるだろう」と思っていた。評価面談で、上司はこう言った。「〇〇さんの貢献は分かっているんだけど、他のマネージャーに説明しにくいんだよね」。私は意味が分からなかった。成果を出しているのに、なぜ説明しにくいのか。後から分かった。私の仕事は「見えなかった」のだ。他のマネージャーは、私が何をしているか知らなかった。評価会議で私の名前が挙がっても、「誰？」という反応だった。私の上司は、私を推そうにも、材料がなかった。見えない仕事は、存在しないのと同じ——「おい、辞めないなら頑張れ」で書いたことだ。でも、それは「見せる」だけでは解決しない。誰に見せるか。どのタイミングで見せるか。どの文脈で見せるか。それを間違えると、見せても意味がない。私は「政治」を嫌悪していた。でも、その嫌悪が、私自身の足を引っ張っていた。媚びないと無礼を混同していたここで、痛い告白をする。「私は媚びない」——それが私のプライドだった。しかし「媚びない」と「無礼」は違う。私は単に無礼だった。コードレビューで、つい正論を優先してしまう癖があった。「このコード、正直ひどくないですか？全部書き直した方が早いです」——そんなコメントを書いていた。ある日、シニアエンジニアが個別に連絡をくれた。「君の指摘は技術的には正しい。でも、そのコメントを見た人がどう感じるか考えたことある？彼は他のタスクも抱えながら、期限に間に合わせようと必死だった。君のコメントは、その努力を全否定している」その言葉にハッとした。私は技術的な正しさばかりを見て、人の気持ちを踏みにじっていたのだ。別の機会には、マネージャーが1on1で厳しい指摘をした。「君は優秀だ。でも、チームメンバーが君を避け始めている。それでいいの？技術力があっても、一人では何も作れないよ」媚びないことと、相手を尊重することは両立する。でも当時の私にはその区別がつかなかった。率直であることと配慮がないことを混同していた。技術的な正しさを盾に、人としての礼儀を忘れていた。私は様々な言い訳を用意していた。「エンジニアは成果で評価されるべきだから人間関係は二の次」「技術的に正しいことが最優先だから言い方なんて些細な問題」「実力があれば多少の態度の悪さは許される」これらはすべて、自分の社会性の欠如を正当化するための、頭の悪い言い訳だった。まるで反抗期の中学生が「大人は汚い」と言い訳するように、私は「技術的正しさ」を盾に、自分の未熟さを隠していたのだ。転機は、年次が上がって後輩ができたときに訪れた。私の何気ない「それは違うよ」という一言で、新卒エンジニアが完全に萎縮してしまった。その後、彼は私に質問することを避けるようになり、分からないことを抱え込むように。私は、かつて自分が嫌っていた「怖い先輩」になっていたのだ。このとき、ようやく理解した。正しいことを、正しい方法で伝えられなければ、それはただの暴力だ。技術力は重要だが、それをどう使うかはもっと重要。正しいことを言っているつもりで、実際には相手の立場に立てていなかっただけだった。そういう時代もあったでよいここで、過去の自分との向き合い方について書いておく。ある時期、私は過去の失言や態度を思い出しては、布団の中で悶えていた。「あの時、なぜあんなことを言ったんだ」「もっと早く気づいていれば」——後悔の反芻は止まらなかった。コードレビューで人を傷つけた記憶。会議で空気を凍らせた記憶。「正しいことを言っているのに、なぜ分かってもらえないんだ」と憤っていた記憶。思い出すたびに、顔が熱くなった。でも、ある時気づいた。過去を責めても、過去は変わらない。変えられるのは、これからだけだ。だから、こう割り切ることにした。「過去はすべて正しかった」と。誤解しないでほしい。過去の行動が道徳的に正しかったと言いたいわけではない。あの無礼な態度は、やはり間違っていた。でも、あの経験があったから、今の自分がいる。痛い目に遭わなければ、私は変われなかった。あの失敗がなければ、この記事を書くこともなかった。過去を否定し続けると、エネルギーが過去に吸い取られる。後悔に費やす時間は、未来への投資に使えない。「あの時こうすればよかった」と100回考えるより、「これからどうするか」を1回考える方が、よほど生産的だ。過去を受け入れろ。そして、これからの人生に全力で取り組め。私は自分の未熟さを認めた。媚びないことと無礼の区別がついていなかったことを認めた。では、これからどうするか。答えは明確だった。組織の現実を、ちゃんと見ることだ。私が見ようとしなかったもの。「政治」と呼んで嫌悪していたもの。でも、理解しなければ前に進めないもの。——組織の力学について、正面から向き合う時が来た。組織には「裏の顔」があるここで、現実を直視しよう。組織には、公式なルールと非公式な力学の2つが常に存在している。公式なルールは分かりやすい。組織図、職務権限、承認フロー、評価制度。これらは明文化されていて、誰でもアクセスできる。でも、それだけで組織が動いているわけではない。非公式な力学とは、「正式な手続きには定められていないが、意思決定や資源配分に影響を与える行動」のことだ。根回し、人脈、暗黙の了解、派閥、影響力のある人物——そういうものだ。私はこれを「汚いもの」だと思っていた。でも、違った。彼らは汚いのではなく、泥臭いだけだった。非公式な力学は「善悪」ではなく「手段」だ。根回しは悪いことか。場合による。自分の私利私欲のためなら問題だ。でも、良いプロジェクトをスムーズに通すためなら、むしろ必要なことだ。関係者の懸念を事前に把握し、対処しておく。それは「政治」ではなく「配慮」とも呼べる。私が嫌悪していたのは、「非公式な力学」そのものではなかった。それを私利私欲のために使う人間だった。でも、手段と目的を混同していた。手段自体は中立だ。それをどう使うかが問題なのだ。ここで1つ、大事なことを言っておく。非公式な力学を理解することと、それに迎合することは違う。力学を理解した上で、「自分はこの手段は使わない」と決めてもいい。でも、理解せずに無視するのは、ただの怠慢だ。敵を知らずに戦っているようなものだ。私は長い間、「政治を理解する」こと自体を拒否していた。理解したら、自分も「あっち側」になる気がした。でも、それは間違いだった。理解することと、加担することは違う。そして、「あっち側」の人たちは、別に悪者ではなかった。ただ、泥臭く仕事を通そうとしていただけだった。理解した上で、どう振る舞うかは自分で決められる。組織図を信じるな次に、私が痛い目を見た話をする。組織図に描かれた権限構造と、実際に物事を動かせる力は違う。あるプロジェクトで、私は承認を得るために、組織図上の決裁者に話を持っていった。正式なルートだ。決裁者は「いいんじゃない」と言った。私は安心した。プロジェクトは頓挫した。何が起きたか。決裁者は「いいんじゃない」と言ったが、実際に動く現場のキーパーソンは別にいた。その人は私の提案に反対だった。決裁者が「いい」と言っても、現場が動かなければ、何も進まない。私は組織図を信じすぎていた。「誰が決裁権を持っているか」と「誰が実際に物事を動かせるか」は違う。そして後から気づいたことがある。同じ提案でも、事前に相談していれば通っていた可能性が高い。あのキーパーソンに、会議の前に一度話を聞きに行っていたらどうだっただろう。「こういうことを考えているんですが、懸念点はありますか？」と。相手の観点や懸念が事前に分かれば、提案に反映できる。相手も「聞いてもらった」という感覚がある。「聞かされていない」は、「間違っている」より強い反対理由になる。内容の良し悪しではない。プロセスの問題だ。これは単なる感情論ではない。組織心理学では「手続き的公正」と呼ばれる概念がある。人は、結果だけでなく、そこに至るプロセスが公正かどうかを重視する。自分の意見を聞いてもらえた、自分も関与できたという感覚があれば、たとえ結果が自分の望み通りでなくても、受け入れやすくなる。逆に、プロセスから排除されたと感じると、結果が正しくても反発する。私が会議で潰された提案は、まさにこれだった。内容は正しかった。でも、関係者は「自分は聞かれていない」と感じた。関係者に事前の相談なく、いきなり会議の場で出したことが、「あなたの意見は聞く必要がない」というメッセージになっていた。それだけで、反対する十分な理由になった。技術的に正しいかどうかと、組織で通るかどうかは、別の問題だ。そして、事前に挨拶して、相談して、懸念を聞いておく——それだけで結果が変わることが、驚くほど多い。ここで、少し視点を変えた話をする。私は下っ端として組織図に騙されてきた。でも、ある時気づいた。上に立つ人間こそ、この罠にはまりやすいのだと。下っ端の経験が少ない若いCEOやCTOが率いる組織を見てきた。彼らは往々にして、表側の組織図ばかり意識する。そして、裏側の関係性——長年かけて築かれた非公式なネットワーク——を軽視して、ドラスティックな組織変更をする。「この部署とこの部署を統合しよう」「この人をあのチームに異動させよう」——組織図の上では合理的に見える。でも、その変更が裏のネットワークをぐちゃぐちゃにすることがある。誰と誰が信頼関係を築いていたか。どのルートで情報が流れていたか。誰が実質的なキーパーソンだったか。それを無視して箱だけ動かす。若い頃の私は、そういうリーダーを「革新的だ」「スピード感がある」と思っていた。古い慣習を壊して、新しい組織を作る。カッコいいと思っていた。大人になった今は、違う見え方をする。成果を出すために、下の人間が苦労している。壊された関係性を、現場が必死で繋ぎ直している。組織図の上では「改革成功」に見えても、実際は現場の努力で何とか回っているだけ。これは「組織図を信じるな」の裏返しだ。組織図だけを見て動く危険は、下っ端だけの問題ではない。リーダーが組織図だけを見て動くと、現場が壊れる。私が組織の裏側を理解しようとするようになったのは、こういう経験も影響している。組織図の裏にあるものを無視すると、どうなるか。それを見てきたからだ。では、「組織図の裏にあるもの」とは、具体的に何か。私はそれを「影のネットワーク」と呼んでいる。かっこいい名前をつけたいわけではない。組織図には描かれないが、確実に存在するもの。それを言語化するために、この言葉を使っている。そしてその核心は、役職とは別に存在する権力だ。権力とは、役職に基づく権限だけではない。反対や抵抗を乗り越えて物事を実現する力。人を惹きつけ、巻き込む力。意思決定に実質的な影響を与える力。これらは、役職とは別に存在する。例えば、古株のベテラン社員。役職は高くないが、社内の歴史を全部知っている。誰と誰が仲が悪いか、過去にどんなプロジェクトが失敗したか、どの部署が何を嫌がるか。その人を味方につけると物事がスムーズに進む。敵に回すと、見えない抵抗にあう。例えば、経営者の信頼が厚い若手。役職は低いが、経営者に直接話ができる。その人の意見は、なぜか上まで届く。私は、この「影のネットワーク」を読めていなかった。組織図だけを見て、「この人に話を通せばOK」と思っていた。でも、組織図の裏には、別のネットワークがあった。なぜ「影のネットワーク」が存在するのか。理由は単純だ。組織図は「権限」を示すが、「実行力」を示さない。決裁権を持つ人が「やれ」と言っても、実際に手を動かす人が動かなければ、何も起きない。そして、実際に手を動かす人を動かせるのは、必ずしも決裁権を持つ人ではない。組織が大きくなるほど、この乖離は広がる。決裁者は現場から遠くなり、現場の信頼関係は決裁者の目に見えなくなる。結果として、「承認されたのに進まない」「反対されていないのに協力が得られない」という現象が起きる。これは個人の悪意ではない。権限と実行力が分離している構造の問題だ。組織図の裏にある「影のネットワーク」を読み解け。どの提案に誰が反発するか。誰を味方につければ障壁を突破できるか。情報がどのルートで流れるか。これが見えるようになると、立ち回り方が変わる。ここで、私が学んだ具体的な方法を書いておく。1. 会議での反応を観察する誰かが発言したとき、他の人の表情を見る。賛成しているのか、本音では反対なのか、無関心なのか。言葉ではなく、表情や態度に本音が出る。2. 「あの人に聞いてみたら」の連鎖を追う何か新しいことを始めようとしたとき、「あの人に聞いてみたら」と言われる人がいる。その人が、実質的なキーパーソンだ。組織図上の役職とは関係ない。3. 過去の意思決定を遡る大きな決定が下されたとき、「誰がどの段階で関わっていたか」を調べる。公式の決裁者だけでなく、その前に相談されていた人。その人が、影響力を持っている。4. ランチや雑談の相手を観察する誰と誰がよく一緒にいるか。情報は公式のルートだけでなく、非公式の人間関係を通じて流れる。ここまでが「見る」段階だ。では、見えたものをどう使うか。観察した後にどうするか観察だけでは意味がない。観察した情報を、行動に変える必要がある。キーパーソンが分かったら、提案の前に一度相談に行く。反対しそうな人が分かったら、その人の懸念を先回りして潰す。情報のルートが分かったら、そのルートに自分の情報を流す。最初は気が重い。「なぜこんな面倒なことを」と思う。でも、一度やってみると、驚くほど物事がスムーズに進む。私も最初は抵抗があった。でも、「正しい提案が政治で潰される」ことに比べれば、事前の相談なんて些細な手間だと気づいた。私が変わるまでの話ここまで読んで、「分かったけど、やっぱり嫌だ」と思う人がいるだろう。「政治なんかしたくない」「実力で評価されるべきだ」「こんなことに時間を使いたくない」。その気持ちは分かる。私もそうだった。そして正直に言えば、今でも完全には割り切れていない。私が組織の力学をどう受け止めてきたか、正直に書く。最初は、拒絶していた。長い間、ずっとそうだった。「実力で評価されるべきだ」「政治をやる奴は汚い大人だ」「自分はそういうことはしない」。そう思っていた。この時期は、現実とのギャップに苦しんだ。「なんで自分より実力のないあいつが評価されるんだ」「この会社はおかしい」。怒りや失望があった。でも、状況は変わらなかった。居酒屋で同僚と愚痴を言っていた。「あいつは政治がうまいだけだ」「実力で勝負しろよ」。言うたびに少し楽になった。でも、翌日も同じ状況が続いた。全ての原因を外部に求めていた。自分が提案した新技術が却下されれば「老害が変化を恐れている」と憤り、レガシーコードの改修を任されれば「俺の才能の無駄遣い」と不満を漏らし、ドキュメント作成を頼まれれば「エンジニアの仕事じゃない」と文句を言う。でも振り返ってみれば明らかだ。問題は私自身にあった。技術的な正しさだけを追求し、ビジネス的な制約や組織の事情を理解しようとしなかった。転機があった。尊敬していた先輩が、根回しをしているのを見た。「あの人も政治をやるのか」と最初は失望した。でも、よく見ると違った。先輩は、良いプロジェクトを通すために、関係者の懸念を事前に聞いて回っていた。それは「政治」というより「配慮」だった。「政治」と「配慮」の境界は曖昧だ。私が嫌悪していた「政治」の中には、実は「配慮」も含まれていた。それに気づいてから、少し楽になった。組織の力学を「存在するもの」として認められるようになった。好き嫌いを超えて、「まあ、そういうものだよな」と思えるようになった。過度に振り回されない心理的安定が生まれた。今はどうか。正直に言う。私はまだ、完全には割り切れていない。根回しをすることに、今でも抵抗がある。「これは本当に必要なのか」「実力で勝負すべきじゃないのか」と思う。でも、必要な場面では、やるようになった。割り切れないまま、やっている。——と書いて、立ち止まる。私と同じように変われ、と言いたいわけではない。どこまで受け入れるかは、自分で決めていい。「存在は認めるけど、自分はやらない」でもいい。「存在を認めることすら嫌だ」なら、別の環境を探してもいい。ただ、組織の力学を拒絶し続けていると苦しい。現実と理想のギャップに消耗し続ける。だから、少なくとも「存在を認める」ところまでは進んだ方が、楽になる。その先は、自分で決めればいい。譲れないもののために、譲るものを決める「存在を認める」ところまで進んだとする。でも、それだけでは足りない。認めた上で、どう振る舞うか。全部受け入れるのか。全部拒否するのか。——どちらも違う。私が辿り着いた答えは、もっと戦略的なものだった。ここで、私が学んだ重要なことを書く。本質を守るために、形式では妥協する。やがて私は真剣に考えるようになった。自分が本当に譲れないものは何か？見極める基準は1つ。「あったらいいな」は捨てろ。「なくなったら壊れる」だけを守れ。私にとって譲れないのは3つだった。1つ目は技術的な誠実さ。嘘はつかない、質の低いコードは書かない。これを失ったら、自分を信頼できなくなる。2つ目はユーザーファースト。エンドユーザーの利益を最優先する。これを失ったら、仕事の意味を感じられなくなる。3つ目は継続的な学習。常に新しいことを学び続ける。これを失ったら、市場価値が消える。これ以外は、状況に応じて柔軟に対応することにした。表現方法やタイミングを妥協しても、私は壊れない。だから手放せる。表現方法では本音を建前でオブラートに包むようになった。タイミングも最適な時期を待つように。プロセスでは目的のためなら遠回りも受け入れ、形式的には無駄に見える会議や書類も必要なら対応するようになった。全てを守ろうとすると、全てを失う。なぜか。理由は単純だ。妥協できない領域が増えるほど、交渉の余地は減る。交渉の余地が減るほど、衝突は増える。衝突が増えるほど、消耗する。消耗すると、本当に守りたかったものまで守るエネルギーがなくなる。私は以前、表現方法でも、タイミングでも、プロセスでも、一切妥協しなかった。「正しいことを、正しいタイミングで、正しい方法で言う」——それが自分の信念だと思っていた。結果、毎回衝突し、毎回消耗し、最終的には技術的な誠実さすら保てなくなった。疲れ果てて、どうでもよくなったのだ。だから、何を守り、何を手放すかを決める。これが大人の戦略だ。以前は、「妥協＝敗北」だと思っていた。でも違った。戦略的な妥協は、本質を守るための手段だ。形式で妥協し、本質を守る。それは負けではない。むしろ、本当に大事なもののために、大事でないものを手放す勇気だ。したたかに生きる戦略「譲れないものを守り、それ以外では妥協する」——それは分かった。でも、正直に言えば、それだけでは物足りない。守りに入っているだけだ。もっと攻めの姿勢で、組織を「利用」することはできないのか。——そう考えるようになった。ここで、もう一歩踏み込んだ話をする。技術は手段であって目的ではない——組織から見れば、そうだ。でも正直に言えば、私自身は技術的な興味に駆動されている。新しい技術を学ぶことが楽しいし、エレガントなコードを書くことに喜びを感じる。ビジネス価値なんてどうでもよくて、ただ面白い技術を触っていたいだけ、というのが本音だ。でも、お金をもらって仕事をする以上、建前上それが主目的とは言いづらい。だからこそ「したたかにやろうぜ」という考え方が大切なのだ。つまり、組織が求める「成果」という枠組みを利用して、自分の技術的好奇心を満たすということ。表向きは「ビジネス価値の創出」を掲げながら、実際には「面白い技術で遊ぶ」ための正当性を確保する。例えば、「パフォーマンス改善」という大義名分のもとで、最新のフレームワークを導入する。「開発効率の向上」という建前で、面白そうなツールチェーンを構築する。「技術的負債の解消」という錦の御旗を掲げて、自分が書きたいようにコードを書き直す。重要なのは、これらの建前が単なる口実ではなく、実際に価値を生み出すことだ。新技術で遊びながら、本当にパフォーマンスを改善する。好きなツールを使いながら、実際に開発効率を上げる。コードを書き直しながら、本当に保守性を向上させる。ここで正直に告白しておく。私はこの戦略で失敗したことがある。「開発効率の向上」を名目に、面白そうなビルドツールを導入した。確かに面白かった。でも、チームの学習コストを甘く見積もっていた。結果として、効率は上がるどころか下がった。建前が嘘になった瞬間、「あいつは自分のことしか考えていない」という評価が下された。信頼を取り戻すのに、かなりの時間がかかった。したたかさの前提は、建前が本当に価値を生み出すことだ。建前が嘘になった瞬間、したたかさは不誠実に変わる。自分が楽しいかどうかではなく、本当に成果が出るかどうか。その見極めを間違えると、戦略は破綻する。「プロフェッショナルとして責任を果たします」と胸を張りながら、心の中では「やった！これで堂々とRustが書ける！」と小躍りする。この二重構造こそが、エンジニアとしてのしたたかさだ。ただし、小躍りする前に、本当に成果が出るかを冷静に見極めること。それを怠ると、私のように痛い目を見る。組織は成果を得て満足し、私たちは技術的満足を得る。Win-Winの関係を作り出すこと。それは決して不誠実ではなく、むしろ異なる価値観を持つ者同士が、お互いの利益を最大化する賢明な戦略なのだ。組織をハックしろ。建前で成果を出し、本音で技術を楽しめ。影響力は才能ではなくスキルだここまで「したたかにやれ」と書いてきた。「でも、自分は政治が苦手だ」という人がいるだろう。分かる。私もそうだった。というか、今でもそうだ。人の顔色を読むのが苦手だし、根回しは面倒くさいと思っている。でも、安心してほしい。影響力は先天的な才能ではなく、後天的に磨けるスキルだ。私も苦手だった。今でも得意とは言えない。でも、意識して練習することで、少しずつマシになった。組織における対人影響力は、5つの能力で構成されている。これらは「観察→洞察→共感→表現→一貫性」というプロセスで連鎖する。1. 観察——表面を見る最初の能力は観察だ。目の前で起きていることを正確に捉える。何を観察するか。言葉——誰が何を言ったか。態度——表情、姿勢、声のトーン。関係——誰と誰が近いか、誰が誰を避けているか。反応——ある発言に対して、他の人がどう反応したか。観察は受動的な行為に見えるが、意識しないとできない。会議で自分の発言に集中していると、他の人の反応を見落とす。発言を減らし、観察を増やす——これだけで得られる情報量は変わる。2. 洞察——本質を見抜く観察の次は洞察だ。表面の情報から、見えないものを推測する。洞察とは何か。動機を読む——この人は何を求めているのか、何を恐れているのか。構造を読む——この組織で、誰が実質的な力を持っているのか。文脈を読む——この議論は、どんな歴史の上に成り立っているのか。観察が「何が起きているか」を捉えるなら、洞察は「なぜ起きているか」を捉える。同じ事象を見ても、洞察の深さで解釈は変わる。表面的な反対意見の裏に、本当の懸念が隠れていることがある。3. 共感——相手の立場に立つ洞察の次は共感だ。相手の世界を、相手の視点から理解する。共感は「同意」ではない。相手の意見に賛成しなくても、相手がなぜそう考えるかを理解することはできる。「この人の立場なら、確かにそう思うだろう」——その理解があれば、対立は減る。エンジニアは共感を軽視しがちだ。論理が正しければ、相手の感情は関係ないと思っている。しかし、人は論理だけでは動かない。自分の立場を理解してくれていると感じたとき、初めて耳を傾ける。4. 表現——相手に響かせる共感の次は表現だ。自分の考えを、相手に届く形で伝える。表現の本質は「相手に合わせる」ことだ。論理で動く人には論理を。感情で動く人には感情を。利害で動く人には利害を。同じ提案でも、切り口を変えれば響き方が変わる。「伝える」と「伝わる」は違う。自分が言いたいことを言うのは「伝える」。相手が受け取れる形で届けるのが「伝わる」。影響力とは「伝わる」力だ。5. 一貫性——信頼を積む最後は一貫性だ。これが他の4つを支える土台になる。一貫性とは何か。言ったことを実行する。約束を守る。嘘をつかない。単純だが、最も難しい。なぜ難しいか。一貫性を保つには、「できない約束をしない」という自制が必要だからだ。期待に応えたくて、つい「やります」と言ってしまう。しかし、守れない約束は信頼を削る。「できません」と言える人の方が、長期的には信頼される。一貫性がなければ、観察も洞察も共感も表現も、すべて無駄になる。「あの人の言うことは当てにならない」——そう思われた瞬間、影響力は消える。これら5つは、すべて後天的に磨けるスキルだ。生まれつきの才能ではない。ただし、順番がある。土台となる「一貫性」がなければ、他の4つは機能しない。まず信頼を築き、その上に観察・洞察・共感・表現を乗せる。「専門性」と「人望」が最強のカードだここまで「組織の力学を理解しろ」「影響力を磨け」と書いてきた。でも、ここで安心してほしいことがある。最も持続する影響力は「専門性」と「人望」から生まれる。なぜそう言えるのか。少し整理してみる。人が他人を動かす力——影響力には、いくつかの種類がある。ソフトウェアエンジニアの現場で見かける例で説明する。報酬で動かす場合がある。「このリファクタリングを完了させたら、次のスプリントで好きな技術調査の時間をあげる」。評価で動かすこともある。「このタスクを断ったら、次の評価に響くよ」。役職で動かすパターンもある。「テックリードの判断だから、この設計で行く」。データで動かすこともできる。「ベンチマークの結果、この実装の方が30%速い」。そして、専門性で動かす場合がある。「Kubernetesのことなら〇〇さんに聞けば間違いない」。人望で動かす場合もある。「あの人が言うなら、きっと理由があるはず」。このうち、専門性と人望が最も強い。なぜか。この2つは、相手が「自分から納得して動く」ときに生じるからだ。報酬・評価・役職で動かす場合、相手は「仕方なく」動いている。上司が変わったり、評価制度が変わったりすれば、その影響力は消える。「テックリードが言うから従う」で動いていたチームは、テックリードがいなくなれば元に戻る。専門性と人望で動かす場合、相手は「この人の言うことだから」と自分から動いている。その人がいなくなっても、「あの人ならどう判断するだろう」と考え続ける。影響が内面化されている。外からの圧力で動いた行動は、圧力がなくなれば止まる。内側から納得して動いた行動は、続く。ただし、注意点がある。どのカードが強いかは、組織や部署によって違う。エンジニアだけの組織では、データが圧倒的に強い。「ベンチマークの結果」「障害の根本原因分析」「パフォーマンス計測」——数字で示せば、それだけで説得力がある。論理と数字を重視する文化があるからだ。でも、営業部門やマーケティング部門では違う。データより「この人が言うなら」という人望が効くことがある。経営層との会議では、役職や過去の実績が重みを持つ。同じ会社でも、部署が変われば有効なカードは変わる。私はエンジニア組織にいることが多いので、データと専門性に頼りがちだ。でも、他部署との調整では、それだけでは通用しないことを何度も経験した。相手が何を重視するかを見極めて、カードを使い分ける必要がある。だから、長期的な影響力を構築するなら、専門性を磨き、人として尊敬される存在になることが最も確実な方法だ。専門性と人望は、どの組織でも比較的通用しやすい。「政治力を磨け」と言われると抵抗がある人も、「専門性を磨け」なら抵抗がないだろう。実は、専門性を磨くことは、組織における影響力を高める最も正攻法なアプローチなのだ。ここで、私の経験を1つ書いておく。ある領域で、私はチームの中で一番詳しくなった。別に政治をしたわけではない。ただ、その領域を深掘りし続けた。ドキュメントを読み、実験し、知見を共有した。すると、向こうから相談が来るようになった。「〇〇のことは△△さんに聞けばいい」という評判が立った。会議で発言すると、その領域については私の意見が尊重されるようになった。これは「政治」ではない。専門性による影響力だ。ただし、専門性を万能視するのは危険だ。限界もある。具体的に言おう。専門性が効くのは「その領域の意思決定」に限られる。組織全体の方向性、予算配分、人事——こういった領域横断的な意思決定では、専門性だけでは戦えない。私も経験がある。技術的な判断では尊重されるようになったが、プロジェクトの優先順位を決める会議では、相変わらず発言力がなかった。専門性は「深さ」を与えるが、「広さ」は別の力学で決まる。それでも、専門性があれば、政治力が弱くても、ある程度は戦える。少なくとも、自分の専門領域では発言権が得られる。そこを足がかりにして、徐々に影響力を広げていくことができる。だから、「政治が苦手だ」という人に言いたい。まず専門性を磨け。それが最も確実な道だ。政治力は、専門性という土台の上に乗せるオプションとして考えればいい。土台がないまま政治力だけ磨いても、長続きしない。組織と踊るための心構え最後に、心構えの話をする。おい、頑張るなら組織と踊れ。これは「組織に従属しろ」という意味ではない。ダンスは、相手の動きを感じながら、自分も動く。一方的にリードするわけでも、一方的にフォローするわけでもない。相手と自分の動きが調和して、初めてダンスになる。組織も同じだ。組織の力学を無視して突っ走ると、壁にぶつかる。かといって、組織に完全に従属すると、自分の意志がなくなる。組織の力学を理解し、その中で自分の目標を追求する。組織を動かしながら、自分も動く。これが「組織と踊る」ということだ。組織を敵視するな。かといって、盲従するな。組織は、自分の目標を達成するためのプラットフォームだ。うまく使えば、一人ではできないことができる。敵視していたら、使いこなせない。短期の勝ち負けにこだわるな。組織での影響力は、長期的に築くものだ。一回の会議で勝った負けたは、大した問題ではない。信頼の蓄積、専門性の蓄積、関係性の蓄積。これらが時間をかけて積み上がったとき、本当の影響力が生まれる。自分の価値観を失うな。組織の力学を理解し、活用することと、自分の価値観を捨てることは違う。「この方法は使えるけど、自分はやりたくない」と思うなら、やらなくていい。別の方法を探せばいい。「媚びない」ことと「無礼」であることは全く違う。前者は信念を持つことであり、後者は単なる社会性の欠如だ。同様に、「したたか」であることと「ずる賢い」ことも違う。前者は双方の利益を最大化する戦略的思考であり、後者は単なる利己主義だ。私は今でも、根回しに抵抗がある。でも、必要な場面ではやる。やりながら、「これでいいのか」と自問する。割り切れないまま、やっている。組織と踊るというのは、自分を殺すことではない。自分を活かしながら、組織の中で成果を出す方法を見つけることだ。その方法は、人によって違う。自分なりの踊り方を見つければいい。届かない人へここまで書いてきて、立ち止まる。「組織の力学を理解しろ」「影響力を磨け」「組織と踊れ」——私はそう書いた。でも、この記事には前提条件がある。この記事が有効なのは、以下の条件が揃っている場合だ。組織がまともである——努力が報われる余地がある自分にエネルギーがある——行動を起こす余力がある組織で働くことを選んでいる——別の選択肢を選んでいないこの前提が成り立たない場合、この記事は役に立たない。それぞれ見ていく。組織が合わない人がいるそもそも、組織で働くことが向いていない人がいる。組織の力学を理解しろと言われても、理解する気力がない。人間関係を築けと言われても、それ自体がストレスだ。会議で発言しろと言われても、声が出ない。彼らは「能力がない」のではない。組織という形態が合わないのだ。フリーランス、起業、小規模チーム、リモートワーク——組織以外の働き方もある。そちらが合う人もいる。「おい、頑張るなら組織と踊れ」は、組織で働くことを前提としている。その前提自体が合わない人には、この記事は届かない。力学を理解しても動けない人がいる組織の力学を理解した。影響力を磨く方法も分かった。でも、動けない。すでに消耗している人。根回しをする気力がない人。人間関係を築くエネルギーがない人。彼らに「影響力を磨け」と言っても、無理だ。まず休む必要がある。構造的に無理な組織もあるどんなに力学を理解しても、どんなに影響力を磨いても、無理な組織もある。腐敗した評価制度。声の大きい人だけが勝つ文化。変える気のない経営層。そういう組織では、個人の努力で変えられることに限界がある。「組織と踊れ」と言っても、相手がダンスをする気がないなら、成立しない。この記事は、「組織がまともで、自分にエネルギーがある」ことを前提にしている。その前提が成り立たないなら、この記事は役に立たない。「踊らない」という選択肢もある「組織と踊る」ことを選ばない、という選択肢もある。専門性だけで勝負する。政治には一切関わらない。評価されなくても気にしない。自分のペースで、自分のやり方で働く。それは「負け」ではない。評価ゲームから意識的に降りるという戦略だ。「おい、辞めないなら頑張れ」で書いたことを繰り返す。頑張れないなら、頑張らなくていい。降りてもいい。休んでもいい。それも、1つの選択だ。おわりに「おい、辞めるな」で辞めないことを選んだ。「おい、辞めないなら頑張れ」で頑張り方を学んだ。そして今回、「おい、頑張るなら組織と踊れ」で組織の力学を学んだ。正直に言う。この記事を書くことには抵抗があった。「政治のやり方を教える」みたいで、気が進まなかった。でも、過去の自分は、これを知りたかった。組織の力学を理解せず、「政治は汚い」と嫌悪しながら、壁にぶつかり続けていた。正義のエンジニアという幻想に囚われて、媚びないことと無礼を混同していた。その時間は、もったいなかった。組織の力学を理解しろ。でも、専門性と人望が最強のカードだ。政治に長けていても、実力がなければ長続きしない。実力があっても、組織の力学を無視していたら成果につながらない。両方必要だ。でも、長期的に見れば、専門性と人望が最も確実な道だ。譲れないもののために、譲るものを決めろ。したたかに生きろ。組織を敵視するな。盲従するな。組織と踊れ。——と書いて、自分でも苦い顔をしている。「お前も結局、体制に飲み込まれたのか」——かつての私なら、今の私をそう批判しただろう。しかし、それでいいのだ。技術的な純粋さを追求することと、社会的な成熟を遂げることは矛盾しない。むしろ、両方を兼ね備えてこそ、プロの仕事と言えるのではないだろうか。媚びないことと無礼の区別がつかなかった、頭の悪い反抗期は終わった。正直に言えば、私はまだ上手に踊れていない。根回しに抵抗がある。状況認識力が弱い。会議で空気を読めない。それでも、以前よりはマシになった。壁にぶつかる回数は減った。この記事が、かつての私のような人に届けばいいと思う。「政治は汚い」と思いながら、壁にぶつかり続けている人。組織の力学を理解することに抵抗がある人。「正義のエンジニア」という幻想に囚われている人。理解することと、加担することは違う。理解した上で、どう振る舞うかは自分で決められる。おい、頑張るなら組織と踊れ。踊れないなら、休め。踊り方は、自分で決めろ。——と、ここまで書いてきた。でも、最後に付け加えておく。組織が合わないなら、別の場所を探せばいい。それも、1つの選択だ。私も、まだ上手に踊れていない。それでも、やっている。それでいいのだと思う。かつての私のような若いエンジニアを見かけたら、優しく、でもはっきりと伝えたいと思う。「君の気持ちはよく分かる。でも、もっといい方法があるよ。一緒にしたたかにやっていこうぜ」と。多分昔の私だったら「は？日和って迎合した負け犬が何言ってんの？」とか思って、心の中で見下しながら表面上は「はい、参考にします」って適当に流すんでしょうね。まあ、それでいいんです。私も通った道だから。痛い目に遭うまで、人は変われない。私もそうだった。その時になって初めて、この言葉の意味が分かるはずです。けど大人として言う義務があるので言っておきました。参考書籍人を動かす　改訂文庫版作者:Ｄ・カーネギー創元社AmazonDD(どっちもどっち)論 「解決できない問題」には理由がある (WPB eBooks)作者:橘玲集英社Amazonその仕事、全部やめてみよう――１％の本質をつかむ「シンプルな考え方」作者:小野 和俊ダイヤモンド社Amazonアーキテクチャモダナイゼーション【リフロー型】 組織とビジネスの未来を設計する作者:Nick Tune,Jean-Georges Perrin翔泳社Amazonこれからの「正義」の話をしよう ──いまを生き延びるための哲学 (ハヤカワ・ノンフィクション文庫)作者:マイケル・サンデル早川書房AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazonスタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazon組織が変わる――行き詰まりから一歩抜け出す対話の方法2 on 2作者:宇田川 元一ダイヤモンド社Amazonモンク思考―自分に集中する技術作者:ジェイ・シェティ東洋経済新報社AmazonSOFT SKILLS ソフトウェア開発者の人生マニュアル 第2版作者:ジョン・ソンメズ日経BPAmazon社内政治の科学　経営学の研究成果 (日本経済新聞出版)作者:木村琢磨日経BPAmazon社内政治の教科書作者:高城 幸司ダイヤモンド社Amazon多様性の科学作者:マシュー・サイドディスカヴァー・トゥエンティワンAmazon［新版］組織行動の考え方―個人と組織と社会に元気を届ける実践知作者:金井 壽宏,高橋 潔,服部 泰宏東洋経済新報社Amazonソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社AmazonTHE CULTURE CODE 最強チームをつくる方法作者:ダニエル・コイル,楠木建かんき出版Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[プログラミングが好きな人こそ今の時代、プログラマーになる方がいいと思う。- 「プログラミングが好きな人は、もうIT業界に来るな。」を読んで]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/18/123151</guid>
            <pubDate>Sun, 18 Jan 2026 03:31:51 GMT</pubDate>
            <content:encoded><![CDATA[はじめにAIにリサーチをさせていた。結果が返ってくるまで数分かかる。待っている間、Xを開いた。流れてきたタイトルに、手が止まった。「プログラミングが好きな人は、もうIT業界に来るな。」note.comリサーチは終わっていた。結果を確認しないまま、記事を読んでいた。小学生の頃から黒い画面に向かい続けてきたエンジニアが、生成AIの登場によって「自分の手で作る喜び」を奪われつつあると語っていた。「心の中で何かが音を立てて崩れる」という表現があった。共感したのか、と聞かれると困る。共感しなかったのか、と聞かれても困る。たぶん、どちらでもある。読み終えて、エディタに戻った。さっきまで何をしていたか、思い出せなかった。反論したいわけではなかった。ただ、何かが引っかかっていた。「プログラミングが好き」という言葉だ。この人の「好き」と、私の「好き」は、同じものを指しているのだろうか。コーヒーを淹れた。飲みながら、書くことにした。「好き」の中身を分解する元記事の筆者と私の違いは、能力でも経験でもない。「プログラミングが好き」という言葉が指す範囲が違うのだ。言葉は同じでも、中身が違う。「好き」と言ったとき、何を思い浮かべているか。キーボードを叩く指先か、頭の中で組み上がる構造か、動いた瞬間の達成感か。同じ「好き」でも、その中身は人によって違う。中身が違えば、奪われるものも違う。プログラミングという行為には、少なくとも3つのレイヤーがある。——もちろん、これは私の視点からの便宜的な分類だ。元記事の筆者にとっては、まったく別の分け方があるかもしれない。「書くこと」と「考えること」は分離できない、という人もいるだろう。書きながら考える。考えながら書く。その不可分さこそがプログラミングだ、と。それでも、ここでは一旦この枠組みで話を進める。身体感覚：キーボードを叩く感触、コードを書く行為そのもの、画面に流れる快感知的作業：問題を分解し、設計し、実装する思考プロセス創造行為：何かを作り、動かし、世に出す達成感元記事の筆者が愛していたのは、単なる身体感覚ではないように思う。「自分の指先から生まれるロジックが、動かなかったものを動かす」——その創造の主体性だ。自分の手で書いたコードが動く。その実感が奪われたから「楽園が消えた」と感じている。AIが書いたコードをチェックする「検品係」では、創造の主体は自分ではない。これは「間違った好き」ではない。彼の文脈では、それが正解だった。小学生の頃から黒い画面に向かい続け、自分の手でコードを書くことで世界を動かしてきた。その積み重ねの上に立っている。私が「設計が好き」と言えるのも、私の文脈があるからだ。どちらが正しいという話ではない。私が好きだったのは、2だった。課題があったら適切なサイズに分割して、「この問題はこう実装すれば解決するな」と構造が頭の中で閃く。そこからコードを書き続けていく。その一連のプロセスが好きだった。AIがコードを出力するようになって初めて、自分の「好き」が何だったのか見えた。コードを書けなくなったとき、喪失感を感じたか？ 感じなかった。むしろ「これで設計に集中できる」と思った。そのとき気づいた。自分が好きだったのは、タイピングではなく、考えて、作って、また考える、その繰り返しだった。では、何が奪われ、何が残ったのか整理しよう。奪われたもの：タイピングの時間、実装の試行錯誤、ググって解決策を探す時間。残ったもの：設計を考える時間、AIの出力を評価する時間、「もっと良い方法はないか」と考える時間。私にとって、後者こそがプログラミングの核心だった。もっと言えば、コードを書く時間に追われて後回しにしていた「設計」や「トレードオフの検討」——いわゆるソフトウェアエンジニアリングの領域に、ようやく時間を使える。だから、「奪われていない」と言い切れる。実践ソフトウェアエンジニアリング (第9版)作者:ロジャー・プレスマン,ブルース・マキシムオーム社AmazonGoogleのソフトウェアエンジニアリング ―持続可能なプログラミングを支える技術、文化、プロセス作者:Titus Winters,Tom Manshreck,Hyrum WrightオライリージャパンAmazon正直に言う、めちゃくちゃ楽しい今、めちゃくちゃ楽しい。作りたいものがある。指示を出す。コードが出てくる。レビューする。直す。動く。——以前なら「面倒だな」と後回しにしていたアイデアや知識が、数分で形になる。さっき書いた「ソフトウェアエンジニアリングの領域」——設計、トレードオフ、アーキテクチャ。そこにやっと向き合えている感覚がある。楽しい。素直に、楽しい。これを「検品係」と呼ぶなら、私は世界一楽しい検品係だ。コードレビューが好きで良かった思えば、私はコードレビューが比較的好きだった。他人のコードを読んで、「この人はこう考えてるんだろうな」というのが見えると嬉しい。なぜこの設計にしたのか、どこで迷ったのか。コードの向こうに思考が透けて見える瞬間が好きだった。知らない言語機能や、思いつきもしなかった構造で課題を解決しているのを見ると、良し悪しにかかわらず楽しい。正解かどうかより、発見の方が全然面白い。正解とは常に文脈の中にしかない。チームの習熟度、プロダクトのフェーズ、パフォーマンス要件、保守する人間の数。同じ課題でも、文脈が変われば最適解は変わる。だから「このコードは正しいか」という問いより、「この文脈でなぜこう書いたのか」という問いの方が面白い。そして、その問いに答えようとする過程で、自分の中の「正解」も揺らぐ。揺らぐことが学びだ。AIが出力したコードをレビューしていると、これが想像以上に勉強になる。先日、AIに「CSVパーサーを書いて」と頼んだ。返ってきたコードを見て驚いた。私なら正規表現でゴリ押しするところを、状態機械で書いている。エスケープ処理も完璧だ。「なるほど、このアプローチがあったか」と笑った。逆に、「いや、これは現場では使えない」と思う瞬間もある。過剰に抽象化されていたり、エラーハンドリングが甘かったり。その判断力こそ、レビューを通じて研ぎ澄まされる。——もっとも、私の判断が正しいとは限らない。AIの提案を「使えない」と却下した翌週、まったく同じアプローチを別の記事で「ベストプラクティス」として紹介されているのを見たこともある。ただ、これは15年やってきた人間だから言えることだ。状態機械の良さが分かるのは、正規表現ゴリ押しで痛い目を見たことがあるからだ。「なるほど、このアプローチがあったか」と唸れるのは、比較対象を持っているからだ。経験ゼロの人がAIの出力から体系的に学べるかは、正直分からない。むしろ、学べない可能性の方が高い気がする。コードレビューが苦手な人には、AIとの協働は苦行かもしれない。でも、レビューが好きな人間にとっては、無限に相手がいるジムのようなものだ。疲れない、休まない、いつでも付き合ってくれる相手。しかも、毎回違うアプローチを見せてくれる。「検品」と「協働」の違い元記事は「検品係になった」と嘆いている。創造の主体でありたかった人にとって、この表現は正確だと思う。自分が書きたかったコードを他者（AI）が書き、自分はそれをチェックするだけ。主体と客体が入れ替わっている。ただ、私の場合は少し違った。携帯電話は私のことをめちゃくちゃ記憶している。連絡先、スケジュール、位置情報、検索履歴。私より私のことを知っているかもしれない。でも、携帯電話を使っているとき、「主体を奪われた」とは感じない。道具として使っている感覚がある。生成AIは違う。コードを書く、文章を書く、設計を考える——これまで「私がやること」だった領域に、AIが入り込んでくる。携帯電話が記憶を代替しても主体性は揺らがなかったが、生成AIは創造を代替しようとする。だから主体性が脅かされる感覚が生まれる。それでも、私は自分が主体だと思っている。なぜか。www.youtube.com答えは、関わり方にある。「検品」と「協働」の違いは何か。検品は受動的だ。ラインを流れてくる製品をチェックし、不良品を弾く。渡されたものをチェックするだけ。協働は能動的だ。方向性を示し、フィードバックを与え、成果物を一緒に作り上げる。私がAIとやっているのは後者だ。具体的に言うと——「こういう設計で書いて」と指示を出す（方向性）出てきたコードを見て「ここはこう直して」とフィードバックする「いや、アプローチ自体を変えよう」と軌道修正する最終的な成果物が完成するこのやりとりは、人間同士のペアプログラミングと構造的に同じだ。相手が人間かAIかの違いしかない。検品係は受け身だが、私は能動的にAIを導いている。方向を決め、判断を下し、軌道修正をかける。この能動性が、主体性を保つ鍵だ。AIとの関係を一言で表すなら、「相棒」ではなく「優秀だが判断できない後輩」が近い。指示を明確にすれば良い仕事をする。曖昧にすると、意図しないものが返ってくる。筆者が言うように、書く時間は減った。でも、考える時間は増えた。どう分割するか。どう設計するか。AIが出してきたコードのどこを採用し、どこを直すか。そして、仮にこれが「検品」だったとしても、私はその過程でかなり学んでいる。「こんな書き方があるのか」と何度も唸った。特に経験の浅い言語では顕著だ。自分で書いていたら絶対に思いつかないイディオムを、AIは平気で出してくる。検品のつもりが、いつの間にか授業を受けている。ただし、ここには落とし穴がある。AIが出したコードをそのまま使って「動いた、終わり」で済ませると、何も残らない。効率は上がる。成果も出る。でも、1週間後に「なぜこう書いたの？」と聞かれても、答えられない。因果を辿れない。自分が責任を持って出力したコードのはずなのに、説明しようとすると言葉が出てこない。以前、「AIエージェントと協働しながら学習する方法」という記事で詳しく書いたが、学びには「摩擦」が必要だ。エラーが出る。原因がわからない。仮説を立てる。試す。失敗する。また試す。この摩擦の中で、経験が意味に変わる。学習とは、経験を意味に変換する行為だ。AIが摩擦を消してくれると、経験が意味に変わる機会も消える。syu-m-5151.hatenablog.comだから私は、AIが出したコードを「なぜこう書いたのか」と考える時間を意図的に作っている。効率だけを求めるなら不要な時間だ。でも、この「不効率な時間」が学びを生む。摩擦は削減対象ではない。設計対象だ。何を学び、何を省略するか。その選択を自分でしている限り、主体は私だ。これを「検品」と呼ぶか「協働」と呼ぶかは、本人の姿勢次第なのかもしれない。——と書いて、自分で読み返して思った。「姿勢次第」では何も言っていないのと同じだ。具体的に、検品を協働に変えるための3つのポイントを挙げてみる。意図を言語化する: 「こう書いて」ではなく「この問題を解決したい。制約はこれ」と伝える。AIに考えさせる余地を残す。出力から学ぶ: AIが出したコードを「動くかどうか」だけでなく、「なぜこう書いたか」を考える。知らないパターンがあれば調べる。フィードバックを重ねる: 一発で完璧を求めない。「ここを直して」「いや、やっぱりこっち」のやりとりを楽しむ。この3つができれば、検品は協働になる。逆に言えば、「動くか確認するだけ」なら、それは検品だ。この3つを実践するかどうか。それが検品と協働を分け、主体性を保てるかどうかを分ける。——と偉そうに書いたが、これが「正解」かどうかは分からない。私の文脈ではうまくいっている。でも、別の文脈では別の答えがあるはずだ。締め切りに追われているときは「動けばいい」になるし、疲れているときは「なぜこう書いたか」なんて考えない。理想と現実は違う。ただ、「こうありたい」という指針があるのとないのとでは、違うと思っている。それすらも、私の文脈での話だ。ただし、これは私のケースだここまで書いてきて、一つ断っておきたいことがある。これは私の話だ。コードレビューが好きで、設計を考えるのが好きで、タイピング速度に自信がなかった人間の話だ。もし元記事の筆者のように、「自分の手で書いたコードが動く」その実感こそが喜びだったなら——この記事は何の慰めにもならないだろう。創造の主体でありたかった人に、「検品も楽しいよ」とは言えない。その人たちに「考え方を変えろ」と言うつもりはない。「自分が書きたかった小説をAIに書かせ、誤字脱字を直す校正者のような気分」——元記事のこの表現は、痛いほど分かる。創造の主体性を奪われた感覚は、姿勢や考え方でどうにかなるものではない。彼の文脈では、それが真実だ。私が「楽しい」と言えるのは、私の文脈がたまたまそうだったからに過ぎない。「でも、結局プログラマーの仕事は減るのでは？」という反論もあるだろう。正直、分からない。AIの進化は私の想像を超えている。5年後にどうなっているか、予測する自信がない。そして、ここは誤魔化さずに言っておくべきだと思う。「楽しい人がいること」と「職業として持続可能かどうか」は、まったく別の話だ。ドライバーが100人必要だった時代から、10人で済む時代へ。私が楽しくても、市場が縮小すれば、その楽しさを職業にできる人は減る。元記事の筆者が問うているのは、たぶんそっちの話でもある。この問いについて、エンジニアに許された特別な時間の終わりというスライドがある。エンジニアがドライバー席から助手席へ移る時代が来ている、という話だ。AIが「副操縦士（Copilot）」から「操縦士（Pilot）」へ進化しつつある。続編のたかが特別な時間の終わりでは、9ヶ月後にその予測が現実化しつつあると報告されている。私がこの記事で書いてきた「協働」も、結局は助手席からの関わり方なのかもしれない。ドライバー席に座っていた時代は終わりつつある。それでも、助手席には助手席の仕事がある。呑気なドライブデートを思い浮かべたかもしれないが、全然様相は違う。ラリーのコ・ドライバーは、ただ座っているだけではない。本番前にコースを試走し、コーナーの角度、直線の距離、路面の状態、危険なポイントをすべてペースノートに書き込む。本番では猛スピードで揺さぶられる車内で、そのノートを絶妙なタイミングで読み上げる。「左3、50m、右2、クレスト注意」。ドライバーは全コースを暗記できない。コ・ドライバーなしでは走れない。AIとの協働も似ている。事前にコードベースを把握し、設計を考え、制約を整理する——これがペースノートの作成だ。本番では、AIが猛スピードでコードを生成する中、「次は右だ」「ここは危険だ」と指示を出し続ける。曖昧な指示を出せば、車は崖から落ちる。ただし、その車は時速200kmで走っている。のんびり景色を眺める余裕はない。コ・ドライバーとして生きる。それがこの時代の選択だ。——と書いたが、この比喩にも限界がある。ラリーではコ・ドライバーの仕事がなくなることはない。しかし、AIとの協働では、その保証がない。今は「設計」「評価」「判断」が人間の仕事として残っている。だが、AIが設計し、AIが実装し、AIがレビューする世界が来たら？ コ・ドライバーの席さえ、自動運転に置き換わるかもしれない。この記事は「設計や評価は人間に残る」という前提で書いている。その前提が崩れたら、私の話は無効になる。ただ、「だから今やっても意味がない」とは思わない。今この瞬間、プログラミングが楽しいなら、それでいい。未来のことは、未来の自分が考える。——もっとも、この「楽しい」を大声で言うのは少し気が引ける。誰かが失った「楽しさ」の上に、私の「楽しさ」が成り立っているかもしれないからだ。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われたら、反論できない。おわりに書き終えて、コーヒーを淹れ直した。冷めていた。あの日から何日か経った。書いている間、ずっと考えていた。私は本当に「楽しい」のか。それとも、楽しいと思いたいだけなのか。正直、分からない。分からないまま書いた。元記事の筆者が読んだら、どう思うだろう。「お前はたまたま運が良かっただけだ」と言われるかもしれない。反論できる気がしない。私の「好き」と、あの人の「好き」は、たまたま違った。彼の文脈では彼が正しく、私の文脈では私が正しい。それだけのことだ。どちらかが間違っているわけではない。明日もたぶん、AIにコードを書かせる。レビューする。直す。動かす。それを「楽しい」と感じるかどうかは、そのときになってみないと分からない。ただ、少しだけ違うことがある。「プログラミングが好き」という言葉を使うとき、自分が何を指しているのか、前より意識するようになった。摩擦は削減対象ではない。設計対象だ。——この言葉を、自分に言い聞かせるようになった。そして、自分の「正解」も揺らぐことを知った。書く前と書いた後で、考えが変わっている。元記事の筆者の気持ちが、前より分かる気がする。揺らぐことが学びだと書いた。この記事を書くこと自体が、そうだった。「IT業界に来るな」と言われた君へ。私は「来い」とは言わない。言えない。生成AIがいつか道具になる日までは。ただ、私は来てよかった。少なくとも、今日は。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。技術が人類を幸せにするかみたいな問いは常に面白いです。技術革新と不平等の1000年史　上作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon技術革新と不平等の1000年史　下作者:ダロン アセモグル,サイモン ジョンソン早川書房Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【Vul】CodeBuildの設定ミスによるフィルタバイパス]]></title>
            <link>https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</link>
            <guid isPermaLink="false">https://www.rowicy.com/blog/vulmemo-aws-codebuild-misconfig/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[AWS CodeBuildにおける設定ミスにより、AWS提供のGitHubリポジトリがサプライチェーン攻撃のリスクにあった件]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[不動点コンビネータと無名再帰]]></title>
            <link>https://silasol.la/posts/2026-01-16-01_least_fixed_point/</link>
            <guid isPermaLink="false">https://silasol.la/posts/2026-01-16-01_least_fixed_point/</guid>
            <pubDate>Fri, 16 Jan 2026 00:00:00 GMT</pubDate>
            <content:encoded><![CDATA[不動点コンビネータと実践的な示唆について紹介します．]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Agent Development Kit (ADK)における評価駆動型開発（EDD）]]></title>
            <link>https://sreake.com/blog/evaluation-driven-development-with-adk/</link>
            <guid isPermaLink="false">https://sreake.com/blog/evaluation-driven-development-with-adk/</guid>
            <pubDate>Thu, 15 Jan 2026 04:42:40 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部の井上 秀一です。私はSreake事業部にて、SREや生成AIに関するResearch & Developmentを行っています。 Agent Developmen […]The post Agent Development Kit (ADK)における評価駆動型開発（EDD） first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Ory Kratosで認証を委譲する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</link>
            <guid isPermaLink="false">https://syu-m-5151.hatenablog.com/entry/2026/01/14/140248</guid>
            <pubDate>Wed, 14 Jan 2026 05:02:48 GMT</pubDate>
            <content:encoded><![CDATA[前回からの続き前回の記事では、Playwright MCPを使ったE2Eテストで5つのバグを発見した。CORS設定の欠如、JWTトークンの切り詰め、Hydraトークンとの不一致、ミドルウェアの適用漏れ、X-Tenant-Slugヘッダーの欠如。RBACの検証とOWASP Top 10との比較まで行い、マルチテナント認証システムが一通り動くようになった。前提知識: この記事はOry Hydraシリーズの続編です。OAuth2認可コードフローの基礎知識と、Login/Consent Providerの役割を理解している前提で進めます。前回記事はこちら。動く。ちゃんと動く。でも、レビューコメントが気になった。「パスワードリセット機能は？」「MFA対応の予定は？」「メール確認フローは？」全部、自分で実装しなければならない。Argon2idでパスワードをハッシュ化するコードは書いた。ログイン認証は動く。でも、パスワードを忘れたユーザーへのリセットメール送信、そのトークン管理、有効期限の検証。TOTPによる二要素認証。メールアドレス確認のフロー。これ全部、自分で実装するのか？RFCを読んでいたあの3日間を思い出した。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。同じ結論に至った。今度は認証機能についてだ。Ory Kratosという選択肢www.ory.shgithub.comOry Kratosは「ヘッドレスID管理システム」だ。Hydraが「認証をしない認可サーバー」だったことを思い出してほしい。Hydraはプロトコル層（OAuth2/OIDC）に特化し、認証は私たちに任せた。Kratosはその「任された認証」を担当する。┌─────────────────────────────────────────────────────────────┐│                     Ory Stack                               │├────────────────────────┬────────────────────────────────────┤│      Ory Kratos        │           Ory Hydra                ││   (Identity Provider)  │      (Authorization Server)        │├────────────────────────┼────────────────────────────────────┤│ - ユーザー登録         │ - OAuth2/OIDC                      ││ - ログイン認証         │ - トークン発行                     ││ - MFA (TOTP, WebAuthn) │ - クライアント管理                 ││ - パスワードリセット   │ - Consent管理                      ││ - プロフィール管理     │ - セッション管理                   ││ - メール確認           │                                    │└────────────────────────┴────────────────────────────────────┘つまり、これまでに私がRustで書いたAuthService——パスワード検証、ユーザー登録、セッション管理——これらをKratosに任せられる。アーキテクチャの変化これまでの構成を振り返る。【01-03の構成】┌─────────────┐     ┌─────────────────────┐     ┌─────────────┐│   Browser   │────▶│ Rust Login Provider │────▶│  Ory Hydra  ││             │     │ (自前実装)           │     │             │└─────────────┘     └─────────────────────┘     └─────────────┘                              │                              ▼                    ┌─────────────────────┐                    │ PostgreSQL (users)  │                    └─────────────────────┘私が書いたRust Login Providerは認証を担当していた。ユーザーテーブルも自前で管理していた。Kratosを導入すると、こうなる。【Kratos導入後の構成】┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐│   Browser   │────▶│  Kratos UI  │────▶│  Ory Kratos │────▶│  Ory Hydra  ││             │     │  (Node.js)  │     │             │     │             │└─────────────┘     └─────────────┘     └──────┬──────┘     └─────────────┘                                               │                                               ▼                                      ┌─────────────────────┐                                      │ PostgreSQL          │                                      │ (identities)        │                                      └─────────────────────┘私が書くコードは、ほぼゼロになる。パスワード検証、ユーザー登録、セッション管理——これまでに私がRustで実装した機能は、全てKratosが提供する。私が書くのはKratosの設定ファイルと、必要に応じたUIのカスタマイズだけだ。「それって、学習した意味がないのでは？」いや、逆だ。認証システムを自前で実装した経験は、Kratosの設定を理解する上で役立った。例えば、Kratosの設定にhashers.argon2.memory: 128MBという項目がある。自前実装の経験がなければ、その意味を理解できなかっただろう。メモリコストを上げればセキュリティは向上する。しかし同時接続数の増加でOOMのリスクも上がる——この判断ができるのは、OWASPのドキュメントを読み、自分でパラメータを選んだ経験があるからだ。「ドキュメントを読めば同じでは？」——そう思うかもしれない。確かに、ドキュメントを読めば設定はできる。しかし、障害時に「この設定が原因かもしれない」と仮説を立てられるのは、自分で同じ問題に苦しんだ経験があるからだ。ログを見て「これはセッション固定化攻撃への対策が発動した」と判断できるか。エラーメッセージから「Identity Schemaの定義が間違っている」と気づけるか。これは学習効率の問題ではなく、デバッグ能力の問題だ。これまでの実装で、認証システムの複雑さを体験した。Argon2idのパラメータ設定、ユーザー列挙攻撃への対策、セッション管理の罠。58個のテストを書いて「できないこと」を確認した。だからこそ、Kratosのありがたみが分かる。そして、Kratosで問題が起きたときに対処できる。全員が自前実装を経験すべきとは言わない。しかし、チームに1人は「中身を理解している人」がいた方がいい。Docker Composeで動かすwww.ory.com実際に動かしてみよう。services:  postgres:    image: postgres:16-alpine    environment:      POSTGRES_USER: postgres      POSTGRES_PASSWORD: secret      POSTGRES_DB: postgres    volumes:      - postgres_data:/var/lib/postgresql/data      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro  # 後述の初期化スクリプト    healthcheck:      test: ["CMD-SHELL", "pg_isready -U postgres -d postgres"]      interval: 5s      timeout: 5s      retries: 5    networks:      - ory  kratos-migrate:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable    volumes:      - ./kratos:/etc/config/kratos:ro    command: migrate sql -e --yes --config /etc/config/kratos/kratos.yml    depends_on:      postgres:        condition: service_healthy    networks:      - ory  kratos:    image: oryd/kratos:v1.3.1    environment:      DSN: postgres://postgres:secret@postgres:5432/kratos?sslmode=disable      LOG_LEVEL: debug      SERVE_PUBLIC_BASE_URL: http://localhost:4433      SERVE_ADMIN_BASE_URL: http://localhost:4434    volumes:      - ./kratos:/etc/config/kratos:ro    command: serve all --dev --config /etc/config/kratos/kratos.yml    ports:      - "4433:4433"  # Public API      - "4434:4434"  # Admin API    depends_on:      kratos-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4433/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  hydra-migrate:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable    command: migrate sql -e --yes    depends_on:      postgres:        condition: service_healthy    networks:      - ory  hydra:    image: oryd/hydra:v2.2    environment:      DSN: postgres://postgres:secret@postgres:5432/hydra?sslmode=disable      SECRETS_SYSTEM: super-secret-system-secret-at-least-32-chars      URLS_SELF_ISSUER: http://localhost:4444      URLS_CONSENT: http://localhost:4455/consent      URLS_LOGIN: http://localhost:4455/login      URLS_LOGOUT: http://localhost:4455/logout      LOG_LEVEL: debug    command: serve all --dev    ports:      - "4444:4444"  # Public API      - "4445:4445"  # Admin API    depends_on:      hydra-migrate:        condition: service_completed_successfully    healthcheck:      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4444/health/ready"]      interval: 10s      timeout: 5s      retries: 5    networks:      - ory  kratos-ui:    image: oryd/kratos-selfservice-ui-node:v1.3.1    environment:      PORT: 4455      KRATOS_PUBLIC_URL: http://kratos:4433      KRATOS_BROWSER_URL: http://localhost:4433      HYDRA_ADMIN_URL: http://hydra:4445      COOKIE_SECRET: super-secret-cookie-secret-32chars      CSRF_COOKIE_NAME: ory_csrf_ui      CSRF_COOKIE_SECRET: super-secret-csrf-secret-32-chars    ports:      - "4455:4455"    depends_on:      kratos:        condition: service_healthy      hydra:        condition: service_healthy    networks:      - oryvolumes:  postgres_data:networks:  ory:注意: 上記の設定は開発環境用です。本番環境ではSECRETS_SYSTEMやCOOKIE_SECRETに32文字以上の暗号学的に安全な値を設定してください。サービスが6つある。PostgreSQL、KratosとHydraそれぞれのmigrate/serveサービス、そしてKratos UI。以前の自前実装（auth-provider）はKratosに置き換わった。ポイントはkratos-uiだ。これはOry公式が提供するセルフサービスUI。ログイン画面、登録画面、パスワードリセット画面などが含まれている。「自分でUI書かなくていいの？」開発環境ではこれで十分だ。本番環境では、このUIを参考に自前のUIを実装できる。Kratosの「ヘッドレス」設計により、UIは完全に切り離されている。github.comKratos設定ファイルの解説Kratosの設定ファイルkratos.ymlを見てみよう。version: v1.3.1dsn: memoryserve:  public:    base_url: http://localhost:4433/    cors:      enabled: true      allowed_origins:        - http://localhost:4455  admin:    base_url: http://localhost:4434/selfservice:  default_browser_return_url: http://localhost:4455/  allowed_return_urls:    - http://localhost:4455    - http://localhost:4444  methods:    password:      enabled: true    totp:      enabled: true      config:        issuer: OryKratosVerification    lookup_secret:      enabled: true    link:      enabled: true    code:      enabled: true  flows:    error:      ui_url: http://localhost:4455/error    settings:      ui_url: http://localhost:4455/settings      privileged_session_max_age: 15m    recovery:      enabled: true      ui_url: http://localhost:4455/recovery      use: code    verification:      enabled: true      ui_url: http://localhost:4455/verification      use: code      after:        default_browser_return_url: http://localhost:4455/    logout:      after:        default_browser_return_url: http://localhost:4455/login    login:      ui_url: http://localhost:4455/login      lifespan: 10m    registration:      lifespan: 10m      ui_url: http://localhost:4455/registration      after:        password:          hooks:            - hook: sessionlog:  level: debug  format: text  leak_sensitive_values: truesecrets:  cookie:    - super-secret-cookie-secret-32chars  cipher:    - super-secret-cipher-key-32-charsciphers:  algorithm: xchacha20-poly1305hashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16identity:  default_schema_id: default  schemas:    - id: default      url: file:///etc/config/kratos/identity.schema.jsoncourier:  smtp:    connection_uri: smtps://test:test@mailslurper:1025/?skip_ssl_verify=trueoauth2_provider:  url: http://hydra:4445セルフサービスフローselfservice:  methods:    password:      enabled: true    totp:      enabled: true以前、私がRustで実装したパスワード認証。Kratosではpassword: enabled: trueの一行で有効になる。TOTPも同様だ。以前は「MFA対応の予定は？」という質問に答えられなかった。Kratosなら設定1つで有効化できる。パスワードハッシュhashers:  algorithm: argon2  argon2:    parallelism: 1    memory: 128MB    iterations: 2    salt_length: 16    key_length: 16以前、私はArgon2::default()を使った。Kratosも同じArgon2を使っている。設定値を明示的に指定することで、チーム内で「なぜこのパラメータか」を共有できる。cheatsheetseries.owasp.orgHydra連携oauth2_provider:  url: http://hydra:4445これが最も重要な設定だ。KratosがHydraのAdmin APIに接続し、login_challengeを処理する。以前は私がRustでHydraClientを実装し、accept_loginを呼び出していた。Kratosはこれを自動で行う。https://www.ory.com/docs/kratos/self-hosted/hydra-integrationwww.ory.comIdentity Schemaの設計Kratosはユーザー情報を「Identity」として管理する。その構造はJSON Schemaで定義する。{  "$id": "https://schemas.ory.sh/presets/kratos/identity.email.schema.json",  "$schema": "http://json-schema.org/draft-07/schema#",  "title": "Person",  "type": "object",  "properties": {    "traits": {      "type": "object",      "properties": {        "email": {          "type": "string",          "format": "email",          "title": "E-Mail",          "ory.sh/kratos": {            "credentials": {              "password": {                "identifier": true              },              "totp": {                "account_name": true              }            },            "recovery": {              "via": "email"            },            "verification": {              "via": "email"            }          }        },        "name": {          "type": "object",          "properties": {            "first": {              "title": "First Name",              "type": "string"            },            "last": {              "title": "Last Name",              "type": "string"            }          }        }      },      "required": ["email"],      "additionalProperties": false    }  }}ory.sh/kratosという拡張プロパティが特徴的だ。credentials.password.identifier: true — このフィールドがログインIDになるrecovery.via: email — パスワードリセットはこのメールアドレスに送信されるverification.via: email — メール確認もこのアドレスに送信される以前、私はユーザーテーブルを自前で設計した。Kratosではスキーマを宣言的に定義するだけでいい。www.ory.com実際にハマったことでも、最初のdocker compose upは失敗した。データベースが存在しないFATAL: database "kratos" does not exist (SQLSTATE 3D000)KratosとHydraはそれぞれkratosとhydraという名前のデータベースを期待する。でも、PostgreSQLコンテナはpostgresデータベースしか作らない。解決策は初期化スクリプトだ。-- init.sqlCREATE DATABASE kratos;CREATE DATABASE hydra;# docker-compose.ymlpostgres:  volumes:    - ./init.sql:/docker-entrypoint-initdb.d/init.sql:roPostgreSQLは/docker-entrypoint-initdb.d/にあるSQLファイルを起動時に実行する。これで両方のデータベースが作成される。最初は「なぜ自動で作ってくれないんだ」と思った。おそらく、本番環境では既存のデータベースサーバーに接続することが多いからだろう。いずれにせよ、初期化スクリプトで解決できる。ポート競合Bind for 0.0.0.0:4444 failed: port is already allocated以前の記事で作ったory-hydra-rust環境がまだ動いていた。同じポート4444を使おうとして衝突。# 他の環境を停止cd ../ory-hydra-rust && docker compose down複数のOry環境を並行して動かす時は、ポートを変える必要がある。開発環境では素直に片方を停止した方がいい。Kratosが教えてくれた盲点E2EテストでTestPassword123!というパスワードを使おうとした。{  "id": 4000034,  "text": "The password has been found in data breaches and must no longer be used.",  "context": {    "breaches": 3330  }}KratosはデフォルトでHave I Been PwnedのAPIを使い、パスワードが過去のデータ漏洩に含まれていないかチェックする。TestPassword123!は3,330件の漏洩で見つかっていた。haveibeenpwned.comなぜ私は思いつかなかったのか。振り返ると、私の58個のテストは「攻撃者がシステムに対して行う操作」をテストしていた。間違ったパスワードでログインできないこと存在しないユーザーで情報が漏れないこと同時登録で競合状態が起きないことこれは全て「システムへの攻撃」に対するテストだ。攻撃者がシステムの外側から突破を試みるシナリオ。HIBPチェックは視点が異なる。「ユーザーが持ち込むリスク」に対処している。ユーザーが「password123」を使おうとするユーザーが他のサービスで使い回しているパスワードを登録するユーザーが過去に漏洩したパスワードを選ぶこれはシステムへの攻撃ではない。ユーザー自身がリスクを持ち込むシナリオだ。私はこのカテゴリを完全に見落としていた。なぜ見落としたのか。おそらく「ユーザーは正しく行動する」という暗黙の前提があった。パスワード強度のバリデーション（8文字以上、英数字混合など）を入れれば十分だと思っていた。でも、TestPassword123!は典型的な強度バリデーションを通過する。英大文字、英小文字、数字、記号、8文字以上。全ての条件を満たしている。にもかかわらず、3,330件の漏洩で見つかっている。強度バリデーションは「推測しやすいか」をチェックする。HIBPチェックは「既に漏洩しているか」をチェックする。両者は補完関係にある。Kratosを使うことで、私が想定していなかった脅威カテゴリまでカバーできる。これが「専門家が作ったツールを使う」ことの価値だ。自分の盲点を、他者の知見で補える。E2Eテストではタイムスタンプを含むランダムなパスワードを生成して回避した。TEST_PASSWORD="Kratos$(date +%s)E2E!Xk9#mN"本番環境では、この機能を有効にしたまま運用すべきだ。ユーザーに「このパスワードは漏洩しています」と伝えることで、アカウント乗っ取りのリスクを下げられる。環境の起動と動作確認初期化スクリプトを追加した状態で起動する。docker compose up -ddocker compose logs -fヘルスチェック用エンドポイントにアクセスしてみる。# Kratosのヘルスチェックcurl http://localhost:4433/health/ready# {"status":"ok"}# Hydraのヘルスチェックcurl http://localhost:4444/health/ready# {"status":"ok"}両方ともokが返ってきた。セルフサービスフローの確認ブラウザでhttp://localhost:4455/registrationにアクセスする。登録画面が表示される。メールアドレスとパスワードを入力して登録。次にhttp://localhost:4455/loginにアクセス。ログイン画面が表示される。先ほど登録した認証情報でログイン。ログイン成功。これだけだ。拍子抜けするほど簡単だった。以前、私は以下を実装した。AuthService::register() — ユーザー登録AuthService::authenticate() — パスワード検証login_page() — ログインフォームのHTMLlogin_submit() — フォーム送信処理58個のテストKratosでは、設定ファイルを書くだけでこれらが全て動く。OAuth2フローの確認OAuth2クライアントを作成する。docker compose exec hydra hydra create oauth2-client \  --endpoint http://localhost:4445 \  --grant-type authorization_code \  --response-type code \  --scope openid,profile,email \  --redirect-uri http://localhost:8080/callback \  --name "Test Client"クライアントIDとシークレットが出力される。ブラウザで認可エンドポイントにアクセスする。http://localhost:4444/oauth2/auth?client_id=<CLIENT_ID>&response_type=code&scope=openid+profile+email&redirect_uri=http://localhost:8080/callback&state=test-stateHydraがKratos UIにリダイレクトKratos UIがログイン画面を表示ログイン成功後、Kratosがlogin_challengeをHydraに送信HydraがConsent画面にリダイレクトConsent承認後、認可コードがコールバックURLに返される以前、私がRustで実装したlogin_submit()の処理を、Kratosが自動で行っている。// 前回の実装（不要になった）pub async fn login_submit(    State(state): State<AppState>,    Form(form): Form<LoginForm>,) -> Result<Redirect, AppError> {    let user = state.auth.authenticate(&form.email, &form.password).await?;    let completed = state.hydra        .accept_login(&form.login_challenge, &user.id.to_string(), false)        .await?;    Ok(Redirect::to(&completed.redirect_to))}このコードは、もう書く必要がない。E2Eテストで確認したこと実際にAPIを叩いて、フロー全体が動くことを確認した。Registration Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/registration/api"# Flow ID: 77ff9653-ccd2-4f91-aeea-8fbb4d67fce7# 2. 登録を実行curl -s -X POST "http://localhost:4433/self-service/registration?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "password": "Kratos1767517527E2E!Xk9#mN",    "traits": {      "email": "e2etest@example.com",      "name": { "first": "E2E", "last": "Test" }    }  }'Registration successful!Identity ID: 169e0834-4b45-441f-95f8-5adc45d8a3e9Email: e2etest-1767517527@example.comSession Token: ory_st_WugR5gisST7SO...Kratosのセルフサービスフローは2段階構成だ。まずフローを初期化してFlow IDを取得し、そのIDを使ってデータを送信する。これにより、CSRFトークンやフローの有効期限が管理される。Login Flow# 1. フローを初期化curl -s -X GET "http://localhost:4433/self-service/login/api"# 2. ログインを実行curl -s -X POST "http://localhost:4433/self-service/login?flow=$FLOW_ID" \  -H "Content-Type: application/json" \  -d '{    "method": "password",    "identifier": "e2etest@example.com",    "password": "Kratos1767517527E2E!Xk9#mN"  }'Login successful!Session ID: 8b97d548-8436-48ee-b4fd-8e1c643dac04Session Token: ory_st_ty15oU5JLIABh...Session Verificationcurl -s -X GET "http://localhost:4433/sessions/whoami" \  -H "Authorization: Bearer $SESSION_TOKEN"Session valid!Identity: e2etest-1767517527@example.comActive: trueセッショントークンを使って/sessions/whoamiを呼ぶと、現在のセッション情報が返ってくる。これは以前私がRustで実装したJwtService::verify()に相当する機能だ。OAuth2 Authorization Flow# OAuth2クライアントを作成curl -s -X POST "http://localhost:4445/admin/clients" \  -H "Content-Type: application/json" \  -d '{    "client_id": "e2e-test-client",    "client_secret": "e2e-test-secret",    "grant_types": ["authorization_code"],    "response_types": ["code"],    "scope": "openid profile email",    "redirect_uris": ["http://localhost:8080/callback"]  }'認可エンドポイントにアクセスすると、HydraがKratos UIにリダイレクトする。http://localhost:4444/oauth2/auth?client_id=e2e-test-client&...  ↓http://localhost:4455/login?login_challenge=Xv84rhGlXQQrVNL7SlICdNobNbYvcK7z8il...login_challengeパラメータが付与されている。Kratos UIはこのチャレンジを使ってHydraと連携し、認証完了後に適切なリダイレクトを行う。E2Eテスト結果サマリー テスト項目  結果  Registration Flow  成功  Login Flow  成功  Session Verification  成功  OAuth2 Client Setup  成功  OAuth2 Authorization Flow  成功（login_challenge生成確認） 全てのフローが期待通りに動作した。以前の自前実装と比較して、コード量はゼロになり、機能は増えた。自前実装との比較 観点  自前実装（02）  Kratos  パスワード認証  Argon2id実装  組み込み  MFA  未実装  TOTP, WebAuthn対応  パスワードリセット  未実装  フロー組み込み  メール確認  未実装  フロー組み込み  ソーシャルログイン  未実装  OIDC対応  漏洩パスワードチェック  未実装  HIBP連携  ログイン画面  HTML手書き  公式UI or 自前  セキュリティテスト  58個書いた  Oryが検証済み  学習コスト  Rust知識  Kratos設定  カスタマイズ性  完全自由  スキーマ/フック 特筆すべきは漏洩パスワードチェックだ。Have I Been Pwnedとの連携により、過去のデータ漏洩で流出したパスワードを拒否できる。これは以前書いた58個のテストでも考慮していなかった観点だ。Kratosを使うことで、私が思いつかなかったセキュリティ対策まで自動的に適用される。自前実装は無駄だったのか？いや、違う。以前の実装で学んだこと——Argon2idのパラメータ、ユーザー列挙攻撃への対策、タイミング攻撃の考慮——これらはKratosの設定を理解する上で役立った。「なぜこの設定があるのか」が分かるのは、自分で実装した経験があるからだ。いつKratosを使うべきかKratosを選ぶかどうかは、3つの軸で判断する。技術的要件: カスタマイズの複雑さはどの程度か。Kratosはフック機構やIdentity Schemaで柔軟性を提供するが、「3回目のログインでは必ずCAPTCHAを表示」のような独自フローは難しい。標準的な認証フローなら、Kratosで十分だ。組織的要件: チームにセキュリティ専門家がいるか。いないなら、Kratosに任せた方がいい。脆弱性対応、ベストプラクティスの追従——これらを自前でやるには専門性が必要だ。SOC2やISO27001の監査でも「専門企業の製品を使っています」と答えられる。ビジネス要件: 認証がコア価値か否か。パスワードマネージャーや認証SaaSなら、自前実装に意味がある。ECサイトや社内ツールなら、認証に時間をかけるより本業に集中すべきだ。私がこれまで関わってきたプロジェクトの8割は、最初からKratosで良かった。残り2割は、レガシーシステムとの統合が複雑すぎるか、認証自体がプロダクトの価値だった。今回のケースでは、学習目的で自前実装から始めたが、本番プロジェクトなら最初からKratosを選ぶ。認証に独自性は不要で、チームにセキュリティ専門家もいない——判断は明確だ。次回予告Kratosを導入したことで、認証（Authentication）は解決した。ユーザーはログインできる。セッションも管理される。でも、ログインしたユーザーが「何をできるか」は、まだ決まっていない。認証と認可は別物だ。認証は「誰であるか」を確認する。認可は「何ができるか」を判断する。次回は、Ory Ketoを使ってZanzibarモデルによる認可システムを構築する。おわりに正直に言うと、Kratosの設定を書いている時、何度か「自分で実装した方が分かりやすいのでは」と思った。YAMLの設定項目が多い。ドキュメントを何度も読み返した。でも、動いた時の感覚が違う。これまでに私が書いた数百行のRustコード。それがYAML数十行で置き換わった。しかも、MFAやパスワードリセットなど、私が「次回以降に実装する」と書いていた機能が、既に含まれている。「自前で作ることの非合理性」第1回で書いた言葉を思い出した。認可サーバーだけでなく、認証システムも同じだった。仕様は理解できる。実装もできる。でも、プロダクション品質で検証し続けることは、私たちの仕事ではない。Kratosに移行しても、設定の検証やアップグレード対応、障害時の判断は残る。責任が消えるのではなく、「実装の責任」から「選定と運用の責任」に形を変える。その上で、認証の基本的な部分——パスワード認証、MFA、セッション管理——は、毎回ゼロから考える問題ではなくなった。そして、もう1つ学んだことがある。Have I Been Pwnedの件だ。私は58個のテストを書いて「完璧だ」と思っていた。でも、「ユーザーが持ち込むリスク」という視点が完全に抜けていた。専門家が作ったツールを使う価値は、自分の盲点を補えることにある。レビューコメントに返信しよう。「パスワードリセット機能は？」——Kratosで対応します。この記事が参考になれば、読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料Ory KratosOry Kratos GitHubOry Kratos DocumentationKratos QuickstartIdentity SchemaHydra IntegrationOry HydraOry Hydra DocumentationLogin and Consent FlowセキュリティガイドラインOWASP Password Storage Cheat SheetOWASP Authentication Cheat Sheet検証環境ory-kratos-verification（GitHub）]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Error ReportingとCloud Runでアプリエラーをいい感じにグループ化してGitHubイシューにする]]></title>
            <link>https://zenn.dev/kimitsu/articles/report-error-to-github</link>
            <guid isPermaLink="false">https://zenn.dev/kimitsu/articles/report-error-to-github</guid>
            <pubDate>Mon, 12 Jan 2026 02:52:23 GMT</pubDate>
            <content:encoded><![CDATA[Error Reporting とは皆さん、Google Cloud の Error Reporting はご存知でしょうか。あまり知られていないのではないかなと思っています。アプリからは日々エラーが出ておりエンジニアはそれに対応する必要がありますが、エラーというものは同じ原因で複数回出るものです。ログを眺めていて同じようなエラーがたくさん並んでいてもあまり情報は増えません。Error Reporting はアプリケーションのエラーログを収集し、同じ原因のエラーをグループ化してくれるサービスです。[1]例えば以下の例では同じエラーが 3 回出ていますが、Error Rep...]]></content:encoded>
        </item>
    </channel>
</rss>