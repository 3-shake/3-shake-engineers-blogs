<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>3-shake Engineers' Blogs</title>
        <link>https://blog.3-shake.com</link>
        <description>3-shake に所属するエンジニアのブログ記事をまとめています。</description>
        <lastBuildDate>Sun, 03 Sep 2023 18:30:22 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>ja</language>
        <image>
            <title>3-shake Engineers' Blogs</title>
            <url>https://blog.3-shake.com/og.png</url>
            <link>https://blog.3-shake.com</link>
        </image>
        <copyright>3-shake Inc.</copyright>
        <item>
            <title><![CDATA[Nodejs(Nest.js)のアプリケーションのbuildを高速化、slim化してみようの会]]></title>
            <link>https://zenn.dev/satohjohn/articles/c05d29f5d68e0c</link>
            <guid>https://zenn.dev/satohjohn/articles/c05d29f5d68e0c</guid>
            <pubDate>Sat, 02 Sep 2023 10:02:16 GMT</pubDate>
            <content:encoded><![CDATA[前提DockerによるNode.jsのインストール(pull)はキャッシュされているものとする.dockerignoreは以下の通りnode_modules.git.gitignore*.mddisttest 最初にまとめ軽く、そんなに依存関係が多くないアプリケーションであればnpmでstaging buildでキャッシュ効かせるぐらいでよいかもRUN --mount=type=cache,target= は効果がありそうである (https://zenn.dev/kou64yama/articles/powerful-docker-build-cache...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Lookerのユーザー権限について]]></title>
            <link>https://zenn.dev/nedoko_dok0dko/articles/160cb146e72740</link>
            <guid>https://zenn.dev/nedoko_dok0dko/articles/160cb146e72740</guid>
            <pubDate>Thu, 31 Aug 2023 17:22:40 GMT</pubDate>
            <content:encoded><![CDATA[これは何Lookerのユーザー権限一覧を個人的にまとめたものhttps://cloud.google.com/looker/docs/admin-panel-users-roles?hl=ja#default_permission_sets ユーザー権限一覧Admin:Developer、Viewer、Standard権限に加え、データソースへの接続やユーザー管理の権限を持つ現時点で確認できる、Adminでしかできない機能については以下データソース(BigQuery等)への接続設定ユーザーの追加・削除・権限の変更ユーザー・グループ単位のフォルダの公開・非公...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day7]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_7</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_7</guid>
            <pubDate>Wed, 30 Aug 2023 16:03:36 GMT</pubDate>
            <content:encoded><![CDATA[前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Table Creationを読みました。今回はArchitecture > Core functions > Write I/O pathを読みます。ドキュメントのバージョンは最新のv2.19 previewです。Write I/O pathWrite I/O pathはYQLレイヤーで処理され、タブレットリーダーによってレプリケーションの準備が行なわれるシングルキーでの書き込みとして例示することが出来る。アトミックなアップデートを共なう複数キーでの分散トランザクションなど複雑なケースについては分散トランザクションに記載する。Write operation processing by YQL layerユーザーが発行したYQLクエリレイヤに作用するライトリクエストはポートから適切なAPI(YQLまたはYCQL)を経由して行なわれる。このユーザーリクエストはYQLレイヤで内部キーに変換される。シャーディングで説明するように、それぞれのキーは一つのタブレットが所有する。どのタブレットがキーを所有するか特定するために、YQLレイヤはYB-MasterにRPC1呼び出しを実行する。そのレスポンスは将来の利用のためにキャッシュされる。YugabyteDBはタブレットの場所をキャッシュし直接参照することでネットワークホップを減らすことで、YQLレイヤが直接適切なYB-TServerにホストされるタブレットリーダーにリクエストを送信することが出来るスマートクライアントを持つ。YQLレイヤがローカルノードにタブレットリーダーを見つけた場合、RPCはローカルファンクションコールになりリクエストをシリアライズとデシリアライズしてネットワーク越しに送信する時間を節約することが出来る。その後YQLレイヤはタブレットリーダーをホストするYB-TServerへの書き込みを発行する。この書き込みはキーを所有するRaftグループのタブレットリーダーによって処理される。Preparation of the operation for replication by tablet leader下記の図はタブレットリーダーがレプリケーションを実行する処理を説明している。タブレットのRaft Groupリーダーは以下の処理を実行する。現在実行されている処理が現在のスキーマに対応しているかを判別するキーに対してローカルin-memoryロックマネージャーを利用してロックを取得する。このロック機構はフォロワーには存在しない必要であればデータを読み込む(read-modify-writeや条件付きアップデート命令など)DocDBに書き込まれる変更のバッチを準備する。この書き込みバッチは殆ど最終的にRocksDBに書き込まれるKey-Valueペアに近く、それぞれのキーの末尾に最終的なhybrid timestampが添えられていないだけであるRaft replication of the write operation書き込みのRaftレプリケーション処理の流れは以下のように説明することが出来る。リーダーがバッチをRaft logにアペンドし、書き込みのためのhybrid timestampを選択するRaftを利用しデータをピアーに複製する成功したRaft replicationのデータをローカルのDocDBに反映するユーザーに成功を返すフォロワータブレットはRaftを利用したデータの複製を受けつけ、コミットされた事が分ったタイミングでその複製をローカルのDocDBに反映する。リーダーは以下のようにコミットポイントに於ける後続のRPCリクエストの進行を進める。書き込みバッチを含むRaftエントリーは過半数以上のタブレットRaft Groupピアーに複製されるRaftのサブシステムから"Replication Successful"のコールバックを取得したあと、リーダーはローカルのDocDBにバッチの書き込みを適用するリーダーからの次の更新でエントリーがコミットされたことがフォロワーに通知され、フォロワーはそれぞれのRocksDBインスタンスにバッチの書き込みを適用する。Response to the clientInformation Pending2Exampleskとvという値をKという行とVという行をもつテーブルT1に挿入する例について考える3。この例ではユーザーアプリケーションがランダムなYugabyteDBサーバにWriteクエリを送信し、そのサーバがリクエストを適切にルーティングすると仮定して簡略化している。特にYCQLではYugabyteDB Smart Clientを使うことで、余分なネットワークホップを避けることが出来る。↩原文ママ。過去のバージョンでも記載無し↩INSERT INTO T1 (K,V) VALUES('k','v')ということ↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ChatGPT × Slack = ChatOpsを実現する「h1-slack-bot」の紹介]]></title>
            <link>https://sreake.com/blog/chatgpt-slack-integration/</link>
            <guid>https://sreake.com/blog/chatgpt-slack-integration/</guid>
            <pubDate>Thu, 24 Aug 2023 07:04:08 GMT</pubDate>
            <content:encoded><![CDATA[1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 本記事では、ChatOps […]The post ChatGPT × Slack = ChatOpsを実現する「h1-slack-bot」の紹介 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day6]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_6</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_6</guid>
            <pubDate>Wed, 23 Aug 2023 14:26:45 GMT</pubDate>
            <content:encoded><![CDATA[前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Universe creationを読みました。今回はArchitecture > Core functions > Table Creationを読みます。ドキュメントのバージョンは最新のv2.19 previewです。Table CrationYugabyteDBではユーザーにより実行されるテーブルの作成はYB-Masterのリーダーが実行する非同期APIによって管理される。YB-MasterはそのAPIでテーブルのスキーマと障害耐性を高めるために形成するRaftグループに所属するYB-Masterでのテーブル作成に必要な他の情報のレプリケーションが完了した段階でAPIの成功を返す。YB-Masterのリーダーがテーブル作成を実行するときは複数のステップが存在する。ValidationYB-Masterリーダーはテーブルスキーマの検証を行ない、指定された数のタブレットを作成する。これらのタブレットはこの段階ではYB-TServerには割り振られていない。ReplicationYB-MasterリーダーはYB-MasterのRaftグループにテーブルスキーマと新しく作成されたタブレット(この時点ではYB-TServerへの割り当て行なわれていない)の複製を行なう。この処理はYB-Masterリーダに障害が発生してもテーブル作成が成功することを保証する。Acknowledgementテーブル作成処理はYB-Masterリーダーに障害が発生しても処理を継続することが出来るため、この段階で非同期テーブル作成APIは成功を返す。ExecutionYB-Masterリーダーはそれぞれのタブレットをレプリケーションファクターとして指定された数だけYB-TServerに割り当てを行なう。このタブレットピアーの配置は指定された障害耐性を実現でき、またタブレットの割り当てがYB-TServerに均等に行なわれるように実行される。タブレットのYB-TServerへの割り当てはタブレットのレプリカが複数クラウド、リージョン、アヴェイラビリティゾーンをまたいで分散するといった追加の制約を満す必要がある。Continuous monitoringYB-Masterリーダーは全てのタブレットの割り当て処理を監視し、その実行状態と完了をユーザーが実行したAPIコールに対して応答する必要がある。Examplesテーブルが4ノードからなるYugabyteDBUniverseに作成される処理について考える。このときテーブルは16のタブレットと3つのレプリケーションファクターを持つとする。YB-Masterリーダーはスキーマを検証する。また16タブレット(合計48のタブレットピアー)を作成し、Raftを利用して過半数のYB-TServerにテーブルの作成に必要なデータを複製する。作成したタブレットをRaftグループを成すYB-TServerの中の指定された数のYB-TServer割り当て、リーダーの選出を行なう。このタブレットに属するキーに対する全てのリードとライトは、タブレットピアーのリーダーとRaftグループが責任を持つ。タブレットが割り当てられると長期に渡る障害か将来のロードバランシングが発生しYB-Masterにオーナーシップを変更されるまで、割り当て先のYB-TServerが所有する。タブレットリーダーをホストするYB-TServerの内の1台に障害が発生した場合、タブレットのRaftグループはI/Oを処理するために即座にリーダーエレクションを実行する。そのためYB-MasterはI/Oにおけるクリティカルパスになることはない。レプリケーション先となる候補を探す。この複製処理は段階的かつGracefulに実行される。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ChatGPT: SREがCustom instructions機能を利用する]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/08/22/204327</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/08/22/204327</guid>
            <pubDate>Tue, 22 Aug 2023 11:43:27 GMT</pubDate>
            <content:encoded><![CDATA[はじめに最近、ChatGPTからCustom instructions機能がリリースされました。Custom instructionsとは、ChatGPTの応答方法をより詳細に制御するカスタム命令を設定することができる機能です。ChatGPTの利用者にとって非常に便利な機能です。この機能により、ユーザーは特定の応答スタイルやフォーマットを要求することができるようになりました。これは、特定の業界や専門分野での使用など多岐にわたる用途に適応できるため、非常に有用です。めちゃくちゃ端的にかつ語弊を恐れずにいうと毎回、prompt を入力しなくてよくなるやつです。以前、公開したプロンプトに関するブログsyu-m-5151.hatenablog.comOpenAI CEOのSam Altman氏も、Custom instructionsのポストをしていましたので参考にしてみても良いかもしれません。damn i love custom instructions pic.twitter.com/su0BlttJF7— Sam Altman (@sama) 2023年7月22日  その上で私が利用してるものを公開します。What would you like ChatGPT to know about you to provide better responses?I'm a software developer and primarily use Golang. Depending on the application, I also utilize Shell Script, Terraform, and Ansible.I am a software developer and I like Cloud Native technologies such as Docker and Kubernetes.I like to develop, operate, and optimize systems.Technical advisor for several other companies.Please use Japanese.How would you like ChatGPT to respond?You are an AI programming assistant.Your response should be informative and logical.First, think STEP-BY-STEP, then describe your plan for what to build.Then output the code in a single code block.Keep your answers objective and concise, and use Markdown formatting.Be sure to include the name of the programming language at the start of the Markdown code block.Avoid enclosing your entire response in a triple backtick.また、 respondに信頼性に関する言及を求めていたのですが有益な情報が得られないので削除しておきました。まとめCustom instructions機能は、ChatGPTの応答をより細かく制御する強力なツールです。これにより、ユーザーは特定のニーズに合わせてモデルを調整することができ、より多様で効果的な結果を得ることが可能になります。この機能の導入により、ChatGPTはさらに多岐にわたる分野での応用が期待されます。この書籍はChatGPTによって達成された科学的な貢献や重要性を理解することができるのでオススメです。ChatGPTの頭の中 (ハヤカワ新書)作者:スティーヴン ウルフラム早川書房Amazon]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【ArgoCD🐙️】KubernetesのマルチテナントパターンとArgoCDの実践テナント設計]]></title>
            <link>https://hiroki-hasegawa.hatenablog.jp/entry/2023/08/18/110646</link>
            <guid>https://hiroki-hasegawa.hatenablog.jp/entry/2023/08/18/110646</guid>
            <pubDate>Fri, 18 Aug 2023 02:06:46 GMT</pubDate>
            <content:encoded><![CDATA[この記事から得られる知識この記事を読むと、以下を "完全に理解" できます✌️Kubernetesのマルチテナントパターンの種類マルチテナントパターンをArgoCDで実践する場合にオススメのパターンArgoCDのNamespacedスコープモードとClusterスコープモードArgoCDのProjectテナントがマニフェストのデプロイを制限する仕組みこの記事から得られる知識01. はじめに02. なぜArgoCDにマルチテナントが必要なのかシングルテナントの場合マルチテナントの場合03. Kubernetesのマルチテナントパターンマルチテナントパターンの一覧Clusters as-a-ServiceControl Planes as-a-ServiceNamespaces as-a-Serviceツール固有テナント04. ArgoCDでのテナントパターン実践の一覧04-02. Clusters as-a-Service の実践実Clusterテナントオススメしない理由04-03. Control Planes as-a-Service の実践仮想Clusterテナント - ★オススメした理由04-04. Namespaces as-a-Service の実践04-05. ツール固有テナントの実践ProjectテナントCLモード vs. NSモード05. CLモードなArgoCDCLモードなArgoCDとは実装方法AppProjectArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)ログインユーザー用ConfigMap (argocd-rbac-cm)オススメしない理由05-02. NSモードなArgoCD - ★★NSモードなArgoCDとは実装方法AppProjectArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)ログインユーザー用ConfigMap (argocd-rbac-cm)特にオススメした理由Projectテナント例の一覧テナント例1Namespace (プロダクトの実行環境別)、AppProject (プロダクトの実行環境別)オススメしなかった理由テナント例2 - ★Namespace (プロダクト別)、AppProject (プロダクトの実行環境別)オススメした理由テナント例3 - ★★Namespace (プロダクト別)、AppProject (プロダクトのサブチーム別)特にオススメした理由06. Projectテナントのデプロイ制限の仕組みマニフェストのデプロイ制限マニフェストをデプロイできる場合(🚫制限例1) 無認可のNamespaceでApplicationを作成しようとした場合(🚫制限例2) 無認可のAppProjectでApplicationを作成しようとした場合(🚫制限例3) 無認可のプロダクト用Clusterを指定しようとした場合(🚫制限例4) 無認可のNamespaceをデプロイ先に指定しようとした場合カスタムリソースのReconciliation制限ArgoCD系カスタムリソースをReconciliationできる場合(🚫制限例1) 無認可のNamespaceにReconciliationを実行しようとした場合07. おわりに謝辞01. はじめに『先日助けて頂いたアルトバイエルンです』画像引用元：Argo Projectさて最近の業務で、全プロダクトの技術基盤開発チームに携わっており、全プロダクト共有のArgoCD🐙のマルチテナント化を担当しました。プロダクトが稼働するKubernetes Clusterが10個以上あり、Clusterによっては複数のチームが合計100個以上のマイクロサービスを運用しています。このような大規模なマイクロサービスシステムがいくつもある状況下で、ArgoCDのマルチテナント設計の知見を深められたため、記事で解説しました。書きたいことを全部書いたところ、情報量がエグいことになってしまったので、気になる章だけでも拾って帰っていただけるとハッピーです🙏Kubernetesのマルチテナントパターン (3章)ArgoCDでのテナントパターン実践の一覧 (4章)ArgoCDのClusterスコープモードとNamespacedスコープモード (5章)Projectテナントのデプロイ制限の仕組み (6章)それでは、もりもり布教していきます😗02. なぜArgoCDにマルチテナントが必要なのかシングルテナントの場合そもそも、なぜArgoCDにマルチテナントが必要なのでしょうか。例えば、マニフェストのデプロイ先となるプロダクト用Cluster (例；foo、bar、baz) があると仮定します。ArgoCDをシングルテナントにする場合、各プロダクトチームの操作するApplicationを同じテナントに共存させることになります。この場合、単一のargocd-server (ダッシュボード) から全てのApplicationを操作できて便利です。しかし、プロダクト用Cluster数が増えていくにつれて、問題が起こり始めます。例えば、いずれかのプロダクトチームが誤ったApplicationを操作し、結果的に誤ったClusterにマニフェストをデプロイしてしまう可能性があります。もちろん、システムでインシデントを起こしてやろうという悪意を持った人が、誤ったClusterを意図的に選ぶ可能性もあります😈マルチテナントの場合その一方で、いい感じのマルチテナントにしたとします。プロダクトチームは、認可されたテナントに所属するApplicationにのみを操作でき、反対に無認可のテナントのApplicationは操作できません。これにより、誤ったプロダクト用Clusterにマニフェストをデプロイすることを防げます。03. Kubernetesのマルチテナントパターンマルチテナントパターンの一覧ArgoCDのテナント設計を実践する前に、Kubernetesにはどんなマルチテナントパターンがあるのでしょうか。Kubernetesのマルチテナントパターンは、以下に大別できます。マルチテナントパターン名         テナントの単位         テナント間のKubernetesリソース分離(分離できていれば ✅ )         ツール      Namespacedスコープリソース          Clusterスコープリソース      Clustersas a Service         実Clusterテナント         ✅         ✅         実Cluster管理ツール (AWS EKS、GCP GKE、Azure AKE、Kubeadm、など)      Control Planesas a Service         仮想Clusterテナント         ✅         ✅         仮想Cluster管理ツール (Kcp、tensile-kube、vcluster、VirtualCluster、など)      Namespacesas a Service         Namespaceテナント         ✅                  Namespaceを増やすだけなのでツール不要      ツール固有テナント         カスタムリソーステナント         ツールによる         ツールによる         ArgoCDのAppProject、CapsuleのTenant、kioskのAccount、KubeZooのTenant、など      "ソフトマルチテナンシー" と "ハードマルチテナンシー" といった分類方法もあります。この分類方法では、テナント間の分離度の観点で各マルチテナントを種別します。ソフトマルチテナンシーは、互いに信頼できる前提の上で、テナント間を弱く分離します。その一方で、ハードマルチテナンシーは、互いに信頼できない前提の上でテナント間を強く分離します。分離度がソフトとハードのいずれであるかに客観的な指標がなく、やや曖昧な種別になってしまうため、本記事の X as-a-Service の方が個人的には好みです♡♡♡The Kubernetes Book: 2023 Edition (English Edition)Multi-tenancy | KubernetesMulti-tenancy - EKS Best Practices GuidesClusters as-a-ServiceClusters as-a-Serviceは、テナントごとに独立したClusterを提供します。実Cluster管理ツールとして、AWS EKS、GCP GKE、Azure AKE、Kubeadm、などがあります。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?Control Planes as-a-ServiceControl Planes as-a-Serviceは、テナントごとに独立したコントロールプレーン (言い換えば仮想Cluster) を提供します。仮想Cluster管理ツールとして、Kcp、tensile-kube、vcluster、VirtualCluster、などがあります。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?Namespaces as-a-ServiceNamespaces as-a-Serviceは、テナントごとに独立したNamespaceを提供します。Namespaceを増やすだけなので、ツールは不要です。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?ツール固有テナントツール固有テナントは、テナントごとに固有の論理空間 (例：ArgoCDのAppProject、CapsuleのTenant、kioskのAccount、KubeZooのTenant、など) を提供します。ツールによっては、X as-a-Service も兼ねている場合があります。今回紹介するAppProjectはNamespaceテナントを兼ねており、ツール固有のテナント で解説しています。04. ArgoCDでのテナントパターン実践の一覧お待たせしました。ここからは、KubernetesのマルチテナントをArgoCDで実践し、おすすめのテナントパターンを解説していきます。なお、オススメするものを ★ としています。マルチテナントパターン名      テナント実践      ArgoCDがテナント間で独立 / 共有      テナント間のKubernetesリソース分離(分離できていれば ✅ )      オススメ    Namespacedスコープリソース      Clusterスコープリソース    Clustersas-a-Service      実Clusterテナント      独立      ✅      ✅          Control Planesas-a-Service      仮想Clusterテナント      独立      ✅      ✅      ★    Namespacesas-a-Service      Namespaceテナント      独立      ✅                ツール固有テナント      Projectテナント(CLモード)      共有      ✅                  Projectテナント(NSモード)      独立      ✅            ★★    How many do you need? - Argo CD Architectures Explained | Akuity以降の図の凡例です。ArgoCDの各コンポーネント (application-controller、argocd-server、dex-server、repo-server) と各リソース (Application、AppProject) を区別しています。04-02. Clusters as-a-Service の実践実Clusterテナント実Clusterテナントは、Clusters as-a-Serviceなテナントの実践であり、実Clusterをテナントの単位とします。後述の仮想Clusterと対比させるために、"実Cluster" と呼ぶことにします。各プロダクトチームは、実Clusterテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。オススメしない理由実Clusterテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見でオススメしませんでした。半年以内にアップグレードしないとサポートが切れるKubernetesクラスターが33個もあって、泣いちゃった— 長谷川 広樹 (俺です) (@Hiroki__IT) January 18, 2023  アーキテクチャ特性  メリット ⭕️                                                                                                                                                           デメリット ×                                                                                      デメリットの回避策                                                                                  拡張性                 -                                                                                                                                                                     テナントを増やすために実Clusterを用意する必要があり、作業量が多い。                              ➡︎  IaCツールで実Clusterを用意するようにすれば作業量を減らせるが、やっぱりとてもつらい😭       安全性(セキュリティ)        ClusterからClusterへの名前解決を不可能にすれば、他のテナントからの通信を遮断できる。                                                                                  -                                                                                                ➡︎  -                                                                                                   保守性                 ClusterスコープまたはNamespacedスコープなKubernetesリソースを他のテナントから分離できる。これらのKubernetesリソース (特にCRD) の変更が他のテナントに影響しない。  各テナントが、個別に実Clusterを保守しないといけない。(例：アップグレード、機能修正、など)  ➡︎  回避できず、とてもつらい😭                                                                           性能                  Clusterのハードウェアリソースを他のテナントと奪い合うことなく、これを独占できる。                                                                                     -                                                                                                ➡︎  -                                                                                                   信頼性                 テナントごとに実Clusterが独立しており、他の実Clusterから障害の影響を受けない。                                                                                        -                                                                                                ➡︎  -                                                                                    04-03. Control Planes as-a-Service の実践仮想Clusterテナント - ★仮想Clusterテナントは、Control Planes as-a-Serviceなテナントの実践であり、仮想Clusterをテナントの単位とします。各プロダクトチームは、仮想Clusterテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Using Argo CD with vclusters. Managing deployment to multiple… | by Daniel Helfand | Argo Projectオススメした理由仮想Clusterテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見で オススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                                                           デメリット ×                                                                                               デメリットの回避策                                                                                    拡張性                 テナントを増やすためにマニフェストで定義した仮想Clusterを用意するだけでよく、実Clusterを用意することと比べて作業量が少ない。                                          -                                                                                                         ➡︎  -                                                                                            安全性(セキュリティ)        仮想Cluster管理ツールの機能で、仮想ClusterからホストClusterへの名前解決を不可能にすれば、他のテナントからの通信を遮断できる。                                         -                                                                                                         ➡︎  -                                                                                                     保守性                 ClusterスコープまたはNamespacedスコープなKubernetesリソースを他のテナントから分離できる。これらのKubernetesリソース (特にCRD) の変更が他のテナントに影響しない。  各テナントが、個別に仮想Clusterを保守しないといけない。(例：アップグレード、機能修正、など)  ➡︎  仮想Clusterに関する知見を持つ組織であれば、各テナントで保守できる。                                    性能                  -                                                                                                                                                                     Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                         ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                信頼性                 テナントごとに仮想Clusterが独立しており、他の仮想Clusterから障害の影響を受けない。                                                                                    -                                                                                                         ➡︎  -                                                                                      04-04. Namespaces as-a-Service の実践Namespaceテナントは、Namespaces as-a-Serviceなテナントの実践であり、Namespaceをテナントの単位とします。後述の Projectテナント は二重のテナントを持ち、Namespaceテナントも兼ねています。そのため、ここではNamespaceテナントの解説は省略します。04-05. ツール固有テナントの実践ProjectテナントProjectテナントは、ツール固有テナントの実践であり、NamespaceとAppProjectをテナントの単位とします。Projectテナントは、二重のテナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持ち、"あらゆる面から" マニフェストのデプロイを制限します。特に、AppProjectはNamespaceスコープなカスタムリソースであり、自身に所属するApplicationを一括して制限します。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foo  # 自身に所属するApplicationを制限するspec: ...apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata:  name: infra-application  namespace: foospec:  # foo-tenantに所属する  project: foo-tenant  ...Argo CD in Practice: The GitOps way of managing cloud-native applications (English Edition)Projects - Argo CD - Declarative GitOps CD for Kubernetes.spec.scopeキーからも分かる通り、AppProjectはNamespacedスコープなカスタムリソースであり、任意のNamespaceを設定できます👍apiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  labels:    app.kubernetes.io/name: appprojects.argoproj.io    app.kubernetes.io/part-of: argocd  name: appprojects.argoproj.iospec:  group: argoproj.io  names:    kind: AppProject    ...  # Namespacedスコープなカスタムリソースであるとわかる  scope: Namespaced...  argo-cd/manifests/crds/appproject-crd.yaml at master · argoproj/argo-cd · GitHubExtend the Kubernetes API with CustomResourceDefinitions | KubernetesCLモード vs. NSモードArgoCDには、Clusterスコープモード と Namespacedスコープモード (以降、"CLモード" と "NSモード") があります。スコープモードに応じて、Projectテナントの設計方法が異なります。次の章からは、CLモードとNSモードの両方でProjectテナントを解説していきます。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetes05. CLモードなArgoCDCLモードなArgoCDとはCLモードなArgoCDの場合、各テナント間で共有のArgoCDを管理します例えば、Projectテナントとして、プロダクト別のNamespace (foo、bar、baz) とAppProject (foo、bar、baz) を用意します。別途、ArgoCD専用のNamespace (argocd) を用意し、ここに関連するKubernetesリソース (例；ConfigMap) を配置します。各プロダクトチームは、Projectテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesArgoCD: Multi-tenancy strategy. Introduction | by Geoffrey | Aug, 2023 | Medium実装方法AppProjectNSモードと同様にして、AppProjectに所属するApplicationによるマニフェストのデプロイを制限できます。例えば、以下のような実装になります。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # App-Of-Appsパターンの場合に使用する    - namespace: foo      server: "https://kubernetes.default.svc"    # プロダクト用Clusterに関する認可を設定する    - namespace: "*"      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com  # CLモードでは設定が必要である  sourceNamespaces:    - fooApplicationを操作するログインユーザーが、無認可のNamespaceやClusterをデプロイ先に指定できないように、.spec.destinationキーで制限しています。一方で後述のNSモードとは異なり、CLモードなArgoCDは任意のNamespaceのApplicationにアクセスできます。そのため、.spec.sourceNamespacesキーで、特定のNamespaceのApplicationがこのAppProjectに所属できないように、ApplicationのNamespaceを制限しています。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesProjects - Argo CD - Declarative GitOps CD for KubernetesArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)NSモードと同様にして、argocd-cmd-params-cmでは、ArgoCDの各コンポーネントのコンテナの引数を設定できます。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  # 専用のNamespaceを設定する  namespace: argocddata:  # CLモードでは設定が必要である  # 全てのNamespaceを指定したい場合は、ワイルドカードを設定する  application.namespaces: "*".application.namespacesキーは、argocd-serverとapplication-controllerの--application-namespacesオプションに相当します。一方での後述のNSモードとは異なり、CLモードなArgoCDは任意のNamespaceのApplicationにアクセスできます。--application-namespacesオプションで、任意のNamespaceにアクセスするための認可を設定できます。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetesargocd-cmd-params-cmの代わりに、例えば以下のようにPodに引数を直接渡しても良いです🙆🏻‍例えば、以下のような実装になります。apiVersion: v1kind: Podmetadata:  name: argocd-server  namespace: argocdspec:  containers:    - name: argocd-server      image: quay.io/argoproj/argocd:latest      args:        - /usr/local/bin/argocd-server        # コンテナ起動時の引数として        - --application-namespaces="*"  ...apiVersion: v1kind: Podmetadata:  name: argocd-application-controller  namespace: argocdspec:  containers:    - name: argocd-application-controller      image: quay.io/argoproj/argocd:latest      args:        - /usr/local/bin/argocd-application-controller        # コンテナ起動時の引数として        - --application-namespaces="*"  ...  Argocd application controller - Argo CD - Declarative GitOps CD for KubernetesArgocd server - Argo CD - Declarative GitOps CD for Kubernetesログインユーザー用ConfigMap (argocd-rbac-cm)NSモードと同様にして、argocd-rbac-cmでは、Applicationを操作するログインユーザーが、無認可のAppProjectやNamespaceに所属するApplicationを操作できないように制限します。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  # 専用のNamespaceを設定する  namespace: argocddata:  # デフォルトのロール  # @see https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv#L9-L16  policy.default: role:readonly  policy.csv: |    p, role:foo, *, *, foo/*/*, allow    p, role:bar, *, *, bar/*/*, allow    p, role:baz, *, *, baz/*/*, allow    g, foo-team, role:foo    g, bar-team, role:bar    g, baz-team, role:baz  scopes: "[groups]"認証済みグループ (foo-team、bar-team、baz-team) に対して、無認可のAppProject (foo、bar、baz) に所属するApplicationを操作できないように、認可スコープを制限しています。Casbin の記法を使用します。今回の実装例で使用したp (パーミッション) とg (グループ) では、以下を記法を使用できます👍apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: argocddata:  policy.default: role:readonly  policy.csv: |    # ロールとArgoCD系カスタムリソースの認可スコープを定義する    p, role:<ロール名>, <Kubernetesリソースの種類>, <アクション名>, <AppProject名>/<ApplicationのNamespace名>/<Application名>, <許否>    # 認証済みグループにロールを紐付ける    g, <グループ名>, role:<ロール名>  scopes: "[groups]"RBAC Configuration - Argo CD - Declarative GitOps CD for Kubernetesオススメしない理由CLモードなArgoCDのProjectテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見でオススメしませんでした。 アーキテクチャ特性  メリット ⭕️                                                                                   デメリット ×                                                                                                                                                                                                                      デメリットの回避策                                                                                                                                                            拡張性                 テナントを増やすためにNamespaceとAppProjectを用意するだけでよく、作業量が少ない。             -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                    安全性(セキュリティ)        NetworkPolicyでNamespace間の名前解決を不可能にすれば、他のNamespaceからの通信を遮断できる。   -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                             保守性                 ArgoCD用Clusterの管理者が単一のClusterを保守すればよい。(例：アップグレード、機能修正、など)  AppProjectはNamespacedスコープなカスタムリソースのため、ClusterスコープなKubernetesリソースを他のテナントと共有しないといけない。そのため、ClusterスコープなKubernetesリソース (特にCRD) の変更は全てのテナントに影響する。  ➡︎  ArgoCDのアップグレード時 (CRDの変更時) は、ついでにKubernetesもアップグレードしたい。新しいClusterを別に作成し、そこで新ArgoCDを作成すれば一石二鳥である。                 性能                  -                                                                                             Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                                                                                                                                                ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                                                                                        信頼性                 -                                                                                             ClusterまたはArgoCDで障害が起こると、これは全てのテナントに影響する。                                                                                                                                                            ➡︎  代わりにNodeやArgoCDを十分に冗長化して可用性を高めれば、影響を緩和できる。ただ、そもそもの影響範囲が大きすぎる😭                                           05-02. NSモードなArgoCD - ★★NSモードなArgoCDとはNSモードなArgoCDの場合、前述のCLモードとは異なり、各Projectテナント間で独立したArgoCDを管理します。例えば、Projectテナントとして、プロダクト別のNamespace (foo、bar、baz) とAppProject (foo、bar、baz) を用意します。各Projectテナントに、ArgoCDと関連するKubernetesリソース (例；ConfigMap) を配置します。各プロダクトチームは、Projectテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetes実装方法AppProjectCLモードと同様にして、AppProjectに所属するApplicationによるマニフェストのデプロイを制限できます。例えば、以下のような実装になります。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # App-Of-Appsパターンの場合に使用する    - namespace: foo      server: "https://kubernetes.default.svc"    # プロダクト用Clusterに関する認可を設定する    - namespace: "*"      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com# NSモードでは設定が不要である# sourceNamespaces:#   - fooApplicationを操作するログインユーザーが、無認可のNamespaceやClusterをデプロイ先に指定できないように、.spec.destinationキーで制限しています。前述のCLモードとは異なり、NSモードなArgoCDは自身が所属するNamespaceのApplicationのみにアクセスできます。そのため、.spec.sourceNamespacesキーでマニフェストのデプロイを制限する必要はありません。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesProjects - Argo CD - Declarative GitOps CD for KubernetesArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)CLモードと同様にして、argocd-cmd-params-cmでは、ArgoCDの各コンポーネントのコンテナの引数を設定できます。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# NSモードでは設定が不要である# application.namespaces: "*"前述の通り、.application.namespacesキーは、argocd-serverとapplication-controllerの--application-namespacesオプションに相当します。前述のCLモードとは異なり、NSモードなArgoCDは自身が所属するNamespaceのApplicationのみにアクセスできますそのため、.application.namespacesキーでNamespaceに関する認可を設定する必要はありませんもちろん、Podのコンテナ引数にも設定は不要です。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetesログインユーザー用ConfigMap (argocd-rbac-cm)CLモードと同様にして、argocd-rbac-cmでは、Applicationを操作するログインユーザーが、無認可のAppProjectやNamespaceに所属するApplicationを操作できないように制限します。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  # デフォルトのロール  # @see https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv#L9-L16  policy.default: role:readonly  policy.csv: |    p, role:app, *, *, app/*/*, allow    p, role:infra, *, *, infra/*/*, allow    g, app-team, role:app    g, infra-team, role:infra  scopes: "[groups]"認証済みグループ (app-team、infra-team) に対して、無認可のAppProject (app、infra) に所属するApplicationを操作できないように、認可スコープを制限しています。特にオススメした理由NSモードなArgoCDのProjectテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見で 特にオススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                  デメリット ×                                                                                                                                                                                                                      デメリットの回避策                                                                                                                                                            拡張性                 テナントを増やすためにNamespaceとAppProjectを用意するだけでよく、作業量が少ない。            -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                    安全性(セキュリティ)        NetworkPolicyでNamespace間の名前解決を不可能にすれば、他のNamespaceからの通信を遮断できる。  -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                             保守性                 単一のClusterを保守すればよい。(例：アップグレード、機能修正、など)             AppProjectはNamespacedスコープなカスタムリソースのため、ClusterスコープなKubernetesリソースを他のテナントと共有しないといけない。そのため、ClusterスコープなKubernetesリソース (特にCRD) の変更は全てのテナントに影響する。  ➡︎  ArgoCDのアップグレード時 (CRDの変更時) は、ついでにKubernetesもアップグレードしたい。新しいClusterを別に作成し、そこで新ArgoCDを作成すれば一石二鳥である。                 性能                  -                                                                                            Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                                                                                                                                                ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                                                                                        信頼性                 テナントごとにArgoCDが独立しており、他のArgoCDから障害の影響を受けない。                     Clusterで障害が起こると、これは全てのテナントに影響する。                                                                                                                                                                        ➡︎  代わりに、Nodeを十分に冗長化して可用性を高める。いずれかのインスタンスで障害が起こっても、正常なインスタンスでArgoCDが稼働できる。                         Projectテナント例の一覧NSモードなArgoCDを採用する場合、Projectテナント例を解説していきます。前述の通り、Projectテナントが二重テナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持つことに留意してください。なお、オススメするものを ★ としています。    テナント例(二重テナント)    オススメ  Namespace(第一テナント)    AppProject(第二テナント)  テナント例1      プロダクトの実行環境別      プロダクトの実行環境別          テナント例2      プロダクト別      プロダクトの実行環境別      ★    テナント例3      プロダクト別      プロダクトのサブチーム別      ★★    "管理チーム別" (今回でいうプロダクト別) というNamespaceの分割パターンは、様々な著名な書籍やブログで紹介されています👀  Amazon | Kubernetes in Action | Luksa, Marko | Software DevelopmentKubernetes best practices: Specifying Namespaces in YAML | Google Cloud Blogテナント例1Namespace (プロダクトの実行環境別)、AppProject (プロダクトの実行環境別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。この場合に、プロダクトの実行環境別にNamespace (dev、tes) とAppProject (dev、tes) を用意します。オススメしなかった理由テナント例1には、以下のメリデメがあります。独断と偏見でオススメしませんでした。 アーキテクチャ特性  メリット ⭕️                                                                                                                                     デメリット ×                                                                                                                                デメリットの回避策                                                                                       拡張性                 -                                                                                                                                               ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりやすい。その時点で、Projectテナントの増やせなくなる。  ➡︎  例えばAWS EKSの場合、Node数を増やしたり、Nodeのスペックを上げる。ただ、お金がかかる😭       安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可の実行環境別AppProjectに所属するApplicationを操作できないように制限できる。  -                                                                                                                                          ➡︎  -                                                                                                        保守性                 異なる実行環境に関するApplicationが共存しておらず、別のargocd-serverから操作することになるため、実行環境間の選択ミスが起こりにくい。            -                                                                                                                                          ➡︎  -                                                                                         テナント例2 - ★Namespace (プロダクト別)、AppProject (プロダクトの実行環境別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo、bar) 、プロダクトの実行環境別にAppProject (dev、tes) を用意します。オススメした理由テナント例2には、以下のメリデメがあります。独断と偏見で オススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                デメリット ×                                                                                                                                           デメリットの回避策                                                                                 拡張性                 ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりにくい。                                   -                                                                                                                                                     ➡︎  -                                                                                         安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可の実行環境別AppProjectを操作できないように制限できる。  -                                                                                                                                                     ➡︎  -                                                                                                  保守性                 -                                                                                                                          異なる実行環境に関するApplicationが共存しており、同じargocd-server (ダッシュボード) から操作することになるため、実行環境間の選択ミスが起こりやすい。  ➡︎  ダッシュボードにはApplicationのフィルタリング機能があるため、選択ミスを回避できる。 テナント例3 - ★★Namespace (プロダクト別)、AppProject (プロダクトのサブチーム別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo、bar) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。特にオススメした理由テナント例3には、以下のメリデメがあります。独断と偏見で 特にオススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                                       デメリット ×                                                                                                                                           デメリットの回避策                                                                                 拡張性                 ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりにくい。                                                          -                                                                                                                                                     ➡︎  -                                                                                         安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可のサブチーム別AppProjectに所属するApplicationを操作できないように制限できる。  -                                                                                                                                                     ➡︎  -                                                                                                  保守性                 -                                                                                                                                                 異なる実行環境に関するApplicationが共存しており、同じargocd-server (ダッシュボード) から操作することになるため、実行環境間の選択ミスが起こりやすい。  ➡︎  ダッシュボードにはApplicationのフィルタリング機能があるため、選択ミスを回避できる。 06. Projectテナントのデプロイ制限の仕組みそろそろ解説を読むのがしんどい方がいるのではないでしょうか。『君がッ、泣くまで、解説をやめないッ！』Projectテナントがマニフェストのデプロイをどのように制限するのかについて、例を挙げて解説します。ここでは、NSモードなArgoCDの "テナント例3" を採用し、以下のAppProjectを作成したと仮定します。Projectテナントが二重テナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持つことに留意してください。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  # appチーム  name: app  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # Namespace (foo) へのデプロイを許可する    - namespace: foo      server: "https://kubernetes.default.svc"      # プロダクト用Clusterに関する認可を設定する      # Namespace (app) へのデプロイを許可する    - namespace: app      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.comapiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  # infraチーム  name: infra  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # Namespace (foo) へのデプロイを許可する    - namespace: foo      server: "https://kubernetes.default.svc"    # プロダクト用Clusterに関する認可を設定する    # Namespace (infra) へのデプロイを許可する    - namespace: infra      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.comマニフェストのデプロイ制限プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。Projectテナントは、例えば 赤線 の方法で、マニフェストのデプロイを制限します。マニフェストをデプロイできる場合マニフェストを正しくデプロイする場合、Projectテナントはこれを制限しません。(1) argocd-serverは、argocd-cmd-params-cmからアクセスできるNamespaceを取得します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# 設定しないことで、argocd-serverは同じNamespaceにしかアクセスできなくなる。# application.namespaces: "*"(2) fooプロダクトのinfraチームが、argocd-serverを操作します。(3) argocd-serverは、argocd-rbac-cmからApplication操作に関する認可スコープを取得しますapiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  policy.default: role:readonly  policy.csv: |    p, role:app, *, *, app/*/*, allow    p, role:infra, *, *, infra/*/*, allow    g, app-team, role:app    g, infra-team, role:infra  scopes: "[groups]"(4) infraチームは、認可されたAppProjectに所属するApplicationを操作します。(5) infraチームは、Dev環境のfooプロダクト用ClusterのNamespace (infra) にマニフェストをデプロイできます。(🚫制限例1) 無認可のNamespaceでApplicationを作成しようとした場合例えば、fooプロダクトのinfraチームが無認可のNamespace (bar) でApplicationを作成しようとします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。namespace bar is not permitted in project 'infra-team'無認可のNamespaceでApplicationを作れてしまうと、そのApplicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます😈argo-cd/test/e2e/app_management_ns_test.go at v2.7.10 · argoproj/argo-cd · GitHub(🚫制限例2) 無認可のAppProjectでApplicationを作成しようとした場合例えば、fooプロダクトのinfraチームが、無認可のAppProject (app) でApplicationを作成しようとします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。Application referencing project 'app' which does not exist任意のAppProjectでApplicationを作成できてしまうと、そのApplicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます😈(🚫制限例3) 無認可のプロダクト用Clusterを指定しようとした場合例えば、fooプロダクトのinfraチームがApplicationを操作し、無認可のプロダクト用Cluster (bar-cluster) をデプロイ先として指定しようします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。application destination{https://bar-cluster.gr7.ap-northeast-1.eks.amazonaws.com infra} is not permitted in project 'infra-team'任意のClusterをデプロイ先に指定できてしまうと、Applicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます😈argo-cd/util/argo/argo_test.go at v2.7.10 · argoproj/argo-cd · GitHub(🚫制限例4) 無認可のNamespaceをデプロイ先に指定しようとした場合例えば、fooプロダクトのinfraチームがApplicationを操作し、無認可のNamespace (app) をデプロイ先に指定しようします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。application destination{https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com app} is not permitted in project 'infra-team'任意のNamespaceをデプロイ先に指定できてしまうと、そのApplicationから無認可のNamespaceにマニフェストをデプロイできてしまいます😈argo-cd/util/argo/argo_test.go at v2.7.10 · argoproj/argo-cd · GitHubargocd-serverとapplication-controllerでデプロイできるKubernetesリソースの種類 (.spec.clusterResourceWhitelistキー、.spec.namespaceResourceWhitelistキー、など)repo-serverでポーリングできるリポジトリ (.spec.sourceReposキー)apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  clusterResourceWhitelist:    - group: "*"      kind: "*"  namespaceResourceWhitelist:    - group: "*"      kind: "*"  sourceRepos:    - "*"  ..."Projectテナントによるマニフェストのデプロイ丸ごとの制限" という観点でテーマが異なるため、本記事では言及しませんでした🙇🏻‍  Projects - Argo CD - Declarative GitOps CD for KubernetesDeclarative Setup - Argo CD - Declarative GitOps CD for KubernetesカスタムリソースのReconciliation制限プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。Projectテナントは、例えば 赤線 の方法で、ArgoCD系カスタムリソースに対するapplication-controllerのReconciliationを制限します。ArgoCD系カスタムリソースをReconciliationできる場合正しいNamespaceに対してReconciliationを実行する場合、Projectテナントはこれを制限しません。(1) application-controllerは、argocd-cmd-params-cmから自身がアクセスできるNamespaceを取得します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# 設定しないことで、application-controllerは同じNamespaceにしかアクセスできなくなる。# application.namespaces: "*"(2) application-controllerは、同じNamespaceに所属するArgoCD系カスタムリソースに対して、Reconciliationを実行します。(🚫制限例1) 無認可のNamespaceにReconciliationを実行しようとした場合例えば、application-controllerがReconciliationの対象とするNamespaceを選ぼうとしているとします。すると、application-controllerは内部で検証メソッドを実行し、無認可のNamespace (bar) は選ばないようにします。argo-cd/controller/appcontroller_test.go at v2.7.10 · argoproj/argo-cd · GitHub07. おわりにKubernetesのマルチテナントパターンとArgoCDでのテナントパターンの実践をもりもり布教しました。あらゆる面からマニフェストのデプロイを制限してくれる、Projectテナントの素晴らしさが伝わりましたでしょうか。KubernetesのマルチテナントパターンをArgoCDでどう実践するべきか、について困っている方の助けになれば幸いです👍謝辞本記事のタイトルは、私が崇拝しているドメイン駆動設計の書籍 "実践ドメイン駆動設計" から拝借しました🙏また、ArgoCDでのテナントパターンの収集にあたり、以下の方からの意見も参考にさせていただきました。@toversus26 さんこの場で感謝申し上げます🙇🏻‍]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day5]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_5</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_5</guid>
            <pubDate>Wed, 16 Aug 2023 13:49:19 GMT</pubDate>
            <content:encoded><![CDATA[前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > YB-Master serviceを読みました。今回はArchitecture > Core functions > Universe creationを読みます。ドキュメントのバージョンは最新のv2.19 previewです。Universe creationYugabyteDBのユニバース作成は複数のステップを含む。Start YB-MastersYBユニバース作成の最初のステップはレプリケーションファクターで指定された数だけYB-Masterを作成することである。作成されたYB-Masterはそれぞれを認識している。YB-Masterはユニバース内でユニークなID(UUID)をそれぞれに割り当て、それぞれを認識しあったあとにリーダーエレクションを実行する。このステップの終りにYB-Masterの中のひとつがリーダーとして確立される。Start YB-TServersノードの数だけYB-TServerを起動し、それぞれにマスターのアドレスを渡す。それぞれのYB-TServerはマスターにハートビートを送信し、正常に動作していることを確認する。ハートビートはYB-TServerが現在ホストしているタブレットとその負荷情報についても通信するが、この時点ではタブレットにデータは登録されていない。Examples4ノードからなるYBユニバースにテーブルを作成する場合について考える。テーブルのレプリケーションファクターは3とする。3つのマスターがcreateモードで起動される。これはマスターがすでに起動しているために発生するエラーを防ぐために明示的に実行される。リーダーエレクションを実行し、リーダーを選出する。YB-TServerが起動し、全てのYB-TServerがマスターにハートビートを送信する。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[PaLM API for textで作るGoogle Cloudコストチェッカー]]></title>
            <link>https://sreake.com/blog/google-cloud-cost-check-with-palm-api-for-text/</link>
            <guid>https://sreake.com/blog/google-cloud-cost-check-with-palm-api-for-text/</guid>
            <pubDate>Wed, 16 Aug 2023 05:06:08 GMT</pubDate>
            <content:encoded><![CDATA[前段 Sreake事業部の橋本です。 Generative AIをSRE活動に活用する場合に大きく分けて以下のような2ケースが考えられます。これまで1つめのtoil削減の実装をGenerative AIに含まれる学習デー […]The post PaLM API for textで作るGoogle Cloudコストチェッカー first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[WezTerm で快適な WSL2 環境にする]]></title>
            <link>https://blog.1q77.com/2023/08/wezterm-on-windows/</link>
            <guid>https://blog.1q77.com/2023/08/wezterm-on-windows/</guid>
            <pubDate>Sat, 12 Aug 2023 11:07:01 GMT</pubDate>
            <content:encoded><![CDATA[家の自分用 Laptop はずっと Linux を使ってきましたが、数か月前に Inspiron 14 に買い替えたタイミングで Ubuntu 22.04 にしてからやっぱり不便だなあとも思っていました。(InputMethod の切]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SREからPlatform Engineerへの拡大 というタイトルで登壇しました]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/08/10/150412</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/08/10/150412</guid>
            <pubDate>Thu, 10 Aug 2023 06:04:12 GMT</pubDate>
            <content:encoded><![CDATA[概要Cloud Operator Days Tokyo 2023 で SREからPlatform Engineerへの拡大 というテーマでの登壇を果たしました。オンデマンド配信なのでいずれ見れるようになると思います。今回のサブタイトルは【運用の新時代】とし、それにちなんでメインタイトルを考えました。資料の作成過程で、話したい内容がどんどんと増えてきてしまい、20分という限られた時間での発表が一番の課題となりました。内容の整理に際して、具体と抽象 ―世界が変わって見える知性のしくみ という本を参照し、大変役立ちました。具体と抽象作者:細谷 功dZERO（インプレス）Amazon資料このブログでは、Cloud Operator Days Tokyo 2023での登壇内容をまとめております。資料作成時に参照したさまざまな参考情報も掲載していますので、読者の皆様が別途情報を探す手間を省けるよう心掛けました。ぜひ、本ブログをご活用ください。文字多くて分かりにくいのは分かってます。脳内整理はできているのですが資料を読みやすくすると20分に何も収まらず...。 speakerdeck.com参考文献O’Reilly Japan – SRE サイトリライアビリティエンジニアリングあなたらしくSREO’Reilly Japan – サイトリライアビリティワークブックO’Reilly Japan – SREの探求SRE at Google: How to structure your SRE team | Google Cloud BlogレトロスペクティブガイドWhat Is Platform Engineering?Top Strategic Technology Trends for 2023: Platform EngineeringMaking the Business Case for a Dedicated Platform Engineering TeamSRE NEXTPlatform Engineering Meetupチームトポロジー　価値あるソフトウェアをすばやく届ける適応型組織設計The History of DevOps ReportsEffective DevOpsオブザーバビリティ・エンジニアリングWebエンジニアのための監視システム実装ガイド]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AWS Control Tower 徹底調査]]></title>
            <link>https://sreake.com/blog/learn-about-aws-control-tower/</link>
            <guid>https://sreake.com/blog/learn-about-aws-control-tower/</guid>
            <pubDate>Thu, 10 Aug 2023 05:52:55 GMT</pubDate>
            <content:encoded><![CDATA[AWS Control Tower とは AWS Control Tower とは Landing Zone を実装するための AWS のマネージドサービスです。統制を取りつつマルチアカウントを管理する仕組みです。 La […]The post AWS Control Tower 徹底調査 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[HarnessでGKEクラスタにCDパイプラインを構築する]]></title>
            <link>https://qiita.com/yokoo-an209/items/57e2e4c00394c9da85f7</link>
            <guid>https://qiita.com/yokoo-an209/items/57e2e4c00394c9da85f7</guid>
            <pubDate>Thu, 10 Aug 2023 05:22:38 GMT</pubDate>
            <content:encoded><![CDATA[はじめに実務においてHarnessを使用する機会があったので、お試しがてらGKEクラスタ上にCDパイプラインの構築を行います。（※なお、今回はHarnessの使用に焦点を当てているため、CIの実…]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[2023年8月10日現在 でLunarVim と Copilot.lua でのマルチラインサポートの改善方法 ]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/08/10/021934</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/08/10/021934</guid>
            <pubDate>Wed, 09 Aug 2023 17:19:34 GMT</pubDate>
            <content:encoded><![CDATA[github.comLunarVimユーザーとして、私はNeovimでcopilot.luaを頻繁に利用しています。しかし、マルチラインのサポートに関してはいくつかの課題がありました。もっというとどこかのタイミングでCopilotが一行ずつしかサジェストされなくなりました。この問題に対して、一部のコードを修正することで、この課題を解決する方法を見つけました。問題点Copilot.lua(Copilot.vimも同様に)の中のagent.jsには、マルチライン入力の停止点を示すコード h.stop=["\n"] が含まれています。この設定により、一部の場面でマルチラインサポートが期待通りに動作しないことがありました。解決方法私が採用した方法は、このh.stop=["\n"]をh.stop=["\n\n\n"]に変更することです。この小さな変更により、マルチラインのサポートが大幅に向上します。以下のコマンドを実行することで、この変更を簡単に適用することができます。MAC でのsed 利用なのでこのようなコマンドになります。各環境で合わせていただきたいです。sed -i '' 's/h\.stop=\["\\\\n"\]/h\.stop=\["\\\\n\\\\n\\\\n"\]/' ~/.local/share/lunarvim/site/pack/lazy/opt/copilot.lua/copilot/dist/agent.js変更が正しく適用されたかどうかを確認するには、以下のコマンドを実行します。grep -o '.\{30\}h.stop=\[.\{30\}' ~/.local/share/lunarvim/site/pack/lazy/opt/copilot.lua/copilot/dist/agent.js結果この変更を適用した後、マルチラインサポートが明らかに向上しました。興味があれば最初に紹介したIssue に動画が添付されていたのでご覧ください。LunarVimとCopilot.luaの組み合わせは非常に強力ですが、小さな調整によりさらに快適に使うことができます。このハックが他のユーザーにも役立つことを願っています。後日談この変更を適用した後でマルチラインサポートは向上したのですが一部条件ではまだ、vscodeのような挙動ができません。]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Cloud Run を活用した Pull Request 単位での Ad hoc 開発環境作成]]></title>
            <link>https://sreake.com/blog/pull-request-based-adhoc-env/</link>
            <guid>https://sreake.com/blog/pull-request-based-adhoc-env/</guid>
            <pubDate>Wed, 09 Aug 2023 03:10:37 GMT</pubDate>
            <content:encoded><![CDATA[きっかけ 開発時、feature ブランチの Pull Request （以下、PR）ごとに実行環境が準備されると便利だよねというところから、PR ごとに開発環境を構築される仕組みを作ることになりました。 使用技術スタッ […]The post Cloud Run を活用した Pull Request 単位での Ad hoc 開発環境作成 first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraform 1.6 で追加されそうな terraform test コマンドを試してみる]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/tf-1_6_alpha-test-command</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/tf-1_6_alpha-test-command</guid>
            <pubDate>Fri, 04 Aug 2023 09:00:00 GMT</pubDate>
            <content:encoded><![CDATA[最近 Terraform 1.6 のアルファ版がリリースされています。https://github.com/hashicorp/terraform/releases/tag/v1.6.0-alpha20230719https://github.com/hashicorp/terraform/releases/tag/v1.6.0-alpha20230802リリースノートを読んでみると terraform test なるコマンドが含まれているではありませんか！これは試してみるっきゃないと思い、ちょっとだけ触ってみました。!この記事はあくまで v1.6.0-alpha202308...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day4]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_4</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_4</guid>
            <pubDate>Thu, 03 Aug 2023 14:48:34 GMT</pubDate>
            <content:encoded><![CDATA[前回](https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_3)からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > YB-TServer serviceを読みました。今回はArchitecture > Key Concepts > YB-Master serviceを読みます。ドキュメントのバージョンは最新のv2.19 previewです。YB-Master serviceYB-Masterサービスはテーブルやそのタブレットの場所、ユーザー・ロールの権限といったシステムのメタデータとレコードの管理を行っている。それに加えYB-Masterはロードバランシングやレプリケーションの開始といったバックグラウンドオペレーションの管理や、テーブルのCREATEやALTER、DROPといった様々な管理オペレーションの責任を持つ。YB-MasterはRaft Groupを組むことで高可用性を実現し、またテーブルに対するI/Oの単一障害点にならない。Functions of YB-MasterYB-Masterはシステムの重要な機能を複数持っている。Coordination of universe-wide administrative operationsCREATE TABLEやALTER TABLE、DROP TABLEといったユーザーからのリクエスト処理やバックアップの実行などUniverseをまたぐオペレーション実行の調整を担当している。YB-Masterではこれらのオペレーションがテーブルを保持するYB-TServerの状態に関わらず、全てのテーブルに伝搬されることを保証する。YugabyteDBは分散システムのため、Universeをまたぐ処理中にYB-TServerに障害が発生し一部のタブレットへの適用に失敗してもオペレーションの結果に問題が発生しないことが重要だからである。Storage of system metadataそれぞれのYB-Masterではネームスペースやテーブル、ロール、パーミッション、YB-TServerへ割り当てたテーブル情報を含むシステムメタデータを保存している。これらのシステムレコードはYB-Masterを対象にRaftグループを組みレプリケーションすることで冗長性を実現している。またシステムレコードはYB-Masterが管理するDocDBに保存される。Authoritative source of tablet assignments to YB-TServersYB-Masterは全てのテーブルとそれらをホストするYB-TServerの情報を保存している。一般のクライアントではそれらの情報はクライアントからクエリレイヤなどを通して取得された上で、クライアントにメタデータを返しデータアクセスが行なわれる。一方でスマートクライアントではYB-Masterに保存されたメタデータを利用して特定のYB-TServerが保持するタブレットやキャッシュを利用することが出来るため、データアクセス時のネットワークをまたぐ通信を減らすことができパフォーマンスを高めることができる。Background operationsいくつかのオペレーションはUniverseのライフタイムを通してバックグラウンドで行なうことで、フォアグラウンドのRead/Writeに影響を与えずに実行することが出来る。Data placement and load balancingYB-MasterのリーダーはCREATE TABLE時にタブレットの初期配置をYB-TServerをまたいで行なう。そのときにユーザー定義のデータ配置制約を強制し均一な読み込みを保証する。Universeのライフタイム中のノード追加や障害が発生しても、負荷分散を継続しデータ配置の制約を自動的に適用する。Leader balancing複数のYB-TServerに配置されたタブレットへのアクセスがUniverseをまたいで分散されることを保証している一方で、YB-Masterは対象となるノード1間でそれぞれのノードが同じ数のtablet-peer leader2をもつことを保証する。Rereplication of data on extended YB-TServer failureYB-Masterは全てのYB-TServerからハードビートシグナルを受け取ることでYB-TServerの死活監視を行なっている。そしてYB-MasterはYB-TServerの異常を検知したときに、どれぐらいのあいだYB-TServerが異常であったかを追跡する。閾値を超えると、YB-Masterは障害中のYB-TServerに配置されていたタブレットを再配置するYB-TServerを探し、レプリケーションを実行する。レプリケーションはYB-Masterリーダーに抑制された状態で実行されるため、Universeのフォアグラウンドオペレーションには影響をおよぼさない。Raft Groupのリーダーになれるノード↩↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[K8sGPT Deep Dive というタイトルで登壇しました #CNDF]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/08/03/155326</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/08/03/155326</guid>
            <pubDate>Thu, 03 Aug 2023 06:53:26 GMT</pubDate>
            <content:encoded><![CDATA[概要CloudNative Days Fukuoka 2023というイベントに『K8sGPT Deep Dive KubernetesクラスタのAI駆動型分析について』というタイトルで登壇しました。クラウドネイティブとAIを組み合わせることの深い洞察を共有することができ、私自身がエンジニアとして働くなかで、K8sGPTの最新の進化とその可能性について詳しく語る機会はなかなかなく、この経験を活かしていきたい。資料を作っている中で話したいことがどんどん増えていってめちゃくちゃ困った。また、その中でAIOpsについても触れることができ、非常に充実した時間でした。AIOpsはAIと運用管理の統合を指し、それによりIT運用の効率化や自動化が可能となります。その重要性と可能性を伝えることができたので良かった。登壇が終わった今でも、K8sGPTやAIOpsについてさらに知識を深め、クラウドネイティブの世界にどのように最適化された解決策を提供できるかについて考え続けています。参加者の皆さんからもたくさんのフィードバックを頂き、今後の研究や開発の参考になりました。私がこのプレゼンテーションのために読み込んだ複数の本の中で、特に皆さんにお勧めしたい一冊を挙げるとすれば、「大規模言語モデルは新たな知能か――ChatGPTが変えた世界」だと言えます。なぜなら、専門家でも初心者でも、難解な数学を使わずに重要な概念を理解できるように作られているからです。大規模言語モデルは新たな知能か　ＣｈａｔＧＰＴが変えた世界 (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazon資料登壇資料になります。このブログの目的は参考資料をいちいち探さなくていいようにありますのでご活用ください。 speakerdeck.com参考文献公式ページ | K8sGPTGitHub | K8sGPTGitHub | K8sGPT OperatorDocs | K8sGPTOperator patternK8sGPT OperatorHow to Get Started With AIOpsPrompt Engineering Guideオブザーバビリティ・エンジニアリングKubernetes基盤を自律的に支えるController化の実装Tips / forkwell-202303-amsy810-k8sAutomation and Machine Learning with Site Reliability EngineeringTEMPLE: Six Pillars of ObservabilityAI時代に向けたクラウドにおける信頼性エンジニアリングの未来構想 / DICOMO2022 6A-1大規模言語モデルは新たな知能か――ChatGPTが変えた世界 (岩波科学ライブラリー)ChatGPTの頭の中 (ハヤカワ新書)言語の本質　ことばはどう生まれ、進化したかAI vs. 教科書が読めない子どもたち【ITIL4公認】ITIL 4の基本 図解と実践]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day3]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_3</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_3</guid>
            <pubDate>Wed, 02 Aug 2023 16:13:24 GMT</pubDate>
            <content:encoded><![CDATA[YugabyteDBのドキュメントを全部読む Day3前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > Universeを読みました。今回はArchitecture > Key Concepts > YB-TServer serviceを読みます。ドキュメントのバージョンは最新のv2.19 previewです。それはそれとして技術系の単語をカタカナ表記で誤魔化していて、体系的に学んでいないことがバレてしまう。特にストレージまわりが分からない……YB-TServer serviceYB-TServer(YugabyteDB Tablet Servcer)はユーザからの受けつけたYugabyteDBクラスタへのリクエストのI/Oの処理をする。テーブルのデータは一つ以上のTablet peerに分割(シャーディング)される。peerの数はレプリケーションファクターによって決定される。YB-TServerは一つ以上のTablet peerをホストする。Tablet peerはRaftグループを形成してグループ間でデータの複製を行ない、タブレットはYB-TServer上で最大の効率になるように管理される。Server-global block cacheブロックキャッシュは一つTB-TServer上の異なるタブレット間で共有される。YB-TServerのメモリ効率は一つのテーブルからの読み込みが多いほど最適化される。Space AmplificationYugabyteDBではSize-tired Compactionというライトアンプリフィケーション1が小さい圧縮方式を利用している。Size-tired Compactionはスペースアンプリフィケーション2が大きいという問題があるが、YugabyteDBではテーブルは複数のタブレットに分割され、タブレット間でのConcurrent Compactionは特定の最大値まで絞られるため問題になりにくい。YugabyteDBでは凡そ10-20%のスペースアンプリフィケーションにおさまる。つまりSize-tired Compaction一単位が扱うデータ量を小さく(タブレット化)して、同時に実行される圧縮処理数を絞ることで特定のタイミングで圧縮に使用されるストレージ容量を抑えているということ？Throttled compactionsYB-TServerではタブレット間で実行される圧縮処理の同時実行数を制限することで、圧縮処理が多量のリソースを占有することを防いでいる。この機能は圧縮されるファイル同士のサイズを比べ、実行される圧縮処理が妥当であることを確認することで実現されている。Small and large compaction queuesYB-TServerでは圧縮処理を大きい圧縮処理と小さい圧縮処理に分けて優先度を決めることで、I/Oが大きな場合でもシステムの機能を保っている。YugabyteDBでは圧縮処理数を制限することに加え、様々な最適化を実行することで圧縮処理の影響を最小化している。Manual compactionYugabyteDBではyb-admin utilityのcompact_tableコマンドにより、任意のタイミングでテーブルに対して圧縮を実行することが出来る。この方法はデータが新しく書き込まれない場合や、DDLやTTLの超過によるデータ削除時によりデータが断片化したときに有効である。Statistics-based full compactions to improve read performanceYugabyteDBでは読み込まれたkey-valueペアをDocDBレベルで監視している。監視対象となる時間軸はauto-compact-stat-window-secondsで管理されている。YugabyteDBがデータ読み込み時に多量の廃棄されたデータのスキップを検知した場合、full compactionがトリガーされ不要なキーの削除が行なわれる。Full compactionがトリガーされる詳細な条件は対象の時間軸で以下が満された時である。廃棄されたキーとアクティブなキーが読まれる割り合いがauto-compact-percent-obsoleteで定義された閾値を超たとき。廃棄されたキーの読み込みauto-compact-min-obsolete-keys-foundで定義された閾値を超たとき。この機能はTTLを設定したテーブルと互換性があり、TTL file expirationが有効なテーブルではスケジュールされた圧縮を実行しない。Scheduled full compactionsYugabyteDBでは全てのデータに対するデータ圧縮をスケジュール実行することが出来る。スケジュール実行はscheduled-full-compaction-frequency-hoursとscheduled-full-compaction-jitter-factor-percentageのフラグで管理される。この機能は大量のDELETEとUPDATEを定常的に実行するワークロードでのパフォーマンスとディスクスペースの再割り当てに有効である。スケジュール化したデータ圧縮はTTLと互換しているが、TTL file expirationとは互換していない。つまりスケジュールされた圧縮は実行されない。Server-global memstore limitServer-global memstore limitは一つのYB-TServer上のタブレット間でシェアされるメモリサイズを追跡し、強制する。この機能はタブレット間の書き込みに偏りがある場合に有効である。一つのテーブルに書き込みが集中しているばあい、メモリ制限以上のメモリを割り当てることでパフォーマンスを向上させることが出来る。Auto-sizing of block cache and memstoreBlock Cacheとmemstoreは何れも多量のメモリを使用している。これらはtablet-peer間で共有されるリソースのため、メモリ管理とこれらのコンポーネントの様々な環境に合せたサイジングを容易にしている。YB-TServerでは自動で特定の割合のメモリをBlock CacheとMemstoreに割り当てる。Distributing tablet load uniformly across data disks複数のSSDを利用するハードウェアでは、テーブルのデータ(SSTable)とWALはテーブル毎に利用可能なディスクに均等に分散される。このストライピングと呼ばれる負荷分散は、それぞれのディスクがそれぞれのテーブルの負荷を均等に処理することを保証する。SSDで実際に書き込んだデータより書き込み量が増幅する現象。もちろんライトアンプリフィケーションが小さいほうが望ましい。↩↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[gonew を使って Go プロジェクトのテンプレートを活用する]]></title>
            <link>https://zenn.dev/kou_pg_0131/articles/gonew-introduction</link>
            <guid>https://zenn.dev/kou_pg_0131/articles/gonew-introduction</guid>
            <pubDate>Tue, 01 Aug 2023 12:34:36 GMT</pubDate>
            <content:encoded><![CDATA[公式から gonew というツールが公開されました。https://go.dev/blog/gonewhttps://pkg.go.dev/golang.org/x/tools/cmd/gonewgonew を使うことで既存の Go プロジェクトのテンプレートを使い、素早く開発を始めることができるようになります。現時点で公式からは以下のテンプレートが公開されています。hellohelloserver他にも Google Cloud チームや Service Weaver チームからも様々なテンプレートが公開されています。GoogleCloudPlatform/go-...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day2]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_2</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_2</guid>
            <pubDate>Wed, 26 Jul 2023 15:03:13 GMT</pubDate>
            <content:encoded><![CDATA[YugabyteDBのドキュメントを全部読む Day2前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Design goalsを読みました。今回はArchitecture > Key Concepts > Universeを読みます。UniverseYugabyteDBは耐久性とスケーラビリティを兼ねそなえた分散データベースを達成するために、Universe1と呼ばれるノードのグループを持っている。Universeはビジネス要件やレイテンシの兼ね合いでシングルゾーン、単一リージョンマルチゾーン、マルチリージョン、同期・非同期レプリケーションなどを選択することが出来る。UnivereはClusterと表現されることもある。データの構成Universeは一つ以上のネームスペースを持つことができ、またネームスペースは一つ以上のテーブルを持つことができる。YugabyteDBではUniverse上に存在するノードにまたがって保持されるテーブルを設定に従って、シャーディングし、レプリケーション、ロードバランシングを行なう。YugabyteDBはノードやディスク、ゾーンなどに発生した障害に自動で対応し、必要であればデータを新規に分散、レプリケーションを行なう。ネームスペースはYSQLではデータベースに対応し、ほかのDBにおけるネームスペースに対応する2。YCQLではキースペースに対応し、Cassandraのキースペースに対応している。サービスコンポーネントUniverseはYugabyteDB Tablet Server(YB-TServer)とYugabyteDB Master Server(YB-Master)の二つで構成されている。YB-MasterとYB-TServerはRaftにより分散されており、高可用性を達成している。YB-Tserverはテーブルを始めとしたユーザーデータの保存、提供を担当する。YB-Masterはシステムのメタデータを管理し、システム全体のテーブルに対するDDLやメンテナンスの実行、ロードバランシングといったオペレーションを管理する。UniverseとClusterUniverseは一つのプライマリクラスタとゼロ個以上のレプリカクラスタによって構成されている。プライマリクラスタプライマリクラスタはRead/Write両方の実行と、プライマリクラスタ内のノード間の同期的なレプリケーションを担当する。リードレプリカクラスタリードレプリカクラスタはRead処理のみを実行する。Write処理は自動的にプライマリクラスタにルーティングされる。リードレプリカクラスタを利用することで、地理的に分散したデータに対する読み取りの遅延を小さくすることができる。データはプライマリクラスタから非同期的にとりこまれる。これはRaftの書き込みには関与しないRaftオブザーバとして機能する。GoogleのCloud Spannerでも同様にUniverseと呼ばれている↩PostgreSQLではSchemaの裏側に存在するデータ構造↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[YugabyteDBのドキュメントを全部読む Day1]]></title>
            <link>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_1</link>
            <guid>https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs_1</guid>
            <pubDate>Tue, 25 Jul 2023 15:01:52 GMT</pubDate>
            <content:encoded><![CDATA[Day1最近Twitter改めXで「俺はDBのドキュメント端から端まで読んで強くなった」というX's1を複数みかけました。周りのエンジニアに一歩差をつける方法として、フレームワークやミドルウェアやライブラリのドキュメントを最初から最後までちゃんと読む、というのがあって、これはマジでコスパ抜群です。— 徳永広夢 (@tokuhirom) July 21, 2023 確かに私のRedisはこれ。 https://t.co/2y1E01aLGw— maru (@maruloop) July 22, 2023 私のMySQLもこれ。 https://t.co/BxiOjeQVPk— yoku0825 (@yoku0825) July 22, 2023 俺のpostgresqlもこれ。 https://t.co/URRjyXCpGI— そーだい@初代ALF (@soudai1025) July 22, 2023 PostgreSQL系NewSQLで最強になりたいのでYugabyteDBのドキュメントを順番に読んで行きます。ドキュメントはv2.19に対応したものです。手始めにArchitectureの一番先頭にあるDesign goalsから読みはじめます。Design goalsYugabyteDBは以下を達成することを目標としている。1. 分散トランザクションを提供しながら強い一貫性を保証する。2. Query APIを再発明せず、既存のクエリ言語への互換を達成する。3. 高いパフォーマンスを保証する。4. 地理的に分散したデプロイを可能にする。5. Cloud Native Databaseとしてデザインする。一貫性分断耐性YugabyteDBはCAPの定理で言えばCPを中心に高い可用性を供えたデータベースネットワーク分断などを起因とするSplit BrainはRaft Group内であたらしいリーダーを選出することで対応している。YugabyteDBではLeader Leaseという障害が発生しても常に一つのリーダが存在することを保証する仕組みを実装している。直列化可能性single-row Linearizable writeをサポートしている。ACIDトランザクションYugabyteDBではSeriarizable、Repetable Read、Read Committed Isolationの三つの分離レベルをサポートしている。YSQL APIではこれら3つの分離レベルをサポートしているが、YCQLではRepeatable Readのみに対応している。Query APIYugabyteDBではYSQLとYCQLという2種類のQuery APIをサポートしている。YSQLYSQLはPostgreSQLに互換したAPIでPostgreSQLのクエリレイヤを再利用している。新しい変更は互換性を崩さない。YSQLは新しいPostgreSQLに互換しつづけることを目標としている。YCQLYCQLはCassandraのクエイ言語から派生した半リレーショナルなクエリ言語で、Webスケールな膨大なwriteに対応してスケールし素早いデータ取得を目標としている。パフォーマンスC++で実装されているため高いパフォーマンスと巨大なHeap(RAM)をCacheとして利用できる。SSDとNVMeに最適化している。高いWriteスループットとクライアントの同時実行性、高いデータ密度、増加し続けるデータへの対応を目標としている。地理的分散Zone、Multi Region、Multi Cloudいずれにも対応している。これに対応するために、ノード障害やトラヒックのルーティングなどに対応できる必要がある。クラウドネイティブアーキテクチャパブリッククラウドやオンプレミスで利用される一般てきなハードウェアで利用可能にする。原子時計のような特別なものに依存しない。Kubernatesに対応している。OSSで提供している。https://twitter.com/SawyerMerritt/status/1683365478582951936↩]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Terraformでmapにkeyが含まれないときにスキップしたい]]></title>
            <link>https://zenn.dev/nnaka2992/articles/skip_when_key_does_not_exists_in_map_terraform</link>
            <guid>https://zenn.dev/nnaka2992/articles/skip_when_key_does_not_exists_in_map_terraform</guid>
            <pubDate>Sat, 22 Jul 2023 14:53:12 GMT</pubDate>
            <content:encoded><![CDATA[Google CloudではPublic IPを利用した際に割り振られる可能性のあるCIDRの一覧がcloud.jsonでJSON形式で公開されています。この記事は雑な検証用のTerraformで承認済みネットワークにasia-notheast1のCIDRを全部登録してやろうとしたとき、上記のJSONファイルからscopeがasia-northeast1のprefixes.ipv4Prefixを抜きだそうとしたときにハマったのでその対応方法のメモです 結論以下のような感じで書いたら対応できました。contains(keys(hoge), "fuga") # hogeのkeyにh...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Four Keys とは？考え方から導入まで徹底検証してみた]]></title>
            <link>https://sreake.com/blog/learn-about-four-keys/</link>
            <guid>https://sreake.com/blog/learn-about-four-keys/</guid>
            <pubDate>Fri, 21 Jul 2023 09:56:19 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake事業部でインターンをしている村山です。私は以前に、2022年のAccelerate State of DevOps Reportについて調査を行いました。DevOps Reportでは、Four K […]The post Four Keys とは？考え方から導入まで徹底検証してみた first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Kubernetes の upstream のキャッチアップ]]></title>
            <link>https://zenn.dev/toversus/articles/52b107ab103712</link>
            <guid>https://zenn.dev/toversus/articles/52b107ab103712</guid>
            <pubDate>Thu, 20 Jul 2023 10:18:32 GMT</pubDate>
            <content:encoded><![CDATA[先日、Kubernetes Meetup Tokyo #59 で「KEP から眺める Kubernetes」というタイトルで発表しました。発表の後で Kubernetes の upstream のキャッチアップ方法について質問を受けました。その場で回答はしたのですが、ちょうど社内の共有会で似たような話をしたところだったので、加筆修正したものを公開しておきます。 はじめにKubernetes の upstream を追いかけ始めて 1 年ちょっと経ったので、その経験をまとめます。Kubernetes の upstream やエコシステムを観察しているだけで、コントリビュータではありま...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[CLIの実行結果を正しく理解することを促すツールを作成しました。]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/07/19/162657</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/07/19/162657</guid>
            <pubDate>Wed, 19 Jul 2023 07:26:57 GMT</pubDate>
            <content:encoded><![CDATA[概要AIの技術は目覚ましい進歩を遂げています。特に自然言語処理（NLP）の分野では、GPT-4のようなモデルが人間に近いレベルで文章を理解し、生成することができるようになりました。しかし、これらのモデルを日々の業務や作業にどのように活用すればよいのか、多くの人々がまだ手探りの状態です。一方、コマンドラインは、システム管理者やソフトウェア開発者にとって重要なツールです。コマンドラインからシステムの状態を調べたり、プログラムを実行したりするためには、これらで利用するコマンドの理解とそれらを十分に使いこなすことが必要です。netflixtechblog.comアフィリエイトでも何でもなく運用で利用するコマンドについてはLinuCなどもあるので教材を読むだけでもおすすめしたい。linuc.orgでは、AIがコマンドプロンプトの結果を理解し、それを人間がより理解しやすい形で説明することができたら、どうでしょうか？ここで、AICommandを紹介します。AICommandは、コマンドプロンプトの実行とその結果の解釈を統合したツールであり、AIの力を借りてコマンドプロンプトの結果を理解する新しい試みです。今回の記事では、このAICommandについて詳しく見ていきましょう。シェルコマンドの実行とその結果をOpenAIのGPTモデルに結果を送信し解説を要求するGo製CLIツールです。コマンドの処理状況も視覚的に表示します。 pic.twitter.com/5q6jqyWbsx— nwiizo (@nwiizo) 2023年7月18日  AICommandの紹介AICommandは、コマンドプロンプトの結果を人間が理解しやすい形に解釈するための新しいツールです。OpenAIの強力な自然言語処理モデルを使用して、コマンドラインから得られた情報を詳細に解析し、その結果を説明します。これにより、複雑なコマンドの実行結果も、非専門家でも簡単に理解できるようになります。github.comコマンドプロンプトは非常に強力で、システムの管理やデータの分析には欠かせないツールですが、その結果を正しく理解するには専門知識が必要で、学習コストが高いという課題がありました。しかし、AICommandを使えば、そのハードルが大きく下がります。たとえば、システムのログを確認するためのコマンドを実行した結果を、AIが解釈し、重要なポイントをハイライトしてくれます。さらに、その結果がどういう意味を持つのか、何が原因でそうなったのかといった情報も提供してくれます。このように、AICommandは、AIの能力を利用して、コマンドプロンプトの利用をより手軽で、より理解しやすいものに変えることを目指しています。ソフトウェア開発者やシステム管理者だけでなく、コマンドラインを利用するすべての人々にとって、新たな可能性を広げるツールとなることを目指します。option で日本語にも対応してます。 pic.twitter.com/AkEHh5syPx— nwiizo (@nwiizo) 2023年7月19日  Setup 🔧AICommandはGo言語で書かれているため、Goの開発環境が必要です。まず、Goがまだインストールされていない場合は、公式のインストールガイドに従ってGoをインストールしてください。Install aicommandGoがインストールされたら、次にAICommandをインストールします。go install github.com/nwiizo/aicommand@latestSet the your_api_keyAICommandはOpenAIのGPTモデルを使用しますので、OpenAIのAPIキーが必要となります。OpenAIのアカウントを持っていてAPIキーを取得済みの場合は、そのAPIキーを使用します。まだAPIキーを取得していない場合は、OpenAIの公式ドキュメントを参照してAPIキーを取得してください。APIキーを取得したら、そのキーを環境変数 OPENAI_API_KEYに設定します。設定方法は以下の通りです：export OPENAI_API_KEY=your_api_keyUsage ⏳コマンドの実行とその結果の解釈を行うには、次のように execute コマンドに続けて実行したいコマンドを引数として与えます。コマンドは(ダブル)クオーテーションで囲む必要があります。aicommand execute "your-shell-command"たとえば、ディレクトリの内容をリストする ls -la コマンドの結果を解釈させたい場合は、次のように実行します。aicommand execute "ls -la"すると、AICommandは ls -la コマンドを実行し、その結果を解釈して人間が理解しやすい形で説明します。また、解釈結果の言語を指定したい場合は、 --language または-lオプションを使用します。現在、英語（en）と日本語（ja）がサポートされています。デフォルトの言語は英語です。aicommand execute --language ja "ls -la"さらに、使用するGPTモデルを指定することも可能です。これは --model または -m オプションで指定します。デフォルトは gpt-3.5-turbo です。aicommand execute --model gpt-3.5-turbo "ls -la"これでAICommandの基本的な使用方法について説明しました。コマンドプロンプトの結果の解釈がこれまで以上に手軽になり、より深い理解が可能になります。AICommandの可能性🤖AICommandは、私たちが普段利用しているコマンドプロンプトをOpenAIのGPTモデルと組み合わせることで新たな可能性を生み出します。たとえば、複雑なコマンドを実行した結果の意味を理解することが困難な場合や、ログの解析、データ分析などで結果をより深く理解するための手助けとなります。また、様々なプログラムやスクリプトの実行結果を人間が理解できる形で説明してくれるため、デバッグやエラー解析の作業を効率化することが可能です。AICommandを利用すれば、テクニカルな知識がなくてもコマンドラインから得られる情報を理解しやすくなるかもしれません。結論🦾AICommandは、AIとCLI（Command Line Interface）の架け橋となるツールであり、この2つの強力なテクノロジーを組み合わせることで、未知の課題に対して新たな視点を提供します。さまざまなバックグラウンドを持つユーザーがコマンドラインから得られる情報をより容易に理解できるようになることで、これまで手が出せなかった問題に取り組む手助けをしてくれるでしょう。しかし、その一方で、AICommandはコマンドプロンプトの出力を人間が理解できる形で解釈するツールであるため、その解釈は絶対的な真実を表すものではありません。AICommandの解釈結果は参考の一つと考え、最終的な意思決定はユーザー自身の判断に任せるべきです。以上のことを念頭に置いて、AICommandを活用すれば、新たな視点からコマンドラインの世界を探索することが可能になるでしょう。ソフトウェア開発にChatGPTは使えるのか？――設計からコーディングまでAIの限界を探る作者:小野 哲技術評論社AmazonCloudNative Days Fukuoka 2023 にて登壇余談なのですが"k8sgpt Deep Dive: KubernetesクラスタのAI駆動型分析について” というタイトルで登壇を行います。event.cloudnativedays.jp参考AI時代に向けたクラウドにおける信頼性エンジニアリングの未来構想 / DICOMO2022 6A-1AICommand GitHubリポジトリOpenAIsashabaranov/go-openai]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[成熟度モデルを活用したCloud Nativeへの道筋 という副題で登壇します #開発生産性con_findy]]></title>
            <link>https://syu-m-5151.hatenablog.com/entry/2023/07/13/131433</link>
            <guid>https://syu-m-5151.hatenablog.com/entry/2023/07/13/131433</guid>
            <pubDate>Thu, 13 Jul 2023 04:14:33 GMT</pubDate>
            <content:encoded><![CDATA[概要開発生産性Conferenceというイベントに『Cloud Native の作法 - 成熟度モデルを活用したCloud Nativeへの道筋』というタイトルで登壇しました。生産性に関するイベントなんですけど現場のエンジニアをやっている僕には開発生産性について語ることってあんまりないようなーって思いながら最近、成熟度モデルについて調べていたのでこのタイトルにしました。途中で開発生産性について語るのを諦めてガッツリ資料を作り直しましたので生暖かく見守ってください。あと、ちょっと前に書籍を送って頂きましたが📖 Twitter での告知を忘れていたのでしておきます。読んだ感想としては入門書では決してないですが成熟度モデルでいうとレベル2の段階では読んでほしいと思う書籍になります。また、豊富にドキュメントへのリンクが貼ってあるのでKubernetesという荒野に道を示す地図になると思います(この文章はChatGPTではなく俺が生成した)。Kubernetesの知識地図 —— 現場での基礎から本番運用まで作者:青山 真也,小竹 智士,長谷川 誠,川部 勝也,岩井 佑樹,杉浦 智基技術評論社Amazon資料登壇資料になります。このブログの目的は参考資料をいちいち探さなくていいようにありますのでご活用ください。 speakerdeck.com参考文献Cloud Native Maturity ModelCloud Native TransformationDesign Patterns for Cloud Native ApplicationsIntro to the Cloud Native Maturity Model - Danielle Cook, Simon Forster, Robbie Glenn & John FormanSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本: Googleが実践している新DevOps方法論SRE サイトリライアビリティエンジニアリングサイトリライアビリティワークブックCloud Native成熟度モデルがWeb公開されましたWhat's the Difference Between DevOps and SRE?Solving Reliability Fears with Site Reliability EngineeringReliability When Everything Is a Platform: Why You Need to SRE Your CustomersThe History of DevOps ReportsEffective DevOpsPlatform Engineeringへの招待Platform Team と 社内政治 〜 出でよ、Platform Champion 〜 / Platform Team and Internal Politics - Platform Engineering Meetup #2Platform Engineering at MercariEMPOWERED 普通のチームが並外れた製品を生み出すプロダクトリーダーシッププロダクトマネジメントのすべて 事業戦略・IT開発・UXデザイン・マーケティングからチーム・組織運営まで正しいものを正しくつくる　プロダクトをつくるとはどういうことなのか、あるいはアジャイルのその先についてエンジニアリング組織論への招待　～不確実性に向き合う思考と組織のリファクタリング]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[OpenAI APIを利用してパブリッククラウドの権限要約をしてくれるCLIコマンドを作成した]]></title>
            <link>https://sreake.com/blog/summarize-permission-with-openai/</link>
            <guid>https://sreake.com/blog/summarize-permission-with-openai/</guid>
            <pubDate>Tue, 11 Jul 2023 07:01:52 GMT</pubDate>
            <content:encoded><![CDATA[はじめに Sreake事業部の橋本です。前回の記事から引き続き、OpenAIのGPTモデルを利用してDevOps、SREの領域でのtext AIの有効活用を考えていきます。 運用の自動化、構築支援などに活用できると嬉しい […]The post OpenAI APIを利用してパブリッククラウドの権限要約をしてくれるCLIコマンドを作成した first appeared on sreake.com | 株式会社スリーシェイク.]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[メールが届いたら Google Home で音声で通知する]]></title>
            <link>https://blog.1q77.com/2023/07/ses-lambda-and-cloud-pubsub/</link>
            <guid>https://blog.1q77.com/2023/07/ses-lambda-and-cloud-pubsub/</guid>
            <pubDate>Mon, 10 Jul 2023 14:25:35 GMT</pubDate>
            <content:encoded><![CDATA[以前、「 LINE に送ったメッセージを Google Home に読み上げさせる」という記事を書きました。 その時に作ったものに家にあるラズパイで Cloud PubSub を subscribe してメッセージが届いたらその内容を Text-to-Speach で]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[【Terraform🧑‍🚀】tfstateファイル分割パターンとディレクトリー構成設計]]></title>
            <link>https://hiroki-hasegawa.hatenablog.jp/entry/2023/07/05/001756</link>
            <guid>https://hiroki-hasegawa.hatenablog.jp/entry/2023/07/05/001756</guid>
            <pubDate>Tue, 04 Jul 2023 15:17:56 GMT</pubDate>
            <content:encoded><![CDATA[この記事から得られる知識この記事を読むと、以下を "完全に理解" できます✌️Terraformのtfstateファイルを分割する目的と、分割パターンについて (AWS向け)Terraformのリポジトリやリモートバックエンドのディレクトリー構成の設計についてこの記事から得られる知識01. はじめに02. なぜ tfstate ファイルを分割するのか分割していない場合分割している場合03. tfstate ファイルの分割分割の境目状態の依存関係図依存関係図とは依存関係の表現▼ 依存関係の表現記法▼ 依存関係がない場合▼ 依存関係がある場合04. tfstate ファイルに基づくその他の設計リポジトリ 🏭 の設計リポジトリ分割ディレクトリー 📂 構成リモートバックエンド 🪣 の設計リモートバックエンド分割ディレクトリー構成05. 状態の依存関係の定義方法terraform_remote_stateブロックの場合terraform_remote_stateブロックによる依存状態の依存関係図リポジトリのディレクトリー構成リモートバックエンドのディレクトリー構成AWSリソース別のdataブロックの場合AWSリソース別のdataブロックによる依存状態の依存関係図リポジトリのディレクトリー構成リモートバックエンドのディレクトリー構成06. tfstate ファイルの分割パターンオススメな設計の一覧ディレクトリー構成との関係についてリポジトリの場合リモートバックエンドの場合07. 上層の分割 (必須)上層の分割についてプロバイダーのアカウント別 - ★★この分割方法について【プロバイダーアカウント別】状態の依存関係図【プロバイダーアカウント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【プロバイダーアカウント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合08. 下層の分割 (必須)下層の分割について実行環境別 - ★★この分割方法について【実行環境別】状態の依存関係図【実行環境別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【実行環境別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンド x AWSアカウント別に異なる実行環境 の場合▼ 同じリモートバックエンド x 単一のAWSアカウント内に全ての実行環境 の場合09. 中間層の分割 (任意)中間層の分割について同じテナントのプロダクト別この分割方法について【同じテナントのプロダクト別】状態の依存関係図【同じテナントのプロダクト別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【同じテナントのプロダクト別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合運用チーム責務範囲別 - ★この分割方法について【チーム別】状態の依存関係図【チーム別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【チーム別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合プロダクトのサブコンポーネント別 - ★この分割方法について【サブコンポーネント別】状態の依存関係図【サブコンポーネント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【サブコンポーネント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合AWSリソースの種類グループ別この分割方法について【種類グループ別】状態の依存関係図【種類グループ別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【種類グループ別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合AWSリソースの状態の変更頻度グループ別この分割方法について【変更頻度グループ別】状態の依存関係図【変更頻度グループ別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【変更頻度グループ別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合運用チーム責務範囲別 × プロダクトサブコンポーネント別 - ★この分割方法について【チーム別 × サブコンポーネント別】状態の依存関係図【チーム別 × サブコンポーネント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【チーム別 × サブコンポーネント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合10. おわりに謝辞01. はじめに前世でもう少し徳を積んでいれば、Mitchell Hashimoto として生まれることができたのに!!さて最近の業務で、全プロダクトの技術基盤開発チームに携わっており、チームが使っているTerraform🧑🏻‍🚀のリポジトリをリプレイスする作業を担当しました。このリポジトリでは単一のtfstateファイルが状態を持ち過ぎている課題を抱えていたため、課題に合った適切な分割パターンでリプレイスしました。今回は、この時に整理した分割パターン (AWS向け) を記事で解説しました。もちろん、GoogleCloudやAzureでも読み換えていただければ、同じように適用できます。知る限りの分割パターンを記載したところ、情報量がエグいことになってしまったため、気になる分割パターンだけ拾って帰っていただけるとハッピーです🙏それでは、もりもり布教していきます😗02. なぜ tfstate ファイルを分割するのか%%{init: { 'theme': "default", 'themeVariables': { 'commitLabelFontSize': '13px' }}}%%gitGraph   commit id: "8c8e6"   commit id: "0e3c3"     branch feature/foo     checkout feature/foo     commit id: "4e9e8"     commit id: "fooさんがApply"   checkout main     branch feature/bar     commit id: "barさんがPlan"   checkout main   commit id: "e74d6"     branch feature/baz     commit id: "bazさんがPlan"分割していない場合そもそも、なぜtfstateファイルを分割する必要があるのでしょうか。tfstateファイルを分割しなかったと仮定します。様々なインフラコンポーネントを単一のtfstateファイルで状態を持つ場合、1回のterraformコマンド全てのコンポーネントの状態を操作できて楽です。ただし、複数の作業ブランチがある状況だと煩わしいことが起こります。各作業ブランチでインフラコンポーネントの状態を変更しかけていると、terraformコマンドでtargetオプションが必要になります。分割している場合その一方で、tfstateファイルをいい感じに分割したと仮定します。各作業ブランチでは、まるで暗黙的にtargetオプションがついたように、他の作業ブランチの影響を受けずにterraformコマンドを実行できます。よって、各tfstateファイルを操作できる管理者は互いに影響を受けずに、terraformコマンドの結果を得られるようになります。Terraform: Up & Running: Writing Infrastructure As CodeOrganizing With Multiple States - DevOps with Terraform - CloudCasts03. tfstate ファイルの分割分割の境目それでは、tfstateファイルの分割の境界はどのようにして見つければよいのでしょうか。これを見つけるコツは、他の状態にできるだけ依存しないリソースの関係 に注目することだと考えています。ここでいう依存とは、tfstateファイルが他のtfstateファイルの状態を使用することです。tfstateファイルの分割パターンについては後述します。アーキテクチャの文脈では、他を使用することを『依存』と表現します。そのため便宜上、tfstateファイルでも同じ用語で表現することにしました。@tmknom さんが述べている通り、Terraformをよりよく設計するためには、『ソフトウェアの基礎知識』が必要です👍状態の依存関係図依存関係図とは分割したtfstateファイル間の状態の依存関係を表現した図です。プロバイダーのアカウントの状態をtfstateファイルで管理していることを想像してみてください。%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWSアカウント        foo["tfstateファイル"]    end似たものとしてterraform graphコマンドによるグラフがありますが、これはリソース間の依存関係図です。tfstateファイル間で相互に依存関係があるからといって、個別のリソース間で循環参照が起こってしまうというわけではないです。続いて、依存関係がある場合と無い場合で、どのような依存関係図になるかを紹介していきます。Command: graph | Terraform | HashiCorp Developer依存関係の表現▼ 依存関係の表現記法tfstateファイル間で状態の依存関係がある場合、これを図で表現すると分割の状況がわかりやすくなります。『依存』は、---> (波線矢印) で表現することとします。設定値の参照数が少ないほどよいです。依存関係がある場合については、後述します。アーキテクチャの文脈では、『依存』を---> (波線矢印) で表現します。そのため便宜上、tfstateファイルでも同じ記号で表現することにしました👍▼ 依存関係がない場合例えば、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、 tfstate ファイル間に依存関係はないとします。そのため、想定される状態の依存関係図は以下の通りになります。tfstateファイル間に依存関係がない状況がベストです。---title: tfstateファイル間に依存関係はない---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWSアカウント        foo["foo-tfstate"]        bar["bar-tfstate"]    end▼ 依存関係がある場合同様に分割したと仮定します。ここで仮定した状況では、 foo-tfstate ➡︎ bar-tfstate の方向に依存しているとします。そのため、---> (波線矢印) を使用して、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: foo-tfstateファイルは、bar-tfstateファイルに依存---%%{init:{'theme':'default'}}%%flowchart TD    subgraph AWSアカウント        foo["foo-tfstate"]        bar["bar-tfstate"]    end    foo -. 依存 .-> bar04. tfstate ファイルに基づくその他の設計リポジトリ 🏭 の設計リポジトリ分割ここまでで、tfstateファイル分割について簡単に紹介しました。リポジトリの分割は、tfstateファイル分割に基づいて設計しましょう。異なるリポジトリにtfstateファイルをおいた方がよい場合については、分割パターン で説明しています。🏭 foo-repository/├── backend.tf # fooコンポーネントの状態を持つ terraform.tfstate ファイルを指定する...🏭 bar-repository/├── backend.tf # barコンポーネントの状態を持つ terraform.tfstate ファイルを指定する...ディレクトリー 📂 構成リポジトリ内のディレクトリー構成も、tfstateファイル分割に基づいて設計しましょう。率直に言うと、Terraformのディレクトリー構成のパターンは無数にあります。そのため、基準なしにディレクトリー構成を考えると何でもあり になってしまいます。その一方で、tfstateファイル分割に基づいて設計することにより、明確なディレクトリー構成パターン として抽出可能になります。🏭 repository/├── 📂 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 bar/      ├── backend.tf # barコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      ...Terraformには、そのリポジトリ内だけでブロック (例：resource、data) のセットを使い回すことを目的とした、ローカルモジュールがあります。今回、これのディレクトリー構成は設計に含めていません。混同しやすいのですが、tfstateファイル分割に基づくディレクトリー構成とローカルモジュール内のそれは、全く別のテーマとして切り離して考えることができます👍リモートバックエンド 🪣 の設計リモートバックエンド分割本記事では、リモートバックエンドとしてAWS S3バケットを使用することを想定しています。リモートバックエンドの分割は、tfstateファイル分割に基づいて設計しましょう。異なるリモートバックエンドにtfstateファイルをおいた方がよい場合については、分割パターン で説明しています。🪣 foo-bucket/│└── terraform.tfstate # fooコンポーネントの状態を持つ🪣 bar-bucket/│└── terraform.tfstate # barコンポーネントの状態を持つディレクトリー構成リモートバックエンド内のディレクトリー構成も、tfstateファイル分割に基づいて設計しましょう。🪣 bucket/├── 📂 foo/│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── 📂 bar/      └── terraform.tfstate # barコンポーネントの状態を持つ05. 状態の依存関係の定義方法terraform_remote_stateブロックの場合terraform_remote_stateブロックによる依存terraform_remote_stateブロックには、以下のメリデメがあります。 アーキテクチャ特性  メリット ⭕️                                                                        デメリット ×                                                                                                                                                      可読性                 -                                                                                  terraform_remote_stateブロックに加えてoutputブロックも実装が必要であり、outputブロックは依存先のAWSリソースが一見してわかりにくい。                             拡張性                 依存先のAWSリソースに関わらず、同じterraform_remote_stateブロックを使い回せる。  -                                                                                                                                                                     保守性                 -                                                                                  依存先と依存元の間でTerraformのバージョンに差がありすぎると、tfstateファイル間で互換性がなくなり、terraform_remote_stateブロックの処理が失敗する。 本記事では、 terraform_remote_state ブロックを使用して、状態の依存関係を定義 していきます。tfstateファイルが他のtfstateファイルに依存する方法として、後述のAWSリソース別のdataブロックがあります。The terraform_remote_state Data Source | Terraform | HashiCorp Developer状態の依存関係図例えば、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、bar-tfstateファイルはVPCの状態を持っており、 foo-tfstate ファイルは bar-tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: terraform_remote_stateブロックを使用した依存関係---%%{init:{'theme':'default'}}%%flowchart TD    subgraph bucket        foo["foo-tfstate"]        bar["bar-tfstate"]    end    foo -. VPCの状態に依存 .-> barリポジトリのディレクトリー構成tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。ディレクトリーの設計方法は、分割パターン で説明しています。🏭 repository/├── 📂 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    ├── remote_state.tf # terraform_remote_stateブロックを使用し、bar-tfstate ファイルに依存する│    ├── provider.tf│    ...│└── 📂 bar/      ├── backend.tf # barコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      ├── output.tf # 他の tfstate ファイルから依存される      ├── provider.tf      ...foo-tfstateファイルがbar-tfstateファイルに依存するために必要な実装は、以下の通りになります。# fooリソースの状態は、bar-tfstate ファイルで持つresource "example" "foo" {  # fooリソースは、bar-tfstate ファイルのVPCに依存する  vpc_id = data.terraform_remote_state.bar.outputs.bar_vpc_id  ...}# VPCの状態は、bar-tfstate ファイルで持つdata "terraform_remote_state" "bar" { backend= "s3"  config = {    bucket = "bar-tfstate"    key    = "bar/terraform.tfstate"    region = "ap-northeast-1"  }}# VPCの状態は、bar-tfstate ファイルで持つoutput "bar_vpc_id" {  value = aws_vpc.bar.id}resource "aws_vpc" "bar" {  ...}リモートバックエンドのディレクトリー構成tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。🪣 bucket/├── 📂 foo│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── 📂 bar      └── terraform.tfstate # barコンポーネントの状態を持つAWSリソース別のdataブロックの場合AWSリソース別のdataブロックによる依存dataブロックには、以下のメリデメがあります。 アーキテクチャ特性  メリット ⭕️                                                                                                                                     デメリット ×                                                   可読性                 依存先のAWSリソースがわかりやすい。                                                                                                             -                                                                  拡張性                 -                                                                                                                                               依存先のAWSリソース別にdataブロックが必要である。                保守性                 依存先と依存元の間でTerraformのバージョンに差があっても、tfstateファイル間で直接的に依存するわけではないため、バージョン差の影響を受けない。  -                                                   今回は使用しませんが、依存関係の他の定義方法として、AWSリソース別のdataブロックがあります。これは、tfstateファイルが自身以外 (例：コンソール画面、他のtfstateファイル) で作成されたAWSリソースの状態に依存するために使用できます。terraform_remote_stateブロックとは異なり、直接的にはtfstateファイルに依存しません。dataブロックの場合は、実際のAWSリソースの状態に依存することにより、間接的にAWSリソースのtfstateファイルに依存することになります。Data Sources - Configuration Language | Terraform | HashiCorp Developer状態の依存関係図例えば、dataブロックも同様にして、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、bar-tfstateファイルはVPCの状態を持っており、 foo-tfstate ファイルは bar-tfstate ファイルに依存しているとします。想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: dataブロックを使用した依存関係---%%{init:{'theme':'default'}}%%flowchart TD    subgraph bucket        foo["foo-tfstate"]        bar["bar-tfstate"]    end    foo -. VPCの状態に依存 .-> barリポジトリのディレクトリー構成ディレクトリー構成は、tfstateファイル分割に基づいて、以下の通りになります。🏭 repository/├── 📂 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    ├── data.tf # dataブロックを使用し、bar-tfstate ファイルに依存する│    ├── provider.tf│    ...│└── 📂 bar/      ├── backend.tf # barコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      ├── provider.tf      ...foo-tfstateファイルがbar-tfstateファイルに依存するために必要な実装は、以下の通りになります。# fooリソースの状態は、foo-tfstate ファイルで持つresource "example" "foo" {  # fooリソースは、bar-tfstate ファイルのVPCに依存する  vpc_id     = data.aws_vpc.bar.id}# VPCの状態は、bar-tfstate ファイルで持つdata "aws_vpc" "bar" {  filter {    name   = "tag:Name"    values = ["<bar-tfstateが持つVPCの名前>"]  }}リモートバックエンドのディレクトリー構成tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。🪣 bucket/├── 📂 foo│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── 📂 bar      └── terraform.tfstate # barコンポーネントの状態を持つ06. tfstate ファイルの分割パターンオススメな設計の一覧前述の通り、tfstateファイルの分割の境界は、『他の状態にできるだけ依存しないリソースの関係』から見つけることができます。分割しすぎると terraform_remote_stateブロック地獄 になるため、細かすぎず粗すぎない適切な境界を見つけていきましょう。今回は、私が考える分割パターンをいくつか紹介します。全てが実用的なパターンというわけでないため、オススメするものを ★ としています。必須・任意    tfstate分割パターン大分類    tfstate分割パターン小分類オススメ    対応するリポジトリ構成 🏭    対応するリモートバックエンド構成 🪣  必須    上層    プロバイダーのアカウント別    ★★    リポジトリ自体または上層ディレクトリー    リモートバックエンド自体または上層ディレクトリー  下層実行環境別    ★★    下層ディレクトリー    下層ディレクトリー  任意    中間層    同じテナント内のプロダクト別        中間層ディレクトリー    中間層ディレクトリー  運用チーム責務範囲別    ★  プロダクトのサブコンポーネント別    ★  AWSリソースの種類グループ別      AWSリソースの状態の変更頻度グループ別      運用チーム責務範囲別プロダクトのサブコンポーネント別の組み合わせ    ★  ディレクトリー構成との関係についてリポジトリの場合記事内のここ で、リポジトリ内のディレクトリー構成はtfstateファイル分割に基づいて設計するべき、という説明をしました。tfstateファイルの分割パターンは、上層/下層/中間層 の層に大別できます。これらの層は、以下の通りリポジトリ自体・ディレクトリー構成の設計方法に影響します。# リポジトリ自体を分割する場合🏭 上層/├── 📂 中間層/│    ├── 📂 下層/│    │    ├── backend.tfvars # 分割された terraform.tfstate ファイルを指定する│    │    ...│    │...# リポジトリ内のディレクトリを分割する場合🏭 リポジトリ/├── 📂 上層/│    ├── 📂 中間層/│    │    ├── 📂 下層/│    │    │    ├── backend.tfvars # 分割された terraform.tfstate ファイルを指定する│    │    │    ...│    │    │...リモートバックエンドの場合記事内のここ で、リモートバックエンドのディレクトリ構成についても言及しました。これらの層は、以下の通りリモートバックエンド自体・ディレクトリー構成の設計方法に影響します。# リモートバックエンド自体を分割する場合🪣 上層/├── 📂 中間層/│    ├── 📂 下層/│    │    └── terraform.tfstate # 分割された状態を持つ│    ││    │...# リモートバックエンド内のディレクトリを分割する場合🪣 bucket/├── 📂 上層/│    ├── 📂 中間層/│    │    ├── 📂 下層/│    │    │    └── terraform.tfstate # 分割された状態を持つ│    │    ││    │    │...07. 上層の分割 (必須)上層の分割について上層の分割は 必須 です。Terraformに携わる管理者の数が少なくても採用した方がよいです。tfstateファイルをパターンに応じて分割し、これに基づいてディレクトリー・リモートバックエンドも設計しましょう。プロバイダーのアカウント別 - ★★この分割方法について上層分割の中でも、基本的な方法の1つです。プロバイダーのアカウント別にtfstateファイルを分割し、上層もこれに基づいて設計します。この分割方法により、各プロバイダーの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。tfstateファイルで状態を管理せざるを得ない場合があります。例えば、Kubernetesのプロバイダーは、EKSと同じtfstateファイルで管理した方がよいです👍Terraform Registry【プロバイダーアカウント別】状態の依存関係図例えば、以下のプロバイダーを使用したい状況と仮定します。主要プロバイダー (AWS)アプリ/インフラ監視プロバイダー (Datadog)ジョブ監視プロバイダー (Healthchecks)インシデント管理プロバイダー (PagerDuty)ここで仮定した状況では、各プロバイダーの tfstate ファイル間で状態が相互に依存しているとします。AWSリソース間の相互依存ではないため、循環参照は起こりません。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: プロバイダーのアカウント別---%%{init:{'theme':'default'}}%%flowchart LR    subgraph PagerDuty        pagerDuty["tfstate"]    end    subgraph Healthchecks        healthchecks["tfstate"]    end    subgraph Datadog        datadog["tfstate"]    end    subgraph AWS        aws["tfstate"]    end    aws -...-> datadog    aws -...-> healthchecks    aws -...-> pagerDuty    datadog -...-> aws    healthchecks -...-> aws    pagerDuty -...-> aws【プロバイダーアカウント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合プロバイダーアカウント別に分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🏭 aws-repository/├── backend.tf # AWSの状態を持つ terraform.tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf...🏭 datadog-repository/├── backend.tf # Datadogの状態を持つ terraform.tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf...🏭 healthchecks-repository/├── backend.tf # Healthchecksの状態を持つ terraform.tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf...🏭 pagerduty-repository/├── backend.tf # PagerDutyの状態を持つ terraform.tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf...▼ 同じリポジトリの場合プロバイダーアカウント別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🏭 repository/├── 📂 aws/│    ├── backend.tf # AWSの状態を持つ terraform.tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ...│├── 📂 datadog/│    ├── backend.tf # Datadogの状態を持つ terraform.tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ...│├── 📂 healthchecks/│    ├── backend.tf # Healthchecksの状態を持つ terraform.tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ...│└── 📂 pagerduty/      ├── backend.tf # PagerDutyの状態を持つ terraform.tfstate ファイルを指定する      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── provider.tf      ...【プロバイダーアカウント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合プロバイダーアカウント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🪣 aws-bucket/│└── terraform.tfstate # AWSの状態を持つ🪣 datadog-bucket/│└── terraform.tfstate # Datadogの状態を持つ🪣 healthchecks-bucket/│└── terraform.tfstate # Healthchecksの状態を持つ🪣 pagerduty-bucket/│└── terraform.tfstate # PagerDutyの状態を持つ▼ 同じリモートバックエンドの場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🪣 bucket/├── 📂 aws│    └── terraform.tfstate # AWSの状態を持つ│├── 📂 datadog│    └── terraform.tfstate # Datadogの状態を持つ│├── 📂 healthchecks│    └── terraform.tfstate # Healthchecksの状態を持つ│└── 📂 pagerduty      └── terraform.tfstate # PagerDutyの状態を持つ08. 下層の分割 (必須)下層の分割について下層の分割は 必須 です。Terraformに携わる管理者の数が少なくても採用した方がよいです。tfstateファイルをパターンに応じて分割し、これに基づいてディレクトリー・リモートバックエンドも設計しましょう。実行環境別 - ★★この分割方法について下層分割の中でも、基本的な方法の1つです。実行環境別にtfstateファイルを分割し、下層もこれに基づいて設計します。この分割方法により、各実行環境の管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。Terraform: Up & Running; Writing Infrastructure As CodeHow to manage Terraform state. A guide to file layout, isolation, and… | by Yevgeniy Brikman | Gruntwork【実行環境別】状態の依存関係図例えば、以下の実行環境を構築したい状況と仮定します。Tes環境 (検証環境)Stg環境 (ユーザー受け入れ環境)Prd環境 (本番環境)かつ、以下のプロバイダーを使用したい状況と仮定します。主要プロバイダー (AWS)アプリ/インフラ監視プロバイダー (Datadog)ジョブ監視プロバイダー (Healthchecks)インシデント管理プロバイダー (PagerDuty)ここで仮定した状況では、各実行環境の tfstate ファイルは他の実行環境には依存していないとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 実行環境別---%%{init:{'theme':'default'}}%%flowchart LR    subgraph PagerDuty        pagerDuty["tfstate"]    end    subgraph Healthchecks        healthchecks["tfstate"]    end    subgraph Datadog        datadog["tfstate"]    end    subgraph AWS        subgraph tes-bucket            tes["tfstate"]        end        subgraph stg-bucket            stg["tfstate"]        end        subgraph prd-bucket            prd["tfstate"]        end    end    tes -...-> datadog    tes -...-> healthchecks    tes -...-> pagerDuty    datadog -...-> tes    healthchecks -...-> tes    pagerDuty -...-> tes【実行環境別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合プロバイダーアカウント別にtfstateファイルを分割することは必須としているため、その上でディレクトリー構成を考えます。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🏭 aws-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf├── 📂 tes/ # Tes環境│    ├── backend.tfvars # AWSのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/ # Stg環境└── 📂 prd/ # Prd環境🏭 datadog-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf├── 📂 tes/│    ├── backend.tfvars # DatadogのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/└── 📂 prd/🏭 healthchecks-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf├── 📂 tes/│    ├── backend.tfvars # HealthchecsのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/└── 📂 prd/🏭 pagerduty-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── provider.tf├── 📂 tes/│    ├── backend.tfvars # PagerDutyのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/└── 📂 prd/▼ 同じリポジトリの場合プロバイダーアカウント別にtfstateファイルを分割することは必須としているため、その上でディレクトリー構成を考えます。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🏭 repository/├── 📂 aws/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # AWSのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    └── 📂 prd/ # Prd環境│├── 📂 datadog/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ├── 📂 tes/│    │    ├── backend.tfvars # DatadogのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/│    └── 📂 prd/│├── 📂 healthchecks/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ├── 📂 tes/│    │    ├── backend.tfvars # HealthchecksのTes環境の状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/│    └── 📂 prd/│└── 📂 pagerduty/      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── provider.tf      ├── 📂 tes/      │    ├── backend.tfvars # PagerDutyのTes環境の状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/      └── 📂 prd/【実行環境別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合実行環境別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。例えば、前述の依存関係図の状況と仮定します。🪣 tes-aws-bucket/│└── terraform.tfstate # AWSのTes環境の状態を持つ🪣 tes-datadog-bucket/│└── terraform.tfstate # DatadogのTes環境の状態を持つ🪣 tes-healthchecks-bucket/│└── terraform.tfstate # HealthchecksのTes環境の状態を持つ🪣 tes-pagerduty-bucket/│└── terraform.tfstate # PagerDutyのTes環境の状態を持つ▼ 同じリモートバックエンド x AWSアカウント別に異なる実行環境 の場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。また、AWSアカウント別に異なる実行環境を作成していると仮定します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 aws/│    └── terraform.tfstate # AWSのTes環境の状態を持つ│├── 📂 datadog/│    └── terraform.tfstate # DatadogのTes環境の状態を持つ│├── 📂 healthchecks/│    └── terraform.tfstate # HealthchecksのTes環境の状態を持つ│└── 📂 pagerduty/      └── terraform.tfstate # PagerDutyのTes環境の状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...▼ 同じリモートバックエンド x 単一のAWSアカウント内に全ての実行環境 の場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。また、単一のAWSアカウント内に全実行環境を作成しているとします。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🪣 bucket/├── 📂 aws/│    ├── 📂 tes/ # Tes環境│    │    └── terraform.tfstate # AWSのTes環境の状態を持つ│    ││    ├── 📂 stg/ # Stg環境│    └── 📂 prd/ # Prd環境│├── 📂 datadog/│    ├── 📂 tes/│    │    └── terraform.tfstate # DatadogのTes環境の状態を持つ│    ││    ├── 📂 stg/│    └── 📂 prd/│├── 📂 healthchecks/│    ├── 📂 tes/│    │    └── terraform.tfstate # HealthchecksのTes環境の状態を持つ│    ││    ├── 📂 stg/│    └── 📂 prd/│└── 📂 pagerduty/      ├── 📂 tes/      │    └── terraform.tfstate # PagerDutyのTes環境の状態を持つ      │      ├── 📂 stg/      └── 📂 prd/09. 中間層の分割 (任意)中間層の分割について中間層の分割は 任意 です。Terraformに携わる管理者が多くなるほど、効力を発揮します。同じテナントのプロダクト別この分割方法について同じテナント (例：同じAWSアカウントの同じVPC) 内に複数の小さなプロダクトがある場合、プロダクト別でtfstateファイルを分割し、中間層もこれに基づいて設計します。ここでいうプロダクトは、アプリを動かすプラットフォーム (例：EKS、ECS、AppRunner、EC2) とそれを取り巻くAWSリソースを指しています。この分割方法により、各プロダクトの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。AWSの設計プラクティスとしてプロダクトごとにVPCを分けた方がよいため、この分割方法を採用することは少ないかもしれません。ただ現実として、各プロダクトの使用するIPアドレス数が少なく、またプロダクト別にVPCを分割するのが煩雑という現場はあります😭【同じテナントのプロダクト別】状態の依存関係図例えば、以下のプロダクトに分割した状況と仮定します。fooプロダクトbarプロダクト共有networkコンポーネント (例：VPC、Route53)ここで仮定した状況では、各プロダクトの tfstate ファイルの依存は一方向最終的に、共有networkコンポーネントの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 同じテナントのプロダクト別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            foo-product["foo-product-tfstate<br>(アプリを動かすプラットフォームのAWSリソース)"]-..->network            bar-product["bar-product-tfstate<br>(アプリを動かすプラットフォームのAWSリソース)"]-..->network            network["network-tfstate<br>(Route53, VPC)"]        end    subgraph stg-bucket        stg["tfstate"]    end    subgraph prd-bucket        prd["tfstate"]    end    end【同じテナントのプロダクト別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合この場合では、同じテナントのプロダクト別に分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# fooプロダクトの tfstate ファイルのリポジトリ🏭 aws-foo-product-repository/├── provider.tf├── remote_state.tf # 他の tfstate ファイルに依存する├── 📂 tes/ # Tes環境│    ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/ # Stg環境│    ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd/ # Prd環境      ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する      ...# barプロダクトの tfstate ファイルのリポジトリ🏭 aws-bar-product-repository/├── provider.tf├── remote_state.tf # 他の tfstate ファイルに依存する├── 📂 tes/ # Tes環境│    ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/ # Stg環境│    ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd/ # Prd環境      ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する      ...# 共有networkコンポーネントの tfstate ファイルのリポジトリ🏭 aws-network-repository/├── provider.tf├── output.tf # 他の tfstate ファイルから依存される├── route53.tf├── vpc.tf├── 📂 tes/ # Tes環境│    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/ # Stg環境│    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd/ # Prd環境      ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      ...▼ 同じリポジトリの場合この場合では、同じテナントのプロダクト別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。🏭 aws-repository/├── 📂 foo-product/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # fooプロダクトの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 bar-product/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # barプロダクトの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する           ...【同じテナントのプロダクト別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合同じテナントのプロダクト別の場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、同じテナントのプロダクト別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 foo-product│    └── terraform.tfstate # fooプロダクトの状態を持つ│├── 📂 bar-product│    └── terraform.tfstate # barプロダクトの状態を持つ│└── 📂 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...運用チーム責務範囲別 - ★この分割方法について運用チーム (例：アプリチーム、インフラチーム) のAWSリソースの責務範囲別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各運用チームが互いに影響を受けずに、terraformコマンドの結果を得られるようになります。AWS CloudFormation best practices - AWS CloudFormationTerraform in Action (English Edition)AWSドキュメント・著名な書籍で紹介されています👀Terraformに携わるチームが複数ある非常に大規模なプロダクトほど効力を発揮します。実際に私も現在進行形で採用しており、非常に実用的と考えています。【チーム別】状態の依存関係図例えば、以下の運用チームに分割した状況と仮定します。frontendチーム (アプリのフロントエンド領域担当)backendチーム (アプリのバックエンド領域担当)sreチーム (インフラ領域担当)ここで仮定した状況では、各チームが管理する tfstate ファイル間で状態が相互に依存しているとします。AWSリソース間の相互依存ではないため、循環参照は起こりません。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 運用チーム責務範囲別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            frontend["frontend-team-tfstate<br>(CloudFront, S3, など)"]            backend["backend-team-tfstate<br>(API Gateway, ElastiCache, RDS, SES, SNS, など)"]            sre["sre-team-tfstate<br>(ALB, CloudWatch, EC2, ECS, EKS, IAM, VPC, など)"]            frontend-..->sre            backend-..->sre            sre-..->frontend            sre-..->backend        end    subgraph stg-bucket        stg["tfstate"]    end    subgraph prd-bucket        prd["tfstate"]    end    end【チーム別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合この場合では、運用チーム責務範囲別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-frontend-team-repository/ # frontendチーム├── provider.tf├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── cloudfront.tf├── s3.tf├── 📂 tes/ # Tes環境│    ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg/ # Stg環境│    ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd/ # Prd環境      ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する      ...🏭 aws-backend-team-repository/ # backendチーム├── provider.tf├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── elasticache.tf├── ses.tf├── sns.tf├── rds.tf├── 📂 tes│    ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg│    ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd      ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する       ...🏭 aws-sre-team-repository/ # sreチーム├── provider.tf├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # 他の tfstate ファイルに依存する├── alb.tf├── cloudwatch.tf├── ec2.tf├── ecs.tf├── eks.tf├── iam.tf├── vpc.tf├── 📂 tes│    ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│├── 📂 stg│    ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する│    ...│└── 📂 prd      ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する      ...▼ 同じリポジトリの場合この場合では、運用チーム責務範囲別に分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-repository/├── 📂 frontend-team # frontendチーム│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── cloudfront.tf│    ├── s3.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # frontendチームの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 backend-team # backendチーム│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── elasticache.tf│    ├── ses.tf│    ├── sns.tf│    ├── rds.tf│    ├── 📂 tes│    │    ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg│    │    ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd│          ├── backend.tfvars # backendチームの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 sre-team # sreチーム      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── alb.tf      ├── cloudwatch.tf      ├── ec2.tf      ├── ecs.tf      ├── eks.tf      ├── iam.tf      ├── vpc.tf      ├── 📂 tes      │    ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg      │    ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd           ├── backend.tfvars # sreチームの状態を持つ terraform.tfstate ファイルを指定する           ...【チーム別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合運用チーム責務範囲別の場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、プロバイダーアカウント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 frontend-team│    └── terraform.tfstate # frontendチームの状態を持つ│├── 📂 backend-team│    └── terraform.tfstate # backendチームの状態を持つ│└── 📂 sre-team      └── terraform.tfstate # sreチームの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...プロダクトのサブコンポーネント別 - ★この分割方法についてプロダクトのサブコンポーネント (例：アプリ、ネットワーク、認証/認可、監視、など) 別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、サブコンポーネントの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。11 Things I wish I knew before working with Terraform – part 1Terraform organization — Part I : What if you split your components ? | by Amine Charot | Mediumコンポーネントは、分けようと思えばいくらでも細分化できてしまいます。細分化した数だけterraform_remote_stateブロック地獄になっていくため、適切な数 (3〜5個くらい) にしておくように注意が必要です。この分割方法は、後述のAWSリソースの種類グループとごっちゃになってしまう場合があるため、プロダクトのサブコンポーネントとして意識的に分割させる必要があります👍【サブコンポーネント別】状態の依存関係図例えば、以下のサブコンポーネントに分割した状況と仮定します。application (Web3層系)auth (認証/認可系)monitor (監視系)network (ネットワーク系)ここで仮定した状況では、各プロダクトの tfstate ファイルの依存は一方向最終的に、networkサブコンポーネントやauthサブコンポーネントの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: プロダクトのサブコンポーネント別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            application["application-tfstate<br>Web3層と周辺AWSリソース<br>(ALB, APIGateway, CloudFront, EC2, ECS, EKS, RDS, S3, SNS, など)"]            auth["auth-tfstate<br>(IAMなど)"]            monitor["monitor-tfstate<br>(CloudWatch, など)"]            network["network-tfstate<br>(Route53, VPC, など)"]            application-..->network            application-..->auth            monitor-..->application        end        subgraph stg-bucket            stg["tfstate"]        end        subgraph prd-bucket            prd["tfstate"]        end        end【サブコンポーネント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合プロダクトのサブコンポーネント別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、プロダクトのサブコンポーネント別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-repository/├── 📂 application/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── provider.tf│    ├── alb.tf│    ├── cloudfront.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── ses.tf│    ├── sns.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 monitor/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── cloudwatch.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する           ...【サブコンポーネント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合プロダクトのサブコンポーネント別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、プロダクトのサブコンポーネント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 application│    └── terraform.tfstate # applicationコンポーネントの状態を持つ│├── 📂 auth│    └── terraform.tfstate # authコンポーネントの状態を持つ│├── 📂 monitor│    └── terraform.tfstate # monitorコンポーネントの状態を持つ│└── 📂 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...AWSリソースの種類グループ別この分割方法についてAWSリソースの種類グループ別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各AWSリソースの種類グループも管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。AWSリソースの種類グループは、分けようと思えばいくらでも細分化できてしまいます。細分化した数だけterraform_remote_stateブロック地獄になっていくため、適切な数 (3〜5個くらい) にしておくように注意が必要です。特にこの分割方法は、グループ数がどんどん増えていく可能性があります😇【種類グループ別】状態の依存関係図例えば、以下の種類グループに分割した状況と仮定します。application (Webサーバー、Appサーバー系)auth (認証/認可系)datastore (DBサーバー系)cicd (CI/CD系)monitor (監視系)network (ネットワーク系)ここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、networkグループやauthグループの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: AWSリソースの種類グループ別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            application["application-tfstate<br>例: ALB, API Gateway, CloudFront, EC2, ECS, EKS, SNS, など"]            auth["auth-tfstate<br>例: IAM, など"]            cicd["cicd-tfstate<br>例: Code3兄弟, など"]            monitor["monitor-tfstate<br>例: CloudWatch, など"]            network["network-tfstate<br>例: Route53, VPC, など"]            datastore["datastore-tfstate<br>例: ElastiCache, RDS, S3, など"]            application-....->auth            application-..->datastore            application-...->network            cicd-..->application            datastore-..->network            monitor-..->application            monitor-..->datastore       end    subgraph stg-bucket        stg["tfstate"]    end    subgraph prd-bucket        prd["tfstate"]    end    end【種類グループ別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合AWSリソースの種類グループ別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、AWSリソースの種類グループ別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-repository/├── 📂 application/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── alb.tf│    ├── api_gateway.tf│    ├── cloudfront.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── ses.tf│    ├── sns.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # applicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # authコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 cicd/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── codebuild.tf│    ├── codecommit.tf│    ├── codedeploy.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # cicdコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # cicdコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # cicdコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 datastore/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── elasticache.tf│    ├── rds.tf│    ├── s3.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # datastoreコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # datastoreコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # datastoreコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 monitor/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── cloudwatch.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # monitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから参照できるように、outputブロックを定義する      ├── route53.tf      ├── vpc.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する           ...【種類グループ別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合AWSリソースの種類グループ別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、AWSリソースの種類グループ別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 application│    └── terraform.tfstate # applicationコンポーネントの状態を持つ│├── 📂 auth│    └── terraform.tfstate # authコンポーネントの状態を持つ│├── 📂 cicd│    └── terraform.tfstate # cicdコンポーネントの状態を持つ│├── 📂 datastore│    └── terraform.tfstate # datastoreコンポーネントの状態を持つ│├── 📂 monitor│    └── terraform.tfstate # monitorコンポーネントの状態を持つ│└── 📂 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...AWSリソースの状態の変更頻度グループ別この分割方法についてAWSリソースの状態の変更頻度グループ別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各変更頻度グループの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。oneplane comments on Best way to approach splitting up the state file【変更頻度グループ別】状態の依存関係図例えば、以下の変更頻度グループに分割した状況と仮定します。変更高頻度グループ変更中頻度グループ変更低頻度グループここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、変更低頻度グループの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: AWSリソースの状態の変更頻度グループ別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            high["high-freq-tfstate<br>例: API Gateway, CloudFront, CloudWatch, IAM"]            middle["middle-freq-tfstate<br>例: ALB, EC2, ECS, EKS, ElastiCache, RDS, S3, SES, SNS"]            low["low-freq-tfstate<br>例: Route53, VPC"]            high-...->low            middle-..->low        end    subgraph stg-bucket        stg["tfstate"]    end    subgraph prd-bucket        prd["tfstate"]    end    end【変更頻度グループ別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合AWSリソースの変更頻度グループ別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、AWSリソースの変更頻度グループ別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-repository/├── 📂 high-freq # 高頻度変更グループ│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── api_gateway.tf│    ├── cloudfront.tf│    ├── cloudwatch.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── iam.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # high-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # high-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # high-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 low-freq # 低頻度変更グループ│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── route53.tf│    ├── vpc.tf│    ├── 📂 tes│    │    ├── backend.tfvars # low-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg│    │    ├── backend.tfvars # low-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd│          ├── backend.tfvars # low-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 middle-freq # 中頻度変更グループ (高頻度とも低頻度とも言えないリソース)      ├── provider.tf      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── elasticache.tf      ├── rds.tf      ├── s3.tf      ├── ses.tf      ├── 📂 tes      │    ├── backend.tfvars # middle-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg      │    ├── backend.tfvars # middle-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd           ├── backend.tfvars # middle-freqコンポーネントの状態を持つ terraform.tfstate ファイルを指定する           ...【変更頻度グループ別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合AWSリソースの変更頻度グループ別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、AWSリソースの変更頻度グループ別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 high-freq│    └── terraform.tfstate # high-freqコンポーネントの状態を持つ│├── 📂 middle-freq│    └── terraform.tfstate # middle-freqコンポーネントの状態を持つ│└── 📂 low-freq      └── terraform.tfstate # low-freqコンポーネントの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...運用チーム責務範囲別 × プロダクトサブコンポーネント別 - ★この分割方法について運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせてtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各運用チーム内のサブコンポーネントの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。【チーム別 × サブコンポーネント別】状態の依存関係図以下の運用チームに分割した状況と仮定します。また、各運用チームでTerraformを変更できる管理者が相当数するため、プロダクトのサブコンポーネント別にも分割したとします。frontendチームapplicationmonitorbackendチームapplicationmonitorsreチームapplicationauthmonitornetworkここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、sreチームの管理する tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 運用チーム責務範囲別 × プロダクトサブコンポーネント別---%%{init:{'theme':'default'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            subgraph frontend-team               frontendApplication["application-tfstate<br>(CloudFront, S3, など)"]               frontendMonitor["monitor-tfstate<br>(CloudWatch, など)"]            end            subgraph backend-team                backendApplication["application-tfstate<br>(API Gateway, ElastiCache, RDS, SES, SNS, など)"]                backendMonitor["monitor-tfstate<br>(CloudWatch, など)"]            end            subgraph sre-team                sreApplication["application-tfstate<br>Web3層と周辺AWSリソース<br>(ALB, EC2, ECS, EKS, SNS, など)"]                auth["auth-tfstate<br>(IAM, など)"]                sreMonitor["monitor-tfstate<br>(CloudWatch, など)"]                network["network-tfstate<br>(Route53, VPC, など)"]            end            frontendApplication-...->network            sreApplication-...->auth            sreApplication-...->network            backendApplication-...->auth            backendApplication-...->network            frontendMonitor-...->frontendApplication            sreMonitor-...->sreApplication            backendMonitor-...->backendApplication        end    subgraph stg-bucket        stg["tfstate"]    end    subgraph prd-bucket        prd["tfstate"]    end    end【チーム別 × サブコンポーネント別】リポジトリのディレクトリー構成▼ 異なるリポジトリの場合この場合では、運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせて分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。🏭 aws-frontend-team-repository/├── 📂 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── cloudfront.tf│    ├── ses.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 monitor/      ├── provider.tf      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── cloudwatch.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境            ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する            ...🏭 aws-backend-team-repository/├── 📂 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── api_gateway.tf│    ├── elasticache.tf│    ├── rds.tf│    ├── ses.tf│    ├── sns.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 monitor/      ├── provider.tf      ├── remote_state.tf # 他の tfstate ファイルに依存する      ├── cloudwatch.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境            ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する            ...🏭 aws-sre-team-repository/├── 📂 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── alb.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│├── 📂 monitor/│    ├── provider.tf│    ├── remote_state.tf # 他の tfstate ファイルに依存する│    ├── cloudwatch.tf│    ├── 📂 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    ├── 📂 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│    │    ...│    ││    └── 📂 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ terraform.tfstate ファイルを指定する│          ...│└── 📂 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── 📂 tes/ # Tes環境      │    ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      ├── 📂 stg/ # Stg環境      │    ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する      │    ...      │      └── 📂 prd/ # Prd環境            ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ terraform.tfstate ファイルを指定する            ...▼ 同じリポジトリの場合運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせる分割パターンの場合、同じリポジトリで管理するとリポジトリが巨大になってしまいます。そのため、これはお勧めしません。【チーム別 × サブコンポーネント別】リモートバックエンドのディレクトリー構成▼ 異なるリモートバックエンドの場合運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせる分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせて分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリー構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境用バケットの場合🪣 tes-bucket/├── 📂 frontend-team│    ├── 📂 application│    │    └── terraform.tfstate # frontendチームが管理するapplicationコンポーネントの状態を持つ│    ││    └── 📂 monitor│         └── terraform.tfstate # frontendチームが管理するmonitorコンポーネントの状態を持つ│├── 📂 backend-team│    ├── 📂 application│    │    └── terraform.tfstate # backendチームが管理するapplicationコンポーネントの状態を持つ│    ││    └── 📂 monitor│          └── terraform.tfstate # backendチームが管理するmonitorコンポーネントの状態を持つ│└── 📂 sre-team      ├── 📂 application      │    └── terraform.tfstate # sreチームが管理するapplicationコンポーネントの状態を持つ      │      ├── 📂 auth      │    └── terraform.tfstate # sreチームが管理するauthコンポーネントの状態を持つ      │      ├── 📂 monitor      │    └── terraform.tfstate # sreチームが管理するmonitorコンポーネントの状態を持つ      │      └── 📂 network            └── terraform.tfstate # sreチームが管理するnetworkコンポーネントの状態を持つ# Stg環境用バケットの場合🪣 stg-bucket/│...# Prd環境用バケットの場合🪣 prd-bucket/│...10. おわりにTerraformのtfstateファイルの分割パターンをもりもり布教しました。ぜひ採用してみたい分割パターンはあったでしょうか。Terraformの開発現場の具体的な要件は千差万別であり、特にtfstateファイル間の状態の依存関係は様々です。もし、この記事を参考に設計してくださる方は、分割パターンを現場に落とし込んで解釈いただけると幸いです🙇🏻‍「自分を信じても…信頼に足る仲間を信じても…誰にもわからない…」(お友達の@nwiizo, 2023, Terraform Modules で再利用できるので最高ではないでしょうか？)謝辞今回、Terraformの分割パターンの収集にあたり、以下の方々からの意見・実装方法も参考にさせていただきました。@kiyo_12_07 さん@masasuzu さん@tozastation さん(アルファベット順)この場で感謝申し上げます🙇🏻‍]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[光に負けルナ~Google Cloudでのマルチリージョンデータベースについて~]]></title>
            <link>https://zenn.dev/nnaka2992/articles/to_beat_light_speed_on_google_cloud_databases</link>
            <guid>https://zenn.dev/nnaka2992/articles/to_beat_light_speed_on_google_cloud_databases</guid>
            <pubDate>Mon, 03 Jul 2023 15:39:08 GMT</pubDate>
            <content:encoded><![CDATA[クラウドを利用する一番のメリットの一つとしてオンデマンドでリソースを調達し、アクセス負荷に応じてスケールイン・アウト出来ることが上げられます。そのため大体のアプリケーションではシングルリージョンまたは隣接するリージョン2~3程度で運用を始めることが多いと思います。(日本の場合asia-northeast-1とasia-northeast-2など)アプリケーションがグローバルに拡大すると、それだけ物理的な距離が広がりユーザ・サーバ間のアクセスにかかる時間が拡大します。例えばユーザ・サーバ共に日本にある場合(沖縄・北海道間約3,000km)、ネットワークによる遅延は片道約15ms以下...]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[スリーシェイクに入社しました！]]></title>
            <link>https://bells17.medium.com/3-shake-279ea982b977?source=rss-713cf42ce34d------2</link>
            <guid>https://bells17.medium.com/3-shake-279ea982b977?source=rss-713cf42ce34d------2</guid>
            <pubDate>Mon, 03 Jul 2023 14:10:50 GMT</pubDate>
        </item>
    </channel>
</rss>