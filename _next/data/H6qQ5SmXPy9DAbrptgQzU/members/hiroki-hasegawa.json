{"pageProps":{"member":{"id":"hiroki-hasegawa","name":"Hiroki Hasegawa","role":"SRE","bio":"Let me know your favorite technology! ✌️","avatarSrc":"/avatars/hirokihasegawa.png","sources":["https://hiroki-hasegawa.hatenablog.jp/feed"],"includeUrlRegex":"","twitterUsername":"Hiroki__IT","githubUsername":"hiroki-it","websiteUrl":"https://hiroki-it.github.io/tech-notebook/"},"postItems":[{"title":"【ArgoCD🐙】ArgoCDのアーキテクチャと自動デプロイの仕組み","contentSnippet":"01. はじめに02. 概要アーキテクチャ▼ レイヤー▼ コンポーネント仕組み【１】【２】【３】【４】【５】【６】【７】【８】【９】【１０】03. repo-serverrepo-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】04. application-controller、redis-serverapplication-controllerとはredis-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】05. dex-serverdex-serverとは仕組み【１】【２】【３】【４】【５】【６】06. argocd-server (argocd-apiserver)argocd-serverとは仕組み【１】【２】【３】【４】【５】【６】【７】【８】【９】【１０】07. アーキテクチャのまとめ08. おわりに謝辞01. はじめにロケットに乗るArgoくんのツラが腹立つわー。さて最近の業務で、全プロダクトが共通基盤として使用するArgoCDとAWS EKS Clusterのアーキテクチャをリプレイスしています。採用した設計プラクティスの紹介も兼ねて、ArgoCDのアーキテクチャと自動デプロイの仕組みを記事で解説しました🚀ArgoCDは、kubectlコマンドによるマニフェストのデプロイを自動化するツールです。現在に至るまでArgoCDのアーキテクチャには変遷があり、今回紹介するのは執筆時点 (2023/05/02) 時点で最新の 2.6 系のアーキテクチャです。アーキテクチャや仕組みはもちろん、個々のマニフェストの実装にもちょっとだけ言及します。それでは、もりもり布教していきます😗02. 概要アーキテクチャ▼ レイヤーまずは、ArgoCDのアーキテクチャのレイヤーがどのようになっているかを見ていきましょう。ArgoCD公式から、コンポーネント図が公開されています。図から、次のようなことがわかります👇下位レイヤー向きにしか依存方向がなく、例えばコアドメインとインフラのレイヤー間で依存性は逆転させていない。レイヤーの種類 (UI、アプリケーション、コアドメイン、インフラ) とそれらの依存方向から、レイヤードアーキテクチャのような構成になっている。特にコアドメインレイヤーが独立したコンポーネントに分割されており、マイクロサービスアーキテクチャを採用している。↪️：argo-cd/components.md at master · argoproj/argo-cd · GitHubArgoCDのマイクロサービスアーキテクチャは、機能単位の分割方法を採用していると推測しています。本記事では詳しく言及しませんが、マイクロサービスアーキテクチャの分割方法には大小いくつかの種類があり、境界付けられたコンテキストで分割することがベタープラクティスと言われています😎(境界付けられたコンテキストについても、ちゃんと記事を投稿したい...)機能単位による分割は、境界付けられたコンテキストのそれよりも粒度が小さくなります。↪️：モノリスからマイクロサービスへ ―モノリスを進化させる実践移行ガイド | Sam Newman, 島田 浩二 |本 | 通販 | AmazonArgoCDでは、マイクロサービスアーキテクチャの設計図にコンポーネント図を使用しています。コンポーネント図では、依存方向 (そのコンポーネントがいずれのコンポーネントを使用するのか) に着目できます。そのため、これはマイクロサービス間の依存方向を視覚化するために有効なUML図です🙆🏻‍↪️：Component Diagrams - Code With Engineering Playbook▼ コンポーネント次に、コンポーネントの種類を紹介します。ArgoCDの各コンポーネントが組み合わさり、マニフェストの自動的なデプロイを実現します。ArgoCD (2.6系) のコンポーネントはいくつかあり、主要なコンポーネントの種類とレイヤーは以下の通りです👇 コンポーネント                    レイヤー               機能                                                                                                                                                                                                             argocd-server (argocd-apiserver)  UI / アプリケーション  みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能します。現在、複数のレイヤーの責務を持っており、将来的にUIとアプリケーションは異なるコンポーネントに分割されるかもしれません。  application-controller            コアドメイン           Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。                                                                                            repo-server                       コアドメイン           マニフェスト/チャートリポジトリからクローンを取得します。また、クローンからマニフェストを作成します。                                                                                                        redis-server                      インフラ               application-controllerの処理結果のキャッシュを保管します。                                                                                                                                                       dex-server                        インフラ               SSOを採用する場合に、argocd-serverの代わりに認可リクエストを作成し、IDプロバイダーにこれを送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。                             ↪️：Amazon | GitOps and Kubernetes: Continuous Deployment with Argo CD, Jenkins X, and Flux | Yuen, Billy, Matyushentsev, Alexander, Ekenstam, Todd, Suen, Jesse | PMP Exam仕組みそれでは、ArgoCDは、どのようにコンポーネントを組み合わせて、マニフェストをデプロイするのでしょうか？ここではデプロイ先Cluster管理者 (デプロイ先Clusterを管理するエンジニア) は、ArgoCDのダッシュボードを介してマニフェストをデプロイするとしましょう。まずは、概要を説明していきます。【１】ArgoCDのCluster上で、repo-serverがマニフェスト/チャートリポジトリのクローンを取得します。【２】application-controllerは、repo-serverからマニフェストを取得します。【３】application-controllerは、デプロイ先Clusterの現状を確認します。【４】application-controllerは、処理結果をredis-serverに保管します。【５】argocd-serverは、redis-serverからキャッシュを取得します。【６】デプロイ先Cluster管理者は、argocd-serverにログインしようとします。【７】argocd-serverは、ログイン時にIDプロバイダーに認可フェーズを委譲するために、dex-serverをコールします。【８】dex-serverは、IDプロバイダーに認可リクエストを作成し、これをIDプロバイダーに送信します。【９】argocd-serverで認可フェーズを実施します。ログインが完了し、デプロイ先Cluster管理者は認可スコープに応じてダッシュボードを操作できます。【１０】application-controllerは、Clusterにマニフェストをデプロイします。マニフェストのデプロイの仕組みをざっくり紹介しました。ただこれだと全く面白くないので、各コンポーネントの具体的な処理と、各々がどのように通信しているのかを説明します✌️03. repo-serverrepo-serverとはまずは、コアドメインレイヤーにあるrepo-serverです。マニフェスト/チャートリポジトリ (例：GiHub、GitHub Pages、Artifact Hub、AWS ECR、Artifact Registry、など) からクローンを取得します。repo-serverを持つPodには、他に軽量コンテナイメージからなるInitContainerとサイドカー (cmp-server) がおり、それぞれ機能が切り分けられています👍仕組み【１】repo-serverの起動時に、InitContainerでお好きなマニフェスト管理ツール (Helm、Kustomize、など) やプラグイン (helm-secrets、KSOPS、SOPS、argocd-vault-plugin、など) をインストールします。また、サイドカーのcmp-serverでは起動時に/var/run/argocd/argocd-cmp-serverコマンドを実行する必要があり、InitContainer (ここではcopyutilコンテナ) を使用して、ArgoCDのコンテナイメージからargocd-cliのバイナリファイルをコピーします。repo-serverのざっくりした実装例は以下の通りです👇ここでは、ArgoCDで使いたいツール (Helm、SOPS、helm-secrets) をInitContainerでインストールしています。apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: argocdspec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  initContainers:    # HelmをインストールするInitContainer    - name: helm-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # SOPSをインストールするInitContainer    - name: sops-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # helm-secretsをインストールするInitContainer    - name: helm-secrets-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /helm-working-dir/plugins          name: helm-working-dir    ...    # cmp-serverにargocd-cliのバイナリをコピーするInitContainer    - name: copyutil      image: quay.io/argoproj/argocd:latest      command:        - cp        - -n        - /usr/local/bin/argocd        - /var/run/argocd/argocd-cmp-server      volumeMounts:        - name: var-files          mountPath: /var/run/argocd  # Podの共有ボリューム  volumes:    - name: custom-tools      emptyDir: {}    - name: var-files      emptyDir: {}↪️：Custom Tooling - Argo CD - Declarative GitOps CD for KubernetesArgoCDのコンテナイメージ (quay.io/argoproj/argocd) には、いくつかのツール (例：Helm、Kustomize、Ks、Jsonnet、など) の推奨バージョンがあらかじめインストールされています。そのため、サイドカーのcmp-serverを採用しない場合は、これらのツールをインストールする必要はありません。反対に、cmp-serverを採用する場合はインストールが必要になります🙇🏻 公式が、推奨バージョンを含むサイドカー用イメージを公開してくれるのを待ちたいところなのですが、リポジトリからバージョンを取得することもできなくはないです🙆🏻‍# ArgoCDのコンテナイメージに内蔵されたHelmの推奨バージョンを取得する$ curl -s https://raw.githubusercontent.com/argoproj/argo-cd/<バージョンタグ>/hack/tool-versions.sh \\    | grep helm3_version | sed -e 's/^[^=]*=//'↪️：argo-cd/tool-versions.sh at master · argoproj/argo-cd · GitHubKustomizeによるマニフェスト作成は、サイドカーではなくrepo-serverで実行した方が良いかもしれません (Helmはサイドカーで問題ないです)。執筆時点 (2023/05/02) では、ArgoCDとKustomizeが密に結合しています。例えば、ArgoCD上のKustomize系オプションはrepo-serverでマニフェストを作成することを想定して設計されています。無理やりサイドカーでKustomizeを実行しようとすると、ArgoCDの既存のオプションを無視した実装になってしまうため、Kustomizeだけはrepo-serverで実行することをお勧めします😢【２】repo-serverは、Secret (argocd-repo-creds) からリポジトリの認証情報を取得します。argocd-repo-credsではリポジトリの認証情報のテンプレートを管理しています。指定した文字列から始まる (最長一致) URLを持つリポジトリに接続する場合に、それらの接続で認証情報を一括して適用できます。argocd-repo-credsのざっくりした実装例は以下の通りです👇ここでは、リポジトリのSSH公開鍵認証を採用し、argocd-repo-credsに共通の秘密鍵を設定しています。apiVersion: v1kind: Secretmetadata:  name: argocd-repo-creds-github  namespace: argocd  labels:    argocd.argoproj.io/secret-type: repo-credstype: Opaquedata:  type: git  url: https://github.com/hiroki-hasegawa  # 秘密鍵  sshPrivateKey: |    MIIC2 ...あとは、各リポジトリのSecret (argocd-repo) にURLを設定しておきます。すると、先ほどのargocd-repo-credsのURLに最長一致するURLを持つSecretには、一括して秘密鍵が適用されます。# foo-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: argocd  name: foo-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: bar-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/bar-chart.git---# baz-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: foo  name: baz-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: baz-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/baz-chart.git↪️：Declarative Setup - Argo CD - Declarative GitOps CD for Kubernetes【３】repo-serverは、認証情報を使用して、リポジトリにgit cloneコマンドを実行します。取得したクローンを、/tmp/_argocd-repoディレクトリ配下にUUIDの名前で保管します。また、リポジトリの変更をポーリングし、変更を検知した場合はgit fetchコマンドを実行します。# クローンが保管されていることを確認できる$ kubectl -it exec argocd-repo-server \\    -c repo-server \\    -n foo \\    -- bash -c \"ls /tmp/_argocd-repo/<URLに基づくUUID>\"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yaml↪️：custom repo-server - where is the local cache kept? · argoproj/argo-cd · Discussion #9889 · GitHub2.3以前では、repo-serverは/tmpディレクトリ配下にURLに基づく名前でクローンを保管します。$ kubectl -it exec argocd-repo-server \\    -c repo-server \\    -n foo \\    -- bash -c \"ls /tmp/https___github.com_hiroki-hasegawa_foo-repository\"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yaml【４】repo-serverは、Volume上のUnixドメインソケットを介して、サイドカー (cmp-server) によるプラグインの実行をコールします。Unixドメインソケットのエンドポイントの実体は.sockファイルです。$ kubectl exec -it argocd-repo-server -c foo-plugin-cmp-server\\    -- bash -c \"ls /home/argocd/cmp-server/plugins/\"foo-plugin.sockUnixソケットドメインは、同じOS上のファイルシステムを介して、データを直接的に送受信する仕組みです。Unixソケットドメインを使用すると、同じVolumeがマウントされたコンテナのプロセス間で、データを送受信できます👍↪️：ASCII.jp：Unixドメインソケット (1/2)【５】cmp-serverは、暗号化キー (例：AWS KMS、Google CKM、など) を使用してSecretストア (AWS SecretManager、Google SecretManager、SOPS、Vault、など) の暗号化変数を復号化します。クラウドプロバイダーがHTTPSプロトコルの使用を求める場合があります。cmp-serverに軽量なコンテナイメージを使用していると、/etc/sslディレクトリ (OSによる)　にSSL証明書が無く、cmp-serverがHTTPSプロトコルを使用できない可能性があります。その場合は、お好きな方法で証明書をインストールし、コンテナにマウントするようにしてください👍apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: foospec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  ...    # サイドカーのcmp-server    - name: helm-secrets-cmp-server      image: ubuntu:latest      ...      volumeMounts:        # サイドカーがAWS KMSを使用する時にHTTPSリクエストを送信する必要があるため、SSL証明書をマウントする        - name: certificate          mountPath: /etc/ssl  ...  initContainers:    - name: certificate-installer      image: ubuntu:latest      command:        - /bin/sh        - -c      args:        - |          apt-get update -y          # ルート証明書をインストールする          apt-get install -y ca-certificates          # 証明書を更新する          update-ca-certificates      volumeMounts:        - mountPath: /etc/ssl          name: certificate  volumes:    - name: certificate      emptyDir: {}【６】cmp-serverがマニフェスト管理ツール (例：Helm、Kustomize) でマニフェストを作成する時に、これのプラグイン (helm-secrets、argocd-vault-plugin、など) を使用したいようなユースケースがあるかと思います。マニフェストの作成時の追加処理として、ConfigMap配下のConfigManagementPluginでプラグインの処理を定義します。ざっくりした実装例は以下の通りです👇ここでは、プラグインとしてhelm-secretsを採用し、helm secrets templateコマンドの実行を定義します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmp-cm  namespace: foodata:  helm-secrets-plugin.yaml: |    apiVersion: argoproj.io/v1alpha1    kind: ConfigManagementPlugin    metadata:      namespace: foo      name: helm-secrets    spec:      generate:        command:          - /bin/bash          - -c        args:          - |            set -o pipefail            helm secrets template -f $ARGOCD_ENV_SECRETS -f $ARGOCD_ENV_VALUES -n $ARGOCD_APP_NAMESPACE $ARGOCD_APP_NAME .  foo-plugin.yaml: |    ...複数のConfigManagementPluginのマニフェストを定義できるように、各ConfigManagementPluginで異なるファイル名とし、ConfigMapで管理すると良いです👍【７】cmp-serverは、を実行し、Secretを含むマニフェストを作成します。ConfigMap配下のファイルをplugin.yamlの名前でサイドカーにマウントする必要があります。また、先ほどのUnixドメインソケットの.sockファイルや、 cmp-serverがプラグインを実行するための各種バイナリファイルもマウントが必要です。ざっくりした実装例は以下の通りです👇ここでは、helm-secretsプラグインを実行するサイドカー (helm-secrets-cmp-server) を作成します。apiVersion: v1kind: Podmetadata:  name: argocd-repo-serverspec:  containers:    # repo-server    - name: repo-server      image: quay.io/argoproj/argocd:latest    ...    # helm-secretsのcmp-server    - name: helm-secrets-cmp-server      # コンテナイメージは軽量にする      image: ubuntu:latest      command:        - /var/run/argocd/argocd-cmp-server      env:        # helmプラグインの場所を設定する        - name: HELM_PLUGINS          value: /helm-working-dir/plugins      securityContext:        runAsNonRoot: true        runAsUser: 999      volumeMounts:        # リポジトリのクローンをコンテナにマウントする        - name: tmp          mountPath: /tmp        # ConfigManagementPluginのマニフェスト (helm-secrets.yaml) を \"plugin.yaml\" の名前でコンテナにマウントする        - name: argocd-cmp-cm          mountPath: /home/argocd/cmp-server/config/plugin.yaml          subPath: helm-secrets.yaml        # コンテナ間で通信するためのUnixドメインソケットファイルをコンテナにマウントする        - name: plugins          mountPath: /home/argocd/cmp-server/plugins        # 任意のツールのバイナリファイルをコンテナにマウントする        - name: custom-tools          mountPath: /usr/local/bin        # helmプラグインのバイナリをコンテナにマウントする        - name: helm-working-dir          mountPath: /helm-working-dir/plugins      ...  # Podの共有ボリューム  volumes:    # リポジトリのクローンを含む    - name: tmp      emptyDir: {}    # Helmなどの任意のツールを含む    - name: custom-tools      emptyDir: {}    # helmプラグインを含む    - name: helm-working-dir      emptyDir: {}ArgoCDのv2.6では、ConfigManagementPluginのマニフェストを/home/argocd/cmp-server/configディレクトリに、plugin.yamlの名前でマウントしないといけません。これは、cmp-serverの起動コマンド (/var/run/argocd/argocd-cmp-server) がplugin.yamlの名前しか扱えないためです。今後のアップグレードで改善される可能性がありますが、v2.6では、ConfigManagementPluginの数だけcmp-serverが必要になってしまいます🙇🏻‍今回は詳しく言及しませんが、クラウドプロバイダーのSecretストア (例：AWS SecretManager、Google SecretManager、など) の変数を使用する場合は、Secretのデータ注入ツールのプラグイン (特にargocd-vault-plugin) は必須ではありません。この場合、代わりにSecretsストアCSIドライバーやExternalSecretsOperatorを使用できます。これらは、クラウドプロバイダーから変数を取得し、これをSecretにデータとして注入してくれます🙇🏻‍↪️：How to manage Kubernetes secrets with GitOps? | Akuity04. application-controller、redis-serverapplication-controllerとはコアドメインレイヤーにあるapplication-controllerです。Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。redis-serverとはインフラレイヤーにあるredis-serverです。application-controllerの処理結果のキャッシュを保管します。仕組み【１】ArgoCD用Clusterの管理者は、ClusterにArgoCD系のカスタムリソース (例：Application、AppProject、など)　をデプロイします。マニフェストを作成できます。️↪️：GitHub - argoproj/argo-helm: ArgoProj Helm ChartsただしHelmの重要な仕様として、チャートの更新時に使用するhelm upgradeコマンドは、CRDを作成できる一方でこれを変更できません。HelmでCRDを作成するとHelmの管理ラベルが挿入されてしまうため、作成の時点からCRDがHelmの管理外となるように、kubectlコマンドでCRDを作成した方がよいです👍$ kubectl diff -k \"https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>\"$ kubectl apply -k \"https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>\"ArgoCD上でHelmを使用してデプロイする場合はこの仕様を気にしなくて良いのかな、と思った方がいるかもしれないです。ですが本記事で解説した通り、ArgoCDはcmp-serverのhelm templateコマンド (この時、--include-crdsオプションが有効になっている) や、application-controllerのkubectl applyコマンドを組み合わせてマニフェストをデプロイしているため、CRDもちゃんと更新してくれます👍🏻️↪️：Helm | Custom Resource Definitions【２】kube-controller-managerは、application-controllerを操作し、Reconciliationを実施します。application-controllerは、Etcd上に永続化されたマニフェストと同じ状態のArgoCD系カスタムリソースを作成/変更します。How Operators work in Kubernetes | Red Hat Developer【３】application-controllerは、repo-serverからリポジトリのマニフェストを取得します。取得したマニフェストは、repo-serverのサイドカーであるcmp-serverが作成したものです。【４】application-controllerは、デプロイ先Clusterをヘルスチェックします。application-controllerには、gitops-engineパッケージが内蔵されており、これはヘルスチェックからデプロイまでの基本的な処理を実行します。ディレクトリからなります👇gitops-engine/├── pkg│   ├── cache│   ├── diff   # リポジトリとClusterの間のマニフェストの差分を検出する。ArgoCDのDiff機能に相当する。│   ├── engine # 他のパッケージを使い、GitOpsの一連の処理を実行する。│   ├── health # Clusterのステータスをチェックする。ArgoCDのヘルスチェック機能に相当する。│   ├── sync   # Clusterにマニフェストをデプロイする。ArgoCDのSync機能に相当する。│   └── utils  # 他のパッケージに汎用的な関数を提供する。│...↪️：gitops-engine/design-top-down.md at master · argoproj/gitops-engine · GitHub【５】application-controllerは、デプロイ先Clusterのマニフェストと、repo-serverから取得したマニフェストの差分を検出します。ここで、kubectl diffコマンドの実行が自動化されています。【６】application-controllerは、処理結果をredis-serverに保管します。redis-serverは、Applicationやリポジトリのコミットの単位で、application-controllerの処理結果を保管しています。$ kubectl exec -it argocd-redis-server \\    -n foo \\    -- sh -c \"redis-cli --raw\"127.0.0.1:6379> keys *...app|resources-tree|<Application名>|<キャッシュバージョン>cluster|info|<デプロイ先ClusterのURL>|<キャッシュバージョン>git-refs|<マニフェスト/チャートリポジトリのURL>|<キャッシュバージョン>mfst|app.kubernetes.io/instance|<Application名>|<最新のコミットハッシュ値>|<デプロイ先Namespace>|*****|<キャッシュバージョン>...【７】application-controllerは、Applicationの操作に応じて、Clusterにマニフェストをデプロイします。ここで、kubectl applyコマンドの実行が自動化されています。Kubernetesリソースのマニフェストには、metadata.managedFieldsキーがあり、何がそのマニフェストを作成/変更したのかを確認できます。実際にマニフェストを確認してみると、確かにapplication-controllerがマニフェストを作成/変更してくれたことを確認できます。apiVersion: apps/v1kind: Deploymentmetadata:  managedFields:    # ArgoCDのapplication-controllerによる管理    - manager: argocd-application-controller      apiVersion: apps/v1      # kube-apiserverに対するリクエスト内容      operation: Update      time: \"2022-01-01T16:00:00.000Z\"      # ArgoCDのapplication-controllerが管理するマニフェストのキー部分      fields: ...️↪️：Server-Side Apply | Kubernetes05. dex-serverdex-serverとはインフラレイヤーにあるdex-serverです。SSO (例：OAuth 2.0、SAML、OIDC) を採用する場合に、argocd-serverの代わりに認可リクエストを作成し、IDプロバイダー (例：GitHub、Keycloak、AWS Cognito、Google Auth、など) にこれを送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。↪️：GitHub - dexidp/dex: OpenID Connect (OIDC) identity and OAuth 2.0 provider with pluggable connectorsdex-serverを使わずに、argocd-serverからIDプロバイダーに認可リクエストを直接的に送信することもできます。執筆時点 (2023/05/02) で、argocd-serverは特にOIDCの認可リクエストを作成できるため、ログイン要件がOIDCの場合は、dex-serverを必ずしも採用してなくもよいです。言い換えれば、その他のSSO (例：OAuth 2.0、SAML) を使用する場合は、dex-serverを採用する必要があります👍️↪️：Overview - Argo CD - Declarative GitOps CD for Kubernetes仕組み【１】デプロイ先Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。【２】argocd-serverは、認証フェーズをIDプロバイダーに委譲するために、dex-serverをコールします。argo-cd/authz-authn.md at master · argoproj/argo-cd · GitHub【３】dex-serverは、認可リクエストを作成します。認可リクエストに必要な情報は、ConfigMap (argocd-cm) で設定しておく必要があります。argocd-cmのざっくりした実装例は以下の通りです👇ここでは、IDプロバイダーをGitHubとし、認可リクエストに必要なクライアントIDとクライアントシークレットを設定しています。apiVersion: v1kind: ConfigMapmetadata:  namespace: foo  name: argocd-cmdata:  dex.config: |    connectors:      - type: github        id: github        name: GitHub SSO        config:          clientID: *****          clientSecret: *****        # dex-serverが認可レスポンスを受信するURLを設定する        redirectURI: https://example.com/api/dex/callbackdex.configキー配下の設定方法に関しては、dexのドキュメントをみると良いです👍↪️：Dex【４】dex-serverは、前の手順で作成した認可リクエストをIDプロバイダーに送信します。【５】IDプロバイダー側でSSOの認証フェーズを実施します。IDプロバイダーは、コールバックURL (<ArgoCDのドメイン名>/api/dex/callback) を指定して、認可レスポンスを送信します。認可レスポンスは、argocd-serverを介して、dex-serverに届きます。GitHubをIDプロバイダーとする場合、 Developer settingsタブ でSSOを設定する必要があり、この時にAuthorization callback URLという設定箇所があるはずです👍🏻【６】argocd-serverは、AuthZで認可フェーズを実施します。ConfigMap (argocd-rbac-cm) を参照し、IDプロバイダーから取得したユーザーやグループに、ArgoCD系リソースに関する認可スコープを付与します。ざっくりした実装例は以下の通りです👇ここでは、developerロールにはdevというAppProjectに属するArgoCD系リソースにのみ、またmaintainerロールには全てのAppProjectの操作を許可しています。またこれらのロールを、IDプロバイダーで認証されたグループに紐づけています。特定のArgoCD系リソースのみへのアクセスを許可すれば、結果として特定のClusterへのデプロイのみを許可したことになります👍apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  # デフォルトのロール  policy.default: role:developer  policy.csv: |    # ロールと認可スコープを定義する。    p, role:developer, *, *, dev/*, allow    p, role:maintainer, *, *, *, allow    # IDプロバイダーで認証されたグループにロールを紐付ける。    g, developers, role:developer    g, maintainers, role:maintainer  scopes: \"[groups]\"ConfigMap (argocd-rbac-cm) の認可スコープの定義には、 Casbin の記法を使用します。今回の実装例で使用したpとgでは、以下を定義できます。    記号        説明        記法    p (パーミッション)         ロールと認可スコープを定義する。        p, <ロール名> <Kubernetesリソースの種類> <アクション名> <AppProject名>/<Kubernetesリソースの識別名>    g (グループ)         グループにロールを紐付ける。        g, <グループ名> <ロール名>    RBAC Configuration - Argo CD - Declarative GitOps CD for Kubernetes06. argocd-server (argocd-apiserver)argocd-serverとは最後に、インフラレイヤーにあるargocd-serverです。『argocd-apiserver』とも呼ばれます。みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能し、他のコンポーネントと通信します🦄仕組み【１】application-controllerは、デプロイ先Clusterをヘルスチェックします。【２】application-controllerは、デプロイ先Clusterのマニフェストと、ポーリング対象のリポジトリのマニフェストの差分を検出します。【３】application-controllerは、処理結果をredis-serverに保管します。【４】argocd-serverは、redis-serverから処理結果を取得します。【５】デプロイ先Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。【６】Ingressコントローラーは、Ingressのルーティングルールを参照し、argocd-serverにルーティングします。【７】argocd-serverは、ログイン時にIDプロバイダーに認可フェーズを委譲するために、dex-serverをコールします。【８】IDプロバイダー上で認証フェーズが完了します。argocd-serverは、ConfigMap (argocd-rbac-cm) を参照し、デプロイ先Cluster管理者に認可スコープを付与します。【９】argocd-serverは、認可スコープに応じて、デプロイ先Cluster管理者がApplicationを操作できるようにします。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:  # 設定してはダメ  # application.namespaces: \"*\" # 全てのNamespaceを許可する。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: dev-foo-project  namespace: foospec:  # 設定してはダメ  # sourceNamespaces:  #  - \"foo\"これらにより、`foo`に属するArgoCDは、他のNamespaceにはアクセスできなくなります👍↪️：Installation - Argo CD - Declarative GitOps CD for Kubernetes【１０】デプロイ先Cluster管理者は、ダッシュボード (argocd-server) を使用して、ClusterにマニフェストをSyncします。この時、Applicationを介してapplication-controllerを操作し、マニフェストをデプロイします。図では、App-Of-Appsパターンを採用したと仮定しています👨‍👩‍👧‍👦デザインパターンがあります。これは、Applicationを階層上に作成するものであり、最下層のApplication配下のマニフェストをより疎結合に管理できます✌️例えば以下の画像の通り、最上位のApplication配下に、チーム別の親Applicationを配置します (アプリチームの親Application、インフラチームのそれ) 。その後、両方のApplication配下にさらにチャート別に最下層の子Applicationを配置し、チャートのデプロイを管理します。アプリチーム最下層の子Applicationではアプリコンテナのチャート、インフラチームの子Applicationでは監視/ネットワーク/ハードウェアリソース管理系のチャートを管理します👍07. アーキテクチャのまとめ今までの全ての情報をざっくり整理して簡略化すると、ArgoCDは以下の仕組みでマニフェストをデプロイすることになります👇08. おわりにArgoCDによるデプロイの仕組みの仕組みをもりもり布教しました。ArgoCDは、UIが使いやすく、仕組みの詳細を知らずとも比較的簡単に運用できるため、ユーザーフレンドリーなツールだと思っています。もしArgoCDを使わずにマニフェストをデプロイしている方は、ArgoCDの採用をハイパー・ウルトラ・アルティメットおすすめします👍なお、登場した設計プラクティスのいくつかは、以下の書籍にも記載されていますので、ぜひご一読いただけると🙇🏻‍↪️：Amazon.co.jp: GitOps Cookbook (English Edition) 電子書籍: Vinto, Natale, Bueno, Alex Soto: 洋書Amazon | Argo CD in Practice: The GitOps way of managing cloud-native applications | Costea, Liviu, Economakis, Spiros, Matyushentsev, Alexander | Design Tools & Techniques謝辞ArgoCDの設計にあたり、@yaml_villager さんに有益なプラクティスをご教授いただきました。この場で感謝申し上げます🙇🏻‍","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/05/02/145115","isoDate":"2023-05-02T05:42:57.000Z","dateMiliSeconds":1683006177000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"【Istio⛵️】Istioの安全なアップグレード手法の仕組み","contentSnippet":"01. はじめに02. Istioのアップグレード手法を説明する前にカナリアリリースとはカナリアリリースの手順【１】【２】【３】【４】『カナリアリリース』の呼称の由来03. アップグレード手法の概要ここで説明すること【１】アップグレード前の検証【２】新Istiodのインストール【３】Webhookの宛先のServiceの変更【４】IngressGatewayをインプレースアップグレード【５】一部のNamespaceのistio-proxyコンテナをアップグレード【６】ユーザの手を借りたテスト【７】istio-proxyコンテナの段階的なアップグレード【８】旧Istiodのアンインストール04. アップグレード手法の詳細ここで説明すること前提NamespaceIstiodIngressGatewayマイクロサービス【１】アップグレード前の検証ここで実施することistioctl x precheckコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ 宛先のServiceを決めるMutatingWebhookConfiguration【２】新Istiodのインストールここで実施することistioctl versionコマンドistioctl installコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ Webhookの宛先のServiceを決めるMutatingWebhookConfiguration【３】Webhookの宛先のServiceの変更ここで実施することistioctl tag setコマンド【４】IngressGatewayをインプレースアップグレードここで実施することkubectl rollout restartコマンド【５】一部のNamespaceのistio-proxyコンテナをアップグレードここで実施することkubectl rollout restartコマンド【６】ユーザの手を借りたテストここで実施することもし問題が起こった場合【７】istio-proxyコンテナの段階的なアップグレードここで実施することkubectl rollout restartコマンド【８】旧Istiodのアンインストールここで実施することistioctl uninstallコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ 宛先のServiceを決めるMutatingWebhookConfiguration05. おわりに01. はじめに隠しません。有吉弘行のサンデーナイトドリーマーが生きがいです。さて、最近の業務でIstioをひたすらアップグレードしています。採用したアップグレード手法の紹介も兼ねて、Istioの安全なアップグレード手法の仕組みを記事で解説しました🚀執筆時点 (2023/02/26) では、IstioのIstiodコントロールプレーン (以降、Istiodとします) のアップグレード手法には、『インプレース方式』と『カナリア方式』があります。また合わせてアップグレードが必要なIstioのIngressGatewayにも、その手法に『インプレース方式』と『カナリア方式』があります。今回の安全なアップグレード手法として、Istiodでは『カナリアアップグレード』、IngressGatewayでは『インプレースアップグレード』を採用します。それでは、もりもり布教していきます😗↪️：Istio / Canary UpgradesIstio / Installing Gateways02. Istioのアップグレード手法を説明する前にカナリアリリースとはIstiodのカナリアアップグレードが理解しやすくなるように、カナリアリリースから説明したいと思います。カナリアリリースは、実際のユーザーにテストしてもらいながらリリースする手法です。もしカナリアリリースをご存知の方は、 03. アップグレード手法の概要 まで飛ばしてください🙇🏻‍カナリアリリースの手順カナリアリリースは、一部のユーザーを犠牲にすることになる一方で、アプリを実地的にテストできる点で優れています。手順を交えながら説明します。↪️：CanaryRelease【１】旧環境のアプリを残したまま、新環境をリリースします。この段階では、全てのユーザー (100%) を旧環境にルーティングします。【２】ロードバランサーで重み付けを変更し、一部のユーザー (ここでは10%) を新環境にルーティングします。【３】ユーザーの手を借りて新環境を実地的にテストします (例：該当のエラーメトリクスが基準値を満たすか) 。【４】新環境に問題が起こらなければ、重み付けを段階的に変更し、最終的には全てのユーザー (100%) を新環境にルーティングします。『カナリアリリース』の呼称の由来カナリアリリースについては、その呼称の由来を知ると、より理解が深まります。カナリアリリースは、20世紀頃の炭坑労働者の危機察知方法に由来します。炭鉱内には有毒な一酸化炭素が発生する場所がありますが、これは無色無臭なので、気づくことに遅れる可能性があります。そこで当時の炭鉱労働者は、一酸化炭素に敏感な『カナリア』を炭鉱内に持ち込み、カナリアの様子から一酸化炭素の存在を察知するようにしていたそうです。つまり、先ほどの『犠牲になる一部のユーザー』が、ここでいうカナリアというわけです😨画像引用元：George McCaa, U.S. Bureau of Mines↪️：About canary deployment in simple wordsThis Device Was Used to Resuscitate Canaries in Coal Mines After They Signaled Danger03. アップグレード手法の概要ここで説明することカナリアリリースについて理解したところで、Istioの安全なアップグレード手法の概要を説明します。おおよそ以下の手順からなります。なお各番号は、04. アップグレード手法の詳細 の【１】〜【８】に対応しています。【１】アップグレード前の検証旧Istiodが稼働しています。ここで、アップグレードが可能かどうかを検証しておきます。【２】新Istiodのインストール新Istiod (discoveryコンテナ) をインストールします。【３】Webhookの宛先のServiceの変更新Istiodのistio-proxyコンテナをインジェクションできるように、Webhookの宛先のServiceを変更します。この手順は重要で、後の 【３】Webhookの宛先のServiceの変更 で詳細を説明しています。【４】IngressGatewayをインプレースアップグレードIngressGatewayをインプレースアップグレードします。【５】一部のNamespaceのistio-proxyコンテナをアップグレード一部のNamespaceで、istio-proxyコンテナをカナリアアップグレードします。カナリアリリースのような重み付けがなく、カナリアアップグレードの『カナリア』という呼称に違和感を持つ方がいるかもしれません。これについては、全てのNamespaceのistio-proxyコンテナを一斉にアップグレードするのではなく、段階的にアップグレードしていく様子を『カナリア』と呼称している、と個人的に推測しています。もし『カナリアアップグレード』の由来をご存じの方は、ぜひ教えていただけると🙇🏻‍【６】ユーザの手を借りたテストユーザーの手を借りて、実地的にテストします (例：該当のエラーメトリクスが基準値以下を満たすか) 。【７】istio-proxyコンテナの段階的なアップグレード新Istiodのistio-proxyコンテナに問題が起こらなければ、他のNamespaceでもistio-proxyコンテナを段階的にカナリアアップグレードしていきます。一方でもし問題が起これば、Namespaceのistio-proxyコンテナとIngressGatewayをダウングレードします。【８】旧Istiodのアンインストール最後に、旧Istiodをアンインストールします。↪️：Istio / Canary Upgrades04. アップグレード手法の詳細ここで説明することここからは、03. アップグレード手法の概要 を深ぼっていきます。ヤサイニンニクアブラマシマシな説明になってしまったので、ここまでを食べ切れた方のみ進むことをお勧めします🥺今回は、ドキュメントで一番優先して記載されているistioctlコマンドを使用した手順を説明します。なお各番号は、03. アップグレード手法の概要 の【１】〜【８】に対応しています。istioctlコマンド以外のツール (例：helmコマンド、helmfileコマンド、ArgoCD) を使用してもアップグレードできます。細かな手順が異なるだけで、アップグレード手法の概要に関しては同じです🙆🏻‍前提Namespaceまず最初に、前提となる状況を設定しておきます。各Namespaceのistio.io/revラベルにdefaultが設定されているとします。$ kubectl get namespace -L istio.io/revNAME              STATUS   AGE   REVfoo               Active   34d   defaultbar               Active   34d   defaultbaz               Active   34d   defaultistio-ingress     Active   34d   default...エイリアスはどんな値でも問題なく、よくあるエイリアスとしてdefaultやstableなどを使用します。さらに、マニフェストに書き起こすと以下のようになっています。apiVersion: v1kind: Namespacemetadata:  name: foo  labels:    istio.io/rev: defaultこのistio.io/revラベルがあることにより、そのNamespaceのPodにistio-proxyコンテナを自動的にインジェクションします。istio-proxyコンテナのインジェクションの仕組みについては、こちら記事で説明してます。もし気になる方はこちらもよろしくどうぞ🙇🏻‍Istiodすでに1-14-6のIstiodが動いており、1-15-4にカナリアアップグレードします。IstiodはDeployment配下のPodであり、このPodはIstiodの実体であるdiscoveryコンテナを持ちます。$ kubectl get deployment -n istio-system -l app=istiodNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6          1/1     1            1           47s # 1-14-6IngressGatewayIngressGatewayはIstiodとは異なるNamespaceで動いており、インプレースアップグレードします。IngressGatewayはistio-proxyコンテナを持ちます。$ kubectl get deployment -n istio-ingressNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistio-ingressgateway   1/1     1            1           47sセキュリティのベストプラクティスでは、IstiodとIngressGatewayは異なるNamespaceで動かすことが推奨されています。↪️：Istio / Installing Gatewaysマイクロサービス各Namespaceでマイクロサービスが動いています。マイクロサービスのPodはistio-proxyコンテナを持ちます。$ kubectl get deployment -n fooNAME   READY   UP-TO-DATE   AVAILABLE   AGEfoo    2/2     1            1           47s...$ kubectl get deployment -n barNAME   READY   UP-TO-DATE   AVAILABLE   AGEbar    2/2     1            1           47s..$ kubectl get deployment -n bazNAME   READY   UP-TO-DATE   AVAILABLE   AGEbaz    2/2     1            1           47s...【１】アップグレード前の検証ここで実施することアップグレード前に、現在のKubernetes Clusterがアップグレード要件を満たしているかを検証します。↪️：Before you upgradeistioctl x precheckコマンドistioctl x precheckコマンドを実行し、アップグレード要件を検証します。問題がなければ、istioctlコマンドはNo issue ...の文言を出力します。$ istioctl x precheck✅ No issues found when checking the cluster.Istiois safe to install or upgrade!  To get started, check out https://istio.io/latest/docs/setup/getting-started/もし、問題がある場合、istioctlコマンドはエラー文言を出力します。例えば、Istioのistio-proxyコンテナのインジェクションではkube-apiserverと通信する必要があります。そのため、kube-apiserverのバージョンが古すぎるせいでIstioが非対応であると、エラーになります😭kubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、現在のIstiodのバージョンを確認します👀まずはIstiodのDeploymentを確認すると、1-14-6のDeploymentがあります。$ kubectl get deployment -n istio-system -l app=istiodNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6          1/1     1            1           47s # 1-14-6istio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です👇▼ Webhookの宛先のService次に、 Serviceを確認すると、1-14-6のServiceがあります。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-14-6   ClusterIP   10.96.93.151     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   109s # 1-14-6このServiceは、kube-apiserverからIstiodへのWebhookを仲介することにより、istio-proxyコンテナのインジェクションを可能にします。istio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です👇▼ 宛先のServiceを決めるMutatingWebhookConfiguration最後に、MutatingWebhookConfigurationを確認すると、istio-revision-tag-<エイリアス>とistio-sidecar-injector-<リビジョン番号>のMutatingWebhookConfigurationがあります。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # カナリアアップグレード用istio-sidecar-injector-1-14-6   2          2m16s # インプレースアップグレード用のため今回は言及しないistio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です👇これらのうち、前者 (istio-revision-tag-<エイリアス>) をカナリアアップグレードのために使用します。このMutatingWebhookConfigurationは、Webhookの宛先のServiceを決めるため、結果的にistio-proxyコンテナのバージョンを決めます。ここで、MutatingWebhookConfigurationのistio.io/revラベルとistio.io/tagラベルの値も確認しておきます。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\    | yq '.metadata.labels'...istio.io/rev: 1-14-6istio.io/tag: default...istio.io/revラベルはIstiodのバージョン、istio.io/tagラベルはこれのエイリアスを表しています。また、.webhooks[].namespaceSelectorキー配下のistio.io/revキーの検知ルールを確認します。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\    | yq '.webhooks[]'...namespaceSelector:  matchExpressions:    - key: istio.io/rev      operator: In      values:        - default...合わせて、.webhooks[].clientConfig.serviceキー配下のServiceを名前を確認します。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\    | yq '.webhooks[].clientConfig'...service:  name: istiod-1-14-6...ここで、重要な仕組みをおさらいします。Namespaceでistio.io/revラベルにdefaultを設定してあるとします。すると、上記のMutatingWebhookConfigurationがこれを検知します。MutatingWebhookConfigurationにはdefaultに対応するIstioのリビジョンが定義されており、kube-apiserverが特定のIstioのバージョンのServiceにWebhookを送信できるようになります🎉↪️：Istio / Safely upgrade the Istio control plane with revisions and tags【２】新Istiodのインストールここで実施することそれでは、新Istiodをインストールします。↪️：Control planeistioctl versionコマンド新しくインストールするIstiodのバージョンは、istioctlコマンドのバージョンで決まります。そこで、istioctl versionコマンドを実行し、これのバージョンを確認します。$ istioctl versionclient version: 1.15.4        # アップグレード先のバージョンcontrol plane version: 1.14.6 # 現在のバージョンdata plane version: 1.14.6istioctl installコマンドカナリアアップグレードの場合、istioctl installコマンドを実行します。ドキュメントではrevisionキーの値がcanaryですが、今回は1-15-4とします。この値は、Istioが使用する様々なKubernetesリソースの接尾辞や、各種リソースのistio.io/revラベルの値になります。$ istioctl install --set revision=1-15-4WARNING: Istio is being upgraded from 1.14.6 -> 1.15.4WARNING: Before upgrading, you may wish to use 'istioctl analyze' to check for IST0002 and IST0135 deprecation warnings.✅ Istio core installed✅ Istiod installed✅ Ingress gateways installed✅ Installation completeThank you for installing Istio 1.15.  Please take a few minutes to tell us about your install/upgrade experience!kubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、istioctl installコマンドで何をインストールしたのかを確認します👀まずはIstiodのDeploymentを確認すると、1-15-4というDeploymentが新しく増えています。$ kubectl get deployment -n istio-system -l app=istiodNAME            READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6   1/1     1            1           47s # 1-14-6istiod-1-15-4   1/1     1            1           47s # 1-15-4接尾辞の1-15-4は、revisionキーの値で決まります。この段階では、旧Istiodと新Istioが並行的に稼働しており、kube-apiserverはまだ旧Istiodと通信しています今の状況は以下の通りです👇▼ Webhookの宛先のService次に Webhookの宛先のServiceを確認すると、istiod-1-15-4というServiceが新しく増えています。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-14-6   ClusterIP   10.96.93.151     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   109s # 1-14-6istiod-1-15-4   ClusterIP   10.104.186.250   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   87s  # 1-15-4この段階では、まだWebhookの宛先はistiod-1-14-6のServiceです。今の状況は以下の通りです👇▼ Webhookの宛先のServiceを決めるMutatingWebhookConfiguration最後にMutatingWebhookConfigurationを確認すると、istio-sidecar-injector-1-15-4というMutatingWebhookConfigurationが新しく増えています。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # カナリアアップグレードで使用するistio-sidecar-injector-1-14-6   2          2m16sistio-sidecar-injector-1-15-4   2          2m16sカナリアアップグレードでは、istio-revision-tag-<エイリアス>のMutatingWebhookConfigurationを使用します。今の状況は以下の通りです👇【３】Webhookの宛先のServiceの変更ここで実施することこの手順では、エイリアスのistio.io/tagラベルはそのままに、istio.io/revラベルの値を変更します。さらに、Webhookの宛先のServiceを変更します。↪️：Default tagSafely upgrade the Istio control plane with revisions and tagsistioctl tag setコマンドistioctl tag setコマンドを実行し、istio.io/revラベルの値と宛先のServiceを変更します。$ istioctl tag set default --revision 1-15-4 --overwrite実行後に、もう一度MutatingWebhookConfigurationを確認すると、istio.io/revラベルの値が変わっています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\    | yq '.metadata.labels'...istio.io/rev: 1-15-4istio.io/tag: default...また、Webhookの宛先のServiceも変わっています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\    | yq '.webhooks[].clientConfig'...service:  name: istiod-1-15-4...これらにより、Webhookの宛先が1-15-4のServiceとなります。そのため、1-15-4のistio-proxyコンテナをインジェクションできるようになります。今の状況は以下の通りです👇【４】IngressGatewayをインプレースアップグレードここで実施することWebhookの宛先が1-15-4のServiceに変わったところで、IngressGatewayをインプレースアップグレードします。↪️：In place upgradekubectl rollout restartコマンドkubectl rollout restartコマンドを実行し、IngressGatewayをインプレースアップグレードします。$ kubectl rollout restart deployment istio-ingressgateway-n istio-ingress再作成したPodのイメージを確認してみると、istio-proxyコンテナを1-15-4にアップグレードできています。$ kubectl get pod bar -n bar -o yaml | yq '.spec.containers[].image'docker.io/istio/proxyv2:1.15.4 # istio-proxyコンテナkubectl getコマンドの代わりに、istioctl proxy-statusコマンドを使用して、アップグレードの完了を確認してもよいです。今の状況は以下の通りです👇【５】一部のNamespaceのistio-proxyコンテナをアップグレードここで実施すること続けて、一部のNamespaceのistio-proxyコンテナをアップグレードします。Podの再作成により、新Istiodのistio-proxyコンテナがインジェクションされるため。istio-proxyコンテナをアップグレードできます。↪️：Data planekubectl rollout restartコマンド前提にあるように、Namespaceには foo bar baz があります。kubectl rollout restartコマンドを実行し、barのistio-proxyコンテナからアップグレードします。$ kubectl rollout restart deployment bar -n bar再作成したPodのイメージを確認してみると、istio-proxyコンテナを1-15-4にアップグレードできています。$ kubectl get pod bar -n bar -o yaml | yq '.spec.containers[].image'bar-app:1.0 # マイクロサービスdocker.io/istio/proxyv2:1.15.4 # istio-proxyコンテナkubectl getコマンドの代わりに、istioctl proxy-statusコマンドを使用して、アップグレードの完了を確認してもよいです。今の状況は以下の通りです👇【６】ユーザの手を借りたテストここで実施することIstioを部分的にアップグレードしたところで、アップグレードが完了したNamespaceをテストします。ユーザーの手を借りて実地的にテストします (例：該当のエラーメトリクスが基準値を満たすか) 。今の状況は以下の通りです👇もし問題が起こった場合もし問題が起こった場合、1-14-6にダウングレードしていきます。istioctl tag setコマンドを実行し、istio.io/revラベルの値を元に戻します。$ istioctl tag set default --revision 1-14-6 --overwriteその後、kubectl rollout restartコマンドの手順を実行し、istio-proxyコンテナをダウングレードしてきます。【７】istio-proxyコンテナの段階的なアップグレードここで実施すること先ほどのNamespaceで問題が起こらなければ、残ったNamespace (foo、baz、...) のistio-proxyコンテナも段階的にアップグレードしていきます。kubectl rollout restartコマンド同様にkubectl rollout restartコマンドを実行し、istio-proxyコンテナからアップグレードします。$ kubectl rollout restart deployment foo -n foo$ kubectl rollout restart deployment baz -n baz...最終的に、全てのNamespacemのistio-proxyコンテナが新しくなります。今の状況は以下の通りです👇【８】旧Istiodのアンインストールここで実施すること最後に、旧Istiodのアンインストールします。↪️：Uninstall old control planeistioctl uninstallコマンドistioctl uninstallコマンドを実行し、旧Istiodをアンインストールします。$ istioctl uninstall --revision 1-14-6✅ Uninstall complete今の状況は以下の通りです👇kubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、istioctl uninstallコマンドで何をアンインストールしたのかを確認します👀まずはIstiodのDeploymentを確認すると、1-14-6というDeploymentが無くなっています。$ kubectl get deployment -n istio-system -l app=istiodNAME            READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-15-4   1/1     1            1           47s # 1-15-4▼ Webhookの宛先のService次に Webhookの宛先のServiceを確認すると、istiod-1-14-6というServiceが無くなっています。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-15-4   ClusterIP   10.104.186.250   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   87s  # 1-15-4▼ 宛先のServiceを決めるMutatingWebhookConfiguration最後にMutatingWebhookConfigurationを確認すると、istio-sidecar-injector-1-14-6というMutatingWebhookConfigurationが無くなっています。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # 次のカナリアアップグレードでも使用するistio-sidecar-injector-1-15-4   2          2m16sこれで、新Istiodに完全に入れ替わったため、アップグレードは完了です。今の状況は以下の通りです👇05. おわりにIstioの安全なアップグレード手法の仕組みをもりもり布教しました。Istioへの愛が溢れてしまいました。Istioのアップグレードの異常がシステムに与える影響力は非常に大きく、様々な問題 (体験談：istio-proxyコンテナのPodへのインジェクションがずっと完了せず、アプリコンテナを作成できない) が起こる可能性があります😇これからIstioを採用予定の方は、Istioを安全にアップグレードするために十分に準備しておくことをお勧めします👍","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/02/26/202548","isoDate":"2023-02-26T11:25:48.000Z","dateMiliSeconds":1677410748000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"【Istio⛵️】Istioのサイドカーインジェクションの仕組み","contentSnippet":"01. はじめに02. サイドカーによるサービスメッシュなぜサイドカーが必要なのかサイドカープロキシメッシュ03. admission-controllersアドオンについてadmission-controllersアドオンとはadmissionプラグインの種類MutatingAdmissionWebhookプラグインMutatingAdmissionWebhookプラグインとはAdmissionReview、AdmissionRequest、AdmissionResponse▼ AdmissionReview▼ AdmissionRequest▼ AdmissionResponse04. サイドカーインジェクションの仕組み全体のフロークライアント ➡︎ kube-apiserverここで説明するフロー箇所【１】Podの作成をリクエストkube-apiserver ➡︎ Serviceここで説明するフロー箇所【２】認証/認可処理をコール【３】アドオンの処理をコール【４】AdmissionRequestに値を詰める【５】AdmissionReviewを送信Service ➡︎ webhookサーバーここで説明するフロー箇所【６】15017番ポートにポートフォワーディングkube-apiserver ⬅︎ Service ⬅︎ webhookサーバー (※逆向きの矢印)ここで説明するフロー箇所【７】patch処理を定義【８】AdmissionResponseに値を詰める【９】AdmissionReviewを返信kube-apiserver ➡︎ etcdここで説明するフロー箇所【１０】patch処理をコール【１１】マニフェストを永続化クライアント ⬅︎ kube-apiserverここで説明するフロー箇所【１２】コール完了を返信以降の仕組み05. おわりに01. はじめにどーも。正月で激太りしましたが、ダイエットの予定はありません🙋🏻‍さて、前回の記事の時と同様に、最近の業務でもオンプレとAWS上のIstioをひたすら子守りしています。子守りの前提知識の復習もかねて、サービスメッシュを実装するIstioのサイドカーインジェクションを記事で解説しました🚀執筆時点 (2023/01/14) では、Istioが実装するサービメッシュには、『サイドカープロキシメッシュ』と『アンビエントメッシュ』があります。サイドカープロキシメッシュの仕組みの軸になっているものは、サイドカーコンテナであるistio-proxyコンテナです。Istioは、KubernetesのPodの作成時に、istio-proxyコンテナをPod内に自動的にインジェクション (注入) しますそれでは、もりもり布教していきます😗02. サイドカーによるサービスメッシュなぜサイドカーが必要なのかそもそも、なぜサービスメッシュでサイドカーが必要になったのでしょうか？マイクロサービスアーキテクチャのシステムには、アーキテクチャ固有のインフラ領域の問題 (例：サービスディスカバリーの必要性、マイクロサービス間通信の暗号化、テレメトリー作成、など) があります。アプリエンジニアが各マイクロサービス内にインフラ領域の問題に関するロジックを実装すれば、これらの問題の解決できます。しかし、アプリエンジニアはアプリ領域の問題に責務を持ち、インフラ領域の問題はインフラエンジニアで解決するようにした方が、互いに効率的に開発できます。そこで、インフラ領域の問題を解決するロジックをサイドカーとして切り分けます。これにより、アプリエンジニアとインフラエンジニアの責務を分離できるようになり、凝集度が高くなる。また、インフラ領域の共通ロジックをサイドカーとして各マイクロサービスに提供できるため、単純性が高まります。こういった流れの中で、サイドカーを使用したサービスメッシュが登場しました。↪️：servicemesh.es | Service Mesh ComparisonWhat is Service Mesh and Why is it Necessary?サイドカープロキシメッシュIstioのサイドカーによるサービスメッシュ (サイドカープロキシメッシュ) は、サイドカーコンテナ (istio-proxyコンテナ) が稼働するデータプレーンサイドカーを中央集権的に管理するIstiod (discoveryコンテナ) が稼働するコントロールプレーンからなります。↪️：Istio / Architecture03. admission-controllersアドオンについてadmission-controllersアドオンとはIstioのPod内へのサイドカーインジェクションの前提知識として、admission-controllersアドオンを理解する必要があります。もし、admission-controllersアドオンをご存知の方は、 04. サイドカーインジェクションの仕組み まで飛ばしてください🙇🏻‍kube-apiserverでは、admission-controllersアドオンとして有効化できます。有効化すると、認証ステップと認可ステップの後にmutating-admissionステップとvalidating-admissionステップを実行でき、admissionプラグインの種類に応じた処理を挿入できます。クライアント (kubectlクライアント、Kubernetesリソース) からのリクエスト (例：Kubernetesリソースに対する作成/更新/削除、kube-apiserverからのプロキシへの転送) 時に、各ステップでadmissionプラグインによる処理 (例：アドオンビルトイン処理、独自処理) を発火させられます。↪️：Admission Controllers Reference | KubernetesKubernetes Best Practices: Blueprints for Building Successful Applications on Kubernetes: Burns, Brendan, Villalba, Eddie, Strebel, Dave, Evenson, Lachlan: 9781492056478: Amazon.com: Booksadmissionプラグインの種類admission-controllersアドオンのadmissionプラグインには、たくさんの種類があります。IstioがPod内にサイドカーをインジェクションする時に使用しているアドオンは、『MutatingAdmissionWebhook』です。CertificateApprovalCertificateSigningCertificateSubjectRestrictionDefaultIngressClassDefaultStorageClassDefaultTolerationSecondsLimitRangerMutatingAdmissionWebhook 👈 これ！NamespaceLifecyclePersistentVolumeClaimResizePodSecurityPriorityResourceQuotaRuntimeClassServiceAccountStorageObjectInUseProtectionTaintNodesByConditionValidatingAdmissionWebhook↪️：Admission Controllers Reference | KubernetesMutatingAdmissionWebhookプラグインMutatingAdmissionWebhookプラグインとはMutatingAdmissionWebhookプラグインを使用すると、mutating-admissionステップ時に、リクエスト内容を変更する処理をフックできます。フックする具体的な処理として、webhookサーバーにAdmissionRequestリクエストとして送信することにより、レスポンスのAdmissionResponseに応じてリクエスト内容を動的に変更します。MutatingWebhookConfigurationで、MutatingAdmissionWebhookプラグインの発火条件やwebhookサーバーの宛先情報を設定します。MutatingWebhookConfigurationの具体的な実装については、サイドカーインジェクションの仕組みの中で説明していきます。↪️：Diving into Kubernetes MutatingAdmissionWebhook | by Morven Cao | IBM Cloud | MediumKubernetes Admission Webhook覚書き - gashirar's blogAdmission Webhookを作って遊んで、その仕組みを理解しよう（説明編）AdmissionReview、AdmissionRequest、AdmissionResponse▼ AdmissionReviewAdmissionReviewは以下のようなJSONであり、kube-apiserverとwebhookサーバーの間でAdmissionRequestとAdmissionResponseを運びます。{  \"apiVersion\": \"admission.k8s.io/v1\",  \"kind\": \"AdmissionReview\",  # AdmissionRequest  \"request\": {},  # AdmissionResponse  \"response\": {},}↪️：v1 package - k8s.io/api/admission/v1 - Go Packages▼ AdmissionRequestAdmissionRequestは以下のようなJSONです。kube-apiserverがクライアントから受信した操作内容が持つことがわかります。例で挙げたAdmissionRequestでは、クライアントがDeploymentをCREATE操作するリクエストをkube-apiserverに送信したことがわかります。{  \"apiVersion\": \"admission.k8s.io/v1\",  \"kind\": \"AdmissionReview\",  # AdmissionRequest  \"request\": {    ...    # 変更されるKubernetesリソースの種類を表す。    \"resource\": {      \"group\": \"apps\",      \"version\": \"v1\",      \"resource\": \"deployments\"    },    # kube-apiserverの操作の種類を表す。    \"operation\": \"CREATE\",    ...  }}↪️：Dynamic Admission Control | Kubernetes▼ AdmissionResponse一方でAdmissionResponseは、例えば以下のようなJSONです。AdmissionResponseに応じたマニフェスト変更処理をpatchキーの値に持ち、これはbase64方式でエンコードされています。{  \"apiVersion\": \"admission.k8s.io/v1\",  \"kind\": \"AdmissionReview\",  # AdmissionResponse  \"response\": {      \"uid\": \"<value from request.uid>\",      # 宛先のwebhookサーバーが受信したか否かを表す。      \"allowed\": true,      # PathによるPatch処理を行う。      \"patchType\": \"JSONPatch\",      # Patch処理の対象となるKubernetesリソースと処理内容を表す。base64方式でエンコードされている。      \"patch\": \"W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\",    },}エンコード値をデコードしてみると、例えば以下のようなpatch処理が定義されています。# patchキーをbase64方式でデコードした場合[{\"op\": \"add\", \"path\": \"/spec/replicas\", \"value\": 3}]マニフェストに対する操作 (op) 、キー (path) 、値 (value) が設定されています。kube-apiserverがこれを受信すると、指定されたキー (.spec.replicas) に値 (3) に追加します。↪️：Dynamic Admission Control | Kubernetes04. サイドカーインジェクションの仕組み全体のフロー前提知識を踏まえた上で、admission-controllersアドオンの仕組みの中で、サイドカーのistio-proxyコンテナがどのようにPodにインジェクションされるのかを見ていきましょう。最初に、サイドカーインジェクションのフローは以下の通りになっています。(画像はタブ開き閲覧を推奨)↪️：Amazon.co.jp: Istio in Action (English Edition) 電子書籍: Posta, Christian E., Maloku, Rinor: 洋書クライアント ➡︎ kube-apiserverここで説明するフロー箇所『クライアント ➡︎ kube-apiserver』の箇所を説明します。(画像はタブ開き閲覧を推奨)【１】Podの作成をリクエストまずは、クライアントがkube-apiserverにリクエストを送信するところです。クライアント (Deployment、DaemonSet、StatefulSet、を含む) は、Podの作成リクエストをkube-apiserverに送信します。この時のリクエスト内容は、以下の通りとします。# Podを作成する。$ kubectl apply -f foo-pod.yaml# foo-pod.yamlファイルapiVersion: v1kind: Podmetadata:  name: foo-pod  namespace: foo-namespacespec:  containers:    - name: foo      image: foo:1.0.0      ports:        - containerPort: 80またNamespaceでは、あらかじめistio-proxyコンテナのインジェクションが有効化されているとします。Istioではv1.10以降、リビジョンの番号のエイリアスを使用して、istio-proxyコンテナのインジェクションを有効化するようになりました。apiVersion: v1kind: Namespacemetadata:  name: foo-namespace  labels:    # istio-proxyコンテナのインジェクションを有効化する。    # エイリアスは自由    istio.io/rev: <エイリアス>↪️：Istio / Announcing Support for 1.8 to 1.10 Direct Upgradesistio.io/revラベル値は、どんなエイリアスでも良いです。よくあるエイリアスとしてdefaultやstableなどを使用します👍kube-apiserver ➡︎ Serviceここで説明するフロー箇所『kube-apiserver ➡︎ Service』の箇所を説明します。(画像はタブ開き閲覧を推奨)【２】認証/認可処理をコールkube-apiserverは、認証ステップと認可ステップにて、クライアントからのリクエストを許可します。【３】アドオンの処理をコールkube-apiserverは、mutating-admissionステップにて、MutatingAdmissionWebhookプラグインの処理をコールします。前提知識の部分で具体的な実装を省略しましたが、Istioのバージョン1.14.3時点で、MutatingWebhookConfigurationは以下のようになっています。Namespaceでサイドカーインジェクションを有効化する時に使用したエイリアスは、このMutatingWebhookConfigurationで実体のリビジョン番号と紐づいています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yamlapiVersion: admissionregistration.k8s.io/v1beta1kind: MutatingWebhookConfigurationmetadata:  name: istio-revision-tag-default  labels:    app: sidecar-injector    # エイリアスの実体    istio.io/rev: <リビジョン番号>    # リビジョン番号のエイリアス    istio.io/tag: <エイリアス>webhooks:  - name: rev.namespace.sidecar-injector.istio.io    # MutatingAdmissionWebhookプラグインの処理の発火条件を登録する。    rules:      - apiGroups: [\"\"]        apiVersions: [\"v1\"]        operations: [\"CREATE\"]        resources: [\"pods\"]        scope: \"*\"    # Webhookの前段にあるServiceの情報を登録する。    clientConfig:      service:        name: istiod-<リビジョン番号>        namespace: istio-system        path: \"/inject\" # エンドポイント        port: 443      caBundle: Ci0tLS0tQk ...    # Namespace単位のサイドカーインジェクション    # 特定のNamespaceでMutatingAdmissionWebhookプラグインの処理を発火させる。    namespaceSelector:      matchExpressions:        - key: istio.io/rev          operator: DoesNotExist        - key: istio-injection          operator: DoesNotExist    # Pod単位のサイドカーインジェクション    # 特定のオブジェクトでMutatingAdmissionWebhookプラグインの処理を発火させる。    objectSelector:      matchExpressions:        - key: sidecar.istio.io/inject          operator: NotIn          values:            - \"false\"        - key: istio.io/rev          operator: In          values:            - <エイリアス>    ...MutatingWebhookConfigurationには、MutatingAdmissionWebhookプラグインの発火条件やwebhookサーバーの宛先情報を定義します。MutatingAdmissionWebhookプラグインの発火条件に関して、例えばIstioでは、 NamespaceやPod.metadata.labelsキーに応じてサイドカーインジェクションの有効化/無効化を切り替えることができ、これをMutatingAdmissionWebhookプラグインで制御しています。webhookサーバーの宛先情報に関して、Istioではwebhookサーバーの前段にServiceを配置しています。MutatingAdmissionWebhookプラグインが発火した場合、Serviceの/inject:443にHTTPSプロトコルのリクエストを送信するようになっています。また、宛先のServiceの名前がistiod-<リビジョン番号>となっていることからもわかるように、Serviceは特定のバージョンのIstiodコントロールプレーンに対応しており、想定外のバージョンのIstiodコントロールプレーンを指定しないように制御しています。一方で発火しなかった場合には、以降のAdmissionReviewの処理には進みません。【４】AdmissionRequestに値を詰めるkube-apiserverは、mutating-admissionステップにて、クライアントからのリクエスト内容 (Podの作成リクエスト) をAdmissionReveiew構造体のAdmissionRequestに詰めます。{  \"apiVersion\": \"admission.k8s.io/v1\",  \"kind\": \"AdmissionReview\",  # AdmissionRequest  \"request\": {    ...    # 変更されるKubernetesリソースの種類を表す。    \"resource\": {      \"group\": \"core\",      \"version\": \"v1\",      \"resource\": \"pods\"    },    # kube-apiserverの操作の種類を表す。    \"operation\": \"CREATE\",    ...  }}【５】AdmissionReviewを送信kube-apiserverは、mutating-admissionステップにて、Serviceの/inject:443にAdmissionReview構造体を送信します。Service ➡︎ webhookサーバーここで説明するフロー箇所『Service ➡︎ webhookサーバー』の箇所を説明します。(画像はタブ開き閲覧を推奨)【６】15017番ポートにポートフォワーディングServiceは、/inject:443でリクエストを受信し、discoveryコンテナの15017番ポートにポートフォワーディングします。Istioのバージョン1.14.3時点で、Serviceは以下のようになっています。$ kubectl get svc istiod-service -n istio-system -o yamlapiVersion: v1kind: Servicemetadata:  labels:    app: istiod  name: istiod-<リビジョン番号>  namespace: istio-systemspec:  type: ClusterIP  selector:    app: istiod    istio.io/rev: <リビジョン番号>  ports:    - name: grpc-xds      port: 15010      protocol: TCP      targetPort: 15010    - name: https-dns      port: 15012      protocol: TCP      targetPort: 15012    # webhookサーバーにポートフォワーディングする。    - name: https-webhook      port: 443      protocol: TCP      targetPort: 15017    - name: http-monitoring      port: 15014      protocol: TCP      targetPort: 15014.spec.selector.istio.io/revキーに、ポートフォワーディング先のPodを指定するためのリビジョン番号が設定されており、このPodはdiscoveryコンテナを持ちます。Istioは、discoveryコンテナ内でwebhookサーバーを実行し、15017番ポートでリクエストを待ち受けます。discoveryコンテナがリクエストを待ち受けているポート番号を見てみると、15017番ポートでリッスンしていることを確認できます👍$ kubectl exec foo-istiod -n istio-system -- netstat -tulpnActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:9876          0.0.0.0:*               LISTEN      1/pilot-discoverytcp6       0      0 :::15017                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::8080                 :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15010                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15012                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15014                :::*                    LISTEN      1/pilot-discovery↪️：istio/webhook.go at 1.14.3 · istio/istio · GitHubIstio / Application Requirementskube-apiserver ⬅︎ Service ⬅︎ webhookサーバー (※逆向きの矢印)ここで説明するフロー箇所『kube-apiserver ⬅︎ Service ⬅︎ webhookサーバー』の箇所を説明します。矢印が逆向きなことに注意してください。(画像はタブ開き閲覧を推奨)【７】patch処理を定義仕組みの中でも、ここは重要な部分です。discoveryコンテナ内のwebhookサーバーは、リクエスト内容を書き換えるためのpatch処理を定義します。webhookサーバーは、マニフェストの.spec.containers[1]パスにistio-proxyキーを追加させるようなpatch処理を定義します。この定義によって、結果的にサイドカーのインジェクションが起こるということになります。[  ...  {    \"op\": \"add\",    # .spec.initContainers[1] を指定する。    \"path\": \"/spec/initContainers/1\",    # マニフェストに追加される構造を表す。    \"value\": {      \"name\": \"istio-init\",      \"resources\": {                     ...      }    }  },  {    \"op\": \"add\",    # .spec.containers[1] を指定する。    \"path\": \"/spec/containers/1\",    # マニフェストに追加される構造を表す。    \"value\": {      \"name\": \"istio-proxy\",      \"resources\": {                     ...      }    }  }  ...]↪️：istio/webhook.go at a19b2ac8af3ad937640f6e29eed74472034de2f5 · istio/istio · GitHubistio/webhook_test.go at 1.14.3 · istio/istio · GitHubサイドカーコンテナのistio-proxyコンテナの他に、InitContainerのistio-initコンテナもインジェクションできるようにします。このistio-initコンテナは、istio-proxyコンテナを持つPodでインバウンド/アウトバウンド通信の経路を制御するために、Pod内にiptablesのルールを適用する責務を担っています💪🏻↪️：Istio Sidecar's interception mechanism for traffic - SoByte【８】AdmissionResponseに値を詰めるdiscoveryコンテナ内のwebhookサーバーは、patch処理の定義をAdmissionReveiew構造体のAdmissionResponseに詰めます。patchキーの値に、先ほどのpatch処理の定義をbase64方式でエンコードした文字列が割り当てられています。{  \"apiVersion\": \"admission.k8s.io/v1\",  \"kind\": \"AdmissionReview\",  # AdmissionResponse  \"response\": {      \"uid\": \"*****\",      \"allowed\": true,      \"patchType\": \"JSONPatch\",      # Patch処理の対象となるKubernetesリソースと処理内容を表す。base64方式でエンコードされている。      \"patch\": \"<先ほどのpatch処理の定義をbase64方式でエンコードした文字列>\",    },}↪️：istio/webhook.go at 1.14.3 · istio/istio · GitHub【９】AdmissionReviewを返信discoveryコンテナ内のwebhookサーバーは、AdmissionReview構造体をレスポンスとしてkube-apiserverに返信します。kube-apiserver ➡︎ etcdここで説明するフロー箇所『kube-apiserver ➡︎ etcd』の箇所を説明します。(画像はタブ開き閲覧を推奨)【１０】patch処理をコールkube-apiserverは、AdmissionReview構造体を受信し、AdmissionResponseに応じてリクエスト内容を書き換えます。patch処理の定義をAdmissionReview構造体から取り出し、クライアントからのリクエスト内容を書き換えます。具体的には、istio-proxyコンテナとistio-initコンテナを作成できるように、リクエストしたマニフェストの該当箇所にキーを追加します。apiVersion: v1kind: Podmetadata:  name: foo-pod  namespace: foo-namespacespec:  containers:    - name: foo      image: foo:1.0.0      ports:        - containerPort: 80    # kube-apiserverが追加    - name: istio-proxy      ...  # kube-apiserverが追加  initContainers:    - name: istio-init    ...【１１】マニフェストを永続化kube-apiserverは、etcdにPodのマニフェストを永続化します。クライアント ⬅︎ kube-apiserverここで説明するフロー箇所『クライアント ⬅︎ kube-apiserver』の箇所を説明します。(画像はタブ開き閲覧を推奨)【１２】コール完了を返信kube-apiserverは、クライアントにレスポンスを受信します。$ kubectl apply -f foo-pod.yaml# kube-apiserverからレスポンスが返ってくるpod \"foo-pod\" created以降の仕組み(画像はタブ開き閲覧を推奨)kube-apiserverは、他のNodeコンポーネント (kube-controlleretcd、kube-scheduler、kubelet、など) と通信し、Podを作成します。このPodのマニフェストは、アプリコンテナの他に、istio-proxyコンテナとistio-initコンテナを持ちます。結果として、サイドカーコンテナのistio-proxyコンテナをインジェクションしたことになります。コンポーネントの通信については、以下の記事が非常に参考になりました🙇🏻‍↪️：Kubernetes Master Components: Etcd, API Server, Controller Manager, and Scheduler | by Jorge Acetozi | jorgeacetozi | Medium05. おわりにIstioのサイドカーインジェクションの仕組みをもりもり布教しました。Istioへの愛が溢れてしまいました。今回登場したMutatingAdmissionWebhookプラグインに関して、私の関わっているプロダクトではIstio以外 (例：CertManager、Prometheus、AWSのaws-eks-vpc-cniアドオン、など) でも使用しています✌️そのため、MutatingAdmissionWebhookプラグインをどのように使っているのかを一度知れば、知識の汎用性が高いと考えています。サイドカーインジェクションはIstioでも基本的な機能であり、もし未体験の方がいらっしゃれば、お手元でサイドカーコンテナが追加されることを確認していただくとよいかもしれません👍","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/01/14/223815","isoDate":"2023-01-14T13:38:15.000Z","dateMiliSeconds":1673703495000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"【Istio⛵️】Istioのサービスディスカバリーの仕組み","contentSnippet":"01. はじめに02. サービスディスカバリーについてマイクロサービスアーキテクチャにおけるサービスディスカバリーサービスディスカバリーとはなぜサービスディスカバリーが必要なのかサービスディスカバリーの要素サービスディスカバリーのパターンサービスディスカバリーのパターンとはサーバーサイドパターンクライアントサイドパターン03. IstioのサービスディスカバリーIstioのサービスディスカバリーの仕組み全体像【１】【２】【３】【４】【５】discoveryコンテナの仕組みistio-proxyコンテナの仕組み04. istio-proxyコンテナ内のEnvoyの仕組みEnvoyの処理の流れ全体像【１】【２】【３】【４】【５】【６】EnvoyがADS-APIから取得した宛先情報を見てみようconfig_dumpエンドポイントリスナー値▼ 確認方法▼ 結果ルート値▼ 確認方法▼ 結果クラスター値▼ 確認方法▼ 結果エンドポイント値▼ 確認方法▼ 結果Envoyの処理の流れのまとめ【１】【２】【３】【４】【５】【６】05. おわりに謝辞01. はじめに3-shake Advent Calender 2022 最終日の記事です🎅私は普段は 俺の技術ノート に知見を記録しており、はてなブログはデビュー戦となります。最近の業務で、オンプレとAWS上のIstioをひたすら子守りしています。子守りの前提知識の復習もかねて、サービスメッシュを実装するIstioのサービスディスカバリーを記事で解説しました🚀Istioの機能の一つである『サービスディスカバリー』は、その仕組みの多くをEnvoyに頼っているため、合わせてEnvoyの仕組みも説明します。それでは、もりもり布教していきます😗3-shake SRE Tech Talk で発表した内容に加えて、スライドの余白と発表時間の制約で記載できなかったことも記載しました。02. サービスディスカバリーについてマイクロサービスアーキテクチャにおけるサービスディスカバリーサービスディスカバリーとはマイクロサービスアーキテクチャでは、マイクロサービスからマイクロサービスにリクエストを送信する場面があります。サービスディスカバリーとは、宛先マイクロサービスの宛先情報 (例：IPアドレス、完全修飾ドメイン名、など) を検出し、送信元マイクロサービスが宛先マイクロサービスにリクエストを継続的に送信できるようにする仕組みのことです。なぜサービスディスカバリーが必要なのかそもそも、なぜサービスディスカバリーが必要なのでしょうか。マイクロサービスアーキテクチャでは、システムの信頼性 (定められた条件下で定められた期間にわたり、障害を発生させることなく実行する程度) を担保するために、マイクロサービスのインスタンスの自動スケーリングを採用します。この時、自動スケーリングのスケールアウトでマイクロサービスが増加するたびに、各インスタンスには新しい宛先情報が割り当てられてしまいます。また、マイクロサービスが作り直された場合にも、宛先情報は更新されてしまいます。このように、たとえインスタンスの宛先情報が更新されたとしても、インスタンスへのリクエストに失敗しない仕組みが必要です。サービスディスカバリーの要素サービスディスカバリーの仕組みは、次の要素からなります。名前解決に関しては、DNSベースのサービスディスカバリー (例：CoreDNS + Service + kube-proxyによるサービスディスカバリー) で必要となり、Istioでは使いません。そのため、本記事では言及しないこととします🙇🏻‍ 要素                    責務                                                                    送信元マイクロサービス  リクエストを送信する。                                                  宛先マイクロサービス    リクエストを受信する。                                                  サービスレジストリ      宛先マイクロサービスの宛先情報を保管する。                              ロードバランサー        宛先マイクロサービスのインスタンスにロードバランシングする。            名前解決                宛先マイクロサービスへのリクエスト送信時に、名前解決できるようにする。 サービスディスカバリーのパターンサービスディスカバリーのパターンとはサービスディスカバリーの仕組みにはいくつか種類があります。Istioのサービスディスカバリーは、このうちのサーバーサイドパターンを実装したものになります。サーバーサイドパターン送信元マイクロサービスから、問い合わせとロードバランシングの責務が切り離されています。送信元マイクロサービスは、ロードバランサーにリクエストを送信します。ロードバランサーは、宛先マイクロサービスの宛先をサービスレジストリに問い合わせ、またリクエストをロードバランシングする責務を担っています💪🏻(例) Istio、Linkerd、など↪️：Amazon.co.jp: Cloud Native Patterns: Designing change-tolerant software (English Edition) 電子書籍: Davis, Cornelia: 洋書Server-side service discovery patternクライアントサイドパターン通信の送信元マイクロサービスは、宛先マイクロサービスの宛先をサービスレジストリに問い合わせ、さらにロードバランシングする責務を担います。(例) NetflixのEureka、など↪️：Amazon.co.jp: Cloud Native Patterns: Designing change-tolerant software (English Edition) 電子書籍: Davis, Cornelia: 洋書Client-side service discovery patternService Discovery in Kubernetes: Combining the Best of Two Worlds03. IstioのサービスディスカバリーIstioのサービスディスカバリーの仕組みIstioが実装するサービスメッシュには、サイドカープロキシメッシュとアンビエントメッシュがあり、今回はサイドカープロキシメッシュのサービスディスカバリーを取り上げます。Istioのサービスディスカバリーは、discoveryコンテナとistio-proxyコンテナが軸となり、サーバーサイドパターンのサービスディスカバリーを実装します。全体像【１】〜【６】の全体像は、以下の通りです👇istio-proxyコンテナは、サービスレジストリへの問い合わせと、ロードバランシングする責務を担っていることに注目してください。【１】kube-apiserverは、Pod等の宛先情報をetcd等に保管します。これは、Kubernetesの通常の仕組みです。【２】discoveryコンテナは、kube-apiserverからPod等の宛先情報を取得し、自身に保管します。【３】istio-proxyコンテナは、discoveryコンテナからPod等の宛先情報を双方向ストリーミングRPCで取得します。【４】送信元マイクロサービスがリクエストを送信します。サーバーサイドパターンでの責務通り、送信元マイクロサービスはロードバランサー (ここではistio-proxyコンテナ) にリクエストを送信します。この時、送信元マイクロサービスがistio-proxyコンテナに直接的にリクエストを送信しているというよりは、iptablesがistio-proxyコンテナにリクエストをリダイレクトします。istio-proxyコンテナこれを受信します。【５】istio-proxyコンテナは、リクエストをロードバランシングし、宛先Podにこれを送信します。↪️：Amazon | Istio in Action | Posta, Christian E., Maloku, Rinor | Software DevelopmentJimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式Tech-赵化冰的博客 | Zhaohuabing Blogdiscoveryコンテナの仕組みdiscoveryコンテナを詳しく見てみましょう。discoveryコンテナは、別名Istiodと呼ばれています。XDS-APIというエンドポイントを公開しており、XDS-APIのうち、サービスディスカバリーに関係するAPIは以下の通りです。 APIの種類  説明                                                   LDS-API    Envoyのリスナー値を取得できる。                        RDS-API    Envoyのルート値を取得できる。                          CDS-API    Envoyのクラスター値を取得できる。                      EDS-API    Envoyのエンドポイント値できる。                        ADS-API    各XDS-APIから取得できる宛先情報を整理して取得できる。 discoveryコンテナは、kube-apiserverからPod等の宛先情報を取得して自身のメモリ上に保管し、各XDS-APIから提供します。XDS-APIとistio-proxyコンテナの間では、gRPCの双方向ストリーミングRPCの接続が確立されています。そのため、istio-proxyコンテナからのリクエストに応じて宛先情報を返却するだけでなく、リクエストがなくとも、XDS-APIからもistio-proxyコンテナに対して宛先情報を送信します。各種XDS-APIから個別に宛先情報を取得できますが、Envoy上で宛先情報のバージョンの不整合が起こる可能性があるため、Istioでは実際にはADS-APIを使用しています。↪️：Amazon | Istio in Action | Posta, Christian E., Maloku, Rinor | Software Developmentistio-proxyコンテナの仕組みistio-proxyコンテナを詳しく見てみましょう。istio-proxyコンテナでは、pilot-agentとEnvoyが稼働しています。先ほどistio-proxyコンテナは、双方向ストリーミングRPCでADS-APIから宛先情報を取得すると説明しました。厳密にはEnvoyが、pilot-agentを介して、ADS-APIから双方向ストリーミングRPCで宛先情報を取得します。istio-proxyコンテナが送信元マイクロサービスからリクエストを受信すると、EnvoyはADS-APIから取得した宛先情報に基づいて、宛先マイクロサービスのインスタンスにロードバランシングします。↪️：Amazon | Istio in Action | Posta, Christian E., Maloku, Rinor | Software DevelopmentJimmy Song - 专注于探索后 Kubernetes 时代的云原生新范式Tech-赵化冰的博客 | Zhaohuabing Blog04. istio-proxyコンテナ内のEnvoyの仕組みEnvoyの処理の流れEnvoyがADS-APIから取得した宛先情報を見ていく前に、Envoyの処理の流れを解説します。istio-proxyコンテナ内のEnvoyでは、以下の仕組みでリクエストを処理します。全体像【１】〜【６】の全体像は、以下の通りです👇【１】istio-proxyコンテナは、送信元マイクロサービスからリクエストを受信します。【２】Envoyは、リクエストの宛先情報 (例：宛先IPアドレス、ポート番号、パス、ホスト、など) に応じてリスナー値を選びます。【３】Envoyは、リスナーに紐づくルート値を選びます。【４】Envoyは、クラスターに紐づくクラスター値を選びます。【５】Envoyは、クラスターに紐づくエンドポイント値を選びます。【６】Envoyは、エンドポイント値に対応するインスタンスにリクエストを送信します。Envoyで確認した宛先情報を👆に当てはめて見ていくことにしましょう。↪️：Amazon.co.jp: Istio in Action (English Edition) 電子書籍: Posta, Christian E., Maloku, Rinor: 洋書Amazon | Istio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe | Calcote, Lee, Butcher, Zack | Design Tools & TechniquesArchitecture Analysis of Istio: The Most Popular Service Mesh Project - Alibaba Cloud CommunityEnvoyがADS-APIから取得した宛先情報を見てみようconfig_dumpエンドポイント実際にEnvoyに登録されている宛先情報は、istio-proxyコンテナ自体のlocalhost:15000/config_dumpからJSONで取得できます。もしお手元にIstioがある場合は、Envoyにどんな宛先情報が登録されているか、Envoyを冒険してみてください。$ kubectl exec \\    -it foo-pod \\    -n foo-namespace \\    -c istio-proxy \\    -- bash -c \"curl http://localhost:15000/config_dump\" | yq -PJSONだと見にくいので、yqコマンドでYAMLに変換すると見やすくなります👍リスナー値▼ 確認方法istio-proxyコンテナがADS-APIから取得したリスナー値は、/config_dump?resource={dynamic_listeners}から確認できます。ここでは、foo-pod内でbar-podのリスナー値を確認したと仮定します。$ kubectl exec \\    -it foo-pod \\    -n foo-namespace \\    -c istio-proxy \\    -- bash -c \"curl http://localhost:15000/config_dump?resource={dynamic_listeners}\" | yq -P▼ 結果以下を確認できました。宛先IPアドレスや宛先ポート番号に応じてリスナー値を選べるようになっており、ここでは<任意のIPアドレス>:50002。リスナー値に紐づくルート値の名前configs:  - \"@type\": type.googleapis.com/envoy.admin.v3.ListenersConfigDump.DynamicListener    # リスナー名    name: 0.0.0.0_50002    active_state:      version_info: 2022-11-24T12:13:05Z/468      listener:        \"@type\": type.googleapis.com/envoy.config.listener.v3.Listener        name: 0.0.0.0_50002        address:          socket_address:            # 受信したパケットのうちで、宛先IPアドレスでフィルタリング            address: 0.0.0.0            # 受信したパケットのうちで、宛先ポート番号でフィルタリング            port_value: 50002        filter_chains:          - filter_chain_match:              transport_protocol: raw_buffer              application_protocols:                - http/1.1                - h2c            filters:              - name: envoy.filters.network.http_connection_manager                typed_config:                  \"@type\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager                  stat_prefix: outbound_0.0.0.0_50001                  rds:                    config_source:                      ads: {}                      initial_fetch_timeout: 0s                      resource_api_version: V3                    # 本リスナーに紐づくルート値の名前                    route_config_name: 50002  ...  - \"@type\": type.googleapis.com/envoy.admin.v3.ListenersConfigDump.DynamicListener  ...↪️：Administration interface — envoy 1.27.0-dev-71a50b documentationConfigDump (proto) — envoy 1.27.0-dev-71a50b documentationルート値▼ 確認方法istio-proxyコンテナがADS-APIから取得したリスナー値は、/config_dump?resource={dynamic_route_configs}から確認できます。ここでは、foo-pod内でbar-podのルート値を確認したと仮定します。$ kubectl exec \\    -it foo-pod \\    -n foo-namespace \\    -c istio-proxy \\    -- bash -c \"curl http://localhost:15000/config_dump?resource={dynamic_route_configs}\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。リスナー値を取得した時に確認できたルート値の名前リクエストのパスやHostヘッダーに応じてルート値を選べるようになっているルート値に紐づくクラスター値の名前configs:  - \"@type\": type.googleapis.com/envoy.admin.v3.RoutesConfigDump.DynamicRouteConfig    version_info: 2022-11-24T12:13:05Z/468    route_config:      \"@type\": type.googleapis.com/envoy.config.route.v3.RouteConfiguration      # ルート値の名前      name: 50002      virtual_hosts:        - name: bar-service.bar-namespace.svc.cluster.local:50002          # ホストベースルーティング          domains:            - bar-service.bar-namespace.svc.cluster.local            - bar-service.bar-namespace.svc.cluster.local:50002            - bar-service            - bar-service:50002            - bar-service.bar-namespace.svc            - bar-service.bar-namespace.svc:50002            - bar-service.bar-namespace            - bar-service.bar-namespace:50002            - 172.16.0.2            - 172.16.0.2:50002          routes:            - match:                # パスベースルーティング                prefix: /              route:                # 本ルートに紐づくクラスター値の名前                cluster: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local                timeout: 0s                retry_policy:                  retry_on: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes                  num_retries: 2                  retry_host_predicate:                    - name: envoy.retry_host_predicates.previous_hosts                  host_selection_retry_max_attempts: \"5\"                  retriable_status_codes:                    - 503                max_stream_duration:                  max_stream_duration: 0s                  grpc_timeout_header_max: 0s              decorator:                operation: bar-service.bar-namespace.svc.cluster.local:50002/*  ...  - '@type': type.googleapis.com/envoy.admin.v3.RoutesConfigDump.DynamicRouteConfig  ...↪️：Administration interface — envoy 1.27.0-dev-71a50b documentationConfigDump (proto) — envoy 1.27.0-dev-71a50b documentationクラスター値▼ 確認方法istio-proxyコンテナがADS-APIから取得したクラスター値は、/config_dump?resource={dynamic_active_clusters}から確認できます。ここでは、foo-pod内でbar-podのクラスター値を確認したと仮定します。$ kubectl exec \\    -it foo-pod \\    -n foo-namespace \\    -c istio-proxy \\    -- bash -c \"curl http://localhost:15000/config_dump?resource={dynamic_active_clusters}\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。ルート値を取得した時に確認できたクラスター値の名前クラスター値に紐づくエンドポイント値の親名configs:  - \"@type\": type.googleapis.com/envoy.admin.v3.ClustersConfigDump.DynamicCluster    version_info: 2022-11-24T12:13:05Z/468    cluster:      \"@type\": type.googleapis.com/envoy.config.cluster.v3.Cluster      # クラスター値の名前      name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local      type: EDS      eds_cluster_config:        eds_config:          ads: {}          initial_fetch_timeout: 0s          resource_api_version: V3        # 本クラスターに紐づくエンドポイント値の親名        service_name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local  ...  - \"@type\": type.googleapis.com/envoy.admin.v3.ClustersConfigDump.DynamicCluster  ...↪️：Administration interface — envoy 1.27.0-dev-71a50b documentationConfigDump (proto) — envoy 1.27.0-dev-71a50b documentationエンドポイント値▼ 確認方法istio-proxyコンテナがADS-APIから取得したクラスター値は、/config_dump?include_edsから確認できます。ここでは、foo-pod内でbar-podのクラスター値を確認したと仮定します。$ kubectl exec \\    -it foo-pod \\    -n foo-namespace \\    -c istio-proxy \\    -- bash -c \"curl http://localhost:15000/config_dump?include_eds\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。クラスター値を取得した時に確認できたエンドポイントの親名bar-podのインスタンスが3個あるため、3個のエンドポイントがありますconfigs:  dynamic_endpoint_configs:    - endpoint_config:        \"@type\": type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment        # エンドポイントの親名        cluster_name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local        endpoints:          - locality:              region: ap-northeast-1              zone: ap-northeast-1a            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.1                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 80                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1          - locality:              region: ap-northeast-1              zone: ap-northeast-1d            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.2                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 80                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1          - locality:              region: ap-northeast-1              zone: ap-northeast-1d            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.3                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 80                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1        policy:          overprovisioning_factor: 140    ...    - endpoint_config:    ...↪️参考：Administration interface — envoy 1.27.0-dev-71a50b documentationConfigDump (proto) — envoy 1.27.0-dev-71a50b documentationSupported load balancers — envoy 1.27.0-dev-71a50b documentationload_balancing_weightキー値が等しい場合、EnvoyはP2Cアルゴリズムに基づいてロードバランシングします👍Envoyの処理の流れのまとめ確認できた宛先情報を、Envoyの処理の流れに当てはめてみました。【１】送信元マイクロサービスは、宛先マイクロサービス (<任意のIP>/:50002) にリクエストを送信し、サイドカーコンテナのistio-proxyコンテナはこれを受信します。【２】Envoyは、リクエストの宛先 (IPアドレス、ポート番号、パス) からPodのリスナー値 (0.0.0.0_50002) を選びます。【３】Envoyは、リスナーに紐づくPodのルート値 (50002) を選びます。【４】Envoyは、クラスターに紐づくPodのクラスター値 (outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local) を選びます。【５】Envoyは、クラスターに紐づくPodのインスタンスのエンドポイント値 (11.0.0.X/:80) を選びます。【６】Envoyは、エンドポイント値の宛先にPodのリクエストを送信します。サービスディスカバリーの冒険は以上です⛵05. おわりにIstioの機能の一つである『サービスディスカバリー』の仕組みを、Envoyを交えながらもりもり布教しました。愛が溢れてしまいました。Istioの機能を一つとっても、複雑な仕組みで実現していることがお分かりいただけたかと思います (Istioありがとう🙏)謝辞3-shake SRE Tech Talk での発表前後に、以下の方々に、発表内容について助言をいただきました。@ido_kara_deru さん@yosshi_ さん@yteraoka さん(アルファベット順)また、今回の 3-shake Advent Calender 2022 は、以下の方々に企画いただきました。@jigyakkuma_ さん@nwiizo さん(アルファベット順)皆様に感謝申し上げます🙇🏻‍","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2022/12/25/060000","isoDate":"2022-12-24T21:00:00.000Z","dateMiliSeconds":1671915600000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"}]},"__N_SSG":true}