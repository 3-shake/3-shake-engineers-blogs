<!DOCTYPE html><html lang="ja"><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><link rel="icon shortcut" type="image/png" href="https://blog.3-shake.com/logo.png" data-next-head=""/><title data-next-head="">nwiizo | 3-shake Engineers&#x27; Blogs</title><meta property="og:title" content="nwiizo" data-next-head=""/><meta property="og:url" content="https://blog.3-shake.com/members/nwiizo" data-next-head=""/><meta name="twitter:card" content="summary_large_image" data-next-head=""/><meta property="og:site" content="3-shake Engineers&#x27; Blogs" data-next-head=""/><meta property="og:image" content="https://blog.3-shake.com/og.png" data-next-head=""/><link rel="canonical" href="https://blog.3-shake.com/members/nwiizo" data-next-head=""/><link rel="preload" href="/_next/static/css/683b82a315c74ead.css" as="style"/><link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;700&amp;family=Roboto:wght@300;400;500;700&amp;display=swap" rel="stylesheet"/><link rel="stylesheet" href="/_next/static/css/683b82a315c74ead.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-6ffd07a3317375c1.js" defer=""></script><script src="/_next/static/chunks/framework-d7de93249215fb06.js" defer=""></script><script src="/_next/static/chunks/main-03ed03852b4b217f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-eb27c9050fc0d186.js" defer=""></script><script src="/_next/static/chunks/736-d8cffe19258348d0.js" defer=""></script><script src="/_next/static/chunks/pages/members/%5Bid%5D-52a9b06cdc5c9345.js" defer=""></script><script src="/_next/static/UdZ-gNg4n6NQDyq0LRXx0/_buildManifest.js" defer=""></script><script src="/_next/static/UdZ-gNg4n6NQDyq0LRXx0/_ssgManifest.js" defer=""></script></head><body><link rel="preload" as="image" href="/logo.svg"/><link rel="preload" as="image" href="/avatars/nwiizo.jpeg"/><link rel="preload" as="image" href="/icons/twitter.svg"/><link rel="preload" as="image" href="/icons/github.svg"/><link rel="preload" as="image" href="/icons/link.svg"/><link rel="preload" as="image" href="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com"/><link rel="preload" as="image" href="https://www.google.com/s2/favicons?domain=speakerdeck.com"/><div id="__next"><header class="site-header"><div class="content-wrapper"><div class="site-header__inner"><a class="site-header__logo-link" href="/"><img src="/logo.svg" alt="3-shake Engineers&#x27; Blogs" class="site-header__logo-img"/><span class="site-header__logo-text">3-shake<br/>Engineers&#x27; Blogs</span></a><div class="site-header__links"><a class="site-header__link" href="/feed.xml">RSS</a><a href="https://jobs-3-shake.com/" class="site-header__link">Recruit</a><a href="https://3-shake.com/" class="site-header__link">Company</a></div></div></div></header><section class="member"><div class="content-wrapper"><header class="member-header"><div class="member-header__avatar"><img src="/avatars/nwiizo.jpeg" alt="nwiizo" width="100" height="100" class="member-header__avatar-img"/></div><h1 class="member-header__name">nwiizo</h1><p class="member-header__bio">The Passionate Programmer</p><div class="member-header__links"><a href="https://twitter.com/nwiizo" class="member-header__link"><img src="/icons/twitter.svg" alt="Twitterのユーザー@nwiizo" width="22" height="22"/></a><a href="https://github.com/nwiizo" class="member-header__link"><img src="/icons/github.svg" alt="GitHubのユーザー@nwiizo" width="22" height="22"/></a><a href="https://nwiizo.github.io/" class="member-header__link"><img src="/icons/link.svg" alt="ウェブサイトのリンク" width="22" height="22"/></a></div></header><div class="member-posts-container"><div class="post-list"><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-23T02:16:51.000Z" class="post-link__date">9 hours ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/23/111651" class="post-link__main-link"><h2 class="post-link__title">書評や要約は「圧縮」ではなく「変換」であり、「変換」に価値がある。</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a><div class="post-link__new-label">NEW</div></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-22T04:55:17.000Z" class="post-link__date">a day ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/22/135517" class="post-link__main-link"><h2 class="post-link__title">なぜ「何でも作れる時代」に私は作れないのか</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a><div class="post-link__new-label">NEW</div></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-21T06:25:59.000Z" class="post-link__date">2 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/21/152559" class="post-link__main-link"><h2 class="post-link__title">cargo-coupling: Visualizing Coupling in Rust Projects</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a><div class="post-link__new-label">NEW</div></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-21T00:24:56.000Z" class="post-link__date">2 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/21/092456" class="post-link__main-link"><h2 class="post-link__title"> おい、休め</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a><div class="post-link__new-label">NEW</div></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-20T10:53:29.000Z" class="post-link__date">3 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/20/195329" class="post-link__main-link"><h2 class="post-link__title">cargo-coupling: Rustプロジェクトの結合度を可視化する</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-19T09:31:48.000Z" class="post-link__date">4 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/19/183148" class="post-link__main-link"><h2 class="post-link__title">生成AI時代のMarp によるスライド環境の構築</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-19T08:33:09.000Z" class="post-link__date">4 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/19/173309" class="post-link__main-link"><h2 class="post-link__title">Claude Codeの Agent Skills は設定したほうがいい</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-18T02:15:00.000Z" class="post-link__date">5 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/18/111500" class="post-link__main-link"><h2 class="post-link__title">「自分の環境では動く」から解放される Nix Flake </h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-17T03:17:05.000Z" class="post-link__date">6 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/17/121705" class="post-link__main-link"><h2 class="post-link__title">2025年版 私がAIエージェントと協働しながら学習する方法</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-16T01:22:27.000Z" class="post-link__date">7 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/16/102227" class="post-link__main-link"><h2 class="post-link__title">AI時代の異常系テストについて考える</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-15T04:00:00.000Z" class="post-link__date">8 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/15/130000" class="post-link__main-link"><h2 class="post-link__title">おい、戦略を語れ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-14T04:25:52.000Z" class="post-link__date">9 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/14/132552" class="post-link__main-link"><h2 class="post-link__title">2025年版 PDE（Personal Development Environment）のすすめ：自分だけの刀を打つ開発環境構築</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-13T05:51:59.000Z" class="post-link__date">10 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/13/145159" class="post-link__main-link"><h2 class="post-link__title">『おい、テックブログを書け』というタイトルで登壇しました</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-12T07:32:20.000Z" class="post-link__date">11 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/12/163220" class="post-link__main-link"><h2 class="post-link__title">専門家は話さないですよ(『専門家が「力」をセーブせずに全力で専門性を振り回してもリスペクトされる組織をつくりたい』を読んで)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-11T01:41:43.000Z" class="post-link__date">12 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/11/104143" class="post-link__main-link"><h2 class="post-link__title">2025年 俺が愛した本たち 技術書編</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-10T00:27:06.000Z" class="post-link__date">13 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/10/092706" class="post-link__main-link"><h2 class="post-link__title">2025年版 私がAIエージェントと協働しながら集中する方法</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-09T00:22:56.000Z" class="post-link__date">14 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/09/092256" class="post-link__main-link"><h2 class="post-link__title">実力とは“最悪の自分”が決める</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-08T06:26:14.000Z" class="post-link__date">15 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/08/152614" class="post-link__main-link"><h2 class="post-link__title">技術広報はちゃんとなめてやれ（技術広報をなめるなを読んで）　</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-05T21:02:08.000Z" class="post-link__date">18 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/06/060208" class="post-link__main-link"><h2 class="post-link__title">おい、類推するな</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-05T20:56:37.000Z" class="post-link__date">18 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/06/055637" class="post-link__main-link"><h2 class="post-link__title">RustでOWASP API Security Top 10を体験する（後編）：リソース制御と攻撃検知</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-05T01:49:19.000Z" class="post-link__date">18 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/05/104919" class="post-link__main-link"><h2 class="post-link__title">RustでOWASP API Security Top 10を体験する（前編）：認証・認可の基礎とデータ保護</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-04T05:00:00.000Z" class="post-link__date">19 days ago</time></div></a><a href="https://speakerdeck.com/nwiizo/oi-tetukuburoguwoshu-ke" class="post-link__main-link"><h2 class="post-link__title">おい、テックブログを書け</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=speakerdeck.com" width="14" height="14" class="post-link__site-favicon"/>speakerdeck.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-02T15:20:23.000Z" class="post-link__date">21 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/03/002023" class="post-link__main-link"><h2 class="post-link__title">おい、努力しろ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-02T15:11:46.000Z" class="post-link__date">21 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/03/001146" class="post-link__main-link"><h2 class="post-link__title">生成AIエージェントによるブログレビュー環境の構築（下）</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-02T03:47:02.000Z" class="post-link__date">21 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/02/124702" class="post-link__main-link"><h2 class="post-link__title">おい、がんばるな</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-12-01T15:26:01.000Z" class="post-link__date">22 days ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/12/02/002601" class="post-link__main-link"><h2 class="post-link__title">生成AIエージェントによるブログレビュー環境の構築（上）</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-11-25T04:52:20.000Z" class="post-link__date">a month ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220" class="post-link__main-link"><h2 class="post-link__title">「Postgres で試した？」と聞き返せるようになるまでもしくはなぜ私は雰囲気で技術を語るのか？ — Just use Postgres 読書感想文</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-11-23T19:33:14.000Z" class="post-link__date">a month ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314" class="post-link__main-link"><h2 class="post-link__title">おい、本を読め</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-11-22T03:30:28.000Z" class="post-link__date">a month ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/11/22/123028" class="post-link__main-link"><h2 class="post-link__title">Fish Shell の abbr で使う。キミが好きだよ、エイリアス</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-11-21T01:05:03.000Z" class="post-link__date">a month ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/11/21/100503" class="post-link__main-link"><h2 class="post-link__title">たぶん、読んでない</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-11-19T10:48:09.000Z" class="post-link__date">a month ago</time></div></a><a href="https://syu-m-5151.hatenablog.com/entry/2025/11/19/194809" class="post-link__main-link"><h2 class="post-link__title">おい、対話しろ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=syu-m-5151.hatenablog.com" width="14" height="14" class="post-link__site-favicon"/>syu-m-5151.hatenablog.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/nwiizo"><img src="/avatars/nwiizo.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">nwiizo</div><time dateTime="2025-09-30T04:00:00.000Z" class="post-link__date">3 months ago</time></div></a><a href="https://speakerdeck.com/nwiizo/baibukodeingutoji-sok-de-depuroimento" class="post-link__main-link"><h2 class="post-link__title">バイブコーディングと継続的デプロイメント</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=speakerdeck.com" width="14" height="14" class="post-link__site-favicon"/>speakerdeck.com</div></a></article></div><div class="post-list-load"><button class="post-list-load__button">LOAD MORE</button></div></div></div></section><footer class="site-footer"><div class="content-wrapper"><p>© <!-- -->3-shake Inc.</p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"member":{"id":"nwiizo","name":"nwiizo","role":"Software Developer","bio":"The Passionate Programmer","avatarSrc":"/avatars/nwiizo.jpeg","sources":["https://syu-m-5151.hatenablog.com/feed","https://zenn.dev/nwiizo/feed","https://speakerdeck.com/nwiizo.rss"],"includeUrlRegex":"","twitterUsername":"nwiizo","githubUsername":"nwiizo","websiteUrl":"https://nwiizo.github.io/"},"postItems":[{"title":"書評や要約は「圧縮」ではなく「変換」であり、「変換」に価値がある。","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/23/111651","contentSnippet":"タイトルがそのままゴールなのですがダラダラ書きます。はじめにある技術書の要約を読んで、「なるほど、この本の主張はこういうことか」と納得した。数ヶ月後、原著を手に取って驚いた。要約で「核心」とされていた部分は、実は本全体の一部に過ぎなかった。著者が本当に伝えたかったことは、要約では一行も触れられていなかったのだ。技術書、非技術書に限らず、書評や要約を読んでいると、ある違和感に気づく。これは「圧縮」ではなく、本質的に異なるものではないか。原著者の思考プロセスは消失し、要約者のフィルタリングと優先度により情報が再構成される。同じ本から異なる要約が生まれる。そのコンテキストを知らずに読むと、原典でなく要約者の思想を取り込んでしまう。これはデジタル圧縮に例えるなら、可逆圧縮ではなく非可逆変換だ。ZIPファイルのように元に戻せる圧縮ではなく、JPEGのように一度変換すれば二度と元には戻らない変換。情報は永久に失われ、何を残して何を捨てるかは、変換アルゴリズム——この場合は要約者——の判断に依存する。でもここで誤解してほしくないのは、これは要約や書評を否定する話ではないということだ。むしろ、その本質を正しく理解し、積極的に活用するための話だ。要約や書評は「圧縮」ではなく「変換」であり、その変換には独自の価値がある。原著の劣化コピーではなく、原著から触発された新しい思考の産物として。原著への入り口として、あるいは原著と対話する形で展開される考察として。そしてその価値を最大限に引き出すには、私たちが変換というプロセスを意識し、批判的に読み、創造的に使いこなす必要がある。同時に、本当に大切な本については、時間をかけてちゃんと読む——その価値を見失わないことも重要だ。なぜなら、本をちゃんと読むことは、自分を長い時間をかけて変容させることだから。そしてその変容は、要約では決して起きないから。圧縮という幻想私たちは「要約」という言葉を、まるでファイルの圧縮のように捉えている。元の情報を小さくしただけ、重要な部分だけを取り出しただけ、だから本質は変わらない——そう思い込んでいる。しかし本当は違う。デジタルの世界には二つの圧縮がある。ZIPファイルのような可逆圧縮と、JPEGのような非可逆変換。前者は展開すれば元に戻るが、後者は一度変換すれば二度と元には戻らない。情報は永久に失われ、何を残して何を捨てるかは、変換アルゴリズムの設計思想に依存する。書評や要約は、後者だ。これは単なる比喩ではない。要約という行為は、情報を小さくしているのではなく、情報を別の何かに変えているのだ。そしてその過程で、何かが——多くの場合、最も大切な何かが——失われる。要約という名の変換要約のプロセスを観察してみるといい。そこには、本人も気づいていない、いくつもの「変換」が起きている。選択的フィルタリングという暴力要約者は、自分が「重要だ」と判断した部分を抽出する。しかしこの「重要性」は、極めて主観的だ。要約者の経験、専門性、価値観、そして何より要約者が今抱えている問題意識——これらすべてが、無意識のフィルターとして機能する。マーケティング担当者が読む技術書と、エンジニアが読む同じ技術書では、心に残る章が違う。当然だ。でもこれは、どちらかが間違っているという話ではない。同じ本から、異なる意味が立ち上がっているだけだ。そしてその意味の違いは、読者ではなく、読み方によって生まれる。要約という行為は、この「読み方」を一つに固定する。要約者の読み方が、唯一の読み方として提示される。そして要約を読む私たちは、その固定された読み方を、本の内容そのものだと錯覚する。文脈の切断という喪失著者は意図的に章を配置する。第一章から第十章へと、徐々に論理を積み上げていく。前の章の具体例が、後の章の理論の基礎になる。ある議論は、数章前の別の議論を前提としている。この「流れ」は、本の核心的な要素の一つだ。理解とは情報の構造化だからだ。著者が用意した順序で読むことで、私たちの頭の中に、新しい思考の枠組みが少しずつ形成されていく。でも要約は、この流れを切断する。第三章の結論、第七章の重要なポイント、第十章のまとめ——それらは箇条書きになって並べられ、互いの繋がりは失われる。結論だけが残り、そこに至るまでの思考の階段は消える。そして多くの場合、その階段こそが、最も価値のある学びだったのだ。言語の置き換えという変質著者が選んだ言葉には、意味がある。その比喩、その言い回し、その微妙なニュアンス——それらはすべて、著者が伝えたい何かを形にするために、慎重に選ばれている。しかし要約者は、それを自分の言葉に置き換える。分かりやすく、簡潔に、読みやすく。善意からの行為だ。しかしこの過程で、著者の「声」は失われる。翻訳と同じだ。どれだけ優れた翻訳でも、原文の響きは失われる。要約も同じ。どれだけ丁寧な要約でも、著者が選んだ言葉の持つ微妙な意味の重なりは、消えてしまう。そして私たちは、その消失に気づかない。思考プロセスの消失という致命的な欠落最も大きな喪失は、これだ。著者が試行錯誤の末に辿り着いた結論。一度は正しいと思ったが、後に間違いだと気づいた考え。検討したが採用しなかった別のアプローチ。考えを変えた転機となった出来事。こうした思考の軌跡は、要約では「結論」だけが残る。「著者はこう主張している」と。でも、なぜその主張に至ったのか、どんな葛藤があったのか、何を捨てて何を選んだのか——そのすべてが消える。けれど、学びとは結論を知ることではなく、その結論に至るプロセスを追体験することだ。著者の思考の軌跡を辿ることで、私たちの思考の枠組みが変わる。結論だけを知っても、それは情報の追加にすぎず、思考の変容は起きない。要約は、この核心を、最初に捨てる。同じ本、異なる要約の謎興味深い実験をしてみるといい。同じ一冊の本について、複数の要約を読んでみるといい。驚くほど違う内容が書かれている。ある経営書を読んだ場合を想像してみよう。スタートアップの創業者は、リスクテイクと革新の章を強調するかもしれない。彼らにとって重要なのは「いかに新しいことを試すか」だから。大企業の管理職は、組織マネジメントと持続可能性の部分に注目するだろう。彼らが直面しているのは、既存の組織をいかに動かすかという問題だから。学者は、理論的フレームワークと研究手法に焦点を当てる。彼らが関心を持つのは、この本がどんな学術的文脈に位置するかだから。それぞれの要約は「正しい」。でもそれは、元の本ではない。要約者のレンズを通して屈折した像だ。そして恐ろしいことに、私たちはその屈折を、見ることができない。なぜか。要約には、要約者の視点が明示されないからだ。「私はスタートアップ創業者として、この本からリスクテイクの部分に注目した」とは書かれない。ただ「この本の重要なポイントは」と書かれる。まるで客観的な事実であるかのように。しかし客観的な要約など、存在しない。すべての要約は、誰かの主観を通した変換だ。その主観は、透明なレンズのように見えて、実は色付きのフィルターなのだ。「読んだ」と「読んでいない」という曖昧な境界私たちは「本を読んだ」という言葉を単純に使いすぎている。ピエール・バイヤールの『読んでいない本について堂々と語る方法』では、読書という行為を興味深く分類している。バイヤールは読書を次のように区分けする。UB（unread book） ——まったく読んでいない本SB（skimmed book） ——ざっと目を通した本HB（heard about book） ——人から聞いた本、あるいは書評で知った本FB（forgotten book） ——読んだが内容を忘れた本この分類を見て、私たちは気づく。「読んだ」と「読んでいない」という二項対立は、実は単純なものではないのだと。最後のページまで目を通したが内容をほとんど覚えていない本と、要約だけ読んだが著者の主張をよく理解している本——どちらが「読んだ」と言えるのか。人から聞いた話を通じてその本の核心的なアイデアに触れた場合と、本は買ったが積読のまま放置している場合——どちらが「読んでいない」と言えるのか。答えは簡単ではない。というより、この問い自体が間違っているのかもしれない。問われるのは「読んだか読んでいないか」という形式的な区分けではなく、その本とどのような関係を結んでいるか、その本を通じてどのような思考を展開できるか——そういった実質的な次元なのだ。読んでいない本について語る「状況」バイヤールは、読んでいない本について語ることの不可避性を指摘する。私たちは日常的に、読んでいない本について語らざるを得ない状況に置かれている。会議で誰かが本を引用する。読んだことはないが、その場で意見を求められる。友人が「あの本、どう思う？」と訊いてくる。正直に「読んでない」と言えば会話は終わるが、書評で得た知識をもとに語れば、豊かな対話が生まれる。就職面接で「最近読んだ本は？」と訊かれる。最後まで読了した本だけを答えの対象にすれば、選択肢は著しく狭まる。これらの状況を具体的に検討してみると、面白いことが見えてくる。本について語ることは、必ずしも本を最後まで読了していることを前提としていない。むしろ、本の周辺にある言説——書評、要約、他者の解釈、断片的な引用——これらを媒介にして語ることが、実は創造的な思考を生み出すことがある。友人が薦めた本について、その友人の語り口から想像を膨らませて議論する。その過程で、原著にはない新しい視点が生まれることがある。書評を読んで著者の意図を「誤読」し、その誤読から独自の考察を展開する。その考察が、時として原著を超える洞察に至ることもある。つまり、本について語ることは、必ずしも本を「正確に」理解することを目的としていない。本を触媒として、自分の思考を展開すること——それが本質なのだ。要約という考察の可能性ここで視点を変えてみよう。要約や書評を、単なる「劣化コピー」ではなく、一種の「考察」として捉え直すとどうなるか。要約者は、本を読んで何かを感じ取る。その「何か」を言語化しようとする。この過程は、実は高度に創造的な行為だ。無数の情報の中から何を選び、どう配置し、どう言葉にするか——この選択と構成の過程で、要約者独自の思考が立ち上がる。だから、優れた書評や要約は、原著とは別の価値を持つ。それは原著の「圧縮版」ではなく、原著から触発された、要約者の「考察」なのだ。私自身、年間でかなりの書評を作っている。しかし公開しているのはごくごく一部だ。なぜか。私はあまり有能な方ではないから、書籍を漫然と読んでも深い学びを得ることができない。だから書評を書く。書評を書くという行為を通じて、自分が何を理解し、何を理解していないのかを明確にする。どこに引っかかったのか、どこが腑に落ちたのか——それを言語化することで、初めて本当の理解が生まれる。公開しているものは、作った書評の中で「公開して良いかな」と思った文章を加筆修正したものだ。つまり、書評を書く行為そのものは、公開のためではなく、自分の理解を深めるためのものなのだ。たとえば、ある技術書について複数のエンジニアが書評を書いたとする。一人は実装の観点から、一人は設計思想の観点から、一人は歴史的文脈の観点から語る。これらの書評を読むことで、私たちは原著が持つ多面性に触れることができる。そしてそれぞれの書評が、原著を読むための異なる「補助線」になる。この意味で、要約や書評は原著を補完し、豊かにする。原著だけを読むよりも、原著と複数の書評を読む方が、理解が深まることがある。なぜなら、一冊の本が持つ可能性を、複数の視点から照らし出すことができるからだ。ファスト教養という時代の文脈ただし、ここで注意すべき点がある。要約や書評が「考察」として価値を持つのは、それが原著への入り口として機能するか、あるいは原著と対話する形で展開される場合だ。現代には「ファスト教養」とでも呼ぶべき現象がある。短時間で「教養」を身につけたように見せるための、効率化された知識の消費。要約を読んで「読んだ」と言い、書評を見て「理解した」と思い込む。そこには、本との本当の対話はない。ファスト教養の問題は、効率性そのものではない。問題は、本と自分の間に常に誰か（要約者、解説者、インフルエンサー）が介在し、自分で考える機会が失われることだ。バイヤールが指摘するように、読んでいない本について語ることは、自分で思考し言語化する状況では創造的な行為になり得る。しかしそれは、自分の頭で考え、自分の言葉で語る場合に限る。誰かの要約をコピー\u0026ペーストして語るのは、創造ではなく模倣だ。変換を活用する五つの方法では、要約や書評という「変換」を、どう活用すべきなのか。1. 要約者の視点を意識する要約を読むとき、「これは誰の視点か」を常に問う。その人の専門性、立場、問題意識——それらを意識することで、フィルターの存在が見えてくる。そして「では自分なら、どこに注目するか」と考える。2. 複数の解釈を並置する一つの要約だけを読むのではなく、複数の異なる視点からの解釈を集める。それらを比較することで、本が持つ多面性が見えてくる。そして何より、「絶対的な正解」など存在しないことが分かる。3. 要約を「問い」として読む要約を「答え」として受け取るのではなく、「問い」として読む。「この要約者はなぜこの部分を重要だと判断したのか」「省略された部分には何があったのか」——そう問うことで、要約は思考の出発点になる。4. 自分なりの考察を加える要約を読んで、自分なりの考察を加えてみる。「自分の経験ではどうか」「別の文脈ではどうなるか」「反対の立場から見たらどうか」——そうやって思考を展開することで、要約は単なる情報から、思考の触媒へと変わる。5. 原典への道標として使うそして何より、要約を原典への道標として使うことだ。要約で興味を持ったら原典を読む。要約で疑問を持ったら原典で確認する。要約と原典の間を行き来することで、理解は深まる。読んでいない本について語る際の倫理バイヤールは、読んでいない本について語る際に注意すべき点を挙げている。第一に、読んでいないことを隠す必要はない。「詳しくは読んでいないが」「書評で読んだ限りでは」——そう前置きすることで、誠実さを保ちながら対話を続けられる。第二に、自分の解釈を絶対化しない。「私はこう理解した」「私にはこう見える」——主語を「私」にすることで、それが一つの視点に過ぎないことを示す。第三に、他者の解釈を尊重する。要約や書評は、誰かの真剣な思考の結果だ。それを軽んじることなく、一つの有効な視点として受け止める。そして第四に、思考を停止させないことだ。要約を読んで「分かった」で終わらせず、そこから自分なりの思考を展開する。これらの点を意識すれば、読んでいない本について語ることは、単なる知ったかぶりではなく、創造的な知的活動になり得る。要約や書評を通じて、新しい視点を獲得し、自分の思考を深め、時には原著を超える洞察に至ることさえ可能なのだ。書評・要約と著作権——変換者の責任変換の価値を語るとき、避けて通れない現実がある。それは法律だ。私たちが要約や書評を「考察」として価値あるものにできるのは、それが著作者の権利を侵害しない範囲で行われる場合に限る。自分の良し悪しだけで判断できる問題ではない。著作権法では、著作物を「翻案」する権利は著作権者に帰属する。要約はこの「翻案」に該当する可能性がある。一方で、「引用」は一定の条件を満たせば許諾なく行える。興味深いのは判例だ。「血液型と性格」事件（東京地判平成10年）では、やむを得ない範囲での要約引用は著作権侵害にならないと判断された。全文をそのまま引用するより、要約する方が著作権者の利益を損なわない場合があるという理由だ。ここに、変換という行為の本質が見える。情報そのものには著作権はないが、表現には著作権がある。著者の文章表現をそのまま使うのは問題になり得る。しかし、その情報を自分の言葉で表現し直すのであれば——つまり、本当の意味で「変換」するのであれば——著作権侵害には該当しにくい。これは単なる法的な制約ではない。むしろ、変換者としての私たちに課された創造的な責任だ。他人の言葉をコピーするのではなく、自分の言葉で語り直す。その過程で、私たちは否応なく考えることを強いられる。要約や書評は、著作者と読者をつなぐ架け橋になり得る。しかしそれは、著作者の権利を尊重し、自分の言葉で語るという責任を引き受けた上でのことだ。変換の二面性要約や書評という「変換」は、原著者の思想と要約者の解釈が混ざり合ったハイブリッドだ。そしてその混ざり具合は、多くの場合見えない。ここには確かに危険性がある。要約を原著そのものだと錯覚し、要約者の解釈を著者の思想だと思い込む。そして気づかないうちに、自分で考える機会を失う。著者の思想と対峙し、自分の経験と照らし合わせ、時には反論し、格闘する——その過程が省略される。でも同時に、ここには可能性もある。要約や書評は、原著にはない新しい視点を提供してくれることがある。著者自身も気づいていなかった含意を、要約者が読み取ることがある。異なる文脈に置き直すことで、原著が持つ新しい意味が立ち上がることがある。つまり、変換は単なる劣化ではなく、一種の創造なのだ。原著というテキストに、要約者という読者が介入することで、新しい意味が生成される。そしてその新しい意味は、原著を豊かにすることもあれば、原著を歪めることもある。問題は変換そのものではなく、私たちがその変換を意識しているかどうかだ。変換を透明なものとして扱えば、それは欺瞞になる。でも変換を変換として認識し、その特性を理解した上で活用すれば、それは強力な思考のツールになる。現代という時代の加速装置そして現代という時代が、この問題を加速させている。インスタント化という麻薬スマホを開けば、10分で読める要約が溢れている。YouTubeには、本の内容を解説する動画が無数にある。ChatGPTに聞けば、数秒で本の要約を生成してくれる。便利だ。効率的だ。時間を節約できる。しかし私たちは、その便利さの代償を理解しているだろうか。私たちの脳は、インスタントな刺激に適応してしまっている。10分で読める要約、数秒で生成されるAIの解説、流し読みで済む箇条書き——これらに慣れた脳は、長い文章を追うこと、モヤモヤを抱えること、結論が出ないまま考え続けることに、耐えられなくなっている。スマホを見すぎて長い文章が頭に入らないエンジニアは多い。メンターしている若者も「技術書を読むのがしんどい」と言っていた。しかし一週間デジタルデトックスをしたところ、普通に読めるようになった。これは脳が「即時反応モード」から「深く考えるモード」に戻ったからだ。本を読むことは、時間がかかる。最初は分からない。モヤモヤする。何度も読み返す。考える。また読む。この不快で面倒なプロセスを経て、ようやく理解が生まれる。しかしインスタントな要約は、このプロセスをスキップさせる。分からないまま待つ必要がなく、モヤモヤを抱える必要もなく、すぐに「分かった」という感覚が得られる。この即座の満足は、甘い。甘すぎる。そして一度この甘さを知ってしまうと、本を読むという苦行には戻れなくなる。AI要約という危機AIの発展により、要約はさらに加速する。数秒で本を要約し、重要なポイントを箇条書きにし、分かりやすく説明してくれる。思考とは、情報を「受け取る」ことではなく、情報と「格闘する」ことだ。著者の主張に疑問を持ち、自分の経験と照らし合わせ、別の解釈の可能性を探る——この格闘が、思考を育てる。しかしAI要約やファスト教養は、この格闘を省略する。すぐに「分かった」という感覚を提供し、考える時間を奪う。私たちは、知識は増えているが、思考は深まらない——そんな状態に陥る。孤独の喪失という静かな危機もう一つ、見落とされがちな喪失がある。それは、本と一対一で向き合う時間だ。スマホがなかった時代、本を読むとは孤独な行為だった。自分と本だけ。他の誰も介在しない。理解できなくても、退屈でも、そこに居続けるしかなかった。しかし今は違う。少し難しい箇所に来れば、すぐにスマホに手が伸びる。「この部分、要約ないかな」と検索する。あるいは「ちょっと休憩」と言って、SNSを開く。私たちは、孤独に耐えられなくなっている。モヤモヤを抱えたまま、一人で考え続けることができなくなっている。でも本を読むという行為の本質は、この孤独にある。自分の頭で考え、自分の言葉で理解しようとする。誰も助けてくれない、その孤立した状態で、著者の思想と格闘する。この孤独な格闘を経てこそ、本当の意味での理解が生まれる。でも要約は、この孤独を奪う。常に誰かが横にいて、「正解はこれだよ」と教えてくれる。その優しさが、私たちから考える力を奪っていく。本を読むということの本質では、本を読むとは、本当は何をすることなのか。それは、自分を変えることだ。長い時間をかけて、ゆっくりと、確実に。理解とは変容である本を読んで「理解した」というとき、私たちは何を指しているのか。情報を獲得したこと？　結論を知ったこと？違う。理解とは、自分の思考の枠組みが変わることだ。本を読む前と読んだ後で、同じ現象を見ても、違うものが見えるようになる。同じ問題に直面しても、違う解決策が浮かぶようになる。同じ言葉を聞いても、違う意味が響くようになる。これが理解だ。情報の追加ではなく、認識の変容。そしてこの変容は、時間をかけて、ゆっくりと起きる。著者の思考を辿る。分からない箇所で立ち止まる。自分の経験と照らし合わせる。疑問を持つ。また読む。少しずつ、著者の視点が自分の中に入ってくる。そして気づけば、自分の見ている世界が、少し変わっている。この変容は、要約では起きない。なぜなら要約には、この「時間」が含まれていないからだ。結論だけを知っても、それは自分の外側にある情報のままだ。内側に入ってこない。格闘としての読書本を読むとは、著者と格闘することだ。著者の主張を理解しようとする。でも納得できない部分がある。「本当にそうだろうか」と疑問を持つ。自分の経験では違うと感じる。でも著者はこう言っている。なぜだろう。何が違うのか。この格闘の過程で、私たちは考える。自分の前提を疑い、著者の前提を探り、両者の違いを見つけようとする。そして時には、自分が間違っていたことに気づく。あるいは、著者の限界を見抜く。どちらにせよ、この格闘を経て、私たちの思考は深まる。でも要約は、この格闘を省略する。著者の主張は、すでに要約者によって消化されている。疑問を持つ余地もなく、「重要なポイントはこれです」と提示される。私たちは、受け取るだけだ。格闘がなければ、成長もない。反復という学び本は、一度読んで終わりではない。本当に価値のある本は、何度も読み返す価値がある。なぜか。同じ本でも、読むたびに違うものが見えるからだ。一年前に読んだとき、心に響いた章がある。でも今読み返すと、別の章が響く。当時は流し読みした箇所が、今は重要に思える。著者の何気ない一言が、今の自分の状況と重なって、深い意味を持って迫ってくる。これは、私たちが変わったからだ。経験を積み、視点が変わり、問題意識が変わった。同じ本を読んでも、違う自分が読んでいる。だから、違うものが見える。この反復的な読書によって、本は私たちの中で育っていく。最初は30%の理解だったものが、二度目で50%になり、三度目で70%になる。そして何度目かの読書で、「ああ、著者はこのことを言いたかったのか」と、ようやく本当の理解に到達する。でも要約は、この反復を許さない。一度読めば終わりだ。すべてが書かれている。何度読んでも、同じことしか書いていない。要約は、本の成長を止める。そして私たちの成長も、止める。余白という豊かさ本には、余白がある。著者が明示的に書いていないこと、行間に隠れた意味、読者に委ねられた解釈の余地——これらの余白が、本を豊かにする。余白があるから、私たちは考える。「著者はここで何を言おうとしているのか」「この比喩は何を意味するのか」「なぜこの順序で書いたのか」。そして余白があるから、読者ごとに違う解釈が生まれる。同じ本を読んでも、ある人はビジネスのヒントを得て、ある人は人生の指針を見出し、ある人は哲学的な洞察を得る。この多様性こそが、本の価値だ。一つの正解があるのではなく、無数の読み方が可能である——その豊かさが、本を読む喜びを生む。でも要約は、この余白を埋める。すべてを明示し、すべてを説明し、一つの解釈に固定する。「この本の意味はこれです」と。余白が消えたとき、本は死ぬ。そして読む喜びも、死ぬ。要約の正しい役割要約は、門だ。家ではない。本を読むかどうか判断するために、要約を読む。これは合理的だ。すべての本を精読する時間は、誰にもない。要約を読んで、「この本は自分に必要そうだ」「この本は今の自分には合わないかもしれない」と判断する。この使い方なら、要約は有用なツールだ。あるいは、すでに読んだ本の要約を読む。記憶を呼び覚ますトリガーとして。「ああ、そうだった」と思い出すために。これも正しい使い方だ。問題は、要約を読んで「本を読んだ」と思うことだ。門をくぐって「家に入った」と思うことだ。要約は入口であって、目的地ではない。複数の要約を読むという戦略一つの本について、複数の要約を読んでみる。すると、面白いことが見えてくる。要約ごとに、強調されている部分が違う。ある要約が重要だと言っている章を、別の要約は触れてもいない。ある要約の解釈と、別の要約の解釈が、矛盾している。この違いこそが、要約の本質を暴く。要約は客観的な事実ではなく、誰かの主観的な解釈だということが、複数の要約を比較することで見えてくる。そして同時に、本の多面性も見えてくる。一つの本が、いかに豊かで、いかに多様な読み方を許容しているか——それを複数の要約から、間接的に感じ取ることができる。ただし、この戦略も、本を読む代わりにはならない。あくまで、本を読む前の準備、あるいは読んだ後の確認として機能する。要約者のバックグラウンドを知るという習慣「誰が要約しているのか」に注目する習慣を持つといい。その人の専門性は何か。どんな立場で、どんな問題意識を持っているか。どんなバイアスを持っている可能性があるか。これを意識するだけで、要約の読み方が変わる。「ああ、この人はマーケティングの専門家だから、この部分を強調しているのか」「この人はエンジニアだから、技術的な側面に注目しているのか」。要約者のフィルターが見えてくる。そのフィルターを通して、何が強調され、何が省略されているのかが、推測できるようになる。そして何より、「では自分が読んだら、どこに注目するだろうか」と考えることだ。要約者と自分の違いを意識することで、自分のフィルターも見えてくる。自分で要約してみるという修行最も効果的な学びは、自分で要約を書いてみることだ。本を読んで、自分なりの要約を書く。すると、いかに難しいかが分かる。何を残して何を捨てるか、その判断の難しさ。著者の言葉を自分の言葉に置き換える際の、意味のズレ。思考のプロセスを、結論だけに圧縮することの暴力性。この体験を経ると、要約の限界が肌で分かる。そして要約を読むときの姿勢が、変わる。「これは要約者の解釈である」「著者の本当の意図は、もっと複雑かもしれない」「失われた部分があるはずだ」——そう意識しながら読むようになる。自分で要約を書くことは、要約に対する批判的読解力を育てる。読書リテラシーという現代の必須能力情報が溢れる時代だからこそ、必要なのは情報の「形式」を理解するメタ認知だ。速読という幻想を捨てる「速く読む」ことを目標にするのは、間違っている。大切なのは、速さではなく、深さだ。一冊の本を一時間で読むことより、一冊の本と一ヶ月向き合うことの方が、はるかに価値がある。もちろん、すべての本をそう読む必要はない。流し読みでいい本もあるし、要約で十分な本もある。でも少なくとも、年に数冊は、時間をかけて、深く読む本があっていい。その数冊が、あなたを変える。要約を百冊読むより、原典を三冊、じっくり読む方が、思考は深まる。速読を目指すのではなく、深読を目指す。これが、現代の読書リテラシーだ。不完全な理解を受け入れる勇気本を読んでも、すべては理解できない。これは当たり前のことだ。著者が何年もかけて考えてきたことを、数時間や数日ですべて理解できるはずがない。分からない部分があって当然だし、誤読することもある。しかし私たちは、この不完全さを受け入れられない。すぐに「分かった」という感覚を求めて、要約に逃げる。肝心なのは、不完全な理解を抱えたまま、読み続けることだ。分からない部分を、分からないまま保留しておく。「いつか分かるかもしれない」と思いながら、先に進む。この「分からなさ」を抱える力が、深い理解への鍵だ。すぐに「分かった」と結論づけず、モヤモヤを抱え続ける。そして時間をかけて、徐々に理解が深まっていく。要約は、この不完全さを許さない。すべてを明快に説明し、すべてを分かりやすくする。でもその分かりやすさは、理解の深さを犠牲にしている。不完全さを受け入れる勇気を持つこと。これが、本を読むということの本質だ。変容には時間がかかる最後に、最も重要なことを言いたい。本を読むことは、自分を変えることだ。そして変容には、時間がかかる。本を読んで即座に変わる、ということは、ほとんど起きない。自己啓発書を読んで「明日から変わろう」と思っても、明日になれば何も変わっていない。でもそれは、本が悪いのではなく、私たちの期待が間違っているのだ。本による変容は、もっとゆっくりと起きる。読んだ内容は、すぐには自分のものにならない。でも心のどこかに引っかかる。数週間後にふと思い出し、数ヶ月後に似た状況で無意識に浮かんでくる。そして気づけば、半年前の自分とは少し違う判断をしている。この反芻の過程で、本の内容は私たちの中に染み込んでいく。最初は外側にあった考え方が、徐々に内側に入ってくる。要約は、この反芻を許さない。分かりやすく整理されすぎていて、心に引っかからないからだ。百冊の本という選択では、どんな本を読むべきなのか。すべての本を深く読む時間は、誰にもない。だからこそ、選択が必要になる。ある人は言った。「私の人生を変えた一冊がある」と。その本を、彼は何度も読み返している。二十代で初めて読み、三十代で読み返し、四十代でまた読む。そして読むたびに、違うものが見える。これが、本との本当の付き合い方だ。一度読んで終わりではなく、人生を通じて対話し続ける。そしてこの長い対話を通じて、本は私たちの一部になる。著者の思想が、自分の思想と混ざり合い、区別がつかなくなる。「これは本で読んだ考えか、自分で考えたことか」分からなくなる。でもそれでいい。それこそが、本を読むことの到達点だ。ただし、一冊の本をそこまで深く読むのは難しい。だから私は思う。本は、百冊あればいい。これは、大量の本の中から自分にとっての正典となる百冊を、自分の力で選ぶということだ。世間で話題の本、ベストセラー、有名人が推薦する本——それらを漫然と読むのではなく、自分にとって本当に大切な百冊を見極める。本棚に深みがあり見栄えの良い本を並べておけば、すぐに読めなくても次第に自分が本に似合う人間になれる。これは不思議な現象だが、本当だ。手元に置いた本は、読まなくても、その存在だけで私たちに影響を与える。「いつか読もう」と思いながら本棚にある本は、私たちに問いかけ続ける。「お前はまだ、私を読む準備ができていないのか」と。読む本を選ぶときには、二つの軸が必要だ。一つは、自分がはまっている関心事を深堀りするように選ぶ。自分の興味、自分の問題意識、自分が今向き合っている課題——それらに関連する本を追いかける。これは内側からの選択だ。もう一つは、定評のある必読リストに沿って選び、外からの影響で自分を変えること。古典と呼ばれる本、専門家が推薦する本、時代を超えて読み継がれている本——自分の興味の外側にある本を、意識的に選ぶ。これは外側からの選択だ。この二つのバランスが、百冊を豊かにする。自分の関心だけで選べば視野が狭くなり、他人の推薦だけで選べば自分を見失う。両方を組み合わせることで、百冊は自分を映す鏡であると同時に、自分を超える窓になる。おわりに書評や要約は、可逆圧縮ではなく非可逆変換だ。それは欠陥ではなく、本質だ。変換を透明なものとして扱えば欺瞞になる。変換として認識し、活用すれば強力なツールになる。要約を入り口として本を探索し、気になったものは原典に当たる。自分にとっての百冊を見極め、その百冊は時間をかけて深く読み、人生を通じて対話し続ける。これは効率性の問題ではない。どう思考するか、どう生きるかという、知的態度の問題だ。溢れる情報の海で溺れないために必要なのは、泳ぐ速度ではない。情報の形式を見抜く目と、それを使いこなす知恵だ。そして何より、自分で考える時間を守ること。誰かの変換を受け取るだけでなく、自分自身が変換者になること。本と格闘し、自分の言葉で語り直し、その過程で少しずつ変わっていくこと。私は今日も、まだ読み終えていない本を開く。昨日とは少し違う自分が、違うページを読んでいる。参考書籍百冊で耕す〈自由に、なる〉ための読書術作者:近藤 康太郎ＣＥメディアハウスAmazon庭の話作者:宇野 常寛講談社Amazon書評の仕事 (ワニブックスPLUS新書)作者:印南 敦史ワニブックスAmazonニッポンの書評 (光文社新書)作者:豊崎 由美光文社Amazonビブリオバトル　本を知り人を知る書評ゲーム作者:谷口忠大文藝春秋Amazon世界は知財でできている (講談社現代新書)作者:稲穂健市講談社Amazon読んでいない本について堂々と語る方法 (ちくま学芸文庫)作者:ピエール・バイヤール,大浦康介筑摩書房Amazon勉強の哲学　来たるべきバカのために　増補版 (文春文庫)作者:千葉 雅也文藝春秋Amazonセンスの哲学作者:千葉 雅也文藝春秋Amazon武器になる哲学 人生を生き抜くための哲学・思想のキーコンセプト50 (角川文庫)作者:山口 周KADOKAWAAmazon自分とか、ないから。　教養としての東洋哲学作者:しんめいPサンクチュアリ出版Amazonファスト教養　10分で答えが欲しい人たち (集英社新書)作者:レジー集英社Amazon映画を早送りで観る人たち～ファスト映画・ネタバレ――コンテンツ消費の現在形～ (光文社新書)作者:稲田 豊史光文社Amazon","isoDate":"2025-12-23T02:16:51.000Z","dateMiliSeconds":1766456211000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"なぜ「何でも作れる時代」に私は作れないのか","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/22/135517","contentSnippet":"はじめに年末、2025年を振り返る。フォロワーは7倍になった。副業も順調。書籍の執筆や翻訳にも関わった。登壇の依頼も増えた。どこからどう見ても、良い年だったはずだ。なのに、胸の奥に澱のようなものが溜まっている。コードは書いた。山ほど書いた。でもそれは、誰かに頼まれたコードだ。お金になるコード。評価されるコード。「これを作ってください」と言われて、「はい」と答えて、作ったコード。自分のためのOSSも、作った。公開もした。そこそこ使われもした。でも、そこそこ止まりだ。「これが俺の代表作です」と言えるものが、ない。スターはついた。ダウンロードもされた。いくつかは今でも自分で使っている。完走した。自分なりに頑張った。でも、「代表作」と呼べるインパクトには届かなかった。厄介なことに、nwiizoというアカウントは大きくなってしまった。フォロワーが増えた分、「代表作」のハードルも上がっている。昔なら「動くものを公開した」で満足できた。今は違う。期待値が上がった分、自分で自分の首を絞めている。でも、諦めたくない。代表作を持つソフトウェアエンジニアに憧れて、この道に入った。あの人みたいになりたい、と思った先輩たちがいる。彼らのようにはなれていない。でも、まだ諦めたくない。新しいプロジェクトを始めようとするたび、手が止まる。「既存のツールで十分じゃないか」「誰が使うんだ、これ」。もっともらしい問いを自分に投げかけて、そのまま手を下ろす。完走したプロジェクトはある。でも、次の一歩が踏み出せない。検証のふりをした、逃避だ。「作らなくていい理由」を探して、見つけて、安心している。AIは「どう作るか」を教えてくれる。でも「何を作るか」は教えてくれない。技術力はもうボトルネックじゃない。足りないのは、決断だ。覚悟だ。「これを作る」と宣言して、不確実性の中に飛び込む蛮勇だ。私は「隙間家具屋」を自称してきた。大きな家具は作らない。洗濯機と壁の間の収納。冷蔵庫の上のラック。誰も気にしないけれど、あると少し楽になる小さなもの。それを作るのが好きだった。はずだった。2025年、隙間を見つける目は曇っていなかった。手も動いた。完走もした。でも、「これだ」という手応えが残らなかった。副業は収入になる。登壇は評価される。ブログはフォロワーが増える。全部、目に見えるリターンがある。OSSは違う。作っても誰にも使われないかもしれない。時間を注いでも、何も返ってこないかもしれない。その「かもしれない」に怯えて、私は確実なほうへ流れやすかった。OSSは作った。完走もした。でも、賭け金を上げられなかった。時間を注ぎ込むより、確実なリターンがある副業や登壇に逃げた。結果、そこそこ止まり。「これだ」と言えるものは掴めなかった。問題は、才能がないことだけじゃない。問題は、狂えなかったことだ。どこにも振り切れなかった。副業も、登壇も、OSSも、全部やりたかった。全部にいい顔をして、どれにも本気を出せなかった。半端な賭け金には、半端なリターンしか返ってこない。当たり前のことだ。このブログでは、「狂って量をやって、そこから引き算する」ための思考法を書く。 speakerdeck.com結論を先に言う。まず狂え。量をやれ。そして、量が満ちたら、容赦なく削れ。ポジティブケイパビリティとネガティブケイパビリティ「ネガティブ・ケイパビリティ」という概念がある。不確実さ、不思議さ、疑いの中に、結論を急がずに留まる能力のことだ。これに対して、「ポジティブ・ケイパビリティ」というのもある。問題を分析し、解決策を導き、実行する能力だ。ゴールが明確なときに発揮される力。私はおそらくだがこれが得意だ。生成AIは、ポジティブケイパビリティを劇的に強化した。「このAPIを叩いて、結果をパースして、DBに保存するコードを書いて」と指示すれば、動くコードが出てくる。「このエラーメッセージの原因を調べて」と頼めば、調査結果が返ってくる。ゴールが明確なタスクは、AIとの協働で驚くほど速く片付く。私の2025年は、まさにこれだった。仕事のコードは書けた。クライアントから「これを作ってほしい」と言われれば、作れた。締め切りがあり、要件があり、ゴールが明確なタスクは、以前より速く終わるようになった。しかし、ネガティブケイパビリティは強化されなかった。むしろ、弱体化した気もする。 以前なら、分からないまま3日間コードを書き続けることができた。今は、30分詰まるとAIに聞いてしまう。「分からない」という状態に耐える筋力が、確実に落ちている。OSS開発には、ネガティブケイパビリティが必要だ。「何を作るか」は誰も教えてくれない。「これが正解」という保証はない。作っている途中で「これは違うかも」と思うことがある。それでも手を動かし続ける。完成するかどうか分からない。使われるかどうか分からない。その不確実さの中に留まり続ける力。生成AIに「何を作るべきか」と聞いても、答えは出ない。AIは優秀なアシスタントだが、ゴールを設定するのは人間の仕事だ。ゴールが明確な仕事が速く片付くようになった結果、私の中で奇妙なことが起きた。「答えがすぐに出る」ことに慣れてしまった。 仕事では、AIに聞けば数分で方向性が見える。それに慣れた脳は、「答えが出ない状態」に耐えられなくなっている。では、どうすれば不確実さに耐えられるのか。いくつかの仮説がある。ゴールを小さくする（「Kubernetesのログ管理を改善したい」ではなく「Podの再起動ログをSlackに送る」）。「完成」の定義を下げる（動けば完成、READMEは3行でいい）。公開してしまう（不確実性の一部が確定に変わる）。AIに頼らない時間を作る（自分で考える筋力を維持する）。「狂う」とは何か「狂う」という言葉を使うと、何か特別な才能や突飛な発想が必要に思える。しかし、私が考える「狂う」はもっと単純だ。狂うとは、常識的な量を超えて、時間と労力を注ぐことだ。天才的なアイデアは必要ない。奇抜な発想も必要ない。ただ、普通の人が「そこまでやらなくていいだろう」と思う量を投入する。これが狂うということだ。しかし、ここまで書いて気づいた。「量をやれ」というアドバイスは、ゴールが見えている人へのアドバイスだ。これはポジティブケイパビリティの話だ。「OSSを20個作れ」と言われても、「何を作るか」が決まっていなければ、手は動かない。私の問題は、量が足りないことではなく、ゴールが見えない状態に耐えられないことだった。ネガティブケイパビリティの欠如だ。だから、「狂う」にはもう1つの意味がある。答えが出ない状態に留まり続けることだ。「これが正解かどうか分からない」「誰にも使われないかもしれない」「もっといい方法があるかもしれない」。その不確実さの中で、それでも作り続ける。確信がないまま、手を動かし続ける。普通の人は、不確実さに耐えられない。「これで合ってる？」と誰かに確認したくなる。確認できないと、手が止まる。狂っている人は、確認しないまま走り続ける。生成AIは「確認」を容易にした。コードを書いたら、AIにレビューしてもらえる。設計を考えたら、AIに壁打ちしてもらえる。これは素晴らしいことだ。でも同時に、「確認なしで走り続ける」筋力が衰えた。量を積むことと、不確実性に耐えること。この2つは、実は表裏一体だ。量を積めば、その中から「これだ」というものが見えてくる。不確実性に耐えていれば、やがてゴールが見えてくる。どちらも「狂う」ことでしか到達できない。狂気の最も簡単な表現方法は、物量か時間を使うことだ。1日1時間を5年続ける。同じテーマのブログを100本書く。OSSを年間20個作る。なぜ20個か。月に1〜2個のペースだ。1つのツールを2週間で完成させる。完璧じゃなくていい。動けばいい。このペースなら、仕事をしながらでも無理がない。かつ、「そこそこ止まり」の自分とは明らかに違う場所に立てる。特別な才能がなくても、量を積めば、誰も追いつけない場所にたどり着く。ここで「衝動」という言葉を使いたい。不便を見つけたとき、「あ、これ自動化できそう」と思う。その瞬間、手が動き出す。誰に頼まれたわけでもない。でも、気づいたらコードを書いている。これが私にとっての衝動だ。「将来の夢」とは違う。他者の評価を求めている「有名なOSSメンテナになりたい」は、衝動ではない。衝動は、評価とは無関係に動く。10年経っても変わらない。「不便を見つけたら、すぐ直したくなる」。隙間家具を作るのは、この衝動の表れだ。問題は、この衝動を他者の目で覆い隠してしまうことだ。「作っても誰にも使われないかも」。そう考えた瞬間、衝動が埋もれる。2025年の私は、まさにこれだった。衝動は「発見」するものではなく「掘り出す」ものだ。他者の目や評価への恐れで覆い隠されている。それを掘り出すには、まず量をやる必要がある。考える前に手を動かす。作る前に悩まない。作った後に、何が自分を動かしているのかが見えてくる。まず量をやる私たちは、最初から量が足りない。2025年、私のOSSがそこそこ止まりだった理由は何か。作った。完走もした。でも、「代表作」と呼べるインパクトには届かなかった。振り返ると、1つに賭け切れていなかった。あれもこれもやろうとして、どれにも全力を注げなかった。「ゴールが見えないから突き抜けられない」と思っていた。でも、それは逆だ。一つに賭け切らないから、ゴールが見えない。作りはした。でも、広く浅く。一つに集中しなかったから、どれも「これだ」に辿り着けなかった。私の経験を話す。以前、「Kubernetesのログをなんとかしたい」という漠然とした不満があった。何を作ればいいか分からなかった。とりあえず、Podの再起動を検知するスクリプトを書いた。動いた。使ってみた。すると、「再起動の直前のログが見たい」という次の不満が見えた。それを解決するコードを足した。使ってみた。今度は「Slackに通知したい」という欲求が出てきた。最初に「Podの再起動時に直前のログをSlackに送るツール」というゴールが見えていたわけではない。作っているうちに、ゴールが形成されていった。ゴールは、作る前に見つかるものではない。作る過程で見えてくるものだ。量をやることで、初めて「自分が本当に作りたいもの」が浮かび上がる。完璧な1つより、動く20個。磨き上げた1つより、荒削りな50個。これが私の2026年の方針だ。物量で狂う。 OSSを年間20個作る。完璧じゃなくていい。動けばいい。20個作れば、1個くらいは当たる。当たらなくても、20個分の経験が残る。時間で狂う。 毎日30分、何かを作る時間を確保する。1年で小さなツールを20個作れば、5年で100個になる。100個のOSSを持っているエンジニアは、採用市場で見たことがない。試行で狂う。 1つのアイデアに固執しない。「これは違うな」と思ったら、すぐ次に行く。打席に立つ回数を増やす。三振しても気にしない。次の打席がある。私は30代で独身だ。守るべきものが少ない。狂えるうちに狂っておく。量をやることで、初めて見えてくるものがある。どのアイデアに自分の熱量が続くのか。どのツールが使われるのか。作る前に「どれが正解か」を考えても分からない。作った後に、結果が教えてくれる。量だけでは足りないからセンスを磨くここで反論が聞こえる。「量をやるだけなら、生成AIでもできるのでは？」正しい指摘だ。そして、もう1つ重要な変化がある。ソフトウェアは供給過多の時代に入った。あらゆる領域で「フロンティアの閉鎖」が起きている。かつてソフトウェアには未開拓の荒野があった。問題はそこら中に転がっていて、誰かが手を挙げて解決すれば、それだけで価値になった。参入障壁が高かったから、作れる人が少なかった。だから「作った」という事実そのものに希少性があった。今は違う。生成AIが参入障壁を破壊した。誰でも作れる。結果、供給が需要を超えた。ユーザーの時間と注意力が、ツールよりも希少になった。ツールが人を選ぶ時代から、人がツールを選ぶ時代へ。選ばれないツールは、存在しないのと同じだ。これは「量で勝てた時代の終焉」を意味する。かつての戦略は「とにかく作れ、出せ、数で勝負しろ」だった。今、その戦略は逆効果になりうる。大量の凡庸なツールを公開すると、ノイズを増やすだけで、作り手の信用を毀損する。つまり、量を公開しすぎることが、むしろマイナスになる時代が来ている。では、量をやる意味はどこにあるのか。ここで「センス」について考えたい。センスとは何か。私は、意味よりも先に、形式やリズムを感じ取る能力だと考えている。普通、私たちは物事を「これは何を意味するのか」で理解しようとする。コードを見て「このツールは何をするのか」と問う。ブログを読んで「著者は何を主張しているのか」と問う。意味を求める。でも、センスの本質はそこにない。センスとは、意味の手前にある「リズム」を感じ取ることだ。リズムとは、反復と差異の織り成すパターンのことだ。赤ちゃんが「いないいないばあ」で喜ぶのは、不在から存在への移行、つまり0→1のビートを感じているからだ。予測があり、裏切りがあり、また予測に戻る。この往復運動が快感を生む。あらゆる表現にリズムがある。音楽のビート。文章の緩急。コードの構造。APIの応答パターン。人間は意味を理解する前に、このリズムを身体で感じている。優れた表現は、セオリーを押さえた上で、あえてそこからはみ出す。 反復の中に絶妙な差異を混ぜている。予測可能でありながら、どこか予測を裏切る。この「ズレ」がセンスだ。ここで重要な逆説がある。完璧を目指すほど、センスは死ぬ。お手本を完璧に再現しようとすると、二つの問題が起きる。一つは、お手本との差異が「欠点」に見えてしまうこと。もう一つは、自分固有のリズムが消えてしまうこと。結果として、劣化コピーが生まれる。逆に、お手本から離れることを肯定すると、「ヘタウマ」が生まれる。完璧ではないが、作り手固有のリズムがある。技術的には未熟でも、個性がある。その個性が、使う人に刺さる。なぜ個性が刺さるのか。人間は、パターンを認識する生き物だからだ。完璧にパターン化されたものは、最初は心地よい。でも、すぐ飽きる。予測通りすぎて、刺激がない。一方、パターンから少しズレたものは、脳に引っかかる。「なぜここでこうなる？」という小さな疑問が生まれ、それが記憶に残る。AIは反復とパターンを生成できる。しかし、その人固有の「どうしようもなさ」は生成できない。「どうしようもなさ」とは何か。個人の癖、偏り、こだわり。論理では説明できない選好。なぜか惹かれるもの。なぜか避けたくなるもの。この非合理な偏りが、人間の表現に陰影を与える。私がツールを作るとき、そこには私の「どうしようもなさ」が刻まれる。なぜこの設計を選んだのか、論理的に説明できない部分がある。それは私の経験、私の好み、私の盲点が複合的に作用した結果だ。AIが同じ仕様で作っても、同じものにはならない。センスとは、リズムを感じ取る能力であり、同時に、自分固有のリズムを表現する能力でもある。では、どうやってセンスを磨くのか。答えは逆説的だ。量をやることだ。多様なものに触れると、最初は不安を感じる。「分からない」「理解できない」。この不安は、パターンを認識できていないサインだ。量を重ねると、パターンが見えてくる。不安が面白さに変換される。これがセンスが磨かれる過程だ。ここで矛盾が生じる。センスを磨くには量が必要だ。しかし、量を公開しすぎるとマイナスになる。答えは、「作る量」と「公開する量」を分けることだ。20個作る。でも、公開するのは、センスが良いと判断した5個だけ。残りの15個は、センスを磨くための練習だ。公開しない。でも、作ったことに意味がある。量をやることには、二重の意味がある。1つ目は、センスを磨くこと。多様なものを作ることで、「何が良くて何が良くないか」を判断する回路ができる。リズムを感じ取る力が育つ。2つ目は、自分の「どうしようもなさ」を発見すること。量をやると、自分のパターンが見えてくる。どういう問題に惹かれるか。どういう設計を好むか。それは私の固有性であり、AIには真似できない。だから、量をやる意味は「AIより速く作る」ことではない。量を通じて、リズムを感じ取る力と、自分固有のリズムを発見することだ。そして、センスが磨かれた後は、公開するものを厳選する。供給過多の時代に求められるのは、「たくさん作れる人」ではない。「たくさん作った上で、良いものだけを選べる人」だ。AIは「どう作るか」を効率化する。でも、「何を作るか」「どれを公開するか」「どう判断するか」は、量を経験した人間にしか分からない。そして引き算する量をやった。20個作った。では、20個全部を維持できるか。できない。私には経験がある。かつて、複数のプロジェクトを同時に走らせていた。イシューは溜まり、プルリクエストは放置され、READMEは古くなった。全部やろうとして、全部が死んだ。量をやることと、量を維持することは違う。 量をやるのは一時的な狂気だ。量を維持するのは持続的な負担だ。人間のリソースは有限だから、量をやった後には、引き算という別の問題が待っている。私たちは、量が満ちた後に引かなすぎる。 量をやった後は、容赦なく削る。使われないツールは捨てる。熱量が続かないプロジェクトはアーカイブする。失うのは「いつかやるかもしれない」という幻想だ。守れるのは「今、本当にやりたいこと」への集中だ。削らずに広げ続けた結果が2025年の私だ。副業も、登壇も、ブログも、OSSも、全部やった。全部それなりに成果は出た。でも、どれも「これが俺の本業だ」と言い切れない。器用貧乏の完成形だ。ここで「引き算」の思考法が必要になる。シーナ・アイエンガー氏の有名な実験では、24種類のジャムより、6種類に絞った方が購入率は高かった。選択肢が多すぎると、人は「選ぶ」という行為自体ができなくなる。選択の科学 コロンビア大学ビジネススクール特別講義 (文春文庫 S 13-1)作者:シーナ アイエンガー文藝春秋Amazonアイエンガー氏は『THINK BIGGER』で、選択肢が多すぎて選べないときの思考法を体系化した。その本質は「引き算」だ。課題を選ぶ、分解する、誰のためかを決める、材料を集める、何を作らないかを決める、他者の目で検証する。すべて「絞る」プロセスだ。THINK BIGGER 「最高の発想」を生む方法：コロンビア大学ビジネススクール特別講義 (NewsPicksパブリッシング)作者:シーナ・アイエンガーニューズピックスAmazon狂って量をやるフェーズでは、複数のアイデアが同時に走っている方が自然だ。順番通りに1つずつ片付けようとすると、むしろ手が止まる。どれかが熱を帯びてきたら、そこに集中する。足し算ではない。引き算だ。優れた開発者のOSSが失敗するのは、怠けているからではない。正しいことをしすぎるからだ。 ユーザーの声を聞く。機能を追加する。対応範囲を広げる。全部、正しいことだ。でも、正しいことを積み重ねた結果、複雑になり、重くなり、新しく登場したシンプルなツールに足元をすくわれる。私たちは「正しさ」に殺される。ユーザーの声を聞くのは正しい。だから聞く。機能を追加するのは正しい。だから追加する。テストを書くのは正しい。だから書く。ドキュメントを整えるのは正しい。だから整える。気づいたら、最初に解決したかった問題が見えなくなっている。正しいことの山に埋もれて、本質が窒息している。「正しさ」は麻薬だ。やればやるほど気持ちいい。やればやるほど、完成から遠ざかる。隙間家具を作るとは、引き算をすることだ。機能を削る。対象を絞る。スコープを小さくする。「これだけは解決する」を決め、残りは捨てる。生成AIを使うとき、この引き算が難しくなる。AIは指示すれば無限に足し算を提案してくる。「この機能も追加しましょうか」「こういうオプションもあると便利です」「エラーハンドリングをもっと丁寧にしましょう」。全部、正しい提案だ。でも、全部受け入れると、隙間家具は大きな家具になる。AIは足し算が得意だ。引き算は人間がやる。私がAIに「削らせる」ときに使う問いかけがある。「この機能がなくても、最小限の価値は提供できるか？」。答えがYESなら、その機能は削る候補だ。AIの提案を聞いたら、「本当に必要か？」と問い直す。これが、AIとの協働における引き算の基本姿勢だ。「何を作るか」を決める課題を選ぶ引き算の最初は、「何を作るか」を1つに決めることだ。私が2025年に「代表作」に届かなかった理由の1つは、課題が大きすぎたことだ。「Kubernetesのログ管理を改善したい」と思った。でも、それは「どのログ」「どう改善」「誰のため」が決まっていない。漠然としすぎていた。結果、インパクトのあるものが作れなかった。「作りたいものはあるけど、何から手をつければ...」という状態は、課題が大きすぎるか小さすぎるかのどちらかだ。大きすぎると作りきれない。小さすぎると作る意味がない。「1つのツールで完結する」サイズを探す。課題が大きすぎる例:「Kubernetesの代替」「CI/CDパイプライン全体の改善」「インフラ自動化ツール」課題が小さすぎる例:「kubectl getのラッパー」「特定のエラーメッセージを整形するスクリプト」ちょうどいい例:「Podが再起動したときに直前のログを保存するツール」「複数リポジトリのCIステータスを一覧表示するCLI」「Terraformの差分をSlackに見やすく投稿するBot」ちょうどいいサイズの見つけ方は、「自分が1〜3日かけて解決したこと」を思い出すことだ。それは、深みがある。かつ、1つのツールで完結する気がする。隙間を見つける大きなツールが解決していない小さな問題。それが「隙間」だ。Kubernetes（コンテナオーケストレーション）は素晴らしい。しかし、Kubernetesが解決していない問題は山ほどある。Podが再起動したとき、前後のログを自動でSlack に送りたい。これはKubernetesの仕事ではない。Terraform（インフラ構成管理）も素晴らしい。ただ、差分をSlackに見やすく投稿したい。これはTerraformの仕事ではない。GitHubも同様だ。複数リポジトリのCIステータスを一覧で見たい。これはGitHubの仕事ではない。隙間を見つけるヒントは5つある。自分の不便。「こういうツールが欲しいのに、ない」という体験。私が作った隙間家具の中で、最も使われたものは、自分自身の問題を解決するために作ったものだった。自分が不便を感じているとき、そこには片づけたい「用事」がある。でも、それを片づける手段がない。私のGithub リポジトリからのスクショここで疑問が浮かぶ。「自分の不便」が特殊すぎるときはどうするのか。自分だけが困っている問題を解決しても、誰も使わないのではないか。だから2026年、私はこう決めた。最初は特殊すぎて構わない。なぜなら、特殊な問題を解決するツールでも、自分が本当に使うなら完成する。「誰かが使うかも」で作ったツールは、途中で手が止まる。まず完成させることが最優先だ。公開してみれば、同じ問題を抱えている人が意外といることに気づく。特殊だと思っていた不便が、実は普遍的だったというケースは多い。仮に本当に特殊で誰も使わなくても、自分の問題は解決している。それで十分だ。繰り返しの手作業。同じコマンドを何度も打っている。同じ手順を何度も実行している。毎回「面倒だな」と思いながら、やっている。ここで立ち止まる。この問題は「自動化すべき問題」か、それとも「慣れるべき問題」か。ツール化することで、本当に人間の負荷は減るのか。自動化によって、別の複雑さを生んでいないか。判断基準は、その作業が月に何回・何分発生しているかだ。月に1回、5分で終わる作業なら、自動化ツールを作るより慣れた方が早い。週に10回、毎回10分かかる作業なら、自動化する価値がある。感覚で判断しない。数字で判断する。例えば、複数のGitHubリポジトリのCIステータスを確認するとき、1つずつページを開いていた。毎回、5分くらいかかる。週に5回やっていた。月に100分。年に1200分。ツールを作る価値がある。作った。5分が10秒になった。コンテキストスイッチ。ある情報を得るために、複数のツールを行き来している。Slackを見て、Grafanaを見て、ログを見て、またSlackに戻る。情報を一箇所に集めるツールを作れば、コンテキストスイッチが減る。頭の負荷が減る。判断が速くなる。暗黙知。「あの人に聞けば分かる」「Slackのどこかにある」「この手順は、前にやったことある人しか知らない」。暗黙知をツールに埋め込めば、誰でも同じことができるようになる。複雑さ。「このツールは高機能だけど、使いこなせない」「設定項目が多すぎて、何を設定すればいいか分からない」。高機能なツールが、その機能を使い切れていない人たちを置き去りにしている。彼らに、シンプルで分かりやすい選択肢を提供する。これも隙間家具の仕事だ。課題を分解する課題が決まったら、5つまでに分解する。私がよくやる失敗は、分解せずに作り始めることだ。「ログ保存ツールを作ろう」と思って、いきなりコードを書き始める。途中で「保存先どうしよう」「認証どうしよう」「エラーハンドリングどうしよう」と考え始める。そのたびに手が止まる。最初に分解しておけば、こうはならない。「〇〇を作ろう」だけでは手が動かない。サブ課題に分解して、5つまでに絞る。5つに絞るのは、正直、苦しい。あれもこれも入れたくなる。でも、ジャムの法則と同じだ。サブ課題を10個、20個と出すと、どれに注力すべきか分からなくなる。例: 「Podが再起動したときに直前のログを保存するツール」Podの再起動を検知する仕組み直前のログを取得する方法ログを保存する先（S3など）CLIのインターフェースエラーハンドリング分解した項目が、そのまま実装の順番になる。「これは本当に必要か？」と自問すると、いろいろ見えてくる。実は同じことをしている項目。なくても動く項目。別のツールに任せた方がいい項目。削ることで本質が見える。5つに分解したら、次に優先順位をつける。何を基準に「残す1つ」と「後回しにする4つ」を決めるか。私の基準は、「これがないと、ツールとして成立しない」だ。技術的な実現性でも、ユーザーの感動でも、自分の興味でもない。「ツールの存在意義に関わるか」だ。例えば、「Podの再起動を検知する仕組み」がなければ、ログ保存ツールは成立しない。これが最優先だ。「CLIのインターフェース」は後でもいい。最初はハードコードでも動く。ここまでで、「何を作るか」と「どう分解するか」が決まった。でも、まだ足りない。「誰のために作るか」が決まっていない。「誰のために作るか」を決める望みを比較する同じツールでも、誰向けに作るかで設計が変わる。自分用なら雑でいい。他人に使ってもらうなら、READMEが必要だ。コミュニティに貢献したいなら、テストも書く。私が2025年に「代表作」に届かなかったもう1つの理由は、「誰のため」が曖昧だったことだ。「これ、公開したら使ってもらえるかな」と考えた瞬間、設計が複雑になる。「あの人はこういう使い方するかも」「この環境もサポートした方がいいかも」。考えれば考えるほど、作るものが膨らむ。膨らめば膨らむほど、作れなくなる。3つの望みがある。自分が作りたいもの。ユーザーが使いたいもの。コミュニティへの貢献。全部満たそうとすると、どれも中途半端になる。だから2026年、私はこう決断する。まず自分の問題を解決するツールを作る。当たり前すぎるかもしれない。でも、これが私の経験則だ。自分が本当に困っている問題なら、熱量が出る。熱量のあるツールは、ユーザーにも伝わる。これは「プロダクト」ではなく「道具」として十分に割り切れているか。プロダクトは他者のためにある。道具は自分のためにある。隙間家具は道具だ。自分の問題を解決するために作る。他者が使ってくれたらラッキー、くらいの気持ちでいい。汎用性を上げようとして、複雑さを持ち込んでいないか。持ち込みがちだ。「S3だけじゃなくGCSにも対応しよう」「Kubernetes以外でも使えるようにしよう」。その瞬間、道具がプロダクトになろうとする。複雑さが増す。完成しなくなる。READMEは「思想」ではなく「使い方」を語っているか。思想を語りがちだ。「なぜこのツールが必要か」「どんな設計思想か」。でも、ユーザーが知りたいのは「どう使うか」だ。インストール方法、実行方法、オプション。これだけでいい。自分以外の利用者がゼロでも、このツールは成立しているか。成立している必要がある。自分の問題が解決しているなら、それで十分だ。他者が使うかどうかは、結果論だ。「まだ誰も使っていない人」を見る自分が不便を感じているとき、同じ不便を感じている人は他にもいる。片づけたい用事があるのに、それを片づける手段を持っていない人。私はこの人たちを「まだ誰も使っていない人」と呼んでいる。自分がその一人だったなら、同じ境遇の人が他にもいるだろう。隙間家具は、この人たちに届ける。ここで注意が必要だ。ツールを公開すると、ユーザーからフィードバックが来る。「この機能が欲しい」「ここが使いにくい」。これは嬉しい。でも、ここに罠がある。既存ユーザーの声を聞けば聞くほど、既存ユーザーのためのツールになる。そして、「まだ誰も使っていない人」を見落とす。既存ユーザーの声に応え続けると、隙間家具は大きな家具になろうとし始める。機能が増え、複雑になり、最初のシンプルさを失う。新規ユーザーが求めているのは、高機能ではなく「すぐ使える」「分かりやすい」だ。「声」と「用事」を区別するフィードバックを受けるとき、「声」と「用事」を区別する。私も失敗したことがある。あるCLIツールを公開したとき、「設定ファイルで動作を変えたい」というフィードバックを複数もらった。嬉しかった。使ってくれている人がいる。だから、設定ファイル機能を実装した。YAMLで書けるようにした。オプションを増やした。結果、設定項目が20個を超えた。新しいユーザーは「設定が多すぎて何を設定すればいいか分からない」と言い始めた。シンプルさが売りだったツールは、複雑なツールになっていた。「声」は、ユーザーが言語化したものだ。「この機能が欲しい」「ここが使いにくい」。「用事」は、ユーザーが本当に片づけたいことだ。なぜその機能が欲しいのか。なぜそこを使いにくいと感じるのか。この「なぜ」の先に、本当の用事がある。例えば、CLIツールに「YAML出力オプションが欲しい」というフィードバックが来たとする。声をそのまま受け取れば、--output yamlフラグを実装することになる。でも、「なぜYAMLが欲しいのか」を問うと、「他のツールにパイプしたい」「設定ファイルとして保存したい」という用事が見えてくる。用事が分かれば、YAMLだけでなくJSONでも解決できるだろう。あるいは、標準出力をそのままパイプできる設計にすれば、フォーマット変換はjqに任せられるだろう。「この機能が欲しい」と言われたら、「なぜ」を問う。その人の用事は何か。その用事を片づける方法は、言われた機能だけか。もっとシンプルな方法はないか。ツールがヒットすると、「汎用化」の要望が必ず来る。「S3だけでなくGCSにも対応して」「Kubernetes以外でも使えるようにして」。これに応えると、隙間家具は大きな家具になる。だから私は、こう決めている。READMEが複雑になるなら、その機能は入れない。機能を追加するとき、READMEがどう変わるかを見る。説明が長くなるなら、別のツールにする。READMEがシンプルなら、ツールもシンプルだ。これが私の制約であり、美学だ。ここまでで、「何を作るか」「誰のために作るか」が決まった。次は、作る前に調べる。調べて、削る箱の中と外を探すいきなり作り始めたくなる。でも、その前に下調べをする。私は以前、「これ、俺が作らなくても既存ツールで十分だな」と気づいて手を止めたことがある。それ自体は正しい判断だった。でも、その後「じゃあ俺の経験は何に使えるか」を考えなかった。既存ツールを調べて終わり。それでは何も生まれない。似たツールはあるか。どんなアプローチがあるか。先人の知恵を借りる。「箱の中」は同じ領域の情報だ。公式ドキュメント、他の人の同じテーマのツール、GitHub Issues、Stack Overflow。正確性を担保し、抜け漏れを防ぐ。「箱の外」は自分の経験だ。実際に試した結果、ハマったポイントと解決策、自分なりの工夫や改善。これがオリジナリティの源泉になる。ここで重要なのは、インプットだ。本を読む。既存のOSSのコードをちゃんと読む。何のライブラリが使われていて、どのように問題を解決しているかを理解する。これが「箱の中」を深く知ることだ。例えば、Kubernetesのログ保存ツールを作るなら、既存の類似ツールのコードを読む。どのKubernetesクライアントライブラリを使っているか。どうやってPodの再起動を検知しているか。ログの取得にはどのAPIを使っているか。保存先との接続はどう抽象化しているか。コードを読まずに作り始めると、車輪の再発明をする。既に解決されている問題を、苦労して解き直す。あるいは、先人が避けた落とし穴にハマる。インプットの具体例を挙げる。本を読む：技術書だけでなく、設計思想やアーキテクチャの本も読む。『A Philosophy of Software Design』『The Art of Unix Programming』。隙間家具を作る視点が変わる。OSSのコードを読む：GitHubで似たツールを探して、main.goやlib.rsを読む。README だけでなく、実装を見る。「なるほど、こう解決するのか」という発見がある。ライブラリの使い方を学ぶ：使おうとしているライブラリのexampleを全部読む。ドキュメントを端から端まで読む。「こんな機能もあったのか」という発見が、設計を変える。「既に同じようなツールがある」は気にしない。同じ課題を解決するツールでも、価値を出せる理由はある。環境が違う。文脈が違う。深さが違う。切り口が違う。あなたのツールにしかない価値は、あなたの環境で動いた事実、あなたがハマったポイント、あなたの言葉での説明だ。「n番煎じ」でも、あなたの経験を加えれば価値になる。「箱の外」の材料を増やすために、私が意識的にやっていることがある。「自分の仕事を観察する」だ。エンジニアリング以外のインプットも大事だが、それ以上に、自分が日常的にやっている作業を観察する。「今、何に時間を使っているか」「何に苛立っているか」「何を繰り返しているか」。この観察が、隙間を見つける材料になる。選択マップで削る材料が揃ったら、「何を作り、何を作らないか」を選ぶ。私は「全部入り」を目指しがちだ。ログ保存ツールを作るなら、S3もGCSもAzure Blobも対応したくなる。Slack通知もメール通知もつけたくなる。そうこうしているうちに、何も作れなくなる。選択マップとは、集めた選択肢を視覚的に整理し、最適な組み合わせを見つける方法だ。課題から分岐して選択肢を並べ、各選択肢のメリット・デメリットを可視化する。例: 「OOMKilled（メモリ不足による強制終了）の調査方法を紹介するツール」調査方法は複数ある。kubectl top（リソース使用状況確認）、Grafana（可視化ダッシュボード）、pprof（プロファイリングツール）、サードパーティツール。読者に最も役立つのはどれか。kubectl topは簡単ですぐ使えるが、瞬間値しか見られない。Grafanaは履歴を見られるが、セットアップが必要。pprofは詳細に分析できるが、設定が必要で学習コストは高い。選択結果：読者の多くは「まず何が起きてるか知りたい」→ kubectl top + Grafanaを中心に作る。pprofは発展編として軽く触れるか、別のツールにする。足し算の発想だと、全部の方法をサポートしようとする。焦点がぼやける。誰にも刺さらない。引き算の発想だと、「これだけは作る」を決める。残りは捨てる。刺さるツールになる。良いツールは「何を作らないか」で決まる。スコープを絞る勇気隙間家具は、特定の問題を解決する。汎用性を追求しない。「このコンテキストで、この問題を解決する」に集中する。例えば、「KubernetesのPodが再起動したとき、直前のログを自動でS3に保存するツール」。汎用的ではない。Kubernetesを使っていて、ログをS3に保存したい人だけを対象にする。でも、それでいい。特定の問題を、特定のコンテキストで、確実に解決する。これが隙間家具の価値だ。汎用性は、使われてから考えればいい。最初から汎用的に作ろうとすると、要件が膨らみ、複雑になり、いつまでも完成しない。ここまでで、何を作るか、誰のために作るか、何を作らないかが決まった。いよいよ作る。小さく作って、見せる第三の目で検証する作った。動いた。自分では完璧に見える。でも、それは危険なサインなんだ。私にも経験がある。あるCLIツールを作って、自分では「完璧だ」と思った。README も書いた。インストール方法も書いた。でも、同僚に見せたら「これ、何をするツールなの？」と聞かれた。私には当たり前すぎて、説明を省略していた。「前提知識がないと、何も分からない」。そのツールは結局、私しか使わなかった。使い方を説明する手間を惜しんだ結果だ。作った本人には見えない穴がある。「当然わかるでしょ」と省略している。専門用語を説明なしで使っている。論理の飛躍に気づかない。自分では完璧に見える。だから、他者に見せる。使ってもらう。フィードバックをもらう。隙間家具を必要としている人は、探していない。問題を抱えているが、解決策があるとは思っていない。だから、「検索してたどり着く」ことを期待できない。では、どうやって届けるか。自分の体験を語る。「私はこういう問題を抱えていた。だから、このツールを作った。」「まだ誰も使っていない人」は、同じ問題を抱えているだろう。ブログやTwitterで体験を語れば、「あ、自分もこの問題を抱えている」と思ってもらえる。READMEに機能を列挙するだけでは届かない。「なぜこのツールを作ったか」「どんな問題を解決するか」を語る。「まだ誰も使っていない人」は、自分の不便を言語化できていないことが多い。だから、状況を描写する。「毎朝、Slackを開いて、Grafanaに移動して、ログを確認して、またSlackに戻る...この作業、面倒じゃないですか？」。機能ではなく、状況を語る。「あ、それ自分だ」と思わせる。ツールの説明ではなく、問題の描写から始める。これが、言語化できていない不便に気づかせるストーリーテリングだ。入り口を簡単にするインストールが面倒だと、人は離れる。設定が複雑だと、人は離れる。最初の一歩を、できるだけ簡単にする。go install 一発でインストールできる。設定ファイルは最小限。デフォルトで動く。これが理想だ。なぜなら、新しいツールを試すとき、人は「動かすまでの時間」を無意識に測っている。5分で動かなければ、「また今度」になる。設定が多いツールは、5分では動かない。だから、試されずに終わる。パワーユーザーは細かい設定を求めるだろう。でも、パワーユーザーは「まだ誰も使っていない人」ではない。最初に届けるべきは、5分で動くシンプルさだ。新しいツールは、最初は既存のツールより「劣っている」ことが多い。機能が少ない。パフォーマンスが低い。でも、シンプルで、分かりやすくて、すぐに使える。それでいい。隙間家具は、シンプルでいい。1つのことを、確実にやる。それが、「まだ誰も使っていない人」に届く。「ジャムの法則」をインターフェースにも適用する。CLIツールなら、フラグを減らす。理想は、引数なしで動くこと。mytoolと打てば、最も一般的なユースケースが実行される。設定が必要なら、対話的に聞く。フラグは上級者向けのショートカットだ。最初から覚えてもらうものではない。選択肢を減らすことで、ユーザーは「考える」から「使う」にすぐ移れる。このツールは「技術的に正しい」より「現場で生き残る」設計になっているか。技術的に正しい設計は、しばしば複雑になる。すべてのエッジケースに対応する。すべてのエラーを丁寧にハンドリングする。でも、現場で使われるツールは、シンプルで、雑でも動く。エッジケースを切り捨てた理由を説明できるか。説明できる必要がある。「このケースは月に1回しか発生しない。手動で対応すればいい。だから、ツールでは対応しない。」こう言い切れるなら、切り捨てていい。例外処理より「何も起きないこと」を優先していないか。優先していい。エラーが発生したとき、丁寧なエラーメッセージを出すより、そもそもエラーが発生しない設計の方がいい。入力を厳しくする。想定外の状態を作らない。現場の雑さ・曖昧さ・不完全さを前提にできているか。現場は綺麗ではない。設定ファイルにtypoがある。環境変数が設定されていない。ネットワークが不安定。この雑さを前提に設計する。「完璧な環境でしか動かないツール」は、現場では使われない。小さく始める6ステップを踏んでも、完璧なツールは作れない。だから、小さく始める。最初から完璧なツールを作ろうとしない。自分の問題を解決するスクリプトから始める。それが動いたら、少し整えて公開する。私の場合、多くの隙間家具は、最初はただのシェルスクリプトだった。自分の問題を解決するために、ちょっと書いた。それが便利だったので、もう少し整えた。それを公開した。完璧を目指すと、いつまでも公開できない。「もう少し機能を追加してから」「もう少しドキュメントを整えてから」。そうこうしているうちに、作る気力がなくなる。動くものを、まず作る。公開する。使ってもらう。フィードバックをもらう。改善する。このサイクルを回す。隙間家具を1つ公開したら、終わりではない。むしろ、ここからが始まりだ。探索を続ける捨てやすく作るここからが、私が一番伝えたいことだ。隙間家具には寿命がある。状況が変われば、不要になる。だから、捨てやすく作る。このツールは「自分が将来保守したいコード」になっているか。正直に言えば、保守したくないコードの方が多い。だから、捨てやすく作る。保守したくなるほど愛着が湧くツールは、20個に1個くらいでいい。半年後の自分が読んで理解できる設計になっているか。なっていなくてもいい。半年後に必要なら、そのとき書き直せばいい。必要なければ、捨てればいい。機能追加ではなく「削除」するとしたら、どこを真っ先に消すか。この問いを常に持っておく。削除できる部分があるなら、それは最初から作らなくてよかった部分かもしれない。このコードは、使われなくなったときに綺麗に捨てられるか。捨てられる設計にしておく。依存を少なく。外部サービスとの結合を弱く。捨てるときに、誰にも迷惑がかからないように。私が作ったツールの中で、すでに捨てたものがある。Kubernetesをインストールするツールを作っていた。当時、Kubernetesのインストールは複雑で、手順を間違えると動かなかった。だから、自動化ツールを作った。便利だった。でも、kubeadmがリリースされて、インストールが簡略化された。ツールは不要になった。リポジトリをアーカイブした。悲しくはなかった。むしろ、「自分の問題意識は正しかった」と思えた。Kubernetesの開発者も同じ問題を認識していたのだから。このOSSは「本流に取り込まれる未来」を想定できているか。想定しておく。もしKubernetesやTerraform本体に同等機能が入ったら、どうするか。喜んで捨てる。それは「失敗した」のではなく「役目を終えた」のだ。本流に吸収されるために、意図的にやっていないことは何か。汎用化だ。本流は汎用的になろうとする。隙間家具は特殊なままでいい。特殊だから、本流が取り込みにくい。特殊だから、生き残れる。隙間家具は、状況が変われば不要になる。Kubernetesのバージョンが上がって、その問題が解決されるだろう。別のツールが登場して、より良い解決策を提供するだろう。だから、依存を少なく、シンプルに作る。捨てやすく作る。大きな家具は、捨てにくい。多くのリソースを投入している。多くの人が使っている。捨てることが難しい。隙間家具は、捨てやすい。役目を終えたら、捨てる。そして、新しい隙間を見つけて、新しい隙間家具を作る。「隙間」が「本流」に飲み込まれるリスクもある。Kubernetesのサイドカー機能が進化するように、プラットフォーム自体が隙間を埋めてしまうことがある。これに対する私の戦略は2つだ。1つ目は、捨てやすく作ること。本流に飲み込まれたら、素直に捨てる。自分の問題意識が正しかった証拠だと喜ぶ。2つ目は、本流が手を出さないニッチに特化すること。Kubernetesは汎用的になろうとする。だから、特定の会社の特定のワークフローに特化したツールは、本流が取り込みにくい。汎用化できないほど特殊なニッチを狙う。これも生存戦略だ。捨てたツールから得られる学びもある。単なる「失敗」で終わらせず、次の探索に活かせる知見を抽出する。私がやっているのは、「なぜこのツールは役目を終えたのか」を言語化することだ。本流に取り込まれたのか。別のツールが出てきたのか。そもそも問題設定が間違っていたのか。この分析が、次の隙間を見つける精度を上げる。「問題設定が間違っていた」が一番の学びだ。次は同じ間違いをしない。深化と探索隙間家具を1つ作ったら、終わりではない。隙間家具の開発には、2つの仕事がある。「深化」と「探索」だ。「深化」は、既存の隙間家具を改善すること。バグを直す。パフォーマンスを改善する。ドキュメントを整える。「探索」は、新しい隙間を見つけること。新しい用事を発見すること。新しい隙間家具を作ること。問題は、「深化」へ偏りやすいことだ。既存のツールへイシューが立つ。プルリクエストが来る。対応すると達成感がある。でも、これだけやっていると、最初に見つけた隙間だけを相手にし続けてしまう。競争のないところに宝がある。既存の競合がひしめく場所ではなく、誰も見ていない場所を探す。だから2026年、私はこう決めた。小さな実験を続ける。1つの隙間家具に全力を注ぐのではなく、複数の隙間家具を作り、どれが使われるか見る。全部が使われるわけではない。むしろ、使われないものの方が多い。でも、それでいい。使われなかったツールからも、学びがある。その学びが、次の探索に活きる。チーム開発での引き算ここまでの話は、一人で作る「隙間家具」を前提にしてきた。では、複数人で開発するときはどうか。「引き算の哲学」をチームで共有できるのか。私の経験では、スコープを最初に合意することが鍵だ。「このツールは何を解決し、何を解決しないか」を、開発を始める前にドキュメントへ書く。機能追加の提案が来たら、このドキュメントに立ち返る。「このスコープ外です」と言える根拠になる。チームでの合意形成は、一人のときより難しい。でも、「1つのREADMEで説明できる範囲」という制約は、チームでも使える。「この機能を追加したら、READMEはどう変わるか」を問う。READMEが複雑になるなら、その機能は入れないか、別のツールにする。この基準は、チームメンバー全員が判断できる。個人の好みではなく、客観的な基準だ。おわりにここまで読んでくれた人に、正直に書く。この文章を書きながら、私は何度も手を止めた。「こんなこと書いて意味あるのか」「誰が読むんだ」「もっといい構成があるんじゃないか」。書いている最中に、書くのをやめる理由を探している自分がいた。「代表作」に届かない理由と、まったく同じ構造だ。笑えない。2026年、私は隙間家具を20個作ると決めた。完璧じゃなくていい。動けばいい。使われなくてもいい。作ることそのものに意味がある。そう自分に言い聞かせている。本当にできるかは、分からない。来年の今頃、GitHubにリポジトリが20個並んでいる保証はどこにもない。また「時間がなかった」「優先順位が」と言い訳しているかもしれない。その可能性は、正直、かなり高い。でも、書いた。こうして宣言してしまった。「何を作ればいいか分からない」という人へ。それは正常だ。ゴールは最初から見えているものじゃない。作っているうちに、少しずつ輪郭が浮かんでくる。だから今日、30分だけ時間を取って、最近「面倒だな」と思った作業を1つ書き出してみてほしい。それを解決するスクリプトを書く。動いたら公開する。それだけでいい。20回繰り返す頃には、自分が本当に作りたいものが見えてくる。たぶん。見えてこなかったら、そのときはまた考える。ところで、ここまで偉そうに書いてきたが、私は孤独な独身男性だ。家族はいない。守るべきものが少ない分、狂いやすい環境にいるとも言える。歯止めをかけてくれる人がいない分、自分で自分を律する必要がある。友達との飯の予定。ジムの予約。強制的に「コードを書かない時間」を作らないと、際限なく沈んでいく。独身には独身の戦い方がある。OSSより大事なものはある。友達と話す時間。体を動かす時間。コードは逃げない。隙間家具はいつでも作れる。でも、友人との関係は放っておくと薄れる。健康は一度壊すと戻らない。狂うなら、余裕のあるときに狂え。順番を間違えると、人生ごと壊れる。......と、説教じみたことを書いたが、たぶん来年の今頃の私は、この文章を読み返して頭を抱えている。「狂う」とか言って、結局また「そこそこ」で終わったじゃないか、と。nwiizoというアカウントは、また少し大きくなっているだろう。「代表作」のハードルも、また少し上がっているだろう。自分で自分の首を絞める構造は変わらない。それでも、諦めたくない。憧れたエンジニアたちがいる。彼らのように、「これを作りました」と胸を張れる日が来るまで、手を動かし続ける。だから、書いておく。まず狂え。量をやれ。そして、量が満ちたら、容赦なく削れ。答えが出ない状態は、苦しい。でも、その苦しさの中を泳ぎ続けることでしか、本当に作りたいものは見つからない。完璧を待たない。不完全なまま公開する。恥をかく覚悟で、手を動かす。2026年は、そういう年にする。できるかどうかは知らない。でも、やると決めた。隙間を見つけたら、小さく狂おう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考文献人生にコンセプトを (ちくまプリマー新書)作者:澤田智洋筑摩書房Amazonセンスは知識からはじまる作者:水野学朝日新聞出版Amazonセンスの哲学 (文春e-book)作者:千葉 雅也文藝春秋Amazon人生の経営戦略――自分の人生を自分で考えて生きるための戦略コンセプト２０作者:山口 周ダイヤモンド社Amazon「面白い！」を見つける　――物事の見え方が変わる発想法 (ちくまプリマー新書)作者:林雄司筑摩書房AmazonTHINK BIGGER 「最高の発想」を生む方法：コロンビア大学ビジネススクール特別講義 (NewsPicksパブリッシング)作者:シーナ・アイエンガーニューズピックスAmazonわかったつもり～読解力がつかない本当の原因～ (光文社新書)作者:西林 克彦光文社Amazon知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazon私が間違っているかもしれない作者:ビョルン・ナッティコ・リンデブラッド,キャロライン・バンクラー,ナビッド・モディリサンマーク出版Amazon不完全主義　限りある人生を上手に過ごす方法作者:オリバー・バークマンかんき出版Amazon熟達論―人はいつまでも学び、成長できる―作者:為末大新潮社Amazon新版 いくつになっても、「ずっとやりたかったこと」をやりなさい。作者:ジュリア・キャメロン,エマ・ライブリーサンマーク出版Amazonいくつになっても恥をかける人になる【DL特典 恥克服ワークシート】作者:中川諒ディスカヴァー・トゥエンティワンAmazon増補改訂版 スマホ時代の哲学 なぜ不安や退屈をスマホで埋めてしまうのか (ディスカヴァー携書)作者:谷川嘉浩ディスカヴァー・トゥエンティワンAmazon自分とか、ないから。　教養としての東洋哲学作者:しんめいPサンクチュアリ出版Amazon人生のレールを外れる衝動のみつけかた (ちくまプリマー新書)作者:谷川嘉浩筑摩書房Amazon行動する人に世界は優しい―自分の可能性を解き放つ言葉―作者:佐藤航陽新潮社Amazon","isoDate":"2025-12-22T04:55:17.000Z","dateMiliSeconds":1766379317000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"cargo-coupling: Visualizing Coupling in Rust Projects","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/21/152559","contentSnippet":"cargo-coupling Web UI - Self-diagnosis viewIntroduction\"I really don't want to touch this module...\"If you've been developing software long enough, you know this feeling. Every change breaks something else. Tests are painful to write. Understanding what the code even does feels impossible. These symptoms share a common root cause: modules that depend too heavily on each other—the problem of coupling.Coupling problems are insidious. They're hard to notice while you're writing code, only revealing themselves later when you wonder why changes are so difficult. What makes it worse is that even when you know \"coupling is too tight,\" it's hard to see exactly where and how, or where to start fixing it.Looking back, I realize my understanding of coupling was quite shallow. I was making judgments based on vague feelings—\"this seems tightly coupled\" or \"loose coupling is supposedly better\"—but when I tried to articulate why, I couldn't explain it clearly.To address this lack of visibility, we need a way to measure coupling. But the traditional single axis of \"strong vs. weak\" isn't enough. The same \"strong coupling\" means different things depending on where it occurs and in what context.This brings us to Vlad Khononov's concept of \"Balanced Coupling.\" It's a framework that evaluates coupling across three dimensions: strength, distance, and volatility, then assesses their balance. cargo-coupling is a tool I developed to implement this framework for Rust projects.Even as AI writes more of our code, this coupling metric becomes increasingly important. Regardless of who or what writes the code, humans still need to understand, maintain, and extend it. In fact, precisely because AI generates code, we need objective measures to evaluate its structure.Let's start with an overview of the tool, then explore the underlying concepts, and finally see how to use it in practice.What is cargo-coupling?cargo-coupling is a coupling analysis tool I developed for Rust projects.The inspiration came from Vlad Khononov's book \"Balancing Coupling in Software Design.\" The challenges I had vaguely sensed about coupling design were systematically organized in this book. I was impressed by the framework that captures coupling through three dimensions—strength, distance, and volatility—and wanted to create a tool that makes this practical for Rust projects. I highly recommend picking up the book.The tool is available on GitHub. If you find it useful, I'd appreciate a star!GitHub:github.comcrates.io: https://crates.io/crates/cargo-couplingNow, let's challenge a common assumption.\"Coupling should be minimized\"—isn't that what you believe?This tool doesn't aim to \"reduce coupling.\" It aims to \"design coupling appropriately.\" Why? Because coupling isn't inherently bad. Related functionality working closely together is natural. The problem is \"strong coupling in inappropriate places\" or \"tight coupling between distant modules.\" This shift in perspective is at the heart of this tool.# Installationcargo install cargo-coupling# Basic usagecargo coupling ./srcAnalyzing Coupling Across Three DimensionsSo what exactly constitutes \"appropriate coupling\"?Traditional coupling analysis tends to think in terms of a single \"strong/weak\" axis. But stop and consider: strong coupling with an adjacent module versus strong coupling with a distant external library—shouldn't these mean different things? And coupling with code unchanged for five years versus coupling with code modified weekly—shouldn't these carry different risks?A single axis can't capture these differences. cargo-coupling measures coupling across three independent dimensions.1. Integration StrengthThe first dimension is \"coupling strength\"—how much modules know about each other's internals.Have you seen code like user.password_hash that directly accesses struct fields? That's the strongest form of coupling. Meanwhile, code that interacts through impl Trait works without knowing the other's internals. This difference gets quantified as a score. Level  Score  Description  Rust Example  Intrusive  1.00  Direct dependency on internal implementation  struct.field direct access  Functional  0.75  Dependency on function signatures  Method calls  Model  0.50  Dependency on data structures  Type definitions, type parameters  Contract  0.25  Interface/trait only  impl Trait 2. DistanceThe second dimension is \"distance\"—how far apart coupled modules are in the code's scope hierarchy.Functions within the same file working closely together is natural. But what if src/auth/login.rs directly references src/billing/invoice.rs? Or worse, depends on an external crate's internal structure? The farther the distance, the \"heavier\" that coupling becomes. Level  Score  Description  SameModule  0.25  Within the same file/module  DifferentModule  0.50  Different module in the same crate  DifferentCrate  1.00  External crate dependency 3. VolatilityThe third dimension is \"volatility\"—how frequently the code changes.Your project surely has stable modules untouched for over a year alongside modules modified weekly. Depending on stable code versus frequently changing code carries different risks. cargo-coupling automatically calculates this volatility from Git history. Level  Score  Changes in 6-month Git history  Low  0.00  0-2 changes  Medium  0.50  3-10 changes  High  1.00  11+ changes Calculating the Balance ScoreWe've covered the three dimensions. But if you're told \"strength is 0.75,\" \"distance is 0.50,\" \"volatility is medium\"—how do you judge whether this coupling is actually good or bad?cargo-coupling combines these three dimensions into a balance score. By consolidating three numbers into one score, you can intuitively assess whether coupling is appropriate.The concept is simple: multiply \"strength-distance balance\" by \"volatility risk.\"ALIGNMENT = 1.0 - |STRENGTH - (1.0 - DISTANCE)|VOLATILITY_IMPACT = 1.0 - (VOLATILITY × STRENGTH)BALANCE_SCORE = ALIGNMENT × VOLATILITY_IMPACTThe first formula measures whether strength and distance are proportionate. Close distance can tolerate strong coupling; far distance should mean weak coupling. The second formula measures the combined risk of change frequency and coupling strength. Strong coupling with frequently changing code means higher risk of being affected by every change.The conclusions this formula leads to:Strong coupling + Close distance → Good: High cohesion with related functionality in one moduleWeak coupling + Far distance → Good: Loose coupling architecture with minimal inter-module dependenciesStrong coupling + Far distance → Bad: Global complexity where changes affect wide areasStrong coupling + High volatility → Bad: Change propagation risk where frequent changes cascadePractical UsageNow that we understand the theory, let's see how to use it on real projects. cargo-coupling offers multiple output formats depending on your needs.Summary Displaycargo coupling --summary ./srcExample output:Coupling Analysis Summary:  Health Grade: B (Good)  Files: 14  Modules: 14  Couplings: 389  Balance Score: 0.83  Issues:    Medium: 2  Top Priority:    - [Medium] cargo-coupling::main → 21 dependencies    - [Medium] 21 dependents → cargo-coupling::cargo_coupling  Breakdown:    Internal: 33    External: 356    Balanced: 33    Needs Review: 0    Needs Refactoring: 0  Connascence:    Total: 807 (avg strength: 0.23)    High-strength: Position=2, Algorithm=2  APOSD Metrics:    Pass-Through Methods: 12 (simple delegation)    High Cognitive Load: 2 modules    Avg Module Depth: 7.9Hotspot AnalysisIdentify high-priority modules that need refactoring.cargo coupling --hotspots ./src#1 my-project::main (Score: 55)   🟡 Medium: High Efferent Coupling   💡 What it means:      This module depends on too many other modules   ⚠️  Why it's a problem:      • Changes elsewhere may break this module      • Testing requires many mocks/stubs      • Hard to understand in isolation   🔧 How to fix:      Split into smaller modules with clear responsibilities      e.g., Split main.rs into cli.rs, config.rs, runner.rsImpact AnalysisExamine the impact scope when changing a specific module.cargo coupling --impact metrics ./srcWeb UI VisualizationVisualize coupling relationships with an interactive graph.cargo coupling --web ./srcA browser opens automatically, displaying an interactive graph using Cytoscape.js. Click nodes to see detailed information; problematic modules are color-coded.CI/CD IntegrationBeyond manual analysis, you can continuously monitor quality. Incorporating cargo-coupling as a quality gate enables early detection of coupling design degradation.cargo coupling --check \\  --min-grade=B \\  --max-circular=0 \\  ./srcGitHub Actions example:- name: Check coupling health  run: |    cargo coupling --check \\      --min-grade=B \\      --max-critical=0 \\      ./srcReturns exit code 1 when the grade falls below the threshold, making it easy to integrate into CI pipelines.AI IntegrationWhen using with Claude Code or GitHub Copilot, the --ai option is convenient.cargo coupling --ai ./srcOutput is formatted in an AI-friendly way, so you can paste it directly into AI tools to get refactoring suggestions.Detected Problem PatternsHaving covered usage, you might wonder what specific problems get detected. Here are the representative patterns cargo-coupling warns about.God ModuleA module with too many functions, types, or impls.Functions: 30+Types: 15+Impls: 20+High Efferent CouplingA module with too many dependencies. Default threshold is 20+ dependencies.High Afferent CouplingA module depended on by too many others. Default threshold is 30+ dependents.Cascading Change RiskThe combination of intrusive coupling and high volatility. A dangerous state where changes propagate across wide areas.Interpreting Health GradesDetection results are ultimately consolidated into a single grade representing overall project health. Grade  Description  S  Over-optimized. Might be over-refactored  A  Well-balanced. Ideal state  B  Healthy. Manageable condition  C  Room for improvement  D  Attention needed  F  Immediate action required Interestingly, S grade is considered \"overdone.\" Why?Reducing coupling too much fragments code excessively, making the big picture harder to see. Have you experienced needing to open 10 files to trace a single operation, or getting lost in abstraction layers so deep you wonder \"what does this actually do?\"Coupling isn't simply \"less is better.\" Balance is key.Library UsageBeyond the CLI tool, you can embed it in your own tools. cargo-coupling is also published as a library, allowing you to call analysis functions directly from code.use cargo_coupling::{    analyze_workspace,    analyze_project_balance_with_thresholds,    IssueThresholds,    VolatilityAnalyzer,};fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {    // AST analysis    let mut metrics = analyze_workspace(Path::new(\"./src\"))?;    // Git volatility analysis    let mut volatility = VolatilityAnalyzer::new(6);    volatility.analyze(Path::new(\"./src\"))?;    metrics.file_changes = volatility.file_changes;    metrics.update_volatility_from_git();    // Balance analysis    let report = analyze_project_balance_with_thresholds(        \u0026metrics,        \u0026IssueThresholds::default()    );    println!(\"Grade: {}\", report.health_grade);    Ok(())}Performancecargo-coupling is designed to run fast even on large projects.Parallel AST analysis with RayonStream processing of Git historyBenchmarks: 655ms on tokio (488 files)Use the --no-git option to skip Git analysis for even faster operation.LimitationsWhile useful, this tool isn't omnipotent. Know these limitations before using it.External crate dependencies aren't analyzed: Dependencies on serde, tokio, etc. aren't analyzed since developers can't control themStatic analysis only: Runtime behavior and macro expansion aren't fully capturedGit history required: Volatility analysis needs Git history. Short history reduces accuracyConclusioncargo-coupling provides a practical approach of \"choosing appropriate coupling\" rather than the simplistic view that \"coupling is bad.\"3-dimensional analysis: Considers strength, distance, and volatility simultaneouslyGit integration: Reflects actual change frequency as dataActionable suggestions: Presents concrete refactoring actionsMultiple output formats: Text/JSON/Web UI/AI-friendlyCI/CD integration: Automated checks as quality gatesYou don't need perfect design. With a pragmatic attitude that \"80% improvement is enough,\" gradually improve your project's health.# Try it outcargo install cargo-couplingcargo coupling --summary ./srcJust visualizing coupling problems is the first step toward better design.The next time you feel \"I really don't want to touch this module...\"—that's no longer a vague anxiety. It's a tractable challenge you can analyze across three dimensions of strength, distance, and volatility, and translate into concrete improvement actions. That feeling isn't something to fear; it's the entry point to improvement.A related concept is \"Complexity\" from John Ousterhout's \"A Philosophy of Software Design.\" It offers another valuable perspective and is well worth reading.","isoDate":"2025-12-21T06:25:59.000Z","dateMiliSeconds":1766298359000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":" おい、休め","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/21/092456","contentSnippet":"はじめに金曜日の夜、ベッドの上でこの文章を書き始めている。先週の土日は何をしていたかと聞かれたら、たぶん「寝てた」と答える。嘘ではない。ベッドにいた時間は長かった。ただ、眠っていたかというと怪しい。スマホを持ったまま横になって、気づいたら夕方だった。そういう二日間だった。休んだのか、と聞かれると困る。体は動かしていない。仕事もしていない。だから休んだと言えば休んだのだろう。でも、回復したかというと、していない。月曜の朝を迎える自分は、金曜の夜の自分より確実に疲れている。 何もしていないのに。何もしていないから、かもしれない。30歳になった。エンジニアとして働いている。在宅勤務というやつだ。2025年、AIエージェントが当たり前になった時代を生きている。AIは文句を言わない。疲れたとも言わない。24時間動ける。私にはそれができない。コーヒーがないと朝は動けないし、金曜の午後は集中力が死んでいる。土日は「充電」と称してベッドに沈んでいる。それでも充電されない。この一年、ずっとそうだった。ある日、気づいたことがある。私は「休んでいる」んじゃなくて、「動けなくなっている」だけだった。充電じゃなくて、バッテリー切れの放置だった。「休んでいるのに休めていない」とは、「休んでいるのに休めていない」。変な問答である。矛盾しているように聞こえるが、多くの人がこの感覚を知っていると思う。週末を過ごしたはずなのに、月曜日の朝に疲れが残っている。肉体的には、労働的には、確かに「活動していない」。仕事をしていない。オフィスにいない。だから「休んでいる」となんとなく認識する。しかし脳は、休んでいない。ベッドに横になりながらスマホを見ている時、目は画面を追い、脳は情報を処理し、感情は刺激に反応し続けている。通知が来るたびに注意が引かれる。SNSのタイムラインをスクロールするたびに、微小な判断が積み重なる。「これは読む価値があるか」「これにいいねするか」「これに反応すべきか」。身体は止まっているが、脳は回り続けている。これが「休んでいるのに休めていない」の正体だ。情報を入れ続けると、脳は整理する暇がない。食べ続けて消化できない胃のように、頭がパンク状態になる。入力過多で、整理が追いついていない。なぜ本人は「休んでいるつもり」になってしまうのか厄介なのは、本人が気づいていないことだ。私もそうだった。「横になっている＝休んでいる」。この等式が、骨の髄まで染み込んでいる。かつて「休む」とは、物理的に動かないことを意味した。畑仕事を終えて家に帰り、座って何もしない。工場での労働を終えて、ソファに横になる。肉体労働の時代には、「動かない = 休息」という等式が成り立っていた。しかし現代のデスクワーク的な仕事は、主に脳を使う。特にエンジニアは、一日中座っている。肉体は動いていない。だから「仕事 = 動くこと」という図式が崩れている。そして「休息 = 動かないこと」という古い図式をそのまま適用すると、「横になってスマホを見ること」も休息にカウントされてしまう。肉体的には動いていないのだから。でも実際には、脳は仕事中と同じかそれ以上に動いている。休息の定義を更新する必要がある。現代において「休む」とは、脳への入力を減らすことだ。物理的な動きの有無ではなく、認知的な負荷の有無で判断すべきなのだ。この状態を言語化できないと、何がさらに悪化するのか「休んでいるのに休めていない」という感覚を言葉にできないと、さらに深刻な問題が起きる。まず、自己診断を誤る。「十分休んでいるはずなのに疲れている。だから自分は病気かもしれない」「自分は人より弱いのかもしれない」。実際には休息の質の問題なのに、自分の身体や精神に問題があると思い込んでしまう。次に、対処法を間違える。「もっと休めばいい」と考えて、さらに長時間ベッドでスマホを見る時間を増やす。これは逆効果だ。質の悪い休息を量で補おうとしても、回復はしない。そして最も深刻なのは、周囲に理解されないことだ。「週末何してたの？」「ずっと寝てた」「じゃあ休めたね」。この会話で、問題は見えなくなる。本人も「確かに休んだはずだ」と思い込み、周囲も「休んだのだから元気なはずだ」と期待する。「休んだ」という事実と、「休めた」という実感の乖離。これが現代の休息における新しい病だ。言語化できない問題は、解決できない。 だからまず、この状態に名前をつけることが重要だ。「偽りの休息」「見せかけの休息」「脳が休まらない休息」。何でもいい。言葉にすることで、初めて問題として認識できる。AIエージェント時代の疲労2025年、AIエージェントが本格的に動き始めた。Claude Code、Devin、Cursor Agent。これらは単なるツールではない。私たちと同じように考え、判断し、実行する存在になった。コードを書くだけでなく、何を書くべきかを考える。指示を待つだけでなく、自ら次のステップを提案する。この変化は、エンジニアの疲労の質を根本から変えた。AIは無限に働ける。私たちは有限だ。AIエージェントは疲れない。朝も夜も関係ない。週末も祝日も関係ない。感情の浮き沈みもない。モチベーションの低下もない。常に一定のパフォーマンスで、無限に働き続ける。私たちは、そうではない。8時間働けば疲れる。集中力は25分で途切れる。昼食を食べすぎると眠くなる。金曜日の午後は効率が落ちる。睡眠不足の翌日は判断を誤る。感情に左右される。体調に左右される。天気にすら左右される。この対比が、2025年の疲労を特殊なものにしている。かつて、比較対象は同僚だった。隣の席のエンジニアより速くコードを書けるか。チームの中で自分はどの位置にいるか。人間同士の比較だった。今、比較対象にAIが加わった。AIエージェントが一晩で書いたコードを見て、「自分が一週間かかることを、一晩でやった」と思う。AIが瞬時に出した答えを見て、「自分が一時間悩んだことを、数秒で解決した」と思う。無限と有限を比較している。もちろん、話はそう単純じゃない。AIにも限界がある。文脈を読み違える。ハルシネーションを起こす。「それっぽい嘘」を自信満々に言う。コードレビューなしでマージしたら、後で痛い目に遭う。AIが「無限に働ける」のは事実だが、「無限に正しい」わけではない。でも、そんなことは分かっている。分かっていても、比較してしまう。「比較しなければいい」と思ったこともある。でも、環境がそれを許さなかった。同じSlackチャンネルに、自分が1日かけて作ったPRと、AIが1時間で作ったPRが並んでいる。見た瞬間に、脳が勝手に比較する。「あっちの方が速い」と。そう思った時点で、もう比較している。これは意志の問題じゃない。環境の問題だ。同じ画面に並んで表示されている限り、見比べてしまう。見比べれば、負ける。負ければ、「自分は遅い」「自分は非効率だ」「自分は価値がない」と感じる。この感覚が、静かに、確実に、私たちを消耗させている。AIによって増えたのは「作業量」だけではなく、AIエージェントを使うと、作業は速くなる。コードの生成、ドキュメントの作成、調査の実行。これらは確かに効率化される。しかし、楽にはならない。夕方になると、頭が重い。コードを書く時間は減った。でも疲労感は増えている。増えたのは作業量じゃない。判断の回数だ。AIエージェントは大量の選択肢を提示する。コードの候補を10個出す。アプローチを5つ提案する。修正案を複数示す。これらを評価し、選択し、修正し、採用するかどうかを決めるのは人間だ。従来の仕事では、一つのタスクに対して一つの判断があった。自分で作るから、作成と判断が一体化していた。AIを使うと、この構造が変わる。AIが10個の選択肢を提示する。人間は10個を評価し、1つを選ぶ。あるいは「どれも違う」と判断して再生成を指示する。今度は別の10個が出てくる。また評価する。選ぶ。修正を指示する。一つのタスクに対して、判断の回数が爆発的に増える。思い当たることがある。午前中は「これがいい」「あれはダメ」とサクサク判断できる。でも夕方になると、どれを選んでいいか分からなくなる。「どれでもいいから決めてくれ」と思う。頭が重くなって、判断を先延ばしにしたくなる。これは、判断そのものに消耗があるということだ。人間が一日に下せる質の高い判断の数には限りがある。 判断を重ねるほど、後の判断の質は落ちる。AIは判断を代行してくれない。むしろ、判断すべき選択肢を増やす。だから、作業時間が減っても、認知的な消耗は増える。「便利になった」という感覚と、「楽になった」という現実は、必ずしも一致しない。「速くなった」と感じるのに、疲れは減らない。この乖離は危険だ。「速くなっている」と思い込んでいる限り、「なぜ疲れるのか」という問いにたどり着けない。AI疲れはスキル不足の問題ではないAI疲れを感じたとき、多くの人はこう考える。「自分のスキルが足りないからだ」「もっとAIを使いこなせるようになれば楽になる」。正直に言うと、私もそう思っていた。だから毎晩、新しいツールを試し、プロンプトを改善し、ワークフローを最適化した。でも楽にはならなかった。むしろ疲れた。ただ、ここで立ち止まって考えたい。私はこう思うようになった。それは違うんじゃないか、と。確かに、AIツールの使い方には習熟曲線がある。最初は戸惑う。慣れれば効率が上がる。しかし、ある程度習熟した後も、判断疲れは消えない。むしろ、AIを使いこなせるようになるほど、使う頻度が上がり、判断の回数も増える。AI疲れ ≠ スキル不足。AI疲れ = 判断疲れだ。 あなたが下手なんじゃない。ゲームのルールが変わったのだ。AIは人間の判断を代行しない。判断の対象を増やす。この構造的な問題は、スキルアップでは解決しない。解決策は、使い方を変えることだ。AIに全てを任せるのではなく、判断の負荷が高い場面では意識的に使わない。あるいは、AIの出力をそのまま採用する覚悟で使う（評価・修正のループを断ち切る）。しかしこれは、「AIを使いこなす」という文脈では語られない。だから多くのエンジニアは、スキル不足を疑い、さらに学習し、さらに使い、さらに疲れる。AI疲れの原因を正しく特定して認識することが、回復への第一歩だ。「恥」という名の監視システムエンジニアには、独特の恥の文化がある。Xを開く。誰かが「今週読んだ技術書3冊」と投稿している。誰かが「個人開発で新しいフレームワークを試した」と書いている。GitHubの草が青々と茂っている。日曜日の夜に。その瞬間、土日に何もしなかった自分が恥ずかしくなる。「エンジニアは勉強し続けなければならない」。これは真実だ。技術は進化する。学ばなければ置いていかれる。それは分かっている。でも、いつからか「土日に勉強するのが当たり前」になった。休日に技術書を読まないと不安になる。個人開発をしていないと焦る。Qiitaに何も投稿していない月があると、自分の価値が下がった気がする。私たちは「恥」に操られている。恥は、外部から強制されるものではない。誰かに「勉強しろ」と言われているわけではない。上司が土日の学習を義務付けているわけでもない。自分で自分を監視している。 SNSで他人の「充実した週末」を見て、勝手に比較して、勝手に恥じて、勝手に休めなくなっている。これが最も効率的な搾取システムだ。命令する必要がない。監視する必要がない。本人が勝手に自分を追い詰めてくれる。見せかけの「学習」が休息を奪う問題は、この「恥を避けるための学習」が、本当の意味での学習になっていないことだ。土曜日の朝、罪悪感から技術書を開く。でも頭に入ってこない。疲れているから。ページをめくるけど、内容が定着しない。それでも「読んだ」という事実が欲しくて、最後までめくる。これは学習ではない。休息でもない。どちらでもない時間だ。本当に学びたいときの読書と、恥を避けるための読書は、まったく別物だ。前者は楽しい。後者は苦痛だ。前者は定着する。後者は忘れる。恥を避けるために費やした土日は、学習にも休息にもならない。最悪の投資だ。 時間を使って、何も得られず、回復もしない。SNSで他人の土日を見るな。あれは広告だ。冷静に考えてほしい。Xに投稿される「充実した週末」は、全員の週末の平均ではない。投稿したくなるような週末だけが投稿される。何もしなかった週末は投稿されない。つまり、タイムラインに流れてくるのは、全エンジニアの「最も充実した瞬間」の集合体だ。それを自分の「普通の週末」と比較している。勝てるわけがない。他人の土日は広告だ。 広告と自分を比較して落ち込むのは、モデルの写真を見て自分の顔を恥じるのと同じだ。フィルターがかかっている。編集されている。現実ではない。恥を手放すことは、怠惰ではない「じゃあ勉強しなくていいのか」と思うかもしれない。そうではない。学びたいときに学べばいい。休みたいときに休めばいい。恥に駆動されるのをやめろ、と言っている。恥から学習すると、燃え尽きる。好奇心から学習すると、続く。この違いは大きい。土日に何もしなかった自分を、責めなくていい。月曜日に元気に働けるなら、それが正解だ。GitHubの草が生えていなくても、あなたの価値は変わらない。恥は、休息の最大の敵だ。 そして恥は、自分で自分にかけている呪いだ。縛りである。しかし、呪いは、気づいた瞬間に弱くなる。注意力が商品化されるとは、人生に何が起きることなのかふと考えた。なぜ、こんなに疲れているのか。答えの一つは、私たちの注意力が「商品」として売買されているということだ。スマホを開く。通知が来る。タップする。広告が表示される。スクロールする。また通知が来る。この一連の行動の中で、私たちの「注意力」は企業に売り渡されている。そして企業はその注意力を広告主に売る。注意力が奪われることは、なぜ「時間」以上の損失なのか「時間が奪われている」という表現は、まだ甘い。時間は、失っても取り戻せる可能性がある。今日の2時間を失っても、明日の2時間で何かができる。少なくとも、時間は均質に見える。しかし注意力は違う。注意力とは、「今この瞬間に何を経験するか」を決める力だ。何に注意を向けるかが、何を経験するかを決める。何を経験するかが、何を記憶するかを決める。何を記憶するかが、自分が誰であるかを決める。つまり、注意力を奪われることは、経験を奪われることであり、記憶を奪われることであり、最終的にはアイデンティティを奪われることだ。2時間スマホをスクロールして過ごした後、何が残っているか。私の場合、ほとんど何も覚えていない。「何を見てたっけ」と思い返しても、断片的な画像がぼんやり浮かぶだけ。時間は確かに過ぎた。でも経験は残っていない。一方で、友人と2時間話した後は違う。「あの話、面白かったな」「あのとき笑ったな」と、具体的な場面が残っている。同じ2時間でも、記憶への定着度がまるで違う。スマホを見ることも、一応は「経験」だ。でも、受け身で流れてくる情報を処理するだけの経験と、自分で選んだ活動に没頭する経験では、残り方が違う。受け身の時間は、砂に書いた文字のように消えていく。時間泥棒ではなく、人生泥棒だ。なぜ人は自分の注意力の価値に無自覚なのか注意力は、意識しないと見えない。お金は数字として見える。時間は時計として見える。しかし注意力は、どこにも可視化されていない。そして注意力は、「使っている」という感覚がない。お金を使うとき、財布が軽くなる感覚がある。時間を使うとき、時計が進む感覚がある。しかし注意力を使うとき、何かが減っていく感覚は薄い。ただ、気づいたら疲れている。さらに問題なのは、注意力を奪う側が、その事実を隠すインセンティブを持っていることだ。SNSは「つながり」を売り物にする。「あなたの大切な人とつながるためのツール」。しかし実際には、あなたの注意力を広告主に売るためのツールだ。この真実は、マーケティングでは語られない。だから私たちは、自分の注意力が商品になっていることに気づかない。気づかないまま、どんどん売り渡していく。この構造に気づいても、人はなぜ抗えないのか気づいても、抗えない。これが最も絶望的な部分だ。理由の一つは、脳の報酬系がハックされているからだ。通知が来る。ドーパミンが出る。確認する。また通知が来る。この「不定期な報酬」は、脳にとって最も中毒性が高い。スロットマシンと同じ原理だ。いつ当たるか分からないから、ずっと引き続けてしまう。理由のもう一つは、社会的なプレッシャーだ。みんなが使っている。使わないと取り残される。返信しないと失礼。既読をつけないと心配される。SNSから離れることは、社会から離れることのように感じられる。そして最後の理由は、代替手段がないことだ。仕事の連絡もスマホで来る。友人との約束もスマホで確認する。情報収集もスマホでする。スマホを捨てることは、現代社会で生きることを諦めることに近い。構造的な問題には、個人の意志力だけでは対抗できない。だからこそ、意識的な「デジタルデトックス」が必要になる。完全に離れることはできなくても、時間を区切って距離を取る。それが、今できる最大の抵抗だ。オンライン会議は、なぜ「効率的なのに疲れる」のかスマホから注意を奪われるだけではない。在宅勤務の日常には、もう一つの消耗源がある。オンライン会議だ。最初はただ素晴らしいと思った。移動時間がない。どこからでも参加できる。効率的だ、と。でも二年、三年と続けるうちに、何かがおかしいと気づいた。ある日、オンライン会議が5本続いた後、私は何も考えられなくなっていた。画面を閉じても、頭の中がぼんやりしている。簡単なメールすら書けない。対面で5本会議しても、こんなに消耗しなかった。何かが違う。対面では無意識に処理していた情報とは何か対面のコミュニケーションでは、膨大な情報が交換されている。言葉だけではない。表情、視線、姿勢、身振り、声のトーン、間の取り方、呼吸のリズム、空間的な距離感。これらの非言語情報が、コミュニケーションの大部分を占めている。そして重要なのは、これらの情報を無意識に処理しているということだ。対面で話しているとき、相手の表情を「分析」しているわけではない。自然と読み取っている。相手が不快そうなら、無意識に話し方を変える。相手が興味を持っていそうなら、無意識に詳しく説明する。この調整は、意識的な努力なしに行われている。オンライン会議では、この無意識の処理が機能しなくなる。画面越しでは、表情が見えにくい。解像度が低い。タイムラグがある。視線が合わない（カメラを見ると相手の目を見られない）。空間的な距離感がない。全員が同じサイズで画面に並んでいる。無意識に処理できていた情報を、意識的に処理しなければならなくなる。「この人は今、何を考えているのだろう」「この沈黙は同意なのか、困惑なのか」「自分の話は伝わっているのか」。対面なら自動的に分かることが、オンラインでは分からない。だから脳がフル回転して、推測し、分析し、判断する。これが、オンライン会議の疲労の正体だ。さらに、自分の顔が常に画面に映っている。鏡を見ながら会話しているようなもの。音声も微妙に不完全で、脳は余計な労力を使う。同じ1時間でも、処理している情報の密度が違う。だから疲れる。私はこの疲労を個人の問題だと思っていた。でも違った。組織の設計そのものが、この疲労を生み出している。振り返ると、非同期のコミュニケーションで済むことを、わざわざ会議で行っていた。私自身、「対話が必要な場面」に限定することで、オンライン会議の負荷を減らせた。ある日、仕事を一つ担当から外してもらった。「これ、ちょっと抱えすぎてます」と正直に言った。その週、少しだけ頭がクリアだった。「手放してもいい」と思えた瞬間だった。境界線が消えたとき、人間の回復機構はどう壊れるのか在宅勤務で最も失われたもの。それは「境界線」だ。帰りたいのに家に居る。オフィスに通っていた頃は、自然と境界線があった。家を出る。通勤する。オフィスに着く。仕事モードになる。仕事が終わる。オフィスを出る。通勤する。家に着く。オフモードになる。この物理的な移動が、心理的な切り替えを助けていた。在宅勤務では、その境界線が消えた。起きたらすぐに仕事。寝る直前まで仕事。仕事部屋と寝室が同じ。リビングがオフィス。どこでも働ける = どこにも逃げ場がない。物理的な移動は、なぜ心理的切り替えに効いていたのか通勤を嫌う人は多い。満員電車。渋滞。時間の無駄。その通りだ。しかし通勤には、見えない機能があった。通勤は「儀式」だった。人間の脳は、儀式を通じて状態を切り替える。朝のルーティン、食事の作法、寝る前の習慣。これらの儀式が、脳に「次のモードに入る」というシグナルを送る。通勤は、最も強力な儀式の一つだった。家という空間を離れ、別の空間に移動する。その過程で、脳は自然と「仕事モード」に切り替わっていた。帰宅時には逆のプロセスが起きていた。この儀式が消えると、脳は切り替えのタイミングを失う。「いつ仕事を始めるべきか」「いつ仕事を終えるべきか」が曖昧になる。そして気づけば、常に「なんとなく仕事モード」で過ごすことになる。常に仕事モードということは、常に回復モードに入れないということだ。境界線がない働き方は、どんな人に特に危険か特に危険なのは、責任感が強い人と仕事が好きな人だ。「まだできることがある」と思うと止められない。楽しいから止められない。境界線がないと、いつまでも「まだやれる」と思ってしまう。個人の工夫と、その限界着替える。仕事着から部屋着に。あいさつを声に出す。「お疲れ様でした」。これらの小さな儀式が、切り替えを助ける。しかし、限界がある。本来、境界線は環境によって与えられていた。それを個人の意志で維持し続けることは、それ自体が消耗を伴う。だから、環境そのものを変える必要がある。 仕事専用の部屋を作る。コワーキングスペースを使う。PCを別の部屋に置く。物理的に「できない」状態を作る。意志力に頼らない仕組みを作ること。それが、境界線を維持する現実的な方法だ。「疲れた」と感じるとき、本当に疲れているのはどこか「疲れた」と口にする。でも、どこが疲れているのか、自分でも分かっていない。疲れには三つの種類がある。「自律神経の疲れ」。自律神経とは、意識しなくても働く神経システムだ。活動モードを司る交感神経（心拍を上げ、筋肉を緊張させる）と、休息モードを司る副交感神経（心拍を下げ、消化を促す）がある。この二つのバランスが崩れている状態。常に緊張している。リラックスできない。眠れない。朝起きても疲れが取れない。「心の疲れ」。精神的な消耗。ストレス。不安。焦り。人間関係の疲れ。感情労働による消耗。「体の疲れ」。筋肉の疲労。運動不足による倦怠感。同じ姿勢での身体の凝り。自律神経・心・身体のどれが最初に壊れやすいのかこれは個人差があるが、現代のエンジニアにとって、最初に壊れやすいのは自律神経だ。理由は、自律神経の疲労が最も気づきにくいからだ。身体の疲れは分かりやすい。筋肉痛がある。だるさがある。明確な感覚として認識できる。心の疲れも、ある程度は分かる。「イライラする」「落ち込む」「やる気が出ない」。感情として表れる。しかし自律神経の疲れは、症状が曖昧だ。「なんとなく調子が悪い」「眠れない」「食欲がない」「息苦しい」。これらの症状は、他の原因でも起きる。だから「自律神経が疲れている」とは認識されにくい。そして気づかないまま酷使し続けると、ある日突然、限界を超える。動悸がする。めまいがする。パニック発作が起きる。ここまで来て初めて「何かがおかしい」と気づく。自律神経は悲鳴を上げない。気づいたときには、もう限界を超えている。なぜ現代のエンジニアは三重苦に陥りやすいのか問題は、これらが複雑に絡み合っていることだ。長時間のデスクワークで体が疲れる。動かないから血流が滞り、肩が凝り、腰が痛くなる。AIへのキャッチアップ、締め切りのプレッシャー、評価への不安で心が疲れる。オンライン会議の連続、境界線のない働き方、常時接続のプレッシャーで自律神経が疲れる。これらは独立していない。相互に影響し合う。身体が疲れると、心も疲れやすくなる。運動不足はうつ病のリスクを高める。心が疲れると、自律神経が乱れる。ストレスは交感神経を活性化させる。自律神経が乱れると、身体の回復力が落ちる。悪循環のスパイラル。一つの疲れが、他の二つを引き起こし、それがまた最初の疲れを悪化させる。このスパイラルに入ると、自力で抜け出すのは難しい。疲れを誤診すると、どんな「間違った休み」を選ぶのか疲れの種類を見極めずに休もうとすると、的外れな対処をしてしまう。身体が疲れているのに、心の休息を取ろうとする。例えば、運動不足で身体が固まっているのに、マッサージに行ったり、リラクゼーション音楽を聴いたりする。これは悪くないが、根本解決にならない。必要なのは軽い運動だ。心が疲れているのに、身体の休息を取ろうとする。例えば、人間関係のストレスで消耗しているのに、ひたすら寝ようとする。眠れない。眠れても回復しない。必要なのは、安全な場所で感情を吐き出すことだ。自律神経が疲れているのに、刺激で気分転換しようとする。例えば、交感神経が過剰に活性化しているのに、アクション映画を観たり、激しいゲームをしたりする。一時的に気が紛れても、神経はさらに疲弊する。必要なのは、静かな環境でぼんやりすることだ。自分の疲れの種類を見極めること。それが、正しく休むための第一歩だ。身体がシャットダウンする「動けなさ」は、怠惰と何が違うのかベッドから起き上がれない朝がある。やるべきことは分かっている。でも体が動かない。これは怠けているのか。それとも、何か別のことが起きているのか。自分の「動けなさ」について考えていくうちに、気づいたことがある。怠惰と「動けなさ」は、外から見ると同じに見える。でも中身はまったく違う。怠惰は「やる気がない」状態だ。やろうと思えばできる。でもやりたくない。シャットダウンは「動けない」状態だ。やろうと思っても、身体が言うことを聞かない。脳が「これ以上は危険だ」と判断して、強制的にブレーキをかけている。これは生理的な反応だ。動物が捕食者に捕まったとき、最後の防衛反応として「死んだふり」をすることがある。身体を動かなくすることで、エネルギーを温存する。人間も同じメカニズムを持っている。ストレスが大きすぎて、闘うことも逃げることもできないとき、身体がシャットダウンする。社会はなぜこの状態を「甘え」と誤認するのか問題は、シャットダウン状態が外から見ると「怠けている」ように見えることだ。ベッドから起き上がれない。仕事に行けない。何もする気力がない。社会は、これを「意志の問題」として捉えがちだ。「頑張れば動ける」「やる気がないだけ」「甘えている」。しかし、これは生理的な反応だ。動物が捕食者に捕まったとき、最後の防衛反応として「死んだふり」をすることがある。これがシャットダウン反応だ。身体を動かなくすることで、エネルギーを温存し、捕食者の関心を逸らす。人間も同じメカニズムを持っている。ストレスが大きすぎて、闘うことも逃げることもできないとき、身体がシャットダウンする。これは意志の問題ではない。脳が「これ以上は危険だ」と判断して、強制的に止めているのだ。怠惰との違いは明確だ。怠惰は「やる気がない」状態。やろうと思えばできる。シャットダウン状態は「動けない」状態。やろうと思っても、身体が言うことを聞かない。この区別ができないと、本人も周囲も対応を間違える。本人が自分を責めることで、状態はどう固定化されるのか最も危険なのは、本人が自分を責めることだ。「動けないのは自分が怠けているからだ」「意志が弱いからだ」「努力が足りないからだ」。この自己批判が、状態をさらに悪化させる。自己批判はストレスを生む。ストレスは交感神経を活性化させる。しかし、すでに疲弊した身体は交感神経の活性化に耐えられない。だから、また身体がシャットダウンする。「動けない → 自分を責める → ストレス増加 → さらに動けなくなる → さらに自分を責める」この悪循環が、状態を固定化する。回復するためには、この循環を断ち切る必要がある。そのためにはまず、「動けないのは意志の問題ではない」と理解することが重要だ。 自分を責めることをやめる。これが、回復への第一歩だ。この凍結状態から抜けるには、何が最初の一歩になるのかシャットダウン状態から抜け出すのは、簡単ではない。「頑張って動く」というアプローチは逆効果になりうる。有効なのは、身体への穏やかなアプローチだ。まず、安全を感じること。物理的に安全な場所にいる。誰にも批判されない。時間的なプレッシャーがない。この「安全の感覚」が、安心・つながりモードを呼び起こす。次に、身体を少しだけ動かすこと。激しい運動ではない。深呼吸。ゆっくりとしたストレッチ。5分の散歩。これらの穏やかな動きが、身体に「動いても大丈夫だ」というシグナルを送る。そして、人とのつながり。信頼できる人との会話。これらの社会的なつながりが、安心・つながりモードを呼び起こす。重要なのは、「頑張る」のではなく「許す」ことだ。動けない自分を責めない。ゆっくり回復することを許す。無理に何かを達成しようとしない。このスタンスが、凍結状態から抜け出すための土台になる。私自身、過去の失敗をいつまでも反芻して、自分を追い詰めていた時期がある。でも気づいた。忘れることは、逃げではない。 嫌な記憶を手放すことで、初めて前に進める。回復とは、忘れるべきものを忘れられるようになることでもある。なぜ「何もしない休み」が回復にならない場合があるのか休息も、量を追い求めるだけでは意味がない。「長時間休んだ」という事実よりも、「どう休んだか」という質の方がずっと重要だ。休息には二つのタイプがある。「パッシブレスト（消極的休養）」。何もしない。寝る。横になる。身体を動かさない。これは従来の「休息」のイメージだ。「アクティブレスト（積極的休養）」。軽く身体を動かす。散歩する。ストレッチする。ヨガをする。能動的に身体を使うことで回復する。どちらが正解か、ではない。どちらが自分に足りていないかが問題だ。ただ、直感に反するが、現代のエンジニアにはアクティブレストの方が足りていない場合が多い。パッシブレストが逆効果になる条件は何かパッシブレストが逆効果になるのは、以下のような場合だ。身体が動かなすぎているとき。一日中座っていて、血流が滞っている。筋肉が固まっている。この状態でさらに横になっても、血流は改善しない。疲労物質は排出されない。むしろ、さらに滞留する。脳だけが疲れているとき。身体は使っていない。脳だけが酷使されている。この状態で「何もしない」と、身体と脳のアンバランスが解消されない。脳を休めるには、逆に身体を動かす方が効果的な場合がある。横になりながら刺激を受けているとき。ベッドでスマホを見ている状態。身体は休んでいるが、脳は休んでいない。これは休息ではない。むしろ、最悪の組み合わせだ。社会的な孤立状態のとき。一人で何もしない時間が長すぎると、孤独感が増す。孤独は心身に悪影響を与える。パッシブレストが孤独を深めるなら、逆効果だ。アクティブレストは、なぜ自律神経に効くのかアクティブレストが効果的な理由は、生理学的に説明できる。血流が改善する。軽い運動は心拍数を適度に上げ、血液循環を促進する。これにより、筋肉に蓄積した疲労物質が排出される。新鮮な酸素と栄養が全身に行き渡る。自律神経のバランスが整う。適度な運動は、交感神経と副交感神経の切り替えをスムーズにする。運動中は交感神経が優位になり、運動後は副交感神経が優位になる。このリズムが、自律神経の柔軟性を高める。脳の状態が変わる。運動は脳内のセロトニンやエンドルフィンの分泌を促す。これらの神経伝達物質は、気分を改善し、ストレスを軽減する。「運動後に気分がスッキリする」のは、この効果だ。睡眠の質が向上する。日中に適度に身体を動かすと、夜の睡眠が深くなる。これにより、睡眠中の回復効率が上がる。アクティブレストは、受動的な休息では得られない回復効果をもたらす。「休んでいるのに疲れる」行動には共通点があるか「休んでいるつもりなのに疲れる」行動を分析すると、共通点が見えてくる。脳への入力が続いている。スマホ、テレビ、SNS。これらは「受動的」に見えるが、脳は常に情報を処理している。休息ではなく、低負荷の作業だ。身体が動いていない。座っている。横になっている。血流が滞る。筋肉が固まる。代謝が落ちる。社会的なつながりがない。一人で画面に向かっている。人との会話がない。孤独が深まる。達成感がない。ただ時間が過ぎるだけ。何も生み出していない。何も経験していない。虚しさが残る。能動的に選ばなかった時間は、記憶に残らない。後から振り返っても、「何をしていたんだっけ」と思い出せない。これらを逆転させれば、「本当に休まる休息」が見えてくる。脳への入力を減らす。画面から離れる。静かな環境に身を置く。身体を動かす。散歩する。ストレッチする。軽い運動をする。人とつながる。会話をする。一緒に過ごす。達成感を得る。小さなことでいい。料理を作る。掃除をする。何かを「やった」という感覚を持つ。選択的休養という考え方「休む」というと、どうしても「消極的」なイメージがある。何もしない。停止する。エネルギーを使わない。でも、より効果的な休養の形がある。自分で選ぶ休養だ。休息は空いた時間を埋めるものではない。休息は設計対象だ。どう休むかを、自分で決める。「そんな時間ないよ」と思うかもしれない。でも、選択的休養は時間の量ではなく質の問題だ。30分でもいい。自分で選んだ30分は、誰かに決められた2時間より回復効果がある。選択的休養とは、自分の意志で、自分のために選んだ活動のことだ。ポイントは「自分で選ぶ」ことにある。誰かに言われてやるのではない。義務感でやるのではない。「やるべき」だからやるのではない。自分が「やりたい」と思って選ぶ。この「選ぶ」という行為自体が、回復をもたらす。なぜ「自分で選ぶ」ことに意味があるのか現代の疲労の多くは、選択権を奪われていることから来ている。仕事では、やるべきことが決まっている。締め切りがある。上司の指示がある。クライアントの要望がある。自分で選ぶ余地が少ない。プライベートでも、「やるべきこと」に追われている。家事、育児、介護、人付き合い。「自分のため」ではなく「誰かのため」に時間を使う。そして「空いた時間」にスマホを見る。これも、実は選択ではない。アルゴリズムが見せたいものを見せられている。自分で選んでいるようで、選ばされている。常に誰かに決められた行動をしている。だからこそ、「自分で選ぶ」ことに価値がある。自分で選んだ活動をしているとき、脳は「自分の人生をコントロールしている」と感じる。この感覚が、ストレスを軽減し、回復を促進する。逆に、誰かに決められた行動をしているとき、脳は「コントロールを失っている」と感じる。これがストレスの原因になる。選択的休養とは、人生の主導権を握り直すことだ。 そしてこれは、何を覚えておくかだけでなく、何を忘れるかを選ぶことでもある。AIは全てを記憶できる。でも人間は違う。だからこそ、意識的に手放す。追いかけなくていい情報を捨てる。キャッチアップしなくていい技術を諦める。その余白に、自分だけの発想が生まれる。選択的休養の条件選択的休養が効果的であるためには、いくつかの条件がある。自分で決めた。誰かに言われてではなく、自分の意志で選ぶ。「やらなければ」ではなく「やりたい」という動機。仕事とは関係ない。スキルアップのための勉強は選択的休養ではない。仕事に役立つ読書も違う。仕事と完全に切り離された活動。なぜなら、仕事に関連している限り、「成果を出さなければ」というプレッシャーがつきまとうからだ。没頭できる。時間を忘れて集中できる。義務感ではなく、純粋な興味や楽しさで取り組める。成長の実感がある（任意）。必須ではないが、少しずつ上達していく実感があると、より効果的だ。仕事以外の領域で「できるようになった」という経験は、自己効力感を高める。私の場合、それは楽器を弾くことと、格闘技のジムに通うことだった。ギターを弾く時間は、仕事とは無関係で、自分で決めた活動で、時間を忘れて没頭でき、少しずつ上達していく実感がある。格闘技のジムには、別の効果がある。自分一人では無限に追い込めない。だから、真剣にやる以外に選択肢がない環境に身を置く。スパーリング中は、仕事のことなど考えていられない。相手のパンチを避けることに全神経を集中させている。休む時は、可能な限り忘れる。 この忘却を強制してくれる環境が、私には必要だった。なぜ「楽ではないこと」が回復になるのかここで一つの逆説に気づく。格闘技は楽ではない。むしろ苦しい。汗をかく。息が切れる。翌日は筋肉痛だ。苦しいのに、なぜかジムの帰り道は頭が軽い。通い続けるうちに、分かってきた。私が選んだ苦しみは、喜びになる。考えてみれば不思議だ。ホラー映画、激辛料理、過酷な登山。人は日常では避けるはずの「痛み」や「恐怖」に、わざわざ金と時間を払って近づく。私も格闘技に月謝を払っている。殴られに行っている。なぜか。「選んだ苦痛」と「押しつけられた苦痛」は、まったく別物だからだ。仕事のストレス、人間関係の摩擦、将来への不安。これらは望んでいない。避けたいのに避けられない。コントロールできない。だから消耗する。格闘技の苦しさは違う。私が選んだ。いつでもやめられる。コントロールできる。だから同じ「苦しい」でも、片方は消耗で、片方は回復になる。そしてもう一つ気づいたことがある。楽なだけの人生は、たぶんつまらない。苦しみを避け続けた先に、充実はない。ベッドでスマホを見続ける週末は、苦しみをゼロにしようとする試みだ。でもそれは、意味もゼロにしてしまう。何も残らない。月曜日に「週末何してた？」と聞かれて、答えられない。格闘技は苦しい。でも意味がある。だから回復する。「楽であること」と「良いこと」は違う。 私はこれを、身体で学んだ。「ギターや格闘技なんて、自分には無理だ」と思うかもしれない。でも、選択的休養の本質は特定の活動ではない。「仕事の自分」とは別の自分に会いに行くことだ。ランニングでも料理でも将棋でも絵でも釣りでもいい。重要なのは、「仕事に役立つかもしれない」という思考を捨てること。 役に立たなくていい。役に立たないからこそ、純粋に楽しめる。その純粋さが、回復をもたらす。もう一つ、見つけ方のコツがある。周りに勧められたものを、何も考えずに始めてみる。自分で選ぼうとすると、「合うかな」「続くかな」と考えすぎて動けなくなる。友人が「一緒にやろう」と誘ってくれたら、とりあえず乗ってみる。合わなければやめればいい。始める前に悩むより、始めてから判断する方がずっと早い。「役に立たない」と思って捨てたものの中に、自分を救うものがある。デジタルデトックスという実践ある日、スマホを置いて散歩に出た。1時間後、頭が軽かった。そこで気づいた。私の疲労の大きな部分は、デジタル機器から来ていた。 正確には、デジタル機器が境界線を消し、常時接続状態を作り、注意力を奪い続けていた。全ての疲労がデジタル由来ではないが、デジタルが他の疲労を増幅させている。だからこそ、「デジタルデトックス」が必要だ。大げさなことではない。スマホを別の部屋に置く。一日一時間、画面を見ない時間を作る。寝る前の一時間はスマホを触らない。これだけでも効果がある。最初は落ち着かない。通知が気になる。何かを見逃しているような気がする。FOMO（見逃すことへの恐怖）が襲ってくる。この不快感こそが「摩擦」だ。 そして摩擦があるからこそ、その先にある回復は本物になる。でも、数日続けると気づく。別に何も見逃していない。大抵のことは、後から確認しても問題ない。「今すぐ」反応しなければならないことなど、実際にはほとんどない。そして画面から離れた時間に、不思議なことが起きる。頭がクリアになる。創造性が戻ってくる。ぼんやりと考えごとをする余裕が生まれる。有限であることを受け入れ、有限であるからこそできることを大切にする。デジタルから離れた時間は、人間としての有限性を肯定する時間だ。スマホを置いた瞬間、世界は何も変わらない。でも、自分だけが少し回復する。みんな、もっと真剣に休む方法を考えた方がいい。働き方は語られる。生産性は語られる。キャリアは語られる。でも休み方は、ほとんど語られない。「休めばいい」で片付けられる。それは違う。どう働くかと同じくらい、どう休むかは設計が必要なのだ。孤独という敵在宅勤務を続けていると、ある問題に直面する。孤独だ。孤独は好きだと思っていた。一人で考える時間、一人でコードを書く時間、誰にも邪魔されない自由。それを選んで在宅勤務を続けてきた。思えば、昔からそうだった。初対面だけは愛想がいい。すぐに打ち解ける。でも、それ以上は仲良くならない。小学生の頃から「一番仲の良い友達」というものがいなかった。人のことを、どこかで信用しきれない。だから深い関係を避けてきた。孤独は、選んだというより、そうなっていた。でも気づいた。私が「孤独を好んでいる」と思っていたのは、実は「人間関係の疲れから逃げていた」だけかもしれない、と。オンライン会議で消耗する。Slackで気を遣う。だから一人でいたくなる。これは「孤独を選んでいる」のではなく、「疲弊して引きこもっている」だけだ。健全な孤独と、不健全な孤独は違う。健全な孤独は、充電された状態から自分を選ぶこと。不健全な孤独は、消耗した状態から逃避すること。安心している状態から選ぶ孤独は健全だ。身体がシャットダウンした凍結状態としての孤独は、危険信号だ。休むためには、時に人とつながる必要がある。 矛盾しているようだが、社会的なつながりが足りていない状態では、一人でいても回復しない。孤独を選んでいるのか、孤独に追い込まれているのか。この違いを見極めることが、回復の分岐点になる。有給休暇を取るということ去年、有給休暇を40日以上残したまま年度が終わった。「有給どれくらい残ってる？」「40日以上」「俺も」。この会話を何度もした。笑い話みたいに。でも笑えない。40日間、自分のための時間を放棄したということだ。プロジェクトが忙しい。休むと仕事が溜まる。チームに迷惑がかかる。そう言い聞かせてきた。でも本当の理由は違う気がする。「休む理由がない」と思っていた。体調が悪いわけでもない。旅行の予定があるわけでもない。だから働く。この発想自体がおかしかったのだ。ある日、何の予定もなく有給を取ってみた。朝起きて、コーヒーを淹れて、本を読んで、散歩して、昼寝して、夕方になった。何も生産しなかった。何も達成しなかった。でも、妙に満たされていた。気づいたのは、「理由がないから休まない」は、「理由がないと自分を大切にしない」と同じだということ。病気になるまで働いて、やっと休む権利を得る。それは順序が逆だ。リモートワークでは、この問題がさらに深刻になる。どこでも働けるから、どこにいても「働いていない自分」に罪悪感を覚える。有給を取っても、Slackが気になる。結局PCを開いてしまう。有給休暇の本質は、「働かない時間を作る」ことではない。「働かない自分を許す練習」だ。睡眠という基盤深夜2時。また技術記事を読んでいる。「これだけ読んだら寝よう」と思って開いたブラウザのタブが、気づけば15個になっている。一つ読むと、関連記事が気になる。そっちを開く。また関連記事が出てくる。無限ループだ。睡眠が大事なことくらい、知っている。知っていて、毎晩削っている。「知っている」と「できる」の間には、深い溝がある。ある時期、睡眠時間が4時間を切る日が続いた。最初は平気だった。むしろ「自分は少ない睡眠でも動ける」と思っていた。でも二週間くらいで、明らかにおかしくなった。簡単なコードでミスを連発する。同じ箇所を何度も読み返す。会議で人の話が頭に入ってこない。睡眠不足は、自分では気づけない。認知機能が落ちているから、「認知機能が落ちている」ことを認知できない。これが一番怖いところだ。酔っ払いが「俺は酔ってない」と言うのと同じ構造。睡眠不足の人間は、自分が睡眠不足だと正しく判断できない。睡眠中、脳は単に休んでいるのではない。日中に入ってきた情報を整理し、不要なものを捨て、必要なものを定着させている。この作業が追いつかないと、頭の中がゴミ屋敷になる。思考がまとまらない。創造性が消える。読んだ本の内容が腑に落ちるのは、読んだ直後ではない。数日後、ふと「あれはこういうことだったのか」と分かる瞬間がある。その熟成には、睡眠が必要だ。睡眠を削ることは、未来の自分から時間を前借りしている。 利息は高い。そして返済は、体調不良という形でやってくる。今夜削る2時間は、来週のどこかで4時間になって返ってくる。しかも最悪のタイミングで。「効率を手放す」とは、エンジニアにとってどんな覚悟か私たちエンジニアは、効率を追求することに慣れている。コードを最適化する。プロセスを改善する。無駄を省く。それが仕事だ。でも、休息に効率を求めてはいけない。「最も効率的な休息法は何か」「最短時間で最大の回復効果を得るには」「休息の ROI を最大化するには」こういう発想自体が、休息を台無しにする。余暇にまでROIを求める病一日中「効率」を考えている。その思考パターンが、仕事以外の時間にも染み出してくる。無意識のうちに「この行動の費用対効果は」と考えてしまう。時間の希少性。仕事が忙しい。自由な時間が少ない。だから、その貴重な時間を「最大限に活用したい」と思う。無駄にしたくない。効率的に楽しみたい。成果主義の内面化。成果で評価される環境に長くいると、「成果がなければ価値がない」という信念が内面化される。休息も「何かを得るため」に行うべきだと思ってしまう。不安の回避。何もしないことが怖い。生産性がない自分に価値がないと感じる。だから、休息さえも「生産的」にしようとする。非効率な時間は、どんな価値を回復させるのかしかし、非効率な時間には、効率では得られない価値がある。余白から、ふとしたひらめきが生まれる。このブログの構成も、散歩中にふと浮かんだ。何かを「考えよう」としているときではなく、何も考えていないときに、頭が勝手に整理を始める。そして不思議なことに、この整理の過程で、脳は細部を手放している。細部を忘れているからこそ、異なる記憶同士が自由につながる。全部を完璧に覚えていたら、新しい組み合わせは生まれない。ぼんやりしている時間は、無駄ではなかった。自分を取り戻す時間になる。何かを達成するためではなく、ただ存在する時間。その時間の中で、「自分は何が好きなのか」「自分は何を大切にしたいのか」という問いに向き合える。人間らしさを回復する。効率を追求するのは機械の得意分野だ。非効率を楽しめるのは、人間だけの特権だ。 AIは目標を与えられると、最短経路で達成しようとする。しかし人間は、わざと遠回りすることができる。意味のないおしゃべり、下手な楽器演奏、勝てないゲーム。この「わざと非効率を選ぶ」能力は、目標最適化しかできないAIには原理的に不可能だ。非効率の中にこそ、最適化では見つからない価値がある。関係性を深める。人間関係は効率化できない。信頼を築くには時間がかかる。無駄話をする。一緒に何もしない時間を過ごす。これらの「非効率」が、関係性を深める。だから、休息に効率を求めることをやめよう。先週、何の目的もなく街を散策した。1時間、何も生産しなかった。スマホも持たずに、ただ歩いた。帰ってきたとき、妙に頭がすっきりしていた。非効率な時間を、堂々と楽しもう。それが、AI時代を生き抜くための、逆説的な戦略だ。AIは「無駄」を理解できない。だから、無駄を楽しめる人間は、永遠に代替されない。おわりにこの文章を書き終えて、日曜日の夜が終わろうとしている。正直に言うと、書いている間もスマホを何度か見た。通知を確認した。Xを開いた。自分で書いた「デジタルデトックス」の章を読み返しながら、その直後にスマホに手を伸ばしている自分がいた。笑えない。笑えないけど、それが現実だ。私はこの文章を書いたからといって、来週から完璧に休めるようになるわけではない。たぶん来週も、ベッドでスマホを見ながら「休んだつもり」になる日がある。境界線を引けない日がある。格闘技のジムをサボる日がある。でも、少しだけ違うことがある。「休めていない」と気づけるようになった。「これは回復じゃなくて消耗だ」と言語化できるようになった。 それだけでも、前よりマシなのだと思う。たぶん。AIは無限に働ける。私は有限だ。この事実は変わらない。でも、有限であることを恨まなくなった。有限だから、選ばなければならない。選ぶから、何が大事か分かる。全部はできない。全部は追いつけない。それでいい。それに、正直なところ、こうも思っている。どうせAIはこれからもっと賢くなる。 私たちの無能さを、いずれAIが補ってくれる。足りない部分を埋めてくれる。追いつけなかった技術も、AIが代わりにやってくれるようになる。だったら、その日まで健康で元気でいることの方が大事じゃないか。 壊れた身体では、優秀なAIを使いこなすこともできない。だから、選択的に休んでほしい。休むことは、負けを認めることじゃない。降参でもない。 有限な人間として、まともに機能し続けるための、当たり前の行為だ。当たり前のことを、当たり前にやる。それがこんなに難しいとは思わなかった。明日の朝、目覚ましが鳴る。月曜日が始まる。たぶん私は、また疲れている。でも、今日よりは少しだけマシかもしれない。少しだけ、回復の仕方を知っているから。少しだけ、自分を責めずに済むから。おい、休め。これは誰かへの命令じゃない。自分への、しつこい呼びかけだ。何度も忘れて、何度も思い出す。それでいい。完璧に続けることより、何度でも思い出せることの方が大事だから。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考書籍限りある時間の使い方作者:オリバー・バークマンかんき出版Amazonスタンフォード式　疲れない体作者:山田 知生サンマーク出版Amazon戦略的暇作者:森下彰大飛鳥新社Amazon休養学―あなたを疲れから救う作者:片野 秀樹東洋経済新報社Amazon疲労学: 毎日がんばるあなたのための作者:片野 秀樹東洋経済新報社Amazonスマホ脳（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazon奪われた集中力: もう一度〝じっくり〟考えるための方法作者:ヨハン・ハリ作品社Amazonワイド新版　思考の整理学 (単行本 --)作者:外山　滋比古筑摩書房Amazon新版　「読み」の整理学 (ちくま文庫)作者:外山滋比古筑摩書房Amazon忘却の整理学 (ちくま文庫)作者:外山滋比古筑摩書房Amazon忘却の効用　「忘れること」で脳は何を得るのか作者:スコット・A・スモール,寺町朋子白揚社Amazon苦痛の心理学:なぜ人は自ら苦しみを求めるのか作者:ポール・ブルーム草思社Amazon「恥」に操られる私たち　他者をおとしめて搾取する現代社会作者:キャシー・オニール白揚社Amazon社会は、静かにあなたを「呪う」　～思考と感情を侵食する“見えない力”の正体～ (小学館クリエイティブ)作者:鈴木祐小学館Amazon半うつ　憂鬱以上、うつ未満作者:平 光源サンマーク出版Amazon地に足をつけて生きろ！ 加速文化の重圧に対抗する7つの方法作者:スヴェン・ブリンクマンEvolvingAmazon","isoDate":"2025-12-21T00:24:56.000Z","dateMiliSeconds":1766276696000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"cargo-coupling: Rustプロジェクトの結合度を可視化する","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/20/195329","contentSnippet":"cargo-coupling を自己診断した時のweb ui です。はじめに「このモジュール、なんか触りたくないな...」ソフトウェア開発をしていると、こんな感覚を覚えることがあります。変更するたびに他の箇所が壊れる、テストが書きにくい、そもそも何をしているのか把握しづらい。これらの症状には共通点があります。モジュール同士が過剰に依存し合っている、つまり結合（カップリング）の問題です。結合の問題は厄介です。コードを書いているときには気づきにくく、後から「なぜこんなに変更が大変なのか」と悩むことになります。さらに困るのは、「結合が強すぎる」と分かっても、具体的にどこがどう強いのか、どこから手をつければいいのかが見えにくいことです。振り返ってみると、私は結合に対する解像度がかなり低かったのではないでしょうか。「なんとなく密結合っぽい」「疎結合の方がいいらしい」という感覚で良し悪しを判断していた。でも、その感覚を言葉にしようとすると、うまく説明できない。この「見えにくさ」を解消するには、結合を測る物差しが必要です。しかし、従来の「強い/弱い」という1軸だけでは不十分でした。なぜなら、同じ「強い結合」でも、場所や状況によって意味が変わるからです。そこで注目したいのが、Vlad Khononovの「Balanced Coupling」という考え方です。結合を「強度」「距離」「変動性」の3つの軸で捉え、それらのバランスを評価するフレームワークです。今回紹介するcargo-couplingは、このフレームワークをRustプロジェクト向けに実装したツールです。AIがコードを書く時代になっても、この結合度という指標は重要性を増すはずです。なぜなら、コードを書く主体が誰であれ、そのコードを理解し、保守し、拡張するのは人間だからです。むしろAIが生成したコードだからこそ、その構造を客観的に評価できる物差しが必要になります。まずはツールの概要を見てから、その背景にある考え方、そして実際の使い方へと進んでいきましょう。cargo-couplingとはcargo-couplingは、私がRustプロジェクト向けに開発した結合度分析ツールです。このツールを作るきっかけになったのは、Vlad Khononovの著作「Balancing Coupling in Software Design」との出会いでした。結合設計について漠然と感じていた課題が、この本で体系的に整理されていたのです。「強度」「距離」「変動性」という3つの軸で結合を捉えるフレームワークに感銘を受け、これをRustプロジェクトで実際に使えるツールにしたいと考えました。書籍は翻訳も含めて読みやすいので、ぜひ手に取ってみてください。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonツールはGitHubで公開しています。気に入ったらStarしていただけると励みになります。github.comcrates.ioからインストールできます。https://crates.io/crates/cargo-couplingcrates.ioここで、多くの人が持っている常識を一度疑ってみましょう。「結合は減らすべきだ」——そう思っていませんか？このツールは「結合を減らす」ことを目標にしていません。「結合を適切に設計する」ことを目標にしています。なぜなら、結合は本質的に悪ではないからです。関連する機能が密に連携するのは自然なことで、問題になるのは「不適切な場所での強い結合」や「遠く離れたモジュール間の密結合」です。この視点の転換が、このツールの核心です。# インストールcargo install cargo-coupling# 基本的な使い方cargo coupling ./src3つの次元で結合を分析では、「適切な結合」とは具体的に何を指すのでしょうか。従来の結合分析は「強い/弱い」の1軸で考えがちでした。しかし、ここで立ち止まって考えてみてください。同じ「強い結合」でも、すぐ隣のモジュールとの結合と、遠く離れた外部ライブラリとの結合では、意味が違うはずです。また、5年間ほとんど変更されていないコードとの結合と、毎週のように変更されるコードとの結合では、リスクが違うはずです。この違いを捉えるには、1軸では足りません。cargo-couplingは結合を3つの独立した次元で測定します。1. Integration Strength（結合強度）最初の軸は「結合強度」です。モジュール同士が「どれだけ互いの内部を知っているか」を表します。user.password_hashのように構造体のフィールドを直接触っているコード、見覚えがありませんか？これは最も強い結合です。一方、impl Traitを介してやり取りするコードは、相手の内部を知らなくても動作します。この違いをスコア化します。 レベル  スコア  説明  Rust例  Intrusive  1.00  内部実装に直接依存  struct.field 直接アクセス  Functional  0.75  関数シグネチャに依存  メソッド呼び出し  Model  0.50  データ構造に依存  型定義、型パラメータ  Contract  0.25  trait/インターフェースのみ  impl Trait 2. Distance（距離）2つ目の軸は「距離」です。結合されたモジュール同士が、コードのスコープ階層でどれだけ離れているかを表します。同じファイル内の関数同士が密に連携しているのは自然なことです。しかし、src/auth/login.rsがsrc/billing/invoice.rsを直接参照していたらどうでしょう？さらに、外部クレートの内部構造に依存していたら？距離が遠いほど、その結合の「重さ」は増します。 レベル  スコア  説明  SameModule  0.25  同一ファイル/モジュール内  DifferentModule  0.50  同一クレート内の別モジュール  DifferentCrate  1.00  外部クレートへの依存 3. Volatility（変動性）3つ目の軸は「変動性」です。「どれくらい頻繁に変更されるか」を表します。あなたのプロジェクトにも、1年以上触られていない安定したモジュールと、毎週のように修正が入るモジュールがあるはずです。安定したコードに依存するのと、頻繁に変わるコードに依存するのでは、リスクが違います。cargo-couplingはGit履歴からこの変動性を自動で計算します。 レベル  スコア  Git 6ヶ月での変更回数  Low  0.00  0-2回  Medium  0.50  3-10回  High  1.00  11回以上 バランススコアの計算ここまで3つの次元を見てきました。しかし、「強度が0.75」「距離が0.50」「変動性が中程度」とバラバラに言われても、結局この結合は良いのか悪いのか、判断しづらいですよね。そこでcargo-couplingは、これら3つの次元を組み合わせてバランススコアを計算します。3つの数値を1つのスコアにまとめることで、「この結合は適切か」を直感的に判断できるようになります。考え方はシンプルです。「強度と距離のバランス」と「変動性によるリスク」の2つを掛け合わせます。ALIGNMENT = 1.0 - |STRENGTH - (1.0 - DISTANCE)|VOLATILITY_IMPACT = 1.0 - (VOLATILITY × STRENGTH)BALANCE_SCORE = ALIGNMENT × VOLATILITY_IMPACT最初の式は「強度と距離が釣り合っているか」を測ります。距離が近ければ強結合でも問題なく、距離が遠ければ弱結合であるべきです。2番目の式は「変更頻度と結合強度の組み合わせリスク」を測ります。頻繁に変更されるコードと強く結合していると、変更のたびに影響を受けるリスクが高まります。この計算式が導く結論を整理すると、以下のようになります。強結合 + 近距離 → Good：関連機能が1つのモジュールにまとまった高凝集な状態弱結合 + 遠距離 → Good：モジュール間の依存が最小限な疎結合アーキテクチャ強結合 + 遠距離 → Bad：変更影響が広範囲に及ぶグローバル複雑性の状態強結合 + 高変動性 → Bad：頻繁な変更が連鎖的影響を生む変更波及リスク実際に使ってみる理論を理解したところで、実際のプロジェクトでどう使うかを見ていきましょう。cargo-couplingは目的に応じて複数の出力形式を用意しています。サマリー表示cargo coupling --summary ./src出力例は以下のとおりです。Coupling Analysis Summary:  Health Grade: B (Good)  Files: 14  Modules: 14  Couplings: 389  Balance Score: 0.83  Issues:    Medium: 2  Top Priority:    - [Medium] cargo-coupling::main → 21 dependencies    - [Medium] 21 dependents → cargo-coupling::cargo_coupling  Breakdown:    Internal: 33    External: 356    Balanced: 33    Needs Review: 0    Needs Refactoring: 0  Connascence:    Total: 807 (avg strength: 0.23)    High-strength: Position=2, Algorithm=2  APOSD Metrics:    Pass-Through Methods: 12 (simple delegation)    High Cognitive Load: 2 modules    Avg Module Depth: 7.9日本語出力日本語での出力も対応しています。cargo coupling --japanese ./srcカップリング分析: my-project━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━評価: B (Good) | スコア: 0.67/1.00 | モジュール数: 143次元分析:  結合強度: Contract 1% / Model 24% / Functional 66% / Intrusive 8%           (トレイト)   (型)      (関数)        (内部アクセス)  距離:     同一モジュール 6% / 別モジュール 2% / 外部 91%  変更頻度: 低 2% / 中 98% / 高 0%ホットスポット分析リファクタリングすべき優先度の高いモジュールを特定します。cargo coupling --hotspots ./src#1 my-project::main (Score: 55)   🟡 Medium: High Efferent Coupling   💡 What it means:      This module depends on too many other modules   ⚠️  Why it's a problem:      • Changes elsewhere may break this module      • Testing requires many mocks/stubs      • Hard to understand in isolation   🔧 How to fix:      Split into smaller modules with clear responsibilities      e.g., Split main.rs into cli.rs, config.rs, runner.rs影響分析特定のモジュールを変更したときの影響範囲を調べられます。cargo coupling --impact metrics ./srcWeb UIでの可視化インタラクティブなグラフで結合関係を可視化できます。cargo coupling --web ./srcブラウザが自動で開き、Cytoscape.jsを使った対話的なグラフが表示されます。ノードをクリックすると詳細情報が見られ、問題のあるモジュールは色分けされます。CI/CDでの活用手動で分析するだけでなく、継続的に品質を監視することもできます。cargo-couplingを品質ゲートとして組み込むと、結合設計の劣化を早期に検出できます。cargo coupling --check \\  --min-grade=B \\  --max-circular=0 \\  ./srcGitHub Actionsの例は以下のとおりです。- name: Check coupling health  run: |    cargo coupling --check \\      --min-grade=B \\      --max-critical=0 \\      ./srcグレードが基準を下回るとexit code 1を返すため、CIパイプラインに組み込めます。AI連携Claude CodeやGitHub Copilotと組み合わせて使う場合、--aiオプションが便利です。cargo coupling --ai ./srcAIフレンドリーな形式で出力されるので、そのままAIに貼り付けてリファクタリング提案を得られます。検出される問題パターンここまで使い方を見てきましたが、具体的にどんな問題が検出されるのか気になるところでしょう。cargo-couplingが警告する代表的なパターンを紹介します。God Module（神モジュール）関数、型、implが多すぎるモジュールです。関数: 30個以上型: 15個以上impl: 20個以上High Efferent Coupling（外向き結合過多）依存先が多すぎるモジュール。デフォルトでは20以上の依存で警告されます。High Afferent Coupling（内向き結合過多）依存されすぎているモジュール。デフォルトでは30以上の依存元で警告されます。Cascading Change Risk（変更波及リスク）侵入的結合（Intrusive）と高変動性（High Volatility）の組み合わせ。変更のたびに広範囲に影響が及ぶ危険な状態です。ヘルスグレードの解釈問題パターンの検出結果は、最終的に1つのグレードに集約されます。このグレードがプロジェクト全体の健全性を示します。 Grade  説明  S  Over-optimized。リファクタリングしすぎかも  A  Well-balanced。理想的な状態  B  Healthy。管理可能な状態  C  改善の余地あり  D  注意が必要  F  即刻対応が必要 興味深いのは、Sグレードが「やりすぎ」とされている点です。なぜでしょうか？結合を減らしすぎると、コードが細切れになりすぎて、かえって全体像が見えなくなります。1つの処理を追うために10個のファイルを開く必要があったり、抽象化のレイヤーが深すぎて「結局何をしているの？」と迷子になったり。そういう経験はありませんか？結合は「減らせばいい」という単純な話ではありません。バランスが大切なのです。ライブラリとしての利用CLIツールとして使うだけでなく、独自のツールに組み込むこともできます。cargo-couplingはライブラリとしても公開しているので、プログラムから直接分析機能を呼び出せます。use cargo_coupling::{    analyze_workspace,    analyze_project_balance_with_thresholds,    IssueThresholds,    VolatilityAnalyzer,};fn main() -\u003e Result\u003c(), Box\u003cdyn std::error::Error\u003e\u003e {    // AST解析    let mut metrics = analyze_workspace(Path::new(\"./src\"))?;    // Git変動性分析    let mut volatility = VolatilityAnalyzer::new(6);    volatility.analyze(Path::new(\"./src\"))?;    metrics.file_changes = volatility.file_changes;    metrics.update_volatility_from_git();    // バランス分析    let report = analyze_project_balance_with_thresholds(        \u0026metrics,        \u0026IssueThresholds::default()    );    println!(\"Grade: {}\", report.health_grade);    Ok(())}パフォーマンスcargo-couplingは、大規模プロジェクトでも高速に動作するよう設計されています。Rayonによる並列AST解析Git履歴のストリーム処理実績: tokio（488ファイル）で655ms--no-gitオプションを使えば、Git分析をスキップしてより高速に動作します。制限事項便利なツールですが、万能ではありません。使う前に知っておくべき制限があります。外部クレート依存は分析対象外: serde、tokioなどへの依存は分析されない。開発者がコントロールできない部分のため静的解析のみ: ランタイムの動作やマクロ展開は完全には捉えられないGit履歴が必要: Volatility分析にはGit履歴が必要。履歴が短いと精度が下がるまとめcargo-couplingは、「結合は悪」という単純な考え方ではなく、「適切な結合を選ぶ」という実用的なアプローチを提供します。3次元分析: 強度・距離・変動性を同時に考慮Git連携: 実際の変更頻度をデータとして反映実行可能な提案: 具体的なリファクタリングアクションを提示複数の出力形式: テキスト/JSON/Web UI/AIフレンドリーCI/CD統合: 品質ゲートとして自動チェック完璧な設計を目指す必要はありません。「80%の改善で十分」というプラグマティックな姿勢で、少しずつプロジェクトの健全性を高めていきましょう。# まずは試してみてくださいcargo install cargo-couplingcargo coupling --summary ./src結合の問題が可視化されるだけでも、設計改善の第一歩になります。次にあなたが「このモジュール、なんか触りたくないな...」と感じたとき、それはもう漠然とした不安ではありません。強度・距離・変動性という3つの軸で分析でき、具体的な改善アクションに落とし込める、対処可能な課題です。その感覚は、恐れではなく、改善の入り口なのです。似た概念にA Philosophy of Software DesignのComplexity がある。これも良い考え方なので一読をおすすめします。 speakerdeck.comA Philosophy of Software Design, 2nd Edition (English Edition)作者:Ousterhout, John K. ISSVWOAmazon","isoDate":"2025-12-20T10:53:29.000Z","dateMiliSeconds":1766228009000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"生成AI時代のMarp によるスライド環境の構築","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/19/183148","contentSnippet":"この記事は、3-shake Advent Calendar 2025 19日目のエントリ記事です。はじめにエンジニアがプレゼン資料を作るとき、PowerPointやKeynoteにもどかしさを感じることがあります。コードを書くようにスライドを作りたい。Gitでバージョン管理したい。テーマを一括で変更したい。Marpはこれらの願望を叶えるマークダウンベースのスライド生成ツールです。marp.appしかし生成AI時代の今、新しい課題が生まれています。AIにスライドの下書きを依頼できるようになった反面、「AIっぽいプレゼン」が量産されるようになりました。整然としすぎて、話者の思考が見えない資料です。この記事では、AIの力を借りつつ「自分のプレゼン」を取り戻す仕組みを紹介します。AIっぽいプレゼンの正体生成AIにプレゼン資料を依頼すると、だいたい同じ構成になります。「まず、次に、最後に」という接続詞。きれいに3項目並んだ箇条書き。当たり障りのない結論。これは決して間違っていません。しかし聴衆の記憶には残りません。なぜでしょうか。聴衆の脳は「予測を裏切られた瞬間」に活性化します。「まず、次に、最後に」という予定調和な構成では、脳は省エネモードで聞き流します。3項目の箇条書きを見た瞬間、聴衆は「ああ、3つあるのね」と思考を止めます。AIが生成するプレゼンは、統計的に最も頻出するパターンの再現です。だから誰が作っても似たような資料になります。Marpを選んだ理由はシンプルです。マークダウンはプレーンテキストなので、AIが直接編集でき、人間がTextlintやカスタムルールで機械的にチェックできます。PowerPointのようなバイナリ形式では「AI生成→ルール検証」の流れが困難です。Gitで差分管理でき、CSSでデザインを一括制御できる点も大きいです。プロジェクト構造実際に運用しているMarpプロジェクトの構造を紹介します。github.com3shake-marp-templates/├── templates/              # 再利用可能なテンプレート├── themes/                 # CSSテーマ├── slides/2025/           # 実際のプレゼンテーション├── assets/images/         # 画像資産└── .claude/               # Claude Code統合    ├── commands/          # スラッシュコマンド    ├── agents/            # 専門家エージェント    └── rules/             # 執筆ルールポイントは.claude/ディレクトリです。Claude Codeと統合することで、スライドのレビューを自動化しています。CommandsやSub-agentsの詳細については、以前の記事で解説しています。syu-m-5151.hatenablog.comMarpの基本設定.marprc.ymlでMarpを設定します。allowLocalFiles: truehtml: truemermaid: truebespoke:  progress: trueoptions:  engine: '@marp-team/marp-core'mermaid: trueでMermaid記法の図表が使えます。しかし正直なところ、PDFエクスポート時に崩れることがあります。重要な図は画像として用意するほうが安全です。package.jsonのスクリプトは以下の通りです。{  \"scripts\": {    \"start\": \"marp -s . --html --allow-local-files\",    \"build\": \"marp --html --allow-local-files\"  }}npm startでローカルサーバーが起動し、ファイル保存のたびに自動リロードされます。テーマによるブランディングCSSテーマで全スライドに統一感を持たせます。/* 3shake-theme.css */:root {  --3shake-blue: #4AADDD;  --3shake-blue-dark: #0a1929;  --3shake-yellow: #ECBE30;}section {  background: white;  font-family: 'Noto Sans JP', sans-serif;}/* 全スライドにロゴを自動配置 */section::after {  content: '';  background-image: url('../assets/images/logo.png');  position: absolute;  left: 30px;  bottom: 20px;}ロゴとページ番号が自動配置されます。プレゼン作成者はブランディングを意識する必要がなくなります。AIっぽさを排除するルールここからが本題かもです。.claude/rules/slide-writing.mdに以下の禁止事項を定義しています。箇条書きを3項目で揃えない（2つか4つにする）「まず、次に、最後に」という機械的な接続詞を使わない完全に等分な説明をしない（メリハリをつける）抽象的で当たり障りのない表現を避けるなぜ2つか4つなのか。 聴衆の「3つだろう」という予測を外すためです。2つなら対比が明確になり、4つなら網羅感が出ます。3つは「ちょうどいい」ゆえに印象へ残りません。意図的にパターンを崩すことで、聴衆の能動的な思考を促します。「PowerPointでも同じルールを適用できる」という反論があるでしょう。確かにその通りです。しかしPowerPointでは、このルールを機械的にチェックする手段がありません。Marpならテキストベースなので、「3項目の箇条書きを検出したら警告」というルールを自動実行できます。人間の意志力に頼らず、仕組みで品質を担保します。身体性の供給以前の記事で「AIに記事を書かせるとは何か」について書きました。プレゼンにも同じことが言えます。AIは構造化が得意です。しかし「身体性」は供給できません。ここで言う身体性とは、知識が「情報」から「経験」へと変容する過程で生じる一人称的な認知の軌跡です。たとえば「このツールを導入したら開発効率が上がった」という情報と、「導入時に設定で3時間ハマってドキュメントの不備に気づきPRを送った」という経験は別物です。プレゼンにおける身体性とは、「なぜこのトピックを選んだのか」「どこで躓いたのか」「何に感動したのか」という、話者固有の軌跡です。これはAIには生成できません。私の作業フローはこうです。伝えたいメッセージを箇条書きで書き出す（5-7個）各メッセージに「自分だけが語れる具体例」を追加AIにレビューを依頼し、構成を整える/review-slideで「3項目の箇条書き」「まず・次に」の指摘を確認指摘箇所を修正AIは足す。人間は削る。 AIは情報の網羅性を最適化しますが、プレゼンの核心は何を省くかにあります。「このトピックは聴衆の関心外」と判断するには、聴衆の反応を想像する力が必要です。この能力は現在のLLMには実装されていません。だからアウトラインは自分で考え、AIにはレビューを任せます。おわりにMarpとルールベースのチェック、そしてClaude Codeのエージェント。この組み合わせで実現したのは、「AIの力を借りながら、自分の思考を残す」環境です。完璧に整った資料より、少し不格好でも話者の考えが透けて見える資料のほうが、聴衆の記憶に残ります。AIが生成した「もっともらしい」スライドではなく、自分の経験に根ざした「本物の」スライドを作る。そのための環境がMarp×Claude Codeです。まずは既存のスライドを1枚だけMarkdown化してみてください。それがMarp×AI環境構築の第一歩です。","isoDate":"2025-12-19T09:31:48.000Z","dateMiliSeconds":1766136708000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Claude Codeの Agent Skills は設定したほうがいい","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/19/173309","contentSnippet":"Claude Codeを使い始めて、様々な発信をしてきました。今回は「Agent Skills」について。これも設定しておくと、Claude Codeがグッと使いやすくなる機能です。Claude Code の settings.json は設定した方がいい - じゃあ、おうちで学べるClaude Code の CLAUDE.mdは設定した方がいい - じゃあ、おうちで学べるClaude Code の .claude/commands/**.md は設定した方がいい - じゃあ、おうちで学べるClaude CodeのHooksは設定したほうがいい - じゃあ、おうちで学べるClaude CodeのSubagentsは設定したほうがいい - じゃあ、おうちで学べるはじめに「このプロジェクトではpython-pptxを使ってスライドを作って」「SQLは必ずこのフォーマットで書いて」「コードレビューはこの観点でチェックして」。Claude Codeを使っていると、こういう説明を何度も繰り返すことになります。CLAUDE.mdに書けば解決すると感じるでしょう。しかしCLAUDE.mdに書いても、毎回読み込まれるとは限らない。commandsを作っても、手動で呼び出す必要がある。どちらも「繰り返し」を完全には解決してくれません。私自身、Rustプロジェクトの開発をClaude Codeに任せようとして、この問題に何度もぶつかりました。「ビルドはcargo fmtから始めて」「セキュリティチェックはOWASPの観点で」「テストは統合テストまで回して」。1回のセッションでは覚えてくれる。しかし新しいセッションを始めると、また最初から説明し直し。なぜこうなるのか。この問題の根本にあるのは、LLMのアーキテクチャ上の制約です。LLMはステートレスで、セッション間で記憶を保持しません。トークン制約とコスト制約があるため、すべての知識を常に保持できません。だから、毎回同じ説明が必要になります。Agent Skillsは、この制約を回避する仕組みです。すべての知識を常に持たせるのではなく、必要な時に必要な知識だけを読み込む。この発想の転換により、ステートレスなLLMでも「状態を持っているかのように」振る舞えます。一度Skillを作っておけば、関連するタスクで自動的にその知識が使われます。www.anthropic.comこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。他の設定との違い今まで紹介してきた機能との違いを整理しておきます。 機能  役割  例  CLAUDE.md  プロジェクトの文脈を伝える  「うちはTypeScriptで、こういうアーキテクチャ」  commands  手動で呼び出すショートカット  /test-and-commit で一連の作業を実行  Hooks  特定のイベントで自動実行  ファイル保存後に自動フォーマット  Subagents  専門家を自動で呼び出す  デバッグ時にdebugger subagentが起動  Rules  パス単位でルールを適用  src/api/**/*.tsにセキュリティルール  Agent Skills  専門知識をオンボーディング  PDF操作、Excel分析、独自ワークフロー Rulesについて補足: .claude/rules/ディレクトリに配置することで、特定の拡張子やディレクトリに対して細かいルールを適用できます。CLAUDE.mdがグローバルな設定なのに対し、Rulesは「このパスにはこのルール」という精密なスコープ設定が可能です。無駄なコンテキスト消費を抑えつつ、必要なルールだけを読み込ませる「段階的開示」の考え方に基づいています。Rulesについては別記事で詳しく書く予定です。表を見ると、各機能には明確な役割分担があります。CLAUDE.mdは文脈、commandsはショートカット、Hooksは自動化、Subagentsは専門家の呼び出し。では、Skillsは何が違うのか。ポイントは、Skillsが「Claudeができること自体を拡張する」点です。他の設定がClaudeの「使い方」を定義するのに対し、SkillsはClaudeの「専門知識」を拡張します。LLMの推論能力自体は変わりませんが、専門家の知識を注入することで出力品質が向上します。Agent Skillsとは何かAnthropicの公式ブログでは、Agent Skillsをこう説明しています。Building a skill for an agent is like putting together an onboarding guide for a new hire.（エージェントにSkillを作ることは、新入社員向けのオンボーディングガイドを作るようなものです）Skillは、指示・スクリプト・リソースをまとめたフォルダです。Claudeがタスクに応じて自動的に読み込み、その専門知識を活用します。platform.claude.comSkill に関しても公式ドキュメントが本当に良いのでオススメです。Anthropicはとりあえず、公式ドキュメントこれは標語にしてほしいです。platform.claude.comそれでも自分的にまとめたいので書かせていただきます。なぜSkillsが必要なのかClaude Codeは万能ですが、「特定のタスク」に最適化されていないことがあります。例えばPowerPointを作ろうとすると、どのライブラリを使うか迷います。フォーマットの細かい仕様を知らない。エッジケースでバグる。毎回試行錯誤が発生します。一方、Skillsがあれば違います。AnthropicのエンジニアがPowerPoint作成の最適解を徹底的に検証して、その知識をパッケージ化している。Claudeはそれを読み込んで、最初からプロとしての出力ができます。この辺は松本勇気さんの生成AI「戦力化」の教科書なんかもとても良いし今度、オライリーから翻訳本AIエンジニアリング―基盤モデルを用いたAIアプリケーション開発の基礎と実践もとても良い。Tool、Skills、MCPの違い混乱しやすいポイントを整理しておきます。Anthropicのブログ記事では、わかりやすいたとえが使われています。claude.comMCP is like having access to the aisles. Skills, meanwhile, are like an employee's expertise.（MCPは店の通路へのアクセス。Skillsは店員の専門知識。）壊れたキャビネットを直したいとき、ハードウェアストアに行けば木工用接着剤もクランプも蝶番も揃っています。しかし、何を買えばいいか、どう使えばいいかは別問題です。 概念  役割  たとえ  Tool  何ができるか（Capability）  店にある道具そのもの  MCP  道具へのアクセス（Connectivity）  店の通路に入ること  Skills  どう振る舞うか（Behavior）  道具の使い方を教える店員 Toolは「APIを叩く」「DBに接続する」「ファイルを操作する」といった個別の能力です。MCPはそれらのToolを統一規格で接続するアダプター。外部システムへの安全で標準化されたアクセスを提供します。そしてSkillsは「どう振る舞うか（Behavior）」まで定義します。複数のToolをどの順番で、どういう判断基準で使うか。Toolの使い方マニュアル付きで渡すのがSkillsです。MCPが接続性を提供し、Skillsがその接続性を効果的に使うための手続き的知識を提供する。両者は競合するものではなく、組み合わせることで真価を発揮します。この構造は、BDD（Behavior-Driven Development、振る舞い駆動開発）に似ています。BDDは単なるテスト手法ではなく、チーム全体の「対話」を促進し、ビジネス価値の高いソフトウェアを効率的に生み出すための開発アプローチです。TDD（テスト駆動開発）が「コードが正しく実装されているか」という開発者視点なのに対し、BDDは「システムが期待通りに振る舞うか」というユーザー・ビジネス視点で考えます。BDDでは、Gherkin記法を使って「Given-When-Then」形式でシナリオを書きます。Feature: ログイン機能  Scenario: ユーザーが正しいIDとパスワードでログインできる    Given ログインページが表示されている    When  正しいIDとパスワードを入力してログインボタンを押す    Then  ホームページにリダイレクトされるこのシナリオは、開発者だけでなく、QAエンジニア、プロダクトオーナー、ビジネス担当者など、全員が読めます。これが「生きた仕様書」として機能し、認識の齟齬を埋めます。Skillsも同じ構造を持っています。 BDD  Skills  Given（前提条件）  description（いつ起動するか）  When（アクション）  SKILL.md本文（何をするか）  Then（期待結果）  具体的な手順（どう振る舞うか） BDDがテストで「コードの振る舞い」を保証するように、SkillsはAIエージェントの「振る舞い」を保証します。BDDの本質的な価値は「ビジネス側とエンジニアの共通言語」でした。「3つのアミーゴ」（PO、開発者、QA）が対話し、全員が納得する仕様を作り上げます。Skillsも同じです。現場のドメインエキスパートがMarkdownで書いた手順は、そのままAIの振る舞いになります。つまり、Skillsは「AIエージェントのためのBDD」だと言えます。プログラミングなしで、自然言語で、AIの振る舞いを定義できます。Progressive Disclosure（段階的開示）Skillsの設計で最も重要な概念が「Progressive Disclosure（段階的開示）」です。これは「すべての情報を最初から渡すのではなく、必要になったタイミングで必要な情報だけを渡す」という設計原則です。なぜこの原則がSkillsに必要なのか。LLMには「コンテキストウィンドウ」という物理的な制約があります。Claude 3.5 Sonnetで約200Kトークン。これは多いようで、実際のタスクでは意外と消費が早い。コードファイルを10個読み込めば数万トークン、会話履歴が長くなればより消費される。ここに50個のSkillsの全内容（各5000トークン）を読み込んだら、250Kトークン。コンテキストが溢れます。だからSkillsは3段階で情報を開示します。これは「必要な時に必要な分だけ」というJust-In-Time戦略です。Level 1: メタデータ（常にロード） - 約100トークン/Skill---name: pdf-processingdescription: Extract text and tables from PDF files, fill forms, merge documents.---起動時に全Skillのname/descriptionだけ読み込みます。50個のSkillがあっても5000トークン程度。これでClaudeは「どんなSkillが使えるか」の全体像を把握できます。Level 2: 指示（トリガー時にロード） - 5000トークン以下が目安SKILL.mdの本文。「PDFを操作して」と言われたら、descriptionから「pdf-processingが関連する」と判断し、そのSKILL.mdを読み込みます。関係ないSkillは読み込まない。Level 3: リソース（必要時にロード） - 必要に応じて追加のファイル、スクリプト、リファレンス。SKILL.md内で「フォーム入力が必要ならFORMS.mdを参照」と書いておけば、そのタスクが発生したときだけ読み込みます。pdf-skill/├── SKILL.md              # Level 2: メイン指示├── FORMS.md              # Level 3: フォーム入力ガイド├── reference.md          # Level 3: APIリファレンス└── scripts/    └── fill_form.py      # Level 3: ユーティリティスクリプトこの設計の本質は「推論空間の段階的絞り込み」です。 Level 1で「使えるSkillの候補」を提示し、Level 2で「このタスクにはこの手順」を特定し、Level 3で「この具体的な操作にはこのリソース」を提供する。LLMの自由な推論を、段階的に制約していく。これがSkillsの賢さです。SkillsとSubagentsの使い分け「Skillsって、前に書いたSubagentsと同じじゃないですか」という声が聞こえてきます。確かに両方とも「専門知識をパッケージ化する」という点では似ています。しかし、コンテキストの扱い方に決定的な違いがあります。 観点  Skills  Subagents  コンテキスト  親と共有  独立  向いているタスク  継続的な作業、TDDなど  試行錯誤、調査タスク  状態の引き継ぎ  あり  なし（結果のみ返す） なぜこの違いが生まれるのか。 技術的には、Skillsは「現在のセッションにドキュメントを追加読み込みする」だけです。会話の流れ、ファイルの状態、変数の値、すべてが共有されたままです。一方、Subagentsは「新しいClaude Codeプロセスを起動する」に近い。独立したコンテキストウィンドウを持ち、親とは結果だけをやり取りします。Subagentsはコンテキストが独立しています。Claude Codeの中でClaude Codeを呼ぶようなもの。試行錯誤を伴うエラー調査みたいな「ごちゃごちゃした作業」をSubagentに任せると、親側のコンテキストが汚れません。なぜ「汚れない」ことが重要なのか。 コンテキストウィンドウは有限です。試行錯誤を10回繰り返すと、その10回分の履歴がコンテキストに残ります。成功した最終結果だけが欲しいのに、失敗した9回分も抱え込むことになる。Subagentなら、その試行錯誤は子プロセスの中で完結し、親には「結果：○○が原因でした」という要約だけが返ってきます。Skillsはコンテキストを共有します。テスト駆動開発をさせるとき、RED-GREEN-REFACTORのサイクルごとにコンテキストが分断されると困ります。「さっきテスト書いたよね」「なんでこの設計にしたんだっけ」という文脈を保持したまま作業を続けたい。そういうときはSkillsが向いています。なぜ「共有する」ことが重要なのか。 TDDは本質的に「対話」です。テストを書く→実装する→リファクタリングする、この流れの中で「なぜこのテストを書いたか」「なぜこの設計にしたか」という文脈が失われると、リファクタリングの方向性が定まりません。Skillsなら、この対話の文脈が保持されたまま、TDDの手順だけが注入されます。使い分けの判断基準はシンプルです。コンテキストを共有したい → Skillsコンテキストを独立させたい → Subagents迷ったときの指針: タスクの結果が「要約」で十分ならSubagent、結果だけでなく「過程」も重要ならSkillsです。エラー調査は「原因が分かればいい」のでSubagent。コードレビューは「なぜこの指摘をしたか」の文脈が後続の修正に影響するのでSkills。より詳しい使い分けについては、atusyさんの記事「Claude Codeのユーザー設定プロンプトを使い分けてコンテキスト管理を最適化する」が参考になります。利用可能なビルトインSkillsSkillsの概念は分かった。では、実際にどう使うのか。まずはAnthropicが提供しているビルトインSkillsから見てみましょう。 Skill  機能  PowerPoint (pptx)  プレゼンテーションの作成・編集・分析  Excel (xlsx)  スプレッドシートの作成・データ分析・チャート生成  Word (docx)  ドキュメントの作成・編集・トラック変更  PDF (pdf)  PDF生成・フォーム入力・マージ これはclaude.ai、Claude Code、Claude APIで利用可能です。基本的な使い方Claude Codeでの使い方Claude Codeでは、Skillsはファイルシステムベースで管理されます。配置場所： タイプ  パス  スコープ  個人  ~/.claude/skills/  全プロジェクト共通  プロジェクト  .claude/skills/  現在のプロジェクトのみ Skillを配置するだけで、Claude Codeが自動的に認識し、関連するタスクで使用します。APIでの使い方import anthropicclient = anthropic.Anthropic()response = client.beta.messages.create(    model=\"claude-sonnet-4-5-20250929\",    max_tokens=4096,    betas=[\"code-execution-2025-08-25\", \"skills-2025-10-02\"],    container={        \"skills\": [            {                \"type\": \"anthropic\",                \"skill_id\": \"pptx\",                \"version\": \"latest\"            }        ]    },    messages=[{        \"role\": \"user\",        \"content\": \"再生可能エネルギーについて5枚のプレゼンを作成して\"    }],    tools=[{        \"type\": \"code_execution_20250825\",        \"name\": \"code_execution\"    }])ポイントは以下の通りです。container.skillsでSkillを指定type: \"anthropic\"は公式Skilltoolsでcode_executionを有効化（Skillsの実行に必須）Beta headersが必要claude.aiでの使い方claude.aiでは、ビルトインSkillsはデフォルトで有効です。「PowerPointを作って」と言えば、自動的にPowerPoint Skillが起動します。カスタムSkillsは Settings \u003e Features からZIPファイルでアップロードできます。カスタムSkillの作成ここからが本番です。自分専用のSkillを作る方法を説明します。基本構造Skillの最小構成はSKILL.mdファイル1つだけです。---name: my-custom-skilldescription: このSkillが何をするか、いつ使うべきかを説明。---# My Custom Skill## 指示[具体的な手順をここに書く]## 例[実際の使用例]必須フィールド：name: 小文字とハイフンのみ、64文字以内description: 何をするのか、いつ使うのかを説明。1024文字以内実用的なSkill例私が実際のプロジェクトで使っているSkillsを紹介します。例1: セキュリティレビュー.claude/skills/reviewing-security/SKILL.md:---name: reviewing-securitydescription: \"OWASP API Security Top 10 (2023) と Rust セキュリティベストプラクティス。脆弱性検出。Use when: セキュリティ、脆弱性、OWASP、認証、認可、監査を依頼された時。\"---# セキュリティレビューOWASP API Security Top 10 (2023) と Rust セキュリティベストプラクティスに基づくレビュースキル。## OWASP チェック項目| ID | リスク | チェック内容 ||----|-------|-------------|| API1 | BOLA | tenant_id 検証、file_id との組み合わせ検証 || API2 | Broken Auth | gRPC メタデータ認証 || API3 | Property | レスポンスの不要情報 || API4 | Resource | ファイルサイズ制限、ページネーション |## Rust セキュリティ| 項目 | 検索パターン ||-----|-------------|| 依存関係脆弱性 | `cargo audit` || unsafe コード | `grep -rn \"unsafe {\" src/` || ハードコード認証情報 | `grep -rn \"(password\\|secret\\|api_key)\" src/` |descriptionに「Use when:」を明記しているのがポイントです。これでClaudeが「セキュリティレビューして」と言われたときに確実に起動します。例2: ビルドとテスト.claude/skills/building-and-testing/SKILL.md:---name: building-and-testingdescription: \"Rustプロジェクトのビルドとテスト実行。フォーマットチェック、lint、ユニットテスト、ビルド確認を一括実行。Use when: ビルド、テスト、cargo test、チェック、確認を依頼された時。\"---# ビルドとテスト## 実行手順1. フォーマットチェック: `cargo fmt --check`2. Lint実行: `cargo clippy -- -D warnings`3. ユニットテスト: `cargo test --workspace`4. ビルド確認: `cargo build --workspace`## 一括実行cargo fmt --check \u0026\u0026 cargo clippy -- -D warnings \u0026\u0026 cargo test --workspace \u0026\u0026 cargo build --workspaceシンプルですが、これだけで「テストして」と言えば毎回同じ手順を実行してくれます。例3: リファレンス参照型（QAチェック）Progressive Disclosureを活用して、参照ファイルを分割する例です。職種ごとにリファレンスを分けることで、必要な情報だけを読み込みます。.claude/skills/qa-check/├── SKILL.md└── reference/    ├── backend.md      # Rustバックエンドのチェック項目    ├── frontend.md     # フロントエンドのチェック項目    └── infra.md        # インフラのチェック項目.claude/skills/qa-check/SKILL.md:---name: qa-checkdescription: \"コードレビュー・QAチェック。職種別のベストプラクティスを適用。Use when: レビュー、QA、品質チェック、コードチェックを依頼された時。\"---# QAチェック職種別のリファレンスを参照してレビューを実施します。## リファレンス**Rust バックエンド** → See [reference/backend.md](reference/backend.md)**フロントエンド** → See [reference/frontend.md](reference/frontend.md)**インフラ** → See [reference/infra.md](reference/infra.md)## 実行手順1. 変更ファイルの拡張子・パスから対象領域を判定2. 該当するリファレンスを読み込む3. チェック項目に従ってレビュー実施4. 結果をCRITICAL/WARNING/INFOで分類して報告reference/backend.md（一部抜粋）:# Rust バックエンド QAチェック項目## エラーハンドリング- [ ] unwrap() を本番コードで使用していないか- [ ] Result型を適切に伝播しているか- [ ] カスタムエラー型を定義しているか## セキュリティ- [ ] SQLインジェクション対策（sqlxのバインドパラメータ使用）- [ ] 認証・認可のチェック漏れがないか- [ ] 機密情報のログ出力がないか## パフォーマンス- [ ] N+1クエリが発生していないか- [ ] 不要なclone()がないか- [ ] async/awaitの適切な使用「バックエンドのコードをレビューして」と言えばbackend.mdだけを読み込み、「インフラの設定をチェックして」と言えばinfra.mdだけを読み込みます。slash commandsとskillsの連携Skillsは自動で起動しますが、明示的に呼び出したいときもあります。そういうときはslash commandsと組み合わせると便利です。.claude/skills/git-commit/SKILL.md:---name: git-commitdescription: Stage meaningful diffs and create Conventional Commits with WHY-focused messages. Use when agent needs to commit code changes.---Execute `/git:commit` slash command.claude/commands/git/commit.md:# Git Commit変更をすべてコミットせずに、意味のある範囲でできるだけ小さくコミットする。commit logにはwhyを残す。...こうすると、Claudeが「コミットすべきだな」と判断したら自動でSkillが起動し、ユーザーが明示的に/git:commitを呼んでも同じ挙動になります。自動と手動の両方に対応できる設計です。Skillsのベストプラクティスなぜベストプラクティスが重要なのか。 Skillsは「書けば動く」ものではありません。書き方によって、起動率、出力の安定性、トークン効率が大きく変わります。ベストプラクティスは、多くの試行錯誤から導き出されたパターンです。これを知らずに始めると、同じ失敗を繰り返すことになります。1. 簡潔に書くコンテキストウィンドウは有限です。Claudeが既に知っていることを書く必要はありません。簡潔さが重要な理由は2つあります。 トークンが増えると問題が起きます。1つはコスト。APIの場合、入力トークンに課金されるので、冗長なSkillはそのまま支出増になります。もう1つは「ノイズ」。LLMは与えられた情報を全て考慮しようとします。本質的でない説明が多いと、重要な指示が埋もれて、出力品質が下がります。悪い例（冗長）：PDF (Portable Document Format) files are a common file format that containstext, images, and other content. To extract text from a PDF, you'll need touse a library. There are many libraries available for PDF processing...良い例（簡潔）：## Extract PDF textUse pdfplumber:---import pdfplumberwith pdfplumber.open(\"file.pdf\") as pdf:    text = pdf.pages[0].extract_text()---2. 自由度を適切に設定タスクの性質によって指示の具体性を変えます。自由度の設計が重要な理由は単純です。 LLMは指示が曖昧だと「創造的に解釈」します。コードレビューなら創造性は歓迎ですが、DBマイグレーションで創造性を発揮されると困ります。タスクの「リスク」と「多様性の価値」を天秤にかけて、自由度を決めます。高自由度（テキスト指示）: 複数のアプローチが有効な場合## Code Review1. Analyze structure2. Check for bugs3. Suggest improvements低自由度（具体的スクリプト）: 操作がデリケートな場合。## Database MigrationRun exactly this script:---python scripts/migrate.py --verify --backup---Do not modify the command.3. フィードバックループを入れる複雑なワークフローでは検証ステップを入れます。フィードバックループが必要な理由があります。 LLMは「確信を持って間違える」ことがあります。10ステップのワークフローを一気に実行させると、ステップ3でミスしても気づかずステップ10まで進みます。検証ステップを挟むことで、早期に問題を検出し、修正コストを下げられます。## Document Editing Workflow1. Make edits to XML2. **Validate immediately**: `python validate.py`3. If validation fails:   - Review error message   - Fix issues   - Validate again4. **Only proceed when validation passes**5. Pack the document4. ネストを深くしない参照ファイルはSKILL.mdから1階層までに留めます。深すぎると部分的にしか読まれません。ネストが問題になる理由があります。 LLMは「参照先をどこまで読むか」を自分で判断します。A→B→C→Dとネストしていると、Bまで読んでCは読まない、という判断をすることがあります。重要な情報がDにあると、それが無視される。情報はフラットに配置して、確実に読まれるようにします。悪い例：SKILL.md → advanced.md → details.md → actual_info.md良い例：SKILL.md├── advanced.md├── reference.md└── examples.md100+の実戦投入可能なSkillsコミュニティが既に多くのSkillsを公開しています。github.com人気カテゴリ：Document Processing: docx, pdf, pptx, xlsxDevelopment \u0026 Code Tools: MCP Builder, Webapp Testing, Changelog GeneratorData \u0026 Analysis: CSV Data Summarizer, Root Cause TracingBusiness \u0026 Marketing: Lead Research Assistant, Competitive Ads ExtractorCreative \u0026 Media: Canvas Design, Theme Factory, Image Enhancerよくある失敗と対策Skillsを作り始めると、最初は思い通りに動かないことが多いです。よくある失敗パターンとその対策をまとめました。 問題  原因  対策  Skillがトリガーされない  descriptionが曖昧  「何をするか」と「いつ使うか」を明記  コンテキスト不足  SKILL.mdに情報が足りない  参照ファイルを追加  トークン消費が多すぎる  Progressive Disclosureしてない  情報を複数ファイルに分割  出力が不安定  自由度が高すぎる  具体的なテンプレートや例を追加  スクリプトエラー  エラーハンドリングが甘い  スクリプト内で明示的にエラー処理 セキュリティ上の注意点Skillsはフルユーザー権限で実行されます。信頼できないソースのSkillsは使わないでください。チェックすべきポイントは以下です。全ファイルを監査: SKILL.md、スクリプト、リソースをすべて確認外部接続に注意: 外部URLへアクセスするSkillは特にリスクが高い自分で作る or 公式を使う: 基本的にこの2択Skillsの限界と現実ここまでSkillsの使い方やベストプラクティスを紹介してきました。しかし正直に言うと、Skillsは万能ではありません。実際にシステム化しようとすると、いくつかの困難にぶつかります。限界がある理由は明確です。 Skillsは「LLMに追加の情報を渡す」仕組みです。LLMの推論能力自体を向上させるわけではありません。どれだけ精緻なSkillを書いても、LLMが誤解することはあるし、予期しない振る舞いをすることもあります。これはLLMの本質的な不確実性に起因する問題で、Skillsでは解決できません。時間目安: 最初のSkillを動かすまで1-2時間かかります。descriptionの調整、手順の修正、再テストのサイクルが必要だからです。2個目以降は30分程度になります。週3回以上使うタスクでないと元が取れないので、投資対効果を考えて作りましょう。定義ファイル地獄Skillsを整備していくと、管理すべきファイルが膨大になります。私のプロジェクトでは、こんな構造になりました。.claude/skills/├── building-and-testing/├── running-integration-tests/├── running-e2e-tests/├── running-mutation-tests/├── managing-docker-dev/├── working-with-branches/├── implementing-issues/├── checking-pr/├── reviewing-security/├── reviewing-quality/├── using-rust-patterns/├── using-sqlx-patterns/├── handling-errors/└── ... (50個のSkillフォルダ)「地獄」になる理由は、 Skillsがコードと違って静的解析できないからです。どのSkillがどのタスクで起動するかは、実際に動かしてみないと分からない。コードなら「この関数はどこから呼ばれているか」を検索できますが、Skillsの時は「このSkillはいつ起動するか」をLLMの判断に委ねています。依存関係がブラックボックスになるのです。50個のSkillがあると、どれがどの場面で起動するのか把握しきれなくなります。「なんでこのSkillが動いたんだ」という状況が発生します。結局、設計者がすべてのSkillの挙動を把握していないといけません。これは隠れたコストです。descriptionの試行錯誤Skillが起動するかどうかはdescriptionの書き方次第です。しかし「どう書けば起動するか」は試してみないと分かりません。試行錯誤が必要な理由は、 LLMが「意味」で判断するからです。プログラムのように「この文字列が含まれていたら起動」という決定論的なルールではありません。「code review」と書いても、ユーザーが「PRを見て」と言ったらLLMが「これはcode reviewのことだ」と解釈するかどうかはLLM次第です。LLMの判断基準は私たちには見えません。# 起動しなかった例description: Helps with code review.# 起動した例description: Performs code review. Use when reviewing pull requests, checking code quality, or before merging.「いつ使うか」を明示的に書かないと、Claudeが「このSkillを使うべきだ」と判断してくれません。でも、どこまで具体的に書けばいいのか。書きすぎると他のタスクで起動しなくなるし、曖昧だと意図しない場面で起動します。この塩梅を見つけるのに時間がかかります。これはプロンプトエンジニアリングの本質的な難しさと同じです。 「こう書けば必ずこう動く」という保証がない世界で、試行錯誤を通じて「だいたいこう動く」パターンを見つけていく。Skillsは設定ファイルの形をしていますが、実態はプロンプトエンジニアリングです。デバッグの難しさ「なぜこのSkillが起動しなかったのか」を知る手段が限られています。Claude Codeは内部でどのSkillを候補として検討し、なぜそれを選んだか（選ばなかったか）を教えてくれません。デバッグが難しい理由は、 LLMの判断過程が外部から観察できないからです。プログラムならブレークポイントを置いて変数の中身を確認できますが、LLMには「なぜこの判断をしたか」を聞く標準的なインターフェースがありません。ログを見ても「Skill Xを起動しました」という結果しか分からず、「なぜSkill YではなくXを選んだのか」は分かりません。結果として、「起動しない → descriptionを変える → また試す」のループを繰り返すことになります。プログラムのデバッグと違って、再現性も低い。同じプロンプトでも起動したりしなかったりします。これはLLMベースのシステム全般の課題です。 Skillsに限った話ではありません。LLMの判断を制御したいなら、その不確実性と付き合う覚悟が必要です。Skill同士の競合複数のSkillが似たようなdescriptionを持っていると、どちらが選ばれるか予測できません。競合が起きる理由は、 Skillの選択がLLMの「意味的な類似度判断」に依存しているからです。プログラムなら「優先度」を数値で指定できますが、Skillsにはそういう明示的な優先度設定がありません。LLMが「どちらがより適切か」を毎回判断しますが、その判断基準はコンテキスト依存で変わります。# Skill Adescription: Reviews code for security issues.# Skill Bdescription: Performs security audit on codebase.「セキュリティチェックして」と言ったとき、AとBのどちらが起動するか。両方起動することもあります。Skillが増えるほど、こういう競合が起きやすくなります。対策としては、Skillの責務を明確に分離するしかありません。 「security issues」と「security audit」が重複しているなら、片方を削除するか、descriptionで「Use when: PRの差分をレビューするとき」vs「Use when: プロジェクト全体を監査するとき」のように用途を分けます。これは設計段階で意識する必要があります。「作る、試す、正す」で育てるここまで限界をいくつも挙げてきました。descriptionの試行錯誤、デバッグの難しさ、Skill同士の競合。これだけ聞くと「やっぱり使わない方がいいのでは」と感じるだろう。しかし、限界があるからといって諦める必要はありません。ここで参考になるのが、市谷聡啓氏の『作る、試す、正す。アジャイルなモノづくりのための全体戦略』です。この本の核心は、「正しさ」を探すのではなく、「正しくなる状況」をつくるというアプローチです。私たちの仕事は「正しいSkillを作る」ことではない。「ソフトウェアが正しくなっていく状況」をSkillで設計することです。つまり、Skillの完成度を追い求めるのではなく、適切なタイミングでSkillが発動し、結果としてソフトウェアが正しい方向に進む——その「状況」を整えることが本質です。作る → 試す → 正す → 作る → 試す → 正す → ...作る: 最小限のSKILL.mdを書く試す: 実際に使ってみて、期待通りに動くか確認する正す: 動かなかった部分を修正し、descriptionを調整する「課題を言葉で確認するだけでは分かった気になる」と市谷氏は指摘しています。Skillsも同様で、頭の中で設計を完璧にしようとしても限界があります。実際に動かしてみて初めて、「このdescriptionでは起動しない」「この手順では不十分」という発見が得られます。私も最初のSkillは散々でした。descriptionが曖昧すぎて起動しない、手順が抽象的すぎて出力がブレる。でも何度か直していくうちに、「こう書けば確実に起動する」「この粒度で手順を書けば安定する」という感覚が掴めてきました。Skillsの価値は「完成品」ではなく「育てるプロセス」にあります。 descriptionの試行錯誤を通じて、私たちはLLMの「判断基準」を逆算的に学んでいる。Skillsは単なる設定ファイルではなく、LLMの振る舞いを観察し理解するための実験装置でもあるのです。まとめAgent Skillsは、LLMのステートレスな制約を回避し、専門知識を必要な時に注入する仕組みです。今まで紹介してきた設定（CLAUDE.md、commands、Hooks、Subagents）と組み合わせれば、Claude Codeの使い勝手は大きく変わります。CLAUDE.md: プロジェクトの文脈commands: 手動ショートカットHooks: 自動実行Subagents: 専門家の自動呼び出しSkills: 専門知識の注入 ← NEWこれらの設定を組み合わせることで、Claude Codeは単なるAIアシスタントから、チームの一員のように振る舞うツールへと変わります。Skillsが示唆するAIエンジニアリングの未来では、この変化は何を意味するのか。Skillsが示唆するのは、「AIエージェントの制御は、プロンプトではなくワークフローで行う時代になった」ということです。従来のLLM活用は「良いプロンプトを書く」スキルが中心でした。しかしSkillsの登場で、パラダイムが変わりました。これからのAIエンジニアリングは、「LLMにどう推論させるか」ではなく、「LLMの推論をどう制約し、どう組織の資産として蓄積するか」が問われます。暗黙知として個人の頭の中にあったワークフローが、SKILL.mdという明示的なドキュメントになる。これはチーム全体で共有・改善できる「組織の資産」になります。Skillsは単なる便利ツールではなく、ワークフローの形式知化を促す仕組みでもあるのです。万能ではありません。descriptionの試行錯誤は避けられないし、Skillが増えると管理コストも上がります。でも「作る、試す、正す」のサイクルを回せば、確実に生産性は上がります。Claude Codeが雑魚なんじゃない、設定してないだけ。設定すればちゃんと動いてくれます。今日から試せること記事を読んで「面白そう」と思ったら、まずこれを試してみてください。1. ビルトインSkillsを体験する（1分）claude.aiで「PowerPointで自己紹介スライドを作って」と言ってみてください。Skillsが自動で起動して、プロ級のスライドが生成されます。2. カスタムSkillの最小構成を作る（5分）mkdir -p ~/.claude/skills/hello-skill~/.claude/skills/hello-skill/SKILL.mdを作成します。---name: hello-skilldescription: Says hello in a fun way. Use when user asks for a greeting.---# Hello SkillWhen asked to greet, respond with a creative and fun greeting.Include an emoji and a short motivational message.Claude Codeを再起動して「挨拶して」と言ってみてください。Skillが起動するはずです。3. 既存のSkillsを眺める（10分）awesome-claude-skillsで、他の人が作ったSkillsを見てみてください。SKILL.mdの書き方の参考になります。参考資料Agent Skills - Anthropic Engineering BlogAgent Skills Overview - Claude DocsAgent Skills Best Practices - Claude DocsUsing Skills with the API - Claude Docsawesome-claude-skills - ComposioHQClaude Skills CookbookClaude Codeのユーザー設定プロンプトを使い分けてコンテキスト管理を最適化する - atusyClaude Skillsとは何か - r_kaga振る舞い駆動開発（BDD）とは？ - HQW!作る、試す、正す。 - 市谷聡啓Claude Skillsとは何なのか？Use Agent Skills in VS Code","isoDate":"2025-12-19T08:33:09.000Z","dateMiliSeconds":1766133189000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「自分の環境では動く」から解放される Nix Flake ","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/18/111500","contentSnippet":"はじめに「自分の環境では動くんだけど...」という言葉を、何度聞いたことがあるだろうか。開発環境の差異は、これまで「手順書」「Docker」「asdf/anyenv」で解決を試みてきたが、いずれも時間経過で破綻する。手順書は陳腐化し、Dockerfileのベースイメージは変わり、asdfは言語ごとにツールが分散する。問題の本質は「環境の固定」ではなく「依存関係の完全な追跡」にあった。これを根本から解決するのが、純粋関数型パッケージマネージャ「Nix」と、その最新機能「Nix Flake」だ。これらの課題感については Infrastructure as Code, 3rd Edition が詳しく論じており、参考になる。2025年 俺が愛した本たち 技術書編 に入れれていなくて悲しいほどよい書籍である。オライリー・ジャパンさん 自分は翻訳の準備できてます！！！Infrastructure as Code: Designing and Delivering Dynamic Systems for the Cloud Age (English Edition)作者:Morris, KiefO'Reilly MediaAmazon本記事では、Nix Flake を使った開発環境の統一について、Docker との比較を交えながら包括的に解説する。実際に複数言語のプロジェクトで検証した結果も含めて、実践的な導入方法を紹介する。この記事で分かることNix Flake の基本概念と従来の Nix との違いDocker と Nix の使い分け・組み合わせ方各プログラミング言語（Rust, Go, Python, TypeScript）での開発環境の構築方法CI/CD との統合方法と direnv による自動環境切り替えNix とは何か純粋関数型パッケージマネージャNix は、従来のパッケージマネージャ（apt, brew, npm など）とは根本的に異なるアプローチを取る。その核心は「純粋関数型」（入力が同じなら出力も必ず同じになる仕組み）という概念にある。数学の関数と同様に、Nix では「同じ入力からは常に同じ出力が得られる」。パッケージのビルドに必要な全ての依存関係を明示的に指定し、外部環境に依存しない閉じた環境でビルドを行う。この仕組みにより、以下が保証される。再現性: 誰がいつどこでビルドしても、同じ結果が得られる分離性: システムの既存環境を汚さない共存性: 同じパッケージの異なるバージョンが同時に存在できるnixos.orgハッシュベースの依存管理Nix は全てのパッケージを /nix/store/ 以下にハッシュ付きで保存する。例えば、Node.js 20.10.0 は以下のようなパスに保存される。/nix/store/abc123...-nodejs-20.10.0/このハッシュは、パッケージのソースコード、ビルドスクリプト、全ての依存関係から計算される。つまり、依存関係が少しでも異なれば、異なるハッシュ（異なるパス）になる。これにより、バージョン競合が原理的に発生しない。Nix の核心概念Nix を理解するには、いくつかの重要な概念を押さえておく必要がある。Derivation（導出）Derivation はビルドレシピのようなもので、Nix の中核概念だ。「既存の store object から新しい store object を生成する純粋関数」と捉えれば理解しやすい。ビルドは sandboxed プロセスとして実行され、指定された入力のみを読み込み、決定論的に出力を生成する。Store（ストア）Store は /nix/store/ に存在するオブジェクトの集合だ。全てのパッケージ、ビルド成果物、依存関係がここに保存される。Store は不変（immutable）であり、一度書き込まれたオブジェクトは変更されない。Store PathStore path は store object の一意な識別子だ。例えば以下のような形式になる。/nix/store/a040m110amc4h71lds2jmr8qrkj2jhxd-git-2.38.1この長い文字列（a040m110...）は、パッケージの全ての入力から計算されたハッシュだ。入力が変われば、パスも変わる。これが Nix の再現性を支える基盤となっている。Realise（実現化）Realise は derivation を実際にビルドし、store path を valid な状態にすることだ。既にキャッシュにあればダウンロードされ、なければビルドが実行される。これらの概念については、公式マニュアルと用語集で詳しく解説されている。nix.devnix.devNix Flake とはFlake の基本構造Nix Flake は、Nix の最新機能であり、プロジェクトの依存関係を宣言的に管理する仕組みだ。従来の Nix には2つの問題があった。(1) NIX_PATH や \u003cnixpkgs\u003e などグローバルな状態に依存し、マシンごとに異なる結果を生む可能性があった。(2) 依存関係のバージョンを固定する標準的な方法を欠いていた。nix-channel の更新で環境が変わってしまうのだ。Flake は flake.lock でこれらを解決する。project/├── flake.nix          # プロジェクト定義├── flake.lock         # 依存関係のロックファイル└── src/               # ソースコードflake.nix は以下の構造を持つ。{  description = \"プロジェクトの説明\";  inputs = {    # 依存する外部 Flake を定義    nixpkgs.url = \"github:nixos/nixpkgs?ref=nixpkgs-unstable\";  };  outputs = { self, nixpkgs }: {    # 出力（devShells, packages, etc.）を定義  };}flake.lock による再現性flake.lock は npm の package-lock.json や Rust の Cargo.lock に相当する。全ての依存関係のコミットハッシュが固定されるため、時間が経っても同じ環境を再現できる。{  \"nodes\": {    \"nixpkgs\": {      \"locked\": {        \"lastModified\": 1702312524,        \"narHash\": \"sha256-...\",        \"rev\": \"abc123...\",        \"type\": \"github\"      }    }  }}Flake についての詳細は NixOS Wiki を参照してほしい。nixos.wikiDocker / コンテナエコシステムとの比較Nix と Docker は競合ではなく補完関係にある。Nix は「ビルド時の再現性」を、Docker は「ランタイムの分離とデプロイ」を担う。各ツールとの関係 ツール  役割  Nix との関係  Dockerfile  イメージビルド  Nix で置き換え可能（より再現性が高い）  Docker Compose  マルチコンテナ構成  devenv/process-compose で補完  Kubernetes  コンテナオーケストレーション  Nixidy/kubenix で統合可能  Helm  K8s パッケージ管理  nix-helm で Nix から利用可能  Skaffold  開発ワークフロー自動化  ビルドフェーズで Nix を使用可能 Dockerfile の課題と Nix の解決策Dockerfile は広く普及しているが、再現性に課題がある。# Dockerfile: 再現性の問題FROM python:3.12  # タグは可変RUN apt-get update \u0026\u0026 apt-get install -y curl  # バージョン固定なしRUN pip install requests  # バージョン固定なし# Nix: 完全な再現性{  packages.docker-image = pkgs.dockerTools.buildImage {    name = \"my-app\";    copyToRoot = pkgs.buildEnv {      name = \"image-root\";      paths = [ pkgs.python312 pkgs.curl pkgs.python312Packages.requests ];    };  };}Nix の優位点:- ビット単位で同一の結果を保証- 全ての依存を明示的に管理（暗黙の依存が混入しない）- パッケージ単位の効率的なキャッシュ- SBOM（Software Bill of Materials）の自動生成blog.replit.comwww.devzero.ioNix + Docker の組み合わせ両者を組み合わせることで「再現可能なビルド」と「ポータブルなデプロイ」を両立できる。{  packages.docker-image = pkgs.dockerTools.buildLayeredImage {    name = \"my-app\";    tag = \"latest\";    contents = [ myApp pkgs.cacert ];    config.Cmd = [ \"/bin/my-app\" ];  };}各依存パッケージが独立したレイヤーになるため、パッケージAを更新してもパッケージBのレイヤーは再利用される。Dockerfile を書く必要がなく、Nix の宣言的な記述で完結する。flox.devKubernetes との統合: NixidyNixidy は Nix と Argo CD を組み合わせた GitOps ツールで、クラスター全体を NixOS のように管理できる。{  applications.nginx = {    namespace = \"default\";    helm.releases.nginx = {      chart = inputs.nixhelm.chartsDerivations.nginx;      values = { replicaCount = 3; service.type = \"LoadBalancer\"; };    };  };}nixidy.dev近年、ソフトウェアサプライチェーンのセキュリティが重視されている。ビルドの再現性と依存関係の透明性は「必須」になりつつある。Nix はビルドプロセス全体を宣言的に記述するため、SBOM の自動生成と来歴の追跡が容易だ。thenewstack.io実践：複数言語での開発環境構築flake-parts によるモジュール化複雑な Flake を管理しやすくするために、flake-parts を使う。これは NixOS モジュールシステムの考え方を Flake に適用したもので、設定を複数ファイルに分割できる。{  inputs = {    nixpkgs.url = \"github:nixos/nixpkgs?ref=nixpkgs-unstable\";    flake-parts.url = \"github:hercules-ci/flake-parts\";    treefmt-nix.url = \"github:numtide/treefmt-nix\";  };  outputs = { flake-parts, ... }@inputs:    flake-parts.lib.mkFlake { inherit inputs; } {      imports = [ inputs.treefmt-nix.flakeModule ];      systems = [ \"aarch64-darwin\" \"aarch64-linux\" \"x86_64-linux\" ];      perSystem = { config, pkgs, ... }: {        devShells.default = pkgs.mkShell {          packages = with pkgs; [            nodejs_22            config.treefmt.build.wrapper          ];        };        treefmt = {          projectRootFile = \"flake.nix\";          programs.prettier.enable = true;          programs.nixfmt.enable = true;        };      };    };}flake.partsRust 開発環境Rust プロジェクトでは、rust-overlay を使う。rustupなしで stable/nightly を切り替えられる。rust-analyzer や clippy も flake.nix で宣言的に管理できる。{  inputs.rust-overlay.url = \"github:oxalica/rust-overlay\";  perSystem = { pkgs, system, ... }:    let      overlayPkgs = import inputs.nixpkgs {        inherit system;        overlays = [ inputs.rust-overlay.overlays.default ];      };      rustToolchain = overlayPkgs.rust-bin.stable.latest.default.override {        extensions = [ \"rust-src\" \"rust-analyzer\" \"clippy\" ];      };    in {      devShells.default = pkgs.mkShell {        packages = [          rustToolchain          pkgs.cargo-watch          pkgs.cargo-edit        ];      };    };}github.comGo 開発環境{  devShells.default = pkgs.mkShell {    packages = with pkgs; [      go      golangci-lint      gopls      delve    ];    env = {      CGO_ENABLED = \"0\";    };  };}Python 開発環境Python では uv との組み合わせを推奨する。Nix で Python 本体と uv を提供し、パッケージ管理は uv に任せる。pyenv/venv/pip の組み合わせより高速で、依存解決も確実だ。{  devShells.default = pkgs.mkShell {    packages = with pkgs; [      python312      uv      ruff      pyright    ];    env = {      UV_PROJECT_ENVIRONMENT = \".venv\";    };  };}マルチ言語プロジェクト1つの Flake で複数の開発環境を提供できる。{  devShells = {    default = pkgs.mkShell {      packages = [ rustToolchain pkgs.go pkgs.nodejs_22 ];    };    rust = pkgs.mkShell { packages = [ rustToolchain ]; };    go = pkgs.mkShell { packages = [ pkgs.go ]; };    nodejs = pkgs.mkShell { packages = [ pkgs.nodejs_22 ]; };  };}使用時は以下のように選択できる。nix develop        # デフォルト（全言語）nix develop .#rust # Rust のみnix develop .#go   # Go のみ様々な言語向けのテンプレートが dev-templates リポジトリで公開されている。github.comdirenv との連携direnv とはdirenv は、ディレクトリごとに環境変数を自動で切り替えるツールだ。.envrc ファイルを配置したディレクトリに入ると自動的に環境がロードされ、離れるとアンロードされる。direnv.netnix-direnv のセットアップNix Flake と direnv を連携させるには、nix-direnv が必要だ。実際にセットアップした手順を紹介する。1. nix-direnv のインストール# Nix profile でインストールnix profile install nixpkgs#nix-direnv# インストール確認ls ~/.nix-profile/share/nix-direnv/# direnvrc が存在することを確認2. direnvrc の設定~/.config/direnv/direnvrc に以下を追加する。# nix-direnv を使用して Nix Flake 環境を高速にロード# キャッシュにより、シェル起動時の遅延を大幅に削減if [ -f \"$HOME/.nix-profile/share/nix-direnv/direnvrc\" ]; then  source \"$HOME/.nix-profile/share/nix-direnv/direnvrc\"elif [ -f \"/nix/var/nix/profiles/default/share/nix-direnv/direnvrc\" ]; then  source \"/nix/var/nix/profiles/default/share/nix-direnv/direnvrc\"elif [ -f \"/run/current-system/sw/share/nix-direnv/direnvrc\" ]; then  source \"/run/current-system/sw/share/nix-direnv/direnvrc\"fi3. シェルへの hook 追加使用しているシェルに応じて設定を追加する。# bash (~/.bashrc)eval \"$(direnv hook bash)\"# zsh (~/.zshrc)eval \"$(direnv hook zsh)\"# fish (~/.config/fish/config.fish)direnv hook fish | sourcegithub.comプロジェクトでの使用1. .envrc ファイルの作成プロジェクトルートに .envrc を作成する。# .envrc - 基本的な使い方use flakeより詳細な設定も可能だ。# .envrc - 詳細な設定例# nix-direnv を使用（高速・キャッシュ対応）use flake# 特定の devShell を使用する場合# use flake .#rust# 追加の環境変数export EDITOR=\"nvim\"export MY_PROJECT_ENV=\"development\"2. direnv の許可セキュリティのため、初回は明示的に許可が必要だ。cd my-projectdirenv allow動作確認実際に動作を確認した結果を示す。# direnv のステータス確認$ direnv statusdirenv exec path /opt/homebrew/bin/direnvDIRENV_CONFIG /Users/nwiizo/.config/direnvFound RC path /path/to/project/.envrcFound RC allowed 0Found RC allowPath /Users/nwiizo/.local/share/direnv/allow/...nix-direnv のキャッシュ機構nix-direnv は .direnv/ ディレクトリにキャッシュを作成する。実際のキャッシュ構造は以下のようになる。.direnv/├── bin/                    # 一時的なバイナリラッパー├── flake-inputs/           # 入力 Flake のキャッシュ├── flake-profile-*         # Nix Store へのシンボリックリンク└── flake-profile-*.rc      # 環境変数のキャッシュ（約86KB）キャッシュの効果flake-profile-* は Nix Store の実際のパッケージを指す例: /nix/store/l5rhpr6i98h3kvydy6gww5cvszmqi05a-nix-shell-env2回目以降のロードは数ミリ秒で完了nix-collect-garbage でもキャッシュは保護されるnix-direnv vs 標準 direnv 観点  nix-direnv  標準 direnv + use nix  初回ロード  同等（ビルドが必要）  同等  2回目以降  数ミリ秒  数秒〜数十秒  GC 耐性  保護される  削除される可能性  Flake 対応  ネイティブ  追加設定が必要  キャッシュサイズ  〜100KB/プロジェクト  なし マルチ言語プロジェクトでの設定複数の devShell を持つプロジェクトでは、以下のように使い分けられる。# .envrc# デフォルトで全言語環境をロードuse flake# または特定の言語環境のみロードする場合:# use flake .#rust# use flake .#go# use flake .#python# use flake .#nodejsトラブルシューティングdirenv が反応しない# シェルフックが設定されているか確認which direnvdirenv status# 許可されているか確認direnv allow環境がロードされない# .envrc の構文エラーをチェックdirenv edit# キャッシュをクリアして再構築rm -rf .direnvdirenv allowFlake が見つからない# flake.nix が Git に追加されているか確認git status flake.nixgit add flake.nix flake.lockDeterminate Systems のブログでは、direnv と Nix の連携について詳しく解説されている。determinate.systemsCI/CD との統合GitHub Actions での使用name: CI with Nix Flakeon: [push, pull_request]jobs:  build:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v4      # Nix インストール（Determinate Systems 推奨）      - uses: DeterminateSystems/nix-installer-action@main      # Magic Nix Cache でビルドを高速化      - uses: DeterminateSystems/magic-nix-cache-action@main      # Flake のチェック      - run: nix flake check      # フォーマットチェック      - run: nix develop --command treefmt --ci      # ビルド      - run: nix buildgithub.comCachix によるバイナリキャッシュCI でビルドした成果物を Cachix にプッシュすると、他の開発者やCI環境ではビルド済みバイナリをダウンロードするだけで済む。ビルド時間が大幅に短縮される。- uses: cachix/cachix-action@v15  with:    name: your-cache    authToken: '${{ secrets.CACHIX_AUTH_TOKEN }}'overlay によるカスタマイズパッケージのカスタマイズoverlay を使うと、既存のパッケージをカスタマイズしたり、独自のパッケージを追加したりできる。{  customOverlay = final: prev: {    # 既存パッケージをラップ    myGit = prev.writeShellScriptBin \"git\" ''      exec ${prev.git}/bin/git -c init.defaultBranch=main \"$@\"    '';    # カスタムスクリプト    project-init = prev.writeShellScriptBin \"project-init\" ''      echo \"Initializing project...\"      ${prev.git}/bin/git init      echo \"# New Project\" \u003e README.md    '';  };}treefmt による統一フォーマット複数言語のフォーマッターを1つのコマンドで実行できる。{  treefmt = {    projectRootFile = \"flake.nix\";    programs = {      nixfmt.enable = true;      rustfmt.enable = true;      gofmt.enable = true;      prettier.enable = true;      ruff-format.enable = true;    };  };}treefmt      # 全ファイルをフォーマットtreefmt --ci # CI でのチェック（変更があればエラー）github.comトラブルシューティングexperimental-features エラーerror: experimental Nix feature 'nix-command' is disabled~/.config/nix/nix.conf に以下を追加する。experimental-features = nix-command flakesnix develop が遅い初回は依存関係のダウンロードとビルドに時間がかかる。2回目以降はキャッシュが効くため高速だ。Cachix を使うとより高速化できる。direnv が無限ループするFish shell を使っている場合、shellHook で exec fish を呼ばないように注意する。Flake が見つからないFlake ファイルは Git に追加されている必要がある。未追跡ファイルは Nix から見えない。git add flake.nix flake.lockまとめNix Flake を導入することで、開発環境の「再現性」「分離性」「共有性」を根本から改善できる。Docker とは競合ではなく補完関係にあり、両者を組み合わせることで「再現可能なビルド」と「ポータブルなデプロイ」を両立できる。導入の主なメリットをまとめる。開発環境のセットアップが nix develop の1コマンドにチーム全員が同じツールバージョンを使用CI と開発環境の乖離がなくなるフォーマットの一貫性を自動で保証Docker イメージのビルドも再現可能に学習コストは確かに高い。Nix言語の習得やStore/Derivationの概念理解には時間がかかる。しかし一度導入すれば、環境構築が1コマンドで完了する。「環境差異によるバグ」が原理的になくなり、CIと開発環境が同一になる。特に複数言語プロジェクトでは、rustup/pyenv/nvm/goenvの個別管理から解放され、単一のflake.nixで全ての言語ツールチェーンを統一できる。まずは小規模なサイドプロジェクトで試してみてほしい。nix flake init -t github:the-nix-way/dev-templates#rust ですぐに始められる。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。","isoDate":"2025-12-18T02:15:00.000Z","dateMiliSeconds":1766024100000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2025年版 私がAIエージェントと協働しながら学習する方法","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/17/121705","contentSnippet":"労働こそが最高の学習だったあなたは最近、「成長している」と感じているだろうか。かつて、プログラマーにとって、労働こそが最高の学習の場だった。なぜか。労働には「摩擦」があったからだ。エラーが出る。原因がわからない。仮説を立てる。試す。失敗する。また試す。この摩擦の中で、経験が意味に変わっていた。労働は、経験を意味に変換する装置だった。以前の開発を思い出す。新しいフレームワークを覚えなければならない。エラーと格闘して、ドキュメントを読み漁って、やっと動いたとき。あの達成感は、単なる満足ではなかった。「なぜ動かなかったか」「どう直したか」「次に同じ問題が起きたらどうするか」——この因果の記憶が、脳に刻み込まれていた。困難を乗り越えた記録が、自分の中に残っていた。Claude Codeで開発している今、コードは書ける。動く。レビューも通る。Claude CodeはAnthropicが提供するAIエージェント型の開発ツールで、ターミナル上で動作し、コードの生成・編集・実行・デバッグまでを自然言語で指示できる。従来の「コード補完」とは次元が違う。プロジェクト全体を理解し、ファイルを横断して作業し、テストまで書いてくれる。開発のあり方が、根本から変わった。しかし、その便利さの裏で、何かがおかしくなっていた。コードは書ける。動く。レビューも通る。でも、後から「なぜそう書いたの？」と聞かれても、答えに淀む。自分が責任を持って出力したコードのはずなのに、説明しようとすると言葉が出てこない。因果を辿れない。「なぜこの実装なのか」「他の選択肢は何だったか」「どこで判断したか」——この記憶がない。このままでは「実装ガチャ」を回し続けるだけの存在になってしまう。先日、それを痛感する出来事があった。本番環境で障害が起きた。自分が2週間前に実装した機能だ。ログを見る。エラーメッセージを読む。でも、原因の見当がつかない。「この処理、どういう順序で動くんだっけ」と考える。思い出せない。自分で書いたコードなのに、頭の中でトレースできない。因果がわからない。結局、AIにコードを貼り付けて「このエラーの原因は？」と聞いた。答えは返ってきた。直った。しかし、自分では何も解決していない。2週間前の自分が書いたコードを、今日の自分が理解できていなかった。もしかして、あなたも同じ感覚を持っていないだろうか。最初は自分を責めた。集中力が落ちたのか。学習能力が衰えたのか。でも違った。労働と学習が、分離した。摩擦が消えた。経験が意味に変わる機会が消えた。厄介だったのは、見せかけ上の生産性は上がっていたことだ。タスクは消化されている。アウトプットも出ている。しかし、3ヶ月前にやった案件の技術スタックを聞かれても、ほとんど思い出せない。生産性は上がった。成長は止まった。労働から学習が抜け落ちていた。「成長していない」と感じるとき、私たちは何を失っているのか。成果と経験と理解は、同じものだろうか。違う。成果は外に出たもの。経験は時間の中で起きたこと。理解は内側に残ったもの。成果が出ても、経験を積んでも、理解が残らなければ成長は感じられない。失われているのは「苦労」ではない。「プロセスの記憶」だ。自分が何を考え、どこで躓き、どう乗り越えたか。この記憶が消えている。AIが生成したコードは動く。でも、そこに至るまでに自分が何を試し、何を捨て、何を選んだか——その記憶がない。なぜ過去の仕事を説明できないと不安になるのか。説明できないということは、プロセスの記憶がないということだ。記憶がないということは、自分の中で何も変化が起きていないということだ。成長実感とは、能力の増加ではない。自分の内部で変化が起きたと確認できる手応えだ。では、記憶に残らない仕事は価値がないのか。そうではない。成果としての価値はある。でも、自分を成長させる価値はない。成果は外に残る。成長は内に残る。両者は別物だ。これは集中力の問題ではなかった。前回の記事「2025年版 私がAIエージェントと協働しながら集中する方法」で書いた微観法は、集中の持ち方を変えてくれた。でも、学習の問題は別だった。集中できても、学べていなかったのだ。syu-m-5151.hatenablog.com『信長の野望』をやっているのに『戦国無双』のような強さを求めるのは違う、と言われるかもしれない。地道な内政と、爽快なアクション。求めているものが違う。でも正直なところ、私たちはいつだってエンジニアなのでエンジニア領域で無双したいのです。AI時代の3つの非対称性ここまで、私個人の経験として「労働と学習の分離」を語ってきた。でも、ここまで読んで、「これは自分だけの問題かもしれない。単に自分の学び方が下手なだけでは？」と思った人もいるだろう。そうではない。これは個人の問題ではない。構造の問題だ。なぜ労働と学習が分離してしまったのか。その構造を理解するには、AIがもたらした3つの非対称性を見る必要がある。詳しくは別の記事で書いたが、ここでも簡単に触れておきたい。syu-m-5151.hatenablog.com第一の非対称性：生産と理解の乖離。AIでコードを書く速度は上がった。でも、そのコードを修正しようとすると、予想以上に時間がかかる。システム内にコードが流入する速度と、人間がそれを理解する速度の間に、決定的なギャップが生まれている。第二の非対称性：生産量と成長の乖離。AIを使えば、経験1年目でも大量のコードを生産できる。PRの数も増える。でも半年後、1年後、エンジニアとしての地力はどうなっているだろうか。問題を自分で分析し、設計を考え、トレードオフを検討するプロセス。これがエンジニアの地力を育てる。AIに頼りすぎると、この思考プロセスそのものを外部化してしまう。第三の非対称性：経験の量と学びの質の乖離。毎日AIを使って100行のコードを書く経験を1年積んでも、そこから「AIへの依存」しか学ばなければ、地力にはつながらない。「何を経験したか」ではなく、「そこから何を学んだか」が重要なのだ。この3つの非対称性は、1つのシステムとして機能している。速く書けることを追求すれば、理解が追いつかなくなる。理解しないまま大量に生産すれば、思考力が育たない。経験を積んでも、そこから学ばなければ、成長は起きない。根底にあるメンタルモデル—「速さが価値」「量が成果」「経験が成長」—を変えない限り、どんな対症療法も一時的な効果しか生まない。ここまでで、外部から見た構造——AIと人間の関係性——は理解できた。しかし、これだけでは「なぜ学べないのか」の本当の理由は見えてこない。構造は外側の話だ。学習が起きるのは、私たちの脳の内側だ。では、この構造が私たちの脳に何をしているのか。もう少し掘り下げてみよう。脳が「処理」していない構造の問題は、最終的に脳の問題に帰着する。なぜ知識が残らないのか。しばらく自分を観察してみた。気づいたのは、AIエージェントと働いていると、認知的負荷が下がりすぎるということだ。認知的負荷とは、頭を使う度合いのことだ。問題を解くとき、脳は情報を処理し、比較し、判断する。この「頭を使う」プロセスが、認知的負荷を生む。負荷が下がること自体は、一見良いことに思える。楽に仕事ができる。疲れにくい。でも、学習の観点からは最悪だった。脳は適度な負荷がかからないと、情報を長期記憶に格納しない。「苦労せずに得た情報」は、脳にとって重要度が低いと判断される。楽に得た知識は、楽に消える。では、認知的負荷はどこからが「害」になるのか。問題は量ではない。質だ。認知的負荷には種類がある。タイピングの負荷。構文を思い出す負荷。そして、比較・判断・仮説といった意味処理の負荷。AIが減らしてくれるのは、すべての負荷だ。だが、害になるのは意味処理の負荷が消えたときだ。なぜ脳は負荷がないと学ばないのか。脳は「重要でない」と判断した情報を捨てる。重要かどうかの判断基準は、処理にかかった負荷だ。苦労して得た情報は重要。楽に得た情報は重要でない。意味処理の負荷が消えた瞬間、脳は「これは覚えなくていい」と判断する。記憶も学習も、起こらなくなる。「ちょうどよい負荷」は誰が決めるのか。AIではない。あなただ。負荷をAIに外注すると、脳は怠ける。怠けた脳は弱くなる。認知的負荷は削減対象ではない。設計対象だ。どの負荷を残し、どの負荷を外注するか。その設計を自分でしなければ、学習は起こらない。楽になることと、考えなくなることは同じか。違う。作業が楽になるのはいい。思考が楽になるのは危険だ。手を動かす負荷は減らしていい。意味を処理する負荷は、意図的に残せ。以前のプログラミングを思い出す。エラーが出る。ググる。ブログや公式ドキュメントを読む。試す。またエラー。別の方法を試す。やっと動く。このプロセス全体が、学習だった。途中で「わからない」状態に耐える必要があった。その「耐える時間」が、脳を鍛えていた。記憶を定着させていた。今はどうか。エラーが出る。AIに投げる。答えが返ってくる。動く。終わり。プロセスが消えた。プロセスが死ぬと、学習も死ぬ。考えてみてほしい。あなたが最後に「わからない」と感じたのは、いつだろうか。私は本当に思い出せなかった。AI時代において、「わからない」が絶滅したのだ。聞けば答えが返ってくる。どんな質問にも、それらしい回答が生成される。以前は「わからない」状態で立ち止まり、悩み、調べ、試行錯誤した。その時間が学習だった。今は「わからない」と感じる前に、答えが手に入る。「わからない状態」に耐える力こそ、学習に不可欠だ。わからない状態は不快だ。不確実性は脳にストレスを与える。だから、答えを求める。AIはその欲求を即座に満たしてくれる。だが、「わからない」は単なる欠如ではない。意味を構築するための空白だ。以前の学習を思い出してほしい。エラーが出て、原因がわからない。ドキュメントを読んでも、ピンとこない。仮説を立てて、試して、また失敗する。その空白の中で、脳は問題を構造化していた。何がわかっていて、何がわかっていないか。どこまでは正しくて、どこからが怪しいか。仮説を立て、壊し、更新する。このプロセスを通じて、人は「思考の型」と「判断の軸」を獲得してきた。わからない経験がなくなると、思考の型が育たない。問題をどう分解するか。仮説をどう立てるか。どの順番で検証するか。これは、わからない状態を何度も経験することでしか身につかない。答えが即座に与えられる世界では、この「思考の筋トレ」そのものが消える。AIはわからないと言わない。常に何かを返す。それが正しいかどうかは別として。人間だけが「わからない」を経験できる。その経験を捨てるのは、思考力を捨てることだ。でも、その「すぐわかる」が、実は思考力を奪っていた。自分一人で「じっくり」考える時間が消えた。わからないまま考え続ける能力。不確実性の中にとどまる能力。それが失われていく。「わからない」を経験しないまま、「わかった」に到達してしまう。ここまでは「わからない状態」の話だった。つまり、問題に直面したとき、AIがすぐに答えを出してしまうから、自分で考える時間がなくなるという話だ。でも、認知的負荷が下がる問題は、これだけではない。AIの答えを受け取って「わかった」と思った後にも、別の問題がある。コードへの解像度が下がるのだ。以前、自分で書いたコードは、補完もありながらちゃんと打ち込んでいた。変数名を決めるときに悩んだ。ループの終了条件を頭の中でシミュレートした。このif文の分岐は、こういうケースでtrueになる。この変数には、この時点でこの値が入っている。コードは指先から脳へ流れ込んでいた。AIが生成したコードは、目で見ているだけだ。なんとなく動く気がするから動かす。動く。テストも通る。でも、変数の1つに至るまで把握しているかと言われると、怪しい。コードが「通過」していく感覚。身体に染み込んでいない。見ているのに、触れていない。この違和感の正体は何か。自分で書いたコードと、AIが書いたコードは何が違うのか。結果は同じだろう。動作も同じだろう。でも、因果関係を自分で通ったかどうかが違う。自分で書いたコードには、因果の記憶がある。「この変数名、最初はdataにしようと思った。でも、後から読んだときに意味がわからなくなると思って、userResponseに変えた」。「このループ、最初はforで書いた。でも、副作用がないからmapの方がきれいだと思って書き直した」。迷い、選択し、決断した記憶。その因果を自分で通った記憶が、コードを「理解している」という感覚を生む。AIが生成したコードには、この因果がない。結果だけがある。「なぜこの変数名なのか」「なぜこの書き方なのか」。AIには理由があるのだろう。でも、その理由を自分で通っていない。書く・迷う・選ぶという行為を経ていないコードは、頭の中で「実行」されていない。なぜ説明できないと不安になるのか。説明できないということは、因果を再構成できないということだ。因果がわからないコードは、壊れたとき直せない。変更したとき、何が起きるか予測できない。「動くこと」と「わかること」は別だ。動くことは確認できる。わかることは、因果を辿れるかどうかで決まる。理解とは知識量ではない。因果を身体でトレースできるかどうかだ。「見ているが触っていない」とは、この状態だ。視覚的には認識している。でも、因果を身体で通過していない。だから、記憶に残らない。応用が利かない。自分のものにならない。解像度が低い理解は、何を引き起こすのか。判断ができなくなる。「この実装でいいのか」「この変更は安全か」。判断には、因果の理解が必要だ。因果がわからなければ、判断できない。判断できない人間は、AIの出力を受け入れるしかない。私は自分で書いたコードは、書く過程で何度も頭の中で実行している。「この変数がnullだったらどうなる」「このループは何回まわる」「この関数の戻り値は何型か」。無意識に検証している。AIが生成したコードには、この検証プロセスがない。結果、コードの「解像度」が違う。自分で書いたコードは、ズームインしてもくっきり見える。AIが生成したコードは、全体像はわかるが、細部がぼやけている。動くことは知っている。なぜ動くかは、よくわからない。解像度が低いと、記憶にも残りにくい。ぼんやりした情報は、脳に定着しない。そしてもう1つ、記憶を弱くする要因がある。解像度の問題とは別に、「思い出す」作業をしていないのだ。記憶を定着させるには、能動的に思い出す作業が必要だ。一度覚えたことを、何も見ずに思い出す。その「引き出す」作業が、記憶を強化する。でもAIと働いていると、思い出す必要がない。わからないときは聞けばいい。脳が「引き出す」練習をしなくなった。筋トレと同じだ。重いものを持ち上げないと筋肉はつかない。代わりに機械が持ち上げてくれたら、楽だけど、筋肉は衰える。AIは脳の代行業者だ。頼りすぎると、依頼主が衰える。何をしたか覚えていない、だから自分を過小評価するここまで、認知的負荷が下がることで起きる3つの問題を見てきた。「わからない」状態を経験しなくなること。コードへの解像度が下がること。そして、「思い出す」作業をしなくなること。これらは脳の内側で起きている問題だった。しかし、認知的負荷が下がることには、もう1つ厄介な副作用がある。脳の外側、つまり自分自身の認識に関わる問題だ。自分が何をしたのか覚えていないのだ。1日の終わりに「今日、何やったっけ？」と振り返る。AIと働いていると、驚くほど思い出せない。タスクは消化した。PRはマージされた。でも、何をどう解決したのか、記憶がぼんやりしている。なぜか。苦労しなかったからだ。痛みを伴わない経験は、砂に書いた文字だ。苦労は記憶のアンカーになる。あのエラーで3時間ハマった。あの設計で悩んで何度も書き直した。そういう「苦労の記憶」が、「自分がやった」という実感を生む。AIが苦労を肩代わりすると、このアンカーがなくなる。アンカーがないと、何が起きるか。自分を過小評価するようになる。「今日、大したことやってないな」と感じる。でも実際には、かなりの量のコードがマージされている。客観的には生産性が上がっているのに、主観的には「何もやっていない」気がする。成果と実感が乖離する。これは私だけの感覚ではない。知り合いのエンジニアと話していても、同じことを言う人が多い。「なんか最近、成長している実感がない」「仕事はこなせているけど、自分が何をやったか説明できない」。みんな同じ違和感を抱えている。感覚と現実が、乖離しているのだ。なぜ「何をしたか」を覚えていないと不安になるのか。自己評価はどこから生まれているのか。自己評価は、成果から生まれるのではない。「自分が困難にどう向き合ったか」という記憶から生まれる。あのバグを3時間かけて潰した。あの設計を何度も書き直した。あの障害対応で深夜まで粘った。こうした記憶が、「自分はやれる」という感覚を作る。苦労は自己評価の原材料だ。AIが苦労を肩代わりすると、何が起きるか。成果はある。でも、「自分がやった」という実感が残らない。困難と向き合った記憶がないから、自分を評価する材料がない。結果、成果が出ているのに自分を信じられなくなる。成果と達成感はなぜズレるのか。達成感は「困難を乗り越えた」という認識から生まれる。困難がなければ、達成感も生まれない。AIが困難を消してくれると、成果だけが残り、達成感は消える。成果と達成感の乖離。これがAI時代の新しい病だ。このズレは長期的に何を壊すのか。まず、挑戦を避けるようになる。「どうせAIがやってくれる」と思う。自分で考えることを放棄する。次に、自分を信じられなくなる。難しい問題に直面したとき、「自分にはできない」と感じる。かつて乗り越えた経験がないから、乗り越えられるイメージが湧かない。そして最後に、エンジニアとしてのアイデンティティが揺らぐ。「自分は何ができる人間なのか」がわからなくなる。成果は出ている。でも、それは自分の力なのか、AIの力なのか。区別がつかなくなる。達成の記憶がないなら、何かで補うしかない。では、何で補うのか。答えを先に言う。記録だ。達成の記憶がないなら、記録で作ればいい。苦労の記憶がないなら、躓きを記録で残せばいい。AIが消してしまう「プロセスの記憶」を、意図的に書き留める。それが私の出した答えだった。日報が労働と学習をつなぎ直したここまで読んで、「わかる、でもどうすればいいの？」と思っただろうか。私も同じだった。記録が大事だとわかっても、何をどう記録すればいいかわからなかった。行き詰まっていたとき、藁にもすがる思いで、ある習慣を始めた。日報だ。正直、日報は嫌いだった。面倒くさい。忙しい。後で書こうと思って忘れる。3日分まとめて書いて、何をやったか思い出せない。典型的なサボりパターンだった。何度も挫折した。でも、このままでは本当にまずいと思った。自分が書いたコードを説明できない。障害が起きても自分で解決できない。エンジニアとして、このまま衰えていくのか。その恐怖が、嫌いな日報を続けさせた。でも、日報の目的を変えてみた。上司への報告のためではなく、労働の中で生まれた曖昧さを捕まえるために書く。自分が何をわかっていて、何をわかっていないか。その現状を記録する仕組みだ。日報を続けて、衝撃的な事実に気づいた。その話は後で詳しく書く。でもその前に、一度立ち止まって考えたい。日報を書いて躓きを記録する。それは「学習」につながるはずだ。しかし、そもそも「学習する」とは何なのだろうか。この根本的な問いを考えないと、日報を書く意味も見えてこない。では、「学習する」とは、そもそも何なのでしょうか。この問いを考えるとき、私は為末大さんの『熟達論——人はいつまでも学び、成長できる』（新潮社、2023年）に大きな影響を受けました。400mハードルで日本記録を持つ「走る哲学者」が、様々な分野の達人たちとの対話を重ねて到達した方法論です。www.shinchosha.co.jp為末さんは、人が何かを学び、熟達していくプロセスには、分野を超えた普遍的な構造があると言います。陸上であれプログラミングであれ、学習のプロセスは同じです。技能と自分のどちらかだけを高めても成長できないと説きます。技能と自分は、切り離すことのできない「ひとつのもの」——つまり人間という総体として捉えるべきだと。この人間総体を高めていくことが、学習なのです。この考え方は、ソフトウェアエンジニアとしても腑に落ちます。プログラミングスキルだけを磨いても、良いエンジニアにはなれません。問題を分解する力、チームで働く力、技術を選ぶ判断力。技能と自分の総体が、エンジニアとしての実力です。為末さんによれば、学習には5つの段階があります。この5段階は「学習がどう進むか」を示す地図です。まず、その地図を見てみましょう。「遊（ゆう）」——学習の入口です。新しい技術に触れて、面白いから触る。効率は求めません。目的もありません。遊びとは主体的であり、面白さを伴い、不規則なもの。このモチベーションの源泉が、学習の入口になります。エンジニアなら、新しいフレームワークを触ってみる。ドキュメントを読む前に動かしてみる。「これ何ができるんだろう」と試す。壊してみる。変なパラメータを渡してみる。遊びが好奇心を育て、好奇心が学習を駆動します。「型（かた）」——基礎を身につける段階です。お手本を真似る。ドキュメント通りに書く。型とは「基盤となる最も基本的なもの」であり、個人差を超えて最も安定している普遍的なものです。型は丸呑みするもの。なぜそうするかはわからなくても、まず形から入ります。エンジニアなら、公式チュートリアルを写経する。ベストプラクティスをそのまま真似る。「なぜこう書くのか」は後回し。まず手が覚えるまで繰り返します。型が身体に入ると、考えなくても書けるようになります。「観（かん）」——構造を理解する段階です。「なぜこの書き方なのか」と問う。「見る」とは「分ける」こと。動作を分けて見ることで、技術を構造化します。ある技能は別の技能に支えられている。その関係性が見えてきます。エンジニアなら、コードの設計意図を読み取る。「この抽象化は何のためか」「このパターンはどこで使えるか」と問う。部分（関数）と全体（システム）の関係が見えます。観ができると、他人のコードから学べるようになります。また、コードを「意味の塊」として捉えられるようになります。初心者が「if文があって、関数呼び出しがあって...」と一行ずつ追う場面で、「これはトークン検証の処理だ」と全体を1つの塊として認識できる。塊で捉えるから、複雑なコードも把握できるのです。「心（しん）」——本質を掴む段階です。見極めた本質を軸に、自分なりに自由に動ける状態。いつでもニュートラルポジションに戻れるから、応用的な技術も試せます。中心を柔らかくつかむと、冒険できるようになります。エンジニアなら、技術の本質を掴んでいる状態です。「認証の本質は信頼の証明だ」とわかれば、JWT でも OAuth でも Session でも、状況に応じて選べます。心を掴むと、新しい技術もすぐ理解できます。また、具体的な事例から抽象的なパターンを抽出できます。「このエラーはnullチェック漏れ」という具体から「外部データは信頼しない」という原則へ昇華する。この抽象化ができると、問題を絞り込む力も育ちます。「Invalid token」というエラーを見て、「トークン生成か検証のどちらかが問題」と可能性を狭められる。原理原則を理解していれば、推論で問題にたどり着けるのです。「空（くう）」——学習の到達点です。制約から解き放たれて、技能が自然な形で表現できる状態。いわゆる「ゾーン」です。論理よりも勘が働く。そしてまた「遊」に戻る。学習は循環します。エンジニアなら、コードが自然に流れ出る状態です。設計を考えなくても、手が正しい方向に動く。深夜のデバッグで「なぜかここが怪しい」と直感が働く。空に達した技能は、意識せずに発揮されます。重要なのは、部分の学習が全体を高めるという構造です。「認証処理」という部分を学ぶと、「Webアプリケーション開発」という全体の質が上がります。そして全体の質が上がると、今度は別の部分——たとえば「データベース設計」——を学ぶ意欲が湧いてきます。部分と全体が相互に作用しながら、エンジニアとしての総体が高まっていく。この循環こそが、成長を楽しめる理由です。ここまでが、学習の地図です。でも、抽象的な説明だけでは実感が湧かないかもしれません。私自身の経験に当てはめてみましょう。以前、新しい技術を学ぶとき、何が起きていたでしょうか。ドキュメントを読む。知らない概念が出てくる。調べる。言葉の意味はわかった。でも、まだ腑に落ちない。実際にコードを書いてみる。動かない。なぜ動かないか考える。仮説を立てる。試す。また動かない。別の仮説を立てる。3時間が経つ。ようやく動いた。「ああ、こういうことか」。次からは同じ間違いをしなくなる。この過程で、学習の段階を登っていました。最初は遊びから入った。動かしてみる。壊してみる。次に型を学んだ。ドキュメントを読み、お手本通りに書いた。型を繰り返すうちに、「なぜ」が見えてきた。観の段階です。より深まると、パターンが見える。心の段階です。そして最後に、考えなくても手が動くようになる。摩擦が、学習を生んでいました。繰り返しが、成長を生んでいました。今は違います。AIに聞く。完璧なコードが返ってくる。動く。終わり。私は遊んでいません。型も知りません。観ることもありません。心を掴めません。当然、空には程遠い。結果は出ました。でも、自分の中に何も残っていません。成果だけが先に行き、自分は置き去りにされました。これがAI時代の問題の核心です。摩擦がないから、学べない。繰り返す機会がないから、成長しない。問題の核心は見えました。では、もう少し細かく見てみましょう。学習の5段階で、AIはどこを加速し、どこを壊しているのでしょうか。AIはどの段階を代替しやすいでしょうか。「型」です。正しい書き方、ベストプラクティス、パターンの適用。AIはこれらを高速に提供してくれます。初心者がいきなり熟練者と同じ「型」を使えるようになる。これ自体は悪くありません。では、AIが壊すのはどこでしょうか。「遊」と「観」です。特に「遊」のダメージは深刻です。「遊」が消えると何が起きるのでしょうか。遊びとは、目的なく触ること。壊してみること。限界を探ること。正解がすぐ手に入る環境では、不規則さ・寄り道・失敗が排除されます。効率を求めると、遊びは最初に切り捨てられます。しかし、遊びは単なる入口ではありません。型や観に進むためのエネルギー源でもあります。なぜでしょうか。「型」を学ぶのは退屈です。ドキュメント通りに書く。お手本を真似る。地味な作業です。この退屈に耐えられるのは、「遊」の段階で「面白い」という感覚を得ているからです。「この技術、面白い。だから、ちゃんと学びたい」。このモチベーションがなければ、「型」の段階で挫折します。「観」も同様です。「なぜこうするのか」と問うのは、好奇心がなければできません。好奇心は「遊」で育ちます。遊びがないと、「なぜ」を問う動機がない。「動くからいい」で終わります。遊びがないと、「面白いから学ぶ」がなくなります。「必要だから学ぶ」だけになる。必要性で駆動される学習は、必要がなくなった瞬間に止まります。遊びが失われると、学習への意欲そのものが枯れるのです。「観」も壊れやすい段階です。「なぜこうするのか」と問う前に、AIが答えを出してしまう。構造を自分で見出すプロセスがスキップされます。答えは知っている。でも、答えに至る道筋が見えない。観る力はどこで育つのでしょうか。自分で構造を発見する経験の中です。AIがその経験を奪います。型を飛ばすと、なぜ応用できないのでしょうか。型は「基盤となる最も基本的なもの」です。基盤がないと、その上に何も建てられません。AIが型を代替してくれると、基盤が自分の中にない。だから、少し変わった状況に対応できないのです。学習はなぜ循環構造なのでしょうか。「空」に達しても、また「遊」に戻ります。新しい領域を学ぶとき、再び遊びから始まる。この循環が止まらない限り、人は成長し続けます。AIが「遊」を奪うと、循環そのものが止まります。「心」と「空」は、そもそも到達しにくくなります。基盤となる「遊」と「観」が欠けているからです。本質を掴むには、周辺を十分に探索している必要があります。無意識に動けるようになるには、意識的に何度も繰り返した経験が必要です。AIは上層を加速しますが、基盤を掘り崩します。思い返せば、私が一番成長したのは「遊んでいた」時期でした。学生時代、深夜にLinuxをいじっていました。「このコマンドに変なオプションを渡したらどうなるんだろう」と試した。システムが壊れた。復旧に3時間かかった。でも、その3時間でファイルシステムの構造を理解しました。教科書を読むより、壊して直す方がずっと早く学べました。社会人になってからも、余裕があるときは遊んでいました。「この機能、公式ドキュメントにはこう書いてあるけど、本当にそうなのか」と検証した。ドキュメントが間違っていることもありました。公式が想定していないパターンを見つけることもありました。遊びは、ドキュメントの外側を教えてくれました。今はどうでしょうか。遊ぶ暇があったら、次のタスクをAIに投げています。効率的です。生産的です。でも、技術との「雑談」がなくなりました。目的のない探索がなくなりました。効率を追求した結果、学習の肥沃な土壌を捨てていたのです。遊びがないと、表面的な理解で終わります。ドキュメントに書いてあることは知っている。でも、書いていないことは知らない。想定外の状況に遭遇したとき、対処できません。遊んでいないから、技術の「手触り」がわからないのです。ここまで、学習の5段階と、AIがそれをどう壊すかを説明しました。問題は見えました。では、どうすればいいのでしょうか。答えは単純です。何がわかっていないのかを、見えるようにする。何が欠けているのか。どこで躓いているのか。それを捕まえる。見えれば、対策が打てます。学習の段階で言えば、自分が「遊」で止まっているのか、「型」が足りないのか、「観」ができていないのか。それを知る必要がある。しかし、AIと効率的に働いていると、自分がどこで止まっているかすら見えない。見えないものは改善できない。だから、見えるようにする仕組みが必要だ。そこで私は、先に触れた日報を本格的に活用することにしました。1週間続けて、衝撃を受けました。自分がこんなにも理解していなかったのか。金曜の夜、その週の日報を見返した。「わからない」と書いた項目を数えてみようと思った。月曜の分から順番に。1、2、3... 10を超えたあたりで、手が止まった。まだ火曜だった。水曜、木曜、金曜と続く。「なぜ」がわからないもの、「本質」が見えないもの。多すぎた。画面を見つめながら、胃のあたりがざわついた。正直、途中で数えるのをやめた。自分は理解の入口にすら立っていなかった。しばらく、椅子に座ったまま動けなかった。これが自分の実力なのか。毎日コードを書いて、PRをマージして、それなりにやっているつもりだった。でも、蓋を開けてみれば、理解の穴だらけだった。恥ずかしさ、情けなさ、少しの怒り。それらの混ざった感情が胸に込み上げてきた。でも、その夜、不思議と眠れた。これは希望でもあったからだ。自分の現状を捕まえさえすれば、次に進める。見えない敵は怖いが、見える敵は対策できる。何より、問題が見えた。見えないまま衰えていくより、ずっといい。だから、日報について詳しく説明したい。なぜ日報が効くのか。どう書けばいいのか。日報がなぜ「記録」以上の意味を持つのか。それを理解するには、日報が何を可視化しているかを知る必要がある。日報は単なるログではない。曖昧さを捕まえるためのセンサーだ。書くことは、なぜ理解を深めるのか。AIと働いていると、違和感は一瞬で消える。「なんかわからないな」と思った次の瞬間、AIに聞いている。違和感を感じている時間がない。日報に「なぜ:」と書こうとすると、その違和感を言語化しなければならない。「何がわからないのか」を言葉にする。この言語化のプロセスで、曖昧だった問題が明確になる。書くことは、理解を深める。なぜなら、書けないことは理解していないことだからだ。その場で書くことに意味はあるのか。ある。学習の起点は「わからなかった点」にある。しかし、「わからなかった」という感覚は、時間とともに薄れる。翌日には忘れている。1週間後には、何がわからなかったかすら思い出せない。日報は、躓きを時間差で消えない形に固定する。その場で書かないと、学習の種が消える。日報は何を可視化しているのか。自分が何をわかっていて、何をわかっていないか。どこで繰り返し躓いているか。どのパターンが苦手か。可視化されて初めて、対策が可能になる。見えないものは改善できない。日報は、見えないものを見えるようにする。なぜ「躓き」が重要なのか。躓きは、成長の種だ。スムーズにできたことからは、何も学べない。躓いたところに、学びがある。日報は、躓きを収集するシステムだ。躓きを記録し、パターンを見つけ、対策を打つ。このサイクルが学習を生む。日報がないと何が起きるのか。躓きが流れていく。同じところで何度も躓く。でも、躓いていることに気づかない。気づかないから、対策も打てない。日報がない状態は、センサーのない飛行だ。どこに向かっているかわからない。何が起きているかわからない。墜落してから、問題に気づく。では、日報には何を書けばいいのか。私がよく使うのは「なぜ:」と「試した:」だ。「なぜ:」——理由がわからなかったことを記録する。「なぜ: この実装パターンを選んだ理由」。表面的な理解で終わらせない。「試した:」——目的のない探索を記録する。「試した: このライブラリ、何ができるんだろう。ドキュメントを読まずに動かしてみた」。好奇心が動いた瞬間を残す。キーワードは自分で決めればいい。「写経:」「本質:」「ハマった:」など、必要に応じて増やせばいい。大事なのは、何がわからなかったかを捕まえること。記録することで、自分の躓きパターンが見えてくる。以前は、労働の中で自然と学んでいた。困難にぶつかり、格闘し、乗り越える。そのプロセスが、理解を深めてくれた。今は違う。AIが困難を消してくれるから、格闘する機会がない。だから、意識的に躓きを記録し、学習の種類を分類する必要がある。日報は、そのための道具だ。ここまでは日報の「考え方」を説明した。では、具体的にどう実装するのか。私はClaude Codeのカスタムslash commandsで日報システムを作った。詳しい実装は以前の記事に書いた。syu-m-5151.hatenablog.comClaude Codeには強力なカスタマイズ機能がある。CLAUDE.mdというMarkdownファイルをプロジェクトルートや~/.claude/に置くと、AIがそれを読み込んで動作を調整する。コーディング規約、プロジェクト固有のルール、よく使うパターンなどを書いておけば、AIがそれを参照しながら作業してくれる。また、~/.claude/commands/にMarkdownファイルを置くと、カスタムslash commandsとして使える。/nippo-addと打てば、日報追加用のプロンプトが実行される。AIを「自分専用の道具」に育てる仕組みだ。私の日報システムは3つのコマンドで構成される。/nippo-add - 作業中にその場で記録する。Issue番号や感情も一緒に書く。後から検索しやすくなる。/nippo-finalize - 1日の終わりに実行。散らばった記録をAIが整理して、読みやすい日報に仕上げる。/nippo-show - 日次・週次のサマリーを表示。繰り返し躓いているパターンを可視化する。コマンドファイルは ~/.claude/commands/ に置く。プロジェクトをまたいで使える。CLAUDE.mdにはプロジェクトの文脈を、commands/には繰り返し使う操作を。この2つで、AIは「汎用ツール」から「自分の相棒」に変わる。/nippo-add #456 JWTの検証ロジック実装開始/nippo-add #456 AIが書いたコード動いた。でもなぜRS256なのかわからない/nippo-add なぜ: RS256とHS256の違い/nippo-add 試した: JWTのペイロードに変なデータを入れたらどうなるかポイントは作業中に記録すること。1日の終わりにまとめて書こうとすると、何をやったか思い出せない。その場で書けば、摩擦がない。「なぜ:」「試した:」などのキーワードを入れておけば、後から抽出しやすい。自分がどこで躓いているか、一目でわかる。キーワードは何でもいい。「ハマった:」「理由:」でも、英語で「why:」でも構わない。大事なのは、自分が後から検索しやすく、学習のパターンを把握できること。正解はない。自分にしっくりくる言葉を見つければいい。日報を見返すと、同じ技術で同じ種類の躓きが繰り返されている。非同期処理では「なぜ」がわからない。エラーハンドリングでは「本質」が見えない。新しいライブラリでは「試した」が足りない。繰り返し出てくるということは、その部分で理解が止まっているということだ。弱点が見える。弱点が見えれば、対策が打てる。日報は、学習のガイドになった。日報のキーワードが学習のトリガーここまで、日報を使って躓きを「記録する」方法を説明した。しかし、記録しただけでは学習は起きない。記録は入口に過ぎない。日報に「なぜ:」「試した:」と書いたら、それは学習のトリガーだ。記録して終わりではない。そのまま次に進まない。徹底的にAIと対話する。なぜ「対話する」なのか。ここが重要だ。キーワードで記録した躓きは、「わかっていない」ということだ。わかっていないことを、わかるようにするには、どうすればいいか。自分で調べてもいい。でも、AIがいる。AIは、わからないことを説明してくれる。問題は、AIの説明を受動的に聞くか、能動的に引き出すか、だ。私のルールは単純だ。これらのキーワードを書いたら、その場で最低10分はAIと対話する。10分で理解できなければ、20分かける。理解できるまでやる。次のタスクには進まない。ただ、ここで重要な反論がある。「10分で理解できるわけがない」という反論だ。確かにそうだ。複雑な概念を10分で完全に理解するのは無理がある。でも、重要なのは「10分という制約を設けること」自体にある。制約があるから、「本当にわからないこと」だけに集中できる。制約がなければ、際限なく調べ続けて、結局何も身につかない。では、具体的にどう「徹底的に対話する」のか。ここが最も重要なところだ。「AIと対話する」と言っても、やり方次第で効果は天と地ほど違う。徹底的に対話する技術「AIと対話する」とは具体的にどういうことか。まず、よくある間違いから見てみよう。AIに「答えをもらう」ことと、AIと「徹底的に対話する」ことは、本質的に違う。答えは受動。対話は能動だ。答えをもらう：「このエラーを直して」→ 直るコードが返ってくる → 動く → 終わり。人間側の思考は、ほぼゼロだ。問題を投げて、解決策を受け取る。コピペする。動く。何も考えていない。徹底的に対話する：「このエラーの原因は何？」→「なぜそうなる？」→「他にも同じパターンはある？」→「どう防げる？」。人間側に思考が発生する。質問を組み立てる段階で、自分が何をわかっていないか考える。答えを聞いて、次の質問を考える。このサイクル全体で、脳が動いている。なぜ「教えて」では足りないのか。「教えて」は丸投げだ。AIは何かを返す。でも、それがあなたに必要な説明かどうかわからない。あなたが何を知っていて、何を知らないか、AIには見えない。だから、的外れな説明が返ってくることもある。説明を引き出す側に何が求められるか。自分の理解の輪郭を先に差し出すことだ。「私はここまでわかっている。でも、ここからがわからない」。この輪郭を示すことで、AIは適切な説明を返せる。そして、輪郭を示す行為自体が、すでに学習だ。自分が何をわかっていないか言語化する。これは思考を整理する作業だ。説明を引き出す行為は、どの段階で人間側の思考を必要とするか。最初から最後までだ。何を聞くか考える。聞いた答えを解釈する。次に何を聞くか決める。答えを自分の文脈に当てはめる。このすべてが、能動的な思考だ。答えをもらうだけなら、受動的でいい。説明を引き出すには、能動的でなければならない。なぜ「自分の言葉で書き直す」ことが理解の判定基準になるのか。AIの言葉をそのまま使えるなら、理解していなくてもコピペできる。自分の言葉に置き換えるには、一度、頭の中で「翻訳」する必要がある。翻訳には理解が必要だ。書き直せない説明は、理解ではない。説明できるとはどういう状態か。因果を辿れる状態だ。「なぜこうなるか」を自分の言葉で説明できる。別の人に質問されても、答えられる。説明できるようになって初めて、その知識は「使える」ようになる。この違いは学習速度にどう影響するか。答えをもらい続けると、学習速度はゼロに近づく。説明を引き出し続けると、学習速度は加速する。同じAIを使っても、使い方で学習効果は天と地ほど違う。最初は恥ずかしかった。「こんな基本的なことも知らないのか」と思われるのが怖かった。でも、AIは笑わない。何度聞いても呆れない。AIは、最高の学習パートナーだ。この発見が、学び方を変えた。ステップ1: 自分の理解を言語化するいきなり「教えて」と聞かない。まず自分が何をわかっていて、何がわからないかを言語化する。RS256とHS256について、私の現在の理解を確認させて。私の理解：- 両方ともJWTの署名アルゴリズム- HS256は「対称鍵」を使う（たぶん）- RS256は「非対称鍵」を使う（たぶん）わからないこと：- なぜRS256の方が「セキュア」と言われるのか- どういう場面でどちらを選ぶべきかこの理解は合ってる？こうすることで、AIは私の理解レベルに合わせた説明をしてくれる。輪郭を示す行為自体が、すでに学習だ。ステップ2: 「なぜ」を3回以上繰り返す表面的な理解で終わらせない。本質に到達するまで「なぜ」を繰り返す。なぜJWTの署名にRS256を使うの？→ 「秘密鍵と公開鍵を分離できるから」なぜ分離する必要があるの？→ 「検証側に秘密鍵を渡さなくて済むから」なぜ検証側に秘密鍵を渡すとまずいの？→ 「サービスが増えると秘密鍵を知る場所が増える。1箇所でも漏洩したら全体が危険になる」3回目の「なぜ」あたりから、本質的な理解が始まる。ここで「逆に、HS256を使うべきケースは？」「RS256のデメリットは？」と逆のケースも聞く。正解だけでなく不正解を知ることで、判断基準が明確になる。ステップ3: 自分の言葉で要約するここまで理解したら、AIの説明をコピペせず、自分の言葉で要約する。/nippo-add 振り返り: RS256 vs HS256 を理解した【本質】- HS256 = 共通鍵。署名も検証も同じ秘密を使う- RS256 = 公開鍵暗号。検証側に秘密を渡さなくて済む【使い分け】- モノリス → HS256で十分- マイクロサービス → RS256一択書き直せなかったら、まだ理解していない。もう一度ステップ1から繰り返す。復習のサイクルここまでで、2つのことを説明した。日報で躓きを記録すること。そして、その躓きについてAIと徹底的に対話すること。記録と対話。この2つで、学習は起きるはずだ。でも、実際にやってみると、これだけでは足りなかった。理解しても、忘れる。日報を書いた。AIと対話した。その場では理解した。「ああ、そういうことか」と納得した。でも1週間後、同じことでまた躓いている。「あれ、これ前にも調べなかったか？」。書いただけでは、脳に定着しない。同じ内容を間隔をあけて復習すると、記憶に残りやすい。だから復習のサイクルを作った。翌朝（5分）：前日の日報を見返す。見返すだけでいい。「ああ、これ昨日引っかかったやつだ」と思い出す。思い出す行為自体が、記憶を強化する。実際にやってみると、面白いことが起きた。朝、コーヒーを淹れながら昨日の日報を開く。「RS256とHS256の違い」という項目を見る。「えーと、RS256は公開鍵暗号で...」と頭の中で再生しようとする。すると、昨日は理解したはずなのに、もう曖昧になっている部分がある。忘れかけているタイミングで思い出すことが、記憶を強化する。これを毎朝やるだけで、定着率が全然違う。週末（30分）：その週の日報をまとめて確認。2回以上出てきた項目は、その場でAIと対話して理解を深める。理解できたらチェックを入れる。ある週末、日報を見返していて気づいた。「非同期処理」という項目が、月曜、水曜、金曜と3回出てきている。3回も「なぜ」がわからないと書いているのに、そのたびに次のタスクに進んでいた。繰り返し出てくるということは、その部分の理解が止まっているということだ。その週末、2時間かけてPromiseとasync/awaitを徹底的に理解した。翌週から、非同期処理で詰まることがなくなった。月末（1時間）：月間の傾向分析。3回以上出てきた項目は、根本的な知識の穴だ。書籍を買って体系的に学ぶ。月末の分析で、自分の弱点のパターンが見えてきた。私の場合、「認証・認可」「データベースの最適化」「インフラ周り」が繰り返し出てくる。これは断片的な理解では対応できない。体系的に学ぶ必要がある。だから、月末に1冊ずつ関連書籍を買うことにした。日報は、次に買うべき本を教えてくれる。学んだことは、忘れる。これは避けられない。忘却は敵ではない。思い出せないことが敵だ。日報は「思い出すためのフック」を作る作業だ。完璧に覚えようとしなくていい。戻れる仕組みを作ればいい。人間は意志を保てない。「毎日復習しよう」と決めても、忙しくなれば忘れる。だから仕組みを作る。日報システムは、学習の意志を外部化したものだ。意志に頼らず、習慣に組み込む。学習時間の設計ここまで、日報の書き方、AIとの対話の仕方、復習のサイクルを説明した。方法論は揃った。しかし、ここで当然の疑問が浮かぶ。「いつやるのか？」だ。日報を書く。復習する。わからないことはAIと対話する。週末にまとめて振り返る。どれも時間がかかる。全部やるのは大変そうだ。正直、私もそう思った。会社によっては、学習時間を労働時間としてカウントしてくれるところもある。成果を出すタイミングと学習するタイミングが違っても、それを認めてくれる環境もある。もしそういう会社にいるなら、堂々と労働時間内で学習すればいい。ただ、認めなければならない現実がある。成果を出す時間と、学習する時間は、同時には起きにくくなった。かつて労働は最高の学習だった。しかし今は違う。AIと協働する効率化されたプロセスの中では、学習に必要な「摩擦」が発生しない。タスクは完了する。成果物は出る。それでも、脳には何も残らない。効率の代償は、成長だった。これは構造的な問題だ。だから私は、一度分けることにした。成果を出す時間と学習する時間を、意識的に。成果を出す時間は、AIと一緒に効率よくタスクを消化する。学習する時間は、AIなしで、あるいはAIと徹底的に対話しながら、理解を深める。分けた上で、日報で再接続する。私の1週間はこうなっている。月曜 6:00-6:30：先週の日報を見返す。躓いている項目の中で、今週取り組むべきものを3つ選ぶ。カレンダーに学習時間をブロックする。「今週はこの3つを理解すれば、来週の開発が楽になるはず」。仮説を立て、検証し、修正する。学習も開発と同じだ。水曜 12:00-12:30：昼休みを使って、月曜に選んだ項目を深掘りする。わからないことをAIに問いかけ、徹底的に対話する時間だ。金曜 6:30-8:30：「素手の時間」。AIなしでコードを書く。「AIがあるのに使わないのは非効率だ」という反論があるだろう。確かに、短期的には非効率だ。でも、AIと働き続けていると、基礎力が衰える。使わない筋肉は、静かに萎える。基礎がわかっていれば、AIの出力を評価できる。基礎が怪しければ、動くまでガチャを回すだけになる。この時間にやることは3つある。日報で見つけた躓きをAIなしで調べる。小さなユーティリティ関数を手書きする。エラーメッセージを自力で読み解く。AIに聞けば5分で済むことを、30分かけてやる。この30分が、理解の深さを変える。最初の金曜日、2時間が永遠に感じた。簡単なはずの処理が書けない。「こんなことも自分でできないのか」と、情けなくなった。でも、2時間が終わったとき、達成感があった。自分の手で書いた。久しぶりの感覚だった。AIなしで書いてみると、「本当にわかっていること」と「AIに頼っていたこと」の境界が明確になる。自分の実力が、残酷なほど見える。見えるからこそ、対策が打てる。土曜 朝30分：週の振り返り。3つの項目は理解できたか。理解できなかったものは、来週に持ち越す。完璧を求めない。7割理解できれば、次に進む。最初は週3時間も取れないと思った。でも、試してみると、この3時間で週の残り37時間の労働効率が上がった。学習は消費ではない。複利で回収できる投資だ。理解が深まると、AIへの指示が的確になる。学習への投資は、労働の効率で回収できる。私の場合、成果を出すことと学ぶことが自然には重ならなくなった。だから意識的に交差点を作っている。日報が教えてくれる「次に学ぶべきこと」ここまで、学習の「方法」を説明した。日報で記録する。わからないことはAIと対話して理解を深める。復習する。学習時間を確保する。これで「どう学ぶか」は揃った。しかし、「何を学ぶか」は、まだ説明していない。時間は限られている。何を優先すべきか。闇雲に学んでも、効率が悪い。答えは、日報の中にある。日報を続けていると、躓きのパターンが見えてくる。同じ項目が繰り返し出てくる。認証。非同期処理。データベース。繰り返し出てくるということは、断片的な理解では足りないということだ。そこで、日報をインプットのガイドにする。月末に日報を見返して、3回以上出てきた項目を特定する。それに関連する学習リソースを選ぶ。日報は「次に何を学ぶべきか」を教えてくれる。私の場合、月に技術書を10冊、非技術書を10冊、合わせて20冊前後読んでいる。しかし、これは極端な例だ。最初は月1冊でも十分効果がある。大事なのは冊数ではなく、日報で見つけた躓きに関連する本を選ぶことだ。書籍の良いところは、最低限のクオリティが担保されていることだ。最高のブログは刺さる。でも、最低のブログを引くこともある。書籍は編集者の目を通っている。時間は有限だから、ハズレを引きたくない。日報で繰り返し出てくる躓きを見て、関連する技術書を選ぶ。書籍だけではない。公式ドキュメントやRFC、OSSのソースコードも読む。二次情報で満足せず、一次情報に戻る習慣。これが理解の深さを決める。使っているライブラリの実装を見ると、設計判断の理由がわかる。上級者向けだが、他社の障害報告書も参考になる。ポッドキャストも意外と効く。特にリモートワーカーにおすすめしたい。ちゃんと聞かなくていい。BGMのように流しておくだけでいい。リモートワークを続けていると、雑談が絶望的に下手になる。下手になると、雑談をしたくなくなる。したくなくなると、技術的なことを気軽に話す機会が減る。機会が減ると、間違った理解を指摘してもらえなくなる。悪循環だ。技術系ポッドキャストを聞いていると、エンジニア同士の会話のリズムが耳に残る。話題の引き出しも増える。Xで信用できるアカウントをフォローしておくのもいい。タイムラインを眺めているだけで、今何が話題になっているかがわかる。しかし、Xは使い方が難しい。今やアテンション・エコノミーのど真ん中で、みんなが揉めている。情報収集のつもりが、気づいたら論争を眺めて時間を溶かしていることがある。意識的に距離を取る必要がある。AIに聞けば答えは返ってくる。でも、体系的な理解は書籍や公式ドキュメントでないと得られない。AIは「この問題の解決策」を教えてくれる。書籍は「なぜその解決策が正しいか」を教えてくれる。AIとの協働で生まれた躓きを、AIの外で埋める。あなたの現在地を見つけるためにここまで、私がやってきたことを説明した。日報で躓きを記録する。AIと徹底的に対話する。復習のサイクルを回す。学習時間を確保する。日報から次に学ぶべきことを見つける。インプットを選ぶ。たくさんあるように見えるかもしれない。全部やる必要はない。でも、何かを始める必要はある。ここまで読んでくれたあなたに、問いかけたい。この1週間を振り返ってみてほしい。AIに聞いて解決したけど、なぜその解決策が正しいのか説明できない問題はなかっただろうか。同じ種類の問題に、何度も遭遇していないだろうか。コードは動いた。でも「なぜ動くのか」を同僚に説明できるだろうか。もし1つでも「怪しい」と感じるものがあれば、それがあなたの躓きだ。今日から日報に書き始めてほしい。日報を1週間続けたら、見返してみてほしい。何が繰り返し出てくるか。認証なのか、非同期処理なのか、データベースなのか。繰り返し出てくるものが、次に学ぶべきことだ。そのとき、インプットを意識的に選んでほしい。体系的に理解したいなら書籍。正確な仕様や実装の判断基準を知りたいなら公式ドキュメントやOSSのソースコード。リモートワーカーならポッドキャストもいい。Xで信用できるアカウントをフォローしておくのも悪くない。日報が「次に何を学ぶべきか」を教えてくれる。インプットは、日報を見て選ぶ。AIに聞けば答えは返ってくる。でも、「なぜその答えが正しいか」を理解するのは、AIの外でやる仕事だ。日報は、その仕事を始める場所を教えてくれる。おわりにここまで、私がやってきたことをすべて説明した。日報、AIとの対話の技術、復習のサイクル、学習時間の設計、インプットの選び方。これらは、私がAI時代に「学ぶ」ために見つけた方法だ。最後に、1つだけ伝えたいことがある。この記事で一番大事なことだ。ここまで読んで、気づいた人もいるだろう。私はCLAUDE.mdに学びを書き込んでいる。プロジェクトの文脈、コーディング規約、過去に得た知見。では、AIは賢くなっているのか。答えはNoだ。AIは何も学んでいない。CLAUDE.mdに書かれた内容は、セッションの最初に読み込まれる。でも、それは「学習」ではない。ただの「入力」だ。AIは前回の会話を覚えていない。経験を蓄積しない。「わからない」を経験しない。AIは、このブログが警告している「学ばない労働者」そのものだ。CLAUDE.mdを充実させれば、AIの出力は変わる。使い込むほど手に馴染む道具にはなる。でも、設定を書いたのは誰か。あなただ。試行錯誤したのは誰か。あなただ。AIが賢くなったように見えるのは、あなたが賢くなったからだ。学習とは、経験を意味に変換する行為だ。これが、この記事を通じて私がたどり着いた核心だ。AIは情報を処理できる。でも、AIにとってそれは「意味」を持たない。人間は違う。経験が意味になる。「あのバグを直した」という経験が、「自分はできる」という自信になる。経験を意味に変換できるのは、人間だけだ。AIと協働しながらも、熟達する主体であり続けるために必要な設計がある。遊びの時間を確保すること。目的のない探索がないと、好奇心が死ぬ。「わからない」状態を意図的に作ること。AIに聞けばすぐわかる。でも、あえて聞かない時間が思考力を維持する。記録を習慣にすること。書かないと忘れる。説明を練習すること。説明できなければ、理解していない。「素手」で戦う時間を持つこと。AIなしでコードを書く時間が、基礎力を維持する。これらに共通するのは、摩擦・記録・言語化だ。摩擦が経験を生む。記録が経験を残す。言語化が経験を意味に変える。AIはこの3つを肩代わりしてくれる。だから楽になる。でも、肩代わりさせると、人間は主体でなくなる。最後に、もう一度聞かせてほしい。あなたは最近、「成長している」と感じているだろうか。もし少しでも不安があるなら、今日から日報を開いてみてほしい。「なぜ:」「試した:」と書いてみてほしい。たった1行、それだけでいい。完璧な日報を書く必要はない。その不完全な1行が、次の1行を呼ぶ。そして、その積み重ねがあなたの脳を取り戻す。3日で挫折するだろう。私自身、何度も挫折した。でも、4日目にまた始めればいい。何度でもやり直せる。完璧に続けることより、何度でも再開できることの方が大事だ。1ヶ月後、あなたは変わっている。同僚に「この実装、どうしてこうしたの？」と聞かれたとき、淀みなく答えられる自分がいる。障害が起きたとき、自分のコードを頭の中でトレースできる自分がいる。日報を見返すと、「なぜ:」で埋まっていた項目が、少しずつ減っている。それが、成長の証だ。あなたの脳は、取り戻せる。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考書籍知性の未来―脳はいかに進化し、AIは何を変えるのか―作者:マックス・ベネット新潮社AmazonPLURALITY　対立を創造に変える、協働テクノロジーと民主主義の未来（サイボウズ式ブックス）作者:オードリー・タン,E・グレン・ワイルライツ社Amazon奪われた集中力: もう一度〝じっくり〟考えるための方法作者:ヨハン・ハリ作品社Amazon熟達論―人はいつまでも学び、成長できる―作者:為末大新潮社Amazon学びとは何か－〈探究人〉になるために (岩波新書)作者:今井 むつみ岩波書店Amazon学びをやめない生き方入門作者:中原淳,パーソル総合研究所,ベネッセ教育総合研究所テオリアAmazon私たちはどう学んでいるのか: 創発から見る認知の変化 (ちくまプリマー新書 403)作者:鈴木 宏昭筑摩書房Amazonシン読解力―学力と人生を決めるもうひとつの読み方作者:新井 紀子東洋経済新報社Amazon夏蜜柑とソクラテス作者:新井 紀子草思社Amazon","isoDate":"2025-12-17T03:17:05.000Z","dateMiliSeconds":1765941425000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"AI時代の異常系テストについて考える","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/16/102227","contentSnippet":"はじめに深夜2時、本番環境のアラートが鳴り響きます。外部APIがタイムアウトを返し始め、リトライが暴発し、システム全体が連鎖的に停止しました。原因を調べると、外部サービスの一時的な遅延でした。たった数秒の遅延が、なぜシステム全体を止めたのか。答えは単純です。「外部APIが遅延したらどうなるか」を、誰もテストしていなかったからです。私自身、このような障害を何度か経験してきました。コードをマージした翌朝にSlackが炎上していたこともあります。「なぜこのケースを考えなかったのか」と自分を責めながら、ホットフィックスを書いた夜もあります。そのたびに思います。あのとき、たった1つのテストを書いていれば。これは「異常系テスト」の不足が引き起こした障害です。正常系のテストは比較的書きやすいです。入力があり、期待する出力があり、それを検証します。しかし、プロダクション環境で本当に問題になるのは異常系です。ネットワークが切断されたとき、システムはどう振る舞うべきか。データベースがタイムアウトしたとき、ユーザーには何を伝えるべきか。想定外の入力が来たとき、エラーメッセージは適切か。こうした問いに答えるのが、異常系テストです。そして今、この異常系テストの世界が大きく変わりつつあります。2024年、AIがOpenSSLに20年間潜伏していた脆弱性を発見しました。人間が書いたファジング（ランダムなデータを入力してバグを探す手法）では見つけられなかった欠陥です。Google OSS-FuzzはAIによるファズターゲット生成で26件の脆弱性を発見し、既存の人間作成ターゲットから最大29%のカバレッジ向上を実現しました。人間だけでは見つけられなかった異常を、AIが発見する時代になりました。本記事では、この変化を踏まえて異常系テストの考え方をまとめました。まず基本的な技法を押さえ、その上でAIやカオスエンジニアリングといったものを紹介します。あの深夜のアラートを、未来の自分や読者が経験しなくて済むように。本題に入る前に本記事を読む前に、いくつか断っておきたいことがあります。私はテストの専門家ではありません。日々コードを書きながら、「この処理が失敗したらどうなるだろう」と考える、一人のエンジニアです。ここに書いてあることは、思いついたものをまとめただけなので不足もあるでしょう。実装しながら単体テストやエラーハンドリングを考える際のヒントとして使ってもらえればと思います。もう一つ、大事なことがあります。すべてのパターンをテストする必要はありません。過度なテストパターンは無駄な工数を生むだけではありません。テストが増えれば増えるほどCIの実行時間は長くなり、開発サイクルは遅くなります。テストコードを読んで理解するにも認知コストがかかります。テストが多すぎると、「このテストは何を確認しているのか」を把握するだけで疲弊してしまいます。テストのROI（投資対効果）を意識し、リスクの高い箇所に集中することが重要です。とはいえ、最近は生成AIのモデル精度が上がったおかげで、文脈を読み取ってテストコードを生成してくれるようになりました。「何をテストすべきか」を判断し、AIに生成を任せる。その使い分けが、今のエンジニアに求められています。そして、ここが難しいところなのですが、異常系テストがどれくらい必要かは、その人の経験に大きく依存します。本番障害で痛い目を見た人は「ここまでテストすべきだ」と感じます。幸運にも大きな障害を経験していない人は「そこまでやる必要があるのか」と思います。これは良い悪いではなく、思考の枠組みそのものが異なるのです。だからこそ、チームとして、組織として「どこまでの異常を許容するのか」を明確にしておく必要があります。暗黙の了解ではなく、言語化する。そうしないと、「テストが多すぎる」「テストが足りない」という不毛な議論が続くことになります。では、本題に入りましょう。異常系テストとは異常系テストとは、システムが想定外の状況に遭遇したとき、適切にエラーハンドリングできるかを検証するテストです。正常系テストが「うまくいくパス」を確認するのに対し、異常系テストは「うまくいかないパス」を確認します。異常系と一口に言っても、その原因はさまざまです。「どこから異常が来るか」という視点で整理すると、4つの観点に分けられます。入力値の異常: ユーザーやクライアントから来ます。空文字、境界値超過、不正な形式など状態の異常: システム内部のデータに起因します。リソースが見つからない、すでに処理済み、権限不足など環境の異常: 外部依存に起因します。ネットワーク障害、DB接続失敗、ディスク容量不足など競合の異常: 並行処理に起因します。同時更新、デッドロック、レースコンディションなどこの順番には意味があります。入力値の異常は最も頻繁に発生し、テストも書きやすいです。状態の異常はビジネスロジックと密接に関わります。環境の異常はテストが難しいですが、本番では必ず起きます。競合の異常は最も見つけにくく、再現も難しいです。つまり、テストの書きやすさと、問題の発見しにくさは、おおむね逆の関係にあります。それぞれについて見ていきましょう。入力値の異常フォームに全角スペースだけを入力して送信したら、システムがエラーを吐いた。そんな経験はないでしょうか。あるいは、絵文字を含む名前を登録しようとしたら、データベースエラーが返ってきた。ユーザーは悪意を持っていたわけではありません。ただ、開発者が「想定していなかった」だけです。どれだけ想像力を働かせても、ユーザーは必ずその想像の外側から来ます。ユーザーからの入力は信用できません。これはセキュリティの基本原則ですが、テストにおいても同様です。境界値分析（Boundary Value Analysis）境界値分析は、ソフトウェアテストの古典的な技法です。入力値の境界付近でエラーが発生しやすいという経験則に基づいています。# 例: 文字数制限が255文字の場合- 255文字 → 成功するべき- 256文字 → エラーになるべき- 0文字（空文字） → 要件によるよくある境界値のテストケース：最大値・最小値最大値+1・最小値-1ゼロ負の値（許可されていない場合）この技法は同値分割法（Equivalence Partitioning）と組み合わせて使うことが多いです。同値分割法では、入力値を「同じ振る舞いをするグループ」に分割し、各グループから代表値を選んでテストします。たとえば「1〜255文字」「256文字以上」「0文字」の3グループに分け、それぞれの代表値と境界値をテストします。現代のAPI開発では、境界値分析の対象は数値入力を超えて拡張されています。APIのページネーション制限（page size=99, 100, 101）、リクエストペイロードサイズ制限、タイムアウト閾値、レートリミットの境界などが現代的なBVA対象です。空値の扱い境界値の中でも、特に扱いが難しいのが「空」という概念です。空値の扱いは設計上の判断が必要になります。 値  検討ポイント  空文字 \"\"  許容するか、エラーにするか  スペースのみ \"   \"  トリムするか、エラーにするか  NULL  必須項目か、オプショナルか  undefined  デフォルト値を使うか テストを書くことで、こうした設計の曖昧さが明確になることがあります。「空文字を許容するか」という問いに対して、チームで合意を取る機会になります。ここで気づくべき重要なことがあります。異常系テストの気付きにくい価値は、バグを見つけることではありません。設計を問い直すことです。「この入力が来たらどうするか」という問いを立てることで、仕様の穴が見えます。テストを書く行為そのものが、システムの堅牢性を高めています。テストが通るかどうかは、実は二次的な問題なのです。文字種と特殊文字空値に続いて、もう一つ厄介なのが文字種です。日本語を扱うシステムでは、文字種のテストが特に重要になります。カタカナ・半角カタカナ環境依存文字（㈱、①など）サロゲートペア（𠮷野家の「𠮷」など）絵文字これらの文字が入力されたとき、システムがどう振る舞うかを確認します。データベースの文字コード設定やAPIのエンコーディングによっては、予期しない動作をすることがあります。セキュリティ関連の入力ここまでは「意図しない入力」の話でした。しかし、世の中には「意図的に悪意のある入力」を送りつけてくる人もいます。セキュリティ関連の入力値テストは、インジェクション攻撃（悪意のあるコードを入力に紛れ込ませる攻撃）への耐性を確認します。OWASP Testing Guideは、このようなセキュリティテストの標準的な指針を提供しています。XSS（クロスサイトスクリプティング）: \u003cscript\u003ealert('XSS')\u003c/script\u003e — Webページに悪意のあるスクリプトを埋め込む攻撃SQLインジェクション: '; DROP TABLE users;-- — データベースを不正に操作する攻撃コマンドインジェクション: ; rm -rf / — サーバーで不正なコマンドを実行させる攻撃パストラバーサル: ../../../etc/passwd — 本来アクセスできないファイルを読み取る攻撃インジェクション攻撃への対策はセキュリティテストの領域でもありますが、異常系テストとして「不正な入力が来たときにシステムが適切にエラーを返すか」を確認しておくことは重要です。エラー推測（Error Guessing）という経験ベースの技法も有効です。過去のバグ傾向から共通パターン（NullPointerException、ゼロ除算、日時パース問題など）を識別し、重点的にテストします。AIとファジングによる入力値テストここまで紹介した技法は、人間がテストケースを考えるものでした。しかし、人間の想像力には限界があります。そこで注目されているのが、ランダムな入力を自動生成してバグを探すファジング（Fuzzing）です。冒頭で触れたGoogle OSS-FuzzのAI活用は、まさにこの領域での成果です。AIが生成したファズターゲットにより26件の脆弱性が発見され、OpenSSLに20年間潜伏していた欠陥も見つかりました。人間が「こういう入力が来るかも」と想像する範囲を超えて、AIが異常な入力パターンを生成します。www.theregister.comもう一つ、Property-based testing（性質ベーステスト）という手法も企業での採用が加速しています。従来のテストは「入力Aに対して出力Bが返る」という具体的なペアを書きます。Property-based testingでは「どんな入力に対しても、この性質が成り立つ」という形で定義します。たとえば「リストをソートして逆順にしても、要素数は変わらない」といった性質です。Python向けのHypothesisは週間300万ダウンロードを超え、numpyやastropyなどの科学ライブラリでバグを発見した実績があります。QuickCheck（Haskell）、fast-check（JavaScript）、proptest（Rust）など、各言語でエコシステムが成熟しています。入力値の異常は、ユーザーから直接来るものでした。次に見るのは、システムの内部で起きる異常です。状態の異常「さっきまで動いていたのに」。この言葉に覚えはないでしょうか。ユーザーが画面を開いている間に、別のユーザーがデータを削除します。システムの状態は常に変化しています。画面に表示された瞬間、それはもう過去です。リソースの状態に関するテストでは、以下のようなケースを考慮します。存在しないリソース存在しないIDでアクセスしたときに、適切なエラー（404 Not Foundなど）が返ることを確認します。削除済みリソース削除されたリソースに再度アクセスしたときの動作を確認します。ユーザーがブックマークしていたページが、管理者によって削除されていた。よくある話です。「404 Not Found」で終わりなのか、「このコンテンツは削除されました」と丁寧に伝えるのか。論理削除と物理削除では挙動が異なります。論理削除なら「削除済み」というステータスを返せます。物理削除ならレコード自体が存在しないため、404を返すことになります。どちらの設計を採用しているかで、テストの期待値も変わります。処理中のリソース処理中（アップロード中、変換中など）のリソースにアクセスしたときの動作を確認します。「まだ準備ができていない」ことをクライアントに適切に伝えられるか。不正な状態遷移状態遷移が定義されているシステムでは、不正な遷移を試みたときの動作を確認します。# 例: 注文のステータス遷移作成 → 確定 → 発送 → 完了# 不正な遷移作成 → 完了（確定と発送をスキップ）完了 → 作成（逆方向の遷移）入力値の異常、状態の異常は、どちらもアプリケーション内部の話でした。しかし、システムは単独で動いているわけではありません。次は、システムの外側から来る異常を見ていきます。環境の異常環境の異常は、テストが最も難しい領域です。開発環境では再現しにくいですが、プロダクション環境では必ず発生します。ローカルで動いたからといって、本番で動く保証はありません。開発環境は、ある意味で嘘をつきます。ネットワークは常に安定し、データベースは常に応答し、ディスクは無限にあります。そんな理想的な環境でテストしても、現実の障害には備えられません。だからこそ、どういう異常が起こりうるかを知っておくことが重要です。近年ではChaos Engineering（カオスエンジニアリング）という手法が注目されています。Netflixが提唱したこのアプローチでは、本番環境に意図的に障害を注入し、システムの回復力を検証します。AWS Fault Injection ServiceやAzure Chaos Studioといったクラウドサービスも登場しています。これは上級者向けの手法ですが、まずは以下のような基本的な異常パターンを理解しておきましょう。ネットワーク障害通信経路の遮断タイムアウトオンライン→オフラインの遷移対応方法としては、タイムアウト設定、リトライ、サーキットブレーカーなどがあります。サーキットブレーカーとは、外部サービスへのリクエストが連続して失敗したとき、一時的にリクエストを遮断する仕組みです。電気のブレーカーが過電流を検知して回路を遮断するのと同じ発想で、障害の連鎖を防ぎます。データベース障害DB応答不可コネクションプール枯渇デッドロック対応方法としては、コネクションプールの適切な設定、リトライ、タイムアウトなどがあります。外部サービス障害API応答不可レートリミット予期しないレスポンス形式対応方法としては、サーキットブレーカー、フォールバック、キャッシュなどがあります。リソース枯渇ディスク容量不足メモリ不足ファイルディスクリプタ枯渇リソース枯渇は、テストで再現するより監視とアラートで早期に検知する方が現実的です。とはいえ、リソースが枯渇したときにシステムがどう振る舞うか（gracefulに停止するか、エラーメッセージを出すか）は、設計段階で決めておく必要があります。カオスエンジニアリングの実践「本番環境に障害を注入する？ 正気か？」。最初は誰もがそう思います。しかし、問いを変えてみましょう。「本番で障害が起きたとき、それが予期せぬものであることと、計画されたものであること、どちらがマシか？」カオスエンジニアリングの市場規模は2025年に23.6億ドル、2030年には35.1億ドルに達すると予測されています。もはやニッチな手法ではなく、エンタープライズ標準になりつつあります。www.mordorintelligence.comKubernetes環境ではLitmusChaosとChaos Meshが代表的なツールです。LitmusChaosはCNCFインキュベーティングプロジェクトとして活発に開発が続いています。Chaos MeshはPodChaos、NetworkChaos、IOChaos、StressChaosなど多様な障害タイプを提供します。hub.litmuschaos.ioGameDay（計画的なカオス実験演習）の実践も広がっています。まず最小の爆発半径（障害の影響範囲）から開始し、単一コンテナ→サービス→ゾーンと段階的にスケールアップします。本番環境を最初のターゲットにしてはなりません。レジリエンスパターン環境の異常に備えるには、コードにレジリエンスパターンを組み込む必要があります。先に紹介したサーキットブレーカーに加え、以下のパターンが重要です。Bulkhead（バルクヘッド）: 船の隔壁のように、リソースを区画化して一部の障害が全体に波及することを防ぎますRetry with Exponential Backoff（指数バックオフ付きリトライ）: 失敗したら1秒後、2秒後、4秒後…と間隔を広げてリトライします。リトライストームを防止しながら一時的障害から回復しますこれらのパターンを実装したら、カオスエンジニアリングで実際に障害を注入し、期待通りに動作するか検証します。パターンを実装しただけでは不十分で、テストして初めて信頼できます。ここまで、入力値、状態、環境の異常を見てきました。最後に残るのは、最も厄介な異常です。複数のユーザーが同時にシステムを使うときに起きる問題、競合の異常です。競合の異常「ローカルでは動いたのに」。開発者なら誰もが経験するこの言葉の裏には、しばしば競合の問題が潜んでいます。開発環境では自分一人しかアクセスしません。しかし本番環境では、何百人ものユーザーが同時にボタンを押します。本番は、常に渋滞しています。その渋滞の中で、単体テストでは見えなかった問題が姿を現します。複数のユーザーやプロセスが同時にリソースにアクセスすると、競合が発生しえます。これは単体テストでは見つけにくく、負荷テストや本番環境で初めて発覚することも多いです。だからこそ、「競合が起きたらどうなるか」を事前に設計しておくことが重要です。同時更新典型的なシナリオを考えてみましょう。1. ユーザーAがデータを取得2. ユーザーBが同じデータを取得3. ユーザーAが更新を実行4. ユーザーBが更新を実行 → どうなるべきか？ユーザーBの更新時点で、データはすでにユーザーAによって変更されています。このとき、システムはどう振る舞うべきでしょうか。主な対応方法は3つあります。楽観的ロック: データ取得時にバージョン番号を記録し、更新時に照合します。バージョンが変わっていれば「誰かが先に更新した」と判断し、後から更新しようとした側にエラーを返します悲観的ロック: 更新する意思を示した時点で排他ロックを取得し、他者は同じデータを更新できなくなります。確実ですが、ロック待ちによる遅延が発生しえます最後の更新が勝つ: 競合を検出せず、後から来た更新で上書きします。シンプルですが、先の更新は失われますどの方法を採用するかは、ビジネス要件によります。在庫数のように「先の更新が失われると困る」データには楽観的ロックか悲観的ロック、ユーザーのプロフィールのように「最新の状態が正」でよいデータには最後の更新が勝つ方式、といった使い分けになります。ボタン連打UIにおいて、ユーザーがボタンを連打した場合の動作を確認します。「送信ボタンを押したけど反応がない。もう一度押そう」。ユーザーは待ってくれません。ネットワークが遅いとき、ボタンが反応しないとき、人は本能的に連打します。「購入する」ボタンを連打したら2回購入されてしまった、という事故は避けたいところです。対応方法としては、デバウンス（一定時間内の連続クリックを1回とみなす）や、送信中はボタンを無効化する二重送信防止の仕組みがあります。サーバー側でも、同一リクエストを検出するためにリクエストIDを使った冪等性の担保を検討します。ここまで、4種類の異常（入力値、状態、環境、競合）を見てきました。これらの異常が発生したとき、システムは何らかのエラーを返す必要があります。では、どのようなエラーを返すべきでしょうか。次は、エラーレスポンスの設計について考えていきます。エラーレスポンスの設計異常系テストでは、「エラーが起きないこと」ではなく「適切なエラーが返ること」を検証します。エラーレスポンスの設計は、クライアント側のエラーハンドリングに大きく影響します。適切なエラーを返せば、呼び出し側は何が起きたかを判断し、適切に対処できます。ステータスコードの使い分けステータスコードとは、サーバーがクライアント（ブラウザやアプリ）に返す3桁の数字です。この数字を見れば、リクエストが成功したのか、失敗したのか、何が原因なのかが分かります。HTTPの場合：400 Bad Request: 入力値が不正401 Unauthorized: 認証失敗（ログインが必要）403 Forbidden: 権限不足（ログイン済みだがアクセス権がない）404 Not Found: リソースが存在しない409 Conflict: 競合（同時更新など）429 Too Many Requests: レートリミット（リクエストが多すぎる）500 Internal Server Error: サーバー内部エラー503 Service Unavailable: サービス利用不可（メンテナンス中など）gRPCの場合（gRPCはGoogleが開発した高速な通信方式）：INVALID_ARGUMENT: 入力値が不正NOT_FOUND: リソースが存在しないALREADY_EXISTS: リソースが既に存在FAILED_PRECONDITION: 前提条件不成立PERMISSION_DENIED: 権限不足RESOURCE_EXHAUSTED: リソース枯渇セキュリティ観点でのエラー設計他ユーザーのリソースへのアクセスには、エラーコードの選び方に注意が必要です。ここには、多くの開発者が見落としている盲点があります。素直に考えると、「存在するが権限がない」なら403 Forbiddenを返したくなります。HTTPの仕様としては正しいです。しかし、これには問題があります。攻撃者がIDを総当たりで試したとき、403が返れば「このIDのリソースは存在する」と分かってしまいます。つまり、正しいエラーコードを返すことが、セキュリティホールになるという逆説です。そこで、他ユーザーのリソースへのアクセスには404 Not Foundを返すという設計があります。「存在するが権限がない」と「存在しない」を区別できないようにすることで、攻撃者に情報を与えません。GitHubのプライベートリポジトリも、この設計を採用しています。権限のないリポジトリにアクセスすると、「存在しない」と表示されます。これは「嘘をつく」のではなく、「必要以上の情報を与えない」という設計です。エラーメッセージは親切であるべきですが、攻撃者にも親切である必要はありません。異常の種類と、返すべきエラーレスポンスが分かりました。では、実際にどうやってテストを書けばいいのでしょうか。ここからは、異常系テストの書き方について説明します。テストの書き方期待するエラーの検証異常系テストで最も基本的なのは、「期待するエラーが返ること」の検証です。正常系では「期待する結果が返ること」を確認しますが、異常系では「期待するエラーが返ること」を確認します。# 例: 存在しないリソースへのアクセスで404が返ることを検証def test_get_not_found():    response = client.get(\"/resources/nonexistent-id\")    assert response.status_code == 404テストの独立性各テストは独立して実行できるようにします。テスト間でデータを共有しません。# テストごとに一意のIDを使用TEST_ID=\"test-$(date +%s)\"クリーンアップテスト終了後は作成したリソースを削除します。テストデータが残っていると、次回のテスト実行に影響を与える可能性があります。テストピラミッドにおける異常系テストMike Cohnの伝統的なテストピラミッド（ユニット→インテグレーション→E2E）では「各要件に対し少なくとも2つのテスト、1つは正常系、1つは異常系」が原則です。Kent C. Doddsの「Testing Trophy」モデルでは、インテグレーションテストを重視します。「テストがソフトウェアの使用方法に似ているほど、より多くの信頼を与える」という原則のもと、インテグレーションテストはユニットテストが見逃すエラー（コンポーネント間の相互作用問題）を捕捉します。AIによるテスト生成「テストケースを考えるのが面倒」「どこまでカバーすればいいか分からない」。そんな悩みを抱えたことはないでしょうか。AIによるテスト生成は、この問題に一つの解を与えます。NVIDIAのHEPHフレームワークはLLM（大規模言語モデル）を用いてドキュメントからテストを自動生成します。Diffblue CoverはJavaコードの静的解析からユニットテストを生成します。qodo（旧Codium）はコード動作を分析してエッジケースを含むテストケースを生成します。これらのツールはエラーシナリオ、境界条件、例外処理パスを自動的に導出します。ただし、AIが生成したテストをそのまま使うのは危険です。「何をテストすべきか」の判断は人間がすべきであり、AIはその実装を支援するツールに過ぎません。テストの質を検証する：Mutation Testingテストを書きました。カバレッジも高いです。しかし、そのテストは本当にバグを見つけられるのでしょうか。Mutation testing（変異テスト）は、コードに意図的なバグを埋め込み、テストがそれを検出できるか評価する手法です。たとえばif (x \u003e 0)をif (x \u003e= 0)に変更します。この変更をテストが検出できなければ、そのテストには穴があります。PITest（Java）、Stryker Mutator（JS/TS/C#）、cargo-mutants（Rust）などのツールがCI/CDへの統合を進めています。cargo-mutantsはRustConf 2024で発表され、ソースコード変更なしで任意のRustプロジェクトに適用できます。 speakerdeck.com異常系テストの優先順位すべての異常をテストする時間はありません。リスクベースで優先順位をつけます。Priority 1（毎スプリント）: セキュリティ敏感入力（SQLインジェクション、XSS）、金融・トランザクション操作、認証・認可障害Priority 2（毎リリース）: コアビジネス機能のエラーパス、統合ポイント障害、境界値違反Priority 3（定期テスト）: 複雑なエラーハンドリングフロー、二次機能のエッジケースPriority 4（メジャーリリース前）: 安定したレガシー機能、低トラフィック機能のエラーハンドリングまとめ異常系テストは「何が起きたら困るか」を事前に洗い出し、システムが適切に対処できることを検証する作業です。本記事で紹介した内容を振り返ると、以下の5点が重要になります。すべてをテストする必要はない - リスクの高い箇所に集中します。テストにもROIがあります。チームで「どこまでの異常を許容するか」を言語化しておきましょうAIとツールを活用する - ファジング、Property-based testing、Mutation testingなど、人間の想像力を超える異常を発見する手法があります。AIによるテスト生成も現実的な選択肢になりました環境の異常にはカオスエンジニアリング - 本番環境で必ず起きる障害を、事前に計画して注入します。レジリエンスパターン（サーキットブレーカー、バルクヘッド、指数バックオフ）を実装し、実際にテストしますセキュリティ観点を忘れない - エラーメッセージやエラーコードが情報漏洩につながることがあります。403と404の使い分けはその典型例ですテストを書くことで設計が明確になる - 「空文字を許容するか」「同時更新をどう扱うか」といった曖昧だった仕様が、テストを書く過程で具体化されます異常系テストは面倒に感じることもあります。しかし、プロダクション環境で障害が発生してから対処するコストに比べれば、事前にテストを書くコストは安いです。深夜2時のアラート対応、原因調査、ホットフィックス、ポストモーテム。そのすべてを、1つのテストが防いでくれることがあります。障害が起きたら、その教訓をテストとして残す。それが本当の意味での振り返りです。異常系テストは、将来の自分を助けるための投資です。3ヶ月後の深夜2時、アラートが鳴らなかったとき、過去の自分へ感謝するでしょう。明日からできること大げさに考える必要はありません。次にコードを書くとき、1つだけ試してみてください。「この処理が失敗したら、何が起きるか」を考える。その問いを立てるだけで、異常系テストは始まっています。APIを呼ぶコードを書いたら、「このAPIがタイムアウトしたらどうなるか」と考えます。データベースに保存するコードを書いたら、「保存に失敗したらどうなるか」と考えます。その問いに対する答えをテストとして書く。それだけでいいのです。完璧を目指す必要はありません。昨日より1つだけ、システムを堅牢にする。その積み重ねが、深夜のアラートを1回減らし、ユーザーの信頼を1つ守ります。今日書いた1つのテストが、3ヶ月後の深夜2時を救います。AIがテスト生成を支援してくれる時代だからこそ、この「問いを立てる力」は人間にしかできない価値になります。3ヶ月後の深夜2時。あなたのスマートフォンは静かなままです。アラートは鳴りません。それは偶然ではありません。過去のあなたが書いた1つのテストが、その夜の安眠を守っています。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考資料ソフトウェアテスト徹底指南書 〜開発の高品質と高スピードを両立させる実践アプローチ作者:井芹 洋輝技術評論社Amazon単体テストの考え方/使い方作者:Vladimir Khorikovマイナビ出版Amazon【この1冊でよくわかる】ソフトウェアテストの教科書　［増補改訂 第２版］作者:布施 昌弘,江添 智之,永井 努,三堀 雅也SBクリエイティブAmazonソフトウェアテスト技法練習帳 ~知識を経験に変える40問~作者:梅津 正洋,竹内 亜未,伊藤 由貴,浦山 さつき,佐々木 千絵美,高橋 理,武田 春恵,根本 紀之,藤沢 耕助,真鍋 俊之,山岡 悠,吉田 直史技術評論社Amazonテスト駆動開発作者:ＫｅｎｔＢｅｃｋオーム社Amazon知識ゼロから学ぶソフトウェアテスト 第3版 アジャイル・AI時代の必携教科書作者:高橋 寿一翔泳社Amazonフルスタックテスティング【リフロー型】 10のテスト手法で実践する高品質ソフトウェア開発作者:Gayathri Mohan翔泳社Amazonソフトウェア品質保証入門: 高品質を実現する考え方とマネジメントの要点作者:保田 勝通,奈良 隆正日科技連出版社Amazonソフトウェア品質保証の極意 ―経験者が語る、組織を強く進化させる勘所―オーム社Amazon生成AIによるソフトウェア開発 ―設計からテスト,マネジメントまでをすべて変革するLLM活用の実践体系―オーム社Amazon","isoDate":"2025-12-16T01:22:27.000Z","dateMiliSeconds":1765848147000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、戦略を語れ","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/15/130000","contentSnippet":"はじめに会議室で誰かが「戦略」と言った瞬間、空気が変わる。みんなの背筋が伸びる。うなずきが深くなる。誰かがおもむろにホワイトボードの前に立ち、矢印を描き始める。私も「なるほど」という顔をしてみる。眉間にしわを寄せ、顎に手を当て、いかにも深く考えているふうを装う。会議室にいる全員が、突然「戦略を理解している側の人間」になる。ただ、私は知っている。この部屋にいる何人かは、私と同じことを思っているはずだ。「で、結局、何をするの？」言えない。絶対に言えない。「戦略」という言葉が持つ重厚感に押しつぶされて、そんな素朴な疑問は喉の奥に引っ込んでしまう。分かっていないことがバレたら終わりだ。「あいつ、戦略を理解していない」というレッテルを貼られたら、もうこの会議室での発言権はない。だから黙る。黙って、賢そうな顔を続ける。不思議なのは、誰もが同じ演技をしているように見えることだ。部長も、課長も、隣に座っている同僚も。みんな「戦略」という言葉に真剣な顔で向き合っている。でも、その「真剣な顔」は、本当に理解しているから出てくる表情なのだろうか。それとも、理解していないことを悟られないための防衛反応なのだろうか。私には、区別がつかない。会議が終わると、みんな自分のデスクに戻っていく。誰も「さっきの戦略、よく分からなかったね」とは言わない。私も言わない。言ったら負けだ。何に負けるのかは分からないけど、とにかく負ける気がする。だから、分かったふりを続ける。これは、そういう自分への苛立ちから始まった文章だ。「戦略的に考えろ」と言われるたびに、心の中で「戦略的って何だよ」と毒づいてきた。でも調べなかった。調べるのが怖かった。調べて、やっぱり分からなかったらどうしよう。そんな不安があった。聞くこともできなかった。「戦略って何ですか」なんて質問は、新卒1年目ならまだ許される。でも、何年も働いてきた人間が今さら聞けるわけがない。だから分かったふりを続けてきた。その居心地の悪さを、いい加減どうにかしたかった。だからこの文章を書いている。誰かのためではない。自分のためだ。「おい、戦略を語れ」という言葉は、会議室の誰かに向けているようで、実は鏡の中の自分に向けている。お前は本当に分かっているのか。分かったふりをしているだけじゃないのか。その問いに、いい加減決着をつけたかった。戦略という言葉の氾濫私たちの周りには、「戦略」という言葉が溢れている。経営戦略。マーケティング戦略。販売戦略。顧客戦略。人材戦略。DX戦略。グローバル戦略。デジタル田園都市国家構想総合戦略。こんなに幅広く使われている「戦略」だが、その核心が何なのかと聞かれると、答えられない。いや、答えられないだけならまだいい。答えが人によって違いすぎる。ある人は言う。「戦略とは、目標を達成するための手段だ」。別の人は言う。「戦略とは、ビジョンを実現するための計画だ」。また別の人は言う。「戦略とは、競合に勝つための差別化だ」。どれも間違ってはいない。しかし、どれも正しくはない。なぜなら、これは戦略の「結果」であって、戦略の「核心」ではないからだ。戦略会議で何が起きているか、もう一度見てみよう。「今期の戦略は、売上を前年比130%にすることです」。これは戦略ではない。目標だ。どうやって達成するのかは、何も語られていない。「我々の戦略は、顧客第一、品質重視、イノベーション推進です」。これも戦略ではない。スローガンだ。具体的に何をするのかは、何も示されていない。「我々のマーケティング戦略は、デジタルチャネルの強化、SNS活用の拡大です」。これも戦略ではない。施策の羅列だ。なぜその打ち手なのか、どうつながっているのか、何が問題でそれがどう解決するのかは説明されていない。そして、最悪なのは、これらを「悪い戦略」と呼ぶことさえ正しくないということだ。悪い戦略とは、内部の対立を曖昧にするための妥協の産物である、という指摘がある。経営会議で、営業部と開発部が対立する。営業は「もっと新機能を」と言う。開発は「品質を優先すべき」と言う。すると、誰かが言う。「では、両方やりましょう。それが我々の戦略です」。これは妥協だ。誰も傷つけないための八方美人だ。でも、これを「戦略」と呼んではいけない。なぜなら、戦略とは、選択だからだ。何をやるかを決めることではない。何をやらないかを決めることだ。しかし、私たちは選択できない。なぜか。もちろん、個人の心理もある。選択することは、責任を負うことだ。「これをやる」と決めた人間は、それが間違っていた時、責任を取らなければならない。だから、選択を避ける。全部やると言えば、誰も傷つかない。ただ、問題は個人の心理だけではない。組織の構造が、選択を妨げている。まず、インセンティブの問題がある。営業部長は営業の数字で評価される。開発部長は開発の成果で評価される。全社最適より部門最適が優先される構造になっている。「うちの部門の予算を削るな」という力学が働く。次に、権限の曖昧さがある。誰が「やらない」と決める権限を持っているのか。多くの組織で、これが不明確だ。だから、誰も決めない。決めなければ、責任を問われない。また、評価制度との不整合がある。「やらないと決めた」ことは、評価されにくい。成果として見えないからだ。「100のことをやって80点」より「30に絞って95点」の方が戦略的には正しい。しかし、評価制度が前者を高く評価することがある。だからといって、個人の責任がないわけではない。選択する勇気は必要だ。ただ、勇気だけで組織を変えることはできない。構造を変えなければ、選択は起きない。「全部やる」は、個人の弱さであると同時に、構造の帰結でもある。選択を避け、妥協を繰り返す。その結果、戦略という言葉は、中身のない器になった。何を入れても受け入れる、便利な箱になった。目標を入れる。スローガンを入れる。希望を入れる。妥協を入れる。蓋を閉じて、「戦略」というラベルを貼る。これが、私たちが「戦略」と呼んでいるものの正体なのだろう。この空洞さは、どの立場にいても感じることがあるだろう。ただ、私のように技術寄りの立場にいると、余計に気になることがある。技術顧問として呼ばれているのに、いつの間にか「経営戦略」のスライドを見せられている。自分はシステムのアーキテクチャについて聞かれると思っていたのに、気づくと「売上130%」のスライドの前に立たされている。その「戦略」がどのレイヤーの話なのか、技術側から見るとよくわからない。事業の話なのか、組織の話なのか、プロダクトの話なのか。全部が「戦略」という言葉で括られている。しかし、だからといってエンジニアが戦略と無縁でいられるわけではない。プロダクトのどこにリソースを割くか。どの技術的負債を今返し、どれを後回しにするか。このアーキテクチャで将来の拡張性を取るか、今のシンプルさを取るか。これはすべて戦略的な判断だ。経営会議に出なくても、コードを書いていても、私たちは日々、戦略的な選択をしている。だからこそ、「戦略とは何か」を理解することは、エンジニアにとっても他人事ではない。私自身、最近作った資料を振り返ることがある。「戦略」と書いたスライド。本当に「解決すべき最重要課題と、その解き方」になっていたか。目標やスローガン、施策の羅列に留まっていなかったか。正直、自信がない。核心を見極める戦略を一言で表すなら、こうなる。「戦略とは、解決可能な最重要課題を見極め、それを解決する方法を見つけることだ」。シンプルだ。しかし、深い。まず、「解決可能な最重要課題」とは何か。組織が直面している問題は無数にある。しかし、すべてが同じ重さではない。ある問題を解決すると、他の問題も連鎖的に解決に向かう。そういう問題がある。それが「核心的な課題」だ。技術選定を考えてみよう。新しいプロジェクトを始める時、検討すべきことは無数にある。言語は何にするか。フレームワークは何を使うか。データベースは何が適切か。インフラはどう構成するか。すべて重要だ。ただ、すべてを同時には最適化できない。ここで、核心を見極める必要がある。たとえば、「チームの習熟度」が核心だとする。どんなに優れた技術でも、チームが使いこなせなければ意味がない。だから、チームが慣れている言語を選ぶ。すると、立ち上がりが早くなり、バグも減り、メンテナンスも楽になる。1つの核心を押さえたら、他の問題も動き始めた。戦略も同じだ。核心的な課題を見つけ、そこにリソースを集中させる。戦略とは、この課題を見つけることから始まる。会社が直面している問題は無数にある。売上が伸びない。競合が強い。人材が足りない。技術が古い。すべて問題だ。すべて解決したい。だが、すべてを同時に解決できない。だから、見極める。どれが核心的な課題なのか。どれを解決すれば、他の問題も動き始めるのか。私が関わったある組織で、プラットフォームエンジニアリングチームを立ち上げようとした時、無数の技術的課題があった。どれも難しい。どれも重要だ。Kubernetesの運用。CI/CDパイプラインの整備。監視基盤の構築。セキュリティポリシーの策定。ただ、本当の核心的な課題は、別のところにあった。「開発者がインフラを触るまでのリードタイムが長すぎる」。これが核心だった。どんなに優れた基盤があっても、開発者が使い始めるまでに2週間かかるなら、誰も使わない。だから、セルフサービス化を最優先にした。申請から環境構築までを30分に短縮した。すると、利用率が上がり、開発速度も上がり、プラットフォームチームへの信頼も高まった。1つの核心を解いたら、他の問題も動き始めた。これが、戦略だ。核心的な課題を見つける。解決策を見つける。実行する。もう1つ、私自身のプラットフォームエンジニアリングの経験を話そう。以前いたチームでは、コードの品質、テストのカバレッジ、ドキュメントの不足、レガシーシステムとの連携と、無数の課題があった。ただ、本当の核心的な課題は別のところにあった。「開発者がステージング環境を立てるのに2日かかる」。これが核心だった。インフラチームへの申請、承認待ち、手動でのセットアップ。ステージング環境が作れなければ、検証できない。検証できなければ、リリースできない。Terraformでインフラをコード化し、GitHubのPRをマージするだけで環境が立ち上がるようにした。30分で完了する。すると、リリース頻度が上がり、バグも減り、開発者体験も改善された。1つの核心を解いたら、他の問題も動き始めた。シンプルだが、簡単ではない。課題を見極めることは、選択だからだ。「これが最も重要だ」と決めることは、「他は優先しない」と決めることでもある。戦略会議を思い出そう。「売上を前年比130%にする」は核心的な課題ではない。売上を伸ばすことは結果であって、問題ではない。問題は、なぜ売上が伸びないのか、だ。競合が強いのか。商品が古いのか。チャネルが弱いのか。ブランドが知られていないのか。価格が高いのか。営業力が足りないのか。どれが核心なのか。どれを解決すれば、売上が伸びるのか。それを見極めることが、戦略の第一歩だ。しかし、私たちはそれをしない。見極めることは、責任を負うことだからだ。「これが核心だ」と言った人間は、それが間違っていた時、責任を取らなければならない。だから、誰も言わない。全部重要だと言う。全部やると言う。そして、何も解決しない。選択から逃げる方法は、もう1つある。パーパスやミッションに逃げ込むことだ。パーパスやミッションは、戦略ではない。パーパスとは、企業の存在意義だ。ミッションは、企業が果たすべき使命だ。どちらも重要だ。だが、戦略ではない。「世界中の人々に幸せを届ける」。美しいパーパスだ。では、どうやって届けるのか。それが戦略だ。パーパスは方向を示す。道を示すのは戦略だ。多くの企業が、パーパスを掲げて満足してしまう。具体的に何をするのかは、曖昧なままだ。これは、戦略の放棄だ。難しい選択から逃げているだけだ。これは事業側の話だけではない。技術側にも同じ罠がある。「技術負債をなくす」「きれいなアーキテクチャにする」「開発者体験を向上させる」。美しい技術パーパスだ。しかし、具体的にどの負債を、いつまでに、どうやって返すのか。何を「きれい」と定義し、どの部分から手をつけるのか。開発者体験のどの側面を、どの程度まで改善するのか。それが示されていなければ、技術パーパスもまた、戦略ではない。事業側であれ技術側であれ、パーパスごっこに陥りやすい。美しい言葉を掲げて、具体的な選択から逃げる。私自身、「解決可能な最重要課題」を1つだけ挙げろと言われると、考え込んでしまうことがある。なぜそれが「最重要」だと言えるのか。即答できない時、見極めができていないと気づく。戦略はストーリーであるここまで、戦略の「内容」について語ってきた。何を解決するか。どこに集中するか。これが内容だ。次は、戦略の「形」について考えよう。同じ内容でも、伝え方によって実行力が変わる。バラバラの施策として並べるか、一貫したストーリーとして語るか。この違いが、戦略の成否を分ける。良い戦略は、施策ではなくストーリーだ。ストーリーとは何か。物語だ。因果の連鎖だ。「AだからB、BだからC、CだからD」という流れだ。良い戦略は、この流れがある。個々の施策が、バラバラに存在するのではない。互いに補強し合っている。前の手が、次の手を可能にする。次の手が、前の手の効果を高める。プロダクト開発で考えてみよう。「このアーキテクチャにしたからこそ、新機能の実験が低コストで回せる」。「このモジュール分割をしておくから、将来の料金プランのバリエーションを増やせる」。「このAPIの設計にしたから、パートナー連携がスムーズにできる」。コードの書き方と、事業側の選択肢が、一本の物語になっているかどうか。技術的な決定が、事業の可能性を広げている。事業の方向性が、技術的な決定を正当化している。この双方向のつながりがあるかどうか。それが、技術戦略がストーリーになっているかどうかの分かれ目だ。逆に、悪い戦略には、ストーリーがない。「我々は、高品質な商品を、低価格で、迅速に提供します」。一見、良さそうだ。でも、これはストーリーではない。施策の羅列だ。高品質と低価格は矛盾する。高品質にはコストがかかり、低価格にするにはコストを削る。迅速さも、品質と矛盾することが多い。これらの施策は、互いに補強し合っていない。むしろ、打ち消し合っている。因果の連鎖がないから、実行できない。なぜ、こうなるのか。多くの場合、「あれもこれも」と欲張るからだ。高品質が欲しい。低価格だって欲しい。迅速さまで欲しい。全部欲しい。でも、全部は取れない。ストーリーを一貫させるには、「これ一本」が必要になる。何かを選び、何かを捨てる。この「これ一本」の考え方は、企業の戦略だけでなく、チームや個人にも当てはまる。私が特に強く感じるのは、専業性の強さだ。誤解のないように言えば、多角化がつねに悪いわけではない。あるプラットフォームチームは、CI/CDパイプラインの整備から始まり、監視基盤、セキュリティスキャン、開発者ポータルへと領域を広げた。別のチームは、Kubernetesクラスタの運用から、GitOpsの導入、Terraformによるインフラ管理、コスト最適化へと拡張した。これは成功した多角化だ。しかし、共通するのは、核となる強みから派生して広がったことだ。前者は「開発者体験の向上」を軸に広がった。後者は「セルフサービス化による開発者の自律性」を軸に広がった。つまり、多角化と専業性は二項対立ではない。「何を軸にするか」が明確かどうかが分かれ目だ。問題なのは、軸のない多角化だ。「他のチームがやっているから自分たちも」「とりあえずKubernetesを入れよう」。このタイプの多角化は、リソースを分散させ、どの領域も「そこそこ」にしてしまう。専業的なチームが強いのは、専業だからではない。1つのことを徹底的に掘り下げているからだ。多角化していても、軸が明確で、そこを徹底的に掘り下げているチームは強い。チームの話をしてきたが、これは個人のキャリアにも当てはまる。私は、エンジニアとして働いている。プログラミングができる。インフラも分かる。データベースも触れる。フロントエンドもできる。「フルスタックエンジニア」という肩書きを持っている。しかし、あるとき気づいた。私は、多くのことを「そこそこ」できる。ただ、何1つ「徹底的に」できない。専門性がない。深さがない。だから、代替可能だ。誰かが、私より少し上手にできる。常に、そういう誰かがいる。専業性。1つのことを、徹底的に掘る。それが、競争優位の源泉だ。もしあなたが「何でもそこそこできる人」なのであれば、それをどうポジショニングするのかも戦略だ。「何でも屋」として埋もれるのか、「事業と技術をつなぐ翻訳者」として立つのか。どちらを選ぶかは、「何をやらないか」の選択だ。翻訳者として立つなら、深い専門性を追求することは諦める。代わりに、異なる専門性を持つ人々の間を橋渡しする能力を磨く。これも戦略的な選択だ。私自身、この問いを自分に向けることがある。戦略を説明する時、ストーリーになっているか。キーワードの寄せ集めか。専業性があるのか、何でもそこそこなのか。流されてそうなっているだけではないか。答えは、いつも曖昧だ。明確に「できている」とは言えない。ただ、問い続けること自体に意味があると思っている。まだ顧客ではない人を見つけるあるとき、プラットフォームチームのダッシュボードを眺めていて、違和感を覚えた。利用者数が伸びていない。一方で、既存ユーザーからの機能要望は山のように来ている。私たちは、その要望に応え続けていた。新機能を追加した。ドキュメントを充実させた。既存ユーザーは喜んだ。しかし、利用者数は変わらなかった。何かがおかしい。ふと疑問が浮かんだ。「使っていない人は、なぜ使っていないのか」。私たちは、その問いを持っていなかった。ここまで、戦略の「何を」「どう」の話をしてきた。次は、「誰に」の話だ。戦略を考える時、私たちは既存の顧客ばかり見てしまう。「この機能がほしい」「ここが使いにくい」。フィードバックは大事だ。ただ、本当の成長機会は、別のところにあることが多い。本当の顧客は、まだ顧客ではない。「まだ顧客ではない人」とは、ただ「使っていない人」ではない。彼らは、その機能を必要としていないのではない。「高すぎる」「難しすぎる」「面倒くさすぎる」など、どこかでバリアに引っかかっている。価格のバリア。複雑さのバリア。心理的なバリア。面倒くささのバリア。どのバリアが最も高いのかを見極め、それを下げる。これが、潜在顧客へのアプローチだ。私が関わったあるプラットフォームエンジニアリングのプロジェクトでは、社内の開発者向けに整備したCI/CDパイプラインやKubernetesクラスタが、なかなか使われない問題があった。機能を追加しても、ドキュメントを増やしても、利用率は上がらなかった。調べてみると、問題は別のところにあった。「初期設定が面倒」。パイプラインの機能は十分だった。ただ、自分のプロジェクトに適用するには、YAMLを何十行も書き、権限設定を複数箇所で行う必要があった。これがバリアだった。テンプレートを用意し、3つの質問に答えるだけで初期設定が完了するCLIツールを作った。利用率は上がった。機能の問題でも、ドキュメントの問題でもなかった。面倒くささのバリアだった。別の例を挙げよう。ある組織で、SREチームの構築した本格的な開発者プラットフォームがあったとする。Kubernetesクラスタ、Terraformによるインフラ管理、Prometheusによる監視、ArgoCD によるGitOps。高機能で、クラウドネイティブな開発には必須の基盤だ。ただ、使いこなすにはKubernetesの知識が必要で、YAMLの書き方を理解し、GitOpsのワークフローに慣れなければならない。このプラットフォームの「まだ顧客ではない人」は誰か。入社したばかりの新人エンジニアだ。別チームから異動してきたバックエンドエンジニアだ。彼らも同じ課題を抱えている。「自分のアプリケーションを安定して動かしたい」という同じ「進歩」を求めている。ただ、学習コストが高すぎる。複雑すぎる。だから、ローカル環境や古いVMで我慢している。ここで、セルフサービスポータルが登場したとする。Webの画面でアプリ名と言語を選ぶだけ。裏側ではKubernetesが動いているが、ユーザーはそれを意識しなくていい。すると、今までプラットフォームを使っていなかった新人や他チームのエンジニアが、ユーザーになる。使っているうちに理解が深まり、もっと高度なカスタマイズがしたくなる。直接YAMLを書くようになる。「まだユーザーではない人」が、ユーザーになる。そして、成長とともにプラットフォームのパワーユーザーになる。重要なのは、「機能を削った劣化版」を作ることではない。「まだ顧客ではない人」が抱えているバリアを特定し、そのバリアを下げることだ。価格がバリアなら、価格を下げる。複雑さがバリアなら、シンプルにする。心理的なハードルがバリアなら、入口を低くする。どのバリアが最も高いかを見誤ると、的外れな施策になる。「まだ顧客ではない人」を見つけて、バリアを下げる。この視点は、開発のやり方そのものを変える。開発には2つのアプローチがある。1つは「押しつけ」型だ。上から降りてきた仕様をそのまま実装する。なぜこの機能が必要なのか。誰のどんな課題を解決するのか。それが見えないまま、言われた通りに作る。すると、何が起きるか。作ったものが使われない。ユーザーが喜ばない。現場のモチベーションが下がる。もう1つは「引き寄せ」型だ。ユーザーの「本当に欲しい進歩」を理解する。そこから逆算して、仕様を決める。機能がユーザーのニーズに「引き寄せられている」状態だ。「まだ顧客ではない人」を見つけ、彼らのバリアを理解し、そこから仕様を導く。これこそが、戦略と開発が噛み合っている状態だ。私自身、「まだ顧客ではない人」を見落としていることに気づくことがある。既存ユーザーの声ばかり聞いて、「まだ使っていない人」のことを考えていない。彼らは何を求めているのか。何がバリアになっているのか。この視点を持つだけで、見える景色が変わる。ここで、1つ問いを立てたい。「まだ顧客ではない人」を見つけるのは、誰の仕事だろうか。マーケティング部門の仕事だと思われがちだ。しかし、現場こそ、潜在顧客のバリアを理解できる独自の視点を持っている。セールスは「買わない理由」を知っている。CSは「使い続けない理由」を知っている。そしてエンジニアは、バリアの多くが技術的な問題であることを知っている。「設定が複雑すぎる」。これは技術で解決できる。「動作が遅すぎる」。これも技術で解決できる。「他のツールと連携できない」。これも技術で解決できる。マーケティング部門は「バリアがある」と気づくことはできる。だが、「そのバリアをどう下げるか」を具体的に設計できるのは、現場だ。ここまで読むと、「現場が重要だ」という話に聞こえる。確かにそうだ。ただ、もう1つ、見落としがちな点がある。現場が価値を発揮できるのは、ある条件が揃っている時だけだ。その条件とは、制約だ。制約がなければ、現場は潜在顧客について何も語れない。逆説的に聞こえるだろう。説明しよう。どういうことか。もしリソースに何の制約もなければ、「全部やればいい」で終わる。高速にする。簡単にする。安くする。連携できるようにする。全部やる。それで解決だ。でも、現実にはリソースは有限だ。時間も、人も、予算も。だから、「どのバリアを下げるか」を選ばなければならない。この「選ぶ」という行為において、現場の知見が活きる。エンジニアなら「このバリアを下げるには3ヶ月かかる。でも、こっちのバリアなら1週間で下げられる」と判断できる。セールスなら「このバリアを下げれば、商談の成約率が上がる」と判断できる。制約があるからこそ、優先順位が生まれる。優先順位があるからこそ、戦略が必要になる。制約こそが、現場の貢献を可能にしている。つまり、「まだ顧客ではない人」を見つけて、そのバリアを下げる方法を提案すること。これは、現場ができる最大の「事業への貢献」の1つだ。指示を待つだけの現場には、この貢献はできない。「誰が使っていないのか」「なぜ使っていないのか」「どうすれば使えるようになるのか」。そして、「限られたリソースで、どのバリアから下げるべきか」。この問いを持つ現場だけが、事業の成長に直接貢献できる。理論と実践の間でここまで、戦略について語ってきた。核心的な課題を見極めること。ストーリーとしての一貫性。専業性の強さ。「まだ顧客ではない人」という視点。これらの考え方は、理解できる。頭では分かる。しかし、1つ重要な疑問が残る。これらの考え方を、どう使えばいいのか。正直に言えば、私はエンジニアだ。設計パターンやアーキテクチャの本を何冊も読んできた。ドメイン駆動設計。クリーンアーキテクチャ。マイクロサービス。モジュラーモノリス。どれも「ソフトウェアをどう構造化するか」についての理論だ。複雑さをどう分割するか。変更をどう局所化するか。チーム間の依存をどう減らすか。これらの理論や仕組みについて、語ることはできる。だが、それを自分のプロジェクトに適用できるかは、別の話だ。どの理論が自分たちの状況に適用可能なのか。どのパターンが今の組織規模とスキルセットに合っているのか。それを判断するには、理論を超えた洞察が必要だ。理論を語れることと、戦略を立てられることは、別だ。ただ、「戦略を語れない」ことと「戦略を実行できない」ことは、同じだろうか。私は「戦略を語れ」と言っている。しかし、語れることと実行できることは別だ。美しい戦略を語れても、実行できなければ意味がない。逆に、言葉にできなくても、体で分かっている人もいる。私は、どちらだろうか。語れるけど実行できないのか。実行できるけど語れないのか。それとも、どちらもできていないのか。正直に言えば、分からない。理論は、現実を説明する。「なぜこうなったのか」を教えてくれる。しかし、「どうすればいいのか」は教えてくれない。マイクロサービスアーキテクチャは、システムを小さな独立したサービスに分割する手法だ。各チームが自分のサービスを独立してデプロイできる。大規模組織では強力だ。ただ、5人のチームで導入すべきか。サービス間の通信、障害の伝播、デバッグの難しさ。小さなチームには重荷になる。モノリスのままでいいのか。将来の拡張性は諦めるのか。ドメイン駆動設計は、複雑なビジネスロジックを「境界づけられたコンテキスト」で整理する手法だ。だが、今のプロジェクトは、本当にそこまで複雑か。学習コストに見合う複雑さがあるのか。それを判断するには、理論を超えた洞察が必要だ。理論は、説明する。しかし、処方箋は出さない。これは、理論の限界だ。いや、限界というより、理論というものの性質だ。理論は、世界を理解するためのツールだ。世界を変えるためのツールではない。理論には、必ず適用範囲がある。マイクロサービスは、組織がスケールしている時に有効だ。ただ、チームが小さい時は、むしろ足かせになる。クリーンアーキテクチャは、ビジネスロジックを外部依存から切り離す設計思想だ。データベースやフレームワークを後から差し替えられる。長期保守が前提のプロダクトでは有効だ。一方、3ヶ月で検証して捨てるプロトタイプでは、過剰投資になる。テスト駆動開発は、テストを先に書き、そのテストを通すコードを後から書く手法だ。仕様が明確な時に有効だ。けれど、何を作るか探索している段階では、テストが足かせになることもある。つまり、理論を使うには、まず「どの理論が適用可能か」を判断しなければならない。だが、それを判断するには、理論を超えた洞察が必要だ。これは、逆説だ。理論を使うために、理論を超えた何かが必要だ。その「何か」とは何か。経験だ。直感だ。センスだ。結局、理論は、センスの補助線に過ぎない。センスのある人が理論を使えば、より深く考えられる。一方、センスのない人は理論だけに頼っても、何もできない。では、センスはどう磨くのか。「経験を積め」では答えになっていない。私なりに考えた方法を3つ挙げる。第一に、「判断の言語化」を習慣にする。何かを決めた時、なぜその判断をしたのかを書き残す。1ヶ月後、3ヶ月後に振り返る。当時の判断は正しかったか。何を見落としていたか。この繰り返しが、判断の精度を上げる。第二に、「他者の判断を追体験する」。本を読む時、著者がなぜその結論に至ったかを考える。「自分ならどう判断したか」を先に考えてから、著者の結論を読む。このギャップが学びになる。成功事例だけでなく、失敗事例を読むことも重要だ。第三に、「小さな賭けを繰り返す」。大きな戦略を立てる機会は少ない。ただ、小さな判断は毎日ある。「このタスクを先にやるか、後にやるか」「この機能を入れるか、外すか」。この小さな判断を意識的に行い、結果を観察する。センスは、大きな決断ではなく、小さな判断の積み重ねで磨かれる。センスは才能ではない、と私は思う。観察と振り返りの習慣なのではないか。私自身、この「センス」の不足を痛感したことがある。プラットフォームエンジニアとして「開発者体験を向上させるべきだ」と理論を実践しようとした。ツールのドキュメントを整備し、社内ドキュメントにまとめて共有した。ところが、利用率は変わらなかった。理論を機械的に適用したからだ。開発者体験は、ドキュメントだけでは向上しない。開発者が実際につまずく瞬間を観察する必要がある。「困ったらあの人に聞こう」と思われるプラットフォームチームが必要だ。これは信頼関係であり、組織文化だ。理論の外にある領域だが、理論を機能させるには不可欠だ。理論と実践の間には、常にギャップがある。理論は一般化された知識だ。実践は個別の状況だ。一般を個別に適用する翻訳こそが、実践者のスキルだ。だから、私は「この技術を使うべきか」と聞かれた時、即答しない。「チームの規模は」「プロダクトのフェーズは」「今の技術的負債はどこにある」と聞き返す。理論を適用する前に、文脈を理解しなければならない。文脈なき理論の適用は、害にすらなる。私はエンジニアだ。だから、目の前の現実と向き合うしかなかった。うまくいかないことを何度も経験した。その度に、なぜうまくいかなかったのかを考えた。理論を読み、現実と照らし合わせ、自分なりの理解を深めていった。この捻り出した思考は、今、個人やチームの戦略を立てる時に役立っている。理論を知っている。ただ、理論に頼りすぎない。現場を見る。人を見る。文脈を理解する。その状況に合った答えを探す。これが、実践者の仕事だ。小さな適応範囲なら、語れる。自分のチームで、どういう問題があって、どう解決しようとしたか。何がうまくいって、何がうまくいかなかったか。次はどうするか。この小さな範囲での試行錯誤が、戦略を立てる力を育てる。しかし、この「小さな適応範囲」の中には、純粋な技術領域だけでなく、事業寄りの判断もじわじわと入り込んでくる。「プロダクトのどこにリソースを割くか」「どの顧客セグメントに寄せるか」「この機能を今作るか、後で作るか」。これは、技術的な判断に見えて、実は事業の方向性に関わる判断だ。技術の現場にいながら、事業の戦略にも口を出すことになる。企業全体の戦略を立てることは、私にはできない。立場も違う。経験も足りない。だが、自分の責任範囲では、できる。自分のチームでは、できる。個人の仕事では、できる。この小さな範囲での実践こそが、本当の学びになる。理論を読むことは、重要だ。ただ、理論を読んだだけでは、何も変わらない。理論を使って、現場で試す。失敗する。振り返る。この繰り返しの中でしか、戦略を立てる力は身につかない。私自身、「戦略と言いながら、実は何も捨てていない」ものに関わってきた。理論やフレームワークを「そのまま」適用して、うまくいかなかったことも多い。足りなかったのは、現場の事情への理解だった。人の感情への配慮だった。技術戦略と事業戦略の距離を縮めるここまで、戦略を立てる「個人」の話をしてきた。だが、戦略は組織の中で機能する。特にエンジニアとして気になるのは、技術戦略と事業戦略の関係だ。長いあいだ、自分の中に「事業戦略→技術戦略」という一方向の矢印があった。事業側が「何を作るか」を決め、技術側は「どう作るか」を決める。経営が方向を決めて、エンジニアはそれを実装する。この一方向依存のメンタルモデルは、長らく私の中に染みついていた。しかし、現実には、「どう作るか」が「何ができるか」を大きく変える。変更コストの低いアーキテクチャだから、競合が半年かかる機能を1ヶ月で検証できる。このモジュールの切り方にしておくから、「この部分だけを切り出して別料金プランにする」という事業のオプションが生まれる。このAPIの設計にしておくから、将来のパートナー連携がスムーズにいく。技術戦略は、事業の選択肢を増やす。事業戦略から技術戦略への一方向ではなく、双方向の依存関係がある。技術が事業を制約することもあれば、技術が事業の可能性を広げることもある。この双方向性を理解すると、開発現場で起きる摩擦の見え方が変わる。技術的チャレンジは、想定外の遅延や不具合を生む。これを「技術の問題」として閉じてしまうと、現場は追い詰められる。「なんとかしろ」という圧力だけがかかる。しかし、技術的な課題を「事業戦略を動かす材料」として扱うと、話が変わる。「この技術的な制約があるなら、ローンチ時期をずらすか」。「このリスクがあるなら、この機能は一旦やめて、こっちの顧客セグメントを先に取るか」。技術の現場からの情報が、事業側の判断材料になる。不確実性を飼いならすための対話が生まれる。エンジニアだけでなく、デザイナー、PdM、ドメインエキスパートも同じだ。現場でプロダクトの手触りを一番知っている人たちが、事業戦略の「定数」ではなく「変数」をいじれる立場になっていい。「この仕様だとユーザーは混乱する」というデザイナーの声。「この機能は、実はこの顧客セグメントには刺さらない」というPdMの洞察。「この業界の慣習を考えると、この方向は難しい」というドメインエキスパートの知見。これは、事業戦略を修正するクリティカルなインプットだ。越権行為ではない。むしろ、健康な組織の姿だ。ここまで、技術と事業の対話について語ってきた。対話の相手は人間を想定してきた。しかし最近、対話の相手が変わりつつある。AIを戦略の壁打ち相手にする場面が増えた。試しにAIへ聞いてみたことがある。「戦略を考えてください」。出てきた答えは、驚くほど整っていた。SWOT分析。ファイブフォース分析。「デジタルチャネルの強化」「顧客体験の向上」といった施策。ロジックも通っている。これは、先ほど述べた「理論の限界」と同じ構造だ。AIは理論を適用できる。分析もできる。ただ、「どの理論が今の状況に適用可能か」を判断するのは、AIではなく人間だ。そして、最後に「これでいく」と賭けるのも人間だ。技術戦略と事業戦略の対話において、AIは優秀な壁打ち相手になる。「この技術的制約がある時、事業戦略はどう変わりうるか」と問えば、選択肢を整理してくれる。ただ、その選択肢の中からどれを選ぶかは、現場を知り、責任を負う人間が決める。これは、技術と事業の対話が人間同士であるべき理由と、根は同じだ。私自身、「本当はもっとこうすれば速く進めるのに」と感じることがある。技術の現場から見えている事業の可能性。それを戦略の議論にインプットしようとしたことはある。うまくいった時もあれば、スルーされた時もある。それでも、言い続けることに意味があると思っている。現場こそが仮説を持つべきだここまで、戦略について語ってきた。しかし、1つ疑問が残るだろう。現場の人間は、そもそも戦略なんて考える必要があるのか。与えられた目標を追いかけ、仕様通りに実装するのが仕事ではないのか。私の答えは明確だ。現場こそ、仮説を持つべきだ。エンジニアも、デザイナーも、セールスも、CSもだ。現場は、ビジネスの「手触り」を最も知っている立場だからだ。エンジニアは、プロダクトの構造的な手触りを知っている。「この機能は技術的に難しい」「ここがボトルネックになる」。これはコードを書く人間にしか分からない。同様に、セールスは顧客の「断る理由」の手触りを知っている。CSはユーザーが「つまづく瞬間」の手触りを知っている。本部で数字をこねくり回している時には見えない「事実の断片」を、現場は握っている。この手触りを「仮説」に昇華できた時、現場は戦略を変える力を持つ。たとえば、カスタマーサクセス（CS）。彼らは日々、「解約」という事実に直面する。戦略のないCSは、解約阻止のマニュアル通りに動き、ダメなら「顧客の事情」として処理する。しかし、仮説を持つCSは問う。「なぜ、このタイミングで解約するのか」。彼らは気づく。「機能不足ではなく、オンボーディングの3日目に発生する『設定の面倒さ』に心が折れているのではないか」。この仮説があれば、開発チームに「新機能より設定ウィザードの改善を」と要求できる。それは単なるクレーム処理ではない。立派な「チャーン（解約）阻止戦略」だ。たとえば、セールス。「価格が高いと言われました」と報告するだけなら、誰でもできる。AIでも集計できる。しかし、仮説を持つセールスは考える。「高いと言われるのは、価値が伝わっていないからか、それとも比較対象が間違っているからか」。もし顧客が、競合他社のツールではなく、Excelと比較して「高い」と言っているなら、戦い方は変わる。機能の多さをアピールするのではなく、「手入力のコスト」を訴求すべきだ。その気づきは、マーケティング戦略やプライシング戦略を根底から覆す可能性がある。仮説を持たない現場は、ただの「手足」になる。言われた通りに作り、言われた通りに売る。なぜやるのかは考えない。楽だが、キャリアとしては危うい。「言われたことを正確にやる」だけなら、代替可能だからだ。一方、仮説を持つ現場は、戦略の「センサー」になる。「本社が考えている戦略は、現場感覚とズレているぞ」と気づける。そのズレを言語化し、フィードバックする。時にそれは、経営陣にとって不都合な真実だろう。「今の売り方では絶対に売れない」「この機能は誰も使わない」。しかし、その不都合な真実こそが、組織を救う。仮説を持つことの有用性は、職種を問わず共通している。第一に、学習速度が上がる。仮説を持っていると、結果との差分が学びになる。「このトークなら刺さるはずだ」と思っていたことが、刺さらなかった。このギャップが、次の商談の精度を上げる。仮説がなければ、何が起きても「そんなものか」で終わる。第二に、議論に参加できる。仮説を持っていれば、それをぶつけることができる。「開発側はこう見ていますが、セールス側はどうですか」と問える。これは、単なる状況確認ではない。お互いの「手触り」を照らし合わせる行為だ。この対話の中で、事業の解像度が上がる。第三に、主体性が生まれる。仮説を持つと、「自分ごと」になる。この仮説が正しいかどうか、確かめたくなる。うまくいけば嬉しいし、間違っていれば悔しい。この感情が、仕事へのコミットメントを高める。私はエンジニアだ。だからコードを通じて事業を見る。セールスは対話を通じて、デザイナーは体験を通じて事業を見る。それぞれの「現場」からしか見えない景色がある。その景色を「仮説」という形にしてテーブルに乗せること。それが、私たちが戦略に参加する唯一の方法だ。現場は、戦略の「消費者」ではない。戦略の「参加者」になれる。そのためには、仮説を持つこと。問いを持つこと。それを声に出すこと。これが、現場と経営の距離、技術と事業の距離を縮める第一歩だ。戦略を語れ、責任を持ってここまで、戦略について長々と語ってきた。最後に、個人的な話をしたい。あの会議から、数年が経った。今も、お手伝いしてきた会社で、技術顧問として経営者たちと話をすることがある。会議で、誰かが「戦略」という言葉を使う。相変わらず、中身のない戦略が語られる。正直に言えば、私はそこで「それは戦略ですか」とは言えない。言えなかった。なぜなら、それは私の仕事の範疇を超えているからだ。技術顧問として呼ばれている。システムのアーキテクチャについて助言する立場だ。経営戦略に口を出すのは、越権行為だ。それでも、心のどこかで引っかかっている。本当に必要な場面であれば、立場を超えてでも言うべきではないのか。会社が明らかに間違った方向に進もうとしている時。誰も指摘しない時。そういう時こそ、言うべきではないのか。だが、言わない。言えない。その境界線がどこにあるのか、自分でもわからない。だから、内心では思っている。「その戦略で、どんな問題を解決するのか」。「その問題は、本当に最も重要な問題なのか」。「なぜ、その解決策なのか」。「他の選択肢は、検討したのか」。「何を捨てたのか」。これらの疑問が、頭の中を巡る。だが、口には出さない。出せない。立場が違う。責任の範囲が違う。その代わり、私は慎重に言葉を選ぶ。技術的な観点から、問いを投げかける。「その施策を実現するには、どんな技術的な課題がありますか」。「優先順位をつけるとしたら、どの順番で進めますか」。「リソースの制約を考えると、何かを諦める必要がありませんか」。気を使いながら、遠回しに。それでも、核心を突く問いを。これらの質問は、時に受け入れられる。時に、無視される。経営陣は、自分たちの「戦略」を語り続ける。ただ、不思議なことに、この経験が無駄になることはなかった。経営会議で言えなかったこと。内心で感じていたこと。絞り出した思考。気を使って口に出した言葉。すべて蓄積されていった。自分のチームを持った時、個人として仕事をする時、この経験が役に立った。エンジニアリングチームの方向性を決める時。技術的な選択をする時。プロジェクトの優先順位を決める時。そこでは、私は問うことができた。「この取り組みで、何を解決するのか」。「本当に、それが最も重要な課題なのか」。「なぜ、この方法なのか」。「他にやり方はないのか」。「何を捨てるのか」。チームメンバーと話す。一対一で。ホワイトボードの前で。Slackで。経営会議とは違う。ここでは、私が責任を持てる。私の範疇だ。だから、問える。そして、気づいた。戦略は、スケールの問題ではない。企業全体の戦略でも、チームの戦略でも、個人の戦略でも、根っこは同じだ。核心的な課題を見極める。解決策を見つける。何かを捨てる。実行する。経営会議で見てきた「戦略ごっこ」。あれを、自分のチームでは繰り返さない。そう決めた。チームの目標を立てる時。「全部やる」とは言わない。「これをやる。これはやらない」と明確にする。新しい技術を導入する時。「なんとなく良さそう」では進めない。「どの問題を解決するのか」を明確にする。プロジェクトの優先順位を決める時。「全部重要」とは言わない。「これが最重要。他は後回し」と決める。これは、不快だ。チームメンバーから反発されることもある。「なぜ、私のタスクは優先されないのか」。「なぜ、この技術は使わないのか」。それでも、説明する。なぜその判断をしたのか。何を最優先にしたのか。何を捨てたのか。時に、判断が間違っていることもある。やってみて、うまくいかない。その時は、認める。修正する。ただ、少なくとも、判断はしている。選択はしている。「全部やる」という逃げ方はしていない。これが、私なりの戦略だ。企業全体ではない。自分の責任範囲での戦略だ。それで十分だ。いや、それこそが核心だ。ただ、「小さな戦略で十分だ」と言っているが、それは「逃げ」ではないか。本当は、もっと大きな影響力を持ちたいのに、怖くて小さな範囲に留まっているだけだろう。「小さな戦略」という言葉で、自分の臆病さを正当化しているだけだろう。逆も考えられる。大きな戦略を語りたがる人の中には、目の前の小さな選択から逃げている人もいる。抽象的な「ビジョン」を語ることで、具体的な「何を捨てるか」から逃げている人。私は、少なくともそうはなりたくない。だから、小さな範囲でもいいから、選択し続ける。それが「逃げ」かどうかは、結果が教えてくれるだろう。戦略を立てるスキルは、3つの要素で形成される。第一に、本当に重要なものとそうでないものを見極める能力。第二に、その重要な問題が手持ちのリソースで解決可能かを判断する能力。第三に、リソースを集中投入する決断を下す能力。見極める。判断する。決断する。フレームワークでは学べない。理論でも教えられない。AIにも任せられない。では、どうやって身につけるのか。経験だ。失敗だ。振り返りだ。そして、自分の責任範囲で実践することだ。経営会議では言えなくても、自分のチームでは実践できる。そこで、何度も試す。何度も失敗する。何度も学ぶ。数年間、何度も失敗した。ある時、プラットフォームエンジニアとして、CI/CDパイプラインの刷新を提案した。分析は完璧だった。ビルド時間の短縮率も、デプロイ頻度の改善予測も計算した。しかし、導入されなかった。なぜなら、開発チームの理解が得られなかったからだ。彼らは、新しいパイプラインを信じていなかった。今のJenkinsで十分だと思っていた。彼らの視点を理解していなかった。だから、提案は受け入れられなかった。別の時、Kubernetesへの移行について相談された。コスト分析も、リスク分析も、移行計画も、調べて用意した。しかし、実行されなかった。なぜなら、組織がリスクを取れなかったからだ。今の運用で手一杯だった。新しい基盤に投資する余裕がなかった。組織の状況を理解していなかった。だから、提案は棚上げされた。自分のチームでも失敗した。開発者プラットフォームの改善プログラムを進めようとした。どこを改善すれば、どれだけ効果が出るか、細かく計算した。しかし、実行は中途半端に終わった。なぜなら、改善すべきものを明確にしなかったからだ。「全体的に改善しましょう」と言った。結果、誰もが「自分のところは変えなくていい」と思った。中途半端に変えて、効果も中途半端だった。選択する勇気がなかった。だから、失敗した。これらの失敗から、私は学んだ。戦略は、論理だけでは動かない。人を動かさなければならない。人を動かすには、彼らの視点を理解しなければならない。彼らの懸念を理解しなければならない。彼らの制約を理解しなければならない。戦略は、分析だけでは生まれない。判断が必要だ。「これが最重要だ」という判断。「これは捨てる」という判断。判断には、勇気が必要だ。間違うだろう恐怖と向き合う勇気が。戦略は、計画だけでは実現しない。実行が必要だ。実行には、コミットメントが必要だ。「これをやり遂げる」というコミットメント。困難に直面しても、諦めないコミットメント。論理。判断。コミットメント。この三つが揃って、初めて戦略は機能する。そして、これは、本を読むだけでは身につかない。理論を学ぶだけでは得られない。AIに聞いても教えてくれない。現場で、実際に戦略を作る。実行する。失敗する。振り返る。この繰り返しの中でしか、身につかない。経営は、科学ではない。人に依る。どれだけ理論を学んでも、どれだけデータを分析しても、最後は人の判断だ。その人が、どう見るか。どう感じるか。どう決めるか。そして、その判断は、再現性が低い。同じ状況でも、違う人なら、違う判断をする。同じ人でも、違うタイミングなら、違う判断をする。だから、経営には「正解」がない。あるのは、「その時、その人が、最善だと信じた選択」だけだ。これは、不安だ。頼りない。でも、これが現実だ。戦略を語る人は、多い。でも、戦略を作る人は、少ない。戦略を作ることは、快適ではないからだ。答えのない問いと向き合う。対立を引き受ける。リスクを負う。責任を取る。だからこそ、「語れ」と言いたい。しかし、責任を持って。美しい言葉を並べるだけではなく。フレームワークを使って終わりではなく。スライドを作って満足するのではなく。本当の戦略は、もっと地味だ。もっと泥臭い。現場を見る。数字を見る。人と話す。何度も考える。何度も見直す。何度も修正する。そして、決める。やると決める。やらないと決める。これが、戦略だ。企業全体の戦略を作ることは、私にはできない。立場が違う。責任の範囲が違う。でも、自分の責任範囲では、できる。そして、それで十分だ。小さな範囲でも、根っこは同じだからだ。問題を見極める。解決策を見つける。選択する。実行する。これができれば、それは戦略だ。私自身、最近やったことがある。自分の「責任範囲」で、今週中にやめると決められることを1つだけ、紙に書き出した。それをやめることで、どんなリソースが解放されるか考えた。そして、「戦略」という言葉を使わずに、これから1年の方針を「問題→選択→行動」の3行で書いてみた。書けた時、少しだけ、戦略を作る側に立てた気がした。そして、もう1つ。声に出すことの価値について。私は長いあいだ、「立場を超えて意見するのは越権行為だ」と思っていた。技術顧問は技術のことだけ言えばいい。経営戦略に口を出すのは筋違いだ。そう思っていた。でも、最近は少し考えが変わった。黙っていても、組織が良くなることはない。「これ、おかしいのではないか」と感じた時、黙っていれば波風が立たない。ただ、波風を立てないことと、組織を良くすることは別だ。誰かが声を上げなければ、おかしいことはおかしいままだ。もちろん、声の上げ方は重要だ。対立を煽る言い方ではなく、建設的な問いかけとして。「これは戦略ですか」と詰問するのではなく、「この戦略で解決したい最重要課題は何ですか」と問う。相手を追い詰めるのではなく、一緒に考える姿勢で。「おい、戦略を語れ」。このタイトルには、怒りがある。「おい」という呼びかけには、苛立ちがある。会議で空虚な戦略を語る人たちへの怒り。それもある。しかし、正直に言えば、怒りの多くは自分に向いている。かつての自分も、同じことをしていたから。今でも、完璧にはできていないから。「戦略を語れ」と他人に言いながら、自分は語れているのか。この怒りの裏には、期待がある。もっとうまくやれるはずだ、という期待。自分に対しても、組織に対しても。その期待が裏切られるたびに、怒りが生まれる。そして、その怒りを誰かにぶつけたくなる。「おい、戦略を語れ」と。しかし、怒りだけでは何も変わらない。怒りを、行動に変えなければならない。自分の責任範囲で、選択し続けること。声を上げ続けること。それが、怒りを建設的なものに変える唯一の方法だ。だから、この言葉は、他人に向けているようで、実は自分に向けている。お前は本当に戦略を語れているのか。中身のない言葉を並べていないか。選択から逃げていないか。そう自分に問いかけている。でも同時に、この言葉は、外に向けても発したい。会議で空虚な「戦略」が語られている時。誰もがうなずいているけれど、誰も本当には信じていない時。そういう時に、「それは本当に戦略ですか」と問いかける勇気を持ちたい。声を上げることは、リスクだ。嫌われるだろう。場の空気を壊すだろう。「余計なことを言う奴」と思われるだろう。それでも、本当に重要なことは、声に出さなければ伝わらない。心の中で思っているだけでは、何も変わらない。自分の責任範囲で戦略を実践すること。そして、必要な時には、声を上げること。この2つが揃って、初めて「戦略を語れ」というタイトルに応えられる気がする。「それだけ」の難しさ結局、戦略とは何なのか。長々と書いてきたが、煎じ詰めれば、やるべきことはシンプルだ。核心的な課題を見極めているか、確認する。その課題を本当に解決できているか、問い続ける。妥協なく、選択と集中ができているか、点検する。うまくいっていないなら、うまくいきそうな方に舵を切る。それだけだ。こう書くと、「そんなの当然だ」と感じるだろう。しかし、自分の仕事を振り返ってみてほしい。本当にこれができているだろうか。「課題を見極めているか」を確認するとは、自分たちの判断を直視することだ。これは、自分たちの見立て違いや判断ミスと向き合うことでもある。誰だって、自分が間違っていたとは認めたくない。だから、別の指標を見てしまう。納期に間に合ったか、予算内に収まったか、上司に怒られなかったか。「核心を突けているか」ではなく、「うまくやり過ごせているか」を見てしまう。仕事をしていると、いつの間にか「課題を解決する」という目的が薄れていく。たとえば、内部開発者プラットフォームの構築プロジェクト。最初は「開発者の生産性を上げる」という明確な目的があったはずだ。しかし、プロジェクトが進むにつれて、目的は変質していく。「Kubernetesクラスタを予定通りに構築する」「監視ツールを導入する」「経営層への報告をうまくまとめる」。気づけば、開発者が本当に使いやすいかどうかより、プロジェクトとして「成功」と言えるかどうかが関心事になっている。「課題を解決しているか」という問いは、常に意識しないと蒸発してしまう。なぜなら、その問いに向き合うのは苦しいからだ。解決できていないという不安と向き合わなければならない。「妥協なく」という言葉も、簡単ではない。妥協は悪意からではなく、善意や現実主義から生まれる。「この機能、完璧ではないけど、ないよりましだろう」「全員が満足するものを作れないから、ある程度のところで折り合いをつけよう」「理想を追求していたら、いつまでも終わらない」。一見、成熟した判断に見える。しかし、この「妥協」が積み重なると、最後に出来上がるものが「そこそこ」になる。誰も強く不満を言わないが、強く満足する人もいない。一応使えるが、積極的に使いたいとは思わない。「そこそこ」は、失敗より危険だ。失敗は直せる。「そこそこ」は直せない。明らかな失敗なら、原因を追求して改善できる。しかし「そこそこ」は改善の動機を奪う。「一応は使われている」「致命的な問題はない」という状態は、変化への意欲を殺す。その状態が何年も続いた先に、誰も欲しがらないが捨てることもできない、ゾンビのようなプロダクトやサービスが生まれる。「うまくいっていないなら、舵を切る」。この言葉の中で、最も実行が難しいのはこの部分だろう。まず、「うまくいっていない」と認めることが難しい。これまでの努力を否定することになるからだ。「方向性は間違っていないが、やり方に問題があった」「もう少し続ければ成果が出る」と思いたい。次に、「うまくいきそうな方向」を見つけることが難しい。うまくいっていないことは分かっても、代わりにどうすればいいかは分からない。だから、現状維持を選んでしまう。少なくとも、今のやり方なら「最悪ではない」ことは分かっている。未知の方向に舵を切るのは、博打に見える。では、この「それだけ」を実践するには何が必要なのか。目的を見失わない仕組み。日常の作業に埋没すると、なぜこれをやっているのかを忘れる。定期的に、しかも形式的にではなく真剣に、「何のためにやっているのか」を問い直す機会が必要だ。週に一度でも、チームで「これは本当に問題を解決しているか」と話し合う。その習慣があるかないかで、結果は大きく変わる。小さく試す文化。大きな賭けは、舵を切りにくくする。三年かけて作ったものを「やっぱりダメでした」とは言いにくい。しかし、二週間で作ったものなら、「これは違った、次を試そう」と言える。小さく作り、早く確認し、素早く方向修正する。このサイクルを速く回せる環境があれば、舵取りは格段に楽になる。失敗を許容する空気。「うまくいっていない」と言えるかどうかは、それを言った時に何が起こるかで決まる。責められるなら、誰も言わない。隠す。ごまかす。「うまくいっていない」という報告が、責任追及ではなく改善の起点として扱われる組織でなければ、正直な確認はできない。判断の軸を持つこと。舵を切る方向を決めるには、判断の軸が必要だ。「顧客の困りごとを減らす」「使う人の時間を節約する」「この体験を心地よくする」。何でもいい、しかし具体的で、検証可能な軸。それがあれば、「こっちの方がうまくいくだろう」という仮説を立てられる。軸がなければ、どこに舵を切っていいか分からない。「それだけ」という言葉は、謙遜ではない。本当に、やるべきことはそれだけなのだ。作れているかを見る。問題を解決しているかを問う。妥協しない。確認し続ける。必要なら方向を変える。しかし、この「それだけ」を本当に実践している組織やチームや個人は驚くほど少ない。私たちは目的を忘れ、妥協に流され、現実から目を逸らし、変化を恐れる。「それだけ」の中に、ものづくりの、いや、あらゆる仕事の核心がある。そして、その核心を貫くことの難しさと向き合い続けることが、良い仕事をするということなのだろう。おわりにここまで読んでしまった人がいるとしたら、申し訳ない気持ちが少しある。この文章を読んでも、明日から「戦略が立てられる人」にはならない。私自身がそうだったから分かる。本を読んだ直後は「分かった」と思う。会議で使えそうなフレーズをいくつかメモする。「核心的な課題を見極める」「何をやらないかを決める」。いい言葉だ。これを使えば、自分も戦略を語れる側の人間になれる気がする。でも翌週、いざ自分の仕事で使おうとすると手が止まる。「で、何から始めるんだっけ」。頭の中でフレーズは踊っているのに、目の前の仕事にどう適用すればいいか分からない。結局、また賢そうな顔をして会議に座っている。何も変わっていない。たぶん、戦略を立てる力は、戦略を立てることでしか身につかない。走り方の本を読んでも走れるようにならないのと同じだ。転んで、膝を擦りむいて、また走る。そうやってしか身につかない。身も蓋もないけど、そうとしか言いようがない。だから、この文章には限界がある。読んだだけでは何も変わらない。でも、もしかしたら、何かが引っかかるかもしれない。次の会議で「戦略」という言葉を聞いた時、「それ、本当に戦略？」と心の中でツッコめるようになったら、それだけで意味がある。自分のチームの方向性を考える時に「で、何を捨てるの？」という問いが頭をよぎるようになったら、それで十分だ。その小さな引っかかりが、いつか行動に変わるかもしれない。変わらないかもしれない。でも、引っかかりがなければ、変わる可能性すらない。正直に言えば、この文章は誰かのためというより、自分のために書いた。書きながら「お前、分かってないじゃん」と何度も思った。調べれば調べるほど、自分の理解の浅さが見えてくる。偉そうに「戦略とは何か」を語っているけど、じゃあお前は実践できているのか。そう問われたら、黙るしかない。「おい、戦略を語れ」という怒りは、他人への苛立ちではなかった。鏡に映った自分への問いかけだった。語れているのか。実行できているのか。逃げていないか。分かったふりを続けていないか。その問いに、まだ答えられていない。明日も会議がある。誰かが「戦略」と言うだろう。私はまた、眉間にしわを寄せて「なるほど」という顔をするだろう。それは変わらない。でも、今度は少しだけ、違う気持ちで聞けるかもしれない。「それ、本当に戦略？」と心の中で問いかけながら。その問いかけは、きっと会議室の誰かにではなく、自分自身に向けられている。そう思えるだけで、この長い文章を書いた甲斐はあった。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考書籍ストーリーとしての競争戦略 Hitotsubashi Business Review Books作者:楠木 建東洋経済新報社Amazon戦略の要諦 (日本経済新聞出版)作者:リチャード・Ｐ・ルメルト日経BPAmazon「暗記する」戦略思考　「唱えるだけで」深く、面白い「解」を作り出す破壊的なコンサル思考【電子限定特典付】作者:高松智史かんき出版AmazonArchitecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazonプラットフォームエンジニアリング ―成功するプラットフォームとチームを作るガイドライン作者:Camille Fournier,Ian Nowland,松浦 隼人（翻訳）オーム社AmazonKubernetesで実践する Platform Engineering作者:Mauricio Salatino翔泳社Amazonジョブ理論　イノベーションを予測可能にする消費のメカニズム作者:クレイトン・Ｍ・クリステンセンHarperCollinsAmazonイノベーションの経済学　「繁栄のパラドクス」に学ぶ巨大市場の創り方作者:クレイトン・Ｍ・クリステンセンHarperCollinsAmazonイノベーションのジレンマ 増補改訂版 Harvard business school press作者:Clayton M. Christensen翔泳社Amazon【Amazon.co.jp 限定】戦略のデザイン ゼロから「勝ち筋」を導き出す10の問い（ダウンロード特典：『戦略デザイン力』セルフ診断シート データ配信）: ゼロから「勝ち筋」を導き出す１０の問い作者:坂田 幸樹ダイヤモンド社Amazon良い戦略、悪い戦略 (日本経済新聞出版)作者:リチャード・Ｐ・ルメルト日経BPAmazon君は戦略を立てることができるか 視点と考え方を実感する４時間作者:音部大輔Amazon戦略的思考とは何か 改版 (中公新書 700)作者:岡崎 久彦中央公論新社Amazon戦略、組織、そしてシステム: 「組み立てる」戦略思考の方法論作者:横山　禎徳東洋経済新報社Amazon","isoDate":"2025-12-15T04:00:00.000Z","dateMiliSeconds":1765771200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2025年版 PDE（Personal Development Environment）のすすめ：自分だけの刀を打つ開発環境構築","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/14/132552","contentSnippet":"この記事は、Vim Advent Calendar 2025 13日目のエントリ記事です。はじめにVSCodeやJetBrains製品は、膨大な開発リソースを投じて作られた最強の武器だ。補完、デバッグ、Git統合、拡張機能——すべてが高度に洗練されている。多くの開発者にとって、これらを選ぶのは賢明な判断だと思う。それでも、私は自分で刀を打ちたい。ただし、誤解のないように言っておくと、名刀を打ちたいわけではない。美術館に飾られるような、完璧な一振りを目指しているわけではない。私が欲しいのは、戦場で戦うための道具だ。多少キズがあってもいい。見栄えが悪くてもいい。自分の手に馴染んで、明日の仕事で使えればそれでいい。では、なぜ自分で作るのか。正直に言えば、効率の問題ではない。もっと根本的な、性分の問題だ。思い返すと、私は子供の頃から構造や仕組みがどうしても気になって、分解してしまうクセがあった。おもちゃ、家電、何でも中がどうなっているか知りたくなる。そして仕組みを理解したら、自分なりに改修して「自分だけのもの」を作りたくなる。完成品を受け取るより、自分で手を入れる余地があるものに惹かれる。開発環境も同じだ。私は元々Vimユーザーで、その後Neovimに移行した。途中でVSCode、JetBrains、Cursorに浮気したこともある。どれも素晴らしいツールだった。だが、どうしても「自分で鍛えている」という感覚がなかった。「自分のもの」という実感が湧かなかった。具体的に言うと、こういうことだ。VSCodeを使っていたとき、settings.jsonをいじり、拡張機能を入れ替え、キーバインドを変え——気づけば「VSCodeをカスタマイズする」こと自体が目的になっていた。ならば、最初からカスタマイズ前提のツールを使えばいい。そう考えてNeovimに戻った。理由を論理的に説明するのは難しい。効率だけで言えば、IDEを使いこなす方が早いかもしれない。それでも、自分の手で環境を組み上げ、日々磨き、少しずつ自分の形に変えていく。その過程そのものに惹かれている。これは性分だ。こうした考え方には、実は名前がある。PDE（Personal Development Environment）——「個人開発環境」だ。自分のワークフローに最適化された、自分だけの開発環境を指す。私が10年かけてやってきたことは、まさにこのPDEの構築だった。この記事では、2025年現在の私のPDE構成を紹介する。Rust、Go、TypeScript、Pythonでの開発、そしてKubernetesやTerraformを使ったインフラ作業を想定した構成だ。IDEが合う人にはIDEを勧める。でも、もし「自分で作ってみたい」という気持ちがあるなら、この記事が参考になれば嬉しい。PDEとIDEの違いIDEは万人向けに最適化されており、インストール直後から高い生産性が得られる。学習曲線は緩やか、メンテナンスはベンダー任せ。一方PDEは個人最適化の代わりに、学習コストとメンテナンスを自分で負担する。どちらが優れているかではなく、「すぐ使える便利さ」と「自分で作る楽しさ」のトレードオフだ。もちろん、IDEにも自分の設定を入れられることは知っている。キーバインドを変更できるし、拡張機能は豊富だし、自分でプラグインを開発できる。VSCodeのsettings.jsonを何百行も書いた。それでも、私には「自分で作っている」という実感が足りなかった。論理的に説明するのは難しい。ただ、その実感の有無が、私にとっては大きかった。では、PDEとは結局何なのか。PDEの本質は「自分の手に馴染む道具を自分で作る」こと。職人が道具を磨くように、開発者も環境を育てていく。ただし、職人の道具は飾るためではなく使うためにある。PDEも同じだ。完璧な環境を作ることが目的ではなく、日々の開発で戦えることが目的だ。効率だけを求めるなら、IDEを使った方がいい場面も多い。私のPDE構成概要基盤はWarp Terminal。その上でFish Shellを動かし、プロンプトにはStarshipを使っている。エディタはNeovim（NvChadベース）で、LSPとTreesitterで補完とシンタックスハイライトを実現している。CLIツールはUnixの古典を現代版に置き換えた。lsの代わりにeza、catの代わりにbat、grepの代わりにripgrep。ディレクトリ移動はzoxide、リポジトリ管理はghq + fzf、差分表示はdeltaを使っている。AIアシスタントは複数導入しているが、主軸はClaude Code。Neovim内ではCopilotとAvanteも使っている。1. ターミナル：Warpwww.warp.devかつてはtmux + iTerm2の組み合わせを使っていた。しかし2024年、Warpに移行した。tmuxでやりたかったこと（ペイン分割、セッション管理）がWarp単体でできるし、見た目もかっこいい。使っていて特に不満もない。正直なところ、tmuxの設定をメンテナンスするのが面倒になっていた。tmux + Bash時代は.tmux.confが600行を超えていて、何がどう動いているのか自分でも把握しきれなくなっていた。現在はWarp + Fishという構成で、tmuxの設定は丸ごと不要になった。設定ファイルが減るのは精神衛生上とても良い。あと、Warpはモダンなターミナルらしく、補完がよく効く。コマンドを途中まで打つと候補が出てくる。tmux時代は「あのコマンドなんだっけ」とhistoryを漁ることが多かったが、その頻度が減った。気に入っている点ブロックベースの出力: コマンドの出力が独立したブロックとして扱われ、コピーや再利用が容易。長いログの一部だけコピーしたいときに便利セッション管理の内蔵: tmuxのペイン分割・セッション管理相当の機能が標準搭載。tmuxのプレフィックスキーを覚えなくていいAIアシスタント: 自然言語でコマンドを生成できる。正直あまり使っていないが、たまに「あのコマンドなんだっけ」というときに便利見た目: 単純にかっこいい。毎日使うものなので、見た目の満足度は意外と大事設定のポイント# ~/.warp/keybindings.yamlkeybindings:  # Vim風のペインナビゲーション  - command: move_focus_to_left_pane    keys: ctrl-h  - command: move_focus_to_right_pane    keys: ctrl-l  - command: move_focus_to_pane_above    keys: ctrl-k  - command: move_focus_to_pane_below    keys: ctrl-jtmuxの設定をメンテナンスする必要がなくなったのは大きい。.tmux.confの600行が不要になった。2. シェル：Fish Shellfishshell.comBashやZshではなくFishを選んだ理由は明確だ。設定なしで賢い。「それならIDEを使えばいいのでは」と思うかもしれないが、シェルは基盤だ。基盤が安定しているからこそ、その上で動くエディタやツールを自由にカスタマイズできる。すべてを自分で作る必要はない。Fish選択の決め手補完がすごい: 設定なしでコマンド履歴、ファイルパス、オプションを補完シンタックスハイライト: コマンド入力中にエラーが分かる設定の簡潔さ: ZshからFishに移行して、設定行数が600行から400行に減ったモダンCLIツール統合Unixの古典的コマンドを現代版に置き換えている。# ~/.config/fish/config.fish# ls → eza (icons + git status)if type -q eza    function ls --wraps eza        eza --icons --group-directories-first $argv    endend# cat → bat (syntax highlighting)if type -q bat    function cat --wraps bat        bat --paging=never $argv    endend# grep → ripgrep (faster + smarter)if type -q rg    function grep --wraps rg        rg $argv    endendgithub.comgithub.comgithub.com省略形（abbr）Fishにはabbr（abbreviation、省略形）という機能がある。入力してスペースを押すと、その場で展開される。# ~/.config/fish/config.fish# Git省略形abbr -a g gitabbr -a ga \"git add\"abbr -a gc \"git commit\"abbr -a gco \"git checkout\"abbr -a gd \"git diff\"abbr -a gp \"git push\"abbr -a gl \"git pull\"abbr -a gs \"git status\"abbr -a glog \"git log --oneline --graph\"# Docker省略形abbr -a d dockerabbr -a dc \"docker compose\"abbr -a dcu \"docker compose up -d\"abbr -a dcd \"docker compose down\"abbr -a dps \"docker ps\"# Kubernetes省略形abbr -a k kubectlabbr -a kgp \"kubectl get pods\"abbr -a kgs \"kubectl get svc\"abbr -a kgd \"kubectl get deploy\"abbr -a kd \"kubectl describe\"abbr -a kl \"kubectl logs\"abbr -a kex \"kubectl exec -it\"gsと入力してスペースを押すとgit statusに展開される。私がabbrを気に入っている理由は、展開後のコマンドが履歴に残ること、そして展開後に編集できることだ。Gitコマンドを1日に数十回打つ私にとって、この小さな省力化の積み重ねは大きい。ディレクトリ移動の革命：zoxidecdコマンドをzoxideで置き換えた。一度訪れたディレクトリは、部分一致でジャンプできる。# zoxideの有効化if type -q zoxide    zoxide init fish --cmd z | sourceend# 例：~/ghq/github.com/nwiizo/projectに移動z project  # これだけでOKgithub.comghq + fzf によるリポジトリ管理全てのリポジトリをghqで管理し、fzfで瞬時に移動する。function ghq_fzf_repo -d \"Select repository with fzf\"    set -l selected (ghq list -p | fzf \\        --prompt=\"Repository: \" \\        --preview='ls -la {}')    if test -n \"$selected\"        cd $selected    endend# Ctrl+G でリポジトリ選択bind \\cg ghq_fzf_repogithub.comdirenv による環境の自動切り替えプロジェクトごとの環境変数を.envrcで管理。ディレクトリに入ると自動で読み込まれる。# direnvの有効化（2025年現在のベストプラクティス）if type -q direnv    set -g direnv_fish_mode eval_on_arrow    direnv hook fish | sourceenddirenv.net3. プロンプト：Starshipstarship.rsStarshipは、Rustで書かれた高速なクロスシェルプロンプト。Git状態、言語バージョン、クラウド環境を一目で確認できる。# ~/.config/starship.tomlformat = \"\"\"$directory\\$git_branch\\$git_status\\$golang\\$rust\\$python\\$kubernetes\\$cmd_duration\\$line_break\\$character\"\"\"[character]success_symbol = \"[❯](bold green)\"error_symbol = \"[❯](bold red)\"[kubernetes]symbol = \"☸ \"disabled = falseKubernetes contextが常に表示されるので、本番環境で作業しているか一目で分かる。これで何度か事故を防げた。4. エディタ：Neovim + NvChadneovim.ionvchad.comVim/Neovimを使い始めて10年以上になる。途中でVSCode、JetBrains、Cursorを試したこともあるが、どれも1ヶ月以上メインに居座ったことはない。併用はしても、結局Neovimに戻ってきた。VSCodeもJetBrainsも素晴らしいエディタで、今でもそう思っている。ただ、私は自分で環境を組み立てたかった。その欲求が、他のエディタでは満たされなかった。2025年版 Minimal UI アーキテクチャ2025年の私のNeovim設定で最も特徴的なのは、statusline-less workflowだ。従来のstatuslineやtabuflineを廃止し、必要な情報のみをfloating windowで表示する。編集領域を最大化しつつ、必要な情報は失わない。 コンポーネント  プラグイン  役割  ファイル情報  incline.nvim  右下floating statusline  モード表示  modes.nvim  cursorline色でモード表示  コマンドライン  noice.nvim  floating cmdline (cmdheight=0)  バッファ薄暗化  vimade  非アクティブバッファをdim  コードピーク  overlook.nvim  LSP定義をstackable popup表示  ファイル選択  Snacks.nvim  smart pickerでbufferline代替 github.comincline.nvimは画面右下に小さなfloating windowでファイル情報を表示する。ファイルアイコン、ファイル名、未保存マーク、診断数が一目で分かる。init.luaのような汎用的なファイル名の場合は親ディレクトリも表示される（plugins/init.luaのように）。github.commodes.nvimはInsert/Visual/Deleteなどのモードをcursorlineの色で表現する。Insertはシアン、Visualは紫、Deleteは赤。-- INSERT --のようなテキスト表示が不要になり、視覚的に直感的。-- options.lua での設定vim.o.cmdheight = 0      -- コマンドライン非表示（noice.nvimが代替）vim.o.laststatus = 0     -- statusline非表示（incline.nvimが代替）vim.o.showmode = false   -- モード表示非表示（modes.nvimが代替）キーマップの発見性：which-key.nvimgithub.com2025年のNeovim設定で欠かせないのがwhich-key.nvimだ。キーを押すと、次に押せるキーの一覧がポップアップで表示される。「あのキーマップなんだったっけ」という問題が解消される。{  \"folke/which-key.nvim\",  opts = {    preset = \"helix\",    spec = {      { \"\u003cleader\u003ea\", group = \"AI\" },      { \"\u003cleader\u003eg\", group = \"Git\" },      { \"\u003cleader\u003es\", group = \"Search\" },      { \"\u003cleader\u003ex\", group = \"Diagnostics\" },    },  },  keys = {    { \"\u003cleader\u003e?\", function() require(\"which-key\").show() end, desc = \"Buffer Keymaps\" },  },}\u003cleader\u003eを押して少し待つと、aでAI、gでGit、sでSearch...とグループ分けされたキーマップが表示される。新しいプラグインを入れてもキーマップを覚える必要がない。ナビゲーションSnacks.nvim - 2025年のモダンユーティリティ集。github.comSnacks.nvimはfolke氏による新しいプラグインで、picker、lazygit統合、buffer削除などを統合している。特にsmart pickerが便利で、ファイル・バッファ・最近使用したファイルを一つのインターフェースで検索できる。{  \"folke/snacks.nvim\",  keys = {    { \"\u003cleader\u003e\u003cleader\u003e\", function() Snacks.picker.smart() end, desc = \"Smart Picker\" },    { \"\u003cleader\u003egg\", function() Snacks.lazygit.open() end, desc = \"LazyGit\" },    { \"\u003cleader\u003esf\", function() Snacks.picker.files() end, desc = \"Find Files\" },    { \"\u003cleader\u003esg\", function() Snacks.picker.grep() end, desc = \"Grep\" },  },}lazygit統合ではeditPreset = \"nvim-remote\"を設定することで、lazygit内でファイルを開くと現在のNeovimインスタンスで開かれる。別ウィンドウが立ち上がらない。Telescope - 検索のハブ（サブとして併用）。github.com{  \"nvim-telescope/telescope.nvim\",  keys = {    { \"\u003cleader\u003eff\", \"\u003ccmd\u003eTelescope find_files\u003ccr\u003e\", desc = \"Find Files\" },    { \"\u003cleader\u003efg\", \"\u003ccmd\u003eTelescope live_grep\u003ccr\u003e\", desc = \"Live Grep\" },    { \"\u003cC-p\u003e\", \"\u003ccmd\u003eTelescope find_files\u003ccr\u003e\", desc = \"Find Files\" },  },}oil.nvim - ファイルシステムをバッファとして編集。github.comneo-treeのようなツリー表示ではなく、ファイルシステムを通常のバッファとして扱う。ファイル名の変更は行の編集、削除は行の削除。Vimユーザーには直感的。{  \"stevearc/oil.nvim\",  keys = {    { \"-\", \"\u003ccmd\u003eOil\u003ccr\u003e\", desc = \"Open parent directory\" },  },}flash.nvim - 画面内の任意の位置にジャンプ。github.comsを押して文字を入力すると、その文字にラベルが表示される。ラベルを押すとジャンプ。hop.nvimの後継で、メンテナンスも活発。overlook.nvim - コードピーク。github.comLSPの定義をfloating popupで表示する。ファイルを開かずに定義を確認できる。popupはスタック可能で、複数の定義を同時に表示できる。{  \"WilliamHsieh/overlook.nvim\",  keys = {    { \"\u003cleader\u003epd\", function() require(\"overlook\").open_definition() end, desc = \"Peek Definition\" },    { \"\u003cleader\u003epc\", function() require(\"overlook\").close_all() end, desc = \"Close All Popups\" },  },}診断・デバッグtrouble.nvim v3 - 診断情報のUI。github.com2024年にv3として完全書き直しされた。ツリービュー対応で、エラーの階層構造が見やすい。{  \"folke/trouble.nvim\",  keys = {    { \"\u003cleader\u003exx\", \"\u003ccmd\u003eTrouble diagnostics toggle\u003ccr\u003e\" },    { \"\u003cleader\u003exX\", \"\u003ccmd\u003eTrouble diagnostics toggle filter.buf=0\u003ccr\u003e\" },    { \"\u003cleader\u003exs\", \"\u003ccmd\u003eTrouble symbols toggle\u003ccr\u003e\" },  },}todo-comments.nvim - TODO/FIXME/NOTEのハイライト。コード内のTODOコメントを自動検出してハイライト。Telescopeと連携してプロジェクト全体のTODOを一覧表示できる。Git統合gitsigns.nvim - インラインGit情報。github.com変更行の横にサイン（│）が表示される。hunk単位でのステージ、リセット、プレビューが可能。{  \"lewis6991/gitsigns.nvim\",  opts = {    on_attach = function(bufnr)      local gs = package.loaded.gitsigns      vim.keymap.set(\"n\", \"]c\", gs.next_hunk, { buffer = bufnr, desc = \"Next Hunk\" })      vim.keymap.set(\"n\", \"[c\", gs.prev_hunk, { buffer = bufnr, desc = \"Prev Hunk\" })      vim.keymap.set(\"n\", \"\u003cleader\u003egp\", gs.preview_hunk, { buffer = bufnr, desc = \"Preview Hunk\" })      vim.keymap.set(\"n\", \"\u003cleader\u003egb\", function() gs.blame_line { full = true } end, { buffer = bufnr, desc = \"Blame Line\" })    end,  },}diffview.nvim - Git diffの可視化。github.comGit差分をNeovim内で確認できる。ファイル履歴も見やすい。2025年版では、より多くのキーマップを設定している。{  \"sindrets/diffview.nvim\",  keys = {    { \"\u003cleader\u003egd\", \"\u003ccmd\u003eDiffviewOpen\u003ccr\u003e\", desc = \"Git Diff (working tree)\" },    { \"\u003cleader\u003egD\", \"\u003ccmd\u003eDiffviewOpen HEAD~1\u003ccr\u003e\", desc = \"Diff vs previous commit\" },    { \"\u003cleader\u003egh\", \"\u003ccmd\u003eDiffviewFileHistory %\u003ccr\u003e\", desc = \"File History\" },    { \"\u003cleader\u003egH\", \"\u003ccmd\u003eDiffviewFileHistory\u003ccr\u003e\", desc = \"Branch History\" },    { \"\u003cleader\u003egm\", \"\u003ccmd\u003eDiffviewOpen main...HEAD\u003ccr\u003e\", desc = \"Diff vs main branch\" },    { \"\u003cleader\u003egs\", \"\u003ccmd\u003eDiffviewOpen --staged\u003ccr\u003e\", desc = \"Staged changes\" },    { \"\u003cleader\u003egq\", \"\u003ccmd\u003eDiffviewClose\u003ccr\u003e\", desc = \"Close Diffview\" },  },}Diffview内では-でステージ/アンステージ、Sで全てステージ、Xで変更を復元。コンフリクト解決も\u003cleader\u003eco（ours）、\u003cleader\u003ect（theirs）で直感的に操作できる。LSP設定Mason.nvimで言語サーバーを管理。主要な言語はすべてカバー。ensure_installed = {  -- Rust  \"rust-analyzer\",  -- Go  \"gopls\", \"gofumpt\", \"goimports\",  -- TypeScript/JavaScript  \"typescript-language-server\", \"prettier\",  -- Python  \"pyright\", \"black\", \"isort\",  -- Infrastructure  \"terraform-ls\", \"yaml-language-server\",  -- Shell  \"bash-language-server\", \"shellcheck\",}2025年のポイントとして、JSON/YAMLにはSchemaStoreを統合している。package.jsonやdocker-compose.ymlの補完がスキーマベースで効くようになる。github.com5. AIアシスタント統合2024年から2025年にかけて、開発環境で最も大きく変わったのはAIの存在だ。コード補完、生成、レビュー、デバッグ——あらゆる場面でAIが介在するようになった。2025年のPDEにおいて、AIツールは最も重要な要素になっている。私は複数のAIツールを導入しているが、主軸はClaude Codeだ。Claude Code - 開発の中心docs.anthropic.comターミナルで起動し、コード生成、リファクタリング、デバッグ、質問——ほとんどの作業をClaude Codeで完結させている。私の使い方の特徴は、プロジェクトごとにカスタマイズしている点だ。やっていることはシンプルで、3つのファイルを育て続けている。project/├── CLAUDE.md              # プロジェクト固有の指示├── .claude/│   ├── commands/          # カスタムスラッシュコマンド│   │   ├── review.md│   │   └── test.md│   └── agents/            # 特化型エージェント│       └── reviewer.mdCLAUDE.md にはプロジェクトの文脈を書く。使用技術、コーディング規約、避けるべきパターンなど。これがあるとClaude Codeの回答精度が劇的に上がる。commands にはよく使う操作をスラッシュコマンドとして定義する。/reviewでコードレビュー、/testでテスト生成など。毎回同じプロンプトを書く手間が省ける。agents には特定タスクに特化したエージェントを定義する。レビュー専門、リファクタリング専門など、役割を分けることで精度が上がる。重要なのは、これらを使いながら修正し続けること。「この指示だと意図と違う結果になる」と気づいたらCLAUDE.mdを更新する。コマンドの出力が物足りなければcommandを調整する。PDEと同じで、日々の開発の中で育てていく。Neovimとの連携にはclaude-code.nvimを使っている。github.com{  \"greggh/claude-code.nvim\",  keys = {    { \"\u003cleader\u003ecc\", \"\u003ccmd\u003eClaudeCode\u003ccr\u003e\", desc = \"Claude Code\" },    { \"\u003cleader\u003ecr\", \"\u003ccmd\u003eClaudeCodeResume\u003ccr\u003e\", desc = \"Resume Conversation\" },  },}\u003cleader\u003eccでClaude Codeのターミナルウィンドウをトグル。エディタで開いているファイルをそのままClaude Codeに渡せる。Neovim内のAIツールNeovim内ではCopilotとAvanteを併用している。github.comCopilotはインライン補完用。コードを書いている最中に候補が表示され、Tabで確定する。考えながら書くときに便利。copilot-cmpと組み合わせて、補完メニューの最上位にCopilotの提案が表示されるようにしている。github.comCopilotChatはAIチャット用。コードの説明、修正、テスト生成、ドキュメント生成などをチャット形式で行える。モデルはclaude-sonnet-4を使用。{  \"CopilotC-Nvim/CopilotChat.nvim\",  opts = { model = \"claude-sonnet-4\" },  keys = {    { \"\u003cleader\u003eao\", \"\u003ccmd\u003eCopilotChatOpen\u003ccr\u003e\", desc = \"Open Chat\" },    { \"\u003cleader\u003eae\", \"\u003ccmd\u003eCopilotChatExplain\u003ccr\u003e\", desc = \"Explain Code\", mode = { \"n\", \"v\" } },    { \"\u003cleader\u003eaf\", \"\u003ccmd\u003eCopilotChatFix\u003ccr\u003e\", desc = \"Fix Code\", mode = { \"n\", \"v\" } },    { \"\u003cleader\u003eat\", \"\u003ccmd\u003eCopilotChatTests\u003ccr\u003e\", desc = \"Generate Tests\", mode = { \"n\", \"v\" } },    { \"\u003cleader\u003ead\", \"\u003ccmd\u003eCopilotChatDocs\u003ccr\u003e\", desc = \"Generate Docs\", mode = { \"n\", \"v\" } },    { \"\u003cleader\u003eaR\", \"\u003ccmd\u003eCopilotChatReview\u003ccr\u003e\", desc = \"Review Code\", mode = { \"n\", \"v\" } },  },}github.comAvanteはCursorエディタのAI機能をNeovim上に再現するプラグイン。ファイル横断の変更や設計相談に使う。\u003cleader\u003eaaで質問すると、サイドパネルが開いてAIとの対話が始まる。{  \"yetone/avante.nvim\",  opts = {    provider = \"copilot\",    mode = \"agentic\",    providers = {      copilot = { model = \"claude-sonnet-4\" },    },    mappings = {      ask = \"\u003cleader\u003eaa\",      edit = \"\u003cleader\u003eax\",    },  },}AI統合のキーマップまとめ キー  プラグイン  説明  \u003cleader\u003eaa  Avante  AI質問  \u003cleader\u003eao  CopilotChat  チャットを開く  \u003cleader\u003eae  CopilotChat  コードを説明  \u003cleader\u003eaf  CopilotChat  コードを修正  \u003cleader\u003eat  CopilotChat  テスト生成  \u003cleader\u003ead  CopilotChat  ドキュメント生成  \u003cleader\u003eaR  CopilotChat  コードレビュー  \u003cleader\u003ecc  Claude Code  Claude Code起動  \u003cleader\u003ecr  Claude Code  会話を再開 6. 言語別の設定Rustは私のメイン言語なので、専用プラグインを導入している。rustaceanvimでrust-analyzerを強化し、crates.nvimでCargo.tomlのバージョンを管理する。Cargo.tomlを開くと、各クレートの最新バージョンがインラインで表示される。github.comgithub.comGo、TypeScript、Pythonは特別な設定をしていない。LSP設定セクションで示したensure_installedにより、各言語サーバーが自動でセットアップされる。保存時の自動フォーマットはconform.nvimに任せている。詳細な設定はdotfilesリポジトリを参照してほしい。7. フォーマッター統合conform.nvimで保存時に自動フォーマット。言語ごとに適切なフォーマッターを設定。{  \"stevearc/conform.nvim\",  opts = {    formatters_by_ft = {      lua = { \"stylua\" },      rust = { \"rustfmt\" },      go = { \"gofmt\", \"goimports\", \"gofumpt\" },      python = { \"black\", \"isort\" },      typescript = { \"prettier\" },      javascript = { \"prettier\" },      yaml = { \"prettier\" },      json = { \"prettier\" },      markdown = { \"prettier\" },    },    format_on_save = { timeout_ms = 500, lsp_fallback = true },  },}github.comPDEを育てるということPDEは完成することがない。日々の開発の中で、少しずつ手を入れ続ける。刀から庭へこの記事のタイトルには「刀を打つ」と書いた。実際、PDEには刀を打つ行為がある。ターミナルを選び、シェルを設定し、エディタを組み上げる。ゼロから自分の道具を作り上げていく。ただ、刀には完成がある。名刀は打ち上がれば、あとは研ぎ澄ますだけ。床の間に飾られ、鑑賞される。しかしPDEには完成がない。プラグインは更新され、新しいツールが登場し、自分の作業スタイルも変わる。「完成した」と思った翌週には、また何かをいじっている。最初は名刀を打つつもりだった。「理想の開発環境を作り上げる」という完成形を目指していた。だが10年経って気づいた。私が欲しかったのは名刀ではなく、戦場で使える道具だった。戦場で使える道具とは何か。それは、完成を待たずに使い始められるものだ。使いながら調整し、壊れたら直し、足りなければ足す。常に未完成で、常に変化している。ここで気づいた。私がやっていることは、刀を打つだけではない。打った刀を、日々手入れし続けている。使いながら研ぎ、傷がつけば直し、必要に応じて改良する。この「手入れし続ける」という感覚——何かに似ている。そうだ、庭だ。庭も完成しない。季節ごとに姿を変え、草木は勝手に育ち、手入れを怠れば荒れる。人間が設計するが、人間の思い通りにはならない。それでも手を入れ続けることで、少しずつ自分の形になっていく。宇野常寛さんの『庭の話』という本が、この感覚を言語化してくれた。www.kodansha.co.jp宇野さんは、現代のプラットフォーム（SNS）を「相互評価のゲームに特化した空間」として批判し、対抗概念として「庭」を提示する。プラットフォームが画一化された承認欲求の交換の場であるのに対し、庭は「完全にはコントロールできないもの」との共存の場だ。草木が勝手に育ち、虫が飛び交い、季節によって姿を変える。人間が設計するが、人間の思い通りにはならない。この説明を読んだとき、私は自分のPDEのことを思い浮かべた。プラグインが勝手にアップデートされ、設定が壊れ、新しいツールが登場する。思い通りにならない。でも、手を入れ続けることで、少しずつ自分の形になっていく。宇野さんの言う「プラットフォーム」と「庭」の対比は、そのまま開発環境にも当てはまる。IDEはプラットフォームだ。ベンダーが設計し、万人に最適化されたサービスを提供する。ユーザーはそれを消費する。便利で、効率的で、すぐに使える。しかし、自分でコントロールできる範囲は限られている。一方、PDEは刀を打ち、庭として育てるものだ。自分で道具を作り、その道具を手入れし続ける。プラグインが競合し、設定が壊れ、アップデートで挙動が変わる。それでも、手を入れ続けることで、少しずつ自分の形になっていく。消費ではなく、制作。受け取るのではなく、育てる。宇野さんは「消費から制作へ」という転換を説く。プラットフォームで承認を求めるのではなく、制作に没頭すること。エンジニアとして、私たちは「正解」を求めがちだ。最適解を見つけ、効率を最大化し、その成果で報われたいと思う。だが、PDEにはそういう正解がない。「正しい設定」も「最適なプラグイン構成」も存在しない。ネットで見つけた「おすすめ設定」をコピペしても、それは自分の刀にはならない。正解を求めて報われようとするのをやめる。他者から評価される「模範解答」を探すのではなく、自分の手に馴染む道具を、自分のために作る。PDEを構築する行為は、まさにこの「制作」だ。誰かに見せるためではなく、自分のために作る。その過程で、ツールとの対話が生まれる。「家庭」という言葉は「家」と「庭」でできている。宇野さんは「家」の内部で承認を交換するだけでは見えないものが「庭」にはあると言う。開発環境も同じだ。IDEという「家」の中で完結するのではなく、PDEという「庭」に出ることで、ツールとの新しい関係が見えてくる。ツールと思考の相互作用ツールとの新しい関係とは何か。PDEを10年実践する中で、1つ気づいたことがある。ツールは思考に影響し、思考はツールに影響される。これは単なる比喩ではない。以前、AIエージェントとの協働について書いた記事で、私は「集中とは自分の能力ではなく環境との関係である」と述べた。syu-m-5151.hatenablog.com環境との関係——これはPDEにも当てはまる。具体的な例を挙げよう。Vimのモーダル編集を使い始めると、テキスト操作を「動詞＋名詞」で考えるようになる。d（削除）+ w（単語）で「単語を削除」。この思考パターンは、コードを書くときの発想にも影響する。操作を小さな単位に分解し、組み合わせて目的を達成する。逆に、自分の思考スタイルに合わないツールは、どれだけ高機能でも使いこなせない。合わないものは合わない。それだけのことだ。重要なのは、この相互作用を意識的に活用することだ。新しいツールを導入するとき、私は「このツールは自分の思考をどう変えるか」を考える。AIエージェントを使い始めたとき、深く没入する集中から、複数タスクを並行監視する集中へと、思考のモードを切り替える必要があった。環境が変われば、思考も変わるべきなのだ。PDEとは、単にツールをカスタマイズすることではない。自分の思考とツールの関係を最適化し続けることだ。AIは庭の一部か、庭師か2025年のPDEを語る上で、AIツールの位置づけは避けて通れない。私はClaude Codeを「主軸」と書いた。しかし、これは従来のツールとは異なる存在だ。Neovimは私が設定し、私が操作する。一方Claude Codeは、私と対話し、私の意図を解釈し、時に私が思いつかなかったアプローチを提案する。これは庭の一部なのか、それとも共に庭を育てる存在なのか。正直に言えば、まだ答えは出ていない。ただ、1つ確かなことがある。CLAUDE.mdを更新し、カスタムコマンドを調整し、エージェントを育てる——この作業は、Neovimのプラグイン設定と同じ感覚だ。AIツールもまた、PDEの一部として「育てる」対象になっている。同時に、Claude Codeは私のPDEを育てる側でもある。「この設定、冗長では」「こういうプラグインがある」と提案してくる。人間が庭を育て、庭が人間を育てる。その関係がAIツールとの間にも成り立っている。私のPDE改善サイクルでは、具体的にどうやってPDEを育てているのか。私の場合、こんなサイクルを回している。気づく: 「この操作、毎日10回はやってるな」調べる: 既存のプラグインや設定で解決できないか試す: 設定を追加して数日使ってみる磨く: 使いにくければ調整、良ければ定着このサイクルを回し続けていると、つい完璧を目指したくなる。だが、ここで立ち止まる必要がある。すべてを自作する必要はない。すべてをOSSで揃える必要もない。それをやると疲れる。Fish、Warp、Starshipを選んだのも「設定なしで賢い」からだ。力を入れるところと抜くところを分ける。適度にやっていくことが、PDEを長く続けるコツだと思う。これもまた、刀を打ち、庭として育てることの本質だ。名刀を打つなら妥協は許されない。だが戦場で使う刀は違う。多少キズがあっても戦えればいい。庭も同じだ。すべてを自分で育てる必要はなく、買ってきた苗を植えてもいい。大事なのは、戦場で戦えること。実際の開発で使えること。完璧な道具を作ることではない。設定ファイルの管理dotfilesはGitで管理。どのマシンでも同じ環境を再現できる。dotfiles/├── fish/           # Fish shell├── nvim/           # Neovim├── warp/           # Warp terminal├── starship/       # Starship prompt└── git/            # Git config5年後、PDEは存在するかAIの進化を見ていると、ふと考えることがある。5年後、開発環境を「自分で構築する」という行為に意味はあるのか。AIがコードを書き、テストを実行し、デプロイまで行う時代が来るかもしれない。そのとき、エディタの設定にこだわる意味があるのか。Neovimのキーバインドを覚える価値があるのか。正直に言えば、分からない。5年後の開発がどうなっているか、誰にも予測できない。ただ、こうも思う。むしろ逆かもしれない、と。従来、PDEの構築には学習コストとメンテナンスコストがかかり、それに見合う生産性向上が得られるかは不透明だった。しかしAIは、このトレードオフを限りなく等価に近づけてくれる。環境を改善すれば、その改善がAIを介して直接生産性に反映される。CLAUDE.mdを1行書き足せば、その分だけAIの出力が良くなる。PDEが「趣味」から「現実的に有効な投資」になる時代が来ているのかもしれない。ただ、1つだけ確信していることがある。「自分で作りたい」という欲求は消えない。ツールが変わっても、プラットフォームが変わっても、「与えられたものをそのまま使うのではなく、自分の手を入れたい」という欲求は残る。少なくとも、私はそういう人間だ。AIがすべてを生成する時代が来ても、そのAIをどう使うか、どうカスタマイズするか、どう自分のワークフローに組み込むか——そこにPDEの精神は生き続けると思う。道具は変わっても、道具との関係を自分で設計したいという欲求は変わらない。まとめここまで私のPDE構成を紹介してきた。Warp上でFishを動かし、NeovimとClaude Codeを併用する——これらを組み合わせて、自分だけの「刀」を作り上げて「庭」を育てている。PDEを選ぶ理由は効率では説明できない。最初の2週間は生産性が落ちるし、1ヶ月かけて元に戻る。それでも、自分で組み上げ、日々改善していく過程そのものに価値がある。消費ではなく制作、受け取るのではなく育てる。だからといって、完璧を目指す必要はない。名刀を打つ必要はない。すべてを自作する必要もない。大事なのは、明日の開発で戦えること。そのために、今日少しだけ環境を良くする。その繰り返しがPDEだ。もし「自分で作ってみたい」という気持ちがあるなら、試してみてほしい。合わなかったら戻ればいい。IDEという最強の武器は、いつでもそこにある。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。この記事で紹介した設定ファイルは以下のリポジトリで公開している。github.com","isoDate":"2025-12-14T04:25:52.000Z","dateMiliSeconds":1765686352000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『おい、テックブログを書け』というタイトルで登壇しました","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/13/145159","contentSnippet":"はじめに正直に言うと、私はキャリアの序盤、破滅的な文章を書く人間だった。誰が読むのか考えていない文章を書きまくっていた。学生時代に読書感想文のコンクールで優勝したこともなければ、文章を褒められた経験もほとんどない。それでも書き続けて、今はこうして登壇の機会をいただけるようになった。2025年12月5日、Forkwell Communityのイベント「おい、テックブログを書け」で登壇しました。forkwell.connpass.com発表資料はこちらです。 speakerdeck.com「おい、」シリーズがイベントになった私は「おい、」シリーズというブログを書いている。元々は書籍用に書き溜めていた文章を公開する場所として始めたものだが、ありがたいことに多くの反響をいただいている。syu-m-5151.hatenablog.com今回のイベントは、Forkwellのかわまたさんにお誘いいただいて実現した。かわまたさんには以前も「転職したらMCPサーバーだった件」というイベントでお声がけいただいた。 speakerdeck.com貴重な登壇の機会をいただいているのにこんなことを言うのはあれだが、結構変なことをさせてくれる。変な人だ（褒めている）。自分もこれぐらいふざけた企画をできるくらい組織で信用されたい。とめちゃくちゃに思う。こうした機会をもらえるのは、発信を続けてきたからだ。私よりエンジニアとしても語り手や書き手としても才能のある人はたくさんいる。でも、その才能を発揮せずに誰からも見つからないままでいる人も多い。なぜ発信しないのか。まず、炎上が怖い。間違ったことを書いたら叩かれるんじゃないか。知識不足を晒して恥をかくんじゃないか。そう思うと、公開ボタンを押す手が止まる。次に、時間がない。業務が終わってから記事を書くのは大変だ。言いたいことを整理して、文章にまとめて、推敲して。そこまでの気力が残っていない日も多い。組織の問題もある。評価制度が発信を評価しない会社では、ブログを書いても給料は上がらない。それどころか「そんな暇があったらコードを書け」と言われることもある。発信は「業務外の趣味」として扱われる。こうした障壁は確かに存在する。でも、それらすべてを解決してから書き始める必要はない。まず書いてみることなら、今日からでもできる。炎上が怖いなら、小さな技術メモから始めればいい。時間がないなら、完璧を目指さず短い記事でいい。組織が変わらなくても、自分のブログは自分で始められる。書き始めるとき、人は出発点ばかり気にしがちだ。「自分には文章の才能がない」「最初からうまく書ける人には敵わない」──そう思って発信をためらう人がいる。でも、書く力は後天的になんとかなる。出発点が低くても、続けていれば追いつける。追い越せることだってある。見てくれた皆さんには、発信やアウトプットを通じて才能を発揮し、それに見合った評価や機会を得てほしい。そう思って今回の登壇資料を作った。書くときに大切にしていること資料では「どう書くか」の型を紹介したが、その前提にある考え方も書いておきたい。私が意識しているのは3つある。「なぜ」を問うこと、「変化」を描くこと、「ゆらぎ」を残すこと。この3つは独立しているようで、実は重なり合っているので紹介していきたい。「なぜ」を問い続ける単なる事実や記録ではなく、理由や背景を深掘りする姿勢が大切だ。たとえば「Aを使った」だけでなく「なぜAを選んだのか」「なぜBではダメだったのか」を書く。読者が最も知りたいのは「なぜ」の部分だ。選択の理由を言語化することで、自分の理解も深まる。ところが、技術ブログでありがちなのは、手順だけを淡々と書いてしまうこと。「この設定を入れます」「このコマンドを実行します」──それだけでは公式ドキュメントの劣化コピーになる。「なぜこの設定なのか」「なぜこの順番なのか」「なぜ他の方法ではダメだったのか」を書くことで、初めて読む価値が生まれる。「なぜ？」の部分が業務事情に抵触する場合もある。具体的な数値や社内の意思決定プロセスは書けない。そういうときは、一般的な観点に置き換える工夫をすればいい。「弊社の事情で」ではなく「〇〇のようなケースでは」と書く。具体的な比較ができないなら「一般的にAとBにはこういう違いがある」と整理する。工夫次第で、機密を守りながら「なぜ」を伝える方法はいくらでもある。「なぜ？」を問い続けると、自分の理解の浅さに気づくこともある。それでいい。書くことは、自分の理解を試す行為でもある。書けないということは、わかっていないということだ。その気づきこそが成長の起点になる。「行動」と「変化」のあるストーリーにする「なぜ」を問い続けていると、自然と「変化」が見えてくる。最初はこう思っていた、でも調べていくうちにこう変わった。その変化こそが、記事の核になる。人は変化の物語に心を動かされる。問題に出会い、試行錯誤し、解決に至る。その過程で自分の理解がどう変わったか。「わからない」から「わかった」への変化こそが、読者にとって価値のある情報だ。だから、静的な情報の羅列は退屈だ。「Kubernetesのリソース制限には以下の種類があります」と書くより、「OOMKilledで3時間溶かした。原因を調べていくうちに、リソース制限の仕組みが腹落ちした」と書く方が読まれる。同じ情報でも、変化の物語として語ることで、読者は追体験できる。行動と変化を意識すると、自然と時系列が生まれる。最初に何を思っていたか、何をしたか、何が起きたか、どう理解が変わったか。この流れがあるだけで、記事は格段に読みやすくなる。そして、変化には「失敗」も含まれる。むしろ失敗からの学びの方が読者には刺さる。「最初からうまくいきました」という記事より、「こう考えて失敗し、別のアプローチで解決した」という記事の方が、読者の記憶に残る。失敗を隠さず、そこから何を学んだかを書くことで、記事に深みが出る。「気持ちのゆらぎ」を素直に残す失敗を書くとき、その時の迷いや不安も一緒に残しておくといい。整いすぎた文章は、かえって心に響かない。なぜか。人間味が消えてしまうからだ。「最初は〇〇だと思っていたけど、実際は違った」「正直、これでいいのか迷った」「ここは今でも自信がない」──そうした揺れを正直に書くことで、読者との距離が縮まる。完璧を装う必要はない。技術ブログを書くとき、つい「わかっている人」として振る舞いたくなる。でも、読者が共感するのは「わかっていなかった人がわかるようになる過程」だ。迷い、間違え、遠回りした経験こそが、読者にとって価値がある。気持ちのゆらぎを残すことには、もう1つ意味がある。後から読み返したとき、その時の自分に出会える。「あの頃はこんなことで悩んでいたのか」と思えるのは、整いすぎていない文章だからこそだ。ゆらぎを残すことに抵抗がある人もいるだろう。弱く見えるのではないか、と。でも私の考えは違う。ゆらぎを残すことは、弱さを見せることではない。誠実さを見せることだ。「これが正解です」と断言する記事より、「私はこう考えてこうした、でも別の方法もあるかもしれない」と書く記事の方が、読者は信頼する。技術の世界に絶対の正解は少ない。その不確かさに正直であることが、かえって記事の信頼性を高める。おわりに技術ブログを書くことは、自分の成長のためだ。「なぜ？」を問い続け、変化の物語として語り、気持ちのゆらぎを素直に残す。結果として、それが誰かを救うこともあるかもしれない。私自身がそうだった。冒頭で書いたように、私は「破滅的な文章を書く人間」だった。それでも書き続けて、今がある。苦手から逃げても、その先にあるのはまた別の苦手だ「文章が苦手だから書かない」「人前で話すのが苦手だから発信しない」──そう言って避け続ける人は多い。気持ちはわかる。苦手なことに向き合うのは辛い。できない自分を直視するのは苦しい。でも、逃げた先に何があるだろうか。苦手なことを避け続けても、人生から苦手がなくなるわけではない。文章から逃げれば、別の場面でまた「苦手」にぶつかる。逃げ続けた結果、選択肢がどんどん狭まっていく。気づいたときには、逃げ場すらなくなっている。いま苦手であることと、将来成果を出せるかどうかには、おそらく何の因果関係もない。初期能力が高い人が最終的に優れた成果を出すとは限らない。むしろ、最初から得意な人は壁にぶつかったとき折れやすい。苦手だった人の方が、泥臭く続ける力を持っていたりする。私は明らかに後者だった。最初からうまく書けたわけではない。読み返すと恥ずかしい文章をたくさん書いた。それでも書き続けた結果、今がある。出発点の低さは、到達点を決めない。「自分探し」という名の逃避「自分に向いていることを見つければ、苦労せずに成果が出る」──そんな幻想がある。「自分探し」という言葉は、その幻想を正当化する。本当の自分を見つければ、努力なしに輝ける場所がある。そう信じたい気持ちはわかる。でも、多くの場合それは苦手や欠損から逃れるための言い訳でしかない。向いていないから別のことを探す。それも向いていないから、また別のことを探す。その繰り返しで時間だけが過ぎていく。本当の自分は、探すものではない。目の前のことに向き合い、苦手なことに取り組み続ける中で、少しずつ形作られていくものだ。「これが自分だ」と思えるようになるのは、何かをやり抜いた後だ。やる前からわかるものではない。向いていることを探すより、目の前のことに向き合う方が、よほど確実に成長できる。向いているかどうかは、やってみなければわからない。やり続けてみなければわからない。最初の苦手意識だけで判断するのは、あまりに早すぎる。正しい方向に努力すれば、必ず上達するとはいえ、漫然と続けるだけでは上達しない。書くことと、うまくなることは、自動的にはつながらない。読者の反応を見る。うまい人の文章を読んで、何が違うのか考える。自分の過去記事を読み返して、恥ずかしくなる。そうやってフィードバックを受け取り、意識的に改善しようとすることで、少しずつ書けるようになる。大事なのは、フィードバックループを回すことだ。書く→反応を見る→改善点を見つける→また書く。このサイクルを回し続ければ、必ず上達する。才能の有無ではなく、このループを回し続けられるかどうかが、成長を決める。こう書くと「それは書けた側の言い分だ」と思う人もいるかもしれない。生存者バイアスじゃないか、と。確かにそうだ。書けなかった人の声は届かない。でも、だからこそ書けた側が伝えるしかない。書けないと思っている人、文章に自信がない人に、「それでも書ける」と伝えられるのは、同じ場所から歩いてきた人間だけだ。だから、今日からでも始めてほしい。まずは今日学んだこと、ハマったこと、気づいたことを3行だけ書いてみる。下書きのまま放置している記事があるなら、不完全でもいいから公開してみる。完璧を待っていたら、いつまでも始まらない。苦手だと思っていることほど、始めてしまえば案外なんとかなる。登壇・技術顧問のご依頼について登壇依頼はいつでも募集しています。今回のようなちょっと変わった企画でも大歓迎です。気軽にDMしてください。また、技術顧問業もやっています。SRE、プラットフォームエンジニアリング、組織づくりなど、雑多な質問でもお待ちしております。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。","isoDate":"2025-12-13T05:51:59.000Z","dateMiliSeconds":1765605119000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"専門家は話さないですよ(『専門家が「力」をセーブせずに全力で専門性を振り回してもリスペクトされる組織をつくりたい』を読んで)","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/12/163220","contentSnippet":"はじめに正直に言う。この文章を書くかどうか、ずいぶん迷った。「専門家はもっと声を上げるべきだ」という意見に対して、「いや、話さないんですよ」と返すのは、なんだか後ろ向きに見えるかもしれない。諦めているように聞こえるかもしれない。そういう風に受け取られるのは、ちょっと嫌だな、と思った。でも、書くことにした。なぜなら、「話せばいいじゃん」「振りかざせばいいじゃん」という言葉に、ずっと違和感を抱えてきたからだ。その違和感の正体を、自分なりに言葉にしてみたかった。これは、専門家として組織の中で働いてきた、私個人の経験と考えだ。すべての人に当てはまるとは思わない。でも、同じような経験をしている人が、もしかしたらいるかもしれない。そういう人に届いたらいいな、と思いながら書いている。「専門性の刃で殴りかかってこい」への違和感「専門家が『力』をセーブせずに全力で専門性を振り回してもリスペクトされる組織をつくりたい」という記事を読んだ。専門家はプロらしく専門知識を振りかざしてほしい。そこに忖度はいらない。殺す気でかかってきていい。言っていることが難しくて良い。専門家ってのは、そういうもんだろ、と。痛快だし、気持ちはわかる。専門家がセーブせずに力を振るえる組織という理想像は、多くのエンジニアやデザイナーが心の底で望んでいることだろう。その理想を堂々と語る姿勢には敬意を覚える。フジイさんの記事は、専門家の側に立って「もっと力を発揮していいんだ」と背中を押してくれる。それ自体は素晴らしいことだ。でも、私はこの説明に違和感がある。そして、専門家としては、そういう組織を期待して待っていても仕方ないと思っている。専門家は話さない。振りかざす以前の問題として、そもそも話さない。fujii-yuji.net話すことの面倒くささ専門家が話さない理由は、実はとても単純だ。面倒くさいのだ。自分の中にある専門的な知見を言葉にして口から出した瞬間、それは相手の解釈に委ねられる。どれだけ正確に伝えようとしても、相手の受け取り方は相手次第だ。ずれが生じる。これはいかなるコミュニケーションにおいても不可避だ。そして、ずれた理解に基づいて余計なことを言われる。「それってつまりこういうことですよね」と、全然違う解釈を返される。「でもそれって現実的じゃないですよね」と、文脈を無視した反論が来る。そのたびに「いや、そういうことではなくて」と釈明しなければならない。これが、本当に面倒くさい。だから専門家は話さない。話しても伝わらないし、伝わらなかったときの釈明が面倒くさいから。専門家の言葉が届かなくなるとき他にも組織の中で、専門家の言葉が届かなくなる瞬間がある。エンジニアが「この設計だと将来困ります」と言っても、「今はそれどころではない」と退けられる。デザイナーが「このUIは使いにくい」と指摘しても、「ユーザーは慣れる」と押し切られる。セキュリティの専門家が「この実装は危険です」と警告しても、「リリースを優先して」と言われる。なぜこうなるのか。専門家の意見と非専門家の意見が、同じ重みで扱われるからだ。あるいは、声の大きさや立場の強さで、専門家の意見が上書きされるからだ。「それはあなたの感想ですよね」と言われる。「他の見方もある」と言われる。正しいことを言っているのに、「意見の違い」として処理される。これは単なる無関心ではない。専門知への拒絶だ。専門家が何か言うと、面倒くさがられる。「また難しいことを言っている」「理想論だ」「現場を知らない」と思われる。そのうち、専門家の発言自体が疎まれるようになる。私はこれを「専門家の言葉が死ぬ瞬間」だと思っている。言葉が発せられても、届かない。届いても、受け止められない。受け止められても、行動に変わらない。そういう組織では、専門家は黙るようになる。分業が生む視野の狭さなぜこうなるのか。私なりに考えてみた。こんな経験がある。セキュリティの観点から「この実装は危険だ」と2回警告した。2回とも「リリースを優先して」と言われた。3回目は言わなかった。そして半年後、その実装が遠因でインシデントが起きた。「なぜ発生した」と言われた。言ったんだけどな、と思った。組織が大きくなると、分業が進む。営業、開発、デザイン、マーケティング。それぞれが専門性を高め、効率よく仕事を回せるようになる。これ自体は正しい。でも、分業には副作用がある。自分の担当範囲だけを見るようになる。隣のチームが何をしているか、知らなくても仕事は回る。全体像が見えなくなる。自分の視野がどんどん狭くなっていることに、気づかない。視野が狭くなると、判断がずれる。自分の担当範囲では正しいことが、全体で見ると間違っていることがある。でも、全体が見えないから、そのずれに気づけない。そして、何かを変えようとしても、壁にぶつかる。「それは私の管轄じゃない」「そっちのチームに言ってくれ」「今はそれどころじゃない」。組織の境界線が、行動を阻む。やがて、組織全体が「どうもうまくいっていない」と感じるようになる。でも、何が問題なのかがわからない。みんなが自分の持ち場で懸命に働いているのに、全体としては空回りしている。これが、ある程度成熟した組織が陥る罠だ。誰かが悪いわけではない。構造がそうさせている。話しても届かない構造この構造の中で、専門家はどうなるか。まず、自分の視野が狭くなっていることに気づかなくなる。自分の担当領域のことしか見えない。全体像が見えないから、自分の懸念が組織全体にとってどれだけ重要か、判断できない。「言っても仕方ない」と思ってしまう。次に、何か言っても壁にぶつかる経験を重ねる。「それは私の管轄ではない」「今はそれどころではない」と言われる。何度かそういう経験をすると、言うこと自体をやめる。学習性無力感だ。そして、本質的な問題を指摘しても、表面的な対応で済まされる。「技術的負債を返済しないと」と言っても、「今月のリリースが優先だ」と返される。根本的な問題が見えない組織では、根本的な指摘は届かない。専門家が話さないのは、怠けているからではない。プロ意識が低いからでもない。話しても届かない構造の中に置かれているからだ。何度も壁にぶつかって、学習した結果だ。専門家と非専門家の間にある溝専門家の言葉が届かないのは、誰かが悪いからではない。専門家は自分の領域を深く知っている。だからこそ、何が重要で何が危険かがわかる。でも、その「わかる」が、相手に伝わるとは限らない。非専門家には、非専門家の世界がある。締め切りがあり、予算があり、上からのプレッシャーがある。彼らは彼らなりに合理的に判断している。専門家の言うことが理解できないとき、「今は優先度が低い」と判断するのは、彼らにとっては当然のことだ。つまり、どちらも自分の世界では正しいことを言っている。問題は、それぞれの世界が交差しないことだ。専門家の「危険です」と、非専門家の「今はそれどころじゃない」が、同じ言語で話されているようで、まったく違う文脈に立っている。この溝を埋めるには、お互いの世界を理解しようとする努力がいる。でも、その努力には時間がかかる。そして、時間をかける余裕がない組織では、溝は埋まらないまま放置される。専門知識を振りかざしても、この溝は埋まらない。むしろ、溝を広げてしまうことさえある。「振りかざす」だけでは何も変わらないフジイさんは「専門知識を振りかざせ」と言う。力をセーブするな、忖度するな、と。気持ちはわかる。でも、私の経験では、振りかざしてもうまくいかなかった。専門家が強く主張すればするほど、相手は身構える。「また難しいことを言い始めた」「仕事を遅らせるつもりか」と思われる。こちらは正しいことを言っているつもりなのに、「面倒くさい人」として扱われる。そして、一度そういう印象を持たれると、次から話を聞いてもらえなくなる。「あの人はいつも理想論ばかり言う」というレッテルが貼られる。正しいことを言っているのに、聞いてもらえない。悪循環だ。振りかざすという態度は、相手に「自分の世界を押し付けられている」と感じさせる。人は押し付けられると、反発する。これは自然な反応だ。だから、振りかざしても状況は良くならない。むしろ、悪くなることのほうが多い。対話とは何かじゃあ、どうすればいいのか。私は「対話」だと思っている。ただし、ここで言う対話は「話し合う」という単純なものではない。対話とは、相手の世界に入っていくことだ。相手が何を見ているのか。何を気にしているのか。何を恐れているのか。どういうプレッシャーの中にいるのか。それを理解しようとすること。そして、理解した上で、相手の言葉で、相手の文脈で、自分の知っていることを伝えること。これは「振りかざす」とは正反対の態度だ。振りかざすとは、自分の世界から一歩も出ないまま、相手に自分の言葉を投げつけること。相手が理解できないなら、相手が悪い。対話とは、自分の世界を一度脇に置いて、相手の世界に足を踏み入れること。相手の言葉で考え、相手の文脈で説明する。これは、とても難しい。そして、とても面倒くさい。対話のコストを払える組織対話には膨大なコストがかかる。相手の世界を理解するために時間をかける。相手の言葉で説明するために言葉を選ぶ。ずれが生じたら、丁寧に修正する。誤解が生まれたら、根気強く解きほぐす。このコストを、組織が払えるかどうか。「今月のリリースが優先だ」「そんな時間はない」「とにかく早く作れ」という組織では、対話のコストは払えない。対話に時間をかける人は「仕事が遅い人」として評価を下げられる。対話のコストを払える組織とは、どういう組織か。意思決定のプロセスに専門家を巻き込む組織。評価制度が「言われたものを早く作る」ではなく「価値あるものを作る」を評価する組織。専門家の意見が「めんどくさいこと」ではなく「必要なこと」として扱われる組織。そして、相手の世界を理解しようとする姿勢が、当たり前のこととして共有されている組織。対立を放っておかない対立は放っておくと腐る。私自身、何度も失敗した。相手の話を遮って、自分の正しさを主張して、結局何も変わらなかった。そのたびに学んだのは、急いで反応しないことの価値だ。対立を放っておくと気まずさが積み重なる。仕事の判断がぶれ、人が協力しにくくなる。でも、対立が起きた瞬間に「正しさ」で押し切ろうとすると、もっと悪くなる。私はそれを何度も経験した。だから今は、衝突の場面では一度立ち止まるようにしている。相手の話を最後まで聞く。相手が何を恐れているのか、何を守ろうとしているのかを理解しようとする。それだけで、相手の硬さがゆるむことがある。争点をはっきりさせると、不要な言い合いが減る。「ここは合意できる」「ここは意見が違う」と整理するだけで、議論が前に進む。小さな合意を積み上げると、相手への不信が弱まる。これは言うのは簡単だが、やるのは難しい。私も何度も失敗した。でも、やる価値はある。専門家が話せる組織を作るというのは、対立を避けることではない。対立が起きたときに、それを丁寧に扱える組織を作ることだ。「あなたになら話したい」専門家が話すのは、話しても大丈夫だと思える相手に対してだけだ。自分の言葉が曲解されない。余計な解釈を加えられない。「そういうことではない」と釈明する必要がない。相手が自分の世界を理解しようとしてくれている。そういう相手に対してだけ、専門家は話す。「あなたになら話したい」——この感覚が、専門家に話をさせる。話すことのリスクでも、「あなたになら話したい」と思える相手は、実はとても少ない。専門家が話さないのは、構造の問題だけではない。話すこと自体に、あまりにもリスクがある。曲解される。余計なことを言われる。釈明が必要になる。プライドを刺激する。張り合われる。情報を軽々しく扱われる。他の人に言いふらされる。これだけのリスクを負って、それでも話す価値があるか。多くの場合、ない。だから専門家は黙る。専門知識を振りかざすどころか、そもそも口を開かない。コミュニケーションの不可避的なずれどれだけ丁寧に対話しても、ずれは生じる。私が話したい出来事が言葉となって口から出た時点で、それは私のものではなくなる。相手がどう受け取るかは、相手次第だ。これは、いかなるコミュニケーションにおいても不可避だ。だからこそ、対話のコストを払う意志があるかどうかが重要になる。ずれが生じたときに、「そういうことではない」と切り捨てるのではなく、「どうずれているのか」を一緒に探る。誤解が生まれたときに、「わかってないですね」と責めるのではなく、「どう誤解されたのか」を一緒に確認する。そして、聞いたことを軽々しく他の人に話さない。他者の情報を、自分の優越感のために消費しない。そのコストを払う意志があり、その倫理観を共有できる組織に対してだけ、専門家は話す。組織に期待しても仕方ない専門家が専門家としてリスペクトされる組織。フジイさんが描く理想は、私も心から望んでいる。でも、そういう組織を期待して待っていても、来ない。組織が変わるのを待っていたら、専門家は永遠に黙ったままだ。「いつか理解してくれる組織に出会えるはず」「いつかリスペクトされる日が来るはず」。そう思って待っていても、その日は来ない。だから、専門家の側から動くしかない。振りかざすのではなく、対話する。相手の世界に入っていく。相手の言葉で、相手の文脈で、自分の知っていることを伝える。面倒くさいけど、そのコストを自分から払う。組織が対話のコストを払ってくれるのを待つのではなく、自分から払う。相手が自分の世界を理解してくれるのを待つのではなく、自分から相手の世界を理解しにいく。これは不公平だ。専門家の側だけが努力するのはおかしい。でも、待っていても状況は変わらない。専門家に「振りかざせ」と言うのは、順番が逆だ。でも、「組織が変われ」と言うのも、期待しすぎだ。組織は簡単には変わらない。変わるのを待っていたら、自分が消耗するだけだ。だから、自分から動く。対話のコストを、自分から払う。専門家は話さない、でも専門家は話さない。話しても届かないから。話しても曲解されるから。話しても釈明が面倒くさいから。話したことを軽々しく扱われるから。他人を嫌いになりたくないから。専門家の言葉が届かなくなった組織では、専門家は黙る。それは怠慢ではない。何度も壁にぶつかった結果の、合理的な適応だ。でも、黙ったままでいいのか。フジイさんの理想は素晴らしい。専門家がリスペクトされる組織。専門家が力をセーブせずに振るえる組織。私もそういう組織で働きたい。でも、そういう組織を待っていても来ない。だから、自分から動くしかない。振りかざすのではなく、対話する。面倒くさいけど、相手の世界に入っていく。組織が変わるのを待つのではなく、自分から対話のコストを払う。「あなたになら話したい」——そう思ってもらえる相手に、自分からなる。組織に期待するのではなく、自分がその一人目になる。振りかざせと言う前に、自分から対話のコストを払う。それが、専門家として生き残る唯一の方法だと、私は思っている。私もまだ道半ばだ。何度も失敗するし、面倒くさいと思うこともある。でも、やるしかない。一緒にやっていこう。おわりにここまで書いてきて、ふと思う。結局、私は何を言いたかったんだろう。「専門家は話さない」という事実を伝えたかったのか。「組織に期待するな」と言いたかったのか。「自分から動け」と説教したかったのか。たぶん、どれでもない。私が本当に言いたかったのは、「話さないことを選んでいる自分を、責めなくていい」ということかもしれない。黙っているのは怠慢じゃない。何度も壁にぶつかって、学習した結果だ。それは合理的な適応だ。でも同時に、「黙ったままでいいのか」という問いも、ずっと抱えている。この矛盾を、私はまだ解決できていない。だから、「自分から対話のコストを払う」という答えを、自分自身に言い聞かせているのかもしれない。フジイさんの記事に対する反論というより、自分への言い訳、あるいは自分への励まし。そういう側面もあると思う。この文章を読んで、「わかる」と思ってくれた人がいたら嬉しい。「違う」と思った人もいるだろう。それでいい。ただ、もし同じような経験をして、同じように黙ることを選んでいる人がいたら、伝えたい。あなたは間違っていない。でも、黙ったままでいいのか、という問いは、たぶん消えない。その問いと一緒に、私はこれからも対話のコストを払い続けるんだと思う。面倒くさいけど。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。","isoDate":"2025-12-12T07:32:20.000Z","dateMiliSeconds":1765524740000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2025年 俺が愛した本たち 技術書編","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/11/104143","contentSnippet":"はじめに「今年読んで良かった本」という記事を書こうとしている自分に、ふと気づく。また書くのか。毎年書いている。誰に頼まれたわけでもないのに、12月になると決まってこの作業を始めてしまう。習慣なのか、義務感なのか、それとも単なる自己顕示欲なのか。たぶん、全部だ。100冊近く読んだ、と書こうとして手が止まる。この数字を出した瞬間、どこかで「すごいですね」と言われたい自分がいる。同時に、「いや、冊数なんて意味ないですから」と予防線を張りたがっている自分もいる。めんどくさい人間だ。でも正直に言えば、100冊読んだことより、1冊を血肉にできた人のほうがよほど偉いと本気で思っている。思っているのに、冊数を書いてしまう。そういう矛盾を抱えたまま、この文章を書いている。AIに聞けば答えは返ってくる。2025年はそういう年だった。コードを書いてもらい、設計を相談し、ドキュメントを要約させた。便利だ。本当に便利だ。では、なぜ本を読むのか。300ページもある本を、わざわざ最初から最後まで読む必要があるのか。たぶん、効率の悪さが必要なのだ。AIは正解を返してくれる。でも正解だけでは、何かが足りない。正解を得ることだけが目的なら、エンジニアをやっている意味がない。でも、そうじゃないはずだ。著者が失敗した話。遠回りした話。「今思えば、あれは間違いだった」という告白。そういう「ノイズ」が、不思議と頭に残る。正解は忘れる。でも、誰かの失敗談は覚えている。本を読んでいる時間、私は著者と対話している。いや、対話というより、ほとんど独り言だ。「それはそうだろう」と頷いたり、「いや、それは違うんじゃないか」と反発したりする。声には出さないけれど、頭の中ではずっと喋っている。その過程で、借り物の知識が少しずつ自分の言葉に変わっていく。検索では得られないもの。それを「身体性」と呼ぶのは大げさかもしれないけれど、他に適切な言葉が見つからない。読んだだけでは意味がない、と言われてきた。アウトプットしないと身につかない。実践しないと血肉にならない。わかっている。わかっているけれど、私は本を読むこと自体が好きなのだ。ページをめくる時間が好きだ。知らない概念や文脈に出会う瞬間が好きだ。だからブログを書き、登壇し、実務で試してきた。好きなことを正当化するために、アウトプットという免罪符を手に入れようとしていたのかもしれない。以下に紹介する26冊は、今年の「ベスト」ではない。そんな客観的な評価ができるほど、私は公平な人間ではない。単に「私に刺さった本」を並べただけだ。他の人には響かないかもしれない。でも、この26冊との出会いが、私の2025年を形作った。それだけは確かなことだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。はじめに昨年以前に紹介した本2025年に読んでよかった技術書Beyond Vibe CodingLLMOpsGenerative AI Design PatternsBuilding Applications with AI AgentsLearning GitHub CopilotTerraform in DepthArgo CD: Up and RunningEffective Platform EngineeringData Engineering Design Patternsソフトウェア設計の結合バランスFacilitating Software ArchitectureArchitecture ModernizationBuilding Event-Driven Microservices, 2nd EditionTaming Your Dragon: Addressing Your Technical DebtRefactoring to RustJust Use Postgres!The Software Engineer's Guidebookバックエンドエンジニアのためのインフラ・クラウド大全作る、試す、正す。アジャイルなモノづくりのための全体戦略良いコードの道しるべClean Code, 2nd Edition型システムのしくみFundamentals of Software EngineeringThe Product-Minded EngineerThe Engineering Leader\"Looks Good to Me\"おわりに昨年以前に紹介した本2022年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2023年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2023年 俺が愛した本たち 非技術書編 - じゃあ、おうちで学べる2024年 俺が愛した本たち 技術書編 - じゃあ、おうちで学べる2024年 俺が愛した本たち 非技術書編(物語を除く) - じゃあ、おうちで学べる2025年に読んでよかった技術書Beyond Vibe Codinglearning.oreilly.comwww.oreilly.co.jp※日本語翻訳版が出版予定です。AIツールの導入が進む現場で、私が感じていた違和感がありました。生産性は上がっている。コードは早く書ける。しかし、チームメンバーがAI生成コードについて質問されたとき、「なぜこう書いたのか」を説明できない場面が増えている。Google ChromeチームのAddy Osmani氏による本書は、この違和感に「Vibe Coding」という名前を与えてくれました。Vibe Codingとは、AIが生成したコードを深く理解せずに受け入れてしまう傾向のことです。動くコードと、理解しているコードは違う——この区別は、個人の学習だけでなく、チームの品質管理にも直結します。レビューで「なぜこの実装なのか」と聞かれたとき、「AIがそう書いたから」では通らない。コードの責任は、書いた人間にある。著者は、AIを「単独で使うツール」ではなく「ペアプログラマ」として捉えることを提唱しています。この主張には同意するが、同時に違和感もある。ペアプログラマは、隣に座って一緒に考える存在だ。しかしAIは、こちらが何を求めているか察してくれない。問いを投げなければ答えは返ってこない。つまり、AIを「ペア」として機能させるためには、人間の側に「何を聞くべきか」を知っている力が必要になる。結局、AIを活かせるかどうかは、使う側の問いの質で決まる。ペアプログラマという比喩は美しいが、その美しさに甘えてはいけない。本書の終盤では、自律型コーディングエージェントがもたらす未来像が描かれています。テスト失敗時に自動で修正を試みたり、依存関係の更新PRを生成したりする世界。技術的には魅力的ですが、著者は冷静です。「AIが生成したコードの責任は、承認者にある」——この原則は変わらない。むしろ、エージェントが自律的に動くほど、人間による検証の重要性は高まる。この視点は、運用の現場を知っている人間には納得感があります。AIは相棒であって、魔法使いではない。本書は、その現実を直視しながら、AIとの協働をチームに根付かせるための実践的な指針を提供してくれます。Beyond Vibe Coding: From Coder to AI-Era Developer (English Edition)作者:Osmani, AddyO'Reilly MediaAmazonLLMOpslearning.oreilly.comLLM（Large Language Model、大規模言語モデル）を本番環境で運用し始めると、従来のMLOpsの知見だけでは対応できない課題に直面します。モデルの挙動が予測しにくい、コストが桁違いに高い、出力の品質をどう保証するか分からない——そんな困難にぶつかったとき、本書を手に取りました。著者が掲げるLLMOpsの4つの目標——信頼性、スケーラビリティ、堅牢性、セキュリティ——を見たとき、既視感がありました。これは、システムを運用する人間が長年追求してきた目標と重なる。新しい技術領域でも、運用の本質は変わらない。これまで培ってきた原則は、LLMにも適用できる——その確信を得られたことは、本書を読んだ大きな収穫でした。しかし、ここで私は立ち止まる。「従来の原則が適用できる」という安心感は、危うさも孕んでいる。LLMには従来のシステムにはない難しさがあるからだ。従来のMLモデルは、入力に対して比較的予測可能な出力を返す。しかしLLMは、同じプロンプトでも異なる応答を返すことがある。そもそも「正しい出力とは何か」が曖昧なのです。従来のシステムでは「期待する出力」を定義できた。LLMでは、それ自体が困難になる。この不確実性を前提に、どうSLO（Service Level Objective、サービスレベル目標）を設計し、どうモニタリングするか。本書はその実践的なアプローチを示してくれます。コスト管理の章も現実的で良かった。LLMのAPI呼び出しは、従来のマイクロサービス呼び出しと比較して桁違いにコストがかかる。機能要件を満たすことと、コストを現実的な範囲に収めること。このトレードオフを意識した設計と運用の知見は、実務で即座に役立つものばかりです。「正しい出力」が定義できないシステムを、どう運用するか。答えは、まだ業界全体で模索中です。正解がないから、難しい。正解がないから、面白い。本書はその議論の出発点として、LLMを本番で動かす人が押さえておくべき基盤を提供してくれます。LLMOps: Managing Large Language Models in Production (English Edition)作者:Aryan, AbiO'Reilly MediaAmazonGenerative AI Design Patternslearning.oreilly.comLLMを使ったアプリケーションを作り始めると、繰り返し同じような問題にぶつかります。ハルシネーション（AIが事実と異なる内容をもっともらしく生成してしまう現象）をどう防ぐか。長いコンテキストをどう扱うか。出力の品質をどう担保するか。これらの問題には、すでに先人たちが見つけた解決策がある。本書は、そうしたLLMの限界を克服するための32のデザインパターンを体系化した一冊です。RAG（Retrieval-Augmented Generation、検索拡張生成）、Chain of Thought（思考の連鎖）、Guardrails（安全装置）といったパターンは、今やLLMアプリケーション開発の共通言語になりつつあります。これらのパターンを知っているかどうかで、設計の議論がスムーズになるし、チーム内での認識合わせも早くなる。本書の価値は、単にパターンを列挙していることにあるのではありません。各パターンがなぜ必要か、どのような問題を解決するのか、そしてどのようなトレードオフがあるのか——その背景まで丁寧に解説している点にあります。例えば、RAGパターン。ハルシネーションの軽減策として有効なのは広く知られている。しかし本書は、RAGの導入がもたらす新たな課題も明確に指摘しています。ベクトルデータベースという新しいコンポーネントが加わり、監視対象と障害点が増える。検索の精度がLLMの出力品質を左右するため、検索システムの品質保証という新たな運用課題が生まれる。解決策は、新しい問題を連れてくる——技術選定の現場では、この現実を織り込んだ上で判断する必要があります。Chain of Thoughtパターンも同様です。複雑な推論を段階的に行わせることで出力精度が向上する。しかし、精度を上げれば、コストも上がる。APIコールが複数回になり、レイテンシーとコストが増加する。プロダクトとして許容できるコストとレイテンシーの範囲内で、どこまで精度を追求するか。このトレードオフは、技術だけでなくビジネス要件との兼ね合いで決まります。パターンを知っているかどうかで、設計の選択肢が変わる——本書は、パターンカタログとしても、チームでアーキテクチャを議論するための共通言語としても活用できます。Generative AI Design Patterns: Solutions to Common Challenges When Building GenAI Agents and Applications (English Edition)作者:Lakshmanan, Valliappa,Hapke, HannesO'Reilly MediaAmazonBuilding Applications with AI Agentslearning.oreilly.comLLMを使ったアプリケーションの次のステップとして、AIエージェントへの関心が高まっています。単に質問に答えるだけでなく、タスクを自律的に実行するシステム。しかし、エージェントを本番環境に投入しようとすると、従来のシステム運用とは異なる課題に直面します。従来のAPIは、リクエストを送れば決まった形式でレスポンスが返ってくる。処理時間もおおよそ予測できる。しかしエージェントは違う。どんな行動を取るか予測しにくい。タスクによって実行時間が大きく変わる。外部サービスへの呼び出しも、エージェント自身が判断して行う。従来のSLOの考え方が、そのままでは通用しない。では、どう運用設計するのか。本書を読んで改めて考えさせられたのは、ガードレールの設計です。エージェントは自律的に動く。自律的に動くからこそ、想定外の行動を取る可能性がある。どこまで自律性を許し、どこで人間が介入するか。この境界線を曖昧にしたまま本番投入すると、インシデント時の対応が混乱します。自律的に動くものを、どこまで信頼するか。その答えを、運用設計の段階で明確にしておく必要がある。信頼の境界線を引くのは、AIではなく人間の仕事だ。本書はその設計指針を与えてくれます。Learning GitHub Copilotlearning.oreilly.comGitHub Copilotを使い始めたころ、私はこれを「賢いオートコンプリート」だと思っていました。しかし、最近のCopilotは違います。コード補完だけでなく、チャットで質問に答え、コードの説明を生成し、テストまで書いてくれる。開発ワークフロー全体を変革する可能性を持っている。その進化に追いつくために、本書を手に取りました。インフラエンジニアとしても興味深い内容が多かった。IaC（Infrastructure as Code、インフラのコード化）の自動化、マニフェスト（Kubernetesなどの設定ファイル）の生成、パイプラインの構築。私自身、本書のテクニックが役立った場面は少なくありません。ただ、便利になればなるほど、新しい課題も生まれます。AIが生成したコードを、誰がどうレビューするのか。生成されたコードにバグがあったとき、責任は誰にあるのか。「AIがそう書いたから」では済まされない。コードの責任は、承認した人間にある。便利さの代償は、新しい責任——その両面を理解した上でCopilotを活用していきます。楽になった分だけ、考える責任が増えた。Building Applications with AI Agents: Designing and Implementing Multiagent Systems (English Edition)作者:Albada, MichaelO'Reilly MediaAmazonTerraform in Depthlearning.oreilly.comインフラをコードで管理する。Infrastructure as Code（IaC）は、もはや当たり前の実践になりました。その中でもTerraformは、クラウドを問わず広く使われている。しかし、基本的な使い方を覚えた後、どう深めていくか。本書は、TerraformとOpenTofuの両方をカバーしている点に惹かれて手に取りました。HashiCorpのライセンス変更以降、OpenTofuへの移行を検討している組織も多いでしょう。どちらを選んでも、基本的な概念やスキルは共通しています。ライセンスが変わっても、スキルは変わらない——その安心感は大きいと感じました。大規模環境でのTerraform運用では、ステート管理が最も頭を悩ませる課題の1つです。ステートとは、Terraformが管理するインフラの「現在の状態」を記録したファイルです。このファイルが壊れたり、実際のインフラと食い違ったりすると、意図しない変更が発生する危険がある。ステートが壊れたら、インフラが壊れる——この現実に正面から向き合う必要があります。インフラの信頼性を高めるためには、IaCの品質向上が不可欠です。アプリケーションコードにはテストを書くのが当たり前になっていますが、インフラコードはどうでしょうか。インフラコードも、テストなしには信頼できない——私はこの原則を実践に落とし込むために、本書を読みました。Infrastructure as Code, 3rd Edition はこの辺の内容も普遍的に詳しく論じており同時におすすめしたいです。Terraform in Depth: Infrastructure as Code with Terraform and OpenTofu作者:Hafner, RobertManningAmazonArgo CD: Up and Runninglearning.oreilly.comIaCでインフラを定義できるようになったら、次はデプロイをどう自動化するか。GitOps（Gitリポジトリを中心にインフラやアプリケーションのデプロイを管理する手法）は、その答えの1つです。Gitリポジトリを唯一の真実の源とし、インフラの状態を宣言的に管理する。そのGitOpsの標準ツールとなったArgo CDを深く理解したくて手に取りました。公式ドキュメントには書かれていない設計判断の背景を知ることで、ツールの使い方だけでなく、思想を理解できると感じています。実践者が書いた本には、公式ドキュメントにはない「なぜ」がある——それが技術書を読む理由の1つです。大規模な環境でも管理可能なGitOpsワークフローを構築するためのテクニックを学べました。GitOpsの導入は、デプロイの信頼性を高めるだけではありません。変更管理の透明性が向上し、何か起きたときの原因追跡が容易になる。Gitを見れば、本番が分かる——宣言的なインフラ管理とGitによるバージョン管理の組み合わせは、チーム開発との相性が非常に良いと感じています。Argo CD: Up and Running: A Hands-On Guide to GitOps and Kubernetes (English Edition)作者:Block, Andrew,Hernandez, ChristianO'Reilly MediaAmazonEffective Platform Engineeringwww.manning.comIaCやGitOpsを導入し、インフラの自動化が進むと、次の課題が見えてきます。これらのツールやプラクティスを、どうやって開発チーム全体に展開するか。個人が使いこなしていても、チーム全体のものにならなければ意味がない。プラットフォームエンジニアリングは、その課題に対するアプローチです。しかし、技術的に優れたプラットフォームを作っても、開発者に使ってもらえなければ意味がない。使われないプラットフォームは、存在しないのと同じ——この現実は、プラットフォームチームにいると身に染みてわかります。本書が一貫して主張するのは、プラットフォームを「プロダクト」として扱うというマインドセットです。プラットフォームチームはインフラを提供するだけでなく、開発者体験を向上させる製品を開発している。開発者は顧客であり、彼らのフィードバックを受けて改善を続ける。インフラチームではなく、プロダクトチームである——この視点の転換は、チームの動き方を根本から変えます。この主張には強く共感する一方で、現実の難しさも感じている。「開発者は顧客」と言うのは簡単だ。しかし、顧客である開発者の要望をすべて聞いていたら、プラットフォームは一貫性を失う。標準化と柔軟性のバランス。セキュリティと利便性のトレードオフ。「顧客の声を聞く」と「顧客の言いなりになる」は違う。プロダクトチームとして振る舞うなら、時には「それはできません」と言う勇気も必要になる。本書はその難しさにも触れているが、私はもっと掘り下げてほしかった。開発者の認知負荷を下げながら、システムの信頼性を維持する。このバランスは、簡単ではありません。抽象化しすぎると、開発者がトラブルシューティングできなくなる。抽象化が足りないと、認知負荷が下がらない。プラットフォームの成功は、開発者の生産性で測る——この原則を軸に、どこまで抽象化するかを判断していく必要があります。Effective Platform Engineering (English Edition)作者:Chankramath, Ajay,Alvarez, Sean,Oliver, Bryan,Cheneweth, NicManningAmazonチームトポロジー　価値あるソフトウェアをすばやく届ける適応型組織設計作者:マシュー・スケルトン,マニュエル・パイス日本能率協会マネジメントセンターAmazonData Engineering Design Patternslearning.oreilly.comプラットフォームを運用していると、アプリケーションだけでなくデータパイプラインの信頼性も課題になってきます。データはシステムの血液のようなもので、流れが止まれば、ビジネスも止まる。データエンジニアリングにおけるデザインパターンを学びたくて手に取りました。デザインパターンとは、繰り返し現れる問題に対する定石のようなものです。先人たちが試行錯誤の末にたどり着いた解決策が、パターンとして整理されている。パターンには、先人の失敗が詰まっている——だから学ぶ価値がある。データパイプラインの信頼性、データ品質のモニタリング、レイテンシーの管理。これらの課題は、従来のアプリケーション開発とは異なるアプローチが必要です。たとえば、データパイプラインでエラーが発生したとき、どう対処するか。エラーを無視すればデータ品質が下がる。かといって、パイプライン全体を停止させれば、正常なデータまで届かなくなる。本書が紹介するパターンの1つは、問題のあるレコードを別の場所に退避させて後から対処する、というものです。1件のエラーで、100万件を止めるな——このパターンを知っているかどうかで、障害発生時の影響範囲が大きく変わります。また、データが届かないことも障害です。この視点も重要でした。アプリケーションの障害は目に見えやすいですが、データパイプラインの遅延や欠損は気づきにくい。データの品質をどう保証するか。本書から多くのヒントを得ました。Data Engineering Design Patterns: Recipes for Solving the Most Common Data Engineering Problems (English Edition)作者:Konieczny, BartoszO'Reilly MediaAmazonソフトウェア設計の結合バランスbook.impress.co.jpデータパイプラインでもアプリケーションでも、システムを構成する要素間の「結合」は避けて通れない課題です。疎結合が良い、密結合は悪い——そう教わってきたけれど、本当にそれだけで設計できるのか。この疑問に答えてくれるのが本書です。Vlad Khononov著『Balancing Coupling in Software Design: Universal Design Principles for Architecting Modular Software Systems』の翻訳本です。島田浩二さんの翻訳が秀逸で、原著の概念を自然な日本語で読めることに感謝しています。learning.oreilly.comしかし本書は、その固定観念を覆します。結合がなければ、ソフトウェアはシステムになれない。結合は悪ではない。結合は、システムを成り立たせる力だ。この主張を読んだとき、私は自分の設計判断を振り返った。「疎結合にしなければ」という呪縛に囚われて、過剰に分離したことはなかったか。分離した結果、かえって複雑になったことはなかったか。あった。確実にあった。マイクロサービスに分割したはいいが、サービス間の通信が増えて、障害の原因追跡が困難になった経験。本書の主張は、そうした失敗を言語化してくれた。本書の価値は、「結合」という概念を多次元で捉え直すところにあります。結合の強さだけでなく、結合の距離、結合の揮発性——複数の軸で分析することで、設計の判断基準が明確になる。これは手順書でもルールブックでもない。設計の意思決定に迷ったとき、インプットとして参照するための本です。どこまで結合を許容し、どこで切り離すか。その判断を支える思考の枠組みを、本書は与えてくれます。ある書評では「今後10年くらいの基礎知識になる」と評されていました。私も同感です。マイクロサービス、モジュラーモノリス、ドメイン駆動設計——どのアーキテクチャを選んでも、結合のバランスは避けて通れない。正解を教えてくれる本ではなく、正解を見つけるための視点をくれる本。そういう本こそ、長く手元に置いておきたい。ソフトウェア設計の結合バランス　持続可能な成長を支えるモジュール化の原則 (impress top gearシリーズ)作者:Vlad KhononovインプレスAmazonFacilitating Software Architecturelearning.oreilly.comsyu-m-5151.hatenablog.com結合のバランスを考え、設計判断を重ねていく。しかし、その判断は誰がするのか。アーキテクトの役割が変わりつつあります。一人の天才が全てを決める時代から、チーム全体でアーキテクチャを育てていく時代へ。アーキテクトは、決める人から、決められるようにする人へ——この変化は、私自身の仕事のやり方にも影響を与えています。本書が提唱するのは、決定の権限を分散しつつ、責任の所在を明確にするアプローチです。誰でもアーキテクチャに関する決定を下せる。しかし、その前に適切な人々から助言を求めなければならない。権限は分散されるが、責任は決定者に残る。このバランスが、スピードと品質のトレードオフを緩和してくれます。実務で特に役立っているのは、ADR（Architecture Decision Records、アーキテクチャ決定記録）の考え方です。なぜその設計判断をしたのかを記録しておく。これは、将来のインシデント対応や技術的負債の評価において価値がある。なぜこのシステムはこうなっているのか。その説明ができる状態を維持することは、チームの意思決定の質を高め、運用の効率化にも直結する。決定を記録しないのは、忘れるためである——だから記録が重要なのです。Facilitating Software Architecture: Empowering Teams to Make Architectural Decisions (English Edition)作者:Harmel-Law, AndrewO'Reilly MediaAmazonArchitecture Modernizationlearning.oreilly.comsyu-m-5151.hatenablog.com設計判断を記録し、チームでアーキテクチャを育てる。しかし、既存のレガシーシステムはどうするのか。新規システムなら理想的なアーキテクチャを追求できるが、現実には10年、20年と動き続けているシステムがある。レガシーシステムのモダナイゼーションに関わった経験がある人なら、技術だけでは解決しない問題があることを知っているはずです。コードを書き直しても、組織構造や開発プロセスが同じままでは、また同じ問題が生まれる。コードだけを変えても、問題は戻ってくる——本書は、この現実を正面から扱っています。全てのシステムが同じ重要度ではない。競争優位の源泉となる部分と、汎用的な部分を区別し、限られたリソースをどこに集中すべきかを判断する。全部は直せない。だから、どこを直すか決める——この優先順位付けの考え方は、経営層との対話でも役立ちます。「なぜこのシステムを優先するのか」を説明できるようになる。Collaborative Software Design もかなり良かったので副読本としてオススメしたいです。システムだけを変えても、組織が変わらなければ意味がない——この全体像を把握することは、ソフトウェアに関わるすべての人にとって重要です。なぜこのシステムがこの設計になっているのか。なぜこのチームがこの範囲を担当しているのか。技術的な判断の背景には、組織の歴史や力学がある。それを理解することで、日々の判断もより適切になるし、関係者との対話もスムーズになります。Architecture Modernization: Socio-technical alignment of software, strategy, and structure (English Edition)作者:Tune, Nick,Perrin, Jean-GeorgesManningAmazonBuilding Event-Driven Microservices, 2nd Editionlearning.oreilly.comマイクロサービスを設計するとき、私たちはつい「サービス間の通信をどうするか」という問いから始めてしまう。しかし本書を読んで、その問いの立て方自体が間違っていたのかもしれないと気づかされました。Adam Bellemare氏による本書の初版は2020年に出版され、イベント駆動型アーキテクチャの実践的な指針として多くのエンジニアに読まれてきました。この第2版では、その後の技術進化と実践知が大幅に加筆されています。本書が冒頭で引用するマクルーハンの「媒体はメッセージである」という言葉が象徴的です。私たちがどのような通信手段を選ぶかが、システムの設計だけでなく、組織構造やチーム間のコミュニケーションまで規定してしまう。リクエスト・レスポンス型の同期通信を選べば、サービス間の密結合が生まれる。イベントストリームを選べば、疎結合と自律性が生まれる。技術選択は、組織の形を決める選択でもある——コンウェイの法則を逆手に取るような視点が、本書には一貫して流れています。著者が強調するのは、データ通信構造（Data Communication Structure）という概念です。ビジネスコミュニケーション構造（チームの編成）と実装コミュニケーション構造（コードとAPI）は多くの組織で意識されている。しかし、データをどう流通させるかという構造は、往々にして後回しにされる。その結果、他チームのデータが必要になるたびに、場当たり的なAPI連携やデータコピーが生まれ、システムは複雑化していく。データ通信構造の欠如が、モノリスを肥大化させる——この指摘は、私自身の経験とも重なります。イベント駆動型マイクロサービスの本質は、データを「イベント」として永続化し、それを組織全体で共有可能にすることにあります。プロデューサーはイベントを発行する責任だけを負い、コンシューマーは必要なイベントを自分のペースで消費して独自のデータモデルを構築する。この分離によって、サービス間の依存関係が劇的に減少する。データは、実装に閉じ込めるものではなく、流れるものである——この発想の転換が、本書の核心です。ただし、私はこの主張を手放しで受け入れているわけではない。イベント駆動型アーキテクチャには、リクエスト・レスポンス型にはない複雑さがある。イベントの順序保証、べき等性の担保、結果整合性への対応。「疎結合になる」という美しい言葉の裏には、新たな運用課題が潜んでいる。本書はその課題にも誠実に向き合っているが、現場で直面する泥臭い問題——たとえば、イベントスキーマの進化をどう管理するか、障害時のリカバリをどう設計するか——については、もっと深掘りしてほしかった部分もある。本書の価値は、イベント駆動型アーキテクチャの「なぜ」を丁寧に解説している点にあります。単にKafkaの使い方を説明するのではなく、なぜイベントストリームが必要なのか、なぜ従来のアプローチでは限界があるのかを、組織論まで含めて論じている。リクエスト・レスポンス型マイクロサービスの欠点——ポイントツーポイント結合、依存スケーリング、分散モノリス化——を明確に言語化してくれたことで、私自身が過去に経験した失敗の原因が腑に落ちました。イベントは、サービス間の会話ではなく、組織の記憶である——本書を読んで、私はイベントストリームの捉え方が変わりました。データパイプラインやメッセージキューとしてではなく、ビジネスの出来事を永続化した「正典的な記録」として捉える。その視点があれば、新しいサービスを立ち上げるときも、過去のイベントを再生してデータモデルを構築できる。実装の寿命よりもデータの寿命のほうが長い——この現実を直視したアーキテクチャが、イベント駆動型マイクロサービスなのだと理解しました。Building Event-Driven Microservices: Leveraging Organizational Data at Scale (English Edition)作者:Bellemare, AdamO'Reilly MediaAmazonTaming Your Dragon: Addressing Your Technical Debtlearning.oreilly.comsyu-m-5151.hatenablog.comシステム開発で必ず直面するのが、技術的負債です。どこを優先的に直すかを判断するには、技術的負債の性質を理解する必要がある。技術的負債は「ドラゴン」のようなものです。放っておけば大きくなり、いつか手に負えなくなる。しかし、完全に倒すこともできない。なぜなら、技術的負債は開発を進める限り必ず生まれるものだからです。だから、敵として戦うのではなく、適切に付き合い、共存の道を探る。ドラゴンは殺せない。だから、飼い慣らす——この比喩が、私には刺さりました。本書を読んで、技術的負債を単なる技術的問題ではなく、トレードオフの問題、組織の問題、経済の問題として捉える視点を得ました。「技術的負債」という言葉は、金融の「負債」から借りてきた比喩です。しかし、両者には決定的な違いがあります。金融的負債は明確な金額があり、返済計画を立てられる。しかし技術的負債は、その量を正確に測定することが困難であり、返済のコストも不確実です。借金は金額がわかる。技術的負債は、わからない——このアナロジーの限界を、私たちはもっと意識すべきだと感じています。ここで著者の主張に、私は半分同意し、半分疑問を持つ。「ドラゴンを飼い慣らす」という比喩は美しい。しかし、飼い慣らせるドラゴンと、飼い慣らせないドラゴンがいるのではないか。ある種の技術的負債は、時間が経つほど返済コストが指数関数的に増大する。そういう負債は、早めに倒すべきだ。すべての負債を「共存する相手」として扱うのは、危険な楽観主義に陥る可能性がある。本書の比喩を鵜呑みにせず、「このドラゴンは飼い慣らせるのか、それとも早めに倒すべきなのか」を見極める目が必要だと、私は考える。技術的負債がなぜ蓄積していくのか、なぜ返済が後回しにされるのか。本書はその構造的な原因を可視化してくれます。原因がわかれば、より効果的な介入点を見つけることができる。技術的負債は倒すものではなく、飼い慣らすもの——「なぜこの改善が必要なのか」を経営層に説明するための理論的基盤を、本書から得ました。Taming Your Dragon: Addressing Your Technical Debt (English Edition)作者:Brown, Dr. Andrew RichardApressAmazonRefactoring to Rustlearning.oreilly.comsyu-m-5151.hatenablog.com技術的負債に対処する具体的な手法の1つとして、言語の移行があります。既存のコードベースを一から書き直すのではなく、段階的にRustに置き換えていくアプローチに興味があって手に取りました。全面的な書き直しはリスクが高い。だから、パフォーマンスクリティカルな部分から少しずつ置き換える。全部を書き直すな、一部を置き換えろ——この原則は、私の考え方にも合っています。「Rustを学ぶ」本ではなく、「Rustを実務で使う」本だと感じました。言語を学ぶのと、言語で仕事をするのは違う——その差を埋めてくれる本です。パフォーマンスクリティカルな部分や、メモリ安全性が重要な部分をRustに置き換えることで、システム全体の信頼性を向上させる。全面的な書き換えのリスクを避けながら、段階的に改善を進める方法論は、運用中のシステムを改善する際の参考になるでしょう。Refactoring to Rust (English Edition)作者:Mara, Lily,Holmes, JoelManningAmazonJust Use Postgres!learning.oreilly.comsyu-m-5151.hatenablog.com言語の選択、アーキテクチャの設計、技術的負債の返済——これまで見てきた本は、どれも「何を選ぶか」の判断を扱っていました。しかし、時には「選ばない」という選択が最良のこともある。「PostgreSQLだけで十分」という主張は、時に過激に聞こえるだろう。しかし本書を読んで、その主張にはしっかりとした根拠があることがわかりました。新しい技術スタックを追加することは、運用の複雑性を高める。だから、既存の技術でできることは、既存の技術で解決すべきです。新しいデータベースを導入する前に、Postgresでできないか考える。この姿勢が、私の技術選択の基準になっています。PostgreSQLは、リレーショナルデータベースとしての堅実な機能に加え、JSON処理、全文検索、地理空間データ、時系列データ、ベクトル検索まで対応しています。Postgresは、データベースではなく、プラットフォームである——この主張には説得力があります。ただし、この主張を額面通りに受け取るのは危険だとも思う。「Postgresで十分」という言葉が、技術的判断の放棄に使われることがある。本当にPostgresで十分なのか、それとも単に新しい技術を学ぶのが面倒なのか。その区別は、案外難しい。本書の価値は「Postgresを使え」という結論にあるのではなく、「なぜPostgresで十分なのか」を考えるフレームワークにある。シンプルさには価値がある。しかし、シンプルさを言い訳にして、必要な複雑さから逃げてはいけない。データベースの種類を減らすことで、運用の複雑性が下がるというメリットがあります。監視対象が減り、バックアップ戦略が統一され、チームが習得すべき技術スタックがシンプルになる。もちろん、PostgreSQLが適さないケースもあります。万能ではないことを認めた上で、どこまで対応できるかを知る。複雑さを減らすことも、エンジニアリングである——その境界線を理解することが、適切な技術選択には重要です。Just Use Postgres!: All the database you need (English Edition)作者:Magda, DenisManningAmazonThe Software Engineer's Guidebooklearning.oreilly.comここまで、技術的なトピックの本を紹介してきました。しかし、技術を身につけるだけでは、キャリアは作れない。ジュニアからシニア、そしてスタッフエンジニアへ。キャリアの各段階で求められるスキルは異なります。しかし、次の段階で何が必要になるかは、今の段階からは見えにくい。キャリアの次の段階で必要なスキルは、今の段階では見えない——本書は、その見通しを与えてくれます。技術的なスキルだけではキャリアは作れない。これは、ある程度経験を積むと実感することです。コードレビューの仕方、技術的な意思決定への関わり方、メンタリングの方法、組織への影響力の広げ方。コードを書く力と、キャリアを作る力は別物——両方を意識的に伸ばす必要があります。技術力は武器になる。しかし、武器だけでは戦場を選べない。ここで私は、本書の主張に対してある種の居心地の悪さを感じる。キャリアを「設計」するという発想自体に、違和感がある。私のキャリアは、計画通りに進んだことがない。偶然の出会い、予期せぬ異動、想定外のプロジェクト。そうした「偶然」の積み重ねが、今の自分を作っている。本書が示すロードマップは参考になる。しかし、ロードマップ通りに進むことが正解だとは思わない。計画を持つことと、計画に縛られることは違う。本書を読みながら、私は自分のキャリアを「設計」するのではなく、「振り返る」ことの方が多かった。ソフトウェアエンジニアガイドブック ―世界基準エンジニアの成功戦略ロードマップ作者:Gergely Orosz,久富木 隆一（翻訳）オーム社Amazonバックエンドエンジニアのためのインフラ・クラウド大全www.shoeisha.co.jpキャリアを考えるとき、自分に影響を与えてくれた人の存在は大きい。尊敬するnetmarkjpさんの著書です。私がエンジニアとして仕事をする中で、netmarkjpさんから学んだことは数え切れません。その方が書いた本となれば、読まないわけにはいかなかった。本書は「基礎知識」と銘打たれた23章から構成されています。可用性、キャパシティ、パフォーマンス、監視、セキュリティ、DevOps、SRE——インフラに関わるエンジニアが押さえるべき領域を網羅的にカバーしている。しかし、この本の価値は網羅性だけではありません。各章に、実務経験に裏打ちされた「なぜそうするのか」が詰まっている。基礎とは、簡単という意味ではない。基礎とは、すべての基盤になるという意味だ。バックエンドエンジニアがインフラを理解することの意味は、年々大きくなっていると感じます。クラウドネイティブな環境では、アプリケーションとインフラの境界が曖昧になっている。コンテナ、Kubernetes、オブザーバビリティ——これらを理解せずに、本番環境で動くシステムは作れない。アプリだけ書けても、本番では動かせない。本書は、その橋渡しをしてくれる一冊です。 speakerdeck.comバックエンドエンジニアのためのインフラ・クラウド大全【リフロー型】作者:馬場 俊彰,株式会社X-Tech5翔泳社Amazon作る、試す、正す。アジャイルなモノづくりのための全体戦略作る、試す、正す。　アジャイルなモノづくりのための全体戦略bnn.co.jp技術の基礎を固め、システムを作る。しかし、作ったものが「正しいもの」かどうかは、また別の問題です。市谷聡啓さんの到達点とも言える一冊です。『カイゼン・ジャーニー』『正しいものを正しくつくる』を経て、20年以上の実践知が凝縮されています。note.com本書のタイトル「作る、試す、正す」は、ものづくりの本質を端的に表しています。作って終わりではない。試して、学んで、正す。その繰り返しの中で、少しずつ「正しさ」に近づいていく。完成形を目指すのではなく、動き続けることがゴールだという考え方です。私がこの本で最も考えさせられたのは、「正しさ」の捉え方でした。最初から正しいものを作ろうとすると、動けなくなる。かといって、何も考えずに作り始めると、迷子になる。本書が提示するのは、その中間にある姿勢です。「正しさ」は最初から存在するものではなく、作り、試し、正す過程で立ち現れてくるもの。だから、完璧な計画を立てることより、素早く試して学ぶ仕組みを整えることのほうが大事だと言う。この考え方は、ソフトウェア開発に限った話ではないと思います。仕事全般、もっと言えば生き方にも通じる。最初から「正解」を知っている人はいない。やってみて、失敗して、修正して——その繰り返しの中で、少しずつ「あるべき姿」が見えてくる。正しさを探すのではなく、正しくなる状況をつくる。本書のこの言葉は、私の仕事だけでなく、物事への向き合い方そのものを言語化してくれました。作る、試す、正す。　アジャイルなモノづくりのための全体戦略作者:市谷 聡啓ビー・エヌ・エヌAmazon良いコードの道しるべbook.mynavi.jp素早く適応しながら開発を進める。しかし、その過程で生まれるコードの品質はどう担保するか。この本を読んで、私は「説明の仕方」を学びました。動くコードを書くことは、実はさほど難しくない。大事なのは、書いたコードを他の人や将来の自分が読んで正しく理解できること——本書を通して伝えられる。本書の内容自体は、経験を積んだエンジニアにとって目新しいものではありません。命名、コメント、関数やクラスの分割、依存関係の整理、自動化テスト。どれも「基本」と呼ばれるものばかりです。しかし、この本の価値は内容の新しさではなく、説明の丁寧さにあります。なぜその原則が有用なのか、どうしてそう書くべきなのか——「なぜ」を省略せずに解説している。私がこの本を評価するのは、「人に説明するときの参考になる」からです。チームに若手が入ってきたとき、コードレビューで指摘するとき、「なぜこう書くべきか」を説明する必要がある。そのとき、自分の頭の中にある暗黙知を言語化するのは意外と難しい。本書は、その言語化の手本を見せてくれます。基本を、基本のまま、分かりやすく伝える。それは簡単なことではない。良いコードの道しるべ　変化に強いソフトウェアを作る原則と実践作者:森 篤史マイナビ出版AmazonClean Code, 2nd Editionlearning.oreilly.com良いコードの基本を学んだら、次はその原則を深く考えたい。Robert C. Martin（Uncle Bob）による『Clean Code』の第2版です。2008年に出版された初版から16年、全面的に書き直されました。初版を読んだのは何年も前のことです。その後、私のコードは変わったのか。正直に言えば、変わった部分もあれば、変わらなかった部分もある。だからこそ、第2版を手に取りました。自分がどこまで成長したのか、どこで止まっているのか、確認したかった。第2版で印象的だったのは、AI時代に対する著者の姿勢です。「コードはいずれなくなる」「AIがすべて書いてくれる」——そんな予測に対して、Uncle Bobは明確に反論しています。コードは要求の詳細を表現したものであり、その詳細は抽象化できない。AIがどれだけ賢くなっても、仕様を厳密に記述する行為——つまりプログラミング——はなくならない。コードは消えない。なぜなら、コードとは要求そのものだから。この主張に私は強く共感する。そして驚いたのは、第2版がここまで大幅にアップデートされていたことだ。16年という歳月は、ソフトウェア開発の世界では永遠に等しい。にもかかわらず、Uncle Bobは単なる改訂ではなく、現代の開発環境——AI、クラウド、分散システム——を踏まえた上で原則を再構築している。初版の「良いコードとは何か」という問いは変わらないが、その答え方が2025年の文脈に合わせて書き直されている。古典を現代に蘇らせるとは、こういうことなのだと思った。本書の核心は、タイトルの通り「クリーン」であることです。しかし、「クリーン」とは完璧を意味しない。住めない「ショーハウス」ではなく、住める「クリーンな家」を目指す。クリーンなコードとは、維持し、拡張し、進化させても、その住みやすさを損なわないコードのこと。完璧ではないが、手入れされている。クリーンとは、完璧ではなく、ケアされている状態だ。もう1つ、心に残った言葉があります。「私たちは書くよりも読む時間の方が圧倒的に長い」——だからこそ、読みやすいコードを書くことが、結果として書きやすさにつながる。速く行きたければ、うまくやれ（The only way to go fast is to go well）。この原則は、初版から変わらない。そして、16年経っても色褪せない。型システムのしくみ型システムのしくみ ― TypeScriptで実装しながら学ぶ型とプログラミング言語www.lambdanote.comクリーンなコードを書くための原則を学んだ。では、その原則を支える道具——型システム——はどう動いているのか。遠藤侑介さんの著書です。Rubyコミッタであり、TypeProfの開発者であり、『型システム入門』の訳者でもある。その方が「型システムを実装しながら学ぶ」本を書いた。読まないわけにはいかなかった。現代の開発環境では、コードを書いている最中にエラーが判明し、文脈に適した補完候補が提示される。当たり前のように使っているこの機能、その裏側で何が起きているのか。本書は、TypeScriptのサブ言語に対する型検査器を実装しながら、その「しくみ」を解き明かしていきます。型システムの理論を学ぶ方法は、数学的な教科書を読むことだけではない。実装を通じて理解する道がある——本書はその道を示してくれます。真偽値と数値の型から始まり、関数型、オブジェクト型、再帰型、ジェネリクスへと段階的に進んでいく構成が秀逸です。各章で型検査器を拡張しながら、「なぜこの機能が必要なのか」「どう実装するのか」を体験的に学べる。私がこの本を読んで得たのは、型システムへの「畏れ」と「親しみ」の両方でした。型システムは魔法ではない。人間が設計し、実装したものだ。しかし、その設計には深い思慮がある。エディタが「このコードは間違っている」と教えてくれるとき、その背後には型検査器の地道な仕事がある。その仕事の中身を知ることで、型に対する見方が変わりました。型は、プログラムを制約するものではなく、プログラムを守るものだ。Fundamentals of Software Engineeringlearning.oreilly.com型システム、クリーンコード、アーキテクチャ——ここまで個別の技術トピックを深掘りしてきました。しかし、それらを俯瞰的に捉える視点も必要です。ソフトウェアエンジニアリングの基礎を幅広くカバーしている一冊です。流行のフレームワークは数年で入れ替わる。しかし、基礎的な原則は変わらない。フレームワークは変わる。基礎は変わらない——長くこの業界にいると、この事実を繰り返し実感します。AIがコードを生成してくれる時代になって、基礎の重要性はむしろ高まっていると感じます。AIの出力をそのまま受け入れるのではなく、評価し、改善し、統合する。その判断ができるのは、基礎を理解している人間だけです。AIの出力を評価できるのは、基礎を知っている人だけ——特定の技術やフレームワークに依存しない普遍的な原則を、改めて確認するために本書を読みました。Fundamentals of Software Engineering: From Coder to Engineer (English Edition)作者:Schutta, Nathaniel,Vega, DanO'Reilly MediaAmazonThe Product-Minded Engineerlearning.oreilly.com基礎を学び、技術を深め、システムを作る。しかし、技術的に正しいものを作ることと、ユーザーに価値を届けることは、必ずしも同じではありません。エンジニアとして長く仕事をしていると、技術的に正しいことと、ビジネスとして正しいことが一致しない場面に何度も遭遇します。コードが動くだけでは十分ではない。そのコードが、ユーザーにどんな価値を届けているのか。コードを書くことと、価値を届けることは違う——この違いを理解することは、プロダクトに関わるエンジニアにとって必須のスキルです。エンジニアとして仕事をしていると、「ユーザーにとっての価値」と「技術的な正しさ」の間にギャップがあることに気づきます。たとえば、99.9%の可用性は技術者にとっては誇らしい成果でしょう。しかし、99.9%を裏返すと0.1%のダウンタイム。年間に換算すると約8時間の停止を意味する。ユーザーにとって、その8時間がどれだけ痛いか。99.9%は、ユーザーにとっては年間8時間の停止を意味する——技術的な数値をビジネスインパクトに翻訳できること。それがプロダクト思考の1つの形であり、本書はその視点を養う上で役立ちました。The Product-Minded Engineer: Building Impactful Software for Your Users (English Edition)作者:Hoskins, DrewO'Reilly MediaAmazonThe Engineering Leaderlearning.oreilly.comプロダクト思考を身につけ、技術とビジネスの両方を見られるようになる。すると、次に見えてくるのはリーダーシップの課題です。リーダーシップについて書かれた本は多いですが、本書は地に足のついた実践的なアドバイスが詰まっています。誰かがキャリアを設計してくれるわけではない。自分で考え、自分で動く必要がある。自分のキャリアの責任者は、自分である——ある程度経験を積むと、この現実を受け入れざるを得なくなります。本書は、その受け入れた後に何をすべきかを具体的に示してくれます。自分自身を導くこと、他者を導くこと、チームを導くこと、そしてチームを超えて導くこと。まず自分を導けないなら、他者は導けない——この順序は重要です。自己管理ができていない人間が、チームをまとめられるはずがない。「マネージャーになる」ことだけがリーダーシップではない。ポジションに関係なく、チームに良い影響を与えることはできる。リーダーシップは、ポジションではなく行動である——この考え方は、IC（Individual Contributor）としてのキャリアを続ける上でも指針になっています。The Engineering Leader: Strategies for Scaling Teams and Yourself (English Edition)作者:Huston, CateO'Reilly MediaAmazonエンジニアリングリーダー ―技術組織を育てるリーダーシップとセルフマネジメント作者:Cate Huston,岩瀬 義昌（翻訳）,岩瀬 迪子（翻訳）オーム社Amazon\"Looks Good to Me\"learning.oreilly.comリーダーシップを発揮する場面は、会議室だけではありません。日々の開発で最も頻繁に行われるコミュニケーションの1つが、コードレビューです。コードレビューは、品質保証の手段であると同時に、チームの学習機会でもある。バグを見つけるだけがレビューの役割ではない。知識を共有し、コードの意図を確認し、チーム全体の理解を揃える。レビューは、コードのためではなく、チームのためにある——この視点で見ると、レビューの仕方が変わってきます。コードレビューを「チームスポーツ」として捉える考え方に共感しました。個人の技術力を競う場ではなく、チーム全体の品質とスキルを向上させるための協働の場として位置づける。レビューコメントは、批判ではなく、贈り物である——この姿勢を持てるかどうかで、チームの雰囲気は大きく変わります。しかし、私はこの「贈り物」という表現に、少しだけ引っかかる。贈り物は、受け取る側が喜ぶものだ。しかしコードレビューのコメントは、時に厳しいことも言わなければならない。「ここは根本的に設計を見直すべきだ」と指摘することは、贈り物というより、苦い薬に近い。「贈り物」という美しい比喩に逃げて、言うべきことを言わなくなるのは本末転倒だ。本書の主張は正しいが、その比喩を鵜呑みにすると、レビューが馴れ合いになる危険がある。厳しさと敬意は両立できる。そのバランスこそが、本当の意味での「贈り物」なのだと思う。最後に「LGTM」と承認するのは人間です。その承認は、コードへの同意であると同時に、チームメンバーへの信頼の表明でもある。LGTMは、チームの信頼の証である——この認識を共有できているチームは、レビューが建設的になるし、心理的安全性も高まります。\"Looks Good to Me\": Constructive code reviews (English Edition)作者:Braganza, AdrienneManningAmazonLooks Good To Me作者:Adrienne Braganza秀和システムAmazonおわりに26冊。感想文を書き終えて、その数字を見つめている。多いのか少ないのか、正直わからない。まぁ多いか。「今年もたくさん読みましたね」と言われれば悪い気はしないし、「それだけ？」と言われればちょっとへこむ。結局、他人の評価を気にしている。読書量なんて自己満足だと言いながら、どこかで認めてほしがっている。振り返ると、今年の本には共通点があった。『Beyond Vibe Coding』は、AIに頼りすぎている自分を突きつけてきた。『LLMOps』は、正解が定義できないシステムの難しさを教えてくれた。『ソフトウェア設計の結合バランス』は、疎結合という呪縛から解放してくれた。『Taming Your Dragon』は、技術的負債と共存する道を示してくれた。どの本も、私に「それでいいのか」と問いかけてきた。『Terraform in Depth』を読んだ夜のことを思い出す。ステート管理のベストプラクティスなんて、AIに聞けば30秒で返ってくる。でも私は、著者が過去にやらかした失敗談のほうを覚えている。「これで痛い目を見た」という告白。公式ドキュメントには絶対に載らない、その生々しさ。なぜか、そっちのほうが頭に残る。正解より失敗のほうが記憶に焼きつくのは、私という人間の性質なのかもしれない。『Beyond Vibe Coding』を読んだとき、嫌な気持ちになった。自分のことを書かれている気がしたからだ。AIに聞いて、答えをもらって、なんとなくわかった気になる。その繰り返し。「なぜ」を考えなくなっていた。本を読むという行為は、その怠惰な自分への処方箋だったのかもしれない。ページをめくる時間だけ、「なぜ」を考え続けることができる。本は答えをくれない。くれるのは「そうだろうか」という違和感だ。著者の主張に首をかしげる。その違和感を言語化しようとする。そうやって、自分の考えが少しずつ形になっていく。AIは答えを返してくれる。でも「そうだろうか」とは返してくれない。たぶん、そこが決定的に違う。今年は、AI/LLMの運用が本格化した年だった。プラットフォームエンジニアリングが変わり、組織の話が増えた。技術だけ見ていればよかった時代は、とっくに終わっている。その変化に追いつこうとして、本を読んだ。読んで、ブログを書いて、登壇した。アウトプットしないと身につかない。言い聞かせるように、繰り返してきた。でも、本当のことを言えば、追いつこうとしていたわけではないのかもしれない。変化の中で、自分が何者であるかを確かめたかった。AIがコードを書いてくれる時代に、なぜ私はエンジニアをやっているのか。答えは出ていない。出ていないけれど、本を読むたびに、その輪郭が少しだけ見えてくる気がする。2025年はまだ3週間ほど残っている。年末年始に読んだ本は、来年の記事で。毎年同じことを書いている気がする。でも来年も、たぶんまた書くのだろう。誰に頼まれたわけでもないのに、12月になると、この作業を始めてしまう。本を読むことに意味があるのか。正直、わからない。わからないけれど、やめられない。AIがどれだけ賢くなっても、300ページを読み通した時間は消えない。その時間が、自分を少しだけ変えてくれたような気がする。気がするだけかもしれない。でも、その「気がする」を信じて、来年も本を開くのだと思う。正解を得ることだけが目的なら、エンジニアをやっている意味がない。はじめにで書いたこの言葉が、25冊の感想文を書き終えた今、少しだけ違って聞こえる。正解がないから難しい。正解がないから面白い。正解がないから、エンジニアを続ける価値がある。本を読む意味がある。来年もきっと、答えの出ない本を読み続けるのだろう。そして、また12月になったら、この記事を書く。それでいい。それがいい。","isoDate":"2025-12-11T01:41:43.000Z","dateMiliSeconds":1765417303000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2025年版 私がAIエージェントと協働しながら集中する方法","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/10/092706","contentSnippet":"集中できなくなった何かがおかしい。AIエージェントを使い始めてから、自分が壊れていくのを感じていた。以前は4〜5時間ぶっ通しで集中できた。コードを書き始めたら、気づいたら夕方になっていた。あの没入感。あの充実感。それが、完全に消えた。30分も持たない。いや、10分だろうか。1つの作業に没頭しようとしても、すぐに別の作業に引き戻される。戻ってきたら、さっき何をしていたか忘れている。頭の中が常にざわついている。自分の脳が、自分のものではなくなっていく感覚があった。最初は自分を責めた。集中力が落ちたのは、体力のせいか。年齢のせいか。怠けているのか。スマホの見すぎか。でも違った。同じように苦しんでいる人が、周りにもいた。きっと、最初からうまく馴染める人もいるのだろう。複数のエージェントを同時に回しながら、涼しい顔で成果を出せる人。元々、全体を俯瞰しながら動くのが得意な司令官タイプ。私は違った。複数のエージェントが並行して動いている。1つのエージェントに指示を出して、出力を待っている間に別のエージェントの出力を確認する。確認が終わったら修正指示を出して、また別の作業に移る。案件も複数が同時に走っている。厄介だったのは、見せかけ上の効率は上がっていたことだ。タスクは消化されている。アウトプットも出ている。だから最初は原因に気づけなかった。でも、何かがおかしい。同じ時間、同じ環境で働いているのに、以前のように深く没入できない。達成感がない。自分は変わっていないはずなのに、なぜ？数字に現れない損失があった。タスクの消化数は増えた。しかし、1つ1つの仕事に対する理解の深さが落ちていた。コードをAIと共に書いているのに、なぜそう書いたのか説明できない。レビューを通しているのに、本当に良いコードなのか判断できていない。量は出ている。でも、自分の中に何も残らない。学習効率が落ちていた。成長している実感がなかった。品質の問題もあった。アウトプットは出ている。しかし、それは本当に良いアウトプットなのか。深く考える時間がないまま、次々とタスクを流していく。表面的には回っている。後から振り返ると「なぜこんな設計にしたんだ」と感じることが増えていた。速く走っているつもりが、同じ場所をぐるぐる回っていただけだった。そして何より、このペースや仕事のやり方が続くのかという不安があった。毎日、頭の中が騒がしい。仕事が終わっても、脳が休まらない。週末になっても回復しきれない。短期的には回っている。でも、1年後、3年後も同じように働けるのか。効率が上がったように見えて、実は遠回りしている道を走っていたり自分を前借りしているだけではないのか。ポモドーロ・テクニックを再稼働させた。25分の作業と5分の休憩を繰り返す方法だ。効果はあった。でもAIエージェントとの協働が始まってからは、25分の中での集中すら維持できなくなっていたというた25分のタスクというのを見積もれなくなった。通知を切った。効果なし。まとまった時間を確保した。効果なし。瞑想アプリを入れた。効果なし。何をやっても、うまくいかなかった。追い詰められていた。このままでは仕事にならない。でも、AIエージェントなしで働くという選択肢はもうなかった。環境を変えるのではなく、自分を変えるしかなかった。観察するという発見行き詰まっていたとき、『大人のADHDのためのマインドフルネス』という本に出会った。ADHDの当事者向けに書かれた本だが、読んでいて「これは自分のことだ」と思う記述が多かった。注意が散漫になる。複数のことが同時に気になる。1つのことに没頭できない。ADHDかどうかは関係なかった。今の自分が抱えている問題そのものだった。本の中で紹介されていた手法の1つが、「観察する」ということだった。作業中、ふと自分自身を観察してみる。今どんな気分か。注意はどこに向いているか。身体の感覚はどうか。判断せず、ただ気づく。最初は半信半疑だった。自分を観察しながら作業するなんて、むしろ集中の妨げになるのではないか。リソースを分散させているだけではないか。でも、他に試す手段がなかった。藁にもすがる思いだった。やってみると、不思議なことが起きた。集中が途切れにくくなったのだ。いや、正確には違う。集中は途切れる。でも、途切れた瞬間に気づけるようになった。観察している自分がいるから、「あ、今逸れた」とすぐにわかる。わかるから、すぐに戻れる。これまで私は、集中を「途切れないように維持するもの」だと思っていた。途切れたら負け。だから途切れないように必死に守ろうとしていた。でも違った。集中は「維持するもの」ではなく「戻るもの」だったのだ。途切れること自体は問題ではない。戻れるかどうかが問題だった。この発見は、私の集中に対する考え方を根本から変えた。完璧な集中を目指すのではなく、素早い復帰を目指す。壁を作って守るのではなく、柔軟に戻る力を育てる。防御から回復へ。発想の転換だった。今では複数の案件を並行して回しながら、開発タスクを5本同時に進め、ブログも2〜3本並列で書けるようになった。ポモドーロの25分間、集中が途切れることはほとんどない。途切れても、数秒で戻れる。この実践を、私は「微観法」と呼んでいる。自分の微細な変化を観察する方法、という意味だ。正式な名称があればぜひ教えてほしい。この節の内容は『大人のADHDのためのマインドフルネス』（リディア・ザイローウスカ著）を参考にして自分なりに実践していたものです。また、表現について @tsumikino_ さんの投稿に影響を受けていたため、修正いたしました。参照元を明記せずご不快な思いをさせてしまい、申し訳ありませんでした。ご指摘いただきありがとうございました。以前の集中と何が違うのか以前の私にとって、最良の集中状態とは湖の底に沈んでいくような感覚だった。体の感覚はどこか希薄になる。なぜ自分がキーボードを打っているのかわからなくなる。意識と作業の境界が溶けて、ただコードが生まれ、ただ文章が流れていく。水面の光が遠ざかり、静かな深みに降りていく。その状態に入れたとき、驚くほどの量と質の仕事ができた。あの深さを、私は愛していた。この「深く沈む」集中は、1つの大きなタスクに長時間取り組むときには最適だった。中断がなく、自分のペースで進められるソフトウェア開発や執筆の環境では、これ以上の方法はなかった。しかしAIエージェントと協働する環境では、この方法が通用しなくなった。深く沈もうとしても、エージェントの出力確認で水面に引き戻される。複数の案件を抱えていれば、1つに没入できない。深く沈むには、水面が静かでなければならない。でも今の水面は常に波立っている。そこで発想を変えることにした。深く沈むのではなく、水面近くに留まる。没入するのではなく、観察する。集中の「深さ」ではなく、「復帰の速さ」を重視する。ここで1つ、重要なことに気づいた。集中は、環境次第で形を変える。静かな水面なら深く沈む集中が最適だし、波立つ水面なら水面近くを泳ぐ集中が最適だ。どちらが優れているわけではない。環境に合った集中の持ち方がある。つまり、集中とは「自分の能力」ではなく「環境との関係」なのだ。同じ人間でも、環境が変われば最適な集中の形は変わる。集中できないのは能力の問題ではない。環境と方法のミスマッチだ。私は長い間、自分の集中力が落ちたと思っていた。でも違った。環境が変わったのに、方法を変えていなかっただけだった。なぜ観察すると集中できるのかここで疑問が生じる。作業に100%集中したほうが効率的なはずではないか。なぜ10〜20%を「自分の観察」に割くと、かえって集中できるのか。理由はおそらく「注意の逸脱」の仕組みにある。人間の注意は、放っておくと必ず逸れる。これは避けられない。問題は、逸れること自体ではなく、逸れたことに気づくまでの時間だ。普通は、気が逸れてから5分、10分経って「あ、逸れてた」と気づく。スマホを開いて、気づいたら15分経っていた。そういう経験は誰にでもある。この5分、10分、15分が積み重なって、1日の生産性を静かに破壊していく。微観法では、意識の一部を「自分を観察する視点」として常に確保しておく。すると、注意が逸れ始める瞬間を捉えられるようになる。「スマホを見ようかな」と思った瞬間。「積んである本を読みたいな」と思った瞬間。「コーヒーを淹れに行こうかな」と思った瞬間。「このタスク面倒だな」と感じた瞬間。逸れてから3秒で気づき、すぐ戻れる。5分後に気づくのと、3秒後に気づくのでは、累積の損失がまったく違う。100%集中しようとして5分ごとに逸れるより、90%の集中を安定して維持するほうが、結果的に多くの仕事ができる。もう1つ理由があると思っている。「退屈の無効化」だ。脳は刺激が足りないと退屈を感じ、新しい刺激を求める。SNSを見たくなるのはこのためだ。作業が単調になると、脳が「もっと刺激をくれ」と要求してくる。しかし自分の内面を観察対象にすると、そこには常に微細な変化がある。呼吸の深さ、肩の緊張、思考の流れ、感情の揺らぎ。これは揺らぐ炎のように、予測不能だが安定していて、見続けることができる。外部刺激に頼らなくても、脳が求める新規性は内側から供給できる。具体的なやり方方法は単純だ。ある日、疲れ果てて帰ってきた夜のことだった。だるい。本当にだるい。でも仕事が残っている。そのだるさを抱えたまま、仕方なくキーボードに向かった。そのとき、ふと気づいた。「だるいな」と感じている自分を、どこかで観察している。だるさはある。でも、だるさを見ている自分もいる。その「見ている自分」は、意外と冷静だった。不思議なことに、観察を続けていると作業が進んだ。だるさは消えない。でも、だるさに飲み込まれない。その感覚を忘れたくなくて、言語化しておくことにした。それが微観法の始まりだった。ポイントは、観察の「解像度」を下げることだ。「今、自分は何を考えているか」「なぜそう感じているか」と分析しようとすると、認知資源を食う。作業と同時にはできない。分析せず、ただ「ある」と気づくだけでいい。「退屈だな」と感じたら、なぜ退屈かは考えない。「退屈がある」とだけ認識する。それだけで十分だ。例えば、作業を始める前に5秒だけ自分の状態を確認する。呼吸は浅いか、深いか。肩に力が入っているか。頭の中は静かか、騒がしいか。答えを出す必要はない。ただ気づくだけでいい。これで観察モードが起動する。作業に入ったら、意識の10〜20%を「自分を観察する視点」に割り当てる。残りの80〜90%で作業しながら、バックグラウンドで自分の変化を捉え続ける。「今、少し退屈になってきた」「焦りが出てきた」「集中が浅くなっている」。この観察は論理的に行う必要はない。分析しなくていい。揺らぐ炎を眺めるように、ただ見ていればいい。観察を続けていると、注意が逸れ始める瞬間を捉えられるようになる。「スマホを見ようかな」という考えが浮かんだ瞬間に気づく。気づいたら、その考えを追いかけずに作業へ戻る。「このタスク面倒だな」と感じたら、その感覚を認めて、それでも続ける。別のことを考え始めたら、気づいた時点で戻る。それだけだ。シンプルだが、これが全てだ。作業の構造微観法と組み合わせて効果が上がった作業の構造がある。まず、案件は混ぜない。案件Aで開発をしていて、案件Bのメールに返信して、また案件Aに戻る。以前はこれを普通にやっていた。普通に効率の悪いマルチタスク。でもこれはAIエージェントと働いていても同じだった。案件を切り替えるとき、脳は多くのことを読み込み直している。関係者は誰か。この人にはどう接するべきか。過去にどんな経緯があったか。暗黙の制約は何か。自分はこの案件でどういう立ち位置か。これは単なる情報ではなく、人間関係のシミュレーションだ。技術的は話だけではない。だから重い。案件の「重さ」には差がある。関係者が多い案件は重い。長期で複雑な経緯がある案件は重い。緊張感のある関係を含む案件はより重い。これらを頻繁に切り替えると、作業そのものより切り替えで消耗する。だから案件単位で時間を区切っている。この2時間は案件A、次の2時間は案件B。案件の中で完結させる。次に、同一案件内ではモードを切り替える。開発モードではコーディングや設計、AIエージェントへの指示出しをする。執筆モードではドキュメントや企画書、翻訳に取り組む。準備モードでは開発や執筆を円滑に進めるための下調べ、環境構築、資料整理、Slackの確認などをする。Slackの通知は基本的に無視している。見るのは準備モードのときだけだ。開発中や執筆中にSlackへ戻っていたら、何も進まない。通知は他人の優先順位だ。自分の優先順位を守れ。ポモドーロの25分をモード単位で使っている。アプリはBe Focusedは有料版を買い上げで使っている。随分前に購入したのですがとにかく困ることがないので別に移ろうと思ったことがないなので比較などはできない。Be Focused Pro - Focus TimerDenys Ievenko仕事効率化¥2,000apps.apple.com同じ種類の作業は並列で回す同じモード内であれば、複数の作業を並列で回せる。ブログを書くとき、1本だけを最初から最後まで書くのではなく、2〜3本を並列で進める。1本目の導入を書いて、詰まったら2本目に移る。2本目の本論を書いて、また1本目に戻る。開発でも同様で、5本程度のタスクを並列で回している。なぜこれができるのか。「書くモード」や「開発モード」を維持したまま、対象だけを切り替えているからだ。モードを起動するコストは高いが、一度起動してしまえば、対象を変えるコストは低い。しかし並列できる数には限界がある。開発は5本程度いけるが、ブログは2〜3本が限界だ。この差は「状態の外部化」で説明できる。開発はgit worktree（複数のブランチを同時に扱える開発ツール）やコード自体が「どこまでやったか」「何をしようとしていたか」を保持してくれる。見れば思い出せる。脳が状態を覚えておく必要がない。だから多くを並列にできる。ブログは違う。「この記事で何を言いたかったか」「どういう構成にするつもりだったか」が頭の中にしかない。外部化されていないから、並列の限界が低い。二重の飽き防止ここまで来て、自分が二重の飽き防止システムを走らせていることに気づいた。飽きは敵だ。でも飽きは設計で無効化できる。マクロレベルでは、同種作業の並列によって、外から新規性を供給している。ブログ1からブログ2へ、またブログ1へ。1つの記事を長時間書き続けると退屈になる。でも複数を回していれば、戻ってきたときに新鮮な目で見られる。ミクロレベルでは、微観法によって、内から新規性を生成している。自分自身の微細な変化を「見るもの」として扱っている。外部刺激がなくても退屈しない。この二重構造があるから、飽きによる集中力低下を防ぎながら、並列作業中に「自分がどこにいるか」を見失わずにいられる。実際、微観法がなければ並列作業は成立しない。複数の作業を回していると、「あれ、今どこにいたっけ」「何をしようとしてたんだっけ」となりやすい。微観法で自分の認知状態を観察し続けているから、位置感覚を保てる。迷子にならないから、遠くまで行ける。ようやく気づいたことここまで来て、ようやく気づいた。開発という仕事の性質そのものが変わっていたように思える。戦国無双と信長の野望というゲームがある。どちらも戦国時代を舞台にしているが、まったく別のゲームだ。戦国無双は自分が武将となって敵を斬りまくるアクションゲーム。信長の野望は君主となって複数の武将に指示を出し、国全体を動かすシミュレーションゲーム。自分で戦うか、全体を指揮するかの違いだ。AIエージェントとの協働は、仕事を戦国無双から信長の野望に変えた。プレイヤーから司令官へ。自分で剣を振るうのではなく、複数の部下に指示を出して全体を動かす。求められる集中の質が、根本から違う。私は最初、戦国無双の集中法で信長の野望をプレイしようとしていた。一人で深く没入しようとしていた。だからうまくいかなかった。私にとって微観法は、信長の野望のための集中法だったのだ。自分の状態を観察し続けることで、複数の部下（エージェント）の動きを把握し、全体を俯瞰する。深く沈むのではなく、広く見渡す。集中の形が変わったのではない。仕事の形が変わったのだ。深い集中が戻ってきた微観法を続けて数ヶ月、予想していなかった変化があった。諦めたはずのものが、形を変えて戻ってきた。以前の「湖に沈む」ような深い集中が、少しずつ戻ってきている。最初は水面近くを泳ぐだけだった。浅いけれど安定した集中。それはそれで十分に機能していた。でも続けているうちに、観察しながらでも深く入れる瞬間が出てきた。観察が自動化されてきたのだろう。最初は意識的に10〜20%を割り当てていた。それが習慣になり、無意識でも観察が走るようになった。すると、残りの意識をより深く作業へ向けられるようになった。意識して始めたことが、やがて無意識になる。それが習得だ。今は、水面近くで泳ぎながら、ときどき深く潜れる。潜っている間も、どこかで自分を観察している感覚がある。以前の「なぜキーボードを打っているかわからなくなる」状態とは少し違う。意識はあるのに、深い。完全に以前と同じではない。でも深さと柔軟さの両方を持てるようになりつつある。そして気づいた。あの「見せかけの効率」が消えていた。タスクは消化されている。でも今は、なぜそう書いたか説明できる。自分の中に残るものがある。量だけでなく、質も戻ってきた。集中の持ち方を変えたことで、仕事との向き合い方そのものが変わっていた。最近、もう1つ変化が起きている。案件Aの開発をしている待ち時間に、同じ案件の軽い調整作業ができるようになってきた。エージェントが処理している間の数十秒から数分の隙間で、ちょっとした修正や確認を挟める。自分がどこにいるかを常に把握できているから、短い寄り道をしても迷子にならない。進化は、まだ続いている。これからこれが2025年現在、AIエージェントと協働しながら働いている一人のソフトウェアエンジニアの集中法だ。完璧ではない。でも機能している。環境が変われば、集中の持ち方も変わる。以前の「深く沈む」集中法は、中断と再開が前提の環境には合わなくなった。代わりに見つけたのが、微観法だった。自分の微細な変化を観察し続けることで、注意の逸脱を早期に検知し、復帰を速くする。深さではなく、復帰の速さで勝負する。エージェントはこれからも進化する。集中の持ち方も、また変わるだろう。今の方法が最終形ではない。でも、変化に適応する方法は見つけた。微観法は才能ではなく方法だ。次の作業を始める前に、5秒だけ自分の呼吸を確認してみてほしい。5秒でいい。そこから全てが始まる。かつて愛した湖の深みに、今は違う形で戻れるようになった。水面近くを泳ぎながら、好きなときに深く潜れる。そして、いつでも水面に戻れる。続編を書きました。syu-m-5151.hatenablog.com参考書籍知性の未来―脳はいかに進化し、AIは何を変えるのか―作者:マックス・ベネット新潮社AmazonPLURALITY　対立を創造に変える、協働テクノロジーと民主主義の未来（サイボウズ式ブックス）作者:オードリー・タン,E・グレン・ワイルライツ社Amazon一点集中術――限られた時間で次々とやりたいことを実現できる作者:デボラ・ザックダイヤモンド社Amazon集中力がすべてを解決する　精神科医が教える「ゾーン」に入る方法作者:樺沢 紫苑SBクリエイティブAmazonイェール大学集中講義 思考の穴――わかっていても間違える全人類のための思考法作者:アン・ウーキョンダイヤモンド社Amazon大人のADHDのためのマインドフルネス作者:リディア・ジラウスカ,大野裕,中野有美金剛出版Amazon多動脳―ＡＤＨＤの真実―（新潮新書） （『スマホ脳』シリーズ）作者:アンデシュ・ハンセン新潮社Amazonヤバい集中力　1日ブッ通しでアタマが冴えわたる神ライフハック45作者:鈴木 祐SBクリエイティブAmazon奪われた集中力: もう一度〝じっくり〟考えるための方法作者:ヨハン・ハリ作品社Amazon","isoDate":"2025-12-10T00:27:06.000Z","dateMiliSeconds":1765326426000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"実力とは“最悪の自分”が決める","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/09/092256","contentSnippet":"はじめに私たちは「実力」という言葉を履き違えています。特に私がそうでした。様々な人の助力で得た結果、たまたま条件が揃って出せた最高到達点を「自分の実力」だと勘違いしていました。そして、その水準に届かない日々の自分を見て、「なんでもっとできないんだ」と追い込んでいました。結果は散々なものでした。心身ともに疲弊し、パフォーマンスはさらに落ち、悪循環に陥りました。しかし、その経験から大きな学びもありました。ゾーンに入り、神がかった速度でコードを書く自分。難解なバグを一瞬で特定する自分。私たちは、あの奇跡的な瞬間を「自分の実力」だと信じ、そうでない日を「調子が悪かった」と言い訳します。逆です。何もやる気が起きず、頭も回らず、ただ惰性でキーボードを叩いている日。その泥のような日に絞り出したアウトプット。それこそが、紛れもない私の「実力」です。 絶好調のときの成果は、再現性のない「運」や「上振れ」に過ぎません。この記事では、なぜそう言えるのか、そしてその認識がなぜ重要なのかを考えていきます。これは鬱屈とした日々を過ごしていたかつての自分に向けて書いています。「最高出力」という幻想まず、私たちが「実力」だと思い込んでいるものの正体を見てみましょう。過去半年を振り返ってみてください。「奇跡的にうまくいった日」は何日あったでしょうか。全てが噛み合い、コードがスラスラ書けて、レビューも一発で通り、障害対応も華麗にこなせた日。おそらく、片手で数えられる程度ではないでしょうか。なぜそんなに少ないのでしょうか。理由は単純です。「最高のパフォーマンス」を出すためには、無数の条件を揃える必要があります。十分な睡眠。適度なストレス。興味のある課題。邪魔の入らない環境。体調の良さ。プライベートの安定。これらすべてが揃う日は、人生において稀なのです。その稀な瞬間にしか出せないものを「実力」と呼ぶのは、ギャンブルで勝った日の収支を「年収」と呼ぶようなものです。 奇跡を前提にした人生設計は、破綻することが約束されています。それは本人にもコントロールできない「可能性の上限」であって、信頼できる「能力」ではありません。私自身の話をしましょう。このブログには、いわゆる「おい、」シリーズと呼ばれる記事があります。「おい、本を読め」「おい、スマホを置け」「おい、対話しろ」。ありがたいことに、これらの記事は多くの人に読まれています。はてなブックマークでもたくさんのコメントをいただきました。Xをフォローしてくれている人は知っていると思うのですが4冊紹介するフォーマットも実力以上にアウトプットを出せたと思います。syu-m-5151.hatenablog.comしかし、正直に言いましょう。あれは私の実力からかなり上振れしています。あの記事を書いたとき、たまたま言葉がスラスラと出てきました。たまたま自分の経験と文章のリズムが噛み合いました。たまたま読者の琴線に触れるタイミングでした。書籍レベルの何かを目指してブログとして色々出した。同じクオリティのものを、明日また書けるかと問われれば、自信を持ってイエスとは言えません。あれが私の「実力」だと思い込んでしまうと、危険です。次に書く記事が同じように読まれなかったとき、「調子が悪かった」「本来の力が出せなかった」と言い訳をしてしまいます。しかし実際には、「おい、」シリーズの方が例外なのです。私の本当の実力は、誰にも読まれない記事を淡々と書き続けられるかどうか、そちらの方にあります。では、なぜ私たちは「最高の自分」を実力だと思い込んでしまうのでしょうか。それは、そう思いたいからです。「あれが本当の自分だ」と信じることで、今の不甲斐ない自分を一時的なものとして処理できます。「今日は調子が悪いだけ」という言い訳は、私たちの自尊心を守ってくれます。しかし、その言い訳に甘えていると、現実を直視する機会を失ってしまいます。信頼は「下限」に支払われる「最高の自分」を実力だと思い込むのは、自分一人の問題なら、まだいいかもしれません。しかし、私たちは一人で働いているわけではありません。では、社会はどちらを評価するのでしょうか。「最高の自分」か、「最悪の自分」か。仕事を誰かに頼むとき、私ならどちらを選ぶでしょうか。「調子が良ければ神がかったコードを書くが、悪ければ全く動かないものを出してくる天才」か、「どんなに最悪の状況でも、必ずそこそこ動くものは持ってくる凡人」か。チームで働いていれば、答えは明らかです。前者はリスクであり、後者は計算できる資産です。天才は賭けられる。凡人は任せられる。 組織が求めているのは、後者です。なぜでしょうか。仕事には締め切りがあります。依存関係があります。他のメンバーのスケジュールがあります。私の成果物を待っている人がいます。私が遅れれば、その人も遅れます。その人が遅れれば、次の人も遅れます。「今日は調子が悪いので」という言葉は、その連鎖の中では通用しません。だから、周囲からの信頼とは、「最高の自分」ではなく「最悪の自分」に対して支払われます。あのシニアエンジニアが信頼されているのは、華麗なワンライナーを書けるからではありません。障害が起きたとき、体調が悪いときでも、最低限の品質で対応を完了させるからです。レビューが溜まっているとき、モチベーションが上がらないときでも、的確なコメントを返すからです。「この人に任せれば、最悪でもこのレベルは下回らない」という安心感。それが信頼の正体です。プロフェッショナルとは、派手なファインプレーをする人ではありません。どんな悪条件でも、期待された成果を淡々と、確実に納品できる人のことです。野球で言えば、たまにホームランを打つ選手ではなく、どんな状況でも確実にヒットを打てる選手。料理で言えば、たまに絶品を作る料理人ではなく、毎日安定して美味しいものを出せる料理人。派手さはありませんが、計算できます。それがプロです。なぜマニュアル本を読んでも「床」は上がらないのか「下限」が大事だということはわかった。では、どうすれば下限を上げられるのか。その答えを求めて、私たちは成功者の話に耳を傾けます。本、セミナー、SNS。「こうすればうまくいく」と教えてくれる人はたくさんいます。しかし、残念ながら、それらは役に立ちません。なぜでしょうか。成功とは「その人固有の条件」と「その時点での環境」が噛み合った結果であり、その組み合わせは二度と再現されないからです。10年のキャリアがあった人と、始めたばかりの人では前提が違います。たまたま有名な人にリツイートされた人と、そうでない人では運が違います。他人の成功パターンをコピーしても意味がありません。だから、私たちがやるべきことは、誰かの成功法則を学ぶことではありません。自分自身の「下限」を把握し、その下限を少しずつ上げていく仕組みを作ることです。 それは誰にも教えてもらえません。自分で試行錯誤するしかないのです。能力は文脈の中にしかない他人の成功パターンをコピーしても意味がない。自分の下限を自分で上げていくしかない。そう書きました。しかし、ここで少し立ち止まって考えたいことがあります。そもそも「能力」とは何なのでしょうか。私たちが上げようとしている「下限」とは、何の下限なのでしょうか。私たちは「能力」を、自分の中に固定的に存在するパラメータのように考えがちです。技術力がいくつ、コミュニケーション力がいくつ、というように。しかし、私はそうは思いません。能力は、環境によって大きく変わるものです。私は自分の技術力や業務遂行力を、完全に文脈依存だと思っています。ある環境では、私の思考パターンや働き方が完璧に噛み合い、高いパフォーマンスが出ます。しかし、別の環境では、私は無能になるでしょう。政治的な調整が最優先される組織や、レガシーな技術に固執する現場では、私の強みは発揮されません。あのプロジェクトがうまくいったのは、自分の技術力が高かったからでしょうか。それとも、チームメンバーが優秀だったからでしょうか。上司が適切にスコープを切ってくれたからでしょうか。インフラが安定していたからでしょうか。ドキュメントが整っていたからでしょうか。締め切りに余裕があったからでしょうか。その支えが消えた場合、同じクオリティを出せるでしょうか。「自分には能力がある」と過信するのは危険です。正しい認識はこうです。「この文脈において、これまでの経験と仕組みが噛み合って、たまたま価値が出せている」。この認識があれば、傲慢にはなれません。自分が成果を出せているのは、周囲の環境や、他者のサポートのおかげであるという事実が見えてきます。そして、その環境が変わったときに自分がどうなるかを、冷静に想像できるようになります。たとえるなら、魚と水の関係に似ています。魚は水の中では自由に泳げますが、陸に上がれば何もできません。 私たちは常に、自分の能力が機能する「水」の中にいます。その「水」がなくなったとき、私たちは何もできません。だからこそ、2つのことが必要だと私は思っています。1つは、自分に合った「水」を見つけること。自分の能力が活きる環境を選ぶこと。もう1つは、「水」がなくなったときにも最低限動けるように、自分の「下限」を上げておくことです。環境に恵まれなくても、最低限のアウトプットは出せる状態を作っておくこと。「頑張り」という免罪符能力は文脈に依存する。環境が変われば、同じ人間でも発揮できるパフォーマンスは変わる。だからこそ、自分に合った環境を見つけ、下限を上げる仕組みを作ることが大事だと書きました。ここまで読んで、こう思った人もいるかもしれません。「環境だの仕組みだの言っているけど、結局は頑張れば何とかなるのではないか」と。気持ちはわかります。私もそう思っていた時期がありました。しかし、残念ながら、そうではありません。多くの人は、能力の不足を「頑張り」で埋めようとします。環境が悪くても、仕組みがなくても、気合で乗り越えようとします。私もそうでした。しかし、「頑張り」は実力ではありません。なぜそう言えるのでしょうか。思い返してみてください。「頑張っています」という言葉を、どんなときに使ったでしょうか。私の場合、成果が出ていないときほど、その言葉を使っていました。深夜まで残業した。休日も勉強した。ドキュメントも読んだ。だから許してほしい。私も例外ではありません。締め切り前に焦って残業した経験は何度もあります。そのとき、「これだけやっているのだから」という気持ちが、どこかにありました。成果が出なくても、頑張った事実が自分を守ってくれるような気がしていました。しかし、「これだけ苦労したのだから」という免罪符は、プロの世界では通用しません。 専門的な仕事に対する報酬は、流した汗の量ではなく、生み出した価値に対して支払われるからです。私が「頑張ったのにできなかった」と最後に言ったのはいつだったでしょうか。その頑張りは、成果とどう結びついていたでしょうか。正直に振り返ると、「頑張り」と「成果」の間には、驚くほど相関がありませんでした。もう少し踏み込んで考えてみましょう。なぜ「頑張り」は実力にならないのでしょうか。人間の精神力や体力といった不安定なリソースに依存したシステムは、いずれ破綻するからです。徹夜で乗り切った。気合で押し切った。それは一時的には機能するでしょう。しかし、そのやり方は再現できません。翌週も同じことをやれと言われたら、身体が壊れます。翌月も同じことをやれと言われたら、心が壊れます。「頑張り」で出した成果は、「最高の自分」と同じです。再現性がありません。だから、実力とは呼べないのです。来月も同じことができないなら、それは実力ではありません。誤解しないでほしいのは、「頑張るな」と言いたいわけではないということです。踏ん張るべき時は、踏ん張らなければなりません。問題は、頑張ることそれ自体が目的化してしまうことです。方向を考えずにただ頑張る。成果ではなく、頑張っている姿勢で自分を守ろうとする。それは努力ではなく、努力のふりです。目指すべきは「頑張らなくても成果が出る状態」です。 怠けることではありません。頑張りに依存しなくても回る仕組みを作ることです。そうすれば、本当に踏ん張るべき時に、余力を残しておけます。環境構築という本当の能力「頑張り」に頼らない。では、具体的に何をすればいいのでしょうか。私なりの答えは、「最悪の自分でも動ける仕組みを作る」 ことです。気力ゼロの日でも実行できる仕組みを、私はいくつ持っているだろうか。この問いを自分に投げかけたとき、意外なほど少ないことに気づきました。エディタを開いたら自動でテストを走らせる。プルリクエストを出したら自動でレビュワーをアサインする。障害が起きたらアラートを飛ばし、対応手順書を自動で開く。毎朝同じ時間に、昨日のタスクの振り返りをSlackに届ける。毎週同じ曜日に、今週やるべきことをリストアップする。これはすべて、最悪の状態でも最低限の品質を担保するための仕組みです。私が目指しているのは、最悪の日でも自動的に手が動き、最低限のクオリティのものが出来上がってしまう状態を作ることです。意志の力で動くのではなく、意志がなくても動いてしまう仕組みを作る。これこそが「環境構築能力」であり、本当の意味での「実力」です。逆に、仕組み化されていない行動を見てみましょう。タスク管理ツールを開くのが面倒だから、頭の中で覚えておく。テストを書くのが面倒だから、動作確認は目視でやる。ドキュメントを書くのが面倒だから、後で誰かに聞けばいいと放置する。コードレビューを依頼するのが面倒だから、自分で何度も見直す。これはすべて、調子が良いときにしか機能しないシステムです。調子が悪くなった瞬間、すべてが崩壊します。頭の中のタスクは忘れます。目視の確認は見落とします。誰かに聞こうと思っていたことは、聞きそびれます。手を動かすまでのハードルはどこに潜んでいるでしょうか。それを仕組み化ではなく気合で乗り越えていないでしょうか。私はそう思って、少しずつ仕組みを増やしてきました。「人」を「環境」に合わせるな仕組みを作る話をしてきました。しかし、仕組みを作ろうとするとき、多くの人がある罠にはまります。「自分を変えなければ」という罠です。たとえば、こんなふうに自分を責めていないでしょうか。「なぜ自分はこんなに集中力がないのか」。「なぜ自分はこんなにやる気が出ないのか」。「なぜ自分は普通の人のように働けないのか」。その問いの立て方が、そもそも間違っています。「人」を「環境」に合わせようとするから苦しくなります。「自分を変えなければ」「自分が適応しなければ」と考えるから、うまくいかない自分を責めてしまいます。発想を逆転させるべきです。「集中力がなくても成果が出る環境を作れないか」と考える。「やる気がなくても手が動く仕組みを作れないか」と工夫する。「普通の働き方ができなくても、自分なりの働き方で成果を出せないか」と模索する。「障害」は人側にあるのではありません。環境側にあります。人を直すのではなく、環境を直す。それがエンジニアリングです。これは、私たちエンジニアにとっては馴染みのある考え方のはずです。ユーザーがシステムを使いこなせないとき、「ユーザーの能力が低い」とは言いません。「UIが悪い」と言います。システムがユーザーに合わせるべきであって、ユーザーがシステムに合わせるべきではありません。同じことが、自分自身にも言えます。自分という「ユーザー」が動きやすいように、自分の環境という「システム」を設計する。自分の弱点を克服しようとするのではなく、弱点があっても回るように環境を設計する。私たちは日々、他者のためにシステムを設計しています。そのシステムが、特定の「正常」を前提にしていないでしょうか。最高のコンディションの人間しか使えないように設計されていないでしょうか。最悪の状態の人間でも最低限動けるように設計されているでしょうか。自分自身の働き方も、同じように設計すべきです。「正常」な自分を前提にしない。「最悪」の自分でも回るように設計する。弱さこそが、堅牢なシステムを作る「人」を「環境」に合わせるのではなく、「環境」を「人」に合わせる。自分の弱点を克服しようとするのではなく、弱点があっても回るように環境を設計する。そう書くと、まるで弱さを隠すための工夫のように聞こえるかもしれません。弱い自分を誤魔化して、なんとかやり過ごすためのハックのように。しかし、私が言いたいのは、そういうことではありません。むしろ逆です。弱さは、隠すものではありません。弱さこそが、堅牢なシステムを作るための仕様書になります。私たちは誰でも、何かしら「苦手なこと」を抱えています。朝が弱い。人前で話すのが苦手。細かい作業が続かない。逆に、一度集中すると周りが見えなくなる。そういった、ごく普通の凸凹です。「このエラーメッセージは不親切だ」と感じるのは、かつて自分が同じような場面で困った経験があるからです。「このドキュメントはわかりにくい」と感じるのは、かつて自分がわからなくて苦しんだ経験があるからです。「このUIは使いにくい」と感じるのは、かつて自分が同じように躓いた経験があるからです。欠損は、視点を生みます。 困った経験は、問題を発見する能力になります。痛みを知っているからこそ、他者の痛みに気づけます。うまくいった人には、うまくいかない人の気持ちがわかりません。私自身、そうでした。「正常」に適応できていた頃の私には、「正常」の問題点が見えませんでした。システムにうまく乗れていた頃の私には、そのシステムから弾かれる人の存在が見えませんでした。自分が躓いて初めて、躓く人のための設計ができるようになりました。だから、過去の「苦手」を恥じる必要はありません。それは、視点の源泉です。「最悪の自分」を知っているからこそ、「最悪の状態でも動けるシステム」を設計できるのです。 自分のバグを知り尽くしているからこそ、バグに強いシステムを作れます。評価されるとは、下限が固定されることここまで、「下限を上げることが大事だ」と書いてきました。自分の苦手を知り、それを視点として活かし、最悪の状態でも動ける仕組みを作る。それが実力になると。ここまで読むと、「じゃあ下限を上げ続ければいいんだな」と思うかもしれません。しかし、話はそう単純ではありません。ここで1つ、厄介な問題について触れておかなければなりません。キャリアを積み、シニアになり、周囲から「できる人」として扱われるようになると、ある種の息苦しさが生まれます。「あの人ならこのレベル」という期待。それは信頼の証であると同時に、私たちを縛る鎖でもあります。評価されるということは、自分の「下限」が社会的に可視化され、固定されることを意味します。そして、ここに厄介な問題があります。下限が固定されると、それを下げることが許されなくなるのです。本来、下限を上げていくためには、一時的に下限を下げる必要があります。これは矛盾しているように聞こえるでしょうが、考えてみれば当然のことです。新しい領域に挑戦すれば、最初は当然うまくいきません。慣れない技術を使えば、普段の半分のクオリティしか出せません。未経験の役割を引き受ければ、しばらくは無能に見えます。たとえば、10年間Javaを書いてきたエンジニアがRustを学び始めたとします。最初の数ヶ月、その人の「下限」は確実に下がります。Javaなら寝ぼけていても書けたコードが、Rustでは何時間もかかります。しかし、その一時的な後退を経て、やがてRustでも安定した成果を出せるようになります。同様に、初めてチームリーダーを務める人は、最初は判断を誤り、メンバーとの関係構築に苦労するでしょう。しかし、その経験を経て、リーダーとしての「下限」が形成されていきます。これは成長のための必要なコストです。一時的に下がった下限は、経験を積むことで元の水準を超えていきます。しかし、「あの人ならこのレベル」という期待が固定されてしまうと、その期待を下回ることが許されなくなります。失敗が許されません。実験が許されません。成長のための一時的な後退が、信頼の毀損として記録されてしまいます。だから私は、仕事を選ぶようになりました。自分の下限が確実に通用する領域、自分のシステムが機能する文脈を選ばざるを得なくなりました。「結果を出す以外の選択肢がない」状況で、わざわざ未知の領域に踏み込むリスクを取れなくなりました。これは成長の鈍化を意味します。安全圏に留まり続けることで、下限は維持されますが、それ以上には上がりません。皮肉なことに、「信頼される」ことが「成長できなくなる」ことと表裏一体になっています。 評価されることの代償は、挑戦する自由を失うことです。期待に応え続けることと、成長し続けることは、両立しません。だからこそ、意識的に「失敗してもいい場所」を確保しておく必要があります。誰にも見せないプロジェクト。評価と切り離された実験。下限を一時的に下げることが許される、安全な砂場。それがなければ、私たちは自分の「実力」に閉じ込められてしまいます。余白がなければ成長できない評価されることで挑戦する自由を失う。「失敗してもいい場所」を意識的に確保しなければならない。ここまで書いてきて、気づいたことがあります。これは私個人の問題ではありません。もっと広い話です。「下限」の問題は、個人だけで解決できるものではありません。先ほど書いたように、下限を上げるためには、一時的に下限を下げる必要があります。新しいことに挑戦すれば、最初は失敗します。失敗が許されない環境では、挑戦ができません。挑戦ができなければ、成長もできません。つまり、成長には「余白」が必要なのです。「余白」とは何でしょうか。失敗しても致命傷にならない空間のことです。期待値を下回っても、信頼が毀損されない関係性のことです。最悪の状態を見せても、それを受け入れてもらえる場所のことです。エンジニアは強くなければならない。弱音を吐いてはいけない。立ち止まってはいけない。誰よりも速く学び、誰よりも多くのコードを書き、誰よりも深く技術を理解する。その強迫観念は、私たちを奮い立たせるガソリンであると同時に、余白を奪う呪いでもあります。常に100%を出し続けなければならない。そう思い込んでいる人は多いです。しかし、100%を出し続けることは、人間には不可能です。そして、100%を要求される環境では、人は80%の自分を見せることを恐れます。80%の自分を見せることを恐れるから、新しいことに挑戦できません。新しいことに挑戦できないから、100%のまま停滞します。完璧主義は、成長の敵です。「挑戦しろ」と背中を押す一方で、いざ失敗すれば「自己責任」の名の下に切り捨てる。そんな構造の中では、誰も本当の意味での挑戦ができません。みんな、自分の下限が確実に通用する範囲でしか動かなくなります。結果として、組織全体の成長が止まります。だから、「余白」は個人で確保するだけでなく、チームや組織として設計する必要があります。「最悪の日でも最低限の成果を出せる環境」を作るのは、個人の努力だけでは限界があります。チームとして、組織として、メンバーの「下限」を支える仕組みを作る。失敗を許容する文化を作る。一時的な後退を、成長のための投資として認める空気を作る。それが本当の意味での「強いチーム」です。 全員が常に100%を出し続けるチームではありません。誰かが50%しか出せない日があっても、チーム全体としては回るように設計されたチームです。個人の「下限」を上げる努力と、組織として「余白」を確保する努力。この両方が揃って初めて、持続的な成長が可能になります。床を1ミリずつ上げていくここまで、長々と書いてきました。「最高の自分」ではなく「最悪の自分」が実力である。信頼は下限に支払われる。能力は文脈に依存する。頑張りは実力ではない。環境を設計する。弱さを視点にする。評価は下限を固定する。成長には余白が必要。いろいろ書きましたが、言いたかったことはシンプルです。「能力」の定義を変えてほしい、ということです。「最高のときに出せるもの」から「最悪のときにも出せるもの」へ。自分の状態がベストであることを前提にしない。10分の1のコンディションでも形になるように設計する。緊張しても、失敗しても、体調が悪くてもいい。そのボロボロの状態から這いつくばって出したアウトプットだけを見る。それが、今の私の揺るぎない実力です。絶望する必要はありません。自分の「下限」、つまり床がどこにあるかを知っていれば、その床の上にレンガを積んでいくことができます。半年前の自分は、最悪の日に何ができていなかったでしょうか。タスク管理は頭の中だったでしょうか。テストは書いていなかったでしょうか。ドキュメントは後回しにしていたでしょうか。今はそのうち、どれだけ自動化・習慣化されているでしょうか。継続とは、平均値を上げることではありません。この床を1ミリずつ底上げしていく作業のことです。派手な成功は、運です。華々しい成果は、上振れです。 そんなものを基準にしてはいけません。運は二度来るとは限りませんが、仕組みは何度でも動きます。淡々と、最悪の日でも最低限のことをやる。その積み重ねだけが、誰にも奪われない実力になります。いつかその床の高さが、誰かの天井を超えたとき、私は誰からも信頼されるプロフェッショナルになっているはずです。おわりに最後に、この記事自体の話をさせてください。この記事を書きながら、私自身も自分の「下限」と向き合っています。正直に言えば、この文章を書いている今日も、絶好調とは言えません。頭がぼんやりして、言葉がすぐに出てきません。何度も書いては消し、消しては書いています。「おい、」シリーズのように言葉がスラスラ出てくる日ではありません。しかし、それでいいのです。この記事の価値は、私が絶好調のときに華麗な文章を書けることではありません。調子が悪い日でも、キーボードへ向かい、一文字ずつ積み上げ、最後まで形にできるかどうか。それこそが、私の「書く実力」です。冒頭で書いたように、私はかつて「最高の自分」を実力だと勘違いし、そこに届かない自分を責め続けていました。なんでもっとできないんだ、と。鬱屈とした日々を過ごし、心身ともに疲弊し、パフォーマンスはさらに落ちました。この記事は、あの頃の自分に向けて書きました。伝えたいのは、「基準が間違っている」ということです。私たちは、自分の「最高の瞬間」に執着しすぎています。あの日の自分、あのプロジェクトでの自分、あの輝いていた自分。しかし、その輝きは再現できません。再現できないものを基準にすれば、永遠に自分を肯定できません。だから、視点を変えました。最悪の日に、机に向かえるか。最悪の状態で、最低限のものを出せるか。その「下限」こそが、私の本当の実力です。そしてその下限は、仕組みと環境と、少しずつの積み重ねで、確実に上げていくことができます。派手な成功を追いかける必要はありません。ただ、最悪の日でも崩れない床を、1ミリずつ上げていけばいいのです。その床の高さが、いつか私を支えます。誰にも奪えない、揺るぎない実力として。そして、もう1つ。自分の弱さを恥じる必要はありません。その弱さがあったからこそ、私は「自分を助けるための仕組み」を発明できました。その仕組みは、いずれ同じ弱さを持つ誰かを救うことになります。私の「最悪の日」の対処法は、誰かにとっての「最高のノウハウ」になります。自分の下限を知ることは、諦めではありません。出発点です。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。","isoDate":"2025-12-09T00:22:56.000Z","dateMiliSeconds":1765239776000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"技術広報はちゃんとなめてやれ（技術広報をなめるなを読んで）　","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/08/152614","contentSnippet":"この記事は、whywaita Advent Calendar 2025 8日目のエントリ記事です。whywaita Advent Calendar 10周年ということで、自分もwhywaitaとの出会いと10年という節目を掛けて何か書きたいと考えたのですが、うまいネタが思いつかず。とはいえ、whywaitaと出会ったきっかけがお祭り的な技術イベントだったので、今回は技術イベントの「お祭り性」について語っていきます。思い返すと、技術コミュニティとの出会いは、いつもお祭りのようでした。見知らぬ人と技術の話で盛り上がり、気づいたらとんでもない深い時間になっていた懇親会。準備段階から当日まで、ワクワクしながら作り上げた勉強会。あの空気感こそが、私をエンジニアとして成長させてくれた原動力でもありました。そんな私が最近読んで、考えさせられた記事があります。はじめにSakutaroさんが書かれた「技術広報をなめるな」を読みました。note.comSakutaroさんの主張をこの記事で使うために要約すると、技術広報とは「技術に関する情報流通を最適化すること」であり、採用やブランディングにじわじわ効いてくる組織の筋肉である、ということです。片手間でやるものではなく、専門性を持って取り組むべき重要な機能だと。その主張には100%同意します。技術広報を軽視する組織への警鐘として、価値のある記事でした。詳しくは読んで下さい。ただ、読み終わったあと、ひとつ気になることがありました。「なめるな」と言われて、真面目に取り組んだ人は、どうなるだろう。技術広報の重要性を理解した。だから本気で取り組んだ。毎週ブログを書き、登壇の機会を作り、勉強会を企画した。でも、半年後、1年後、その人はまだ続けているだろうか。私が見てきた現実では、真面目に取り組んだ人ほど、燃え尽きていく。「技術広報は大事だ」と理解しているからこそ、手を抜けない。手を抜けないから、疲弊する。疲弊するから、続かない。続かないから、また新しい誰かが「大事だから」と引き継いで、同じサイクルを繰り返す。ここで断っておくと、私は専任のDevRelや技術広報をやっていたわけではありません。エンジニアとしてブログを書いたり、登壇したり、勉強会を企画したり、そういう活動に参加してきた側です。だから以下は、「現場で技術広報に関わってきたエンジニア」としての個人的な意見です。Sakutaroさんへの反論や批判ではなく、同じテーマを別の角度から眺めてみた、という試みです。Sakutaroさんが「技術広報の重要性」を語ったのなら、私は「技術広報の持続可能性」を語りたい。Sakutaroさんが「なめるな」と言ったのなら、私は「ちゃんとなめてやれ」と言いたい。「なめる」というのは、軽視することではありません。肩の力を抜いて、それでも真剣に向き合うこと。重く構えすぎず、軽やかに、本気で楽しむこと。そういう姿勢を指しています。この記事で言いたいのは、技術広報を「お祭り」として捉え直すことで、どう持続可能な形に設計できるか、という話です。「技術広報を続けられない」のは、個人の努力不足なのか技術広報が続かない。ブログの更新が止まる。勉強会の開催頻度が落ちる。登壇者が見つからない。こうした現象を見たとき、私たちはつい「担当者の努力が足りない」「モチベーションの問題だ」と考えがちです。でも、本当にそうでしょうか。私が見てきた限り、技術広報に関わる人は真面目な人が多い。「会社のためになる」「エンジニアの成長につながる」と信じているからこそ、時間を割いて取り組んでいる。努力が足りないのではなく、むしろ、努力しすぎて燃え尽きている。つまり、個人の努力ではなく、構造に原因があるのではないか。技術広報を「重要な業務」として位置づけるほど、プレッシャーは増す。「会社の顔としてふさわしい記事を」「PVやシェア数で成果を示さないと」「毎月コンスタントに発信を」。こうした期待は、真面目な人ほど重く受け止める。結果として、技術広報は「楽しいからやる」ものではなく「やらなければならない」ものになる。義務感で動く活動は、長くは続きません。だから私は、技術広報を「お祭り」として捉え直すことを提案したい。技術広報を「お祭り」として捉えたとき、何が変わるのか「お祭り」と「業務」の違いは何か。業務には、目標がある。KPIがある。期限がある。評価がある。達成できなければ、失敗になる。お祭りには、もちろん準備や段取りがある。でも、本質は違う。非日常性があって、ワクワクして、参加は自由で、失敗しても笑って済む。みんなで作り上げる。終わったあとに「楽しかったね」と言い合える。思い出してみてください。あなたが「楽しかった」と感じた技術イベントには、何がありましたか。KPIはなかったはずです。評価もなかった。ただ、技術の好きな人たちが集まって、ワイワイやっていた。それだけで、あの場は価値があった。技術広報を「業務」として捉えると、タスクになり、KPIになり、疲弊の原因になります。でも「お祭り」として捉えると、楽しみになり、創造性の源泉になり、持続可能な活動になる。もちろん、会社という組織なのでKPIは必要です。数字で語らないと理解されないこともある。大人ですから、建前として必要なものは必要です。でも本音の部分では、お祭りなんです。Sakutaroさんは技術広報を「技術に関する情報流通を最適化すること」と定義しました。私はその定義に異論はありません。ただ「情報流通の最適化」という言葉は正確ですが、人を動かす力は弱い。「今月の情報流通を最適化しよう」と言われても、イメージが湧かない。でも「お祭りを企画して盛り上げよう」と言い換えると、途端にイメージが湧きます。人は「最適化」という目標には動きにくいけど、「お祭り」という体験には参加したがるんです。そして面白いことに、良いお祭りを企画しようとすると、自然と「情報流通の最適化」が達成されます。読みたくなるブログは情報が届く。参加したくなる勉強会は知見が共有される。面白いカンファレンスブースはブランドが伝わる。お祭りが楽しいのは、予定調和じゃないからです。神輿が予想外の方向に進んだり、知らない人と急に仲良くなったり、思いもよらない出来事が起きる。その「意外性」がお祭りの醍醐味です。技術広報も同じで、完璧に計画されたブログより、思いつきで書いた記事がバズることもある。意外性こそが人の心を動かします。でも、意外性は余裕がないと生まれません。タスクに追われている人に、遊び心は出てこない。「やらなきゃいけない」という義務感からは、「やってみたら面白かった」という発見は生まれない。だから、技術広報には「精神的な遊び」が必要です。お祭りを「業務」として100%真面目にやると、それはもはやお祭りではなくなります。参加の形は、ひとつじゃないお祭りには色んな参加の仕方があります。神輿を担ぐ人もいれば、屋台で焼きそばを売る人もいる。踊る人もいれば、見ているだけの人もいる。写真を撮る人も、SNSで実況する人もいる。ゴミを拾う人も、場所取りをする人もいる。どの参加の仕方も、お祭りの一部です。技術広報も同じです。記事を書く人だけが貢献者ではない。レビューする人も貢献者です。アイデアを出す人も貢献者です。社内で記事をシェアする人も貢献者です。「この前のあの話、ブログにしたら面白そう」と声をかける人も貢献者です。登壇者の練習に付き合う人も貢献者です。「ブログを書いてもらえない」「登壇してもらえない」と悩んでいるなら、視点を変えてみてください。「書いてもらう」「登壇してもらう」以外の参加の形を、用意できているだろうか。神輿を担げる人は限られています。でも、お祭りを楽しむ方法は無数にある。担ぎ手だけがお祭りの参加者ではないんです。「ブログを書いてください」ではなく「先週のSlackでのやり取り、そのままブログにしませんか。私がタイトルと導入書きますよ」。「登壇してください」ではなく「5分のLTでいいので、この前の話をしてくれませんか」。義務ではなく、招待として。「ブログ書いてください」はお願い（義務感）。「ブログ書きませんか」は招待（選択肢）。この違いは大きいんです。あなた自身は、どうでしょうか。技術広報にどんな形でなら、無理なく関われそうですか。「怒られない範囲」は誰が決めているのかお祭りにも「やっていいこと」と「やってはいけないこと」がある。技術広報も同じです。失敗談を書け、人間臭さを出せ、と言われても、リスクが怖い。その懸念は正しいです。だからこそ、「怒られない範囲」を見極める力が必要になります。ただ、その「怒られない範囲」は、誰が決めているのでしょうか。明文化されたルールがあるのか、暗黙の了解なのか。上司が決めているのか、広報部門が決めているのか、法務が決めているのか。あるいは、なんとなく「空気」で決まっているのか。多くの組織では、「怒られない範囲」は明確に定義されていません。だから、発信する側は常に不安を抱えることになる。「これ、出していいのかな」「怒られないかな」。その不安が、発信のハードルを上げている。社内的にはOKだけど、社外的にNGになるケースがあります。「技術的には正しいけど、今その話題は炎上しやすい」という場合です。社外的にはOKだけど、社内的にNGになるケースもあります。「業界では普通の話題だけど、うちの会社ではタブー」という場合です。「怒られない範囲」を見極める能力とは、社内外の文脈を読む力です。これは経験を積むことでしか身につきません。小さく発信して、反応を見て、学んでいく。でも、もし組織として技術広報を続けたいなら、「怒られない範囲」を個人の判断に委ねるのではなく、組織として明確にする努力が必要ではないでしょうか。「ここまではOK」「これはNG」「迷ったらこの人に相談」。そういった指針があるだけで、発信のハードルはぐっと下がります。あなたの組織では、「怒られない範囲」はどのように決まっていますか。誰が決めていますか。それは明文化されていますか。持続可能にするために最後に、どうすれば技術広報を続けられるのか、という話をします。技術広報に関わる人が陥りがちな罠は、自分一人で全部やろうとすることです。ブログの企画、執筆依頼、レビュー、公開作業、SNSでの拡散。全部一人でやると、短期的には回ります。でも、長期的には崩壊します。お祭りは、主催者一人では成立しません。屋台を出す人、演奏する人、ゴミを拾う人、写真を撮る人、SNSで拡散する人。みんなが違う形で参加して、初めてお祭りは盛り上がります。「一人が100やる」のではなく、「10やる人、5やる人、1でも協力してくれる人を探す」これが持続の秘訣です。例えば、こんな工夫ができます。月に1回「ブログネタ出し会」を30分だけ開く。Slackに「こんな話をブログにしたい」と投げるだけのチャンネルを作る。「書けそうな人」ではなく「話が面白かった人」に声をかける。小さな仕組みを作っておくだけで、協力者は見つかりやすくなります。そして、もう1つ大事なこと。人間には波があるということです。10やれる時期もあれば、5しかやれない時期もある。1すらもやれない時期もある。プロジェクトが佳境に入っている時期。体調を崩している時期。家庭の事情がある時期。メンタルが落ちている時期。これは恥ずかしいことでも、甘えでもありません。人間だもの。「去年できたから、今年もできる」という思い込みこそが、燃え尽きの原因なんです。10やれる時は10やる。5しかやれない時は5でいい。やれない時は、休む。大事なのは、この「波」を組織として受け入れられているかどうかです。「先月は3本記事を出したのに、今月は1本もない。どうしたの」というプレッシャーがかかるなら、それは持続可能な仕組みとは言えません。「今月は厳しいので、来月がんばります」と言える文化があるかどうか。あなたのチームでは、パフォーマンスの波を受け入れられていますか。「今は無理」と言える空気がありますか。おわりに冒頭で書いた通り、私とwhywaitaの出会いは、お祭り的な技術イベントでした。あの場には「情報流通の最適化」なんて言葉はなかった。ただ、技術の好きな人たちが集まって、ワイワイやっていただけです。でも、今ならわかります。私がワイワイと参加していたあのイベントの裏側には、真面目に予算を集めてきた人がいた。色んなステークホルダーの合意をまとめてきた人がいた。会場を押さえ、スケジュールを調整し、トラブルに備えていた「ちゃんとした大人」がいた。私はその恩恵を受けて、楽しんでいただけだったんです。10年経って、そのことがようやくわかるようになりました。いずれ自分も、あの「ちゃんとした大人」の側に回らなければならない。恩返しをしなければならない。その自覚はあります。でも、それでも。いや、だからこそ。次の世代の人たちには、お祭り感を味わってほしい。「裏側の苦労」を見せずに、「楽しかったね」と言ってもらえるイベントを作りたい。真面目に準備しながら、参加者には「お祭り」として届ける。それが、私なりの恩返しの形だと思っています。技術広報に関わるすべての人へ。疲れたら、休んでください。無理したら、倒れます。真面目すぎたら、続きません。でも、楽しさだけでも続きません。楽しさと、仕組みと、仲間が必要です。もしあなたが今「何もやれない時期」にいるなら、それでいいんです。休んでください。お祭りは、また元気になってから参加すればいい。技術広報は、あなたがいないと回らないほど脆弱なものであってはいけない。でも、あなたがいると、もっと楽しくなる。それくらいの距離感がちょうどいい。10年前のあの日、技術イベントで会った人と、今もこうしてAdvent Calendarで繋がっている。これこそが、お祭り駆動の技術広報の成果です。どこかのカンファレンスの懇親会で会ったら、お祭りの話をしましょう。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。","isoDate":"2025-12-08T06:26:14.000Z","dateMiliSeconds":1765175174000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、類推するな","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/06/060208","contentSnippet":"この記事は、Rust Advent Calendar 2025 6日目のエントリ記事です。はじめに「それって、○○みたいなものですよね」私は、この言葉に何度救われてきただろう。新しい概念を理解するとき。誰かに説明するとき。問題を解決するとき。類推は、私の思考の基盤だった。いや、今でも基盤だ。ただ、その基盤が思ったほど頑丈ではなかったことを、私は何度も思い知らされてきた。Rustを学び始めた頃の話だ。Rustは、プログラミング言語の1つだ。安全で高速なプログラムを書けることで知られている。私はRustの公式教科書「The Rust Programming Language」を読んでいた。所有権の章に差し掛かったとき、こんな説明に出会った。Rustには「所有権（ownership）」という独特の概念がある。少し専門的な話になるが、プログラムを書くとき、データはコンピュータの「メモリ」という場所に保存される。メモリは有限だから、使い終わったデータは片付けなければならない。片付けを忘れると、メモリがいっぱいになって動かなくなる。逆に、まだ使っているデータを間違えて片付けてしまうと、プログラムが壊れる。多くのプログラミング言語では、この「いつ片付けるか」の管理をプログラマーに任せるか、自動で行うかのどちらかだ。Rustは第三の道を選んだ。「所有権」というルールで、コンパイル時（プログラムを実行する前）に安全性を保証する。ルールはシンプルだ。メモリ上のデータには、必ず1つの「所有者」となる変数が存在する。そして、その値を別の変数に渡すと、所有権が移動（move）する。移動した後は、元の変数からはアクセスできなくなる。所有者がいなくなったデータは、自動的に片付けられる。これがRustの基本ルールだ。（注：この先、コード例が続きます。プログラミングに詳しくない方は、コードの詳細を読み飛ばしても大丈夫です。「類推で理解したつもりになったが、実際は違った」という体験談として読んでいただければ、本記事の主旨は伝わります。）私は頭の中で、勝手に類推を作り上げた。「なるほど、本の貸し借りみたいなものか。本を誰かに貸したら、自分の手元にはない。返してもらうまで読めない」。教科書にそう書いてあったわけではない。私が勝手にそう解釈した。この類推で、所有権の基本は理解できた気がした。コンパイラが怒る理由もわかった。moveが起きる場面も予測できるようになった。私は満足した。「そういうことか」と納得して、次の章に進んだ。しかし、しばらくして困難に直面した。私がやりたかったのは、こういうことだ。本棚に本がある。本を誰かに貸す。貸した本が何かを覚えておきたい。現実世界では当たり前のことだ。これをコードで書こうとした。// 私が書こうとしたコード（コンパイルエラー）struct BookShelf {    books: Vec\u003cString\u003e,    lent_to: Option\u003c\u0026String\u003e,  // 貸した本への参照を持ちたい}Rustでは、所有権を完全に移動させずに、一時的にデータを「見せる」だけの仕組みがある。これを「参照（reference）」や「借用（borrow）」と呼ぶ。\u0026Stringは「Stringへの参照」を意味する。所有権は移動しない。ただ、一時的に覗き見できるだけだ。「本の貸し借り」の類推で考えれば、これは自然なはずだった。本棚には本がある。本を誰かに貸したら、貸した本への参照を持っておく。でも、Rustはこのコードを許さない。error[E0106]: missing lifetime specifier「ライフタイム」。また新しい概念だ。なぜライフタイムが必要なのか。参照は、データの「場所」を覚えている。でも、その場所にあったデータが消えてしまったらどうなるか。参照だけが残って、参照先には何もない。存在しないデータを指す参照。これは危険だ。だから、Rustは参照の「寿命」を追跡する。参照が有効な間は、参照先のデータも存在していなければならない。この寿命を明示するのが、ライフタイムだ。ライフタイムを指定すればいいのか。私は格闘した。// ライフタイムを追加してみるstruct BookShelf\u003c'a\u003e {    books: Vec\u003cString\u003e,    lent_to: Option\u003c\u0026'a String\u003e,}コンパイルは通る。貸し出しもできる。let mut shelf = BookShelf {    books: vec![String::from(\"Rust Book\"), String::from(\"Programming Rust\")],    lent_to: None,};shelf.lent_to = Some(\u0026shelf.books[0]);println!(\"貸し出し中: {:?}\", shelf.lent_to);// =\u003e 貸し出し中: Some(\"Rust Book\")でも、本棚に新しい本を追加しようとすると、地獄が始まる。shelf.books.push(String::from(\"New Book\"));error[E0502]: cannot borrow `shelf.books` as mutable because it is also borrowed as immutable  --\u003e src/main.rs:22:5   |18 |     shelf.lent_to = Some(\u0026shelf.books[0]);   |                           ----------- immutable borrow occurs here...22 |     shelf.books.push(String::from(\"New Book\"));   |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ mutable borrow occurs here23 |     println!(\"新しい本を追加: {:?}\", shelf.books);   |                                      ----------- immutable borrow later used here「本を貸している間は、本棚に新しい本を追加できない」。現実世界ではありえない制約だ。なぜこんなに難しいのか。私は「本の貸し借り」で考え続けた。貸している間も本棚にどの本があるかは覚えている。本棚に新しい本を追加することと、貸した本を追跡することは、まったく独立した操作のはずだ。なのに、なぜRustはそれを許さないのか。長い時間をかけて、やっと気づいた。私の類推が間違っていた。ここで「借用チェッカー（borrow checker）」の話をしなければならない。Rustには、コンパイル時にメモリ安全性を検証する仕組みがある。これが借用チェッカーだ。借用チェッカーの基本ルールはシンプルだ。「参照が有効な間は、参照先のデータを変更してはならない」。なぜこんなルールがあるのか。Vec（可変長配列）の仕組みを考えてみよう。Vecは、内部的には連続したメモリ領域にデータを格納している。本棚でいえば、横一列に並んだ棚だ。最初に5冊分のスペースを確保したとする。6冊目を追加したいとき、どうなるか。今の棚には入らない。だから、より大きな棚を用意して、5冊をすべて移動させる。そして6冊目を追加する。これがVecの動作だ。ここで問題が起きる。移動前の棚の位置を覚えている参照があったとする。本を移動した後、その参照はどこを指すのか。もう本がない場所だ。空っぽの棚を指している。これが「ダングリングポインタ」と呼ばれる危険な状態だ。存在しないデータへの参照。アクセスしたら、何が起きるかわからない。だから、Rustは「参照がある間は変更禁止」というルールを強制する。現実の本の貸し借りには、この問題は存在しない。本棚のサイズを変えても、貸した本が消えることはない。でも、コンピュータのメモリでは、Vecが成長するときにデータが移動する。「本」という類推が、私の理解を助けると同時に、私の理解を歪めていた。正しい設計は、参照ではなくインデックスや識別子を使うことだった。// アプローチ1: インデックスで管理struct BookShelf {    books: Vec\u003cString\u003e,    lent_index: Option\u003cusize\u003e,}let mut shelf = BookShelf {    books: vec![String::from(\"Rust Book\")],    lent_index: None,};shelf.lent_index = Some(0);  // インデックスを記録shelf.books.push(String::from(\"New Book\"));  // これは動く！println!(\"貸し出し中: {:?}\", shelf.books.get(shelf.lent_index.unwrap()));// =\u003e 貸し出し中: Some(\"Rust Book\")// アプローチ2: 所有権を完全に移動struct BookShelf {    books: Vec\u003cString\u003e,}struct LentBook {    book: String,        // 所有権ごと移動    borrower: String,}let mut shelf = BookShelf {    books: vec![String::from(\"Rust Book\"), String::from(\"Programming Rust\")],};let lent = LentBook {    book: shelf.books.remove(0),  // 本棚から取り出す    borrower: String::from(\"Alice\"),};shelf.books.push(String::from(\"New Book\"));  // 本棚は自由に変更できる「本の貸し借り」という類推は、入り口としては正しかった。でも、その類推を引きずりすぎた。Rustにおける「借用」は、現実世界の「貸し借り」とは違う。借用（\u0026T）は「一時的に見せる」だけで、「貸した相手を追跡する」仕組みではない。そして、借用中はデータの変更ができない。この違いに気づくまでに、私は何週間も費やした。類推は、両刃の剣だ。思い返せば、これは初めての失敗ではなかった。非同期処理を学んだときも、同じ罠にはまった。（注：ここからも技術的な話が続きます。コードの詳細は読み飛ばしても、「料理の類推で考えたら、実際の挙動と違った」という話として理解できます。）まず、非同期処理とは何かを説明しておこう。日常生活で考えてみよう。洗濯機を回している間、あなたは洗濯機の前でじっと待っているだろうか。たぶん、その間に別のことをしているはずだ。掃除をしたり、料理を作ったり。洗濯機が終わったら、干しに行く。これが「非同期」の発想だ。プログラムも同じだ。通常のプログラムは、1つの処理が終わるまで次の処理に進めない。ファイルを読み込んでいる間、プログラムは待っている。ネットワークからデータを取得している間も、待っている。これでは効率が悪い。「待っている間に、別のことをやろう」。これが非同期処理だ。私は類推を作り上げた。「非同期処理は、料理を並行して作るようなものか」。パスタを茹でている間にソースを作る。オーブンで肉を焼いている間にサラダを準備する。待ち時間を有効活用して、全体の調理時間を短縮する。この類推で、Rustのasync/await構文の基本は理解できた。async fn cook_dinner() {    let pasta = boil_pasta();      // パスタを茹で始める    let sauce = make_sauce().await; // ソースを作る（待つ）    let pasta = pasta.await;        // パスタが茹で上がるのを待つ    serve(pasta, sauce);}問題は、「共有リソース」にアクセスするコードを書いたときだった。共有リソースとは何か。料理の例で考えよう。キッチンには、コンロが1つしかない。2人の料理人が、同時にそのコンロを使いたいとする。どうなるか。1人が使っている間、もう1人は待つしかない。プログラムでも同じことが起きる。データベース接続、ファイル、あるいはメモリ上のデータ構造。複数の処理が同時に1つのリソースにアクセスしようとすると、混乱が起きる。だから、「Mutex（ミューテックス）」という仕組みで順番を管理する。1つの処理がMutexを「ロック」したら、他の処理はロックが解除されるまで待たなければならない。use std::sync::Arc;use tokio::sync::Mutex;struct Kitchen {    stove: Arc\u003cMutex\u003cStove\u003e\u003e,  // コンロは1つしかない（Mutexで保護）}async fn cook_two_dishes(kitchen: \u0026Kitchen) {    let stove = kitchen.stove.clone();    // 2つの料理を「並行して」作ろうとする    let dish1 = tokio::spawn({        let stove = stove.clone();        async move {            let mut s = stove.lock().await;  // コンロを確保            cook_on_stove(\u0026mut s).await;     // 10分かかる        }    });    let dish2 = tokio::spawn({        let stove = stove.clone();        async move {            let mut s = stove.lock().await;  // コンロを確保しようとする            cook_on_stove(\u0026mut s).await;     // ...が、dish1が終わるまで待つ        }    });    let _ = tokio::join!(dish1, dish2);}私は「並行して料理を作る」と思っていた。2つの料理を同時に調理して、時間を半分にできるはずだと。でも、コンロは1つしかない。片方がコンロを占有している間、もう片方は待っていた。実行してみると、こうなる。[メインコンロ] パスタ の調理を開始[メインコンロ] パスタ の調理が完了[メインコンロ] ソース の調理を開始[メインコンロ] ソース の調理が完了合計調理時間: 4.00秒各料理2秒なら、並行処理で2秒のはずだった。でも、4秒かかった。並行処理の意味がなかった。なぜこうなるのか。現実の料理で考えてみよう。実際のキッチンでは、Aさんがコンロの左側でパスタを茹でている間、Bさんが右側でソースを温められる。コンロには複数の口がある。だから、2人が同時に調理できる。でも、私のコードでは、コンロを「1つのもの」としてMutexで保護していた。「コンロ全体」をロックしていた。だから、1人がコンロを使っている間、もう1人はコンロの前で待つしかなかった。これが「排他制御」の現実だ。Mutexで保護された共有リソースは、一度に1つのタスクしかアクセスできない。「料理を並行して作る」という類推には、この排他制御の概念が含まれていなかった。私の頭の中のキッチンには、コンロの口がいくつもあった。でも、コードの中のキッチンには、コンロが1つしかなかった。より厄介な問題もあった。「デッドロック」だ。デッドロックとは何か。日常の例で説明しよう。AさんとBさんが、食事をしようとしている。テーブルには、ナイフとフォークが1本ずつしかない。食事をするには、両方が必要だ。Aさんは先にナイフを取った。Bさんは先にフォークを取った。Aさんは思う。「フォークがほしい。Bさんが手放すまで待とう」。Bさんも思う。「ナイフがほしい。Aさんが手放すまで待とう」。どちらも、自分が持っているものを手放さない。どちらも、相手が手放すのを待っている。永遠に。これがデッドロックだ。async fn prepare_meal(kitchen: \u0026Kitchen) {    // タスク1: まずコンロを確保、次にオーブンを確保    let task1 = async {        let _stove = kitchen.stove.lock().await;        tokio::time::sleep(Duration::from_millis(10)).await;        let _oven = kitchen.oven.lock().await;  // オーブンを待つ        // ...    };    // タスク2: まずオーブンを確保、次にコンロを確保    let task2 = async {        let _oven = kitchen.oven.lock().await;        tokio::time::sleep(Duration::from_millis(10)).await;        let _stove = kitchen.stove.lock().await;  // コンロを待つ        // ...    };    tokio::join!(task1, task2);  // 永遠に終わらない}タスク1がコンロを持ってオーブンを待ち、タスク2がオーブンを持ってコンロを待つ。お互いが相手を待ち続けて、永遠に進まない。実行してみると、こうなる。[タスク1] コンロを確保しました！[タスク2] オーブンを確保しました！[タスク1] オーブンを確保しようとしています...[タスク2] コンロを確保しようとしています...⚠️  タイムアウト！デッドロックが発生しました。現実の料理では、こんなことは起きない。「ちょっとナイフ貸して」と声をかければ済む。あるいは、「先にフォーク使っていいよ」と譲り合える。人間には、コミュニケーションがある。でも、コンピュータのスレッドは声をかけない。ロックを取得したら、自分の処理が終わるまで手放さない。相手が待っていることすら知らない。だから、永遠に待ち続ける。この問題のデバッグに、私は丸一日を費やした。プログラムが動かない。エラーも出ない。ただ、止まっている。「なぜプログラムが止まるのかわからない」と頭を抱えた。料理の類推では、デッドロックという概念自体が存在しなかったからだ。キッチンで誰かと道具の取り合いになっても、最終的にはどちらかが譲る。でも、プログラムは譲らない。また同じ失敗をしている。私は少し落ち込んだ。でも、まだ終わりではなかった。データベースのトランザクションでも、同じ失敗をした。（注：ここでも技術的な話が続きます。「銀行振込の類推で考えたが、実際のシステムはもっと複雑だった」という話として読んでいただければ大丈夫です。）まず、トランザクションとは何かを説明しよう。日常生活で例えてみる。あなたがコンビニでおにぎりを買うとする。この「買い物」という行為は、2つのことが同時に起きなければ成立しない。「あなたがお金を払う」と「店があなたにおにぎりを渡す」。お金だけ払っておにぎりがもらえなかったら困る。おにぎりだけもらってお金を払わなかったら、それは万引きだ。両方が成功するか、両方が起きないか。どちらかでなければならない。データベースでも同じだ。銀行の振込を考えよう。Aさんの口座から1万円を引いて、Bさんの口座に1万円を足す。この2つの操作は、両方成功するか、両方失敗するか、どちらかでなければならない。Aさんから引いたのにBさんに足されなかったら、1万円が消えてしまう。このような「ひとまとまりの操作」を保証する仕組みがトランザクションだ。途中で失敗したら、最初の状態に戻す（ロールバック）。すべて成功したら、確定する（コミット）。私は類推を作り上げた。「トランザクションは、銀行の振込みたいなものか」。この類推で、データベースの基本的な特性は理解できた。BEGIN TRANSACTION;UPDATE accounts SET balance = balance - 10000 WHERE user_id = 'A';UPDATE accounts SET balance = balance + 10000 WHERE user_id = 'B';COMMIT;問題は、トランザクションが失敗したときの処理を書いたときだった。async fn transfer_money(    pool: \u0026PgPool,    from: \u0026str,    to: \u0026str,    amount: i64,) -\u003e Result\u003c(), Error\u003e {    let mut tx = pool.begin().await?;    // 送金元の残高を減らす    sqlx::query(\"UPDATE accounts SET balance = balance - $1 WHERE user_id = $2\")        .bind(amount)        .bind(from)        .execute(\u0026mut *tx)        .await?;    // 外部APIを呼び出して送金通知を送る（これが問題）    notify_transfer(from, to, amount).await?;    // 送金先の残高を増やす    sqlx::query(\"UPDATE accounts SET balance = balance + $1 WHERE user_id = $2\")        .bind(amount)        .bind(to)        .execute(\u0026mut *tx)        .await?;    tx.commit().await?;    Ok(())}外部APIの呼び出しが失敗したら、トランザクションはロールバックされる。データベースの状態は元に戻る。完璧だと思った。でも、ある日、こんなシナリオを考えた。外部APIの呼び出しが成功した後、2番目のUPDATE文が失敗したらどうなるか。順番を追ってみよう。まず、送金元の残高を減らす。成功。次に、送金通知を送る。成功。通知は、もう相手に届いている。最後に、送金先の残高を増やす。ここで失敗。アカウントが凍結されていた。トランザクションはロールバックされる。データベースの残高は元に戻る。でも、通知は？もう送ってしまった。取り消せない。実際にPostgreSQLで検証してみた。--- シナリオ: 外部API成功後にDB更新が失敗 ---（Alice → frozen_account: 5,000円 - 受取人アカウント凍結で失敗）  → 通知を送信しました（外部API呼び出し）送金失敗: 受取人のアカウントが凍結されています残高:  alice: Alice (90000円)  ← 変わっていない  bob: Bob (60000円)送金通知: 2件  alice → bob: 10000円  alice → frozen_account: 5000円  ← 通知は送信された！トランザクションはロールバックされる。データベースの残高は元に戻る。でも、送金通知はすでに送られている。「5,000円送金しました」という通知が届いているのに、実際には送金されていない。銀行の振込では、こんなことは起きない。なぜか。銀行では、振込処理と通知は同じシステムの中で一貫して管理されている。「お金を動かす」と「通知を送る」が、一体の操作として設計されている。でも、私が書いたコードはそうではなかった。データベースと、通知を送るサービスは、別々のシステムだった。データベースのトランザクションは、データベースの中だけを巻き戻せる。外部サービスへの呼び出しは、トランザクションの外にある。ロールバックしても、すでに送った通知は取り消せない。これが「分散システム」の難しさだ。複数のシステムにまたがる操作を、一貫して管理することは、想像以上に難しい。より厄介な問題もあった。ロールバック自体が失敗することがあるのだ。async fn complex_operation(pool: \u0026PgPool) -\u003e Result\u003c(), Error\u003e {    let mut tx = pool.begin().await?;    // 複数のテーブルを更新    update_table_a(\u0026mut tx).await?;    update_table_b(\u0026mut tx).await?;    update_table_c(\u0026mut tx).await?;  // ここで失敗    tx.commit().await?;    Ok(())}// update_table_c()が失敗すると、txはドロップされてロールバックされる// ...はずだが、ネットワーク障害でロールバックも失敗したら？銀行の振込では、「振込を取り消す」という操作は確実に成功する。窓口で「やっぱりやめます」と言えば、それで終わりだ。でも、コンピュータの世界では、ロールバック自体がネットワーク障害やデータベースクラッシュで失敗することがある。「元に戻す」という操作が、途中で止まる。そうなると、データは中途半端な状態で残る。Aさんから引かれたのに、Bさんには足されていない。1万円が宙に浮いている。この問題に気づいたのは、本番環境で実際に起きてからだった。ユーザーからの問い合わせで発覚した。「送金したのにお金が届いていない」。調べてみると、ネットワーク障害でロールバックが完了していなかった。「銀行の振込みたいなもの」という類推が、分散システムの複雑さを覆い隠していた。銀行の振込は、何十年もかけて作り上げられた堅牢なシステムの上で動いている。私のコードは、そうではなかった。いつになったら学習するのだろう。私は自分に問いかけた。でも、失敗はまだ続いた。キャッシュでも、同じパターンだった。（注：最後の技術的な事例です。「辞書を手元に置いておく類推で考えたが、実際はもっとややこしかった」という話です。）まず、キャッシュとは何かを説明しよう。日常生活で考えてみる。あなたは仕事中、よく使うファイルをどこに置いているだろうか。毎回、会社の書庫まで取りに行くだろうか。たぶん、よく使うファイルは自分の机の上に置いているはずだ。すぐ手に取れるから。これがキャッシュの発想だ。プログラムの世界でも同じだ。データベースからデータを取得するのは、時間がかかる。ネットワーク越しに問い合わせて、データベースが検索して、結果を返す。毎回これをやると遅い。だから、一度取得したデータを「手元」に保存しておいて、次からはそれを使う。これがキャッシュだ。私は類推を作り上げた。「キャッシュは、よく使うものを手元に置いておくことか」。辞書を引くとき、毎回本棚まで行くのは面倒だ。よく使う辞書は、机の上に置いておく。机の上にあれば、すぐに引ける。この類推で、キャッシュの基本は理解できた。use std::collections::HashMap;use std::sync::RwLock;struct UserCache {    cache: RwLock\u003cHashMap\u003cUserId, User\u003e\u003e,}impl UserCache {    async fn get_user(\u0026self, id: UserId, db: \u0026Database) -\u003e User {        // まずキャッシュを確認        if let Some(user) = self.cache.read().unwrap().get(\u0026id) {            return user.clone();        }        // なければDBから取得        let user = db.fetch_user(id).await;        // キャッシュに保存        self.cache.write().unwrap().insert(id, user.clone());        user    }}問題は、データが更新されたときだった。async fn update_user_email(    cache: \u0026UserCache,    db: \u0026Database,    id: UserId,    new_email: String,) -\u003e Result\u003c(), Error\u003e {    // DBを更新    db.update_email(id, \u0026new_email).await?;    // キャッシュを無効化    cache.cache.write().unwrap().remove(\u0026id);    Ok(())}これで十分だと思った。データを更新したら、キャッシュから削除する。次にアクセスしたときは、DBから最新のデータを取得する。シンプルで、正しいはずだった。でも、ある問題が起きた。「競合状態（race condition）」だ。競合状態とは何か。例え話で説明しよう。あなたと同僚が、同時に同じ辞書を使おうとしている。あなたは辞書で「apple」を調べている。その間に、同僚が辞書の「apple」の項目に付箋を貼った。あなたが辞書を閉じて、もう一度開くと、付箋が貼ってある。これは問題ない。でも、こういうケースはどうか。あなたが辞書の「apple」のページをコピーしている間に、同僚が辞書の「apple」の項目を書き換えた。そして、あなたがコピーを終えて、そのコピーを棚にしまった。棚にあるのは、古い情報のコピーだ。これが競合状態だ。複数の処理が同時に動いているとき、その「順番」によって結果が変わってしまう。どの処理が先に終わるかは、そのときの負荷やネットワーク状況で変わる。だから、結果が予測できない。時刻T1: リクエストAがget_user()を呼ぶ時刻T1: リクエストAがキャッシュを確認 → ない時刻T2: リクエストAがDBからuser(email=\"old@example.com\")を取得時刻T3: リクエストBがupdate_user_email()を呼ぶ時刻T3: リクエストBがDBを更新(email=\"new@example.com\")時刻T4: リクエストBがキャッシュを削除時刻T5: リクエストAがキャッシュに古いデータを保存(email=\"old@example.com\")何が起きたのか、順番に見てみよう。リクエストAは、DBから古いデータを取得した。でも、キャッシュに保存する前に、一瞬待たされた。CPUが他の処理をしていたのかもしれない。ネットワークが混んでいたのかもしれない。その隙に、リクエストBがやってきた。リクエストBは、DBのデータを更新した。そして、キャッシュを削除した。「これで、次にアクセスしたときは最新のデータが取得される」と。でも、リクエストAはまだ終わっていなかった。リクエストAは、さっき取得した古いデータを、キャッシュに保存した。リクエストBが削除した後のキャッシュに。結果、キャッシュには古いデータが入った。DBには新しいデータがある。キャッシュとDBで、データが食い違っている。実行してみると、こうなる。[T1] リクエストA: get_user()開始  [キャッシュ] ミス[T2] リクエストA: DBから取得中...  [DB] 取得完了: email=\"old@example.com\"[T3] リクエストB: update_user_email()開始  [DB] メール更新: old@example.com -\u003e new@example.com[T4] リクエストB: キャッシュ無効化[T5] リクエストA: キャッシュに保存  [キャッシュ] 保存: email=\"old@example.com\" ← 古いデータ！--- 結果確認 ---DBの値:        email=Some(\"new@example.com\")キャッシュの値: email=Some(\"old@example.com\")⚠️  キャッシュに古いデータが残っている！結果、キャッシュには古いデータが残り続ける。現実世界の辞書では、こんなことは起きない。なぜか。辞書の内容は、めったに変わらない。そして、辞書を使うのは通常1人だ。複数人が同時に同じ辞書を書き換えながら参照することは、まずない。でも、コンピュータのデータは違う。複数のプロセスが、同時に、同じデータを読み書きする。しかも、ネットワーク遅延やCPUスケジューリングで、処理の順序が予測できない。「Aが先に終わるはず」と思っても、実際にはBが先に終わることがある。この問題をデバッグするのに、3日かかった。「たまにデータが古いままになる」という報告を受けて、最初はDBの問題だと思った。DBを調べた。問題なかった。次にキャッシュの設定を調べた。問題なかった。ログを細かく分析して、やっと気づいた。タイミングの問題だった。特定の順番で処理が実行されたときだけ、問題が起きていた。「手元に置いておく」という類推は、キャッシュの無効化タイミングの複雑さを完全に見落としていた。机の上の辞書は、勝手に内容が変わらない。でも、キャッシュの中のデータは、いつ古くなるかわからない。Phil Karltonの有名な言葉がある。「コンピュータサイエンスで難しいことは2つしかない。キャッシュの無効化と、名前付けだ」。この言葉の意味を、私は身をもって理解した。どれも、類推としては間違っていない。でも、類推が示す以上のことを、私は類推から読み取ってしまっていた。類推は、理解を助ける。しかし、誤解も生む。類推は、新しい視点を与える。一方で、本質を見えなくもする。類推は、創造の源泉だ。同時に、思考停止の入り口でもある。これらの経験以来、私は類推について考え続けてきた。エンジニアとして、類推をどう使い分けるべきか。いつ類推すべきで、いつ類推を断つべきか。類推の力を活かしながら、その罠に落ちないためには、何が必要なのか。そして、もう1つ気づいたことがある。類推は、単なる思考ツールではない。それは、人間の知能の根幹だ。 われわれは、あまりにも無意識に類推的な考え方をしながら日々を過ごしている。だからこそ、類推の限界を知ることが、これほど重要なのだ。これは、類推に救われてきた人間が、類推に何度も裏切られた話だ。そして、それでもなお類推を手放せない人間が、類推とどう向き合うかを考えた記録だ。類推とは何かまず、類推とは何かを明確にしておきたい。類推（アナロジー）とは、2つの異なる領域の間に構造的な類似性を見出し、一方の知識を他方に適用する思考法だ。AとBは表面的には違うが、その関係性の構造は似ている。だから、Aで学んだことを、Bに応用できる。私は、類推こそが人間の思考の根幹だと考えている。論理的思考も、批判的思考も、創造的思考も、よく見ると類推が基盤にある。われわれは類推なしには、新しいことを考えることすらできない。ソフトウェアエンジニアなんて、類推だらけだ。コードを読んでいると、「あ、これ、あのコードと同じ構造だな」と気づく。設計を考えていると、「前のプロジェクトのあのパターンが使えそうだ」と気づく。バグを追っていると、「この挙動、前にも見たことがある」とピンとくる。私たちは、毎日、無意識に類推している。自分でも気づかないうちに。プログラミングを学ぶとき、類推を使っている。「変数は、ラベル付きの箱みたいなものだ」と教わる。値を入れて、取り出す。この類推があるから、抽象的な概念を具体的にイメージできる。新しいデータベースを学ぶとき、類推を使っている。「PostgreSQLのMVCCは、MySQLのInnoDBと似ているか」と考える。この類推があるから、ゼロから学ぶより速く理解できる。新しい言語を学ぶとき、類推を使っている。「Rustのtraitは、Goのinterfaceみたいなものか」と考える。完全に同じではないが、入り口にはなる。われわれの頭の中では、常に類推が働いている。既知の世界での関係づけから、未知の関係づけを推論している。物語を読むときも、私たちは類推している。登場人物の経験を自分の人生に重ね、フィクションの世界から現実への教訓を引き出す。主人公が困難を乗り越える姿を見て、自分の状況に当てはめる。異なる時代や文化を舞台にした物語から、普遍的な人間の営みを感じ取る。共感とは、つまり類推だ。「この人の気持ちは、あのときの自分の気持ちに似ている」。そう感じるから、私たちは物語に心を動かされる。類推がなければ、われわれは毎回ゼロから学ばなければならない。新しいフレームワークに出会うたび、過去の経験が役に立たない。累積的な学習ができない。技術も発展しない。だから、類推は人間の知能の基盤であり、思考の源泉だ。 これは疑いようがない。ここまで書いてきて、ふと気づいたことがある。私は今、類推について説明するために、言葉を使っている。では、言葉を使うとは、どういうことだろうか。目の前に、一冊の本がある。私はそれを見て、「本」と呼ぶ。でも、この「本」という言葉は、どこから来たのか。私がこれまでの人生で見てきた、無数の本。図書館で借りた本、書店で買った本、友人にもらった本。それらに共通する何かを抽出して、「本」というカテゴリを作った。目の前の物体を「本」と呼ぶとき、私はそれを、過去に見てきた本たちと「同じ仲間」だと判断している。これは、類推ではないか。「この物体は、私が知っている『本』に似ている。だから、これも『本』だ」。言葉を使うとは、目の前の具体的な現象を、過去に学んだカテゴリに当てはめることだ。当てはめるためには、類似性を見出さなければならない。つまり、言語化そのものが、類推なのだ。そう考えると、言葉の限界も見えてくる。目の前の本には、固有の特徴がある。紙の質感。インクの匂い。背表紙についた小さな傷。誰かが残した付箋。でも、「本」という言葉は、それらを捉えない。「本」という言葉が指すのは、無数の本に共通する抽象的な特徴だけだ。言葉にした瞬間、具体的な豊かさは零れ落ちる。だから、現状のすべてを完璧に表す言葉は、存在しない。 どんなに言葉を尽くしても、現実には追いつかない。言葉は常に近似だ。現実の一部を切り取っているだけだ。新しい経験をしたとき、私たちは「これは何だろう」と考える。既存の語彙の中から、「これに近い」言葉を探す。ぴったりの言葉が見つからなければ、複数の言葉を組み合わせる。それでも足りなければ、比喩を使う。「○○みたいなもの」と。でも、どれだけ工夫しても、言葉は現実を完全には捉えられない。類推は「AはBに似ている」という認識だ。言語化は「この現象は『X』という言葉に似ている」という認識だ。構造は同じだ。どちらも、目の前のものを、既知のものに当てはめる。そして、当てはめることで、何かを得る代わりに、何かを失う。私たちは、類推なしには思考できない。言葉なしには思考を伝えられない。でも、類推も言葉も、現実を完全には捉えられない。この記事を書いている今この瞬間も、私は類推と言葉の限界の中にいる。その限界を知りながら、それでも書くしかない。だからこそ、類推の限界を知ることが、これほど重要なのだ。しかし、だからこそ危険なのだ。類推はなぜ強力なのか類推の力を、もう少し詳しく見てみよう。抽象と具体の往復運動抽象的な概念は、そのままでは理解しにくい。人間の脳は、具体的なイメージを好む。抽象的な数学の公式より、具体的な例題の方が理解しやすい。抽象的な設計原則より、具体的なコード例の方が頭に入る。類推は、この抽象と具体を往復する運動だ。日常の例で説明しよう。カレーを作れる人は、シチューも作れる。なぜか。カレーとシチューは、表面的には違う料理だ。でも、「材料を切る → 炒める → 水を入れて煮る → ルーを溶かす」という構造は同じだ。カレーを作った経験から、この「構造」を抽出できれば、シチューに応用できる。これが抽象化であり、類推だ。プログラミングでも同じだ。具体的なもの（MySQL）を見て、抽象化（データを永続化するシステム）し、別の具体（PostgreSQL）に適用する。この往復が、類推の本質だ。ここで重要なのは、「抽象化」という能力だ。私の理解では、抽象化とは枝葉を切り捨てて幹を見ることだ。個別の事象から、本質的な構造だけを取り出す。MySQL、PostgreSQL、SQLiteはいずれも「SQLでデータを操作するシステム」という抽象に還元できる。Actix-web、Axum、Rocketはいずれも「HTTPリクエストを処理するRustのWebフレームワーク」という抽象に還元できる。この抽象化ができなければ、類推はできない。類推とは、2つの具体的な事象の間に共通の構造を見出すことだ。共通の構造を見出すには、まず具体から構造を抽出しなければならない。それが抽象化だ。私がこれまで見てきた限り、類推がうまい人は例外なく抽象化がうまい。 正しく抽象化できなければ、正しく類推できない。面白いことに、抽象の世界が見えている人には具体の世界も見える。でも、具体しか見えない人には抽象の世界が見えない。私はこれをマジックミラーのようなものだと思っている。抽象側からは両方見えるが、具体側からは向こう側が見えない。抽象を理解している人は、具体がその抽象の一例であることがわかる。「あ、これは○○の具体例だな」と。一方、具体しか見えない人は、それが何かの一例だとは気づかない。ただ、個別の事象として見るだけだ。だから、別の具体との共通点が見えない。多くの人は、この具体と抽象の往復運動を意識したことすらない。私自身、エンジニアになって何年も経ってから、やっと意識できるようになった。それまでは、類推を「なんとなく」やっていた。うまくいくこともあれば、失敗することもあった。でも、なぜ失敗するのかがわからなかった。抽象化を意識するようになってから、類推の成功率が上がった。「依存性の注入（DI）とは何か」。これはプログラムの設計手法の一つで、名前だけ聞くと難しそうに感じる。これを抽象的に説明すると、「オブジェクトが必要とする依存関係を外部から注入することで、結合度を下げてテスタビリティを高める設計パターン」となる。正確だが、初学者には意味不明だ。でも、「コンセントみたいなものだよ」と言えば、少し見えてくる。家電製品は、壁のコンセントに何が繋がっているか知らなくても動く。発電所が火力でも原子力でも太陽光でも、同じコンセントから電気が来る。DIも同じで、クラスは「何か」からデータベース接続を受け取るが、それが本番のMySQLなのかテスト用のモックなのかは知らなくていい。外部から「注入」される。類推によって、抽象が具体になる。見えなかったものが、見えるようになる。未知への橋渡し人間は、完全に未知のものを理解できない。新しい概念を学ぶとき、われわれは常に既知のものと関連づける。「これは、あれに似ている」。この関連づけがなければ、新しい知識は宙に浮いてしまう。既存の知識ネットワークに接続できない。類推は、未知と既知をつなぐ橋だ。Kubernetesを初めて学ぶとする。Kubernetesとは、たくさんのアプリケーションを複数のサーバーで効率よく動かすための管理システムだ。まったく新しい概念だ。でも、「Kubernetesは、コンテナのオーケストラ指揮者みたいなものだ。各コンテナ（アプリケーションを動かす小さな箱）がどこで動くべきか、いくつ動かすべきか、死んだら再起動すべきかを指示する」という類推があれば、入り口が見える。もちろん、この類推は不完全だ。Kubernetesの本質——宣言的な状態管理、コントロールループ、リコンシリエーション——を完全には捉えていない。でも、入り口にはなる。そこから、より正確な理解に進むことができる。類推は、足場だ。 建設現場の足場のように、本体を作るための仮の構造物だ。足場がなければ、高い建物は建てられない。類推がなければ、深い理解には到達できない。遠くから借りてくる力類推は、新しいアイデアを生む。異なる領域を結びつけることで、どちらの領域にも存在しなかった新しい視点が生まれる。ここで重要なのは、「どこから借りてくるか」だ。興味深いのは、同じ業界から持ってくるとパクりと言われるのに、違う業界からなら革命になることだ。なぜか。同じ業界の人は、同じものを見ている。だから、借りてきたことがすぐにバレる。でも、違う業界から借りてくると、誰も気づかない。そもそも、その業界を知らないからだ。他人が気づかないような遠くから借りてくる。そのために必要なのが、抽象化の力だ。遠い領域同士をつなげるには、それぞれの領域から本質的な構造を抽出しなければならない。表面的な違いを超えて、構造の類似を見抜く。これができる人だけが、革命を起こせる。生物の進化から、遺伝的アルゴリズムが生まれた。「自然選択と突然変異のプロセスを、最適化問題に適用したらどうだろう」。この類推が、新しい計算手法を生んだ。神経細胞のネットワークから、ニューラルネットワークが生まれた。「脳の情報処理を、コンピュータで模倣したらどうだろう」。この類推が、現在のAI革命の基盤を作った。私は、類推を創造の触媒だと思っている。異なる領域の知識を化学反応させて、新しいものを生む。遠くから借りてくるほど、その化学反応は激しくなる。近い領域から借りてくると、小さな改善にしかならない。遠い領域から借りてくると、パラダイムシフトが起きる。コミュニケーションの潤滑油類推は、相手にとって未知の概念を、既知の概念で説明することを可能にする。エンジニア同士でも、専門領域が違えば類推は有効だ。フロントエンドエンジニアにバックエンドの認証を説明するとき、JWT（JSON Web Token、ユーザーの認証情報を暗号化して持ち運ぶ仕組み）の説明をする機会がある。「JWTは、入場チケットみたいなものだよ」と言えば伝わる。一度発行されたら、チケット自体に情報が書いてある。だから毎回本部に問い合わせなくても、チケットを見せるだけで入れる。データベースのインデックス（データを高速に検索するための目次）を説明するときも同じだ。「本の索引みたいなものだよ。全ページをめくらなくても、索引を見れば目的の単語がどこにあるかすぐわかる」。チーム内でも類推は重要だ。リファクタリングとは、プログラムの動作を変えずに、コードの構造を整理・改善することだ。「このリファクタリングは、引っ越しみたいなものだ。荷物を新しい場所に移して、古い場所を片付ける。移行期間中は、両方にアクセスできるようにしておく」。こう言えば、作業のイメージが共有できる。類推は、異なる背景を持つ人々の間で、共通の理解を作る。類推はなぜ危険なのかここまで読むと、類推は素晴らしいものに思える。実際、素晴らしいのだ。でも、同時に危険でもある。なぜか。類推は「AとBは似ている」という前提に立っている。でも、この前提が正しいとは限らない。 似ているように見えて、実は違う。その違いが、致命的な判断ミスを生む。これは、ベストプラクティスが常に機能しないのと同じ構造だ。カンファレンスやブログで見たあの手法、あの技術、あの設計。「あの会社でうまくいったから、うちでもうまくいくはずだ」。こう考える。でも、これは類推だ。あの会社の文脈と、あなたの文脈は違う。あのチームと、あなたのチームは違う。ベストプラクティスが「ベスト」なのは、特定の文脈においてだけだ。 文脈が変われば、ベストではなくなる。デザインパターン（プログラム設計でよく使われる定番の解決策のカタログ）も同じだ。「このケースにはあのパターンが使える」と考える。でも、そのパターンが生まれた文脈と、今の文脈は違う。パターンを適用すれば解決するわけではない。パターンは出発点であって、答えではない。私が「何回説明しても伝わらない」と感じるとき、原因の多くは類推にある。類推は理解のショートカットとして強力だ。でも、相手と自分の「当たり前」が違うと、誤解を生む。なぜなら、類推は相手の頭の中にある既存の枠組みに接続するからだ。その枠組みが私と違えば、同じ言葉でも違う意味になる。冒頭の所有権の話を思い出してほしい。私は所有権を「本の貸し借りみたいなもの」と理解した。でも、「貸し借り」という言葉には、私が意識していなかった意味も含まれていた。「貸した相手との関係が続く」という意味だ。私は無意識にその意味も読み取っていた。だから、所有権を渡した後も「貸した先」を追跡できると思い込んでいた。類推が、私の思考を歪めていた。表面的類似と構造的類似の混同では、なぜ類推は失敗するのか。多くの場合、表面的な類似と構造的な類似を混同しているからだ。表面的な類似とは、見た目や印象の類似だ。「両方とも丸い」「両方とも赤い」「両方とも動く」。これは、誰でもすぐに気づく。構造的な類似とは、関係性のパターンの類似だ。「Aの中でXとYがこういう関係にあるのと同じように、Bの中でPとQもこういう関係にある」。これは、注意深く見ないと気づかない。類推が成立するためには、構造的な類似が必要だ。表面的な類似だけでは足りない。 問題は、人間が表面的な類似に騙されやすいことだ。見た目が似ていると、構造も似ていると思い込んでしまう。あるチームの話を聞いた。少し用語を説明しておこう。「モノリス」とは、1つの大きなプログラムとして構築されたシステムだ。「マイクロサービス」とは、機能ごとに小さなプログラムに分割し、それらを連携させるアーキテクチャだ。大企業が採用して成功したことで有名になった。そのチームは「マイクロサービスが成功しているから」という理由で、モノリスをマイクロサービスに分割しようとした。「あの有名企業がうまくいったんだから、うちもうまくいくはずだ」。表面的には似ている。「複雑なシステムを小さなサービスに分割する」という点で。しかし、構造は根本的に異なる。その有名企業には数千人のエンジニアがいる。専門のプラットフォームチームがいる。成熟した監視基盤がある。一方、そのチームは10人だった。運用の負荷が爆発的に増え、サービス間の通信障害のデバッグに追われ、結局モノリスに戻すことになった。彼らは、表面的な類似に騙されて、1年を失った。類推が思考を固定する類推には、もう1つ危険がある。思考を固定してしまうことだ。類推は、新しい視点を与える。「これはAみたいなものだ」と気づくと、Aの知識が使えるようになる。これは便利だ。でも同時に、Aの枠組みで考えるようになる。Aの論理で判断するようになる。Aで成立したことは、ここでも成立すると期待するようになる。ここに罠がある。BはAではない。Aにはない特性が、Bにはある。Bにはない特性が、Aにはある。類推によってAの枠組みを持ち込むと、Bの固有性が見えなくなる。Aとの共通点ばかりに目が行き、Aとの違いを見落とす。私はかつて、新しいチームのマネジメントで失敗した。前のチームで成功した方法を、そのまま適用しようとした。「前のチームと同じようにやればいい」と類推した。でも、チームが違えば、人が違う。カルチャーが違う。技術スタックが違う。ビジネスの文脈が違う。前のチームでうまくいった方法が、新しいチームでは逆効果だった。類推によって、私は新しいチームの固有性を見落としていた。「前のチームみたい」という枠組みが、目の前のチームを正確に見ることを妨げていた。これは、私だけの話ではない。世の中の「二番煎じ」は、すべてこの構造だ。表面的な成功パターンを真似る。でも、本質的な差異を見落としている。だから、同じ結果が得られない。独自性がないのではない。観察が浅いだけだ。類推が、観察を浅くしている。 成功事例を見て「うちも同じようにやろう」と考えるとき、私たちは無意識に類推している。でも、その類推が正しいかどうかを検証していない。表面的な類似に飛びついて、構造的な違いを無視している。類推は状況証拠であって物的証拠ではないここまでの話をまとめると、こうなる。類推は仮説であって、証明ではない。類推は、2つの領域の間に構造的な類似があるという仮定に基づいている。「AとBは似ているから、Aで成り立つことはBでも成り立つだろう」。これが類推の論理だ。でも、この仮定は、常に正しいとは限らない。似ているように見えて、実は違う。類推は状況証拠レベルであって、物的証拠レベルには至らない。ある領域で成功した法則が、別の領域でも通用する保証は、どこにもない。成功事例は、その文脈での成功を証明するだけだ。別の文脈での成功は、証明されていない。カンファレンスやブログで聞いた、あの会社の組織文化。あの会社でうまくいったからといって、すべての会社で同じ文化がうまくいくわけではない。あの有名な開発手法が成功したからといって、すべてのチームで同じ手法が成功するわけではない。成功事例から学ぶことは重要だ。でも、「あの会社みたいにやればいい」と単純に類推することは、危険だ。あの会社には、あの会社の文脈がある。業界。競合。人材市場。創業者の思想。歴史。規模。成長フェーズ。これらすべてが、あの文化を成立させている。あなたの会社には、あなたの会社の文脈がある。同じ文化を移植しても、機能するとは限らない。むしろ、害になることもある。より危険なのは、まったく新しい概念や技術を既存のものに無理やり当てはめることだ。ブロックチェーン（暗号技術を使って取引記録を改ざん困難な形で保存する技術）を「分散データベースみたいなもの」と類推すると、その本質的な違いを見落とす。信頼モデル（誰を信頼するか）、コンセンサスメカニズム（参加者間でどうやって合意を取るか）、イミュータビリティ（一度記録したら変更できないこと）——これらの特徴が、通常のデータベースとは根本的に異なる。結果的に間違った理解や過小評価につながる。類推を絶対視してはいけない。類推は仮説であって、証明ではない。類推がもたらす知的興奮ここまで、類推の危険性について書いてきた。でも、誤解しないでほしい。類推は危険だからといって、避けるべきものではない。類推には、代えがたい価値がある。類推は楽しい。類推は気持ちいい。 私は、類推が成功した瞬間の快感を、何度も味わってきた。まったく別のことに当てはまった時、頭の中で何かがつながる。あの瞬間——「あ、これって、あれと同じ構造だ」と気づく瞬間——には、独特の快感がある。世界の見え方がガラリと変わる。さっきまでバラバラだったものが、1つの構造で説明できるようになる。混沌が秩序になる。複雑が単純になる。なぜ、これが気持ちいいのか。人間は、わからないことに不安を感じる。新しい状況。未知の概念。複雑な問題。これらは、ストレスだ。脳は「これは何だ？」「どうすればいい？」と警戒モードに入る。でも、類推によって「あ、これは前に見たあれと同じだ」と気づくと、状況が一変する。未知が既知になる。複雑が単純になる。警戒モードが解除される。その瞬間、安堵とともに、快感が走る。これは、たぶん生存本能と関係している。予測できないものは危険だ。草むらで何かが動いた。あれは風か、それとも獲物か、それとも敵か。わからないと、逃げるべきか近づくべきか判断できない。でも、「あれは風だ」とわかれば、安心できる。予測できるものは安全だ。類推によって「これは、あれと同じだ」とわかると、予測ができるようになる。「あれ」のときはこうなった。だから、「これ」もそうなるだろう。予測ができると、安心する。安心は快感だ。しかも、類推は「遠くから借りてくる」ほど快感が大きい。 近い領域の類推——「MySQLはPostgreSQLに似ている」——は、驚きが少ない。当たり前だからだ。でも、遠い領域の類推——「ソフトウェアのリファクタリングは、文章の推敲と同じ構造だ」——は、発見の喜びが大きい。予想外のつながりだからだ。予想外であるほど、「わかった」瞬間のギャップが大きい。だから、快感も大きい。私がコードを書いていて、まったく関係ないはずの日常の出来事が当てはまることに気づいたとき。障害対応をしていて、これは以前経験した別の問題と同じ構造だと気づいたとき。設計を考えていて、過去に読んだ本の概念が使えると気づいたとき。そのたびに、ゾクっとする。「まさか、ここがつながるとは」という驚き。でも、よく考えると「なるほど、確かに同じだ」と納得できる。この驚きと納得の組み合わせが、最高に気持ちいい。類推の快感は、謎解きの快感に似ている。 バラバラだったピースが、カチッとはまる。見えなかった絵が、見えるようになる。あの瞬間の快感を知っている人は、類推をやめられない。日本では、この類推の喜びは昔から庶民の間で楽しまれていた。「○○と掛けて□□と解く。その心は△△である」という謎かけだ。まったく関係なさそうな2つのものが、ある抽象的な構造で結びつく。その発見の喜びが、笑いになる。漫才のツッコミも、類推と関係がある。ボケは、ある種の「間違った類推」だ。常識から逸脱したことを言う。ツッコミは、その逸脱を指摘する。「いや、それは違うやろ」と。ツッコミが面白いのは、観客が「そうそう、それはおかしいよね」と共感できるからだ。観客の頭の中にある「普通はこうだ」という枠組み——フレームと呼ぼう——に沿って、逸脱を指摘する。だから笑いが起きる。これは、類推の逆操作だ。類推が「AはBみたいなものだ」と結びつけるのに対して、ツッコミは「AはBではない」と切り離す。類推の破綻を、観客のフレームに沿って指摘する。ここで重要なのは、ツッコミが機能するためには、観客のフレームを理解していなければならないということだ。観客が「それはおかしい」と感じるポイントを、正確に捉えなければならない。これは、類推を使うすべての場面に通じる。私が所有権を「本の貸し借りみたいなもの」と理解したとき、私は「貸し借り」というフレームの中で考えていた。そのフレームの中では、貸し借りには「誰に貸したか」という追跡可能な関係が含まれていた。私は、そのフレームが当たり前だと思っていた。フレームの存在自体を意識していなかった。だから、フレームの限界が見えなかった。自分自身に対する「ツッコミ」——「いや、Rustの借用は、現実の貸し借りとは違うやろ」——ができなかった。自分がどんなフレームで類推しているかを意識しなければ、類推の限界が見えない。良い学習者は、類推を使うと同時に、自分自身でツッコミを入れる。「本の貸し借りみたいなものだけど、貸し借りと違って……」と。このツッコミができるかどうかが、類推で成功する人と失敗する人を分ける。創造性は異領域からの借用ソフトウェアエンジニアリングの歴史は、異なる領域からの借用の歴史でもある。Gitの分散型バージョン管理は、中央集権的なSVNの限界を、分散システムの発想で打破した。Git とは、プログラムの変更履歴を記録・管理するツールだ。SVNは「中央のサーバーにすべてを保存する」方式だったが、Gitは「全員が完全な履歴を持つ」方式を採用した。「すべてのリポジトリが対等なピアである」という考え方は、P2Pネットワークの構造と同じだ。Dockerのコンテナ技術は、仮想マシンの重さを、プロセス分離の軽さで置き換えた。Dockerとは、アプリケーションを「コンテナ」という小さな箱に詰めて、どこでも同じように動かせるツールだ。「OSレベルの仮想化ではなく、プロセスレベルの分離で十分ではないか」という発想が、コンテナ革命を起こした。MapReduceは、分散処理の複雑さを、関数型プログラミングの抽象で単純化した。これは大量のデータを複数のコンピュータで並列処理するための手法だ。「mapとreduceという2つの操作に分解すれば、並列処理が簡単になる」。この類推が、ビッグデータ処理の基盤を作った。類推は、新しい価値を生むための道具だ。既存の枠組みを超えるための、ジャンプ台だ。だから、類推を完全に否定できない。エンジニアリングにおける類推の両面性ここまで、類推の力と危険について見てきた。類推は強力だ。でも、危険でもある。では、エンジニアとして、類推をどう扱うべきか。答えは、場面によって使い分けることだ。 類推が有効な場面と、危険な場面がある。それを見極めることが重要だ。類推が有効な場面まず、類推が有効な場面を整理しよう。新しい技術を学ぶとき。 前に学んだ技術との類似点を見つけることで、学習が加速する。「Goのgoroutine（ゴルーチン）は、軽量なスレッドみたいなものか」。スレッドとは、プログラムの中で同時に動く処理の単位だ。goroutineはそれをより少ないメモリで実現する。この類推が、入り口になる。チームメンバーに説明するとき。 相手が知っている概念に置き換えることで、理解を助ける。「このアーキテクチャは、マイクロサービスというより、モジュラーモノリスに近いよ」。問題を発見するとき。 「これは前にやったあのプロジェクトに似ている」と気づくことで、早期に問題を予測できる。パターン認識だ。アイデアを発想するとき。 異なる領域の解決策を、目の前の問題に適用してみる。「他の業界ではどうやっているんだろう」。これらの場面では、類推は強力なツールだ。類推が危険な場面一方で、類推が危険な場面もある。共通点は、「判断」が伴う場面だ。設計判断を下すとき。 「あの有名な会社がこうやっているから」は、判断の根拠にならない。なぜか。あの会社にはあの会社の文脈がある。規模、チーム構成、ビジネス要件、技術的制約——すべてが違う。自分たちの文脈で、自分たちの制約を考慮して、判断しなければならない。パフォーマンス予測をするとき。 「前のプロジェクトではこのくらいのスループットだったから」は、予測の根拠にならない。ハードウェアが違う。データが違う。負荷パターンが違う。実測なしに類推で判断すると、本番環境で痛い目に遭う。チーム運営をするとき。 「前のチームではうまくいったから」は、根拠にならない。人が違う。状況が違う。目の前のチームを、目の前のチームとして見なければならない。ビジネス判断をするとき。 「あの会社がこうやって成功したから」は、根拠にならない。市場が違う。タイミングが違う。リソースが違う。「マイクロサービスが流行っているから、うちもマイクロサービスにしよう」。これも類推だ。でも、マイクロサービスが成功した会社と、あなたの会社は違う。チームの規模が違う。運用能力が違う。ビジネスの複雑さが違う。流行りのアーキテクチャは、流行っている理由があるが、あなたの問題を解決する保証はない。「TDD（テスト駆動開発：テストを先に書いてから本体コードを書く開発手法）がいいらしいから、TDDでやろう」。これも類推だ。TDDが有効だった文脈と、今の文脈は同じか。チームのスキルは。締め切りは。要件の安定度は。手法は、文脈とセットでしか評価できない。これらの場面では、類推に頼らず、具体を見なければならない。具体を見ろここまでの話から、私がたどり着いた結論はシンプルだ。類推は入り口として使う。でも、入ったら、具体を見る。どういうことか。「これはAみたいなものだ」と類推したら、まずはその類推で全体像を掴む。ここまでは類推の力だ。でも、判断を下す前に、次の問いを立てる。「Aとは何が違うんだろう」。違いを具体的に列挙する。その違いが、判断にどう影響するかを考える。つまり、抽象ではなく、具体を見る。パターンではなく、個別を見る。類似ではなく、差異を見る。これは、類推の否定ではない。類推の限界を知った上で、類推を使うということだ。類推は入り口として使い、判断は具体に基づいて行う。入り口と判断を、分離する。 これが、私の結論だ。類推を使い分ける技術「入り口と判断を分離する」と言った。では、具体的にどうすればいいのか。私が実践していることを、いくつか紹介する。類推のレベルを意識するまず、自分がどのレベルで類推しているかを意識することだ。類推には、レベルがある。表面的な類推：見た目や印象の類似。「両方とも丸い」「両方ともウェブサービスだ」。機能的な類推：役割や機能の類似。「両方ともユーザー認証する」「両方ともデータを永続化する」。構造的な類推：関係性のパターンの類似。「Aの中でXとYの関係が、Bの中でPとQの関係と同じだ」。原理的な類推：根底にある原理の類似。「両方とも、この物理法則に従う」「両方とも、この経済原理が働く」。レベルが深いほど、類推は有効だ。表面的な類推は危険だ。原理的な類推は強力だ。類推をするとき、自分がどのレベルで類推しているかを意識する。表面的な類推に気づいたら、警戒する。反例を積極的に探す類推が成立しない場面を、積極的に探す。「これはAみたいだ」と思ったら、「Aとは違う点は何か」を列挙する。「この類推が成立しない条件は何か」を考える。「Aでは成立したが、ここでは成立しないことは何か」を洗い出す。なぜ反例を探すのか。人間は、類推が成立する証拠ばかりを集める傾向がある。心理学では「確証バイアス」と呼ばれる現象だ。自分が信じたいことを裏付ける情報ばかりを無意識に集めてしまう。「似ている」と感じると、似ている点ばかり目につく。違う点は、無意識にスルーしてしまう。だから、意識的に反例を探さなければならない。反例は、自然には目に入ってこない。反例が見つかったら、類推の適用範囲を限定する。「この側面ではAに似ているが、この側面では違う」と認識する。反例を探すことは、類推を否定することではない。類推を精密にすることだ。どこまで使えて、どこから使えないのか。その境界線を引く作業だ。類推と実測を組み合わせる類推は仮説だ。仮説は検証しなければならない。私は何度も、類推を信じて痛い目を見てきた。「分かった」と思った瞬間が、一番危ない。類推は、分からないことを「分かったつもり」にさせてくれる。その確信が、検証を怠らせる。大切なのは、分かっていないことに確信を持たないことだ。類推で「たぶんこうだろう」と思っても、それは仮説でしかない。仮説に確信を持ってはいけない。確信を持った瞬間、検証しなくなる。検証しなければ、間違いに気づけない。だから、私は自分にこう言い聞かせている。類推したら、試せ。作ってみろ。動かしてみろ。「前のプロジェクトと同じくらいのパフォーマンスだろう」と類推したら、実測する。「このアーキテクチャパターンがうまくいくだろう」と類推したら、プロトタイプ（動作確認のための試作品）を作る。「このチーム運営方法が有効だろう」と類推したら、小さく試して観察する。試した結果、類推が外れることがある。むしろ、外れることの方が多い。でも、外れたときこそ、学びがある。なぜ外れたのか。どこが似ていて、どこが違ったのか。その差異を言語化できたとき、理解が一段深まる。私は、このサイクルを速く回すことを意識している。1回の大きな検証より、10回の小さな検証。外れることを恐れない。外れるたびに、類推が精密になっていく。類推は「似ている」という感覚に基づいている。でも、感覚は当てにならない。似ていると思っても、実際には違う。逆に、違うと思っても、実際には同じ。感覚を信じすぎると、現実を見誤る。実測は、感覚を現実に引き戻す。「本当にそうなのか？」を確認する。類推で仮説を立てて、実測で検証する。 類推は仮説生成の道具であって、証明の道具ではない。複数の類推を比較する1つの類推に固執しない。複数の類推を試す。「これはAみたいだ」と思ったら、「でも、Bみたいでもあるな」と考える。「Cという見方もできるな」と広げる。なぜ複数の類推を試すのか。最初に思いついた類推が、最適とは限らない。むしろ、最初の類推は表面的なことが多い。パッと見て似ているから、思いつく。でも、もう少し考えると、別の類推の方が本質を捉えていることがある。1つの類推に決め打ちすると、その視点でしか見えなくなる。複数の類推を並べると、それぞれの限界が見えてくる。そして、どの類推が最も適切かを吟味する。どの類推が、最も多くの側面を説明できるか。どの類推が、最も少ない反例を持つか。どの類推が、最も有用な洞察を与えるか。複数の類推を比較することで、1つの類推に囚われることを防ぐ。類推を言語化する類推を曖昧なまま使わない。明示的に言語化する。「これはAみたいだ」と思ったら、何がどうAに似ているのか、具体的に言葉にする。「Aのこの側面と、ここのこの側面が、この点で類似している」と。なぜ言語化が重要なのか。頭の中にある類推は、たいてい曖昧だ。「なんとなく似ている」という感覚で止まっている。でも、言葉にしようとすると、曖昧さが露呈する。「どこが似ているの？」と聞かれて、答えられない。言語化は、自分の思考を試すテストだ。 言葉にできないなら、実はわかっていない。言葉にできて初めて、本当に理解したと言える。言語化することで、類推が精密になる。曖昧な類推は、誤解を生む。精密な類推は、理解を深める。そして、言語化した類推を、他者に共有する。「私はこう類推しているが、どうだろうか」と問う。他者の視点で、類推の妥当性を検証する。類推力を鍛えるここまで、類推の力と限界について語ってきた。では、類推力を高めるには、どうすればいいのか。私が意識していることを3つ挙げる。1つ目は、遠い領域から引き出しを増やすことだ。私は、咀嚼しやすいものばかり読まないようにしている。数学や哲学、物語やSFなどの自分の仕事や語ることとは遠い世界のストーリーを読んで、自分の経験と照らし合わせる。実生活では役に立たないように見える抽象的な知識こそ、遠くから借りてくる力になる。なぜ遠い領域が大事なのか。近い領域の知識は、みんなが持っている。だから、そこから類推しても、みんなと同じ結論にしかたどり着かない。遠い領域の知識は、自分だけの武器になる。他の人が思いつかない類推ができる。2つ目は、常に「これは何かに使えないか」と考えることだ。映画を見ても、歴史を学んでも、スポーツを観戦しても、「これは自分の仕事にどう活かせるか」と考える。「関係ない」と決めつけず、「何か応用できないか」という視点で世界を見る。これを続けていると、頭の中に「類推のアンテナ」が立つ。普段の生活の中で、ふと「あ、これって、あれと同じだ」と気づくようになる。その瞬間が、類推力が育っている証拠だ。3つ目は、構造を2〜3つに絞って抽象化することだ。私の経験では、特徴や要点を2〜3つ挙げて、同じ構造を持つ事象を探すとうまくいく。1つだと何でも結びつけられてしまう。「両方とも存在する」では、類推にならない。4つ以上だと類推先が近くなりすぎて面白味がない。条件が厳しすぎて、同じ業界の似たようなものしか見つからない。2〜3つが、ちょうどいい。適度に絞られていて、適度に広い。この訓練を続けると、世界の見え方が変わる。一見無関係に見えるものの中に、共通の構造が見えてくる。私は、この感覚を得てから、仕事がずっと面白くなった。ニュースを読んでも、本を読んでも、人と話しても、「これは何かに使えるだろう」と思う。世界が、類推のネタの宝庫に見えてくる。類推を断つ勇気ここまで、類推を使い分ける技術について書いてきた。でも、もっと根本的なことがある。それは、類推を断つ勇気だ。一度つなげた類推を、必要なら断たなければならない。でも、これが難しい。なぜ難しいのか。類推は、理解の構造だ。「これはAみたいなものだ」という認識は、思考の足場になっている。その足場の上に、さらに理解を積み重ねている。足場を外すことは、その上に積み重ねたものも崩れることを意味する。一度「わかった」と思ったものを、「わからない」に戻すのは、心理的に辛い。人間は「わかった」状態を好む。「わからない」状態は不安だ。だから、間違った類推でも、手放したくない。間違っていると薄々気づいていても、「まあ、だいたい合っているだろう」と自分を納得させてしまう。認めたくない。また、類推は、コミュニケーションの基盤にもなる。チームで「これはAみたいなもの」と共有されていると、それを覆すことは、混乱を生む。「え、今までの説明は何だったの？」と言われる。自分の言ったことを訂正するのは、恥ずかしい。間違いを認めるのは、プライドが傷つく。だから、間違っているとわかっても、言い出せない。みんなが使っている類推に異を唱えるのは、勇気がいる。でも、間違った類推に固執し続けることの方が、はるかに有害だ。 間違った類推は、間違った判断を生む。間違った判断は、間違った設計を生む。間違った設計は、技術的負債を生む。技術的負債とは、急いで作った不完全なコードが後から修正コストとして跳ね返ってくることだ。借金のように、放置すればするほど利子が膨らんでいく。技術的負債は、チームを疲弊させる。最初の一歩で間違えると、その後のすべてがズレていく。早く気づいて修正するほど、傷は浅い。だから、類推が間違っていると気づいたら、勇気を持って断つ。「前にAみたいだと言ったけど、よく見たら違った。Bで考え直そう」と言う。これは、弱さではない。強さだ。現実を直視する強さだ。おわりに冒頭の話に戻ろう。私は、Rustの所有権を「本の貸し借りみたいなもの」と理解した。その類推で入り口は開けた。でも、その類推に縛られて、ライフタイムの本質を見誤った。非同期処理を料理に例えて、リソース競合を甘く見た。トランザクションを銀行振込に例えて、ロールバックの複雑さに気づかなかった。キャッシュを「手元に置く」と理解して、無効化の難しさを軽視した。私は、何度も同じ失敗を繰り返してきた。正直に言えば、私は今でも類推を使う。毎日のように使う。「これって、あれみたいだな」と考える癖は、もはや私の一部だ。類推なしに思考することなど、私にはできない。たぶん、誰にもできない。でも、これらの経験を経て、私は類推の使い方を変えた。類推は入り口として使う。入ったら、具体を見る。 「本の貸し借りみたいなもの」で入ったら、次に「でも、貸し借りと違って、所有権を渡したら元の変数からは完全にアクセスできなくなる。貸した相手を追跡する仕組みはない」と自分に言い聞かせる。類推と差異を、セットで意識する。そして、類推が成り立たない場面に出会ったら、類推を修正する勇気を持つ。類推は、人間の知能の基盤だ。われわれは類推なしには思考できない。だから、類推を否定するつもりはない。否定できるはずもない。でも、類推の限界を知らなければならない。類推は万能ではない。類推は常に成立するとは限らない。表面的な類推は、本質的な差異を見落とす。類推で入って、具体で判断する。類推は仮説であって、証明ではない。類推を絶対視せず、反例を探し、実測で検証する。そして、間違った類推は、勇気を持って断つ。これが、エンジニアとしての類推の使い方だ。「それって、○○みたいなものですよね」。この言葉を使うとき、私は今、一瞬立ち止まる。「本当にそうか？」と自問する。表面的な類似に惑わされていないか。本質的な差異を見落としていないか。先日、後輩にRustの所有権を説明する機会があった。私は「本の貸し借りみたいなものなんだけど」と言った後、こう続けた。「ただし、本と違って、Rustでは貸した先を追跡する仕組みはない。完全に手放すか、借用するかの二択なんだ」。あの頃の自分には、この補足ができなかった。類推は強力だ。だからこそ、慎重に扱わなければならない。おい、類推するな。いや、違う。類推しろ。でも、類推を疑え。類推で入って、具体で確かめろ。そして、間違っていたら、断つ勇気を持て。それが、類推に救われ、類推に何度も裏切られ、それでも類推を愛する人間からの、静かな呼びかけだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。参考書籍The Rust Programming Language, 3rd Edition (English Edition)作者:Klabnik, Steve,Nichols, Carol,Krycho, ChrisNo Starch PressAmazonプログラミングRust 第2版作者:Jim Blandy,Jason Orendorff,Leonora F.S. TindallオライリージャパンAmazonバックエンドエンジニアを目指す人のためのRust作者:安東 一慈,大西 諒,徳永 裕介,中村 謙弘,山中 雄大翔泳社Amazon類似と思考　改訂版 (ちくま学芸文庫)作者:鈴木宏昭筑摩書房Amazonアナロジー思考作者:細谷 功東洋経済新報社Amazon問題解決力を高める「推論」の技術作者:羽田康祐k_birdフォレスト出版Amazon新装版　アブダクション: 仮説と発見の論理作者:米盛 裕二勁草書房Amazon作る、試す、正す。　アジャイルなモノづくりのための全体戦略作者:市谷 聡啓ビー・エヌ・エヌAmazon","isoDate":"2025-12-05T21:02:08.000Z","dateMiliSeconds":1764968528000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"RustでOWASP API Security Top 10を体験する（後編）：リソース制御と攻撃検知","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/06/055637","contentSnippet":"この記事は、Rust Advent Calendar 2025 6日目のエントリ記事です。はじめに前編からの続き ← API1 (BOLA), API2 (Broken Authentication), API3 (Mass Assignment)の解説はこちら前編では認証・認可の基礎とデータ保護について解説した。後編では、リソース消費制御、機能レベルの認可、そしてサーバーサイド攻撃について体験していく。API4: Rate Limit - 総当たり攻撃対策パスワードクラッキング（パスワードを片っ端から試して突破する攻撃）の現実を体験できるデモ。owasp.orgなぜレート制限が重要なのかレート制限とは、「一定時間内に受け付けるリクエスト数を制限する」仕組みだ。レート制限がないAPIは「無限に試行できる」ことを意味する。 攻撃手法  被害  レート制限での防御  パスワード総当たり  アカウント乗っ取り  試行回数制限  クレデンシャルスタッフィング  流出パスワードでの不正ログイン  IPベースのブロック  OTPブルートフォース  2段階認証（SMS認証など）のバイパス  アカウントロック  APIの過剰呼び出し  サービス停止（DoS）  グローバルレート制限  スクレイピング  データの大量取得  リクエスト間隔の強制 パスワードクラッキングの数学4桁のPINコードを総当たりする時は以下のようになる。組み合わせ: 104 = 10,000通り毎秒10回の試行 → 約17分で全組み合わせを試行レート制限なし → 毎秒1000回で10秒8文字のパスワード（小文字+数字）の時は以下のようになる。組み合わせ: 368 ≒ 2.8兆通り毎秒1000回でも約89年かかるでも、辞書攻撃なら数万語 → 数分で完了レート制限は「総当たりを現実的に不可能にする」ための防御だ。cargo run --release --bin rate-limit-demoでは、実際にどうやってレート制限を実装するのか。単純に「1分間に10回まで」と制限すればいいように思えるが、攻撃者はそう甘くない。IPアドレスを変えながら攻撃したり、複数のアカウントを同時に狙ったりする。だから、防御も複数の観点から行う必要がある。二層の防御：IP追跡とアカウント追跡/// Tracks login attempts per IP address#[derive(Debug, Clone)]struct LoginAttemptTracker {    /// IP -\u003e (attempt_count, first_attempt_time)    ip_attempts: Arc\u003cRwLock\u003cHashMap\u003cString, (u32, Instant)\u003e\u003e\u003e,    /// Email -\u003e (attempt_count, first_attempt_time)    account_attempts: Arc\u003cRwLock\u003cHashMap\u003cString, (u32, Instant)\u003e\u003e\u003e,    /// Blocked IPs    blocked_ips: Arc\u003cRwLock\u003cVec\u003cString\u003e\u003e\u003e,    /// Locked accounts    locked_accounts: Arc\u003cRwLock\u003cVec\u003cString\u003e\u003e\u003e,}なぜ二層必要なのか。IP追跡のみだと、攻撃者がVPNやTorでIP変えながら攻撃できるアカウント追跡のみだと、1つのIPから多数のアカウントを攻撃できる両方で、どちらのパターンも防げるスライディングウィンドウの実装fn record_attempt(\u0026self, ip: \u0026str, email: \u0026str) -\u003e (u32, u32) {    let window = Duration::from_secs(300); // 5分間のウィンドウ    let now = Instant::now();    // Track IP attempts    let ip_count = {        let mut attempts = self.ip_attempts.write().unwrap();        let entry = attempts.entry(ip.to_string()).or_insert((0, now));        if now.duration_since(entry.1) \u003e window {            // 5分経過したらリセット            *entry = (1, now);        } else {            entry.0 += 1;        }        entry.0    };    // Block IP after 10 attempts    if ip_count \u003e= 10 {        let mut blocked = self.blocked_ips.write().unwrap();        if !blocked.contains(\u0026ip.to_string()) {            blocked.push(ip.to_string());            tracing::warn!(ip = ip, \"IP blocked due to too many attempts\");        }    }    // Lock account after 5 attempts    if account_count \u003e= 5 {        // ...    }    (ip_count, account_count)}governorクレートによるグローバルレート制限// Global rate limiter: 10 requests per secondlet rate_limiter = Arc::new(RateLimiter::direct(Quota::per_second(    NonZeroU32::new(10).unwrap(),)));governorはトークンバケットアルゴリズムを実装している。これは「バケツに水が溜まっていく」イメージだ。バケットに毎秒10トークン補充され、リクエストごとに1トークン消費。バケットが空になったら429（Too Many Requests）を返す。脆弱 vs 安全/// VULNERABLE: Login endpoint without rate limitingasync fn vulnerable_login(Json(req): Json\u003cLoginRequest\u003e) -\u003e Result\u003cJson\u003cLoginResponse\u003e, AppError\u003e {    // 何回でも試行可能！    if req.email == \"user@example.com\" \u0026\u0026 req.password == \"password123\" {        Ok(Json(LoginResponse { /* ... */ }))    } else {        Err(AppError::Unauthorized)    }}/// SECURE: Login endpoint with rate limiting and lockoutasync fn secure_login(    State(state): State\u003cAppState\u003e,    ConnectInfo(addr): ConnectInfo\u003cSocketAddr\u003e,    Json(req): Json\u003cLoginRequest\u003e,) -\u003e Result\u003cJson\u003cLoginResponse\u003e, (StatusCode, Json\u003cRateLimitError\u003e)\u003e {    let ip = addr.ip().to_string();    // 1. グローバルレート制限    if state.rate_limiter.check().is_err() {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 2. IPブロック確認    if state.tracker.is_ip_blocked(\u0026ip) {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 3. アカウントロック確認    if state.tracker.is_account_locked(\u0026req.email) {        return Err((StatusCode::TOO_MANY_REQUESTS, /* ... */));    }    // 4. 認証処理    if req.email == \"user@example.com\" \u0026\u0026 req.password == \"password123\" {        state.tracker.reset_on_success(\u0026ip, \u0026req.email); // 成功したらカウンターリセット        Ok(Json(LoginResponse { /* ... */ }))    } else {        state.tracker.record_attempt(\u0026ip, \u0026req.email); // 失敗を記録        Err((StatusCode::UNAUTHORIZED, /* ... */))    }}微妙な脆弱性：レート制限のバイパス手法「レート制限を実装したから安全」と思っていないだろうか。残念ながら、レート制限にもバイパス手法がたくさんある。微妙な脆弱性 #1: X-Forwarded-Forを信用する/// 開発者の意図: 「ロードバランサーの後ろにいるから、X-Forwarded-Forを使わないと」/// 現実: 攻撃者もX-Forwarded-Forを設定できるasync fn subtle_xff_bypass(headers: HeaderMap, ...) -\u003e Result\u003c...\u003e {    // BUG: X-Forwarded-Forを無条件に信用    let ip = headers        .get(\"X-Forwarded-For\")        .and_then(|v| v.to_str().ok())        .and_then(|s| s.split(',').next())        .map(|s| s.trim().to_string())        .unwrap_or_else(|| addr.ip().to_string());    // 攻撃: curl -H \"X-Forwarded-For: 1.2.3.4\" ...    //       curl -H \"X-Forwarded-For: 5.6.7.8\" ...    // 毎回違うIPとしてカウントされる！    if state.tracker.is_ip_blocked(\u0026ip) { /* ... */ }}X-Forwarded-Forは信頼できるプロキシ（ロードバランサーやCDNなど、自分たちが管理しているサーバー）からのみ受け入れるべきだ。信頼チェーンを確立せずにXFFを使うと、攻撃者がIPを自由に偽装できる。微妙な脆弱性 #2: 大文字小文字の不一致/// 開発者の意図: 「メールアドレスでアカウントロックを追跡」/// 現実: 大文字小文字で別アカウント扱いasync fn subtle_case_sensitivity(...) -\u003e Result\u003c...\u003e {    // BUG: アカウントロックは大文字小文字を区別    if state.tracker.is_account_locked(\u0026req.email) {        return Err(...);    }    // でも認証は大文字小文字を無視    let email_lower = req.email.to_lowercase();    if email_lower == \"user@example.com\" \u0026\u0026 req.password == \"password123\" {        // ...    }    // 攻撃:    // user@example.com で5回失敗 → ロック    // User@example.com で5回失敗 → 別カウント！    // USER@example.com で5回失敗 → また別カウント！    // 結果: 15回試行できる}アカウント識別子の正規化を一貫して行わないと、レート制限を回避される。微妙な脆弱性 #3: タイミングリーク/// 開発者の意図: 「ロックされたアカウントは早期リターン」/// 現実: レスポンス時間でアカウントの存在がわかるasync fn subtle_timing_leak(...) -\u003e Result\u003c...\u003e {    // ロック済みアカウントは即座に拒否（速い！）    if state.tracker.is_account_locked(\u0026req.email) {        return Err(/* 数マイクロ秒 */);    }    // パスワードハッシュ検証（遅い！）    tokio::time::sleep(Duration::from_millis(100)).await;    // 存在するアカウントは追加処理（もっと遅い！）    if account_exists(\u0026req.email) {        tokio::time::sleep(Duration::from_millis(50)).await;    }    // 攻撃: レスポンス時間を測定    // 即座に返る → ロック済み（= 存在するアカウント）    // 100ms → 存在しないアカウント    // 150ms → 存在するが間違ったパスワード}レスポンス時間を均一にしないと、アカウント列挙攻撃に使われる。微妙な脆弱性 #4: TOCTOU競合/// 開発者の意図: 「カウンターを確認してから処理」/// 現実: 確認と更新の間に別のリクエストが入るasync fn subtle_race_condition(...) -\u003e Result\u003c...\u003e {    // Step 1: カウンター読み取り（ロック解放）    let current_count = {        let attempts = state.tracker.ip_attempts.read().unwrap();        attempts.get(\u0026ip).map(|(count, _)| *count).unwrap_or(0)    }; // ← ここでロック解放    // この間に並行リクエストが！    tokio::time::sleep(Duration::from_millis(10)).await;    // Step 2: 制限チェック（古い値で判断）    if current_count \u003e= 10 {        return Err(...);    }    // Step 3: 処理後にカウンター更新    state.tracker.record_attempt(\u0026ip, \u0026req.email);    // 攻撃: 100並行リクエストを同時送信    // 全員が current_count = 0 で通過！}チェックと更新はアトミックに行うべき。RwLockではなくアトミック操作や、チェックと更新を1つのロック内で行う必要がある。API5: BFLA - 一般ユーザーが管理者になれてしまう問題前編でBOLA（Broken Object Level Authorization）を解説した。BOLAは「他人のデータにアクセスできてしまう」問題だった。では、「他人のデータ」ではなく「使えないはずの機能」にアクセスできてしまったら？それがBFLA（Broken Function Level Authorization）だ。owasp.orgBOLAが「他人のデータを見られる」なら、BFLAは「使えないはずの機能が使える」。例えば、一般ユーザーが管理者用のユーザー一覧APIを叩けてしまうケース。言ってみれば「平社員が社長の権限でシステムを操作できる」状態だ。BOLAとBFLAの違いを理解するこの2つは混同しやすいので、明確に区別しよう。 項目  BOLA  BFLA  何が壊れているか  オブジェクト（データ）へのアクセス制御  機能（エンドポイント）へのアクセス制御  攻撃例  BobがAliceの注文を見る  一般ユーザーが管理者APIを叩く  チェック対象  「このデータは誰のものか」  「この機能は誰が使えるか」  典型的な対策  リソースごとの所有者チェック  ロール/権限チェック 例えで言えばこうだ。BOLA = 他人のロッカーを開けられる（同じ権限レベル内での越境）BFLA = 社員証がないのに役員室に入れる（権限レベルの越境）この違いを理解すると、なぜBFLAが発生しやすいのかも見えてくる。なぜBFLAが発生するのかエンドポイントの「発見」 - /api/usersがあるなら/api/admin/usersもあるだろうと攻撃者は考えるフロントエンドによる隠蔽への過信 - 「管理メニューは管理者にしか見せてないから大丈夫」→ APIは直接叩ける認証と認可の混同（再び） - 「ログインしてるから管理APIも使えるはず」という誤った思い込みテスト不足 - 管理者機能は管理者アカウントでしかテストしないドキュメント化されていない管理API - 「隠しAPI」は攻撃者に見つかる実際の被害パターンBFLAによって可能になる攻撃を挙げる。ユーザー情報の一括取得 - 全ユーザーのメールアドレス、個人情報を抜き取る権限昇格 - 自分のアカウントに管理者権限を付与するシステム設定の変更 - APIキーの再生成、課金設定の変更データの一括削除 - 管理者用の一括削除機能を悪用監査ログの改ざん - 証拠隠滅のためにログを消去では、脆弱なコードと安全なコードを見比べてみよう。/// VULNERABLE: No role checkasync fn vulnerable_list_users(user: AuthenticatedUser) -\u003e Result\u003cJson\u003cVec\u003cUserInfo\u003e\u003e, AppError\u003e {    Ok(Json(vec![        UserInfo {            id: 1,            email: \"admin@example.com\".to_string(),            role: \"admin\".to_string(),            ssn: \"123-45-6789\".to_string(), // SSNまで露出        },        // ...    ]))}/// SECURE: Admin checkasync fn secure_list_users(user: AuthenticatedUser) -\u003e Result\u003cJson\u003cVec\u003cSafeUserInfo\u003e\u003e, AppError\u003e {    if !is_admin(\u0026user.0) {        return Err(AppError::Forbidden(\"Admin permission required\".to_string()));    }    // ...}is_adminのチェックは単純だ。pub fn is_admin(claims: \u0026UserClaims) -\u003e bool {    claims.permissions.iter().any(|p| p == \"admin\")}「これくらい誰でも書く」と考えるだろう。しかし、本番環境で「認証は通ってるから大丈夫」と言ってこのチェックを忘れる人が後を絶たない。微妙な脆弱性：一見正しく見えるBFLAのバグ「is_adminチェックさえ入れれば安全」と思っていないだろうか。残念ながら、そう単純ではない。微妙な脆弱性 #1: HTTPヘッダーを信用する/// 開発者の意図: 「フロントエンドが送るX-User-Roleヘッダーを信用しよう」/// 現実: curlでいくらでも偽装できるasync fn subtle_header_role_check(    user: AuthenticatedUser,    headers: HeaderMap,) -\u003e Result\u003cJson\u003cAdminResponse\u003e, AppError\u003e {    // BUG: HTTPヘッダーを信用している！    let role = headers        .get(\"X-User-Role\")        .and_then(|v| v.to_str().ok())        .unwrap_or(\"user\");    if role != \"admin\" {        return Err(AppError::Forbidden(\"Admin role required\".to_string()));    }    // 攻撃: curl -H \"X-User-Role: admin\" ...    Ok(Json(admin_data))}フロントエンドから「便利だから」とヘッダーでロール情報を送る設計を見たことがある。これはアウトだ。HTTPヘッダーはクライアントが自由に設定できる。JWTのペイロードのように署名で保護されていない限り、信用してはいけない。微妙な脆弱性 #2: JWTクレームをDBと照合しない/// 開発者の意図: 「JWTに権限が入っているから、それを使えばOK」/// 現実: トークン発行後にユーザーが降格されたら？async fn subtle_client_claims_check(    user: AuthenticatedUser,) -\u003e Result\u003cJson\u003cAdminResponse\u003e, AppError\u003e {    // これ、一見正しそう    let has_admin = user.0.permissions.iter().any(|p| p == \"admin\");    if !has_admin {        return Err(AppError::Forbidden(\"Admin permission required\".to_string()));    }    // 問題: ユーザーが管理者だったのは「トークン発行時」の話    // トークン発行後に降格されていても、トークンが有効な限りアクセスできてしまう    Ok(Json(admin_data))}JWT（JSON Web Token）は便利だが、「トークン発行時点のスナップショット」に過ぎない。JWTとは、ユーザー情報や権限を暗号化して埋め込んだトークンで、サーバーはDBを参照せずに認証できる。しかし、ユーザーの権限が変更されたら、古いトークンは無効にするか、DBで再確認する必要がある。微妙な脆弱性 #3: 大文字小文字の罠/// 開発者の意図: 「adminをチェックすれば安全」/// 現実: 「Admin」「ADMIN」「aDmIn」は？let has_admin = user.0.permissions.iter().any(|p| p == \"admin\");これ自体は問題ないが、トークン生成側で大文字小文字の統一が取れていないと問題になる。ある箇所では\"admin\"、別の箇所では\"Admin\"で権限が付与されていたら、チェックをすり抜けてしまう。// 安全な実装: 大文字小文字を無視let has_admin = user.0.permissions.iter()    .any(|p| p.eq_ignore_ascii_case(\"admin\"));微妙な脆弱性 #4: キャッシュされた権限チェック/// 開発者の意図: 「ミドルウェアで権限チェック済みだから、エンドポイントでは確認不要」/// 現実: そのキャッシュ、どこから来た？async fn subtle_cached_permission_check(    user: AuthenticatedUser,    Query(query): Query\u003cCachedCheckQuery\u003e,) -\u003e Result\u003cJson\u003cAdminResponse\u003e, AppError\u003e {    // BUG: クエリパラメータから「チェック済み」フラグを読んでいる！    let is_verified_admin = query.permission_verified.unwrap_or(false);    if is_verified_admin {        // 攻撃: ?permission_verified=true        return Ok(Json(admin_data));    }    // 本来のチェック    if !is_admin(\u0026user.0) {        return Err(AppError::Forbidden(\"Admin permission required\".to_string()));    }    Ok(Json(admin_data))}「ミドルウェアでチェック済み」というフラグをリクエストに含めるパターンは意外とある。でもそのフラグがクエリパラメータやヘッダーから来ていたら、攻撃者が自由に設定できる。API7: SSRF - サーバーを踏み台にするSSRF（Server-Side Request Forgery）は、サーバーに「代わりにリクエストを送らせる」攻撃だ。普通、攻撃者は外部から内部ネットワークにアクセスできない。でも、サーバーは内部ネットワークにアクセスできる。だから、サーバーを「踏み台」にして、内部ネットワークに攻撃を仕掛けるのがSSRFだ。owasp.orgたとえるなら、「社員に偽の指示書を渡して、機密書類を持ってこさせる」ようなものだ。社員（サーバー）は指示書が正当なものだと思い込んで、機密エリアにアクセスしてしまう。SSRFの危険性を理解するSSRFが特に危険な理由を説明する。ファイアウォールをバイパス - 外部からは遮断されていても、内部からのリクエストは通るクラウドメタデータにアクセス - AWS/GCPの169.254.169.254（クラウド環境で自動的に提供される情報サービス）から認証情報を取得可能内部サービスの探索 - ポートスキャンや内部APIの発見に悪用認証のバイパス - 「内部ネットワークからのアクセスは信頼」という設計を悪用特に2番目の「クラウドメタデータへのアクセス」は、現代のクラウド環境では致命的な被害につながる。なぜなら、メタデータサービスには一時的な認証情報が含まれているからだ。クラウド環境での致命的な被害クラウド環境でのSSRFは特に危険だ。2019年のCapital One事件では、SSRFを使ってAWSのメタデータサービスにアクセスし、1億人以上の顧客データが漏洩した。攻撃の流れを見てみよう。1. 攻撃者: http://169.254.169.254/latest/meta-data/iam/security-credentials/ にアクセスさせる2. サーバー: 内部からのリクエストなので通常通り処理3. AWSメタデータ: IAMロールの一時認証情報を返す4. 攻撃者: その認証情報でS3バケットにアクセス → 大量のデータを取得SSRFが発生しやすい機能この事件を見て「うちはそんな機能ないから大丈夫」と思うだろう。しかし、SSRFが発生する機能は意外と身近にある。以下のような機能はSSRFの温床になりやすい。URLプレビュー/OGP取得 - 「このURLのタイトルと画像を表示」Webhook送信 - 「指定されたURLにPOSTリクエストを送る」PDF生成 - 「このURLの内容をPDFにする」（ヘッドレスブラウザがURLを開く）画像のリサイズ/変換 - 「このURLの画像をサムネイルにする」インポート機能 - 「このURLからデータをインポート」どれも「ユーザーが指定したURLにアクセスする」という共通点がある。この「ユーザーが指定したURL」が問題だ。例えば、「URLを指定したらそのページの内容を取得する」機能があったとする。/// VULNERABLE: Fetches any URLasync fn vulnerable_fetch(Json(req): Json\u003cFetchUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    let response = reqwest::get(\u0026req.url).await?;    Ok(response.text().await?)}攻撃者は内部ネットワークのURLを指定する。curl -X POST http://localhost:8080/vulnerable/fetch \\     -d '{\"url\":\"http://localhost:8080/internal/secrets\"}'/internal/secrets は本来、外部からアクセスできない内部APIだ。しかし、サーバー自身が「localhost」にアクセスするのは許可されている。結果、攻撃者はサーバーを経由して機密情報を引き出す。サーバーは「言われたことを忠実に実行する」だけだ。それが悪意あるリクエストだとは気づかない。対策: 許可リストとプロトコル制限では、どうやってSSRFを防ぐのか。基本的な考え方は「信頼できるURLだけを許可する」ことだ。async fn secure_fetch(Json(req): Json\u003cFetchUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    let url = Url::parse(\u0026req.url)        .map_err(|_| AppError::BadRequest(\"Invalid URL\".to_string()))?;    // HTTPSのみ許可    if url.scheme() != \"https\" {        return Err(AppError::BadRequest(\"Only HTTPS URLs are allowed\".to_string()));    }    // 許可されたドメインのみ    let allowed_domains = [\"api.example.com\", \"cdn.example.com\"];    let host = url.host_str()        .ok_or_else(|| AppError::BadRequest(\"Invalid host\".to_string()))?;    if !allowed_domains.contains(\u0026host) {        return Err(AppError::BadRequest(\"Domain not in allowlist\".to_string()));    }    // 許可リストを通過したURLのみ処理    // ...}「なんでも取ってくる」から「許可されたものだけ取ってくる」へ。自由度は下がるが、セキュリティは上がる。微妙な脆弱性：SSRFの巧妙なバイパス手法「許可リストでドメインをチェックしているから安全」と思っていないだろうか。残念ながら、SSRFは想像以上に狡猾だ。攻撃者は、許可されたドメインを経由して、内部ネットワークにアクセスする方法を探す。微妙な脆弱性 #1: リダイレクトを追跡してしまう/// 開発者の意図: 「最初のURLを検証すればOK」/// 現実: リダイレクト先は検証されていないasync fn subtle_redirect_ssrf(Json(req): Json\u003cFetchUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    let parsed_url = Url::parse(\u0026req.url)?;    // 最初のURLは検証する    if !ALLOWED_DOMAINS.contains(\u0026parsed_url.host_str().unwrap()) {        return Err(AppError::BadRequest(\"Domain not allowed\".to_string()));    }    // BUG: リダイレクトを10回まで追跡する    let client = reqwest::Client::builder()        .redirect(reqwest::redirect::Policy::limited(10))        .build()?;    // 攻撃:    // 1. パートナーサイト webhook.partner.com を許可リストに追加    // 2. パートナーが webhook.partner.com/redirect?to=http://localhost/internal を設定    // 3. 最初は検証を通過、リダイレクトで内部サーバーにアクセス    let response = client.get(\u0026req.url).send().await?;    Ok(response.text().await?)}パートナーサイトやCDNを許可リストに入れていて、そこにオープンリダイレクト（任意のURLにリダイレクトできる機能）があったら終わり。リダイレクト先も検証するか、リダイレクトを無効にするべきだ。微妙な脆弱性 #2: DNSリバインディング/// 開発者の意図: 「DNSで解決されたIPをチェックすれば内部アクセスを防げる」/// 現実: DNSの応答は変わりうるasync fn subtle_dns_rebinding(Json(req): Json\u003cFetchUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    let host = Url::parse(\u0026req.url)?.host_str().unwrap().to_string();    // 最初のDNS解決（ここでは外部IP）    let ips = tokio::net::lookup_host(format!(\"{}:80\", host)).await?;    for ip in ips {        if ip.ip().to_string().starts_with(\"127.\") {            return Err(AppError::BadRequest(\"Internal IP blocked\".to_string()));        }    }    // BUG: 実際のリクエスト時には別のDNS解決が行われる可能性    // 攻撃者のDNSサーバー:    // 1回目のクエリ → 1.2.3.4（外部IP、チェック通過）    // 2回目のクエリ → 127.0.0.1（内部IP！）    tokio::time::sleep(Duration::from_millis(100)).await;  // この間にDNSが変わる    let response = reqwest::get(\u0026req.url).await?;    Ok(response.text().await?)}DNSリバインディング攻撃は、DNSの応答を時間差で変えることで検証をすり抜ける。DNSとは、ドメイン名（例：example.com）をIPアドレス（例：93.184.216.34）に変換する仕組みだ。攻撃者は自分のDNSサーバーを用意し、最初は外部IPを返し、2回目のクエリでは内部IP（127.0.0.1）を返すようにする。対策は「解決したIPを直接使う」か「DNSピンニング」（一度解決したIPを再利用する）を実装すること。微妙な脆弱性 #3: URLパーサーの差異を悪用/// 開発者の意図: 「URLをパースしてホストを検証」/// 現実: 検証時と実際のリクエスト時でパーサーが違うasync fn subtle_parser_differential(Json(req): Json\u003cFetchUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    // url クレートでパース    let parsed_url = Url::parse(\u0026req.url)?;    let host = parsed_url.host_str().unwrap();    if !ALLOWED_DOMAINS.contains(\u0026host) {        return Err(AppError::BadRequest(\"Domain not allowed\".to_string()));    }    // BUG: reqwest内部のHTTPクライアントが別のパースをする可能性    // 攻撃例:    // \"https://api.github.com@localhost/internal/secrets\"    //   → url クレート: github.com がホスト    //   → 一部のHTTPクライアント: localhost がホスト    let response = reqwest::get(\u0026req.url).await?;    Ok(response.text().await?)}URLの解釈は実装によって微妙に異なる。例えば、https://api.github.com@localhost/pathというURLを考えてみよう。あるパーサーはapi.github.comがホストだと解釈し、別のパーサーはlocalhostがホストだと解釈する。この差異を悪用して、検証をすり抜けることができる。微妙な脆弱性 #4: プロトコル/エンコーディングの罠/// 開発者の意図: 「エンコードされたURLもサポートしよう」/// 現実: 検証するURLとリクエストするURLが違うasync fn subtle_protocol_smuggling(Json(req): Json\u003cEncodedUrlRequest\u003e) -\u003e Result\u003cString, AppError\u003e {    let url_to_validate = if req.decode_first.unwrap_or(false) {        // URLデコードしてから検証        naive_percent_decode(\u0026req.url)    } else {        req.url.clone()    };    // デコード後のURLを検証    let parsed = Url::parse(\u0026url_to_validate)?;    // ... validation ...    // BUG: オリジナルのURL（デコード前）でリクエスト！    let response = reqwest::get(\u0026req.url).await?;  // ← url_to_validate じゃない！    Ok(response.text().await?)}検証に使うURLとリクエストに使うURLが一致していないと、検証をバイパスできる。「便利だから」と入力を加工するときは、必ず加工後の値を一貫して使うこと。動作確認：実際に脆弱性を突いてみるここまで、4つの脆弱性（API4: Rate Limit、API5: BFLA、API7: SSRF、そして前編で紹介したAPI1〜3）を解説してきた。でも、コードを読むだけでは「本当にこれで攻撃できるの？」という疑問が残るだろう。そこで、実際にcurlでリクエストを投げて、脆弱性が動作することを確認してみよう。「攻撃者の視点」を体験することで、防御の重要性が腑に落ちるはずだ。BOLA（API1）の動作確認# サーバー起動cargo run --release --bin bola-demo# Bobのトークンを取得BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# 脆弱なエンドポイント：BobがAliceの注文を見れてしまうcurl -H \"Authorization: Bearer $BOB_TOKEN\" http://localhost:8080/vulnerable/orders/1# 結果: {\"id\":1,\"user_id\":\"alice\",\"product\":\"Widget A\",\"amount\":100,...}# → BobがAliceの注文情報を取得できた！# セキュアなエンドポイント：適切に拒否されるcurl -H \"Authorization: Bearer $BOB_TOKEN\" http://localhost:8080/orders/1# 結果: {\"error\":\"Order 1 not found or access denied\"}# Subtle脆弱性：クエリパラメータでuser_idを上書きcurl -H \"Authorization: Bearer $BOB_TOKEN\" \"http://localhost:8080/subtle/orders/1?user_id=alice\"# 結果: {\"id\":1,\"user_id\":\"alice\",\"product\":\"Widget A\",...}# → クエリパラメータでオーナーチェックをバイパス！Mass Assignment（API3）の動作確認# サーバー起動cargo run --release --bin mass-assignment-demo# 脆弱なエンドポイント：statusを注入curl -X POST http://localhost:8080/vulnerable/payments \\  -H \"Content-Type: application/json\" \\  -d '{\"user_id\":\"attacker\",\"amount\":1000,\"status\":\"approved\"}'# 結果: {\"id\":\"...\",\"user_id\":\"attacker\",\"amount\":1000,\"status\":\"approved\",...}# → 攻撃者がstatusを\"approved\"に設定できた！# セキュアなエンドポイント：statusは無視されるcurl -X POST http://localhost:8080/payments \\  -H \"Content-Type: application/json\" \\  -d '{\"user_id\":\"user\",\"amount\":1000,\"status\":\"approved\"}'# 結果: {\"id\":\"...\",\"user_id\":\"user\",\"amount\":1000,\"status\":\"pending\",...}# → statusはサーバー側で\"pending\"に設定される# Subtle脆弱性：serde(flatten)でHashMapに余分なフィールドが入るcurl -X POST http://localhost:8080/subtle/payments/flatten \\  -H \"Content-Type: application/json\" \\  -d '{\"user_id\":\"user\",\"amount\":500,\"status\":\"approved\",\"id\":\"my-custom-id\"}'# 結果: statusが\"approved\"、idも上書きされる可能性# → flatten + HashMapの危険性BFLA（API5）の動作確認# サーバー起動cargo run --release --bin bfla-demo# 一般ユーザーのトークンを取得USER_TOKEN=$(curl -s http://localhost:8080/token/user | jq -r .access_token)# 脆弱なエンドポイント：一般ユーザーでも管理者機能にアクセスcurl -H \"Authorization: Bearer $USER_TOKEN\" http://localhost:8080/vulnerable/admin# 結果: {\"message\":\"Welcome to admin panel\",\"admin_data\":{\"total_revenue\":567890.12,...}}# → 一般ユーザーが管理者データを取得！# セキュアなエンドポイント：適切に拒否curl -H \"Authorization: Bearer $USER_TOKEN\" http://localhost:8080/admin# 結果: {\"error\":\"Admin permission required\"}# Subtle脆弱性1：HTTPヘッダーのロールを信頼curl -H \"Authorization: Bearer $USER_TOKEN\" \\     -H \"X-User-Role: admin\" \\     http://localhost:8080/subtle/admin/role-in-header# 結果: アクセス成功！# → ヘッダーを追加するだけでadminになれる# Subtle脆弱性2：キャッシュされた権限チェックを信頼curl -H \"Authorization: Bearer $USER_TOKEN\" \\     \"http://localhost:8080/subtle/admin/cached-check?permission_verified=true\"# 結果: アクセス成功！# → クエリパラメータで権限チェックをバイパスSSRF（API7）の動作確認# サーバー起動cargo run --release --bin ssrf-demo# 脆弱なエンドポイント：内部サービスにアクセスcurl \"http://localhost:8080/vulnerable/fetch?url=http://localhost:8080/internal/secrets\"# 結果: {\"secrets\":[\"DATABASE_URL=postgres://admin:password@db:5432\",...]}# → 内部の機密情報を取得！# セキュアなエンドポイント：localhost は拒否curl \"http://localhost:8080/fetch?url=http://localhost:8080/internal/secrets\"# 結果: {\"error\":\"Access to internal addresses is not allowed\"}# Subtle脆弱性：URLパーサーの差異を悪用curl \"http://localhost:8080/subtle/fetch/parser-diff?url=http://localhost%2523@evil.com/\"# → 異なるパーサーで解釈が変わり、バイパス可能Rate Limit（API4）の動作確認# サーバー起動cargo run --release --bin rate-limit-demo# 正常なレート制限：5回でロックfor i in {1..6}; do  curl -X POST http://localhost:8080/login \\    -H \"Content-Type: application/json\" \\    -d '{\"email\":\"test@example.com\",\"password\":\"wrong\"}'  echo \"\"done# 6回目: {\"error\":\"Account locked. Too many failed attempts.\"}# Subtle脆弱性1：X-Forwarded-For でIPを偽装for i in {1..10}; do  curl -X POST http://localhost:8080/subtle/login/xff \\    -H \"Content-Type: application/json\" \\    -H \"X-Forwarded-For: 10.0.0.$i\" \\    -d '{\"email\":\"victim@example.com\",\"password\":\"attempt$i\"}'done# → 毎回異なるIPとしてカウントされ、ロックされない！# Subtle脆弱性2：メールアドレスの大文字小文字curl -X POST http://localhost:8080/subtle/login/case \\  -H \"Content-Type: application/json\" \\  -d '{\"email\":\"User@Example.COM\",\"password\":\"wrong\"}'# → user@example.com とは別のエントリとしてカウント# Subtle脆弱性3：タイミング攻撃# 存在するユーザー（高速レスポンス）time curl -X POST http://localhost:8080/subtle/login/timing \\  -H \"Content-Type: application/json\" \\  -d '{\"email\":\"admin@example.com\",\"password\":\"x\"}'# → ~10ms# 存在しないユーザー（遅いレスポンス）time curl -X POST http://localhost:8080/subtle/login/timing \\  -H \"Content-Type: application/json\" \\  -d '{\"email\":\"nobody@example.com\",\"password\":\"x\"}'# → ~110ms（意図的な遅延）# → レスポンス時間の差でユーザーの存在を推測可能！Broken Auth（API2）の動作確認# サーバー起動cargo run --release --bin broken-auth-demo# 期限切れトークンを取得EXPIRED_TOKEN=$(curl -s http://localhost:8080/token/expired | jq -r .access_token)# 脆弱なエンドポイント：期限切れトークンを受け入れるcurl -H \"Authorization: Bearer $EXPIRED_TOKEN\" \\     http://localhost:8080/vulnerable/validate# 結果: {\"message\":\"Token accepted\",\"token_type\":\"expired\"}# → 期限切れなのにアクセス成功！# セキュアなエンドポイント：適切に拒否curl -H \"Authorization: Bearer $EXPIRED_TOKEN\" \\     http://localhost:8080/validate# 結果: {\"error\":\"Token validation failed: ExpiredSignature\"}# Subtle脆弱性：nbf（not before）をスキップFUTURE_TOKEN=$(curl -s http://localhost:8080/token/future | jq -r .access_token)curl -H \"Authorization: Bearer $FUTURE_TOKEN\" \\     http://localhost:8080/subtle/validate/nbf-skip# 結果: まだ有効期間前なのにアクセス成功# → nbfのチェック漏れ動作確認のポイントこれらのテストで確認できる重要な点をまとめる。脆弱なエンドポイント vs セキュアなエンドポイント同じリクエストでも、実装によって結果が全く異なるセキュアな実装は「デフォルト拒否」の原則に従うSubtle脆弱性の危険性コードを見ただけでは問題に気づきにくい「動いているから大丈夫」では見逃すセキュリティテストで初めて発覚することが多い攻撃者の視点攻撃者は正常系だけでなく、エッジケースを狙うヘッダー追加、大文字小文字変換、URL エンコードなど「そんなリクエスト来ないでしょ」は通用しない全テストの実行20のセキュリティテストを一括で実行できる。./scripts/test_all.sh==========================================API Security Demo - Vulnerability TestsOWASP API Security Top 10==========================================[PASS] Vulnerable EP: Bob accessed Alice's order (HTTP 200)  ← 攻撃成功[PASS] Secure EP: Access denied (HTTP 404)                   ← 攻撃失敗...==========================================Test Results Summary==========================================PASS: 20FAIL: 0All security tests passed!「脆弱なエンドポイントで攻撃が成功すること」と「安全なエンドポイントで攻撃が失敗すること」の両方をテストしている。「攻撃が成功してPASS」というのは変な感じがするが、これは「脆弱性のデモとして正しく動作している」ことの確認だ。その他のデモobservability: 攻撃検知システムセキュリティ対策は「防ぐ」だけでは不十分だ。攻撃が起きたことを「検知する」仕組みも必要になる。なぜなら、完璧な防御は存在しないからだ。このデモでは、攻撃パターンを検知してログに記録する仕組みを体験できる。cargo run --release --bin observability-demoセキュリティメトリクス（攻撃の試行回数や種類などの統計情報）を収集し、攻撃パターン（SQLインジェクション、XSSなど）を検知してログ出力する。Prometheus（監視システム）等で収集して、ダッシュボードで監視する想定だ。security_test: 自動セキュリティテスト脆弱性の有無を自動的にテストするデモ。CI/CD（コードの変更があるたびに自動でテストやデプロイを行う仕組み）に組み込むイメージ。開発の早い段階でセキュリティ問題を発見できる。cargo run --release --bin security-test-democurl http://localhost:8080/test/run-allAPI6, 8, 9, 10を扱わない理由本記事ではOWASP API Security Top 10のうち、API6、API8、API9、API10を扱っていない。それぞれ理由がある。API6: Unrestricted Access to Sensitive Business Flowsowasp.orgビジネスロジックの悪用（大量購入、スパムアカウント作成など）に関する脆弱性。これは「コードの脆弱性」というより「ビジネスルールの実装漏れ」であり、汎用的なデモを作りにくい。実際のビジネス要件に依存するため、抽象的なサンプルコードでは本質を伝えにくい。API8: Security Misconfigurationowasp.org設定ミス（デバッグモードの本番有効化、不要なHTTPメソッド許可、CORSの過剰許可など）に関する脆弱性。これはコードではなくインフラ設定やデプロイ設定の問題であり、Rustのコードデモとして示すには適していない。設定ファイルやクラウド設定のベストプラクティス集として別途まとめる方が有用だろう。API9: Improper Inventory Managementowasp.orgAPIバージョン管理の不備（古いAPIの放置、ドキュメント化されていないエンドポイント）に関する脆弱性。これは運用・管理の問題であり、単一のコードデモでは再現しにくい。組織的なAPIガバナンスの話になる。API10: Unsafe Consumption of APIsowasp.orgサードパーティAPIからの応答を信頼しすぎる問題。外部APIとの連携をデモするには実際のサードパーティサービスが必要になり、自己完結型のデモとして構成しにくい。要するに、API1〜5とAPI7は「コードレベルで再現・修正できる脆弱性」であり、API6、8、9、10は「運用・設定・ビジネスロジックレベルの問題」という違いがある。本記事では前者に焦点を当てた。これらの脆弱性を学ぶにはAPI6、8、9、10を含む全ての脆弱性を体験したい場合は、以下の脆弱性学習プラットフォームを推奨する。OWASP Juice Shopowasp.org最も有名な脆弱性学習用Webアプリケーション。OWASP Top 10だけでなく、API Security Top 10の脆弱性も含む100以上のチャレンジがある。Dockerで簡単に起動でき、スコアボードで進捗を確認できる。crAPI (Completely Ridiculous API)owasp.orgAPI脆弱性に特化した学習プラットフォーム。Facebook、Uber、Shopifyなどで実際に発見された脆弱性をベースにしたチャレンジが含まれる。マイクロサービスアーキテクチャで構築されており、現代的なAPI構成を学べる。VAmPI (Vulnerable API)github.comFlaskで作られたシンプルな脆弱性API。OWASP API Top 10の脆弱性が含まれており、セキュリティツールのテストにも使える。Vulnerable REST API (2023 Edition)github.comNode.jsとReactで作られた脆弱性アプリケーション。OWASP API Security Top 10 2023版に対応しており、API6〜10を含む全ての脆弱性をカバーしている。APIsec Universitywww.apisecuniversity.comAPIセキュリティに特化した無料のオンライントレーニング。OWASP API Top 10の解説から実践的なペネトレーションテスト手法まで学べる。まとめ前編・後編を通じて、OWASP API Security Top 10のうち6つの脆弱性を体験してきた。セキュリティは「知っている」と「実感している」の間に大きな溝がある。このデモを作って、自分で攻撃を試して、初めて「あ、これ確かにヤバい」と腑に落ちた。ドキュメントを読むだけでは得られない理解だった。コードはGitHubで公開している。cargo run --release --bin bola-demoで起動して、実際に攻撃を試してみてほしい。最後に、冒頭の話に戻る。「認証してるから大丈夫でしょ」—この言葉を聞いたら、このデモのことを思い出してほしい。そして「認可は」と聞き返してほしい。認証は玄関のチェックに過ぎない。中に入った後、どの部屋に入れるかを制御するのが認可だ。参考リンクOWASP API Security Top 10 (2023)公式ドキュメント。owasp.orgOWASP API Security Projectプロジェクトのホームページ。owasp.org本記事のソースコードgithub.comAlice and Bob - WikipediaBobとAliceの歴史。en.wikipedia.orggovernor - Rust Rate Limiting Libraryレート制限の実装に使用。github.comCWE-918: Server-Side Request Forgery (SSRF)SSRFに関連するCWEエントリ。cwe.mitre.orgCWE-770: Allocation of Resources Without Limits or Throttlingレート制限不足に関連するCWEエントリ。cwe.mitre.orgCWE-285: Improper AuthorizationBFLAに関連するCWEエントリ。cwe.mitre.orgPortSwigger - Server-side request forgery (SSRF)SSRFの詳細な解説とラボ環境。portswigger.netOWASP Cheat Sheet - Authorization認可に関するベストプラクティス。cheatsheetseries.owasp.orgOWASP Cheat Sheet - Authentication認証に関するベストプラクティス。cheatsheetseries.owasp.orgCapital One Data Breach (2019)SSRFによる大規模情報漏洩事例。https://en.wikipedia.org/wiki/2019_Capital_One_data_breachen.wikipedia.orgAWS IMDSv2AWSメタデータサービスのセキュリティ強化。SSRF対策として重要。docs.aws.amazon.comSecurify弊社のプロダクトでもAPIセキュリティのチェックを一部行うことができるらしい。3-shake.com","isoDate":"2025-12-05T20:56:37.000Z","dateMiliSeconds":1764968197000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"RustでOWASP API Security Top 10を体験する（前編）：認証・認可の基礎とデータ保護","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/05/104919","contentSnippet":"この記事は、Rust Advent Calendar 2025 5日目のエントリ記事です。はじめに先日、あるプロジェクトのコードレビューで「このエンドポイント、認証は通ってるけど認可は大丈夫か」と聞いたら、「認証してるから大丈夫でしょ」という返答が返ってきた。その瞬間、私の脳内では警報が鳴り響いた。これはあれだ。「鍵がかかってるから金庫は安全」と言いながら、金庫の中身を誰でも見られる状態にしているやつだ。認証（Authentication）と認可（Authorization）の違い。頭ではわかっていても、実際のコードでどう違うのか、どう危険なのかを体感したことがある人は意外と少ない。かくいう私も、セキュリティの本を読んで「ふーん」と思いながら、翌日には同じミスをやらかしていた口だ。そこで今回、OWASP API Security Top 10の脆弱性を実際に攻撃できる形でRustにより実装してみた。OWASPとは「Open Web Application Security Project」の略で、Webアプリケーションのセキュリティに関するオープンなコミュニティだ。彼らが発表する「Top 10」は、最も危険で頻繁に発生する脆弱性のランキングとして世界中の開発者に参照されている。「脆弱なエンドポイント」と「安全なエンドポイント」を並べて、攻撃がどう成功し、どう防げるのかを手を動かして確認できる。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。なぜBobとAliceなのか初手で余談だがセキュリティの例でやたらと「BobがAliceのデータを〜」という話が出てくる。なぜこの2人なのか。これは1978年にRon Rivest、Adi Shamir、Leonard Adleman（RSA暗号のRSA）が書いた論文「A Method for Obtaining Digital Signatures and Public-Key Cryptosystems」に由来する。彼らは暗号通信の説明で「AさんがBさんにメッセージを送る」ではなく「AliceがBobにメッセージを送る」と書いた。AとBで始まる名前を選んだだけだが、これが定着した。その後、セキュリティの世界では登場人物が増えていった。AliceとBobは通信したい善良な2人（主人公）Eveは盗聴者（Eavesdropperから。悪役その1）Malloryは能動的攻撃者（Maliciousから。もっと悪い悪役）Trentは信頼できる第三者（Trustedから）CarolやCharlieは3人目の参加者が必要なときに登場つまり、BobとAliceは何十年も同じ役を演じ続けている。本記事でも、この伝統に従ってBobとAliceに登場してもらう。Bobには悪役を演じてもらうことになるが、本来のBobは悪い人ではない。「認可が不十分だと善良なBobでも悪いことができてしまう」というのが本質的な問題なのだ。en.wikipedia.orgなぜ「体験」が必要なのかセキュリティの勉強で一番難しいのは、「危険性を実感すること」だ。ドキュメントを読んで「BOLAは危険です」と書いてあっても、「へー、そうなんだ」で終わる。これは人間の性だ。交通事故のニュースを見ても「自分は大丈夫」と考えるのと同じで、実際にBobがAliceのデータを抜き取る瞬間を見ないと、その怖さは伝わらない。このデモを作った動機は単純で、自分が「あ、これ確かにヤバい」と冷や汗をかける教材が欲しかったからだ。本を読んで「なるほど」と思っても、3日後には忘れている。でも、自分の手で攻撃を成功させた経験は忘れない。ちなみに、このデモを作っている最中に「あれ、これ本番のコードにも似たようなのあったな...」と気づいて本当に冷や汗をかいた。勉強は大事。OWASP API Security Top 10 (2023) 一覧まず、OWASP API Security Top 10の全体像を把握しておこう。本記事では、このうち主要な脆弱性を実際にRustで実装して体験する。https://owasp.org/API-Security/editions/2023/en/0x11-t10/owasp.org リスク  説明  API1:2023 - Broken Object Level Authorization  APIはオブジェクト識別子を扱うエンドポイントを公開しがちで、オブジェクトレベルのアクセス制御の問題が広い攻撃対象となる。ユーザーからのIDを使ってデータソースにアクセスするすべての関数で、オブジェクトレベルの認可チェックを考慮すべき。  API2:2023 - Broken Authentication  認証メカニズムは不正に実装されることが多く、攻撃者が認証トークンを侵害したり、実装の欠陥を悪用して一時的または永続的に他のユーザーになりすますことを可能にする。  API3:2023 - Broken Object Property Level Authorization  このカテゴリはAPI3:2019の過度なデータ露出とAPI6:2019のMass Assignmentを統合し、根本原因であるオブジェクトプロパティレベルでの認可検証の欠如または不適切さに焦点を当てている。  API4:2023 - Unrestricted Resource Consumption  APIリクエストの処理にはネットワーク帯域、CPU、メモリ、ストレージなどのリソースが必要。成功した攻撃はサービス拒否や運用コストの増加につながる可能性がある。  API5:2023 - Broken Function Level Authorization  異なる階層、グループ、ロールを持つ複雑なアクセス制御ポリシーと、管理機能と通常機能の不明確な分離は、認可の欠陥につながりやすい。  API6:2023 - Unrestricted Access to Sensitive Business Flows  このリスクに脆弱なAPIは、自動化された方法で過度に使用された場合にビジネスを損なう可能性のある機能を補償せずにビジネスフローを公開している。  API7:2023 - Server Side Request Forgery  SSRFの欠陥は、APIがユーザー提供のURIを検証せずにリモートリソースを取得する際に発生する可能性がある。ファイアウォールやVPNで保護されていても、攻撃者がアプリケーションに細工されたリクエストを予期しない宛先に送信させることができる。  API8:2023 - Security Misconfiguration  APIとそれをサポートするシステムには通常、APIをよりカスタマイズ可能にするための複雑な構成が含まれている。ソフトウェアおよびDevOpsエンジニアがこれらの構成を見落としたり、セキュリティのベストプラクティスに従わない場合がある。  API9:2023 - Improper Inventory Management  APIは従来のWebアプリケーションよりも多くのエンドポイントを公開する傾向があり、適切で更新されたドキュメントが非常に重要。非推奨のAPIバージョンや公開されたデバッグエンドポイントなどの問題を軽減するために、ホストとデプロイされたAPIバージョンの適切なインベントリも重要。  API10:2023 - Unsafe Consumption of APIs  開発者はサードパーティAPIから受信したデータをユーザー入力よりも信頼する傾向があり、より弱いセキュリティ基準を採用しがち。APIを侵害するために、攻撃者はターゲットAPIを直接侵害しようとするのではなく、統合されたサードパーティサービスを狙う。 本記事で実際に体験できる脆弱性を挙げる。前編（本記事）ではAPI1 (BOLA)、API2 (Broken Authentication)、API3 (Mass Assignment)を扱う後編ではAPI4 (Rate Limit)、API5 (BFLA)、API7 (SSRF)を扱うデモの全体像このデモは9つのバイナリで構成されている。それぞれが独立したWebサーバーとして起動する。/token/{user_id} でテスト用JWTを取得（JWTとは「JSON Web Token」の略で、ユーザーの認証情報を安全にやり取りするためのトークン形式だ。ログイン後にサーバーから発行され、以降のリクエストで「私は認証済みのユーザーです」と証明するために使う）/vulnerable/... で脆弱なエンドポイントを叩く/... で安全なエンドポイントを叩くapi-security-demo/├── src/bin/│   ├── bola.rs              # BOLA: オブジェクトレベル認可の不備│   ├── bfla.rs              # BFLA: 機能レベル認可の不備│   ├── mass_assignment.rs   # Mass Assignment: 一括代入の脆弱性│   ├── broken_auth.rs       # Broken Auth: 認証の不備│   ├── rate_limit.rs        # Rate Limit: リソース消費制限の不備│   ├── ssrf.rs              # SSRF: サーバーサイドリクエストフォージェリ│   ├── jwt.rs               # JWT: トークン操作のデモ│   ├── observability.rs     # 攻撃検知システム│   └── security_test.rs     # 自動セキュリティテスト技術スタックはRust + axum（axumはRust用のWebフレームワークで、高速かつ型安全なAPIサーバーを構築できる）。Rust 2024エディションで書いている。前提条件試してみたい方は以下が必要だ。Rust 1.85以上（2024エディション対応）curl と jq（テスト用。curlはコマンドラインからHTTPリクエストを送るツール、jqはJSONデータを整形・抽出するツール）# リポジトリのクローンgit clone https://github.com/nwiizo/workspace_2025.gitcd workspace_2025/infrastructure/api-security-demo# ビルド（初回は依存関係のダウンロードで時間がかかる）cargo build --release実装アーキテクチャの詳細「デモを動かす」だけでなく「なぜこう実装したのか」を理解することで、自分のプロジェクトに応用できる。ここでは設計判断とその理由を詳しく説明する。プロジェクト構成api-security-demo/├── Cargo.toml              # Rust 2024エディション、依存関係定義├── src/│   ├── lib.rs              # ライブラリのエントリポイント│   ├── auth.rs             # JWT認証・認可ロジック│   ├── db.rs               # SQLiteデータベース操作│   ├── error.rs            # エラー型定義│   ├── models.rs           # データモデル定義│   └── bin/                # 各デモのバイナリ│       ├── bola.rs│       ├── bfla.rs│       └── ...└── scripts/    └── test_all.sh         # 全テスト実行スクリプト共通ロジックはsrc/配下にライブラリとして切り出し、各デモはsrc/bin/配下の独立したバイナリとして実装している。これにより以下のメリットがある。コードの再利用: 認証、DB操作、エラーハンドリングを全デモで共有単一責任: 各バイナリは1つの脆弱性カテゴリに集中独立した起動: cargo run --bin bola-demoで特定のデモだけ起動可能エラーハンドリング設計Rustらしいエラー設計を採用した。thiserrorクレートで列挙型エラーを定義し、axumのIntoResponseを実装した。use thiserror::Error;#[derive(Error, Debug)]pub enum AppError {    #[error(\"Authentication required\")]    Unauthorized,    #[error(\"Access denied: {0}\")]    Forbidden(String),    #[error(\"Resource not found: {0}\")]    NotFound(String),    #[error(\"Invalid request: {0}\")]    BadRequest(String),    #[error(\"Rate limit exceeded\")]    RateLimitExceeded,    #[error(\"JWT error: {0}\")]    JwtError(#[from] jsonwebtoken::errors::Error),    #[error(\"Database error: {0}\")]    DatabaseError(String),}なぜanyhow::Errorではなく独自のエラー型なのか。HTTPステータスコードの制御。エラーの種類によって401、403、404、429などを返し分けたいクライアントへのメッセージ制御。内部エラーの詳細は隠し、クライアント向けのメッセージだけ返したいコンパイル時の網羅性チェック。matchで全ケースを処理しているか確認できるIntoResponseの実装を見てみよう。impl IntoResponse for AppError {    fn into_response(self) -\u003e Response {        let (status, error_message) = match \u0026self {            AppError::Unauthorized =\u003e (StatusCode::UNAUTHORIZED, self.to_string()),            AppError::Forbidden(msg) =\u003e (StatusCode::FORBIDDEN, msg.clone()),            AppError::NotFound(msg) =\u003e (StatusCode::NOT_FOUND, msg.clone()),            AppError::BadRequest(msg) =\u003e (StatusCode::BAD_REQUEST, msg.clone()),            AppError::RateLimitExceeded =\u003e (StatusCode::TOO_MANY_REQUESTS, \"Rate limit exceeded\".to_string()),            // ...        };        let body = Json(json!({ \"error\": error_message }));        (status, body).into_response()    }}これにより、ハンドラ関数で?演算子を使うだけで、エラーの種類に応じたHTTPレスポンスに変換される。認証・認可の実装パターンaxumのFromRequestPartsトレイトを実装したExtractorを使う。Extractorとは「抽出器」のことで、HTTPリクエストから必要な情報（ここでは認証情報）を自動的に取り出す仕組みだ。これがこのデモの核心部分だ。/// Extractor for authenticated user claims (secure version)#[derive(Debug, Clone)]pub struct AuthenticatedUser(pub UserClaims);impl\u003cS\u003e FromRequestParts\u003cS\u003e for AuthenticatedUserwhere    S: Send + Sync,{    type Rejection = AppError;    fn from_request_parts(        parts: \u0026mut Parts,        _state: \u0026S,    ) -\u003e impl Future\u003cOutput = Result\u003cSelf, Self::Rejection\u003e\u003e + Send {        let result = extract_auth_from_parts(parts, false);        async move { result.map(AuthenticatedUser) }    }}Extractorパターンの利点を挙げる。宣言的: 関数シグネチャにAuthenticatedUserがあれば認証必須と一目でわかる再利用可能: 同じExtractorを全エンドポイントで使い回せるテスト容易: Extractorを差し替えてテスト可能失敗時の自動レスポンス: 認証失敗時は自動で401を返す「脆弱な」バージョンも用意している。/// Extractor for user claims WITHOUT proper validation (vulnerable version)#[derive(Debug, Clone)]pub struct VulnerableAuthUser(pub UserClaims);これは署名検証をスキップし、期限切れトークンも受け入れる。教育目的のみ。データベース層の設計SQLiteを使い、認可の有無でメソッドを分けている。/// Get order by ID (no authorization check - vulnerable)pub fn get_order_by_id(\u0026self, id: i64) -\u003e Result\u003cOption\u003cOrder\u003e, AppError\u003e {    let conn = self.conn.lock().unwrap();    let mut stmt = conn.prepare(        \"SELECT id, user, product, quantity FROM orders WHERE id = ?1\"    )?;    // ...}/// Get order by ID with user check (secure)pub fn get_order_by_id_for_user(\u0026self, id: i64, user: \u0026str) -\u003e Result\u003cOption\u003cOrder\u003e, AppError\u003e {    let conn = self.conn.lock().unwrap();    let mut stmt = conn.prepare(        \"SELECT id, user, product, quantity FROM orders WHERE id = ?1 AND user = ?2\"    )?;    // ...}「なぜSQLで認可するのか。アプリケーション層でフィルタすればいいのでは」という疑問もあるだろう。アプリケーション層でも可能だが、DB層で認可する利点がある。パフォーマンス: 不要なデータをDBから取得しない防御の多層化: アプリ層のバグがあってもDB層で防げる一貫性: SQLで認可ロジックが一箇所に集約されるしかし、複雑な認可ルール（「自分のチームのデータ」など）はアプリ層で実装したほうが保守しやすい場合もある。依存関係の選定理由Cargo.tomlから主要な依存関係とその理由を説明する。# Web frameworkaxum = { version = \"0.8\", features = [\"macros\"] }axum: Tokioチームが開発、型安全、Extractorパターン。Actix-webより新しく、モダンな設計。# Authentication \u0026 Authorizationjsonwebtoken = \"9\"argon2 = \"0.5\"jsonwebtoken: Rustで最もポピュラーなJWTライブラリ。argon2: パスワードハッシュの現行推奨アルゴリズム。bcryptより新しく、メモリハード。# Error handlingthiserror = \"2\"thiserror: 派生マクロでボイラープレートを削減。#[error(\"...\")]でDisplay実装が自動生成される。# Rate limitinggovernor = \"0.8\"governor: トークンバケットアルゴリズムの実装。非同期対応。# Databaserusqlite = { version = \"0.32\", features = [\"bundled\"] }rusqlite: SQLiteバインディング。bundledでSQLiteを同梱（環境依存を排除）。本番ではPostgreSQLやMySQLを推奨。テスト戦略各モジュールにユニットテストを配置している。#[cfg(test)]mod tests {    use super::*;    #[test]    fn test_order_authorization() {        let db = Database::new_in_memory().unwrap();        let order = db.create_order(\"alice\", \"Test Product\", 5).unwrap();        // Alice can access her order        let result = db.get_order_by_id_for_user(order.id, \"alice\").unwrap();        assert!(result.is_some());        // Bob cannot access Alice's order        let result = db.get_order_by_id_for_user(order.id, \"bob\").unwrap();        assert!(result.is_none());    }}より、scripts/test_all.shでE2E的な統合テストを実行。各エンドポイントに実際にHTTPリクエストを送り、脆弱なエンドポイントで攻撃が成功すること、安全なエンドポイントで攻撃が失敗することを検証する。API1: BOLA - 最も危険で、最も見落とされやすい脆弱性OWASP API Security Top 10の堂々第1位がBOLA（Broken Object Level Authorization）だ。日本語では「オブジェクトレベル認可の不備」。https://owasp.org/API-Security/editions/2023/en/0xa1-broken-object-level-authorization/owasp.org名前が難しそうに見えるが、中身は簡単だ。要するに「BobがAliceのデータを見られてしまう」という、小学生でも「それダメでしょ」とわかる問題だ。しかし、驚くほど多くの本番システムにこれがある。人類は学ばない。なぜBOLAが最も危険なのかBOLAが1位である理由は明確だ。発生頻度が非常に高い - ほぼすべてのAPIがリソースIDを扱う。そのすべてで認可チェックが必要自動化しやすい - 攻撃者はIDを1, 2, 3...と順に試すだけ。スクリプト数行で全データを列挙できる検出が困難 - 正規のリクエストと見分けがつかない。WAFでは防げない影響が甚大 - 顧客データ、取引履歴、個人情報がすべて漏洩する可能性実際のインシデント事例BOLAによる情報漏洩は数え切れないほど発生している。2019年 First American Financial - 不動産の取引記録8億8500万件が流出。URLのIDを変えるだけで他人の書類にアクセス可能だった2018年 Facebook - View As機能の脆弱性で5000万アカウントのトークンが漏洩多数のモバイルアプリ - APIエンドポイントのID推測で他ユーザーのプロフィールにアクセス可能これらに共通するのは「認証はしていたが、認可が不十分だった」という点だ。ログインしているからといって、すべてのデータにアクセスできるわけではない。この当たり前のことを、コードで正しく実装するのは意外と難しい。なぜ開発者はBOLAを生み出してしまうのか認証と認可の混同 - 「ログインしてるからOK」という思い込みフレームワークの過信 - 「認証ミドルウェアを通ってるから安全」という誤解テストの盲点 - 機能テストは自分のデータでしか行わないIDの予測可能性 - 連番IDは攻撃を容易にする（でもUUIDでも根本解決にならない）開発速度優先 - 「認可は後で追加する」と言いながら忘れる脆弱なコード/// VULNERABLE: Returns any order by ID without checking ownershipasync fn vulnerable_get_order(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    _user: AuthenticatedUser, // 認証情報を受け取っているが...    Path(order_id): Path\u003ci64\u003e,) -\u003e Result\u003cJson\u003cOrder\u003e, AppError\u003e {    // 使っていない。アンダースコアプレフィックスがそれを物語っている    let order = state.db.get_order_by_id(order_id)?        .ok_or_else(|| AppError::NotFound(format!(\"Order {} not found\", order_id)))?;    Ok(Json(order))}_userとしてわざわざ認証情報を受け取っているのに、アンダースコアつけて無視している。これは「セキュリティチェックしてますよ」というアリバイ作りにすらなっていない。むしろ「チェックしようとして忘れた」という証拠だ。安全なコード/// SECURE: Returns order only if it belongs to the authenticated userasync fn secure_get_order(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    user: AuthenticatedUser,  // アンダースコアなし    Path(order_id): Path\u003ci64\u003e,) -\u003e Result\u003cJson\u003cOrder\u003e, AppError\u003e {    let user_id = \u0026user.0.sub;    // 「注文ID」と「ユーザーID」の両方でDBを検索    let order = state.db.get_order_by_id_for_user(order_id, user_id)?        .ok_or_else(|| AppError::NotFound(format!(            \"Order {} not found or access denied\", order_id        )))?;    Ok(Json(order))}違いは1行だけ。たった1行。でも、この1行が「情報漏洩インシデント発生」と「平穏な運用」の分かれ道だ。微妙な脆弱性：一見正しそうに見えるバグ本番環境で見つかる脆弱性の多くは、明らかな間違いではない。「一見正しそうに見える」コードに潜んでいる。このデモには3つの「微妙な脆弱性」エンドポイントを用意した。微妙な脆弱性 #1: クエリパラメータによる上書き#[derive(Deserialize)]struct UserIdQuery {    user_id: Option\u003cString\u003e,}/// 「デバッグ用にuser_idをクエリパラメータで指定できるようにしよう」/// という親切心から生まれた脆弱性async fn subtle_vulnerable_get_order(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    user: AuthenticatedUser,  // ちゃんと認証してる！    Path(order_id): Path\u003ci64\u003e,    Query(query): Query\u003cUserIdQuery\u003e,) -\u003e Result\u003cJson\u003cOrder\u003e, AppError\u003e {    // BUG: クエリパラメータが認証情報を上書きしてしまう    let user_id = query.user_id.unwrap_or_else(|| user.0.sub.clone());    let order = state        .db        .get_order_by_id_for_user(order_id, \u0026user_id)?  // user_idが攻撃者の指定した値に！        .ok_or_else(|| AppError::NotFound(\"...\"))?;    Ok(Json(order))}攻撃方法を見てみよう。# Bobとして認証BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# クエリパラメータでAliceになりすましcurl -H \"Authorization: Bearer $BOB_TOKEN\" \\     \"http://localhost:8080/subtle/orders/1?user_id=alice\"このパターンは実際のコードレビューでよく見る。「管理画面でユーザーを切り替えて確認したい」「サポート担当がユーザーの代わりに操作する機能が必要」などの要件から生まれがち。対策は「そもそもこの機能は必要か」を問い直すことと、必要なら別の認証フローを用意すること。微妙な脆弱性 #2: TOCTOU（Time-of-Check-Time-of-Use）async fn race_condition_get_order(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    user: AuthenticatedUser,    Path(order_id): Path\u003ci64\u003e,) -\u003e Result\u003cJson\u003cOrder\u003e, AppError\u003e {    let user_id = \u0026user.0.sub;    // Step 1: 注文を取得（全件から）    let order = state.db.get_order_by_id(order_id)?        .ok_or_else(|| AppError::NotFound(...))?;    // ↑ この時点で機密データがメモリに載っている！    // Step 2: 所有者をチェック    if order.user != *user_id {        // エラーメッセージが情報を漏らす        return Err(AppError::Forbidden(format!(            \"Order {} belongs to another user\",  // 存在することを教えてしまう            order_id        )));    }    Ok(Json(order))}何が問題なのか。データをフェッチしてから認可チェックしている。認可が通らなくても、データは既にメモリ上にあるエラーメッセージが情報を漏らす。「存在しない」と「アクセス権がない」が区別できるログに所有者情報が残る。認可失敗時のログにorder_owner = order.userを出力している正しい順序は「認可チェック → データフェッチ」だが、「IDだけでは認可チェックできない」という理由でこの順序になりがち。解決策はDB層でget_order_by_id_for_userのように、フェッチと認可を一体化すること。微妙な脆弱性 #3: 認可前のログ出力async fn logging_before_auth_get_order(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    user: AuthenticatedUser,    Path(order_id): Path\u003ci64\u003e,) -\u003e Result\u003cJson\u003cOrder\u003e, AppError\u003e {    // 「監査のために全リクエストをログに残す」という要件から    let order = state.db.get_order_by_id(order_id)?;    // 認可チェック前に詳細をログ出力    if let Some(ref o) = order {        tracing::info!(            order_id = o.id,            order_user = o.user,       // 誰の注文かログに残る            order_product = o.product, // 何を買ったかログに残る            requester = user.0.sub,            \"Order access attempted\"        );    }    // ここで認可チェック（でも遅い）    let order = order.ok_or_else(|| AppError::NotFound(...))?;    if order.user != user.0.sub {        return Err(AppError::Forbidden(\"Access denied\".to_string()));    }    Ok(Json(order))}ログは「セキュリティのために残す」という意図だが、認可前にログを取ると攻撃者がアクセスできないデータがログに残る。これは情報漏洩だ。ログ収集基盤に脆弱性があった場合、このログから機密情報が漏れる。正しいパターンを示す。認可前のログは「誰が」「何にアクセスしようとしたか（IDのみ）」認可後のログは詳細情報を含めてOK実際に攻撃してみる# サーバー起動cargo run --release --bin bola-demo# Bobのトークンを取得BOB_TOKEN=$(curl -s http://localhost:8080/token/bob | jq -r .access_token)# 脆弱なエンドポイント: BobがAliceの注文(ID=1)を取得curl -H \"Authorization: Bearer $BOB_TOKEN\" \\     http://localhost:8080/vulnerable/orders/1結果を見てみよう。{  \"id\": 1,  \"user\": \"alice\",  \"product\": \"Widget A\",  \"quantity\": 5}Bobが、Aliceの注文データを取得できてしまった。 Aliceは知らない。Bobは黙っている。システムは何も気づいていない。これが現実のインシデントだったら、ニュースになるやつだ。安全なエンドポイントでは以下のようになる。curl -H \"Authorization: Bearer $BOB_TOKEN\" \\     http://localhost:8080/orders/1結果はこうなる。{  \"error\": \"Order 1 not found or access denied\"}404を返している点もポイントだ。「なんで403（Forbidden）じゃないのか」という疑問があるだろう。403は「その注文は存在するよ。しかしお前には見せない」という意味である404は「何の話だ。そんな注文知らないが」という意味である403は「存在する」という情報を漏らしている。攻撃者にヒントを与えないためには404のほうが適切だ。API2: Broken Authentication - JWT検証の問題「署名さえ正しければOK」という誤解を打ち砕くデモ。https://owasp.org/API-Security/editions/2023/en/0xa2-broken-authentication/owasp.orgなぜJWT検証で失敗するのかJWTは「署名で改ざんを検出できる」という特性から、安全だと誤解されやすい。しかし、JWTのセキュリティは署名検証だけでは不十分だ。以下の検証がすべて必要だ。 検証項目  何をチェックするか  省略するとどうなるか  署名 (signature)  トークンが改ざんされていないか  偽造トークンが通る  有効期限 (exp)  トークンが期限内か  永久に使えるトークンが発生  発行者 (iss)  正当な発行者が作ったか  他システムのトークンが通る  オーディエンス (aud)  このAPIで使うべきか  別サービスのトークンが通る  Not Before (nbf)  まだ使用開始前ではないか  未来のトークンが先に使える JWTに関する危険な誤解「署名が正しければ安全」 → 署名は「改ざんされていない」だけで「使っていい」は別の話「JWTライブラリを使えば安全」 → デフォルト設定が安全とは限らない「短い有効期限だから大丈夫」 → expチェックを無効にしていたら意味がない「リフレッシュトークンで更新するから」 → 古いアクセストークンが使えたら問題cargo run --release --bin broken-auth-demo脆弱な実装：署名以外を検証しない/// VULNERABLE: Validates JWT signature but skips claim validationasync fn vulnerable_validate_token(headers: HeaderMap) -\u003e Result\u003cJson\u003cTokenValidationResponse\u003e, AppError\u003e {    // ...    // VULNERABLE: Disable all validation except signature    let mut validation = Validation::new(Algorithm::HS256);    validation.validate_exp = false; // 有効期限チェックしない！    validation.validate_aud = false; // audience チェックしない！    validation.required_spec_claims.clear(); // 必須クレームなし！    let result = decode::\u003cUserClaims\u003e(        token,        \u0026DecodingKey::from_secret(JWT_SECRET.as_bytes()),        \u0026validation,    );    // ...}これが危険な理由：期限切れトークンが使い放題（退職した社員のトークンが永久に有効）別サービス用のトークンが使える（audがチェックされないため）なりすましトークンが通る（issがチェックされないため）安全な実装：全クレームを検証/// SECURE: Properly validates all JWT claimsasync fn secure_validate_token(headers: HeaderMap) -\u003e Result\u003cJson\u003cTokenValidationResponse\u003e, AppError\u003e {    // ...    // SECURE: Enable all validation    let mut validation = Validation::new(Algorithm::HS256);    validation.set_audience(\u0026[JWT_AUDIENCE]);  // この API 用か？    validation.set_issuer(\u0026[JWT_ISSUER]);      // 正当な発行者か？    validation.validate_exp = true;             // 期限内か？    let result = decode::\u003cUserClaims\u003e(        token,        \u0026DecodingKey::from_secret(JWT_SECRET.as_bytes()),        \u0026validation,    );    // ...}テスト用トークン生成このデモでは4種類のトークンを生成できる。async fn generate_test_token(Path(token_type): Path\u003cString\u003e) -\u003e Result\u003cJson\u003cTokenInfo\u003e, AppError\u003e {    let (claims, description) = match token_type.as_str() {        \"valid\" =\u003e {            // 有効なトークン（1時間後に期限切れ）            let claims = UserClaims {                exp: (Utc::now() + Duration::hours(1)).timestamp() as usize,                aud: Some(JWT_AUDIENCE.to_string()),                iss: Some(JWT_ISSUER.to_string()),                // ...            };            (claims, \"Valid token - expires in 1 hour\")        }        \"expired\" =\u003e {            // 期限切れトークン（1時間前に期限切れ）            let claims = UserClaims {                exp: (Utc::now() - Duration::hours(1)).timestamp() as usize, // 過去！                // ...            };            (claims, \"Expired token - expired 1 hour ago\")        }        \"wrong-audience\" =\u003e {            // 別サービス用のトークン            let claims = UserClaims {                aud: Some(\"https://wrong-audience.com\".to_string()), // 別のサービス！                // ...            };            (claims, \"Token with wrong audience\")        }        \"wrong-issuer\" =\u003e {            // 不正な発行者のトークン            let claims = UserClaims {                iss: Some(\"https://malicious-issuer.com\".to_string()), // 偽者！                // ...            };            (claims, \"Token with wrong issuer\")        }        // ...    };}攻撃シナリオを試してみよう。# 期限切れトークンを取得EXPIRED=$(curl -s http://localhost:8080/token/expired | jq -r .access_token)# 脆弱なエンドポイント → 通る！curl -H \"Authorization: Bearer $EXPIRED\" http://localhost:8080/vulnerable/validate# 安全なエンドポイント → 401 Unauthorizedcurl -H \"Authorization: Bearer $EXPIRED\" http://localhost:8080/validate微妙な脆弱性：JWT検証の巧妙なバイパス「全クレームを検証しているから安全」と思っていないだろうか。残念ながら、JWT検証にはもっと狡猾な問題がある。微妙な脆弱性 #1: アルゴリズム混同攻撃/// 開発者の意図: 「RS256もHS256もサポートして柔軟に」/// 現実: RS256の公開鍵をHS256の秘密鍵として使われるasync fn subtle_alg_confusion(headers: HeaderMap) -\u003e Result\u003c...\u003e {    let header = jsonwebtoken::decode_header(token)?;    // BUG: トークンが主張するアルゴリズムを信用    let mut validation = Validation::new(header.alg);  // ← header.alg を信用！    validation.set_audience(\u0026[JWT_AUDIENCE]);    validation.set_issuer(\u0026[JWT_ISSUER]);    // 攻撃:    // 1. サーバーのRS256公開鍵を取得（公開されてる）    // 2. その公開鍵をHS256の秘密鍵として使ってトークン署名    // 3. {\"alg\": \"HS256\"} としてサーバーに送信    // 4. サーバーは公開鍵を「HS256の秘密鍵」として検証 → 成功！    let result = decode::\u003cUserClaims\u003e(        token,        \u0026DecodingKey::from_secret(JWT_SECRET.as_bytes()),        \u0026validation,    );}対策：アルゴリズムは固定値で指定。トークンのalgヘッダーを信用してはいけない。微妙な脆弱性 #2: Key ID (kid) インジェクション/// 開発者の意図: 「kidヘッダーで鍵を選択」/// 現実: kidに任意の値を入れられるasync fn subtle_kid_injection(headers: HeaderMap) -\u003e Result\u003c...\u003e {    let header = jsonwebtoken::decode_header(token)?;    // BUG: kidを検証なしで使用    let kid = header.kid.unwrap_or_else(|| \"default\".to_string());    // 実際の脆弱なコード例：    // SQLインジェクション: kid = \"key1' OR '1'='1\"    // let key = db.query(f\"SELECT key FROM keys WHERE id = '{kid}'\");    // パストラバーサル: kid = \"../../../etc/passwd\"    // let key = fs::read(format!(\"/keys/{}.pem\", kid));    // NULLキー: kid = \"../../dev/null\"    // 空のキーで署名検証 → 常に成功}kidは信頼できない入力。許可リスト方式でキーを選択するべき。微妙な脆弱性 #3: JKU (JWK Set URL) バイパス/// 開発者の意図: 「JKUヘッダーから公開鍵を取得」/// 現実: 攻撃者のサーバーから鍵を取得させられるasync fn subtle_jku_bypass(headers: HeaderMap) -\u003e Result\u003c...\u003e {    let header = jsonwebtoken::decode_header(token)?;    if let Some(jku) = header.jku {        // BUG: 弱いチェック        let allowed_prefix = \"https://auth.example.com\";        if jku.starts_with(allowed_prefix) {            // 攻撃:            // jku = \"https://auth.example.com.attacker.com/keys\"            // jku = \"https://auth.example.com@attacker.com/keys\"            // jku = \"https://auth.example.com%2F@attacker.com/keys\"            // 全部 starts_with チェックを通過！            let keys = fetch_jwks_from_url(\u0026jku).await?;            // 攻撃者の公開鍵を取得 → 攻撃者が署名したトークンが有効に        }    }}JKUは使わないか、完全一致でURLをチェックするべき。微妙な脆弱性 #4: Not-Before (nbf) 未検証/// 開発者の意図: 「expさえチェックすれば大丈夫」/// 現実: 未来用に発行されたトークンが今使えるasync fn subtle_nbf_skip(headers: HeaderMap) -\u003e Result\u003c...\u003e {    let mut validation = Validation::new(Algorithm::HS256);    validation.set_audience(\u0026[JWT_AUDIENCE]);    validation.set_issuer(\u0026[JWT_ISSUER]);    validation.validate_exp = true;    validation.validate_nbf = false;  // BUG: nbfを検証しない    // 攻撃シナリオ:    // 1. 管理者が「来月1日から有効」なトークンを事前発行    // 2. そのトークンが漏洩    // 3. 攻撃者は今すぐそのトークンを使用 → nbf無視で成功    // または:    // 1. 内部犯行者が未来日付のトークンを大量に生成    // 2. 退職後にそれらを使用    // 3. expはチェックされるがnbfはスルー → アクセス成功}nbfクレームもexpと同様に重要。「まだ有効ではない」トークンを拒否しないと、事前発行されたトークンが悪用される。HS256 vs RS256JWT認証では2つの主要なアルゴリズムがある。// HS256: 同じ鍵で署名と検証（対称鍵）const HS256_SECRET: \u0026str = \"your-256-bit-secret-key-here-must-be-long-enough\";// RS256: 秘密鍵で署名、公開鍵で検証（非対称鍵）const RS256_PRIVATE_KEY: \u0026str = r#\"-----BEGIN PRIVATE KEY-----MIIEvgIBADANBgkqhkiG9w0BAQEFAASC...-----END PRIVATE KEY-----\"#;const RS256_PUBLIC_KEY: \u0026str = r#\"-----BEGIN PUBLIC KEY-----MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8A...-----END PUBLIC KEY-----\"#;なぜRS256が推奨されるのか。HS256は署名と検証に同じ鍵を使う。検証側にも秘密鍵が必要になり、漏洩リスクが高いRS256は署名に秘密鍵、検証に公開鍵を使う。公開鍵は配布しても安全なので、マイクロサービス向きAPI3: Mass Assignment - 見えないフィールドを操作されるこれは個人的に「一番やらかしやすい」脆弱性だ。そして「やらかしても気づきにくい」という意味で最も厄介だろう。https://owasp.org/API-Security/editions/2023/en/0xa3-broken-object-property-level-authorization/owasp.orgMass Assignmentとは何かMass Assignment（一括代入）は、クライアントから送られてきたデータを、サーバー側のオブジェクトにそのまま「一括で」割り当ててしまうことで発生する脆弱性だ。もともとはRuby on RailsやPHPのLaravelなど、「お手軽にCRUDを作れるフレームワーク」で頻発していた。これは「フォームのフィールドをそのままDBカラムにマッピング」する機能が便利すぎて、セキュリティを犠牲にしていた。Rustは型付けが厳格なので「安全」と思われがちだが、serdeでJSONをデシリアライズする際に同様の問題が発生しうる。serdeとはRustで最も広く使われているシリアライズ/デシリアライズ用のライブラリで、JSONなどのデータ形式とRustの構造体を相互変換できる。なぜ開発者はこのミスを犯すのか便利さの誘惑 - 「リクエストとモデルの型を同じにすればコードが減る」フィールド追加時の見落とし - DBにstatusカラムを追加 → Rustの構造体にも追加 → リクエスト型にも追加 → やらかし「デフォルト値があるから大丈夫」という誤解 - #[serde(default)]は「送られなかったら」デフォルト、「送られたら」その値テスト時の盲点 - 正常系では余分なフィールドを送らないので気づかない操作される可能性のあるフィールド攻撃者が狙う典型的なフィールドを挙げる。 フィールド  本来の用途  攻撃による悪用  status  処理状態管理  \"pending\" → \"approved\" で承認をバイパス  role  権限管理  \"user\" → \"admin\" で権限昇格  is_verified  検証フラグ  false → true で検証をスキップ  price  価格  1000 → 1 で値引き  user_id  所有者  他人のIDを指定してなりすまし  created_at  作成日時  過去の日付を指定して古いデータを偽装  id  主キー  既存IDを指定して上書き攻撃 例えば、支払い作成APIで、ユーザーが送ってきたJSONをそのまま使ってしまうケースを見てみよう。/// VULNERABLE: Accepts any fields from user input#[derive(Deserialize)]pub struct UnsafePaymentRequest {    pub amount: f64,    pub currency: String,    #[serde(default)]    pub status: Option\u003cString\u003e,  // ユーザーが設定可能になっている}async fn vulnerable_create_payment(    Json(req): Json\u003cUnsafePaymentRequest\u003e,) -\u003e Json\u003cPayment\u003e {    let payment = Payment {        id: Uuid::new_v4().to_string(),        amount: req.amount,        currency: req.currency,        status: req.status.unwrap_or_else(|| \"pending\".to_string()),        // ↑ ユーザーが\"approved\"を送ってきたらそのまま使っちゃう    };    Json(payment)}攻撃してみよう。curl -X POST http://localhost:8080/vulnerable/payments \\     -H \"Content-Type: application/json\" \\     -d '{\"amount\": 100, \"currency\": \"USD\", \"status\": \"approved\"}'結果は\"status\": \"approved\"であり、未払いの支払いが承認済みになった。支払いステータスを「承認済み」に設定して、実際には支払いをしない。システムは何も気づかない。対策: DTOを分けるDTOとは「Data Transfer Object」の略で、データを受け渡すための専用オブジェクトだ。ここでは「ユーザーからの入力を受け取るための構造体」と「内部処理で使う構造体」を分けるという意味で使っている。/// SECURE: Only accepts allowed fields#[derive(Deserialize)]pub struct CreatePaymentRequest {    pub amount: f64,    pub currency: String,    // statusフィールドは存在しない}async fn secure_create_payment(    Json(req): Json\u003cCreatePaymentRequest\u003e,) -\u003e Json\u003cPayment\u003e {    let payment = Payment::new(req.amount, req.currency);    // statusは常にサーバー側で\"pending\"に設定される    Json(payment)}入力用のDTOと内部用のモデルを分ける。コード量は増える。型定義は増える。でも、これが「自由度の高いAPI」と「セキュアなAPI」の違いだ。自由には責任が伴う。微妙なMass Assignment：serde flattenの罠「入力DTOを分けた」と言っても、実装の仕方次第で脆弱になる。微妙な脆弱性 #1: #[serde(flatten)]の問題#[derive(Deserialize, Serialize)]struct FlattenedPaymentRequest {    amount: f64,    currency: String,    // 「未知のフィールドをログに残したい」という意図    #[serde(flatten)]    extra_fields: HashMap\u003cString, serde_json::Value\u003e,}async fn subtle_flatten_payment(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    _user: AuthenticatedUser,    Json(req): Json\u003cFlattenedPaymentRequest\u003e,) -\u003e Result\u003cJson\u003cPayment\u003e, AppError\u003e {    let mut payment = Payment::new(req.amount, req.currency.clone());    // 「extra_fieldsに有効なstatusがあれば使おう」    // 開発者の意図：「クライアントの便宜を図る」    // 現実：Mass Assignmentの再来    if let Some(status) = req.extra_fields.get(\"status\") {        if let Some(s) = status.as_str() {            if [\"pending\", \"approved\", \"rejected\"].contains(\u0026s) {                payment.status = s.to_string();  // approved も有効な値！            }        }    }    state.db.create_payment(\u0026payment)?;    Ok(Json(payment))}#[serde(flatten)]とHashMapの組み合わせは便利だが、「未知のフィールドを捕捉する」という性質が裏目に出る。コードレビューでflattenを見たら警戒しよう。微妙な脆弱性 #2: 部分更新の罠PATCH（部分更新）エンドポイントは特に危険だ。#[derive(Deserialize)]struct PartialPaymentUpdate {    amount: Option\u003cf64\u003e,    currency: Option\u003cString\u003e,    // 「ユーザーが自分でキャンセルできるように」status を追加    #[serde(default)]    status: Option\u003cString\u003e,}async fn subtle_update_payment(    State(state): State\u003cArc\u003cAppState\u003e\u003e,    _user: AuthenticatedUser,    Path(payment_id): Path\u003cString\u003e,    Json(update): Json\u003cPartialPaymentUpdate\u003e,) -\u003e Result\u003cJson\u003cPayment\u003e, AppError\u003e {    let mut payment = state.db.get_payment_by_id(\u0026payment_id)?        .ok_or_else(|| AppError::NotFound(...))?;    // 部分更新ロジック    if let Some(amount) = update.amount {        payment.amount = amount;    }    if let Some(currency) = update.currency {        payment.currency = currency;    }    // 「キャンセルは許可、でも承認は決済システム経由のみ」のつもり    if let Some(status) = update.status {        if payment.status == \"pending\" \u0026\u0026 status == \"approved\" {            // 開発者：「pendingからapprovedへの遷移だけ許可」            // 現実：これがまさに攻撃者がやりたいこと！            payment.status = status;        } else if payment.status == \"pending\" \u0026\u0026 status == \"cancelled\" {            payment.status = status;        }    }    Ok(Json(payment))}条件分岐で「許可する遷移」を書いたつもりが、攻撃者が欲しいものを許可している。ロジックが複雑になるほど、こういうミスは見つけにくくなる。攻撃方法を見てみよう。# 支払いを作成PAYMENT_ID=$(curl -s -X POST http://localhost:8080/payments \\  -H \"Authorization: Bearer $TOKEN\" \\  -H \"Content-Type: application/json\" \\  -d '{\"amount\": 100, \"currency\": \"USD\"}' | jq -r .id)# 部分更新でステータスを承認済みにcurl -X POST \"http://localhost:8080/subtle/payments/$PAYMENT_ID\" \\  -H \"Authorization: Bearer $TOKEN\" \\  -H \"Content-Type: application/json\" \\  -d '{\"status\": \"approved\"}'実装で学んだこと1. 認証と認可は別物これは何度言っても足りない。認証: 「あなたは誰か」 → 「私はBobです」認可: 「Bobさん、あなたはこれをしていいのか」 → 「...ダメです」JWTを検証して「このユーザーは本物だ」とわかっても、「このユーザーがこのリソースにアクセスしていいか」は全く別の問題だ。会社のビルで例えるとこうだ。認証 = 社員証を見せて入館する認可 = サーバールームに入れるかどうか社員証を持っていても、全員がサーバールームに入れるわけではない。当たり前だ。でも、APIでは「認証してるから大丈夫」と言ってしまいがちなのだ。2. 404 vs 403認可エラーの際に403を返すか404を返すか。403: リソースの存在を明かしつつアクセスを拒否404: リソースの存在自体を隠すセキュリティ的には404が安全だ。403は「存在する」という情報を漏らしている。しかし、デバッグは困難になる。「404なんだけど、本当に存在しないのか、権限がないのか」がわからない。本番環境では404、開発環境では403にするとか、ログには詳細を残すとか、工夫が必要だ。3. DTOの分離は面倒だが必要入力用の構造体と内部用の構造体を分けるのは、確かに面倒だ。同じようなものを2回書くことになる。しかし、Mass Assignment攻撃を防ぐには必要なコストだ。Rustの場合、コンパイル時に型チェックされるので、「うっかりユーザー入力をそのまま使ってしまう」ミスは起きにくい。CreatePaymentRequestにstatusフィールドがなければ、コンパイラが「そんなフィールドないよ」と教えてくれる。これはRustの強みだ。動的型付け言語だと、こうはいかない。続きは後編へ → API4 (Rate Limit), API5 (BFLA), API7 (SSRF), 動作確認、まとめ参考リンクOWASP API Security Top 10 (2023)公式ドキュメント。owasp.orgaxum - Rust Web Framework本デモで使用しているWebフレームワーク。github.comjsonwebtoken - Rust JWT LibraryJWT認証の実装に使用。github.comthiserror - Rust Error Handlingエラー型の定義に使用。github.comJWT.ioJWTのデバッグ・検証ツール。jwt.ioRFC 7519 - JSON Web Token (JWT)JWTの仕様。datatracker.ietf.orgCWE-639: Authorization Bypass Through User-Controlled KeyBOLAに関連するCWEエントリ。cwe.mitre.orgCWE-915: Improperly Controlled Modification of Dynamically-Determined Object AttributesMass Assignmentに関連するCWEエントリ。cwe.mitre.org","isoDate":"2025-12-05T01:49:19.000Z","dateMiliSeconds":1764899359000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、テックブログを書け","link":"https://speakerdeck.com/nwiizo/oi-tetukuburoguwoshu-ke","contentSnippet":"2025年12月5日に「おい、テックブログを書け」という登壇をした。\r\r「おい」である。命令形である。30分間、人前に立って「書け」と言い続けるという、冷静に考えるとなかなか傲慢な振る舞いをしてきたわけだが、登壇資料を作っている最中、ふと気づいてしまった。書けと言っている自分は、なぜ書いているのだろうか、と。\r\r技術ブログを書くことについて語ろうとすると、それは私が「書いてきた」ことを晒すことに他ならず、AIとの付き合い方を語ろうとすると、それは私が「どう仕事をしているか」を開陳することと紙一重になる。そうなると聞いている側からすれば、こいつは結局、自分の話がしたいだけなのではないか、登壇という大義名分を得て気持ちよく自分語りをしているだけなのではないか、と思われても仕方がない。いや、実際そうなのかもしれない。そう見られることへの嫌悪感と、そう見られまいと振る舞う自分への嫌悪感が同時に存在していて、どちらに転んでも結局イヤなやつなのである。\r\rしかし登壇というのは厄介なもので、「書け」と命令するからには、自分がなぜ書いてきたのかを明かさなければ説得力がない。説得力のない登壇ほど空虚なものはない。空虚な登壇をする自分を想像して、それはそれで耐えられない。結局、自己開示から逃げられない構造になっている。なんという罠だろうか。\r\r身体性という言葉を使った。AIに記事を書かせることについて話した。私の答えは明確で、記事はほとんどAIに書かせている、しかし価値の源泉は私にある、と。私が素材を提供し、AIが構造化し、私がレビューして調整する。編集者としてのAI。この協働こそが現代の執筆だと、そう話した。話しながら、これは本当にそうだろうかと自分を疑う自分がいて、でもそういう迷いごと引き受けて喋るしかないのだった。\r\rまず自分のために書け、結果として、それが誰かを救う。そう締めくくった。\r\rhttps://forkwell.connpass.com/event/377267/\r\rhttps://syu-m-5151.hatenablog.com/archive/category/%E3%81%8A%E3%81%84%E3%80%81\r\r自宅からの昼登壇だったので、終わってから昼飯を食べに外に出た。参考書籍として紹介した本をもう一度読み返そうと思って、鞄に入れてきていた。店に向かう道すがら、本を開く。","isoDate":"2025-12-04T05:00:00.000Z","dateMiliSeconds":1764824400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、努力しろ","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/03/002023","contentSnippet":"はじめに「おい、がんばるな」という言葉を書いた。あの文章を読み返して、私は少し後悔している。syu-m-5151.hatenablog.com言いたいことは分かる。がむしゃらに頑張ることが思考停止になる。忙しさが逃避になる。持続可能性が大事だ。それは正しい。私も経験してきたことだ。でも、あの文章には、書かなかったことがある。書けなかったことがある。「頑張らなくていい」という言葉が、どれほど危険な響きを持っているか。その言葉が、どれほど簡単に、怠惰の免罪符になってしまうか。私は「頑張るな」と言った。でも、それを読んだ人の中に、こう受け取った人がいるだろう。「そうか、頑張らなくていいんだ」「無理しなくていいんだ」「今のままでいいんだ」と。もしそう受け取った人がいたら、それは私の責任だ。だから、今日は別のことを書く。「おい、努力しろ」これは、あの文章への補足ではない。あの文章への反論だ。「頑張るな」という言葉の危うさを、私は書かなければならない。そして、「頑張ること」と「努力すること」の違いを、もっと正確に伝えなければならない。あの文章で私が本当に言いたかったのは、「頑張るな」ではなかった。「考えずに頑張るな」だった。でも、その「考えずに」という部分が抜け落ちて伝わってしまったら、メッセージは正反対になる。「頑張らなくていい」は、時に正しい。でも、多くの場合、それは逃げだ。そして、私たちが本当に必要としているのは、「頑張らないこと」ではない。「正しく頑張ること」——つまり、努力することだ。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しい。「頑張らなくていい」という甘い毒「頑張らなくていい」という言葉は、優しく聞こえる。疲れ果てた人に、「もう頑張らなくていいよ」と言う。それは、救いの言葉だ。本当に限界に達している人には、その言葉が必要なときもある。でも、問題がある。この言葉は、本当に限界の人だけでなく、まだ余力がある人にも響いてしまうということだ。なぜか。人間は、楽な方に流れる生き物だからだ。これは誰もが持っている性質であり、責めるべきことではない。ただ、事実としてそうなのだ。「頑張らなくていい」と言われれば、「そうか、頑張らなくていいのか」と受け止める。そう感じること自体は自然だ。誰だって、許可があれば楽な方を選びたくなる。そして、頑張ることをやめる。でも、本当に頑張らなくてよかったのだろうか。ここで、正直に自分に問いかけてみてほしい。「頑張らなくていい」という言葉を聞いて、ホッとした時のことを思い出してほしい。その時、自分は本当に限界だっただろうか。本当に、これ以上一歩も進めない状態だっただろうか。体が動かない、頭が働かない、そういう状態だっただろうか。それとも、まだやれるのに、やらない言い訳を探していただけではなかっただろうか。私は、後者だったことが何度もある。疲れていた。それは本当だ。でも、限界ではなかった。もう少しやれば、もう少し先に進めた。「頑張らなくていい」という言葉が、私に許可を与えた。やめていい許可を。そして、私はやめた。その時は楽になった。肩の荷が降りた。「これでいいんだ」と感じた。でも、後から振り返ると、あの時やめなければよかったと後悔することがある。あと少し踏ん張っていれば、違う景色が見えただろう。あと少し続けていれば、突破口が開けただろう。「頑張らなくていい」は、甘い毒だ。本当に必要な人には薬になる。限界を超えて壊れそうな人には、その言葉が命を救うこともある。でも、必要でない人が飲むと、毒になる。成長の機会を奪い、可能性を閉じてしまう。そして、厄介なことに、自分が「本当に必要な人」なのかどうかは、自分では分からない。なぜなら、人間は自分に甘いからだ。自分の限界を、実際より低く見積もる傾向があるからだ。だから、この言葉は慎重に使わなければならない。そして、この言葉を聞いた時は、慎重に受け取らなければならない。「私は本当に限界なのか、それとも、逃げているだけなのか」。この問いから、逃げてはいけない。いまの自分にとって「頑張らなくていい」という言葉は、薬なのか、あるいは都合のいい麻酔なのか。その区別ができるのは、自分だけだ。誰かの優しさを、自分への甘さにすり替えるな。量をこなすことでしか見えないもの「甘い毒」の話をしてきた。次は、もう少し具体的な話をしたい。「量」の話だ。「おい、がんばるな」という文章で、私は「がむしゃらは若さの特権だ」と書いた。そして、「30歳からは戦略が必要だ」と書いた。これは、半分正しくて、半分間違っている。確かに、がむしゃらに量をこなすだけでは、どこかで限界が来る。効率を考えず、方向性を考えず、ただ時間を投入するだけでは、成果は出ない。それは正しい。でも、量をこなすことでしか見えないものがあるということを、私は書かなかった。どういうことか。何かを始めたばかりの頃、私たちは何も分からない。これは当然のことだ。何が正しいのか分からない。何が効率的なのか分からない。どの方向に進むべきか分からない。この状態で「効率」や「戦略」を考えても、意味がない。なぜか。効率や戦略を考えるためには、材料が必要だからだ。「このやり方は非効率だった」「あのやり方の方が良かった」という比較ができて、初めて効率が分かる。「この方向は間違いだった」「あの方向が正しかった」という経験があって、初めて戦略が立てられる。つまり、効率や戦略を語るためには、まず経験が必要なのだ。では、経験は、どこから来るのか。量をこなすことから来る。最初から効率的にやろうとすると、何が起きるか。何も始められなくなる。「どうやったら効率的か」を考えている間に、時間だけが過ぎていく。最適な方法を見つけようとして、いつまでも動き出せない。私はかつて、あるプログラミング言語を学ぼうとした時、まず「最も効率的な学習方法」を調べることに一週間を費やした。本を読み比べた。オンラインコースを比較した。学習ロードマップを作成した。「この本は評判がいい」「このコースは体系的だ」「こういう順序で学ぶべきだ」と、完璧な計画を立てようとした。一週間後、完璧な計画ができた。でも、一行もコードを書いていなかった。一方、別の言語を学んだ時は、何も考えずにチュートリアルを始めた。「とりあえずやってみよう」と思って、手を動かした。分からないところは飛ばした。エラーが出たら、エラーメッセージをググった。理解が曖昧なまま、とりあえず動くものを作った。非効率だった。無駄なことをたくさんした。後から「ああ、最初からこうすればよかった」と悔やむことが何度もあった。でも、後者の方が、圧倒的に速く身についた。なぜか。手を動かしていたからだ。手を動かすと、分からないことが具体化する。「何が分からないか分からない」という状態から、「これが分からない」という状態になる。そうなれば、調べようがある。学びようがある。これはエンジニアだけの話ではない。セールスも、CSも、デザイナーも、同じだ。セールスなら、100件の商談をこなして初めて「この業界の顧客は、この切り口で話すと響く」が分かる。CSなら、100件の問い合わせに対応して初めて「この機能のこの部分で、ユーザーはつまづく」が見えてくる。デザイナーなら、100個のプロトタイプを作って初めて「このパターンは使いやすい」という感覚が身につく。最初から「効率的なセールストーク」を設計しようとしても、机上の空論にしかならない。最初から「完璧なカスタマージャーニー」を描こうとしても、現実とズレる。まず量をこなすことで、何が効率的で、何が正しい方向なのかが、初めて見えてくる。これは、若者だけの話でもない。何か新しいことを始める時、誰もが初心者だ。30歳、40歳、50歳になっても、新しい領域に踏み出す時は、まず量をこなすしかない。「おい、がんばるな」で私が書いた「戦略」は、量をこなした後に見えてくるものだ。量をこなす前に戦略を立てようとしても、机上の空論にしかならない。だから、まず頑張れ。考えるのは、その後でいい。戦略を語りたければ、まず汗をかけ。効率を追求しすぎることの罠量をこなすことの価値を語った。では、量だけが大事なのか。そうではない。ここで、「効率」の話をしたい。「おい、がんばるな」で、私は効率の重要性を強調した。同じ成果を、より少ない投入で得ること。それが賢い働き方だと。これも、半分正しくて、半分間違っている。効率を追求することは、確かに重要だ。無駄なことに時間を使わない。最短距離で成果を出す。それは、賢いことだ。でも、効率を追求しすぎると、動けなくなるという罠がある。どういうことか。効率を追求するとは、「最小の投入で最大の成果を得ようとすること」だ。これ自体は良いことだ。でも、これを突き詰めると、どうなるか。「成果が保証されていないことには、投入しない」という態度になりやすい。なぜか。効率の計算をするためには、投入と成果の関係が見えている必要がある。「これだけ投入すれば、これだけの成果が得られる」という予測ができて、初めて効率が計算できる。だから、効率を重視するあまり、「成果が予測できること」だけを選ぶようになる。「この作業は、本当に必要か？成果につながるか？」「このアプローチは、本当に効率的か？もっと良い方法はないか？」「この投資は、本当にリターンがあるか？損をしないか？」。こう考え始めると、確実にリターンがあることにしか、時間を使えなくなる。でも、ここで立ち止まって考えてほしい。人生で最も価値のあるものは、リターンの不確実なものが多いのではないだろうか。新しいスキルを学ぶ。そのスキルが役に立つかどうかは、学ぶ前には分からない。学んでみて、使ってみて、初めて分かる。新しい人間関係を築く。その関係が実を結ぶかどうかは、関係を築く前には分からない。時間をかけて、信頼を積み重ねて、初めて分かる。新しいプロジェクトを始める。そのプロジェクトが成功するかどうかは、始める前には分からない。やってみて、失敗して、修正して、初めて分かる。効率を追求しすぎると、これらの「不確実なこと」に時間を使えなくなる。確実にリターンがあることだけをやるようになる。すると、どうなるか。安全な場所から出られなくなる。今までやってきたこと。確実にできること。リスクのないこと。そういうものだけをやり続ける。その結果、成長がない。変化がない。じわじわと、世界が狭くなっていく。新しいことに挑戦しないから、新しい可能性が開かれない。私は、効率を追求するあまり、「無駄なこと」を一切しなくなった時期がある。仕事に直接関係のない本は読まない。読んでも仕事の成果につながらないから。すぐに役立たない技術は学ばない。学んでも今の仕事では使わないから。「これは何の役に立つのか」が説明できないことには、時間を使わない。説明できないということは、効率が計算できないということだから。確かに、目の前の仕事は効率的にこなせるようになった。無駄がなくなった。短時間で成果が出るようになった。でも、新しいアイデアが浮かばなくなった。視野が狭くなった。仕事は回せるけど、面白い発想ができなくなった。つまらない人間になっていった。なぜか。「無駄」の中にこそ、予想外の価値があるからだ。一見無駄に見える読書が、思わぬところで仕事に活きる。すぐに役立たない技術が、数年後には大きな武器になる。「何の役に立つか分からない」経験が、人間としての厚みを作る。効率だけを追求すると、その「予想外の価値」を取りこぼしてしまう。だから、時には非効率を許容しろ。時には「何の役に立つか分からないこと」に時間を使え。それが、長期的には最も効率的な投資になることがある。効率やリターンが見えないことの中に、本当は心のどこかで「それでもやってみたい」と思っているものがないだろうか。その声を、効率という物差しで測って、黙らせていないだろうか。計算できないものにこそ、人生を変える何かが潜んでいる。「持続可能性」という名の逃げ道効率の話をしてきた。次は、もう1つの「賢そうな言葉」について考えたい。「持続可能性」だ。「おい、がんばるな」で、私は持続可能性の重要性を説いた。無理をしない。長く続けられるペースで。燃え尽きないように。これは正しい。燃え尽きて動けなくなったら、意味がない。長く続けることは確かに大事だ。でも、この言葉が逃げ道になることがある。どういうことか。「持続可能なペースで」と言うと、それは賢明に聞こえる。長期的な視点を持っている。自分を大切にしている。無理をしない。計画的だ。でも、その「持続可能なペース」は、本当に適切なのだろうか。ここで、人間の心理について考えてみたい。私たちは、自分の限界を過小評価しがちだ。「これ以上やったら壊れる」と感じる地点は、実際の限界よりもずっと手前にあることが多い。なぜか。人間は、不快なことを避けたい生き物だからだ。辛いこと、苦しいこと、面倒なことは、できれば避けたい。これは自然な感情だ。だから、実際に壊れるよりもずっと手前で、「もう限界だ」と感じてしまう。まだ余力があるのに、「これ以上は無理だ」と思ってしまう。「持続可能なペース」という言葉を使う時、私たちは無意識に、その「過小評価された限界」を基準にしていないだろうか。本当は、もう少し頑張れる。もう少し踏ん張れる。でも、「持続可能性」という言葉を使って、その踏ん張りを回避していないだろうか。「持続可能性」は、時に「楽をするための言い訳」になる。もちろん、本当に限界の人はいる。本当に休まなければならない人はいる。その人たちにとって、「持続可能性」は正当な理由だ。そういう人に「もっと頑張れ」と言うのは、暴力だ。でも、全員がそうではない。まだ余力があるのに、「持続可能性」を理由にブレーキをかけている人もいる。ここで、もう1つ正直に自分に問いかけてみてほしい。「持続可能なペース」と言った時、それは本当に「長期的に最適なペース」なのか。それとも、「今、楽でいられるペース」なのか。この2つは、似ているようで、全く違う。長期的に最適なペースは、時に短期的には辛い。なぜか。成長するためには、今の自分を超える必要があるからだ。今の自分を超えるためには、今の自分には辛いことをする必要がある。筋肉を鍛える時のことを考えてみてほしい。筋肉は、負荷をかけて、一度壊れて、修復される過程で強くなる。楽な負荷だけかけていても、筋肉は成長しない。能力も同じだ。今できることだけやっていても、能力は成長しない。今できないこと、今の自分には辛いことに挑戦して、初めて成長する。「持続可能性」を盾にして、その「辛いこと」を避けていたら、成長はない。踏ん張るべき時には、踏ん張れ。いつでも快適でいようとするな。不快さの中にこそ、成長がある。最近、「持続可能性のため」と言ってブレーキを踏んだ場面を思い出してほしい。それは本当に長期のためだっただろうか。それとも、今ラクでいたい自分のためだっただろうか。答えは、自分の中にしかない。「持続可能性」は免罪符じゃない。逃げ道を正当化する言葉でもない。苦しみの中でしか得られないものここまで、「甘い毒」「量」「効率」「持続可能性」の話をしてきた。これらに共通するのは、「苦しみとどう向き合うか」という問いだ。次は、その「苦しみ」について、もう少し直接的に語りたい。「おい、がんばるな」で、私は「苦しみを美化するな」と書いた。苦しむこと自体には価値がない。同じ成果を楽に得られるなら、その方がいいと。これも、半分正しくて、半分間違っている。確かに、苦しむこと自体を目的にするのは間違っている。苦しめば偉いわけではない。苦労すれば成果が出るわけではない。無意味な苦しみは、ただの消耗だ。でも、苦しみの中でしか得られないものがあるということも、事実だ。それは何か。自分が何者であるかを知ることだ。どういうことか。人間は、追い込まれた時に、本当の自分が出る。楽な時、余裕がある時には、本当の自分は見えない。余裕があると、取り繕える。自分を良く見せられる。でも、苦しみの中では、取り繕う余裕がなくなる。本当の自分が、否応なく姿を現す。自分は、どこまで耐えられるのか。限界だと思った先に、まだ力が残っているのか。自分は、何を諦められないのか。何を捨てても、これだけは手放せないというものは何なのか。自分は、何のために頑張れるのか。お金のためか、評価のためか、それとも、もっと別の何かのためか。これらの問いに対する答えは、快適な場所にいては見つからない。不快な場所に身を置いて、初めて見えてくる。私は、あるプロジェクトで、本当に追い込まれた経験がある。締め切りは迫っている。スケジュールは遅延している。チームは疲弊している。メンバーの顔に疲労が見える。問題は山積みだ。1つ解決すると、別の問題が浮上する。毎日が綱渡りだった。辛かった。何度も逃げ出したいと思った。「こんなの、持続可能じゃない」と思った。「なんでこんなことをしているんだろう」と思った。でも、あの経験がなければ、今の自分はいない。これはエンジニアだけの話ではない。セールスなら、どうしても落とせない大型案件に挑み続けた経験。何度も断られ、それでも食らいついた経験。その中で「自分は何のために営業をしているのか」が見えてくる。CSなら、クレームが殺到した時期を乗り越えた経験。理不尽に怒られ、なお丁寧に対応し続けた経験。その中で「自分はどこまでユーザーに寄り添えるのか」が見えてくる。現場で働くすべての人に、そういう経験がある。あの時、自分が何を大切にしているのかが分かった。チームのために最後まで踏ん張りたいと思っている自分がいた。良いものを作りたいと思っている自分がいた。自分がどこまで頑張れるのかが分かった。「もう無理だ」と思ったところから、より三歩進めた。限界だと思っていたところは、限界ではなかった。そして、自分がそこまで頑張れるという自信が、あの経験から生まれた。この自信は、快適な場所では得られない。苦しみを乗り越えた経験からしか得られない。「あの時、あれだけ辛いことを乗り越えた」という記憶は、次の困難へ立ち向かう力になる。「あの時できたのだから、今回もできる」という自信は、前へ進む勇気になる。だから、苦しみを避けるな。もちろん、無意味な苦しみは避けるべきだ。方向が間違っているなら、修正すべきだ。でも、正しい方向に進んでいるなら、苦しみを恐れるな。その苦しみの中に、あなたのまだ知らない自分がいる。苦しみを避けて到達する場所に、本当の自分はいない。「休むこと」を過大評価していた苦しみの話をしてきた。では、苦しみの反対にある「休息」は、どうだろうか。「おい、がんばるな」で、私は休むことの重要性を強調した。休憩は投資だ。睡眠は投資だ。休むことで、生産性が上がると。これは正しい。休息は大事だ。睡眠不足は判断力を鈍らせる。疲労は生産性を下げる。でも、休むことを過大評価していたという反省もある。どういうことか。休むことが重要なのは、その後にまた頑張るためだ。休息は、次の活動のための準備だ。体を回復させ、頭をリフレッシュさせ、また動き出すための準備だ。つまり、休息の価値は「その後の活動」によって決まる。休んだ後に何もしないなら、休息の意味がない。でも、「休むことが大事」という言葉を聞くと、休むこと自体が目的になってしまうことがある。「今日は休む日だから、何もしない」「疲れているから、休まなきゃ」「持続可能性のために、休息を取る」。そう言いながら、ずっと休んでいる。次の活動が、いつまでも始まらない。休息は、活動のための手段だ。休息自体が目的ではない。この区別を忘れると、「休むこと」が「何もしないこと」にすり替わってしまう。私は、「休息も投資だ」と言いながら、実際には逃避していた時期がある。「今日は休む」と言いながら、本当は面倒なことを避けていた。やるべきことがあるのに、「疲れているから」と言って、やらなかった。「持続可能性のため」と言いながら、実際には楽をしていた。もう少し頑張れる状態なのに、「無理は禁物だから」と言って、手を抜いた。休息と逃避は、外からは区別がつかない。どちらも「何もしていない」ように見える。区別できるのは、自分だけだ。これはエンジニアだけの話ではない。セールスなら、「今日は疲れているから、あのリードへの連絡は明日にしよう」と言い続けて、結局連絡しないまま案件を逃すことがある。CSなら、「この問い合わせは複雑だから、体調が良い時に対応しよう」と言い続けて、対応が遅れてユーザーの信頼を失うことがある。どの職種でも、「休息」と「先延ばし」の境界は曖昧だ。自分に正直に問いかけてほしい。今、休んでいるのは、次に頑張るための準備なのか。それとも、頑張ることから逃げているだけなのか。この2つは、外見は同じでも、本質は全く違う。次に頑張るための休息には、終わりがある。回復したら、また動き出す。頑張ることからの逃避には、終わりがない。いつまでも「まだ疲れている」「まだ準備ができていない」と言い続ける。前者なら、休め。後者なら、立ち上がれ。休息は充電だ。放電しないなら、充電する意味はない。「考えること」を言い訳にするな休息の話をしてきた。次は、もう1つの「賢そうな行為」について考えたい。「考えること」だ。「おい、がんばるな」で、私は「考えること」の重要性を説いた。がむしゃらに動くな。立ち止まって考えろ。方向性を確認しろと。これは正しい。考えずに動くと、間違った方向に全力で進んでしまう。それは危険だ。でも、「考えること」が行動しない言い訳になることがある。どういうことか。「まだ考えがまとまっていない」「もう少し情報が必要だ」「方向性を確認してから動きたい」。こう言いながら、いつまでも動かない人がいる。考えることは大事だ。でも、考えているだけでは、何も起きない。なぜか。世界は、行動によってしか変わらないからだ。頭の中でどれだけ完璧な計画を立てても、行動しなければ、現実は何も変わらない。素晴らしいアイデアがあっても、実行しなければ、ただの妄想だ。そして、皮肉なことに、行動しないと、本当に必要な情報は手に入らない。何かを始める前は、何が分からないかも分からない。何が問題になるかも分からない。どこが難しいかも分からない。頭の中で考えているだけでは、これは分からない。机上で計画を立てているだけでは、見えてこない。実際にやってみて初めて分かる。手を動かし、困難にぶつかり、失敗して初めて「ああ、ここが問題だったのか」と分かる。だから、「もっと考えてから」「もっと情報を集めてから」と言い続けていると、永遠に動き出せない。必要な情報は、動き出さないと手に入らないからだ。これはエンジニアだけの話ではない。セールスなら、「この業界のことをもっと調べてから提案しよう」と言い続けて、結局一度も商談に臨まないことがある。しかし、実際に商談に出て、顧客の反応を見て、初めて「この業界は価格よりもサポート体制を重視する」が分かる。CSなら、「この機能の仕様をもっと理解してから対応しよう」と言い続けて、結局ユーザーを待たせてしまうことがある。ただ、実際に対応しながら調べ、先輩に聞くことで「この機能は、こういう使い方をするユーザーがいる」と分かる。どの職種でも、動くことでしか得られない知識がある。これは鶏と卵のような問題に見えるだろう。動くためには情報が必要だ。しかし、情報を得るためには動く必要がある。どうすればいいのか。答えは、不完全なまま動き始めることだ。完璧な計画を待つな。不完全なまま始めろ。間違っているだろう。失敗するだろう。それでも、始めなければ、何も始まらない。動きながら考えろ。走りながら修正しろ。考えることと動くことは、どちらか一方ではない。順番に行うものでもない。両方同時にやるものだ。動きながら考え、考えながら動く。そうすることで、より良い方向に、より速く進める。「まだ準備ができていない」「もう少し考えてから」と言って先送りしていることがあるなら、立ち止まって考えてみてほしい。それは本当に考える段階なのか。それとも、動くことを怖がっているだけなのか。考えることと、考えているふりをして逃げることは、違う。準備が整う日は、永遠に来ない。来たと思える日は、動き始めた後にしか訪れない。では、何が「努力」なのかここまで、「頑張らなくていい」という言葉の危うさを書いてきた。量をこなすことの価値。効率を追求しすぎることの罠。持続可能性が逃げ道になること。苦しみの中でしか得られないもの。休むことの過大評価。考えることが言い訳になること。では、結局、何をすればいいのか。ここで、「頑張ること」と「努力すること」を区別したい。頑張ることは、「とにかくやること」だ。方向も考えず、効率も考えず、ただ時間とエネルギーを投入する。がむしゃらに動く。汗をかく。疲れる。これは「おい、がんばるな」で批判したことであり、確かに問題がある。方向が間違っていたら、どれだけ頑張っても成果は出ない。努力することは、「考えながらやること」だ。方向を意識し、フィードバックを得て、修正しながら進む。効率を考える。戦略を立てる。ただ、考えるだけでなく、実際に動く。これは、頑張ることとは違う。しかし、努力には「やること」が含まれている。ここが重要なポイントだ。「考えること」だけでは、努力ではない。「やること」が必要だ。そして、「やること」には、しばしば苦しみが伴う。不快さが伴う。疲労が伴う。それを避けていたら、努力にはならない。努力とは、正しい方向に向かって、苦しみを引き受けながら、行動し続けることだ。もう少し分解して説明しよう。まず、「正しい方向に向かって」。これは、考えることだ。自分は何を達成したいのか。どこに向かいたいのか。そのためには、何をすべきか。これを考える。次に、「苦しみを引き受けながら」。これは、踏ん張ることだ。辛くても、やる。不快でも、続ける。逃げ出したくなっても、踏みとどまる。そして、「行動し続ける」。これは、動くことだ。考えるだけでなく、実際に手を動かす。失敗しても、また動く。続ける。この三つが揃って、初めて「努力」になる。これはどの職種でも同じだ。エンジニアなら、正しいアーキテクチャを考え、難しいバグと格闘しながら、コードを書き続ける。セールスなら、顧客の課題を考え、断られる辛さを引き受けながら、提案を続ける。CSなら、ユーザーの真のニーズを考え、クレームの辛さを引き受けながら、対応を続ける。デザイナーなら、ユーザー体験を考え、何度もダメ出しされる辛さを引き受けながら、デザインを続ける。どの仕事でも、努力の構造は同じだ。「頑張るな」と言って、苦しみを避けることを正当化してはいけない。苦しみは、努力の一部だ。「考えろ」と言って、行動しないことを正当化してはいけない。行動は、努力の一部だ。方向を考えながら、苦しみを引き受けながら、行動し続ける。それが、努力だ。楽をしながら成長はできない。考えるだけで変わることもできない。誘惑という名の逃げ道努力の定義をした。正しい方向に向かって、苦しみを引き受けながら、行動し続けること。それが努力だと書いた。しかし、ここで正直に認めなければならないことがある。努力するのは、難しい。なぜか。現代社会には、努力から逃げるための誘惑が溢れているからだ。スマホを開けばSNSが待っている。通知が鳴り続ける。動画は自動再生される。情報は洪水のように押し寄せる。疲れた時、辛い時、つい手が伸びる。「ちょっと休憩」と言いながら、気づけば1時間、2時間が過ぎている。これは、休息ではない。逃避だ。先ほど「休むことの過大評価」の話をした。ここでも同じことが起きている。私たちは「少し気分転換」と言いながら、実際には努力から逃げている。ここで、1つの考え方を紹介したい。ジェイ・シェティという作家がいる。彼は実際に僧侶として修行した経験を持ち、その経験をもとに「モンク思考」という考え方を世界に広めた。私たちはつい、他人と年収を比べたり、社会的なイメージで仕事を選んだりしてしまう。「成功とはこういうもの」「幸せとはこういうもの」という外側からの定義に、無意識に縛られている。しかし、本当はどのような人生を送りたいのか。本当はどのような人間になりたいのか。この問いに、自分の言葉で答えられるだろうか。彼が説くのは、「手放す」「成長する」「与える」という3つのステップだ。まず、執着を手放す。他人の評価、過去の成功体験、「こうあるべき」というプレッシャー。これらを握りしめていると、本当に大切なものが見えなくなる。次に、自分の情熱と才能に向き合う。何をしている時に時間を忘れるか。何に取り組んでいる時に充実感を感じるか。他人の期待ではなく、自分の内側から湧き上がるものを見つける。そして、目的を持って生きる。自分のためだけに努力するのではなく、誰かのために、何かのために努力する。その方が、長く続く。強く踏ん張れる。この考え方の核心は、「小さなノー」の積み重ねだ。SNSを見ない。無駄な飲み会を断る。ダラダラとネットサーフィンしない。1つ1つは小さな「ノー」だ。しかし、この小さな「ノー」を積み重ねることで、本当に大切なことに「イエス」と言えるようになる。誘惑に「ノー」と言うことで、努力に「イエス」と言える。私たちは、誘惑に負けるたび、自分を少しずつ裏切っている。「今日くらいいいか」「疲れているから仕方ない」「明日から頑張ろう」。そう言いながら、努力から逃げている。その言い訳を、いつまで続けるのか。永遠に僧侶のように生きる必要はない。ただ、誘惑を言い訳にするのをやめろ。集中できないのは環境のせいではない。自分が誘惑を選んでいるだけだ。スマホを閉じろ。通知をオフにしろ。そして、今やるべきことに向き合え。それが、努力の第一歩だ。踏ん張るべき時に踏ん張れ努力の定義をした。最後に、1つのことを言いたい。人生には、踏ん張るべき時がある。チャンスは、いつでも来るわけではない。絶好の機会は、そう何度もあるわけではない。その時が来た時に踏ん張れるかどうかで、人生は変わる。踏ん張るべき時に「持続可能性が」と言って引いてしまったら、チャンスを逃す。踏ん張るべき時に「効率が」と言って計算してしまったら、大事なものを取りこぼす。踏ん張るべき時に「休息が」と言って立ち止まってしまったら、流れに乗れない。踏ん張るべき時には、理屈を超えて、踏ん張れ。これはどの職種でも同じだ。エンジニアなら、リリース前の追い込み、障害対応、重要な技術選定の議論。セールスなら、年度末のクロージング、大型案件のコンペ、重要な顧客との交渉。CSなら、大規模障害時のユーザー対応、重要顧客の離脱防止、クリティカルなクレームへの対応。どの仕事にも、「ここが勝負所」という瞬間がある。その瞬間に踏ん張れるかどうかで、キャリアは変わる。もちろん、いつも踏ん張れとは言わない。いつも踏ん張っていたら、壊れる。それは「おい、がんばるな」で書いた通りだ。だからこそ、踏ん張るべき時を見極めることが大事だ。普段は力を温存し、ペースを守り、回復する時間を取る。そして、その時が来たら、全力で踏ん張ることが大事だ。温存していた力を、すべて出し切る。「おい、がんばるな」は、「いつも踏ん張っている人」に向けた言葉だった。常にアクセル全開で、休むことを知らない人。そういう人には、確かに「踏ん張りすぎるな」と言う必要がある。一方で、世の中には、踏ん張るべき時に踏ん張れない人もいる。チャンスが来ても、「疲れているから」「リスクがあるから」「まだ準備ができていないから」と言って、見送ってしまう人。そういう人に「頑張らなくていい」と言ったら、それは間違ったメッセージになる。自分がどちらのタイプか、正直に考えてほしい。いつも踏ん張りすぎて疲弊しているなら、少し力を抜いていい。しかし、踏ん張るべき時に踏ん張れていないなら、今こそ踏ん張る時だ。この一年を振り返ってみてほしい。「あそこであと一歩踏ん張っていれば」と、未来の自分に言われそうな場面はないだろうか。もしあるなら、それが答えだ。次にその場面が来た時、同じ後悔をしないために、今から準備しておくことだ。チャンスは、準備している人のところにしか来ない。来ても、踏ん張れなければ、すり抜けていく。何もしなくても誰かがお膳立てしてくれて、機会が向こうからやってくる。そんな恵まれた環境が、いつまでも続くと信じるな。続いたとしても、それは成長ではない。ただの停滞だ。ここで、厳しいことを言う。世の中は理不尽で、不公平だ。生まれた環境も、与えられた才能も、巡ってくる機会も、平等ではない。それは事実だ。口で何を言っても、不満を並べても、愚痴をこぼしても、その現実は変わらない。SNSで正論を叫んでも、飲み会で上司の悪口を言っても、世の中は1ミリも動かない。行動しなければ、努力しなければ、状況は何も変わらない。これは冷たい言葉ではない。むしろ、希望の言葉だ。なぜなら、行動すれば変わる可能性があるということだからだ。理不尽な世界の中で、自分の手で変えられるものがある。それが、努力だ。ここで、1つの反論が聞こえてくる。「そもそも、このゲーム自体がおかしいのではないか」と。努力すれば成功者が増えるのか。全員が頑張れば、全員が報われるのか。答えはノーだ。構造的に、成功者の席は限られている。全員が努力しても、椅子取りゲームの椅子は増えない。格差は縮まるどころか、広がり続ける。能力主義という名のレースは、走れば走るほど、差が開いていく仕組みになっている。それは、経済学的にも、社会学的にも、既に答えが出ている話だ。では、このゲームから降りればいいのか。「こんな不公平なレースには参加しない」と宣言すればいいのか。私は、その選択を否定しない。降りる自由はある。しかし、自分に問いかけてみてほしい。降りたところで、何が開けるのか。レースから降りた先に、別の人生があるのか。不参加を表明したところで、この社会の中で生きていくことに変わりはない。構造を批判しながら、その構造の中で生きていく。それが、大半の人間の現実だ。だから私は、こう考える。ゲームがおかしいことは分かっている。ルールが不公平なことも分かっている。それでも、このゲームの中で生きていく以上、このゲームの中での戦い方を身につけるしかない。構造を変えることは、個人の努力ではほぼ不可能だ。でも、構造の中での自分の位置を変えることは、できる可能性がある。それが、努力だ。大事なのは、その理不尽さや不公平さを、腹の底から受け入れることだ。「なぜ自分だけ」「もっと恵まれていれば」という思いを抱えたまま努力しても、どこかで折れる。被害者意識を持ったまま走っても、長くは続かない。世の中が不公平であることを認めた上で、それでも前に進む。不公平を嘆く暇があるなら、その時間で一歩でも進め。理不尽に怒るエネルギーがあるなら、そのエネルギーを努力に変えろ。それが、この不完全な世界で生き抜くための唯一の方法だ。努力せずに目標が達成できると、本気で信じているなら教えてほしい。努力もせずに、この淀んだ自分という檻から抜け出せると、本気で信じているなら教えてほしい。私は信じていない。自分を変えるには、努力が必要だ。今の自分を超えるには、苦しみを引き受ける必要がある。檻から出るには、その困難を押し続ける必要がある。それを避けて、「頑張らなくていい」という言葉に逃げ込んでも、檻は壊れない。自分は変わらない。淀んだ水は、そのまま淀み続ける。努力なしに変われると信じるな。苦しみなしに成長できると信じるな。檻を壊すのは、他の誰でもない、自分自身だ。ここまで厳しいことを書いてきた。しかし、1つだけ、白状させてほしい。私は、自分のことを特別だと思えたことがない。ふとした瞬間に気づく。ああ、俺は凡人だな、と。天才じゃない。選ばれた側の人間でもない。器には限界がある。どうしようもなく、限界がある。周りを見れば、自分より優秀な人間なんていくらでもいる。悔しいが、事実だ。そして、もう1つ。万全の状態で仕事に臨める日なんて、一生来ない。体調が悪い。眠れていない。私生活がぐちゃぐちゃだ。そんな日の方が、圧倒的に多い。それでも、やる。最悪の日であっても、最低限の水準は守る。それがプロだ。凡人だから、積み上げるしかない。万全を待っていたら何も始まらないから、不完全なままでも動ける自分を作るしかない。おわりに「おい、がんばるな」と書いた。今日は「おい、努力しろ」と書いた。矛盾しているように見えるだろう。しかし、矛盾していない。どちらも、同じことを言っている。「考えずに頑張るな」「ただし、考えながら頑張れ」。これを一言で言えば、「努力しろ」だ。努力には、考えることが含まれている。方向を意識することが含まれている。フィードバックを得ることが含まれている。同時に、努力には、行動することも含まれている。苦しみを引き受けることも含まれている。踏ん張ることも含まれている。「頑張るな」という言葉だけを受け取って、行動しなくなってはいけない。苦しみを避けてはいけない。踏ん張ることをやめてはいけない。考えながら、頑張れ。方向を意識しながら、踏ん張れ。それが、努力だ。「おい、がんばるな」は、片面だけを描いた絵だった。今日は、もう片面を描いた。両方を見て、初めて全体が見える。——と言いたいところだが、正直に言えば、これでもまだ全体ではない。この問題には、2つの面だけでなく、もっと多くの面がある。私が見えていない角度がある。私が経験していない状況がある。私が想像すらできていない視点がある。たとえば、心身の病を抱えている人にとって、「努力しろ」という言葉がどう響くか。私には、本当の意味では分からない。あるいは、社会的な制約の中で選択肢が限られている人にとって、「踏ん張れ」という言葉がどう響くか。私には、本当の意味では分かっていない。私が書いたのは、私の経験から見えた2つの面に過ぎない。他にも面はある。3つ目も、4つ目も、おそらくもっとたくさんある。それは自覚している。だから、この文章を「正解」として読まないでほしい。これは、1つの視点だ。私という人間が、私の経験を通して見た、1つの景色だ。あなたには、あなたの景色がある。あなたの経験から見える面がある。それは、私には見えない面だろう。あなたが今、どちらの言葉を必要としているかは、あなた自身にしか分からない。頑張りすぎて疲弊しているなら、「おい、がんばるな」を読んでほしい。頑張れずに停滞しているなら、「おい、努力しろ」を読んでほしい。どちらの状態にいても、前に進むことをやめるな。前に進むとは、行動することだ。考えることだ。苦しみを引き受けることだ。そして、それを続けることだ。おい、努力しろ。考えながら、頑張れ。方向を見据えながら、踏ん張れ。休みながらも、また立ち上がれ。それが、あなたを前に進ませる唯一の方法だ。参考書籍バカと無知 (新潮新書)作者:橘　玲新潮社Amazon知ってるつもり　無知の科学 (ハヤカワ文庫NF)作者:スティーブン スローマン,フィリップ ファーンバック早川書房Amazon実力も運のうち　能力主義は正義か？ (ハヤカワ文庫NF)作者:マイケル サンデル早川書房Amazonデジタル・ミニマリスト　スマホに依存しない生き方 (ハヤカワ文庫NF)作者:カル ニューポート早川書房AmazonSLOW　仕事の減らし方――「本当に大切なこと」に頭を使うための３つのヒント作者:カル・ニューポートダイヤモンド社Amazon大事なことに集中する―――気が散るものだらけの世界で生産性を最大化する科学的方法作者:カル・ニューポートダイヤモンド社Amazon深い集中を取り戻せ――集中の超プロがたどり着いた、ハックより瞑想より大事なこと作者:井上一鷹ダイヤモンド社Amazonジェームズ・クリアー式 複利で伸びる1つの習慣作者:ジェームズ・クリアーパンローリング株式会社Amazonクリティカル・ビジネス・パラダイム――社会運動とビジネスの交わるところ作者:山口 周プレジデント社Amazon人生の経営戦略――自分の人生を自分で考えて生きるための戦略コンセプト２０作者:山口 周ダイヤモンド社Amazon知的戦闘力を高める 独学の技法作者:山口 周ダイヤモンド社Amazonモンク思考―自分に集中する技術作者:ジェイ・シェティ東洋経済新報社AmazonSENSE FULNESS　どんなスキルでも最速で磨く「マスタリーの法則」作者:スコット・Ｈ・ヤング,小林　啓倫朝日新聞出版Amazon新版　究極の鍛錬作者:ジョフ・コルヴァンサンマーク出版Amazon心眼――あなたは見ているようで見ていない作者:クリスチャン・マスビアウプレジデント社AmazonQUEST「質問」の哲学――「究極の知性」と「勇敢な思考」をもたらす作者:エルケ・ヴィスダイヤモンド社Amazon資本主義が人類最高の発明である：グローバル化と自由市場が私たちを救う理由作者:ヨハン・ノルベリニューズピックスAmazon資本主義にとって倫理とは何か作者:ジョセフ・ヒース,瀧澤弘和慶應義塾大学出版会Amazon","isoDate":"2025-12-02T15:20:23.000Z","dateMiliSeconds":1764688823000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"生成AIエージェントによるブログレビュー環境の構築（下）","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/03/001146","contentSnippet":"この記事は、3-shake Advent Calendar 2025 3日目のエントリ記事です。上巻の振り返り上巻では、Commandsを使ったブログレビュー環境の基礎を説明しました。/deep-thinking-prompt で書く前に深く考える/blog-quality-review で6つの観点からレビューする/ai-humanity-check でAIっぽさを検出する/full-review で全自動レビューするこれらのCommandsは、レビュー観点を構造化し、一貫性を担保してくれます。syu-m-5151.hatenablog.com下巻では、より高度なSubagentsの活用へ入る前に、いくつかの話題を深掘りします。AIに記事を書かせるとは何か「AIに記事を書かせる」という言葉をめぐって、しばしば議論が起きます。「それは本当にあなたの記事なのか」「AIが書いたものに価値があるのか」。私の答えは明確です。記事はほとんどAIに書かせています。しかし、価値の源泉は私にあります。手書きで書いているという人も別に紙に直接書いている訳ではないでしょう。既に、予測変換やLSP（Language Server Protocol）による補完など、さまざまなレベルで「AIやコンピュータの支援」を受けながら文章を書いています。その延長線上に、生成AIによる執筆があるに過ぎません。では、私は何を担っているのでしょうか。「身体性」を供給しています。ここで言う身体性とは、知識が「情報」から「経験」へと変容する過程で生じる、一人称的な認知の軌跡です。知識と経験の断絶たとえば、あるエンジニアがRustの所有権システムを学んでいるとします。The Bookを読み、概念は「理解した」つもりでいました。しかしいざコードを書くと、コンパイラからcannot borrow as mutable...というエラーを食らいます。「ルールは知っているはずなのに、なぜ」——この「知っている」と「書ける」の間にある断絶こそが、身体性が欠落している状態です。そして、その断絶を越えた瞬間の記録があります。「なぜエラーになったのか格闘し、イテレータの内部構造に気づき、腹落ちした瞬間」——これこそが身体性を伴った学習の言語化です。それは他者に伝達可能な「生きた知見」となります。この「苦闘から理解への遷移（プロセス）」だけは、AIには生成できません。AIは私の代わりに試行錯誤できませんし、私の代わりとしてコンパイラに叱られて悔しがることもできないからです。AIの役割は、私が供給した「生の体験（身体性）」を、他者が読める文章として整えることにあります。混沌とした思考を構造化し、読者にとって消化しやすい形に変換します。それは編集者の仕事に近いです。私が素材（身体性）を提供し、AIが構造化し、私がレビューして調整します。この協働のプロセス全体が、現代における「執筆」なのです。「流暢な嘘」という罠一方で、「AIで書いた記事には価値がない」という批判も、ある意味では正しいです。問題の本質は「AIを使ったこと」ではなく、「検証というプロセスが抜け落ちていること」にあります。AIに丸投げして出力された文章には、不正確な情報の垂れ流しという致命的なリスクが潜みます。厄介なのは、AIの生成する文章が文法的に完璧で、論理の構成も美しすぎることです。人間が書いた拙い文章なら「この人、理解していないな」と直感的に警戒できます。しかし、AIの出力は「もっともらしさ（Plausibility）」に特化しているため、嘘であってもスルスルと頭に入ってきてしまいます。これを検証せずに公開するのは、ブレーキの効かない車を公道に放つようなものです。LLMは確率的に「次の単語」を選んでいるに過ぎません。そこに真偽への誠実さは存在しません。だからこそ、その確率の波を制御し、事実という地面に杭を打つのは、人間にしかできない仕事です。私たちは、AIというエンジンの出力に酔うのではなく、冷静な「監修者」であり続けなければなりません。しかし、この監修作業を人間の力だけで行うには限界があります。だからこそ、「AIを監視するAI」が必要になるのです。それがこれから紹介する「Sub-agents」によるレビュー体制です。Commandsの限界とSub-agentsの登場上巻で紹介したCommands（/blog-quality-reviewなど）は便利ですが、長く使っていると2つの困難にぶつかります。コンテキストの枯渇: 長文記事に対し、複数の観点で深いレビューを繰り返すと、メインの会話履歴（コンテキストウィンドウ）がすぐに溢れてしまう。専門性の欠如: 1つのプロンプトにあらゆる指示を詰め込むと、焦点がぼやけ、鋭い指摘ができなくなる。そこで導入したのが、Claude Codeの強力な機能、Sub-agentsです。Sub-agentsとは何かhttps://code.claude.com/docs/en/sub-agents:embed:citeSub-agentsは、特定のタスクに特化した自律的なAIワーカーです。これまでの「Commands（定型文の挿入）」とは、根本的にアーキテクチャが異なります。1. コンテキストの分離（Context Isolation）これが最大にして最強のメリットです。通常、長い記事をレビューさせると、「思考過程」や「中間生成物」でメインの会話履歴が埋め尽くされてしまいます。しかしSub-agentsは、メインとは独立した別のコンテキストウィンドウで作業します。完全にレビュワーに徹することができます。もちろんデメリットもあるので使い分けが必要です。User │ ▼Main Agent │ [Delegate] 記事テキストを渡し、レビューを依頼 ▼Sub-Agent (Reviewer) ┃ ★独自のコンテキストで思考★ ┃ 1. 全文読み込み ┃ 2. 批判的検討 ┃ 3. 推敲（ここのトークンはメインには見えない） ┃ ▼Main Agent (レビュー結果の要約のみを受取) │ ▼User (修正案の提示)メインエージェントが受け取るのは、Sub-agentが導き出した「結論」だけです。これにより、メインのコンテキストを汚染することなく、大量のトークンを使った深い推論が可能になります。2. 自律的な委譲（Delegation）Commandsはユーザーが手動で呼び出すものですが、Sub-agentsはメインのエージェント（Orchestrator）が必要だと判断した時に自動的に呼び出されます。「この記事、なんか読みづらいから直して」と指示するだけで、メインエージェントが「これは『文章校正エージェント』と『構成作家エージェント』の出番だ」と判断し、仕事を割り振ります。私が実際に配備しているSub-agents私は現在、ブログ執筆チームとして以下のSub-agentsを .claude/agents/ に配備しています。実際にはもっといますが、今回は3つだけ実際に使っているものを紹介します。1. narrative-architect.md （物語構造の専門家）技術記事であっても、読者の感情を動かす「物語」が必要です。このエージェントは、技術的な正しさには口を出しません。その代わり、「読者の感情の旅路（Emotional Journey）」だけを見ます。役割: 導入で共感を得られているか。解決策の提示でカタルシスがあるか。指摘例: 「機能の説明は正確だが、読者が抱えている『辛さ』への共感が不足しており、解決策の価値が伝わりにくい」2. fresh-eye-reviewer.md （永遠の初学者）私の「書き手の呪い」を解くためのエージェントです。ペルソナとして「実務未経験のジュニアエンジニア」が埋め込まれています。役割: 専門用語の困難、論理の飛躍、「なぜ」という素朴な疑問の発見。特徴: 文脈をあえて読まない。「ここまでの説明では、この単語の意味がわからない」と冷徹に指摘する。3. ai-police.md （AI警察）「AIっぽさ」を検知し、排除する専門官です。AIが生成した文章特有の「過剰な接続詞」「中身のない美しいまとめ」「冗長な言い回し」を検挙します。役割: テキストの人間らしさ（Humanity Score）の判定。指摘例: 「『〜ということができる』は冗長だ。『〜できる』と言い切るべき。また、この段落の『いかがでしたか』はAI臭いので削除を推奨する」実践：レビュー体制の構築これらのSub-agentsを連携させることで、私のブログ執筆フローは完全に変わりました。ディレクトリ構造.claude/├── commands/           # ユーザーが叩くショートカット│   └── full-review.md  # 全体を統括する指示書└── agents/             # 自律的に動く専門家たち    ├── narrative-architect.md    ├── fresh-eye-reviewer.md    └── ai-police.mdレビューの流れStep 1: 執筆（協働）私とメインエージェントで対話しながら、記事のドラフトを作成します。私は身体性（エピソード）を話し、エージェントがそれを整えます。Step 2: 全自動レビュー（委譲）書き上がったドラフトに対し、私は一言こう告げるだけです。「/full-review を実行して」すると、メインエージェントが裏側で複数のSub-agentsを起動します。Fresh Eye が「ここがわからない」と文句を言う。Narrative Architect が「構成が退屈だ」と指摘する。AI Police が「AIっぽい表現がある」と警告する。Step 3: 統合と修正メインエージェントは、これらのバラバラな意見を統合し、優先順位をつけて私に提示します。「初学者にとって難解な部分があり、かつAI特有の冗長な表現が残っています。まずは第2章の具体例を修正しましょう」私はその統合されたレポートを見て、最後に修正します。まとめ上巻から下巻を通じて、生成AIエージェントを用いたブログレビュー環境の構築について解説してきました。上巻: ブログの評価基準をCommandsで構造化し、手動レビューの面倒臭さを解消する方法。下巻: Sub-agentsを用いてコンテキストを分離し、専門特化した「編集チーム」を作る方法。この環境を構築して気づいたのは、私の仕事が「執筆者（Writer）」から「編集長（Editor in Chief）」へとシフトしたということです。実際に手を動かして書く（Generate）のはAIでしょう。しかし、「何を書くか（企画）」「なぜ書くか（熱量）」「品質は十分か（承認）」を判断するのは、人間にしかできません。AIエージェントは、我々から仕事を奪うものではありません。我々を、より高次な意思決定を行う「マネージャー」へと押し上げてくれる存在です。もしあなたが「記事を書くのが面倒だ」「自分の文章に自信がない」と感じているなら、まずは小さなCommandを1つ作ることから始めてみてください。そこには、孤独な執筆作業とは違う、頼れるバディとの協働が待っているはずです。","isoDate":"2025-12-02T15:11:46.000Z","dateMiliSeconds":1764688306000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、がんばるな","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/02/124702","contentSnippet":"はじめに先日、久しぶりに会った友人に言われた。「なんか最近、顔が疲れてない？」と。私は「まあ、仕事が忙しくて」と答えた。友人は「頑張ってるんだね」と言って、ビールを一口飲んだ。頑張ってる。その言葉を聞いた瞬間、なぜか胸のあたりがざわついた。褒められているはずなのに、全然嬉しくない。むしろ、何かを見透かされたような、居心地の悪さがあった。帰り道、ずっと考えていた。私は確かに頑張っている。毎日遅くまで働いているし、休日も勉強しているし、やるべきことは山ほどある。でも、だから何なんだろう。頑張っているから、何なんだ。30歳になった。節目だとか、大人になったとか、そういう感慨は特にない。ただ、20代の頃とは何かが決定的に違う。何が違うのか、最初はよく分からなかった。体力が落ちたとか、徹夜ができなくなったとか、そういう分かりやすい話でもない。しばらく考えて、ようやく気づいた。「頑張っている」という言葉が、免罪符にならなくなったのだ。20代の頃は、頑張っていれば許された。成果が出なくても、方向が間違っていても、「でも頑張ってるから」で何とかなった。周りもそう言ってくれたし、自分でもそう信じていた。頑張ることそのものに価値がある、と。でも30歳になって、その魔法が解けた。頑張っているのに何も変わらない自分がいて、頑張っているのに評価されない現実があって、頑張っているのに前に進んでいない焦りがある。頑張ることが、こんなにも虚しいとは思わなかった。これは、そういう話だ。頑張ることをやめろという話ではない。頑張り方を変えろという話でもない。ただ、「頑張っている」という言葉の正体について、30歳になった私が考えたことを書いてみようと思う。読んでも何も解決しないかもしれない。でも、同じようなことを感じている人がいたら、少しだけ楽になるかもしれない。そういう気持ちで書いている。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。頑張ることの正体30歳の誕生日の夜、窓の外を眺めながら「今日も、頑張った」と思いました。でも、その言葉の後に続くはずの達成感はありませんでした。頑張りで全てを説明しようとしていた朝から晩まで働いていました。画面を見つめ、会議に出て、そこから開発をしていました。体は確かに疲れています。なのに、何も前に進んでいないという感覚が胸の奥に重く沈んでいるのです。社会人として8年が経ちました。20代前半の私は「頑張っている自分」が好きでした。努力している姿が自分の価値を証明してくれると思っていたからです。朝誰よりも早く出社し、夜遅くまで残り、休日も勉強する。その生き方が正しいと信じていました。しかし最近、ある事実に気づいてしまったのです。頑張ることそれ自体が、いつの間にか目的になっていたということに。本来、頑張ることは手段であるはずです。何かを達成するため、何かを得るため、どこかに到達するための手段。しかしいつの間にか、頑張ること自体が目的にすり替わっていました。「頑張っている自分」でいることが目的になり、その先に何があるのかを問うことをやめていたのです。ふと考えてしまいます。もし努力が一切報われない世界だったとしても、私はそれでもなお「頑張りたい」と願うだろうか。結果のために頑張っているのか。それとも、頑張ること自体が自分の生き方なのか。この2つは似ているようで、まったく違います。前者であれば、結果が出なければ頑張りは無意味になります。だから私たちは結果を求め、結果が出ないと焦り、自分を責めます。しかし後者であれば、結果に関係なく、頑張ること自体に意味があります。たとえ報われなくても、その過程に価値を見出すことができます。私は長い間、自分は後者だと思っていました。「努力することに意味がある」と信じていたからです。しかし正直に自分を見つめると、違いました。私は結果を求めていました。評価を求めていました。だから結果が出ないと苦しくなり、評価されないと自分を否定したくなったのです。もし本当に「頑張ること自体が生き方」なのだとしたら、結果が出なくても穏やかでいられるはず。しかし私はそうではなかった。頑張ることは純粋な生き方ではなく、結果を得るための手段だったのです。手段であるならば、その手段が有効かどうかを確かめなければなりません。目的地に近づいているかどうかを確認しなければなりません。しかし私は、頑張ること自体を目的にすり替えることで、そこを考えることから逃げていたのです。全部やろうとした結果具体的な話をさせてください。社会人になって数年目のことです。私は様々なことに挑戦させてもらっていました。自分の案件、登壇、ブログ執筆。それにまた、輪読会の運営、勉強会の主催、社内ドキュメントの管理と整備、新卒採用の担当。文脈のない色んなことを並列でやっていました。全部やりたかったのです。全部できると思っていました。結果として、全てが中途半端になりました。輪読会は準備不足で進行がグダグダになり、参加者が気まずそうに沈黙する場面が何度もありました。勉強会は告知が遅れて参加者が集まらず、3人しかいない会場で虚しくスライドをめくりました。ドキュメントは途中まで書いて放置され、それを指摘されることもないまま死にドキュメントが増えていきました。採用面談では候補者の情報を十分に把握できていないまま臨んでしまい、的外れな質問をして相手を困惑させました。自分の案件も遅れ、登壇の準備も直前までバタバタし、ブログは下書きのまま溜まっていきました。どれも「ちょっとずつダメ」だったのです。致命的な失敗ではない。でも、どれも胸を張って「やり遂げた」とは言えない。そして厄介なことに、中途半端にやっている間は、誰からもフィードバックをもらえなかったのです。なぜでしょうか。私が「頑張っているように見えた」からです。人は頑張っている人に「中途半端だ」とは言いにくいものです。遅くまで残っている。色々なことを引き受けている。一生懸命やっている。そういう姿を見ると、たとえ成果が出ていなくても「まあ、頑張ってるし」と見逃してしまう。指摘する側も遠慮してしまうのです。だから私は、自分が中途半端であることに気づけませんでした。周りも言ってくれないし、自分でも「頑張っている」という事実で目が曇っていたのです。ここで気づいたことがあります。私が選んだことだけでなく、選ばずに放置していたものが、私の人生を形作っていたということです。何かを選ぶとき、私たちは選んだものに意識を向けます。しかし、選ばなかったもの、手を付けずに残してしまったものについては、あまり考えません。でも実際には、その「選ばなかったもの」が積み重なって、今の自分を作っています。私の場合、「深く集中する時間」を選ばずに放置していました。「1つのことに没頭する経験」を選ばずに放置していました。全部やろうとすることで、何も深くやらないという選択を、無意識のうちにしていたのです。選択の影にあるもの——それを自覚することが、変わるための第一歩でした。総量が同じなら全部できるタイプの人もいるでしょう。器用にタスクを切り替えて、それぞれに必要な集中を注げる人。でも私は、おそらくそういうタイプではなかったのです。1つのことに深く集中しているときは力を発揮できる。でも、複数のことを並列で抱えると、どれにも集中できなくなる。頭の中が常に「あれもやらなきゃ、これもやらなきゃ」で埋まっていて、目の前のことに没頭できない。問題は、私が怠けていたことではありませんでした。全部やろうとしすぎていたことだったのです。そしてもう1つ気づいたことがあります。私が盲目的に全部を抱え込んでいる間、周りの人にも迷惑をかけていたということです。中途半端な準備で運営した勉強会に参加してくれた人たち。私の遅れのせいでスケジュールを調整しなければならなかったチームメンバー。頑張ることは、時に暴力になります。自分だけでなく、周りの人も苦しめてしまうのです。頑張らないことへの恐怖こうした経験があっても、頑張ることをやめるのは難しい。頑張ることに疲れたと思った瞬間、罪悪感が襲ってきます。「頑張らないなんて怠け者だ」「頑張らなかったら停滞してしまう」。心の中で誰かの声が私を責めるのです。この恐怖はどこから来るのでしょうか。少し立ち止まって考えてみると、そこには1つの混同があることに気づきます。私たちは「頑張らないこと」と「怠けること」を同じものだと思い込んでいるのです。しかしある時気づきました。頑張らないことと怠けることは違い、そして頑張ることと前に進むことも違うのだということに。これを整理すると、こうなります。「頑張る」とは、エネルギーを注ぎ込むことです。「前に進む」とは、目的地に近づくことです。そして「怠ける」とは、必要なことをしないことです。エネルギーを注ぎ込んでも、方向が間違っていたら目的地には近づきません。逆に、エネルギーを節約しても、正しい方向に進んでいれば目的地に近づくことができます。頑張りすぎて何も達成できないより、戦略的に力を抜いて1つを確実に達成した方が価値がある。頑張らないことへの恐怖を掘り下げていくと、その根底にあるのは「失敗への恐れ」でした。しかし、よりその下を掘ると、本質的な恐怖が見えてきます。私が本当に恐れていたのは、失敗そのものだったのか。それとも、「誰かに失敗を見られること」だったのか。私が本当に恐れていたのは、失敗そのものではありませんでした。失敗を誰かに見られること、「あいつは頑張らなかったから失敗した」と思われること、それが怖かったのです。一人で挑戦して一人で失敗するのは、実はそこまで怖くありません。痛いけれど、学びになります。しかし、その失敗を誰かに目撃されること、評価されること、噂されること——それが耐えられなかったのです。つまり、私の恐怖の本質は「社会的評価への恐れ」でした。自分自身の内側の痛みではなく、他者の目に映る自分の像への恐れだったのです。この区別は重要です。なぜなら、恐怖の正体を知ることで、対処の仕方が変わるからです。失敗そのものが怖いのであれば、リスクを減らす工夫をすればいい。しかし「失敗を見られること」が怖いのであれば、問題は失敗ではなく、他者の評価に自分の価値を預けすぎていることにあります。頑張ることをやめて考えることを始めた時、初めて前に進み始め、結果を出せるようになりました。頑張ることへの依存頭では分かっていても、頑張ることをやめられませんでした。私はたぶん、頑張ることに依存していたのです。朝起きるとすぐに仕事を始め、休憩も取らずに夜遅くまで働いて疲れ果てて眠り、土日も「せっかくの時間だから」と何かをしていました。「何もしない時間」が怖かったのです。なぜ怖かったのか。それは、何もしていない自分に価値がないと思っていたからです。この考えをもう少し掘り下げてみましょう。私は無意識のうちに、「自分の価値 = 自分がどれだけ頑張っているか」という等式を信じていました。頑張っていない自分は価値がない。価値のない自分を見たくない。そう感じていたから、常に何かをしている必要があり、頑張っている自分でいる必要があったのです。「頑張らなければ価値がない自分」と、「頑張っていなくてもここにいていい自分」。この2つのうち、私は本当はどちらを生きたいのだろう。これは「どちらが正しいか」という論理の問題ではありません。「どちらを選びたいか」という願望の問題です。頭では「頑張っていなくても価値がある」と分かっています。そう言われれば、そうです。でも、本当にそれを信じているかと問われると、自信がありません。心のどこかで「でも頑張らないと......」という声がするのです。その声の正体を知ることが、変わるための第一歩でした。答えは、すぐには出ませんでした。でも、この疑問を抱え続けることが大切でした。論理ではなく願望のレベルで、自分が何を求めているのかを探ること。それが、変わるための出発点になったのです。しかし不思議なことに、頑張れば頑張るほど、成果は出なくなっていきました。うまくいかない理由は頑張りすぎていたからです。頑張ることが思考を停止させていて、「とりあえず頑張る」「とにかく動く」と考えることから逃げていたのです。「頑張ります」という特権振り返ってみると、若い頃の私にはある種の特権がありました。「頑張ります」と言えば、それで許されていたのです。計画が甘くても「頑張ります」、ミスをしても「頑張ります」、結果が出なくても「頑張ります」と言えば許されていました。周囲は「若いんだから」「まだ経験が浅いんだから」「熱意があればいい」と納得してくれたのです。20代前半は特にそうでした。何も考えずにとにかく動き、深夜まで働き、休日も出社していれば評価されました。方向性が間違っていても、やり方が非効率でも、「頑張っている」という事実が全てを覆い隠してくれたのです。「頑張ります」は、思考停止の免罪符でした。考えなくてよく、戦略を立てなくてよく、ただ熱意を見せればよかったのです。がむしゃらは若さという資本で買えた特権だったのです。そしてその「がむしゃら」が、ある種の万能感を生んでいました。体力や気力は無限にあり、睡眠を削っても平気で、理想の自分に向かって駆け上がっていく。そんな勢いが許されていて、いやむしろ求められていたのです。今、手放せずに握りしめている「頑張り」は、本当に自分を守っているのだろうか。それとも、もう要らなくなった古い防具なのだろうか。かつて「頑張ること」は、私を守ってくれました。若くて経験がなくて、何も分からない時期に、「とにかく頑張る」という姿勢は、私の居場所を確保してくれました。がむしゃらに動くことで、「あいつは一生懸命やっている」と認めてもらえたのです。しかし、時間が経ちました。状況が変わりました。求められることも変わりました。かつて私を守ってくれた防具が、今は私の動きを制限しているのではないか。重すぎて前に進めなくなっているのではないか。そう考え始めた時、その防具を一度外してみる勇気が必要でした。転換点という現実しかしその「がむしゃらが許される特別な時間」は、予告なく終わります。私の場合、それは20代後半でした。ある日突然、それまで当たり前にできていたことができなくなりました。朝起きることも人と話すことも簡単な判断さえも重荷になって、「頑張ります」と言ってももう体が動かなくなったのです。今思えば、それはいつか必ず訪れる終わりでした。30歳という年齢は、「頑張ります」だけでは通用しなくなる境界線なのです。この変化はいくつかの形で現れます。まず、周囲の目が変わります。「頑張っている」だけでは評価されなくなります。「で、結果は」「で、どう改善するの」「がむしゃらにやるんじゃなくて、戦略は」と容赦なく聞かれるようになります。30歳は、熱意ではなく戦略が問われる年齢でした。「頑張っている」と「前に進んでいる」は別物だったのです。次に、身体の限界が見えてきます。20代のように無理が効かなくなり、深夜まで働いたら翌日に響き、休日を潰したら週明けのパフォーマンスが落ちます。がむしゃらはもはやコストの方が大きいのです。そして何より、自分自身が「このまま走り続けることに意味があるのか」と考え始めます。がむしゃらに頑張っても前に進んでおらず、ただ消耗しているだけ。そんな実感が、重くのしかかってくるのです。走り続けることと、前に進むことは違う。この当たり前の事実に、私は30歳になってようやく気づきました。なぜ私たちは頑張ってしまうのかしかし、なぜ私たちはそもそもこうなってしまうのでしょうか。なぜ、頑張ってしまうのでしょうか。私なりの答えは、簡単な答えが欲しいからというものです。どういうことか説明させてください。私たちが生きている現実は複雑です。何が正しいのか分からない。どの選択が最善なのか分からない。努力が報われるかどうかも分からない。そういう不確実性の中で生きることは、とても不安なことです。その不安に耐えられないとき、私たちは「頑張れば救われる」という単純で分かりやすい物語の中に逃げ込みます。この物語の中では、何をすべきかが明確です。とにかく頑張ればいい。努力すればいい。諦めなければいい。ネガティブ・ケイパビリティという言葉があります。不確実さや曖昧さに耐える能力のことです。「自分にもあるだろう」などと言ってみたりしますが、実際には、自分が見えている物語があまりにも狭いだけなのです。「頑張る」という単純な行動原理で、複雑な問題を考えずに済ませているだけなのです。頑張っている間は「前に進んでいる」という錯覚が得られて充実感があります。この充実感が曲者です。なぜなら、その錯覚が問題から目を背けさせ、「方向性が間違っているのではないか」という疑問を封じ込めてしまうからです。思考の罠では、なぜ私たちは頑張ることの問題点という明らかな事実に気づけないのでしょうか。その答えは、私たちの思考の仕組みにあります。自分の判断パターンに気づいたことがあります。結論が先にあって、その結論を支持する証拠だけを集め、矛盾する情報は無視していたのです。そして厄介なことに、その正当化のプロセスがあまりにも自然で論理的に見えるため、本人も気づかないのです。自分の信念を守るために、思考を使ってしまうという、これは無意識の傾向です。具体例を挙げましょう。「頑張れば報われる」という信念が先にあって、その信念を支持する証拠だけを集めていました。努力した人の成功例は記憶に残るのですが、努力したのに報われなかった人の存在は意識から消えていってしまいます。30歳になって振り返ると、20代の私は恐ろしいほど確信に満ちていました。「この方法が正しい」「これだけやれば必ず成功する」と疑うことを知らず、いや疑うことを恐れていました。自分の間違いを認めることこの思考の罠から抜け出すために必要なものがありました。自分が間違っているだろうと認めることです。これは簡単なようで、とても難しいことでした。私は「頑張ることは正しい」と信じていました。だから、頑張っても成果が出ない時、「もっと頑張れば」と考えていました。頑張ることが正しいという前提を疑うことは、自分の生き方を否定することのように感じられたのです。しかしある時、意識的に自分の前提を疑ってみることにしました。「頑張らない方がうまくいくことはないか」と。すると、思い当たることがいくつも出てきました。休みを取った翌日の方が、良いアイデアが浮かぶ。締め切りに追われていない時の方が、コードの質が高い。夜遅くまで粘るより、翌朝やり直した方が早く終わる。これは全て、私自身が経験していたことでした。でも「頑張ることは正しい」という信念が強すぎて、その経験を無視していたのです。見たくないものは、見えないようにするというのが、人間の脳の仕組みなのだと知りました。だからこそ、意識的に自分の前提を疑う必要があります。「自分は正しい」という確信から一歩引いて、「自分は間違っているだろう」という可能性を常に心に留めておくこと。それが、思考の罠から抜け出す第一歩でした。確信は、時に最大の敵になる。有限であることを知っている、でも分かっていないでは、なぜ私たちはわざわざこの思考の罠にはまってしまうのでしょうか。なぜ、自分の信念を守ろうとするのでしょうか。その背景には、1つの根本的な事実から目を背けたいという欲求があると私は考えています。それは、人生は有限であるという事実です。この事実を、私たちは「知っている」はずです。人はいつか死ぬ。時間には限りがある。当たり前のことです。でも、本当に分かっているかというと、そうではないのです。思い出してみてください。中学や高校の卒業式の日のことを。「あー、もっと何かできてたな」と思いませんでしたか。部活にもっと打ち込めばよかった。あの子ともっと話せばよかった。文化祭でもっと楽しめばよかった。卒業式の日、私たちは3年間が有限だったことを、ようやく実感します。でも、その実感はすぐに消えるのです。大学に入り、社会人になり、日常に戻ると、また時間が無限にあるかのように振る舞い始めます。「いつかやろう」「そのうち学ぼう」「まだ時間はある」と。30歳になった時、ふと計算してみました。80歳まで生きるとして、残りは50年。週に換算すると約2600週。月に換算すると約600ヶ月。この数字を見た時、卒業式の日の感覚が蘇ってきました。思ったより、少ないのです。でも、きっとこの実感もまた薄れていくのでしょう。明日になれば、来週になれば、また時間が無限にあるかのように振る舞い始める。それが人間なのです。だからこそ、意識的に思い出す必要があるのです。時間は有限であること。すべてをやることは不可能であること。何かを選ぶということは、何かを諦めるということ。この事実を忘れそうになるたび、卒業式の日の感覚を思い出すようにしています。時間管理術という逃避しかし、この事実を常に意識し続けることは難しいものです。むしろ、私たちは無意識のうちにこの現実から目を背けようとします。その典型的な方法が、時間管理術です。「もっと効率的に」「もっと生産的に」と時間管理術に縋りつくのは、現実から目を背けているだけなのです。どれだけ効率化しても、時間は増えないのです。時間管理術は「もっと多くのことができるようになる」という幻想を与えてくれます。しかし実際には、私たちにできることの総量は変わりません。ただ、その有限性を見ないようにしているだけなのです。ここで逆説的なことが起きます。限られた時間を受け入れることが、実は自由への第一歩なのです。すべてをやることを諦めた時、初めて「本当にやりたいこと」が見えてきます。「やるべきこと」ではなく「やりたいこと」へ集中できるようになります。選ばなければならないという制約が、逆に選択を可能にするのです。忙しさというステータス時間が有限だと分かっていても、人は忙しさを求めます。私もそうでした。「忙しい」と言うことが、ある種のステータスでした。忙しい = 重要な仕事をしている = 価値があるという等式を、疑うことなく信じていたのです。しかし冷静に考えるとおかしな話です。忙しいことと価値を生むことは別のことです。では、なぜ私たちは忙しくなるのでしょうか。理由はいくつかあります。優先順位がついていないから。断れないから。そして何より忙しさそのものを求めているからです。暇になることが怖い。何もしていない時間が耐えられない。だから予定を埋める。忙しくする。これは最初に述べた「頑張ることへの依存」と同じ構造です。意味のない努力忙しくしているうちに、私はたくさんの意味のない努力をしていました。完璧な資料を作るために、美しいデザイン、詳細な分析、見栄えの良いグラフを何日もかけて作ります。しかし実際に見られるのは最初の数ページだけです。定期的な報告のために資料を作って説明して質疑応答する時間を、毎週毎月確保しています。しかしその時間で議論される内容はメール一通で済む内容だったりします。これは全て、「頑張っている感」を得るための努力でした。実際に価値を生むための努力ではなく、自分と周囲に「頑張っている」と思わせるための努力だったのです。なぜこんなことをしていたのでしょうか。「頑張っていない自分」が怖かったからです。「何もしていない」と認めることが怖かったから、何かをしている「ふり」をしたのです。しかしそのせいで、意味のあることをする時間がなくなってしまいました。意味のない努力が、意味のある努力を駆逐していたのです。なぜ意味のない努力を選んでしまうのかこれは努力の世界における残酷な法則です。なぜ残酷かというと、意味のない努力の方が楽で、見た目の成果が出やすいからです。比較してみましょう。完璧な資料を作ることは無理ですが、時間をかければ見栄えはかなり良くなります。しかし複雑な問題を本質的に解決することは難しく、時間をかけてもできるとは限りません。会議に出席することは簡単です。座って話を聞いてたまに発言すればいい。しかし深く考えて独創的な解決策を生み出すことは難しく、孤独で不確実で失敗するだろう。だから人は無意識に意味のない努力を選びます。一日の大半を意味のない努力で埋めてしまうため、本質的な努力をする時間がなくなってしまうのです。楽な努力が、本当の努力を駆逐する。何もしない時間の価値この悪循環を断ち切るために、ある日、試しに一日何もしない時間を作ってみました。会議もキャンセルし、メールも見ずに、ただ窓の外を眺める時間を確保しました。最初は不安でした。「こんなことしていていいのか」「時間を無駄にしているのではないか」と。この不安は、最初に述べた「何もしていない自分に価値がない」という信念から来ています。しかし一時間、二時間と過ごすうちに何かが変わりました。頭の中がクリアになって、今まで見えなかったものが見えるようになったのです。忙しさは、思考を停止させます。忙しい状態では「これって意味あるのか」と問う余裕がないため、意味のないことを延々と続けてしまうのです。そのとき、ふと考えました。何も生み出していない時間や、誰からも評価されない時間にさえ、私の人生の価値は宿りうるのだろうか。窓の外を眺めているだけの時間。何も「生産」していない時間。誰にも見られていない時間。そういう時間に、価値はあるのでしょうか。最初、私は「いいえ」と答えていました。価値とは、何かを生み出すことで生まれるものだと思っていたからです。成果があってこそ価値がある。評価されてこそ価値がある。そう信じていました。しかし、何もしない時間を過ごしているうちに、考えが変わってきました。その時間は、確かに何も「生産」していませんでした。でも、自分の中で何かが整理され、何かが癒され、何かが育っていたのです。それは目に見える成果ではありませんでしたが、確かに何かが起きていました。生産性や成果や他者評価——そういったものを全部はがした後に残るもの。それが「自分の時間」の価値なのだろう。何かを生み出すための時間ではなく、ただ存在するための時間。そういう時間があっていいのだと、少しずつ思えるようになりました。忙しさという霧が晴れて本質が見えたとき、気づきました。今までやっていたことの半分以上は実は必要なく、頑張っていたけれど価値を生んでいなかったのです。立ち止まった時間が、一番遠くまで連れて行ってくれた。選択という技術何もしない時間を作ったことで、30歳になって学んだ最も重要なことの1つが見えてきました。それは、選択することの重要性です。若い頃は「全部やろう」としていました。新しい技術が出れば学び、新しいプロジェクトがあれば参加し、頼まれた仕事は全て引き受けていました。確かに、若い頃や自分の成長を誰かが見守ってくれる時期には、それも良いだろう。がむしゃらに量をこなすことで、見えてくるものはあります。しかしそれだけではありません。自分の能力を発揮できる環境を自分で選び、作ることもまた、自分の能力なのです。全部やろうとし続けると、何が起きるでしょうか。エネルギーが分散してどれも中途半端になり、重要なことに十分な時間と集中を注げなくなります。そして何より、自分が得意なこと、やりたいことが見えなくなってしまいます。若い頃からやりすぎると、自分の可能性を狭めてしまう可能性があるのです。すべてに手を出すことで、「自分は何でもそこそこできる人」にはなれるだろう。しかし「この領域では誰にも負けない」という強みは育ちません。ある時、尊敬する先輩に「どうやったら全部うまくできますか」と相談しました。彼は笑って「全部うまくやろうとするな。1つだけ、圧倒的にうまくやれ」と言いました。「勝てる領域を見つけろ」と彼は続けました。「君が他の誰よりも価値を出せる領域、そこに全てを賭けろ。他は最低限でいい」と。集中することで見えてきたものその日から自分の「勝てる領域」を探し始めました。自分は何が得意なのか、どこで他の人と差別化できるのか。振り返ってみると、私が価値を生んでいたのは、複雑な問題を構造化してシンプルな解決策を示すことでした。資料を何百枚作ることでも、会議を何時間することでもありませんでした。でも当時の私は、そのことに気づいていませんでした。すべてを同じように頑張っていたからです。得意なことと苦手なこと、重要なことと些細なこと、すべてに同じエネルギーを注いでいました。それからは、「勝てる領域」へ集中することにしました。複雑な問題に向き合う時間を最大化し、他の作業を最小化しました。すると不思議なことが起きました。仕事の質が上がり、周囲の評価も上がり、そして忙しさは減ったのです。やることを減らしたのに、成果は増えた。これは最初、信じられませんでした。でも考えてみれば当然のことでした。苦手なことに時間を使っていた分を、得意なことに回しただけなのです。同じ時間を使っても、得意なことの方が成果は出ます。これは怠けているわけではありません。戦略的に力を配分しているだけなのです。やめることを選ぶ選択するということは何かを捨てることです。これが最も難しいことでした。私たちは何かを捨てることに恐怖を感じます。「後で必要になるだろう」「チャンスを逃すだろう」と考えてしまいます。しかし、「やらないこと」を選ぶ決断こそが、人生における優先順位を明確化する鍵なのです。ここでもう一度、選択の影について考えてみます。私は「何を選ぶか」については意識していましたが、「何を選ばずに残してしまっているか」については、ほとんど意識していませんでした。やめることを選ぶとき、私たちは選んだこと（やめること）に意識を向けます。しかし同時に、「続けること」を選んでいるのです。その「続けること」は、続ける価値があるものでしょうか。無意識のうちに惰性で続けているだけではないでしょうか。私は「To Stopリスト」を作り始めました。やることリストではなく、やめることリストです。意味のない定例会議に出席するのをやめました。完璧な資料を作るのをやめました。すべての技術トレンドを追うのをやめました。頼まれた仕事を全て引き受けるのをやめました。忙しいふりをするのもやめました。最初は罪悪感がありました。しかしやめてみると驚きました。誰も困らなかったのです。むしろ重要なことへ集中できるようになって、成果が上がりました。やめることと怠けることは違います。それは本質に集中するための戦略なのです。捨てることが、得ることの始まりだった。努力はベクトルだここまで読んで、頑張ること自体が悪いのだと思われただろう。しかし、そうではありません。問題は「どう頑張るか」なのです。頑張ることは、ベクトルです。大きさだけじゃなく、方向があるのです。どれだけ大きな力で頑張っても、方向が間違っていたら目的地には着きません。むしろ遠ざかっていくのです。多くの人はベクトルの「大きさ」ばかりに注目します。「もっと頑張る」「もっと努力する」「もっと時間をかける」と考え、方向については考えません。しかし重要なのは方向です。間違った方向に全力で走るより、正しい方向にゆっくり歩く方が、目的地には早く着くのです。そして、その「方向」を決めるとき、また同じところに戻ってきます。「前に進む」とは、いったい誰の物差しで測られる「進歩」なのか。社会が示す方向に進むことが「前」なのか。それとも、自分が心から望む方向に進むことが「前」なのか。そこに答えを出さないまま、ベクトルの大きさだけを増やしても、どこにも到達けないのです。努力と評価のミスマッチ努力の方向が間違っていると、どうなるでしょうか。努力と評価が一致しない場所で頑張り続けることになります。それは、尋常ではないほど辛いものです。やっても認められない。いくら頑張っても成果として認識されない。「こんなに頑張っているのになぜ評価されないんだろう」という疑問は、やがて「自分には才能がないのだろう」という絶望に変わっていきます。しかし、ここで立ち止まって考えてみましょう。問題は才能ではなく、環境とのミスマッチなのだろう。あなたの能力が発揮されない環境。あなたの強みが評価されない組織。あなたの価値が認識されない役割。そういう場所でどれだけ頑張っても報われません。これは残酷な事実ですが、同時に希望でもあります。なぜなら、環境は変えられるからです。才能がないのではなく、場所が合っていないだけなら、場所を変えれば状況は改善する可能性があるのです。能力とは環境との相互作用ここで、根本的な認識を改める必要があります。「能力」とは、環境との相互作用の中で初めて発揮されるものなのです。ある環境では高いパフォーマンスを出せる人が、別の環境では全く力を発揮できない。珍しいことではありません。むしろ普通のことです。私自身、この事実を身をもって経験しました。ある組織でやりたくない仕事を頑張り、長時間働いて必死に努力しました。しかし成果は出ず、評価も上がらず、自己肯定感は下がり続けて、「自分は仕事ができない」と思っていました。しかし環境を変えた瞬間、すべてが変わったのです。同じ私が違う組織、違う役割で働き始めると、成果が出て評価され、自己肯定感が戻ってきました。私の「能力」は変わっていませんでした。変わったのは環境だったのです。ですから「自分には能力がない」という結論は早計です。正確には「この環境では、自分の能力が発揮されない」ということなのです。この認識は重要です。なぜなら、「能力がない」という結論は絶望につながりますが、「環境が合っていない」という認識は行動につながるからです。頑張りで全てを説明しようとしていた私は長い間、すべてを「頑張り」で説明していました。環境のことなど、考えもしませんでした。成果が出ない時は「自分がもっと頑張ればよい」と思っていました。だから、もっと時間をかけ、もっと努力し、もっと自分を追い込みました。成果が出た時は「自分が頑張ったから」と思っていました。だから、次も同じように頑張れば、同じように成果が出ると信じていました。うまくいかないのは環境のせいではなく、自分の努力が足りないせい。うまくいったのは環境のおかげではなく、自分の努力のおかげ。すべての原因を「自分の頑張り」に帰属させていたのです。この考え方は、一見すると責任感があるように見えます。「環境のせいにしない」「自分でコントロールできることに集中する」。でも、実際にはこれは視野の狭さでした。なぜなら、同じ努力をしても、環境によって成果は大きく変わるからです。自分の強みが発揮される環境なら、少ない努力で大きな成果が出ます。自分の強みが発揮されない環境なら、どれだけ努力しても成果は限られます。そしてもう1つ、認識しておくべきことがあります。「自分の能力が発揮されない環境」は、常に存在しているということです。どんな組織にも、どんな役割にも、自分に合わない部分があります。完璧にフィットする環境など存在しません。大切なのは、それを認めることです。「ここは自分に合っていない」と認めることは、敗北ではありません。むしろ、そこから戦略が始まります。合わない部分を認めるからこそ、「ではどうするか」を考えられるようになるのです。私は長い間、合わない部分を認めることができませんでした。「もっと頑張れば何とかなる」と思い続けていました。でも実際には、何ともならなかったのです。ただ消耗しただけでした。この事実に気づくまで、私は長い時間を要しました。そして気づいた時、ようやく「どこで頑張るか」を考えられるようになったのです。勝てる領域を見つけるでは、どうすれば「勝てる領域」を見つけられるのでしょうか。これはあくまで私の場合の話ですが、無意味な場所で頑張らず、能力が発揮される場所で努力することが、私が燃え尽きずに長く走り続ける秘訣でした。私は、自分にとって意味の分からない仕事を無限にできる耐久性の高い人間ではありませんでした。合わない環境で合わない仕事を続けることは苦痛でしかありませんでした。それは弱さだろうが、それが私の現実だったのです。私の場合、開発全般が得意でした。設計と開発、どちらも能力を発揮できて楽しいのです。しかしやってはいけなかったのは、マルチタスクをしながら人との調整やステークホルダー管理を大量にこなすことでした。この能力が著しく低く、全体の生産性がとても下がってしまったのです。最初は周囲の期待に応えようとして、開発をしながら調整業務もこなそうとしました。しかし評価されませんでした。「中途半端だ」と言ってもらえればまだ良かった。そうではなく、評価が低いだけ。何が問題なのか分からないまま、成果の出ない日々が続きました。しかしある程度裁量をもらい、開発に集中し始めたら状況が変わりました。「この実装すごく良い」と言われるようになって、チーム全体の生産性が上がり、そして私の評価も上がったのです。勝てる領域とは、自分の能力と環境のニーズが交わる場所です。自分が得意でも誰も必要としていなければ評価されず、環境が必要としていても自分ができなければ価値を出せません。その交点を見つけてそこに集中すること、それが努力の方向性を正しく定める方法でした。戦う場所を選ぶことが、戦い方を決める。環境という見えない制約ここまで読んで、あなたはこう考えるだろう。「確かに正しい場所で頑張ることは重要だけれど、そもそも『自分の能力が発揮される環境』なんて、どうやって見つければいいのか」と。その通りです。自分の能力が発揮される環境は簡単には見つかりません。そしてもっと現実的な問題があります。今いる環境が自分に合っていないと分かっても、すぐには動けないのです。住宅ローンがある。家族を養っている。転職するには経験が足りない。業界の状況が悪い。様々な制約が私たちを今の場所に縛り付けています。だから、戦術的な頑張りも必要なのです。これは矛盾しているように聞こえるだろう。今まで「頑張りすぎるな」と言ってきたのに、「頑張りも必要」と言うのは。しかし、これは矛盾ではありません。問題は「頑張ること」自体ではなく、「考えずに頑張ること」だったのです。戦略を持った上での戦術的な頑張りは、必要なものです。持続可能性という解答ここまで、頑張ることの問題点と、選択と集中の重要性を述べてきました。では、具体的にどうすればいいのでしょうか。私が見つけた答えは、持続可能性でした。面白いことに気づきました。頑張る量を減らしたら、成果が増えたのです。ある時、私は思い切って変えてみることにしました。やるべき仕事とやらない仕事を分けて、不要なミーティングに出なくなりました。やりたくない仕事を整理させてほしいと相談したのです。すると不思議なことが起きました。勤務中の8時間の質が劇的に上がったのです。なぜこうなったのか。理由は単純でした。「この8時間だけが自分の時間だ」と考えると一瞬たりとも無駄にできなくなり、集中力が持続して疲労が少なくなり、翌日もまた集中できるようになったのです。無駄な時間が減りましたが、学びの質は上がりました。必要なことだけを学ぶようになり、「やらなきゃ」ではなく「やりたい」で動くようになったのです。この経験から1つの原則を学びました。持続可能性が、成果を生むという原則です。一時的には全ての時間を注ぎ込む方が多く成果を出せるように見えます。しかし長期的には持続可能なペースの方がずっと多くの成果を生むのです。無理をして一気にやろうとすると、どこかで必ず破綻します。体調を崩すか、質が落ちるか、燃え尽きるか。そして破綻した後のリカバリーには、節約できたはずの時間よりもずっと長い時間がかかるのです。新しいやり方の始まり持続可能性を意識することで、新しいやり方が始まりました。無理をしない働き方。自分の限界を知った上でのアプローチ。がむしゃらではなく戦略的なやり方。私の新しいやり方は、「頑張ります」という言葉を封印することから始まりました。最初は怖かったのを覚えています。「頑張らない」と言ったら「やる気がない」と思われるんじゃないか、評価が下がるんじゃないかと心配していました。しかし違ったのです。「頑張ります」をやめて「こうします」と言い始めた時、初めて信頼されるようになりました。具体的な計画を示す。達成可能な目標を設定する。リスクを評価する。代替案を用意する。そして結果を出す。がむしゃらな熱意ではなく冷静な戦略で勝負するやり方に変えたのです。頑張ることをやめたら時間ができました。その時間で考えることができました。「今の仕事は本当に自分がやりたいことなのか」「この関係性は本当に大切にしたいものなのか」「この努力は本当に価値を生んでいるのか」と。そして気づきました。今まで「頑張らなきゃ」と思ってやっていたことの多くは、実は自分が本当にやりたいことではなかったのです。社会的な期待に応えるため、周囲に認められるため、「できる人」に見られるため、そういう外的な動機で動いていたのです。しかし30歳になって、もうそういう生き方は続けられないと悟りました。体力的な限界、精神的な限界、そして何より残りの人生をそんな生き方で使いたくないと思ったのです。がむしゃらで許された特別な時間の終わりは、敗北ではありません。より賢く、より持続可能なやり方への転換点なのです。自己犠牲という承認への飢え新しいやり方を始めてから、もう1つ重要なことに気づきました。それは、自分を大切にすることと他者を大切にすることのバランスについてです。「他人を優先する自分」でしか価値を感じられない人がいます。自分のニーズを無視して他人に尽くすことで「必要とされている感覚」を得ているのです。一見すると優しさに見えます。しかし、実はこれは承認への飢えなのです。自分の時間を全て他人に捧げる。自分の希望を後回しにする。常に誰かの期待に応える。自分が疲れていても「頼まれたから」と引き受ける。その自己犠牲によって「自分は良い人だ」「自分は必要とされている」と感じているのです。しかし、健全ではありません。自分を大切にできない人は、結局他人を大切にできないからです。見返りを期待する優しさなぜ自己犠牲が健全でないのか、もう少し詳しく説明させてください。自分を犠牲にして他人に尽くすと、無意識のうちに「見返り」を期待するようになるのです。「こんなに頑張ったんだから感謝されるべきだ」「こんなに尽くしたんだから認められるべきだ」という気持ちが湧いてきます。そしてその期待が満たされないと怒りや不満が生まれます。「こんなに頑張ったのに」「こんなに尽くしたのに」と相手を責める気持ちが湧いてきます。これは優しさとは違います。相手のためではなく自分の承認欲求を満たすための行為なのです。見返りを期待しない優しさもあります。相手のために行動し、その結果がどうであれ満足できる。私はそういう優しさを持ちたいと思いました。しかし自分が満たされていない状態では、その無条件の優しさを持つことは難しいのです。まず自分を満たすことだからこそ、まず自分を満たすことが大切なのです。これは理屈としては分かりやすい話です。でも、実行するのは難しいのです。なぜなら、自分を後回しにすることが習慣になっているからです。私の場合、常に誰かのために動いていました。チームのため、会社のため、プロジェクトのため。そう言えば聞こえは良いのですが、実際には自分のことを考える余裕がなかっただけでした。そしてある時、限界が来ました。誰かのために動く気力すら湧かなくなったのです。その時ようやく気づきました。自分が枯れていたら、誰かに何かを与えることはできないのだと。自分を大切にすることは、自己中心的なことではありません。持続可能に誰かを助けるための前提条件なのです。自分の限界を知る。自分のニーズを尊重する。時には「できない」と言う勇気を持つ。これは全て、より長く、より健全に他者を大切にするための準備なのです。そして自分が満たされた状態から他人を助ける。見返りを期待せず純粋に相手のために行動する。私はそういう優しさを持ちたいのです。空っぽの器からは、何も注げない。フェーズによる変化しかしここでも1つ大切なことを付け加えます。キャリアのフェーズによって、求められることは変わるということです。ジュニアの頃は、がむしゃらでも許されました。むしろ、がむしゃらであることが求められていました。何も分からないのだから、とにかく量をこなせ。失敗してもいいから、手を動かせ。その時期に「効率」や「戦略」を語るのは早すぎたのです。しかしミドルになると、状況が変わります。「頑張っています」だけでは評価されなくなります。「で、結果は」「で、何を学んだの」と問われるようになります。がむしゃらに動くだけでなく、方向性を持って動くことが求められるのです。そしてシニアになると、より変わります。自分が頑張ることよりも、チーム全体の成果が問われます。自分一人で抱え込むのではなく、任せることが求められます。「自分が頑張る」から「みんなが頑張れる環境を作る」へ。役割が変わるのです。私は今、ミドルからシニアへの過渡期にいます。ジュニアの頃のやり方が通用しなくなり、新しいやり方を模索している時期です。また同じことを考えます。今、手放せずに握りしめている「頑張り」は、本当に自分を守っているのだろうか。それとも、もう要らなくなった古い防具なのだろうか。ジュニアの頃、「とにかく頑張る」という姿勢は私を守ってくれました。何も分からなくても、がむしゃらにやっていれば居場所がありました。しかし今、同じ姿勢を続けることは、私を守るどころか、足を引っ張っています。かつて自分を守ってくれた「頑張り方」が、フェーズが変わった今もまだ有効なのか。それとも、アップデートすべきなのか。そこに正直に向き合う必要がありました。重要なのは、今の自分がどのフェーズにいるかを認識することであり、そのフェーズに応じたやり方を選ぶことです。ジュニアのやり方をミドルになっても続けていたら、消耗するだけです。ミドルのやり方をシニアになっても続けていたら、チームの足を引っ張ります。フェーズが変われば、やり方も変えなければならないのです。この文章で私が伝えたいのは「頑張るな」ということではありません。「今の自分のフェーズに合った頑張り方を選べ」ということなのです。しかし、1つ補足があります。自分では気づけなくても、上司やマネージャーが適切にコントロールしてくれている場合があるということです。私の場合も、振り返ってみれば、良い上司に恵まれていた時期は自然と適切な仕事量に調整されていました。「それは引き受けなくていい」「今はこっちに集中して」と言ってもらえていたのです。当時は気づいていませんでしたが、それは上司が私の状態を見て、適切に仕事を配分してくれていたからでした。逆に言えば、自分が上司やチームリーダーになった時には、同じことをする責任があるということです。メンバーが頑張りすぎていないか。中途半端になっていないか。「頑張っているように見える」からといって見逃していないか。そして、必要であれば「それはやらなくていい」と言えているか。人は頑張っている人に「中途半端だ」とは言いにくいものです。だからこそ、上司やリーダーは意識的にそれを言う必要があります。言わなければ、かつての私のように、本人は気づかないまま消耗していくのです。「おい、がんばるな」と言ってあげられる人になること。それもまた、フェーズが変わった時に求められる役割なのです。「頑張る自分」というアイデンティティ最後に、最も根深い問題について話させてください。私は「頑張る自分」というアイデンティティに縛られていました。「私は頑張る人だ」「私は努力家だ」「私は諦めない」というような自己像があり、その自己像を守るために頑張り続けなければいけなかったのです。しかしそれは苦しいものでした。「頑張る自分」であり続けるために休めず、立ち止まれず、弱音を吐けなかったのです。「頑張る自分」というアイデンティティが自分を縛る檻になっていました。ある日ふと気づきました。私は「頑張る」ということ自体にしがみついていて、成果を出すためではなく「頑張る自分」でいるために頑張っていたのです。そしてまた、同じところに戻ってきます。「頑張らなければ価値がない自分」と、「頑張っていなくてもここにいていい自分」のどちらを、本当は生きたいのか。頭で考えれば、答えは明らかです。「頑張っていなくても価値がある」と信じたい。でも、心の奥底では、まだその確信が持てませんでした。しかし、考え続けることで、少しずつ変わってきました。そしてもう1つ気づきました。頑張っていなくても自分に価値があるということに。成果を出していなくても自分に価値がある。忙しくなくても自分に価値がある。価値は頑張ることから来るのではなく、存在することそのものに価値があるのです。これは宗教的な話ではなく実際的な話です。頑張り続けて壊れた人をたくさん見てきました。優秀な人ほど「もっとできるはずだ」と自分を追い込んで限界を超えて壊れてしまいます。そして壊れたら何も生み出せなくなってしまいます。何も生み出していない時間にも、価値はあります。誰からも評価されない時間にも、意味があります。生産性という物差しを外した時、初めて見えてくるものがあるのです。だから頑張らないことは自分を守ることであり、長く続けるための戦略なのです。全力で走り続けることはできません。どこかで必ず止まります。でも、適切なペースで歩き続けることはできます。そして、歩き続けた人の方が、結果的には遠くまで行けるのです。「頑張る自分」を降りて「続けられる自分」になり、そして「結果を出す自分」に登る。それが私の選択でした。おわりにこの文章を書き終えて、コーヒーを淹れた。カップを持って窓際に立つと、隣のマンションの明かりがいくつか見える。日曜日の夜だ。明日からまた一週間が始まる。みんな、何をしているんだろう。仕事の準備をしているのか、録画していたドラマを見ているのか、あるいは私と同じように、何となく窓の外を眺めているのか。正直に言うと、この文章を書いたからといって、私が何か変わったわけではない。明日になれば、また同じように仕事に行く。締め切りに追われて、会議に出て、メールを返して、「頑張らなきゃ」と思う瞬間がきっとある。そういう自分を完全になくすことはできない。たぶん、これからもずっと。でも、一つだけ変わったことがある。「頑張っている」と言われたとき、その言葉をそのまま受け取らなくなった。「で、それで何か変わったの？」と自分に聞くようになった。頑張っていることを、言い訳にしなくなった。それだけのことだ。たったそれだけのことなのに、少しだけ楽になった。頑張っていない自分を許せるようになった、というのとは違う。頑張ることの価値を、正しく測れるようになった、という感じだ。この文章を読んで、何か得るものがあったかどうかは分からない。「そんなの当たり前じゃん」と思った人もいるだろうし、「何を言っているのか分からない」と思った人もいるだろう。それでいい。ただ、もし今、頑張っているのに上手くいかなくて苦しい人がいたら。もし今、頑張れない自分を責めている人がいたら。一つだけ伝えたいことがある。頑張っていることは、偉いことじゃない。偉いのは、頑張った結果、何かが変わることだ。何かを生み出すことだ。誰かの役に立つことだ。頑張ること自体には、実は何の価値もない。でも逆に言えば、頑張らなくても、結果を出せばいいということでもある。頑張らなくても、変われればいいということでもある。頑張らなくても、前に進めればいいということでもある。だから、頑張らなくていい。本当に、頑張らなくていい。その代わり、歩くのはやめないでほしい。自分のペースで、自分の方向に、自分の足で。転んでもいい。休んでもいい。立ち止まってもいい。でも、歩くのだけは、やめないでほしい。コーヒーが冷めてきた。明日も、たぶん、いつも通りの一日が来る。でも、いつも通りの一日の中で、少しだけ違う選択ができるかもしれない。「頑張らなきゃ」と思ったとき、「いや、待て」と立ち止まれるかもしれない。それだけで、十分だと思う。おい、がんばるな。syu-m-5151.hatenablog.com参考書籍あっという間に人は死ぬから　「時間を食べつくすモンスター」の正体と倒し方作者:佐藤 舞（サトマイ）KADOKAWAAmazon不完全主義　限りある人生を上手に過ごす方法作者:オリバー・バークマンかんき出版Amazonエッセンシャル思考 最少の時間で成果を最大にする作者:グレッグ・マキューンかんき出版Amazonエフォートレス思考 努力を最小化して成果を最大化する作者:グレッグ・マキューンかんき出版Amazonさあ、才能(じぶん)に目覚めよう　最新版 ストレングス・ファインダー2.0作者:ジム・クリフトン,ギャラップ日経BPAmazon嫌われる勇気作者:岸見 一郎,古賀 史健ダイヤモンド社Amazon幸せになる勇気作者:岸見 一郎,古賀 史健ダイヤモンド社AmazonDIE WITH ZERO　人生が豊かになりすぎる究極のルール作者:ビル・パーキンスダイヤモンド社Amazon部下をもったらいちばん最初に読む本作者:橋本拓也アチーブメント出版Amazon","isoDate":"2025-12-02T03:47:02.000Z","dateMiliSeconds":1764647222000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"生成AIエージェントによるブログレビュー環境の構築（上）","link":"https://syu-m-5151.hatenablog.com/entry/2025/12/02/002601","contentSnippet":"この記事は、3-shake Advent Calendar 2025 2日目のエントリ記事です。はじめにブログを書いては直し、また直す。同じ文章を何度も触っていると、客観的な判断ができなくなってくる。「これで本当に伝わるのか？」という疑問だけが残る。コードにはレビューがあり、デザインには批評がある。しかし、技術ブログには明確な基準がない。その不安を解消するために、最初は自分の文章を評価する「プロンプト」を作って運用していた。防御力、思考整理力、実践応用性など、6つの観点でAIに評価させるのだ。だが、すぐに問題にぶつかった。「面倒」なのだ。記事を書くたびにプロンプトを開き、貼り付け、結果を待つ。この手動のひと手間があるだけで、次第に「今日はまあいいか」とサボるようになり、せっかくの基準も形骸化していった。だから、環境ごと変えることにした。生成AIのエージェント機能を使い、ブログレビューの手順をひとつの動作にまとめたのだ。/blog-quality-review と打てば、必要なチェックが勝手に走る。手間を消し、継続性だけを残す。今回は、そんなブログレビュー環境の構築について紹介する。syu-m-5151.hatenablog.comブログ記事評価プロンプト v2.1 https://syu-m-5151.hatenablog.com/entry/2025/05/19/100659 · GitHubこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。なぜブログレビューにエージェントを使うのか自分で書いた記事を自分で評価するのは、想像以上に難しい。「こんなにわかりやすく書いたのに、なぜ伝わらないんだろう」と思うことはないだろうか。それは私たちが、自分の持つ知識や前提条件を、無意識に読者にも期待してしまうからだ。「これくらい知っているだろう」「説明不要だろう」という思い込みが、読者との間に溝を作る。ここにエージェントが入ると、話が変わる。エージェントは私の「暗黙の前提」を共有していない。だから、初学者が感じるであろう「分からない」を冷静に指摘できる。専門用語の壁、論理の飛躍、「なぜ？」という素朴な疑問——これらを容赦なく洗い出してくれる。さらに、エージェントは疲れないし、基準を忘れない。私が定義した「レビューの観点」を一貫して適用し続ける。これは単なる自動化ではない。私の認知リソースを、「本当に人間にしかできない判断」に集中させるための仕組みだ。Commandsでレビュー観点を構造化するClaude Codeには、よく使うプロンプトをコマンド化できる機能がある。.claude/commands/ ディレクトリにMarkdownファイルを置くだけで、ファイル名がコマンド名になり、中身がプロンプトとして機能する。code.claude.com「毎回『この観点でレビューして』と指示するのは面倒」「記事ごとにレビューの質がバラつくのが嫌だ」そんな悩みを抱えていた私にとって、Commandsは最適解だった。一貫性の担保手打ちのプロンプトでは、表現の揺らぎによりAIの回答も変わってしまう。Commandsなら常に同一の定義で実行されるため、出力の質が安定する。# 悪い例（毎回微妙に違う）「この記事をレビューして」「読みやすさをチェック」「AIっぽくないか見て」# 良い例（カスタムコマンド）/blog-quality-review blog.md# → 常に定義された6つの観点・同じ基準でレビューが走るGitでのVersion管理Commandsの実体はMarkdownファイルだ。つまり、プロンプトの改善履歴をGitで管理できる。「この観点を追加したら、指摘が鋭くなった」「この表現を変えたら、より具体的な改善案が出るようになった」こういった試行錯誤の軌跡が残ることで、プロンプト自体が「育つ資産」になっていく。私が実際に使っているCommandsここからは、私がブログ執筆・レビューで実際に使用しているCommandsを全てではないが紹介する。注意：ここで紹介するのは各Commandの要点のみだ。実際のファイルには、より詳細な指示や評価基準（Few-Shotなど）が含まれている。Phase 1: 書く前に深く考える良いブログは「書く」前に「考える」ことから始まる。/deep-thinking-prompt - 深い思考のための問いかけ# Deep Thinking Prompt - 深く考えるための問いかけブログを書く前に「深く考える」ための問いかけを提供します。表面的な理解や一般論で終わらず、本質に迫るための思考支援ツールです。## 7つの問いかけ1. **原体験への問いかけ** - なぜこのテーマに興味を持ったのか2. **前提への問いかけ** - 当たり前だと思っていることは何か3. **対立への問いかけ** - 矛盾や葛藤はどこにあるか4. **構造への問いかけ** - システムとしてどう機能しているか5. **変化への問いかけ** - 過去と現在で何が変わったか6. **未来への問いかけ** - このまま進むとどうなるか7. **読者への問いかけ** - 誰に届けたいのか、なぜその人なのかこのCommandを使うと、「何を書くか（What）」だけでなく「なぜ書くのか（Why）」が明確になる。一般論ではなく、自分だけの視点を掘り起こすための工程だ。/structural-thinking - 構造設計# Structural Thinking - 構造的思考支援散らばった思考を整理し、論理的な流れを作ります。読者の理解プロセスに合わせた「伝わる」構成を設計します。深く考えたあと、その思考をどう配置するか。このCommandが、散乱したアイデアを読者に届く「ストーリー」へと整えてくれる。Phase 2: 書いた後にレビューする/blog-quality-review - 6つの観点でレビュー以前作成した「ブログ記事評価プロンプト」をCommand化したものだ。# Blog Quality Review - ブログ品質レビュー以下の6つの観点（各0.0-5.0スコア）で評価します：1. **防御力** - 批判や反論への耐性2. **思考整理力** - 情報の論理的構造化3. **実践応用性** - 読者が行動に移せる価値4. **構成と読みやすさ** - 視覚的要素と文体5. **コミュニケーション力** - 人間味のある伝達6. **人間らしさ** - 温度感と個性実行すると記事の強みと弱みが数値化される。「前回は実践応用性が3.2だったが、今回は4.0に上がった」といった具合に、自身の成長や記事の品質を定量的に把握できる。/beginner-feedback - 初学者の視点# Beginner Feedback - 初学者の素朴な意見あなたは**一般読者代表（佐々木ゆい・28歳）**として、素朴な意見を提供します。- 専門用語や前提知識の壁を発見- 論理の飛躍を指摘- 「なぜ？」という素朴な疑問を投げかける- 一般読者が共感できるか確認エキスパートの目では見逃してしまう、初学者の「分からない」を発見するためのCommandだ。具体的なペルソナを設定することで、フィードバックの解像度を高めている。/ai-humanity-check - AIっぽさの評価# ai-humanity-check文章のAIっぽさを評価し、より人間らしい表現への改善提案を行います。## AIっぽさスコア (0.0-5.0) ※低いほど人間らしい**0.0-1.0 (完全に人間的)**- 著者特有の言い回しや癖がある- 具体的な失敗談や苦労話が生々しい- 感情の起伏が自然で共感できるAIに下書きを支援させると、どうしても文章が「AI臭く」なりがちだ。このCommandで機械的な表現を検出し、体温のある文章へと戻していく。Phase 3: 仕上げる/textlint-polish - 文章校正# Textlint Polish - 文章校正・AIっぽさ除去機械的・AIっぽい表現を排除し、自然で読みやすい文章にする。- AIが多用する冗長表現を検出- 比喩的・詩的すぎる表現を簡潔に- 文体の統一（です・ます調）textlint的な観点で、表現の誤りや揺らぎを修正する。AI特有の冗長な言い回しもここでカットする。/redundancy-check - 冗長性チェック# Redundancy Check - 冗長性チェック以下の4つの観点（各0.0-5.0スコア）で評価します：1. **情報密度** - 1文あたりの情報量2. **簡潔性** - 冗長表現・無駄な修飾の少なさ3. **論理効率** - 論理的重複・循環論法の少なさ4. **構造最適性** - 章・節の構成の必要十分性削れる言葉は徹底的に削る。情報の密度を高め、読み手の時間を奪わない文章にするための最終チェックだ。全自動レビューの実行これらを一つずつ実行するのはやはり手間だ。そこで、これらを束ねる /full-review を作成した。# Full Review - 全自動レビュー実行すべての必須レビューを自動で順次実行します。textlint校正から始まり、初学者フィードバック、品質レビューまで一括で実施。## 使用方法/full-review blog.mdこのCommandひとつで、以下のフローが流れる。/textlint-polish（校正）/beginner-feedback（初学者視点）/blog-quality-review（品質スコア）/ai-humanity-check（人間らしさ）一度設定さえしてしまえば、あとは「コマンド一発」で包括的なレビューが完了する。上巻のまとめここまで、Commandsを使ったブログレビュー環境の基礎（Phase 1〜3）を解説してきた。出発点は、「ブログ記事の評価基準がなく、レビューが属人的かつ面倒」という課題だった。これに対し、エージェントを活用して評価観点を構造化し、実行を自動化するというアプローチをとった。ここで重要なのは、AIとの関係性だ。体験や感情といった「身体性」は人間が供給し、それを構造化し整える役割をAIが担う。これはAIへの丸投げではなく、互いの強みを活かした協働である。Commandsによって評価基準を定義し、Gitで管理し、自動化することで、「書くこと」以外のノイズを極限まで減らすことができる。下巻では、より高度なAgents（サブエージェント）の活用と、複数の視点を持つレビュー体制の構築について解説する。下巻に続くsyu-m-5151.hatenablog.com","isoDate":"2025-12-01T15:26:01.000Z","dateMiliSeconds":1764602761000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「Postgres で試した？」と聞き返せるようになるまでもしくはなぜ私は雰囲気で技術を語るのか？ — Just use Postgres 読書感想文","link":"https://syu-m-5151.hatenablog.com/entry/2025/11/25/135220","contentSnippet":"はじめに「Just use Postgres」という言葉を初めて聞いたのは、いつだったか覚えていません。Twitter か Hacker News か、あるいは社内の Slack か。どこで聞いたにせよ、私の反応は決まっていました。「また極端なことを言う人がいる」と。「それ、〇〇でもできますよ」——この手のフレーズはもう100回は聞いてきました。そして大抵の場合、その〇〇は専用ツールに置き換えられていきます。技術が専門分化していくのは自然な流れです。全文検索なら Elasticsearch。時系列データなら InfluxDB。メッセージキューなら RabbitMQ。それぞれの分野に専門家がいて、専用のソリューションがあって、ベストプラクティスがあります。「とりあえず Postgres で」なんて、それは思考停止ではないか、と。でも、心のどこかで気になっていたんです。www.manning.comソフトウェアエンジニアとして 10 年近く働いてきて、システムが複雑化していく様子を何度も見てきました。「全文検索だから Elasticsearch」と導入したら、その運用は誰がやるのか。バックアップは？　モニタリングは？　バージョンアップは？　構成図に新しい箱が増えるたびに、誰かが深夜 3 時のアラート対応をする可能性が増えます。その「誰か」は、たいてい自分です。以前関わったプロジェクトでは、Postgres、Redis、Elasticsearch、RabbitMQ、InfluxDB が同居していました。それぞれに理由があって導入されたはずですが、3 年後には「なぜこれが必要だったのか」を説明できる人が誰もいなくなっていました。ドキュメントはあっても、判断の背景までは残っていません。結局、「触ると怖いから残しておこう」という判断になります。技術的負債の典型です。syu-m-5151.hatenablog.comこの本を手に取ったのは、そういう日常からの逃避だったのかもしれません。「Postgres だけで済むなら、楽になれる」そんな甘い期待を持って読み始めました。そして、最初の数ページで気づきました。この本が言っているのは、私が思っていたことと少し違います。「Postgres は万能だから全部 Postgres でやれ」ではありません。「既に Postgres を使っているなら、新しいデータベースを追加する前に、まず Postgres で試してみよう」ということです。その違いに気づいた瞬間、なんというか、肩の力が抜けました。これは、銀の弾丸を売りつける本ではなかったんです。私たちが日々向き合っている「技術選定」という名の意思決定に、1 つの視点を提供してくれる本でした。10 年近くこの仕事をしてきて、技術選定について 1 つ学んだことがあります。新しい機能や技術が出たとき、いきなり飛びつかない。どれだけ魅力的に見えても、まず「運用時にどうなるか」を考えます。誰がバックアップを取るのか。障害時に誰が対応するのか。3 年後にメンテナンスできる人がいるのか。流行りの技術を追いかけることと、本番環境で安定して動かすことは、別の話です。これは、Postgres の中でも同じです。pgvector や TimescaleDB のような比較的新しい拡張、あるいは Postgres 本体の新機能についても、本番投入前に運用面を検討する必要があります。「Postgres だから安心」ではなく、「その機能が十分に枯れているか」を見極める姿勢が大事です。かといって、新しいことを学ばないわけにもいきません。技術は進歩します。昨日のベストプラクティスが、明日には技術的負債になることもあります。結局のところ、謙虚に学び続けるしかありません。私が最近考えているのは、こういう基準です。替えの利く技術は、流行に従う。フロントエンドのフレームワークとか、CI/CD ツールとか。入れ替えやすいものは、その時点でのベストを選べばいい。替えの利きづらい基盤は、標準に従う。データベースとか、認証基盤とか。長く使うものは、実績のある標準的な選択をする。競争優位の核は、自ら設計する。ビジネスの差別化に直結する部分は、自分たちで考え抜いて設計する。Postgres は、競争優位の核になる場合もありますが、基本的には 2 番目の「替えの利きづらい基盤」であることが多いです。40 年以上の実績があり、コミュニティ主導で開発され、世界中で使われている標準的な選択肢。だからこそ、その可能性を正しく理解しておきたいと思いました。だから、読み進めることにしました。正直に言うと、全部を理解できたわけではありません。「FOR UPDATE SKIP LOCKED」の仕組みを完全に説明しろと言われたら、今でもちょっと怪しいです。でも、それでいいと思うことにしました。完璧に理解することが目的ではありません。「Postgres で試した？」その一言を、自信を持って言えるようになること。それが、この本を読む目的でした。なので、この読書感想文には私の手元で動かした実行結果と書籍の中身がごちゃ混ぜになっています。基本的に明記しているつもりですが抜けていたらごめんなさい。Just Use Postgres!: All the database you need (English Edition)作者:Magda, DenisManningAmazonこのブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。1. Meeting Postgres「Just use Postgres」の再解釈1.2 節の「Just use Postgres」の説明を読んで、自分の理解が間違っていたことに気づきました。私はこれまで、「Just use Postgres」を技術選定の初手として捉えていました。「新規プロジェクトならとりあえず Postgres 立てとけ」みたいな。でも、著者が書いているのは違います。Does this mean Postgres has become a Swiss Army knife and the only database every developer needs? Certainly not.著者は明確に否定しています。Postgres は万能ツールではない、と。じゃあ「Just use Postgres」は何を意味するのでしょうか。「既に Postgres を使っているチームが、新しいユースケース（地理空間、時系列、生成 AI など）が発生したとき、別のデータベースを追加する前に Postgres で解決できるか確認してみよう」これがこのモットーの正しい解釈だと著者は言います。インフラエンジニアとして 10 年近く運用してきた身としては、この視点の転換にハッとしました。「Elasticsearch で全文検索やりたい」と言われた時、私は内心「またか…（心の中で構成図に新しい箱を追加する手が震える）」と思っていました。でも、「Postgres で試した？」と聞き返すことはしませんでした。自分の仕事を増やしたくないという気持ちが先に立って(自分が起こされるのにね)。これ、逆だったんです。Postgres で解決できるなら、新しいデータベースを追加するより運用負荷は減ります。バックアップ戦略も、モニタリングも、アラートルールも、既存のまま使えます。運用対象が増えるたびに、前世で何をしたのかと深夜に考える機会も減ります。この本を読み終えて、「Postgres で試した？」と自信を持って聞き返せるようになったと思います。良いか悪いかは別として。なぜ Postgres が人気なのか1.1 節では、Postgres が人気な 3 つの理由が挙げられています。オープンソース・コミュニティ主導: 1994 年に MIT ライセンスでオープンソース化。単一ベンダーではなくコミュニティ主導で開発。エンタープライズ対応: 35 年の開発で培われた信頼性と堅牢性。年次メジャー バージョン リリース、段階的 改善 重視。拡張性: Michael Stonebraker が設計当初から拡張性を重視。JSON、時系列、全文検索、ベクトル類似検索など多様なユースケースに対応。この 3 つ目の「拡張性」が、「Just use Postgres」を可能にしている核心だと感じました。著者の言葉を借りれば、Postgres は「従来のトランザクショナルワークロードを超えた幅広い用途に対応できる」。だから、新しいユースケースが出てきても、まず Postgres で試す価値があります。運用の観点からも、この 3 つは重要です。オープンソースだから、ベンダーロックインのリスクがないエンタープライズ対応だから、夜中3時にPagerDutyが鳴って「どの DB だ...？」と確認する時間を省略できるインフラエンジニアとして信頼できる拡張性があるから、新しいデータベースを追加する代わりに既存の Postgres を活用できるDocker でサクッと起動1.3 節では、Docker での起動方法が紹介されています。docker run --name postgres \\    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \\    -p 5432:5432 \\    -v postgres-volume:/var/lib/postgresql/data \\    -d postgres:17.2「1 分以内にコンテナとして起動可能」と Summary に書いてありますが、本当にその通りです。この手軽さが、「Just use Postgres」の実践を支えています。新しいユースケースを試すために、まず手元で動かしてみる。それが 1 分でできます。ツールを開発していることがあるのですがこれはツールの普及にめちゃくちゃ大事です。開発環境だからシンプルな設定で OK ですが、本番では当然違います。ユーザー名は postgres 以外にする、パスワードは環境変数じゃなく secrets で管理する、など。でも、それはこの本の scope 外でしょう。PostgreSQL徹底入門 第4版 インストールから機能・仕組み、アプリ作り、管理・運用まで作者:近藤 雄太,正野 裕大,坂井 潔,鳥越 淳,笠原 辰仁翔泳社Amazonpsql と generate_series1.4 節では psql での接続方法、1.5 節では generate_series を使ったモックデータ生成が紹介されています。INSERT INTO trades (id, buyer_id, symbol, order_quantity, bid_price, order_time)SELECT    id,    random(1,10) as buyer_id,    (array['AAPL','F','DASH'])[random(1,3)] as symbol,    random(1,20) as order_quantity,    round(random(10.00,20.00), 2) as bid_price,    now() as order_timeFROM generate_series(1,1000) AS id;generate_series と random の組み合わせで、複雑なモックデータを SQL だけで生成できます。外部ツール不要。これも「Just use Postgres」の一例だと感じました。「テストデータ生成ツールが必要だ」と言い出す前に、Postgres の標準機能で解決できます。普段、テストデータ生成はアプリ側（Rust）でやることが多かったのですが、シンプルなケースなら generate_series で十分かもしれません。試しに手を動かしてみたら、いくつか発見がありました。まず、generate_series は日付生成にも使えます。generate_series('2025-01-01'::date, '2025-12-31', '1 day') でカレンダーテーブルを一発生成できます。これは便利。次に、random() は毎回異なる値を返すので、再現可能なテストには setseed() を事前に呼ぶ必要があります。これを知らずに「テスト結果が毎回違う！」と焦った経験があります。そして一番ハマったのは、配列のインデックスが 1 始まりだということ。(array['AAPL','F','DASH'])[random(1,3)] のように 1 から始めないと想定外の結果になります。Rust や Python に慣れていると、0 から始めたくなるんですよね。私の直感を裏切るポイントでした。ちょっと昔だとこちらの資料とかはめちゃくちゃ良いのでオススメです。 speakerdeck.com speakerdeck.com基本クエリ1.6 節では、基本的な SQL クエリが紹介されています。SELECT symbol, count(*) AS total_volumeFROM tradesGROUP BY symbolORDER BY total_volume DESC;著者は count(*) が「Postgres で特別に最適化されている」と書いています。この本を通して、Postgres の内部動作についての理解が深まりました。DBといえばそーだいさんの資料を読み漁ってほしいです。 speakerdeck.com2. Standard RDBMS capabilitiesデータベースの三層構造を理解していなかったこの章で再認識したのは、Database → Schema → Table という三層構造の実践的な使い方です。10 年近く Postgres を運用してきた中で、Schema は使ってきました。ただ、この章で説明されているような「マイクロサービスのモジュールごとにスキーマを分ける」という設計パターンは、改めて整理されると納得感があります。この章で説明されている eコマースプラットフォームの設計が、その例です。coffee_chain (database)├── products (schema)│   ├── catalog (table)│   └── reviews (table)├── customers (schema)│   └── accounts (table)└── sales (schema)    ├── orders (table)    └── order_items (table)マイクロサービスのモジュールごとにスキーマを分ける。この設計パターン自体は知っていましたが、この本の整理の仕方は参考になります。著者は明確に書いています。Each application module or microservice has its own schema containing all the related data.これなら、アプリケーション層のアーキテクチャとデータベース層の構造が一致します。名前の衝突も避けられます。でも、同じデータベース内だから、JOIN で複数スキーマのテーブルをまたいでクエリできます。マルチテナント構成において Database レベルで分離していることも納得がいきました。テナントごとにデータベースを分け、リソースを共有しながら完全に隔離します。スケールアウト時には特定のテナントだけ別サーバーへ移動可能です。この設計思想、次のプロジェクトで導入を検討しようと思います。制約はデータベースでやるべきか2.3 節のデータ整合性の話で、著者のスタンスが面白かったです。著者のチームは、最初はアプリ層ですべてを検証する想定でした。でも、実際にプロダクトを構築していく中で、アプリ層のチェックが破られてデータ整合性の問題が発生しました。その経験から、著者はこう述べています。we decide to add additional constraints at the database level.私の経験でも、制約をデータベースに入れるべきか、アプリ層でやるべきかという論争が何度もありました。著者は両方を推奨しています。アプリ層でチェックしつつ、データベース層にも防御線を張ります。この章では、バグでアプリ層のチェックが破られたとき、外部キー制約がデータの不整合を防いだ例が出てきます。ERROR: insert or update on table \"reviews\" violatesforeign key constraint \"products_review_product_id_fk\"DETAIL: Key (product_id)=(1004) is not present in table \"catalog\".アプリのバグで product_id が 4 じゃなくて 1004 になっていました。でも、外部キー制約があったから、データベースにゴミが入らずに済みました。多層防御。これがデータ整合性の正しいアプローチだと感じました。アプリ層だけに頼ると、コードが変わったときに破綻します。データベース層だけに頼ると、エラーハンドリングが遅れて UX が悪化します。両方でやるべきです。トランザクション分離レベルの実践理解2.4 節のトランザクションで、MVCC と read committed 分離レベルの説明が具体的で良かったです。理論は知っていました。でも、この章の Table 2.1 の 2 つの psql セッションを並行実行する例を見て、実際の動きがイメージできるようになりました。2 つのトランザクションが同じ商品 (id=1) の在庫数を同時に減らそうとします。トランザクション 1 が UPDATE を実行（まだコミットしてない）トランザクション 2 が SELECT を実行 → まだ 199 が見える（dirty read を防いでいる）トランザクション 2 が UPDATE を実行 → ブロックされるトランザクション 1 が COMMIT → トランザクション 2 がアンブロックされるトランザクション 2 の UPDATE が最新の値（198）を読み直して実行される最後のポイントが重要でした。ブロックが解除された後、トランザクション 2 は再度値を読み直します。だから、結果は 197 になります（199 → 198 → 197）。もしこれがなかったら、トランザクション 2 は古い値（199）から 1 を引いて 198 にしてしまい、トランザクション 1 の更新が消えます（lost update）。Postgres の read committed は dirty write も防ぎます。だから、本番環境でデフォルトの分離レベルとして十分に使えます。もちろん、phantom read や non-repeatable read を防ぎたいケースもあります。そのときは repeatable read や serializable を使います。でも、大半のユースケースでは read committed で問題ありません。この理解、実際に手を動かさないと身につきませんでした。データベース関数で何をやるべきか2.6 節の関数とトリガーは、この章で一番刺激的でした。著者は order_add_item と order_checkout という 2 つの PL/pgSQL 関数を実装しています。ショッピングカートの管理ロジックをデータベース関数として実装した例です。最初は「これ、アプリ層でやればいいんじゃない？」と思いました。モダンなマイクロサービスアーキテクチャを信奉する我々にとって、データベース関数は「おじいちゃんの時代の遺物」みたいなイメージがありました。でも、著者の説明を読んで納得しました。書籍には「At least two scenarios come to mind」として 2 つのシナリオが紹介されています。複雑なビジネスロジックがデータと密結合している場合 → すべてのクライアントアプリやマイクロサービスで同じロジックを実装するより、データベース関数 1 つで済む複数ステップの処理でアプリとデータベース間の往復が必要な場合 → 大量のデータ転送が必要なとき、データベース内で完結させたほうが効率的order_add_item 関数の実装を見ると、この 2 つの利点がよくわかります。CREATE OR REPLACE FUNCTION sales.order_add_item(customer_id_param INT,  product_id_param INT, quantity_param INT)RETURNS TABLE (...) AS $$DECLARE    pending_order_id UUID;BEGIN    -- 1. 既存の pending order を探す    SELECT id INTO pending_order_id FROM sales.orders    WHERE customer_id = customer_id_param AND status = 'pending';    -- 2. なければ作る    IF pending_order_id IS NULL THEN        INSERT INTO sales.orders (customer_id, status)        VALUES (customer_id_param, 'pending')        RETURNING id INTO pending_order_id;    END IF;    -- 3. 商品を追加または更新（MERGE 文）    MERGE INTO sales.order_items AS oi ...    -- 4. 結果を返す    RETURN QUERY SELECT ...;END;$$ LANGUAGE plpgsql;これをアプリ層でやろうとすると、複数のクエリを順次実行する必要があります。SELECT で pending order があるか確認なければ INSERT で作成SELECT で商品の価格を取得INSERT or UPDATE で order_items に追加SELECT で最終的なカート内容を取得アプリとデータベース間で何度もデータをやり取りする必要があり、ネットワークレイテンシの影響を受けます。データベース関数なら 1 回の呼び出しで完結します。しかも、トランザクショナルに実行されます。途中でエラーが起きたら全部ロールバックされます。でも、すべてをデータベース関数でやるべきではありません。著者も「少なくとも 2 つのシナリオ」と言っています。つまり、適切なユースケースを見極めることが重要です。私の基準はこうです。やるべき: データと密結合した複雑なロジック、複数ラウンドトリップが必要なケースやらないべき: ビジネスロジックの大半、頻繁に変更されるロジック、外部 API との連携この判断基準、次のプロジェクトで使いたいです。ちなみに、PL/pgSQL を書いていて何度かハマったポイントがあります。SELECT ... INTO で結果が 0 行の場合、変数は NULL になります。エラーにはなりません。これを知らずに「なぜ NULL が入る？」と 30 分悩んだことがあります。あと、ON CONFLICT DO UPDATE で新しい値を参照するには EXCLUDED を使います。EXCLUDED.quantity のように書きます。最初は「新しい値をどう参照するんだ？」と混乱しました。一番タチが悪いのは、変数名とカラム名の衝突です。SELECT * FROM orders WHERE order_id = order_id と書くと、両方が変数として解釈されて全行が返ってきます。デバッグが本当に難しい。だから p_customer_id や v_order_id のようにプレフィックスを付けるのがベストプラクティスです。トリガーは「見えない魔法」になりやすい2.6.2 節のトリガーの例も実践的でした。order_items テーブルに変更があったら自動的に orders.total_amount を更新します。CREATE TRIGGER trigger_update_order_totalAFTER INSERT OR UPDATE OR DELETE ON sales.order_itemsFOR EACH ROWEXECUTE FUNCTION sales.update_order_total();これはシンプルで便利ですが、トリガーは「見えない魔法」になりやすいと感じました。アプリ層のエンジニアが INSERT INTO sales.order_items を実行したとき、裏で sales.orders が更新されていることに気づかないかもしれません。トリガーが増えると、データベースのパフォーマンス問題の原因を追うのが難しくなります。「なぜこの INSERT が遅い？」と思ったら、実は裏で 3 つのトリガーが動いていた、みたいな。著者はトリガーの適切なユースケースを挙げています。Triggers are particularly useful in audit scenarios, where you need to track who made changes in the database, or in event-driven architectures.監査ログ（誰がいつ変更したか）イベント駆動アーキテクチャ（変更を他のシステムに通知）トリガーを書いていて一度ハマったのは、NEW と OLD の使い分けです。NEW は INSERT/UPDATE で使用可能で、OLD は UPDATE/DELETE で使用可能。DELETE トリガーで NEW.order_id にアクセスしようとしてエラーになりました。COALESCE(NEW.order_id, OLD.order_id) のような対応が必要だと、その時初めて知りました。あと、大量の行を更新する場合、行ごとにトリガーが発火してパフォーマンスが低下します。FOR EACH ROW のトリガーは便利ですが、一括更新のパフォーマンスには注意が必要です。これ以外のケースでは、慎重に検討すべきだと思います。View は「名前付きクエリ」2.7 節の View の説明はシンプルで明快でした。A view is essentially a named query that returns data in a tabular format.View = 名前付きクエリ。この理解が正しいです。複雑な JOIN と集計を含むクエリを、アプリ層の複数箇所で使い回すより、View として定義してしまいます。CREATE VIEW sales.product_sales_summary ASSELECT    c.name AS product_name,    c.category,    SUM(oi.quantity) AS total_quantity_sold,    SUM(oi.quantity * oi.price) AS total_revenueFROM products.catalog cLEFT JOIN sales.order_items oi ON c.id = oi.product_idGROUP BY c.idORDER BY total_quantity_sold DESC, total_revenue DESC;アプリ層からはこうです。SELECT * FROM sales.product_sales_summary WHERE category='coffee';これだけで済みます。Materialized View も便利ですが、リフレッシュのタイミングが悩ましいです。手動リフレッシュ：ユーザーが「更新」ボタンを押したとき定期リフレッシュ：pg_cron で 1 時間ごとイベント駆動：トリガーで特定のテーブルが更新されたとき著者は 3 つのアプローチを提案していますが、ユースケースによって使い分けるべきです。3. Modern SQLSQL-92 の呪縛から解き放たれるこの章を読んで改めて認識したのは、自分がまだ SQL-92 の世界に閉じこもっていたという事実です。pgsql-jp.github.io著者の Markus Winand の言葉を引用します。Since 1999, SQL is not limited to the relational model anymore. Back then, ISO/IEC 9075 (the \"SQL standard\") added arrays, objects, and recursive queries. In the meantime, the SQL standard has grown five times bigger than SQL-92. In other words: relational SQL is only about 20% of modern SQL.SQL-92 は全体の 20% でしかありません。残りの 80% が Modern SQL です。でも、正直に言うと、私は長年その 20% の世界で生きていました。CTE は知っていたけど、「読みやすさのための構文糖衣」程度にしか思っていませんでした。Window Functions も「集計が少し楽になるやつ」くらいの認識。Recursive Queries に至っては、「使う機会がない」と決めつけていました。この章を読み終えて、自分がどれだけ Postgres の可能性を狭めていたかを再認識しました。なぜ Modern SQL を使わないのか著者は、Modern SQL が普及しない理由を 2 つ挙げています。理由1: 獲得した知識の粘着性（Stickiness of gained knowledge）Some developers learned SQL many years ago and mastered the SQL-92 version of the language for various data processing tasks. Even if their SQL queries are verbose or less efficient, the tasks are still solvable. As a result, many people continue doing things the way they originally learned.痛いほど身に覚えがあります。私が SQL を覚えたのは 15 年以上前です。当時の教科書は SQL-92 ベースで、GROUP BY、JOIN、Subquery があれば何でも解決できました。その成功体験が、今も私の手を縛っています。まるで、「ガラケーで十分じゃん」と言い張っていた 2010 年の自分を見ているようです。「CTE を使えば読みやすくなる」と頭ではわかっていても、「でも、Subquery でも書けるしな」と思ってしまいます。結果、冗長で読みにくいクエリを量産します。後輩に「このネストの深さ、どこまで行くんですか…？」と言われたことは秘密です。理由2: ORM フレームワークSome developers fully rely on ORM frameworks as a layer between their application and the database. They trust the ORM framework to generate SQL queries, believing it knows the best way to query or manipulate data.これも痛い指摘です。やめてくれおれにきく。ORM は確かに便利です。でも、ORM が生成するクエリは「汎用的なワークロード」を想定しています。Window Functions を使えば 1 回のクエリで済むケースでも、ORM は複雑な Self-Join を生成するかもしれません。第 1 章で学んだ「Just use Postgres」の思想は、ORM へ任せる前に、Postgres で何ができるかを知ることにも通じます。CTE（Common Table Expressions）は単なる Subquery の糖衣構文ではないCTE は「読みやすい Subquery」という側面で使うことが多かったです。でも、この章を読んで、それ以上の価値があることを再確認しました。Listing 3.3 の例では、2 つの CTE を使って「3 人以上のユーザーが聴いて、半分以下の時間しか再生しなかった曲」をランキングしています。WITH plays_cte AS (    SELECT s.title, s.duration, p.play_duration, p.user_id    FROM streaming.plays p    JOIN streaming.songs s ON p.song_id = s.id    WHERE p.play_start_time::DATE BETWEEN '2024-09-15' AND '2024-09-16'      AND p.play_duration \u003c (s.duration / 2)),user_play_counts AS (    SELECT title, duration, COUNT(DISTINCT user_id) AS user_count,      MIN(play_duration) AS min_play_duration,      COUNT(*) AS total_play_count    FROM plays_cte    GROUP BY title, duration)SELECT title, duration, min_play_duration, total_play_countFROM user_play_countsWHERE user_count \u003e= 3ORDER BY min_play_duration ASCLIMIT 3;このクエリを Subquery で書いたら、どうなるでしょうか。ネストが深くなって、読みにくくなります。メンテナンスもしにくくなります。でも、著者が強調しているのは「読みやすさ」だけではありません。If we want to understand how a query is actually executed by Postgres, we can look at the query execution plan using the EXPLAIN statement.EXPLAIN の結果を見ると、Postgres は plays_cte を user_play_counts に fold しています。つまり、CTE を使っても、実行計画は効率的なままです。これは重要なポイントです。以前のバージョンでは CTE が「最適化の壁」になることがありましたが、現在は改善されています。実際に EXPLAIN ANALYZE で確認してみました。Postgres は CTE をインライン展開して最適化しています。以前は CTE が「最適化の壁」と呼ばれていましたが、現在のバージョンでは改善されています。CTE が展開されて効率的なプランになっていることが確認できました。Postgres は賢いです。Data-modifying CTE という選択肢Listing 3.4 で紹介されている Data-modifying CTE は、私にとって完全に新しい概念でした。WITH updated_play AS (    UPDATE streaming.plays    SET play_duration = 200    WHERE id = 30    RETURNING song_id, play_duration)SELECT s.title, s.duration,       CASE           WHEN up.play_duration = s.duration THEN 'Moved Up the Rank'           ELSE 'Rank Not Changed'       END AS rank_change_statusFROM updated_play upJOIN streaming.songs s ON s.id = up.song_id;UPDATE の結果を RETURNING で受け取り、その結果を使って SELECT を実行します。これが 1 つのトランザクション内で完結します。これまで、「UPDATE してから SELECT」という処理は、2 つのクエリを順番に実行していました。でも、Data-modifying CTE を使えば、1 つのクエリで完結します。アトミック性も保証されます。なぜこの機能を今まで積極的に使ってこなかったのでしょうか。使う場面を意識していなかったというのが正直なところです。Recursive Queries は「特殊なケースでしか使わない」という誤解Recursive Queries は「組織の階層構造を扱う時に使う機能」という認識でした。実際、それ以外の場面で使う機会は多くありませんでした。でも、この章を読んで、活用範囲が広いことを再確認しました。著者が例として挙げているのは、音楽ストリーミングサービスの「連続再生」のトラッキングです。plays テーブルには played_after というカラムがあり、「この曲の前に再生された曲の ID」を保持しています。つまり、連続再生はリンクリストの構造を持っています。Listing 3.8 の Recursive Query は、この連続再生のシーケンスを取得します。WITH RECURSIVE play_sequence AS (    SELECT id, user_id, song_id,      play_start_time, play_duration, played_after    FROM streaming.plays    WHERE id = 5    UNION ALL    SELECT p.id, p.user_id, p.song_id, p.play_start_time,       p.play_duration, p.played_after    FROM streaming.plays p    JOIN play_sequence ps ON p.played_after = ps.id)SELECT user_id, song_id, play_start_time,  play_duration as duration, played_afterFROM play_sequenceORDER BY play_start_time;これを読んで、以前アプリ側で何度もループしてクエリを投げていた処理を思い出しました。以前のプロジェクトで、SNS のスレッド返信を表示する機能を実装した時、「親コメント ID」を辿って、アプリ側で再帰的にクエリを投げていました。その結果、N+1 問題が発生して、パフォーマンスが悪化しました。当時のアプリログを見返すと、同じユーザーの操作で DB への接続数が 47 回。まるでチャットボットが会話のキャッチボールをしているかのようでした。もちろん、レスポンスタイムは 3 秒超え。あの時、Recursive Query を知っていたら、1 回のクエリで全ての返信を取得できました。Recursive Query の実行フローListing 3.7 の擬似コードは、Recursive Query の実行フローを明確に説明しています。# Step 1: 非再帰項を実行（初期データ）non_recursive_result = execute(non_recursive_term);# Step 2: 重複削除（UNION の場合）if (using UNION)    non_recursive_result = remove_duplicates(non_recursive_result);# Step 3: 最終結果に追加final_result.add(non_recursive_result);# Step 4: ワーキングテーブルを初期化working_table = non_recursive_result;# Step 5: 再帰項を実行（ワーキングテーブルが空になるまで）while (working_table is not empty) {    intermediate_table = execute(recursive_term, using=working_table);    if (using UNION)        intermediate_table =            remove_duplicates(intermediate_table, excluding=final_result);    final_result.add(intermediate_table);    working_table = intermediate_table;}このフローを読んで、「Recursive Query は魔法じゃなくて、ちゃんとした仕組みがある」と納得できました。特に重要なのは、UNION と UNION ALL の違いです。UNION は重複削除するので、無限ループを防げます。UNION ALL は重複を許すので、パフォーマンスは良いですが、無限ループのリスクがあります。ところで、Recursive CTE を書いていて気になったのは終了条件です。調べてみると、終了条件は「新しい行が生成されなくなるまで」で、明示的に書く必要はありません。循環検出には配列で訪問済みノードを追跡する方法が有効です。ARRAY[id] AS path で初期化して、ps.path || p.id で追加していく。NOT p.id = ANY(ps.path) で循環を検出できます。このパターンは覚えておくと便利です。ただし、深い階層（数千レベル）ではパフォーマンスが低下します。グラフ DB ほど柔軟なグラフ探索はできません。SNS の友達の友達を無限に辿るような処理には向いていないです。この違いを理解していないと、本番環境で無限ループが発生します。怖いです。データベース監視の Slack チャンネルが「CPU 使用率 100%」「接続数の上限到達」で埋め尽くされる光景は、二度と見たくありません。Window Functions は Self-Join の代替ではないWindow Functions は「Self-Join の代わりに使える構文」という認識で使ってきました。でも、この章を読んで、パフォーマンス面での違いを改めて確認しました。Listing 3.11 の Self-Join と Listing 3.12 の Window Function を比較すると、違いが明確です。Self-Join 版:SELECT DISTINCT p.song_id,       p.user_id,       t.total_durationFROM streaming.plays pJOIN (    SELECT song_id,           SUM(play_duration) AS total_duration    FROM streaming.plays    GROUP BY song_id) t ON p.song_id = t.song_idORDER BY p.song_id;Window Function 版:WITH plays_with_total AS (  SELECT    song_id, user_id, SUM(play_duration)    OVER (PARTITION BY song_id) AS total_duration  FROM streaming.plays)SELECT DISTINCT song_id, user_id, total_durationFROM plays_with_totalORDER BY song_id, user_id;Self-Join 版は、テーブルを 2 回走査しています。Window Function 版は、1 回の走査で済みます。著者の言葉を借りれば、次のようになります。Although the self-join approach works as expected, it's not the most efficient, because every row of the table is accessed twice. Additionally, it's not the easiest to follow when trying to understand the query logic.これを読んで、「Window Functions は単なる糖衣構文じゃなくて、パフォーマンス最適化の手段だった」と気づきました。Running Total と Window FrameListing 3.13 の Running Total の計算は、Window Functions の本質を理解する上で重要でした。SELECT song_id, user_id, play_duration, SUM(play_duration)OVER (PARTITION BY song_id ORDER BY user_id) AS total_play_durationFROM streaming.playsWHERE song_id = 2;結果は次のようになります。 song_id | user_id | play_duration | total_play_duration---------+---------+---------------+---------------------       2 |       1 |           144 |                 144       2 |       2 |           206 |                 350       2 |       3 |           186 |                 654       2 |       3 |           118 |                 654PARTITION BY song_id で Window を作り、ORDER BY user_id で Window を Frame に分割します。各 Frame は、現在の行 + それ以前の行を含みます。この仕組みを理解すると、「累積和」「移動平均」「ランキング」といった処理が、すべて Window Functions で解決できることがわかります。以前のプロジェクトで、時系列データの累積和を計算する時、アプリ側でループを回していました。あれも、Window Functions を使えば 1 回のクエリで済みました。RANK() と ROW_NUMBER() の違いListing 3.14 の RANK() は、同じ値に同じランクを付けます。SELECT song_id, SUM(play_duration) AS total_play_duration,RANK() OVER (ORDER BY SUM(play_duration) DESC) AS song_rankFROM streaming.playsGROUP BY song_idORDER BY song_rank;もし ROW_NUMBER() を使っていたら、同じ値でも異なる番号が振られます。この違いを理解していないと、ランキング機能で不具合が発生します。実際に試してみると、3 つの関数の違いがはっきりします。ROW_NUMBER(): 同じ値でも異なる番号（1→2→3→4→5）RANK(): 同じ値は同じ番号で次は飛ぶ（1→2→2→4→5）DENSE_RANK(): 同じ値は同じ番号で次は飛ばない（1→2→2→3→4）以前、ランキング機能で ROW_NUMBER() を使って、同点の処理がおかしくなったことがあります。「なぜ同じスコアなのに順位が違うの？」というバグ報告を受けて、RANK() に変更しました。この違いは一度経験すると忘れません。4. Indexesインデックスの「当たり前」を疑う第 4 章「Indexes」の冒頭の一文が、自分の習慣を言い当てていました。Indexes are often the first optimization technique that comes to mind when dealing with a long-running query or a slow database operation.そうなんです。遅いクエリがあったら、とりあえずインデックス張る。それが 10 年間の私のパターンでした。まるで風邪を引いたら「とりあえずビタミン C」みたいな、根拠のない安心感でした。でも、著者は続けます。They've proven so effective in many scenarios that we sometimes overlook other optimization methods, turning to indexes right away.インデックスに頼りすぎて、他の最適化手法を見落としている。この指摘は痛かったです。実際、過去のプロジェクトで「遅いクエリ問題」が発生した時、私はいつもまずインデックスを疑っていました。でも、本当は EXPLAIN で実行計画を見て、ボトルネックを特定してから判断すべきでした。この章では、インデックスの「なぜ」と「いつ」を徹底的に掘り下げています。単なるインデックス作成のチュートリアルじゃありません。インデックス戦略の哲学です。なぜインデックスがこんなに人気なのか4.1 節「Why are indexes so popular?」では、O(N)と O(log_b N)の違いが説明されています。100 件のテーブルで ID=5 を探す場合。インデックスなし：最大 100 回のルックアップ（O(N)）B-tree インデックスあり：最大 4 回のルックアップ（O(log_b N)、b=3 の場合）これが 100 万件に増えても、インデックスがあれば 6 回のルックアップで済みます（b=10 の場合）。正直、この計算量の違いは知っていました。でも、著者が示した表を見て改めて驚きました。 テーブルサイズ  インデックスルックアップ回数  100件          2回                         1,000件        3回                         1,000,000件    6回                         10,000,000件   7回                         1,000,000,000件  9回                      10億件のテーブルでも9回のルックアップ。これがインデックスの威力です。深夜の障害対応で「インデックス張れば解決するっしょ」と言い続けてきた自分が、ようやく理論武装できた瞬間でした。そして、著者の言葉が刺さります。As a result, it's no surprise that indexes are such a popular optimization technique.インデックスが人気な理由は、この圧倒的な効率性にあります。でも、だからこそ安易に使いすぎるリスクもあります。EXPLAIN — まず実行計画を見ろ4.4 節で EXPLAIN が詳しく説明されています。私はこれまで、EXPLAIN ANALYZE しか使っていませんでした。でも、この章を読んで EXPLAIN (analyze, costs off) や EXPLAIN (analyze, buffers on) といった他のオプションを知りました。特に印象的だったのは、buffers オプションです。Buffers: shared hit=3これは「3 ページをメモリから読んだ（ディスクアクセスなし）」という意味です。もし read=4 があれば、「4 ページをディスクから読んだ」ということになります。遅いクエリの原因はインデックスの欠如じゃなく、メモリ不足かもしれません。この視点は新鮮でした。私は「遅い = インデックスがない」と決めつけていました。でも、buffers を見れば、ディスク I/O が原因なのか実行計画が原因なのか区別できます。著者は次のように書いています。This information is crucial because a query might run slowly not due to a suboptimal execution plan or missing index but because memory has become a limited resource.インデックスは万能じゃありません。頭ではわかっていても、実務では軽視しがちでした。単一カラムインデックス — B-tree vs Hash4.5 節では、単一カラムインデックスが 2 種類紹介されています。B-tree：範囲検索（\u003e, \u003c, BETWEEN）に対応Hash：等価検索（=, IN）のみ私は今まで、Hash インデックスを積極的に選択してきませんでした。「B-tree がデフォルトだから」という理由で、あえて変える必要性を感じていなかったためです。でも、この章を読んで考えが変わりました。例えば、ゲーム内のチャンピオンタイトル（5 種類のみ）を検索する場合。範囲検索は不要で、等価検索だけで十分です。この場合、Hash インデックスが最適です。CREATE INDEX idx_champion_titleON game.player_statsUSING hash(champion_title);実行計画を見ると、Hash インデックスを使った場合の実行時間は 0.073 ms。フルテーブルスキャンの 1.463 ms と比べて 20 倍速いです。ユースケースに合わせてインデックスタイプを選ぶ。これが正しいアプローチです。複合インデックス — 順番が命4.6 節「Composite indexes」は、この章で最も重要なセクションだと思います。複合インデックスの順番は、クエリのパフォーマンスに直結します。例えば、(region, score DESC, win_count DESC) というインデックスを作った場合。CREATE INDEX idx_region_score_win_countON game.player_stats (region, score DESC, win_count DESC);このインデックスは、次のクエリで使われます。-- ✅ 使われるWHERE region = 'NA' and score \u003e 5000 and win_count \u003e 10-- ✅ 使われるWHERE region = 'EMEA' and score \u003e 1000-- ✅ 使われる（先頭カラムがあるから）WHERE region = 'EMEA'-- ❌ 使われない（先頭カラムがない）WHERE score \u003e 1000 and win_count \u003e 30先頭カラム（leading column）が必須。これがないと、複合インデックスは使われません。この章を読みながら実際に手を動かしてみました。EXPLAIN ANALYZE の出力で Index Scan と Index Only Scan の違いを確認することが重要です。Index Only Scan はテーブルにアクセスしないので高速。Covering Index の威力を実感しました。Partial Index については、WHERE 句が完全に一致する場合のみ使用されるという点に注意が必要です。WHERE play_time \u003c= '50 hours' で作ったインデックスは、WHERE play_time \u003c= '50 hours 1 second' では使われません。1 秒違うだけで使われない。厳密すぎる気もしますが、そういう仕様です。Hash インデックスは範囲検索（\u003c, \u003e, BETWEEN）には使えません。等価検索専用です。これを知らずに「なぜインデックスが使われないんだ？」と悩んだことがあります。インデックスサイズを比較した結果も興味深かったです。idx_champion_hash   - 696 kBidx_covering        - 416 kBidx_region_score    - 248 kBidx_perf_margin     - 120 kBidx_casual_players  - 48 kB   -- Partial Index は最小Partial Index のサイズの小ささは印象的でした。必要な部分だけをインデックス化するという発想、もっと早く知りたかったです。ただし、著者は注記しています。However, starting with Postgres 18, the database introduced support for skip scan lookups on composite B-tree indexes, allowing us to skip leading columns and still use the index in more scenarios.Postgres 18 以降では、skip scan が導入されるらしいです。これは大きな改善です。でも、現時点（Postgres 17 以前）では、複合インデックスの順番を慎重に設計する必要があります。Covering Index — テーブルアクセスをゼロに4.7 節「Covering indexes」は、インデックス最適化の最終形態だと感じました。通常、インデックスは「どの行を読むか」を決めるだけで、実際のデータ（username など）はテーブルから取得します。でも、Covering Index を使えば、インデックスだけで全てのデータを取得できます。CREATE INDEX idx_composite_covering_indexON game.player_stats (region, score DESC, win_count DESC)INCLUDE (username);INCLUDE 句で username をインデックスに含めることで、テーブルアクセスが不要になります。実行計画を見ると。Index Only Scan using idx_composite_covering_index on player_statsHeap Fetches: 0Execution Time: 0.602 msHeap Fetches: 0 — テーブルに一切アクセスしていません。実行時間は 0.602 ms。以前の 1.856 ms（Bitmap Index Scan）から 3 倍速くなりました。ただし、トレードオフがあります。username を更新するたびに、インデックスも更新する必要があります。However, as a tradeoff, all included columns must remain consistent with the table data.更新頻度が低いカラムなら Covering Index は有効。逆に、頻繁に更新されるカラムには向きません。Partial Index — 必要な部分だけインデックス化4.8 節「Partial indexes」では、インデックスのサイズを減らす手法が紹介されています。例えば、10,000 人のプレイヤーのうち、74 人（0.74%）だけが「occasional players（プレイ時間 50 時間以下）」だとします。この 74 人だけを頻繁に検索するなら、全体にインデックスを張る必要はありません。CREATE INDEX idx_occasional_playersON game.player_stats (play_time)WHERE play_time \u003c= '50 hours';この Partial Index により。インデックスサイズが大幅に削減される更新時のインデックスメンテナンスコストが減る検索速度は 2 ms から 0.168 ms に改善（20 倍速）ただし、条件が少しでも違うとインデックスが使われません。-- ✅ 使われるWHERE play_time \u003c= '50 hours'-- ❌ 使われない（1秒超過）WHERE play_time \u003c= '50 hours 1 second'Partial Index は条件が厳密。これを理解して使う必要があります。Expression Index — 計算結果にインデックス4.9 節「Functional and expression indexes」は、私にとって全く新しい概念でした。例えば、「勝数 - 負数」というパフォーマンスマージンで検索したい場合。WHERE (win_count - loss_count) BETWEEN 300 and 450通常、この式はクエリ実行時に毎回計算されます。でも、Expression Index を使えば、計算結果をインデックス化できます。CREATE INDEX idx_perf_marginON game.player_stats ((win_count - loss_count));実行時間は 2.524 ms から 1.200 ms に改善（2 倍速）。ただし、式が複雑になると、インデックスのメンテナンスコストが増えます。win_count または loss_count が更新されるたびに、インデックスも更新されます。頻繁に検索される式にのみ使うのが正しい戦略です。Over-Indexing という警告この章の最後に、著者は重要な警告を発しています。Throughout this chapter, we've explored and added various indexes to the game.player_stats table, bringing the total number of indexes for the table to seven.7 つのインデックス。これは典型的な Over-Indexing だと著者は指摘します。Although this is acceptable for learning purposes, in practice, it represents a classic case of over-indexing.インデックスは無料じゃありません。作成時にディスク容量を消費する更新時にメンテナンスコストがかかる計画時（Planning Time）にオプション評価のコストがかかる私は過去、インデックスを「作りすぎる」傾向がありました。「とりあえずこのカラムにもインデックス張っとくか」という感じで。まるで保険に入りまくる不安な中年のように、あらゆるカラムに「念のため」インデックスを追加していました。そして毎回、INSERT が遅くなってから後悔する、という黄金パターンです。でも、この章を読んで、インデックスは慎重に設計すべきだと改めて理解しました。著者は Appendix A で Over-Indexing と Under-Indexing について詳しく説明しています。実際に読んでみて、自分の過去の設計を振り返る良い機会になりました。この辺の基礎がちゃんとできているか定期的に確認することは大切にしていますが、この Appendix がめちゃくちゃ面白いのでおすすめです。mickindex.sakura.ne.jp5. Postgres and JSONJSON 機能を使うべき場所と使わない場所5.3 節の「JSON in Postgres: Striking the balance」が、この章の核心です。著者が書いているのは、「JSON をすべてのデータに使うな」という明確な警告です。Even though Postgres provides full-fledged support for JSON, you should avoid storing all application data in JSON-specific data types as you would in a pure document database.これが「Just use Postgres」の真髄だと感じました。MongoDB を追加する代わりに Postgres の JSON 機能を使う。でも、すべてのデータを JSON で保存する MongoDB みたいな使い方はするな。ハイブリッドアプローチを取れ、と。pizzeria.order_items テーブルの構造が、この考え方を体現しています。CREATE TABLE pizzeria.order_items (    order_id INT NOT NULL,    order_item_id INT NOT NULL,    pizza JSONB NOT NULL,  -- ここは JSON    price NUMERIC(5,2) NOT NULL,  -- ここは通常の型    PRIMARY KEY (order_id, order_item_id));order_id と price は通常の型で、検索とデータ整合性を重視。pizza の詳細（トッピング、クラスト、ソースなど）は JSONB で、柔軟性を重視。この設計、10 年前のプロジェクトで欲しかったです。JSON を使うべき場面著者が挙げている 3 つの基準が具体的でわかりやすいです。データが静的または更新頻度が低い（設定、メタデータ、顧客プリファレンス）データが疎（スパース）（多くの null や 0、feature flags など）スキーマの柔軟性が必要（外部 API のレスポンス、テレメトリイベント）ピザ注文の詳細は「静的」に該当します。注文確定後はほぼ変更されません。もし従来の正規化モデルで実装すると、5 つのテーブル（pizzas、order_items、pizza_cheeses、pizza_veggies、pizza_meats）が必要になります。著者が示した例を見て、「ああ、これは辛い」と思いました。Write overhead: 1 つのピザ注文のために複数テーブルへの INSERT が必要。7 つのトッピングなら 7 レコード。Read overhead: ピザのレシピを再構築するために複数テーブルの JOIN が必要。Transformation overhead: フロントエンドが JSON で受け取るのに、わざわざ正規化モデルへ分解し、また JSON に戻す。この 3 つの overhead、すべて経験があります。特に Transformation overhead が一番つらいです。API レスポンスを JSON で返すためだけに、複雑な JOIN と整形ロジックを書く。「JSON から分解して正規化して、また JOIN して JSON に戻す」という、まるで水を凍らせてから溶かすような無駄な作業。当時の自分に「Postgres の JSONB を使えばいいぞ」と教えてあげたいです。著者が「hybrid approach」を推奨する理由がよくわかりました。json vs jsonb5.1 節で json と jsonb の違いが説明されています。 型  保存形式  Write 性能  Read 性能  インデックス  推奨度  json  テキスト  速い  遅い（毎回パース）  限定的  ❌  jsonb  バイナリ  遅い（パースあり）  速い  GIN など充実  ✅ 著者の推奨は明確です：jsonb をデフォルトで使え。Overall, the jsonb data type is the recommended default for storing and processing JSON data in Postgres, unless you have a specific use case that requires preserving the order of keys in the original JSON objects.「キーの順序を保持する必要がある」という特殊なケースでない限り、jsonb 一択です。私が過去に扱ったプロジェクトでは、なんとなく json を選んでいたことがありました。「書き込みが速いから」という理由で。でも、検索の度にパースが走るコストを考えていませんでした。典型的な「入口だけ見て出口を見ない」パターン。インフラエンジニアあるあるです。著者が書いているように、jsonb は write 時に変換コストがありますが、検索性能は圧倒的に速いです。そして GIN インデックスとの組み合わせでさらに速くなります。JSON のクエリ：-\u003e と -\u003e\u003e5.4 節の JSON クエリ構文は充実しています。機能自体は知っていましたが、改めて整理すると活用の幅が広がります。基本：-\u003e と -\u003e\u003eSELECT    order_id,    pizza-\u003e'size' as pizza_size,     -- JSON 形式で返す    pizza-\u003e\u003e'crust' as pizza_crust  -- テキスト形式で返すFROM pizzeria.order_itemsWHERE order_id = 100;出力の違い。pizza_size   | pizza_crust-------------|-------------\"small\"      | thin-\u003e は JSON 形式なのでダブルクォート付き。-\u003e\u003e はテキスト型なのでダブルクォートなし。最初は「なぜ 2 つの演算子が必要なのか？」と疑問でしたが、5.4.1 節を読んで納得しました。WHERE 句での比較。-- JSON 形式で比較（ダブルクォート必要）WHERE pizza-\u003e'size' = '\"small\"'-- テキスト形式で比較（ダブルクォート不要）WHERE pizza-\u003e\u003e'crust' = 'gluten_free'-\u003e でダブルクォートを忘れると、こんなエラーが出ます。DETAIL: Token \"small\" is invalid.CONTEXT: JSON data, line 1: smallこの仕様、最初はわかりにくいですが、JSON の仕様に忠実だと理解すれば納得できます。Rust から Postgres に接続して JSON を扱う時、この -\u003e と -\u003e\u003e の違いでハマりました。-\u003e は JSON 型を返すので、そのまま文字列としてデシリアライズしようとするとエラーになります。-\u003e\u003e を使うか、適切な型変換が必要です。特に pg_typeof() で型を確認しようとした時、::TEXT でキャストしないと Rust 側でエラーになりました。ネストした JSON へのアクセスSELECT    order_id,    pizza-\u003e'toppings'-\u003e'veggies' as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力。veggies_toppings-----------------------[{\"tomato\": \"light\"}]配列の特定要素にアクセスするには、インデックス（0 始まり）を指定。SELECT    order_id,    pizza-\u003e'toppings'-\u003e'veggies'-\u003e0 as veggies_toppingsFROM pizzeria.order_itemsWHERE order_id = 100;出力（[] が消える）。veggies_toppings---------------------{\"tomato\": \"light\"}さらに、配列内のオブジェクトのフィールドにアクセスします。SELECT    order_id,    pizza-\u003e'toppings'-\u003e'veggies'-\u003e0-\u003e\u003e'onion' as onions_amountFROM pizzeria.order_itemsWHERE order_id = 100;出力。onions_amount---------------lightこの連鎖、最初は読みづらいと思いましたが、慣れると直感的です。? 演算子と @\u003e 演算子? 演算子：キーの存在確認SELECT    order_id,    pizza-\u003e'toppings'-\u003e'meats' as meatsFROM pizzeria.order_itemsWHERE pizza-\u003e'toppings' ? 'meats'ORDER BY order_id LIMIT 5;「meats キーが存在する注文だけを取得」という意味です。配列内のオブジェクトのキー存在確認は少し複雑になります。SELECT    order_id,    pizza-\u003e'toppings'-\u003e'meats' AS meatsFROM pizzeria.order_itemsWHERE EXISTS (    SELECT 1    FROM jsonb_array_elements(pizza-\u003e'toppings'-\u003e'meats') AS meats    WHERE meats ? 'sausage')ORDER BY order_id LIMIT 5;この書き方、正直、冗長だと思いました。でも著者も同じ意見で、5.4.4 節で JSON path expression を使ってシンプルにしています。@\u003e 演算子：包含関係の確認SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"crust\": \"gluten_free\"}';「crust フィールドが gluten_free の注文を数える」という意味です。複数条件。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"crust\": \"gluten_free\", \"type\": \"custom\"}';ネストした構造も可能。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"crust\": \"gluten_free\", \"type\": \"custom\",                 \"toppings\": {\"veggies\": [{\"tomato\": \"extra\"}]}}';この演算子、MongoDB の $elemMatch みたいな感じだと思いました。ちなみに、@\u003e 演算子は配列にも使えます。tags @\u003e '[\"hot\", \"milk\"]' のように書けば、配列が特定の要素を含むかどうかを検索できます。JSON オブジェクトだけでなく、配列にも対応しているのは便利です。著者が「-\u003e と @\u003e を組み合わせるとより読みやすくなる」と書いています。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"crust\": \"gluten_free\", \"type\": \"custom\"}' AND      pizza-\u003e'toppings'-\u003e'veggies' @\u003e '[{\"tomato\": \"extra\"}]';こっちの方が確かに読みやすいです。JSON Path Expressions5.4.4 節で、SQL/JSON path language が登場します。先ほどの「sausage を含む注文を検索」のクエリが、path expression でこうなります。SELECT    order_id,    pizza-\u003e'toppings'-\u003e'meats' AS meatsFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza, '$.toppings.meats[*] ? (exists(@.sausage))')ORDER BY order_id LIMIT 5;サブクエリが不要になりました。構文の説明です。$: 評価対象の JSON オブジェクト（pizza カラム）.toppings.meats: フィールドへのアクセス[*]: 配列のすべての要素?: フィルタの開始@: 現在評価中のオブジェクトexists(@.sausage): sausage フィールドが存在するか最初は読みづらかったですが、いくつか例を見ていくうちに理解できました。配列のクエリSELECT    count(*) as total_cnt,    jsonb_object_keys(        jsonb_path_query(pizza, '$.toppings.cheese[*]')    ) as cheese_toppingFROM pizzeria.order_itemsGROUP BY cheese_topping ORDER BY total_cnt DESC;$.toppings.cheese[*] で cheese 配列のすべてのオブジェクトを取得。jsonb_object_keys で各オブジェクトのキー（チーズ名）を抽出。出力。total_cnt | cheese_topping----------|----------------     2575 | mozzarella      771 | cheddar      762 | parmesanフィルタ付き path expressionSELECT    count(*) AS total_cnt,    pizza-\u003e'type' as pizza_typeFROM pizzeria.order_itemsWHERE jsonb_path_exists(pizza,        '$.toppings.cheese[*] ? (exists(@.parmesan))')GROUP BY pizza_typeORDER BY total_cnt DESC;評価順序。$.toppings.cheese[*]: すべてのチーズオブジェクトを取得?: フィルタ開始exists(@.parmesan): 現在のオブジェクトに parmesan フィールドがあるか複数フィルタのチェーンSELECT count(*)FROM pizzeria.order_itemsWHERE jsonb_path_exists(    pizza,    '$ ? (@.type == \"custom\") .toppings.cheese[*].parmesan ? (@ == \"extra\")');評価順序（左から右）です。$: pizza オブジェクト? (@.type == \"custom\"): type が custom か確認.toppings.cheese[*].parmesan: parmesan オブジェクトを取得? (@ == \"extra\"): 量が extra か確認この書き方、最初は難解だと思いましたが、左から右に評価されると理解すれば読めます。JSON の更新：jsonb_set と #-5.5 節で JSON の更新方法が紹介されています。最も簡単な方法（非推奨）-- アプリ側で JSON 全体を取得SELECT pizza FROM pizzeria.order_items WHERE order_id = $1 and order_item_id = $2;-- アプリ側で JSON を修正-- DB に書き戻すUPDATE pizzeria.order_itemsSET pizza = new_pizza_order_jsonWHERE order_id = $1 and order_item_id = $2;著者が書いているように、簡単だけど効率的じゃないです。複雑な JSON オブジェクト全体を転送するのではなく、必要なフィールドだけを更新する方が良いです。jsonb_set 関数UPDATE pizzeria.order_itemsSET pizza = jsonb_set(pizza, '{crust}', '\"regular\"', false)WHERE order_id = 20 and order_item_id = 5;jsonb_set の引数です。元の JSON オブジェクト（pizza）更新対象のパス（{crust}）新しい値（\"regular\" — JSON 文字列なのでダブルクォート必要）フィールドが存在しない場合に追加するか（false）ネストした配列の更新。UPDATE pizzeria.order_itemsSET pizza = jsonb_set(    pizza,    '{toppings,veggies}',   '[{\"tomato\":\"extra\"}, {\"spinach\":\"regular\"}]',   false)WHERE order_id = 20 and order_item_id = 5;配列の特定要素を更新する場合、パスにインデックスを含められる。-- 例：{toppings, veggies, 0, tomato} で配列の最初の要素の tomato を更新1 つ注意点があります。jsonb_set のパスが存在しない場合、第 4 引数が true なら新しいキーが作成されます。これは便利な反面、タイプミスで意図しないキーが追加されるリスクもあります。#- 演算子：フィールドの削除UPDATE pizzeria.order_itemsSET pizza = pizza #- '{toppings,meats}'WHERE order_id = 20 AND order_item_id = 5;{toppings, meats} パスのフィールドを削除します。この演算子、シンプルで良いです。インデックス：B-tree と GIN5.6 節がこの章で一番技術的に深い部分でした。Expression Index with B-tree最初の試みです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza -\u003e\u003e 'type' = 'custom';実行計画。Seq Scan on order_items (actual time=0.034..1.062 rows=563 loops=1)  Filter: ((pizza -\u003e\u003e 'type'::text) = 'custom'::text)  Rows Removed by Filter: 2375Execution Time: 1.185 ms全件スキャンです。Expression Index を作成します。CREATE INDEX idx_pizza_typeON pizzeria.order_items ((pizza -\u003e\u003e 'type'));再度実行計画を確認。Bitmap Index Scan on idx_pizza_type (actual time=0.068..0.068 rows=563 loops=1)  Index Cond: ((pizza -\u003e\u003e 'type'::text) = 'custom'::text)Execution Time: 0.376 ms4 倍近く高速化（1.185 ms → 0.376 ms）しました。でも問題があります。このインデックスは pizza -\u003e\u003e 'type' というexact expression にしか効きません。-- これは idx_pizza_type を使わないSELECT count(*)FROM pizzeria.order_itemsWHERE pizza -\u003e 'type' = '\"custom\"';実行計画：Seq Scan に戻ります。さらに、別のフィールド（size など）を検索する場合、また別の Expression Index が必要になります。著者が書いているように、スケールしません。フィールドごとにインデックスを作り続けると、気づいたら「インデックスのインデックス」が欲しくなる世界へようこそ。GIN Index（Default）GIN（Generalized Inverted Index）を作成します。CREATE INDEX idx_pizza_orders_ginON pizzeria.order_itemsUSING GIN(pizza);GIN の仕組み（5.6.2 節の Figure 5.1 参照）です。JSON オブジェクトからすべてのキーと値を抽出して、個別のインデックスエントリとして保存します。例です。{  \"size\": \"large\",  \"type\": \"three cheese\",  \"crust\": \"thin\",  \"sauce\": \"marinara\",  \"toppings\": {    \"cheese\": [      {\"cheddar\": \"regular\"},      {\"mozzarella\": \"extra\"},      {\"parmesan\": \"light\"}    ]  }}インデックスに保存されるエントリです。Keys:- size、type、crust、sauce、toppings、cheese、cheddar、mozzarella、parmesanValues:- large、three cheese、thin、marinara、regular、extra、lightこれらのエントリは辞書順に保存され、複数のインデックスページに分散されます。GIN を使ったクエリです。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"type\": \"custom\"}';実行計画。Bitmap Index Scan on idx_pizza_orders_gin (actual time=0.109..0.110 rows=563 loops=1)  Index Cond: (pizza @\u003e '{\"type\": \"custom\"}'::jsonb)Execution Time: 0.830 ms複雑なネスト構造でも使えます。SELECT count(*)FROM pizzeria.order_itemsWHERE pizza @\u003e '{\"toppings\":{\"cheese\":[{\"cheddar\":\"regular\"}]}}';Postgres はインデックスから toppings, cheese, cheddar, regular の 4 つのエントリを検索して、該当する行を絞り込みます。GIN のメリット：1 つのインデックスで JSON 全体を検索可能です。GIN Index with jsonb_path_opsさらに効率的な GIN インデックスです。CREATE INDEX idx_pizza_orders_paths_ops_ginON pizzeria.order_itemsUSING GIN (pizza jsonb_path_ops);違いは、パス全体をハッシュ化して保存することです。例です。size.largetype.three cheesecrust.thinsauce.marinaratoppings.cheese.cheddar.regulartoppings.cheese.mozzarella.extratoppings.cheese.parmesan.lightこれらのパスをハッシュ関数に通して、固定長の整数として保存します。メリットです。検索が速い：固定長整数の比較は可変長テキストより速いサイズが小さい：ハッシュコードはテキストより小さい実際のサイズ比較です。index_name                      | index_size--------------------------------|------------idx_pizza_orders_gin            | 112 kBidx_pizza_orders_paths_ops_gin  | 56 kB半分のサイズです。デメリットです。jsonb_path_ops は ? 演算子（キー存在確認）をサポートしません。なぜなら、インデックスにはパスのハッシュのみが保存されていて、キー単体は保存されていないからです。-- これは idx_pizza_orders_gin を使う（jsonb_path_ops は使えない）SELECT count(*)FROM pizzeria.order_itemsWHERE pizza ? 'special_instructions';使い分け インデックスタイプ  サイズ  検索速度  サポート演算子  推奨用途  Expression Index (B-tree)  小  特定 expression のみ速い  -\u003e, -\u003e\u003e  特定フィールドの頻繁な検索  GIN (default)  大  速い  ?, @\u003e, @?, @@  柔軟な検索、キー存在確認が必要  GIN (jsonb_path_ops)  中  最速  @\u003e, @?, @@  包含検索のみ、サイズ重視 著者が書いているように、jsonb_path_ops が第一選択です。キー存在確認が必要なら default GIN を追加します。6. Postgres for full-text search「全文検索は難しい」という思い込みこの章を読み終えて思ったのは、「Postgres の全文検索は、思ったより実用的だ」ということです。私はこれまで、全文検索といえばElasticsearchだと思っていました。実際、過去のプロジェクトで「検索機能が必要です」と言われたら、反射的に「Elasticsearch を構築しますか？」と答えていました。まるで、パブロフの犬のように。「検索」という言葉を聞いただけで、脳内で Kibana のダッシュボードが立ち上がっていました。でも、第 1 章で学んだ「Just use Postgres」の真の意味を思い出します。「別のデータベースを追加する前に、まず Postgres で解決できるか確認してみよう」この章は、その実践編でした。Tokenization と Normalization の仕組み6.1 節では、Postgres が全文検索をどう実現しているかが説明されています。基本的な流れは 4 ステップです。Tokenization（トークン化）: 文書を単語やフレーズに分割Normalization（正規化）: トークンを lexeme（語彙素）に変換Storing and Indexing: lexeme を tsvector 型で保存し、インデックスを作成Searching: 保存した lexeme に対してクエリを実行著者が ts_debug 関数を使って、\"5 explorers are traveling to a distant galaxy\" という文がどう処理されるかを見せてくれます。SELECT token, description, lexemes, dictionaryFROM ts_debug('5 explorers are traveling to a distant galaxy');結果を見ると、\"explorers\" は \"explor\" に、\"traveling\" は \"travel\" に変換されています。ストップワード（\"are\", \"to\", \"a\"）は空の lexeme {} にマッピングされています。これがステミング（語幹抽出）です。試しに to_tsvector('english', 'running runs runner') を実行してみると、'run':1,2 'runner':3 と返ってきます。running と runs は run に統一されています。だから「running」で検索しても「runs」がヒットする。これは便利です。位置情報を保持しながらストップワードを削除するという設計が巧妙です。\u003c-\u003e (FOLLOWED BY) オペレータで距離を計算するために、ストップワードの位置も必要になるからです。Elasticsearch でも同じようなことをやっているはずですが、Postgres ではこれが標準機能だということに改めて気づかされました。複数言語への対応6.1.2 節では、Full-text search configuration が紹介されています。Postgres には英語だけでなく、アラビア語、ロシア語、日本語など、多数の言語用の predefined configuration が用意されています。SELECT token, description, lexemes, dictionaryFROM ts_debug('russian','5 исследователей путешествуют к далёкой галактике.');ロシア語の例を見ると、russian_stem 辞書が使われています。\"исследователей\" が \"исследовател\" に、\"путешествуют\" が \"путешеств\" に変換されています。これも Elasticsearch でやろうとすると、analyzer の設定が複雑になります。JSON の設定ファイルを書いて、tokenizer を選んで、filter を設定して、mapping を更新する作業が必要です。設定の沼にハマっていきます。Postgres ではデフォルトで対応しています。でも、ここで疑問が湧きました。日本語はどうなんだろう？この本では日本語の例は出てきません。調べてみると、日本語は形態素解析が必要で、Postgres の標準機能だけでは難しいようです。pg_bigm（2-gram ベース）や pgroonga（Groonga ベース）といった拡張機能が必要になります。「Postgres で試した？」と聞き返す前に、日本語対応が必要かどうかは確認が必要な部分だと思いました。英語圏のサービスなら問題ないですが、日本語がメインなら追加の検討が必要です。tsvector と generated column の活用6.2 節では、生成した lexeme をどう保存するかが説明されています。3 つの選択肢があります。On-the-fly 生成: クエリごとに to_tsvector を実行（非効率）Column に保存: tsvector 型のカラムを追加して保存（推奨）Index のみ: テーブルには保存せず、直接インデックス作成（ストレージ節約）著者は 2 番目の方法を推奨しています。ALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (  to_tsvector(    'english', coalesce(name, '') ||    ' ' ||    coalesce(description, ''))) STORED;GENERATED ALWAYS AS ... STORED という構文が便利です。これで、name や description が変更されると、lexemes も自動的に再生成されます。ただし、configuration は明示的に指定する必要があります（'english'）。これは、generated column の式が immutable でなければならないからです。この辺りの設計判断は、実際に運用してみないと分からない部分が多そうです。全文検索クエリの実行6.3 節では、実際のクエリの書き方が紹介されています。plainto_tsquery: シンプルなクエリSELECT id, nameFROM omdb.moviesWHERE lexemes @@ plainto_tsquery('a computer animated film');plainto_tsquery は、ユーザーが入力した自然な文章を tsquery 型に変換してくれます。ストップワード（\"a\"）を削除し、残りの単語を lexeme に変換して、\u0026 (AND) オペレータで結合します。結果：'comput' \u0026 'anim' \u0026 'film'この手軽さが良いです。Elasticsearch なら、query DSL を書く必要があります。plainto_tsquery と to_tsquery の違いを実際に確認してみました。plainto_tsquery('english', 'ghost in shell') は 'ghost' \u0026 'shell' を返します。「in」はストップワードとして除去されています。一方、to_tsquery は構文を直接指定できるので、OR 検索や NOT 検索も可能です。to_tsquery: 高度なフィルタリングSELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('computer \u0026 animated      \u0026 (lion | clownfish | donkey)');to_tsquery を使えば、AND、OR、NOT、FOLLOWED BY などのオペレータを直接指定できます。SELECT id, nameFROM omdb.moviesWHERE lexemes @@ to_tsquery('lion \u0026 !''The Lion King''');NOT オペレータで特定のフレーズの除外も可能です。この柔軟性は、Elasticsearch と変わりません。むしろ、SQL の中で完結するので、アプリケーション側のコードがシンプルになります。ランキングと重み付け6.4 節では、検索結果のランキングが扱われています。ts_rank による関連度スコアSELECT id, name, vote_average,  ts_rank(lexemes, to_tsquery('ghosts')) AS search_rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('ghosts')ORDER BY search_rank DESC, vote_average DESC NULLS LAST LIMIT 10;ts_rank 関数は、lexeme の出現頻度と位置に基づいてスコアを計算します。でも、最初の実行例では、タイトルに \"ghost\" が含まれる映画と、説明文にだけ含まれる映画が同じように扱われていました。setweight による重み付けALTER TABLE omdb.moviesADD COLUMN lexemes tsvectorGENERATED ALWAYS AS (    setweight(to_tsvector('english', coalesce(name, '')), 'A') ||    setweight(to_tsvector('english', coalesce(description, '')), 'B')) STORED;setweight 関数で、タイトル由来の lexeme に A ラベル、説明文由来の lexeme に B ラベルを付けます。重みは A \u003e B \u003e C \u003e D の順で、デフォルトは D です。これで、ts_rank はタイトルに含まれる単語をより高くランク付けするようになります。   id   |         name          | vote_average | search_rank--------+-----------------------+--------------+-------------    251 | Ghost                 | 6.3333301544 |   0.6957388 210675 | A Most Annoying Ghost |              |   0.6957388   1548 | Ghost World           | 8.1428575516 |  0.66871977タイトルへ \"ghost\" が含まれる映画が上位へ来るようになりました。この重み付けのメカニズムは、Elasticsearch の boosting と同じ発想です。でも、Postgres では setweight 一発で実現できます。ハイライト表示6.5 節では、ts_headline 関数が紹介されています。SELECT id, name, description,    ts_headline(description, to_tsquery('pirates')) AS fragments,    ts_rank(lexemes, to_tsquery('pirates')) AS rankFROM omdb.moviesWHERE lexemes @@ to_tsquery('pirates:B')ORDER BY rank DESC LIMIT 1;結果はこうなります。fragments   | \u003cb\u003epirate\u003c/b\u003e Captain Jack is in a battle with the ocean ➥  itself. Jack knows it won't be easyマッチした単語を \u003cb\u003e タグで囲んでくれます。さらに、オプションでカスタマイズも可能です。ts_headline(description, to_tsquery('pirates'),    'MaxFragments=3, MinWords=5, MaxWords=10,     FragmentDelimiter=\u003cft_end\u003e') AS fragmentsただし、著者が警告している通り、XSS 攻撃のリスクがあります。HTML マークアップを含む文書を扱う場合は、サニタイズが必要です。この辺りは、Elasticsearch でも同じ問題があります。ハイライト機能は便利ですが、セキュリティには注意が必要です。インデックスの選択：GIN vs GiST6.6 節では、全文検索を高速化するためのインデックスが説明されています。GIN インデックスCREATE INDEX idx_movie_lexemes_ginON omdb.moviesUSING GIN (lexemes);GIN（Generalized Inverted Index）は、全文検索に最適化されたインデックスです。各 lexeme ごとにインデックスエントリを作成し、その lexeme を含むテーブル行への参照を保持します。実行計画を見ると、Seq Scan（15.328 ms）から Bitmap Index Scan（0.150 ms）に変わっています。100 倍以上の高速化です。ただし、GIN インデックスは positional information を保存しません。\u003c-\u003e (FOLLOWED BY) オペレータを使うクエリでは、テーブル行を再確認する必要があります。この制約を解決したい場合は、RUM インデックス（Postgres 拡張）を使うと良いようです。GiST インデックスCREATE INDEX idx_movie_lexemes_gistON omdb.moviesUSING GIST (lexemes);GiST（Generalized Search Tree）は、signature tree を構築します。各文書の signature（ビット列）を作成し、lexeme の signature を bitwise OR で結合します。実行時間は 0.395 ms で、GIN（0.150 ms）より遅いです。理由は、signature collision が発生するため、マッチした文書をテーブル行で再確認する必要があるからです。でも、GiST は インデックスサイズが小さく、更新が速いという特徴があります。使い分け著者の推奨はこうです。GIN: 検索速度が最重要で、インデックスメンテナンスコストを許容できる場合GiST: インデックスサイズや更新速度が重要な場合この辺りの判断は、データ量や更新頻度によって変わります。実際に両方試してみる価値があります。Postgres の限界を認識するもちろん、Postgres の全文検索にも限界はあります。日本語の形態素解析はサポートされていない（可能性が高い）大規模データ（数億レコード）では Elasticsearch の方が速い可能性がある分散検索や複雑な aggregation は Elasticsearch の方が得意でも、多くのケースでは Postgres で十分というのがこの章の主張です。7. Postgres extensions拡張性こそが「Just use Postgres」の核心第 7 章を読んで、ようやく腑に落ちました。「Just use Postgres」というモットーは、Postgres の拡張機能によって生まれました。この一文を読んだとき、第 1 章の理解が深まりました。In fact, the motto \"Just use Postgres\" emerged largely due to its rich ecosystem of extensions, which allow us to use the database well beyond the use cases covered in the earlier chapters of the book.第 1 章では「新しいユースケースが発生したとき、まず Postgres で解決できるか確認しよう」という意味だと学びました。でも、なぜ Postgres で解決できるのかという根拠は曖昧でした。答えは拡張機能でした。JSON、全文検索、時系列、地理空間、メッセージキュー、ベクトル検索——これら全て、Postgres の拡張機能が可能にしています。第 2 章から第 6 章までは、コア機能を使ったユースケースでした。でも、それは「氷山の一角」だったんです。本当の多様性は拡張機能にあります。Michael Stonebraker のビジョン7.1 節で、Postgres の拡張性が生まれた背景が語られています。Michael Stonebraker（チューリング賞受賞者）の言葉が印象的でした。1980 年代、多くの研究論文が同じことを言っていました：「リレーショナルデータベースは素晴らしいと言われているが、実際には特定のシナリオでまったく機能しない」そして、それぞれの論文が独自の解決策を提案していました。Stonebraker はこう考えました：それぞれの問題へ個別の解決策を追加するのではなく、RDBMS が特定のユースケースへ適応できるようにする、より良い方法があるはずだ。この哲学が、Postgres の設計思想の根幹になっています。拡張性が Postgres の強みであることは知っていました。ただ、この章を読んで、Postgres は最初から拡張性を前提に設計されているという設計思想を改めて確認できました。インフラエンジニアとして、この設計思想は深く刺さります。運用の現場では、予期しないユースケースが次々に現れます。そのたびに新しいデータベースを追加していたら、運用負荷は青天井です。気づけば Kubernetes クラスタの中に MongoDB、Redis、Elasticsearch、TimescaleDB、Neo4j が同居しています。「あれ、俺たちデータベース動物園を運用してたっけ？」と遠い目をする羽目になります。Postgres は、そういう現実を 40 年以上前から見据えていたんです。拡張性を支える 3 つの基盤7.2 節では、Postgres の拡張性を支える技術的な基盤が説明されています。カタログ駆動操作Postgres は、テーブル、カラム、データ型、関数などのメタデータをシステムカタログに保存しています。これは通常のテーブルと似た構造で、拡張機能はこのカタログを読み書きできます。これ、地味だけど重要だと思いました。システムカタログが「普通のテーブルのような構造」だから、拡張機能が新しいデータ型や関数を追加できます。もし、メタデータが隠蔽された独自フォーマットだったら、拡張機能の開発はもっと難しかったでしょう。データベースフックPostgres のコードベースには、拡張機能がカスタムロジックを注入できるフックポイントが定義されています。クエリ計画、実行、認証など、様々なイベントにフックできます。これ、Linux カーネルの LSM（Linux Security Modules）に似ていると思いました。カーネル本体を変更せずに、セキュリティポリシーを注入できる仕組みです。Postgres も同じ哲学です。コアエンジンを変更せずに、動作を拡張できます。動的ロード拡張機能のロジックは、SQL、PL/pgSQL、C、Rust など、様々な言語で書けます。SQL や PL/pgSQL で書かれた拡張機能は、データベースエンジンが直接解釈します。C や Rust で書かれた拡張機能は、共有ライブラリとして実行時に動的にロードされます。コアエンジンの再コンパイルが不要です。これが重要です。もし、拡張機能を追加するたびに Postgres 本体を再コンパイルしなければならないとしたら、運用はほぼ不可能でした。動的ロードのおかげで、拡張機能の追加・削除が柔軟にできます。pgcrypto を使ってみた感覚7.2.2 節では、pgcrypto 拡張機能を使ったユーザー認証の例が紹介されています。CREATE EXTENSION pgcrypto;INSERT INTO accounts (username, password_hash)VALUES ('ahamilton', crypt('SuperSecret123', gen_salt('bf')));gen_salt('bf') で Blowfish アルゴリズムを使ったソルトを生成し、crypt() で平文パスワードとソルトからハッシュを生成します。この例を読んで、「データベース内で暗号化を完結させる」という選択肢があることに気づきました。これまで、パスワードのハッシュ化はアプリケーション層でやるものだと思い込んでいました。でも、pgcrypto を使えば、データベース層でも実装できます。どちらが良いかはケースバイケースでしょう。でも、選択肢があることを知っておくのは重要です。認証のクエリも興味深いです。SELECT username FROM accountsWHERE username = 'ahamilton'AND password_hash = crypt('SuperSecret123', password_hash);crypt() 関数に、平文パスワードと保存済みのハッシュを渡します。関数がハッシュからソルトを抽出し、再計算して比較します。この設計、エレガントだと思いました。ソルトを別カラムに保存する必要がありません。ハッシュ自体にソルトが含まれています。ちなみに、bcrypt のコストパラメータ（gen_salt('bf', 8) の 8 の部分）は、8〜12 が推奨されています。数字が大きいほどハッシュ計算に時間がかかりますが、セキュリティは向上します。拡張機能の 5 つのカテゴリ7.3 節では、拡張機能を 5 つのカテゴリに分類しています。\"Postgres beyond relational\"Postgres を従来の RDBMS を超えた用途に拡張します。pgvector、pg_ai、pgvectorscale: ベクトルデータベース（生成 AI ワークロード）TimescaleDB: 時系列データベースPostGIS: 地理空間データベースpgmq: メッセージキューpg_duckdb: 高性能分析ワークロード（DuckDB の列指向エンジンを埋め込み）これらが「Just use Postgres」を可能にしている拡張機能です。「Elasticsearch で検索やりたい」「MongoDB で JSON 保存したい」「Redis でキューやりたい」というよくある要求があります。これらに対して、「まず Postgres で試した？」と聞き返せる根拠です。過去の自分に教えてあげたいです。技術選定会議で『最新トレンド』として提案された 3 つのデータベース、実は Postgres の拡張機能で済むやつだから、と。プログラミング言語と手続き型言語第 2 章で PL/pgSQL を学びましたが、それだけではありません。PLV8: JavaScriptPL/Java: JavaPL/Python: PythonPL/Rust: Rust自分の得意な言語で、データベース関数やプロシージャを書けます。特に PLV8 の説明が興味深いです。V8 JavaScript エンジンを Postgres に埋め込むだけでなく、PgCompute クライアントライブラリと組み合わせることで実現します。アプリケーションから SQL を介さずに JavaScript 関数を直接実行できます。これ、SQL とアプリケーションロジックの境界を曖昧にする、面白いアプローチだと思いました。コネクタと外部データラッパー外部のデータソースを、あたかも Postgres のテーブルであるかのようにクエリできます。file_fdw: ファイルシステムからデータを読むpostgres_fdw、mysql_fdw、oracle_fdw、sqlite_fdw: 他の SQL データベースに接続redis_fdw、parquet_s3_fdw、kafka_fdw: Redis、S3、Kafka などの非 SQL データソースに接続Postgres を統合データレイヤーとして使えます。これ、マイクロサービスアーキテクチャで複数のデータソースを扱う場合に便利そうです。各サービスが独自のデータベースを持っていても、Postgres を経由して統一的にクエリできます。でも、パフォーマンスはどうなんでしょう。ネットワーク越しにクエリを投げるわけですから、レイテンシーは増えるはずです。この辺りは実際に試してみないとわかりません。（試した結果「遅い！」ってなって、結局専用のデータ同期パイプラインを構築するところまでがテンプレ。）クエリとパフォーマンス最適化pg_stat_statements: SQL 文の実行統計を追跡auto_explain: 遅いクエリの実行計画を自動ログhypopg: 仮想インデックスのテストauto_explain は便利そうです。普段、遅いクエリを見つけたら、手動で EXPLAIN ANALYZE を実行しています。でも、auto_explain があれば、自動的にログに記録してくれます。hypopg も面白いです。実際にインデックスを作らずに、仮想的にテストできます。本番環境で「このインデックス、効果あるかな？」と試す前に、リスクなしで検証できます。ツールとユーティリティpg_cron: cron ベースのスケジューラーPostgreSQL Anonymizer: 個人情報の匿名化pgaudit: 監査ログpg_partman: パーティション管理の簡素化pg_cron があれば、データベース内で定期タスクを実行できます。外部の cron や Airflow を使わずに。「Just use Postgres」の精神に沿っています。Postgres 互換ソリューション7.4 節では、Postgres の拡張機能ではなく、Postgres のプロトコルやソースコードを活用した別のソリューションが紹介されています。拡張機能で解決できない問題のために、こういった選択肢があります。ゼロから構築されたソリューションGoogle SpannerCockroachDBPostgres のワイヤレベルプロトコル、DML/DDL 構文、一部の機能をサポートしています。でも、内部実装は完全に別物です。分散データベースとしての可用性とスケーラビリティを提供します。Postgres ソースコードをベースにしたソリューションNeon（サーバーレスデータベース）YugabyteDB（分散データベース）Postgres のソースコードを再利用しつつ、ストレージレイヤーを変更・拡張しています。Postgres のアプリケーションをそのまま実行できます。ライブラリ、ツール、フレームワークもそのまま使えます。この 2 つのアプローチの違いは興味深いです。ゼロから構築したソリューションは、自由度が高い反面、Postgres との互換性は限定的になります。Postgres ソースコードベースのソリューションは、互換性が高い反面、アーキテクチャの変更範囲は制約されます。どちらが良いかは、ユースケース次第です。でも、どちらも「Postgres のエコシステムを活用したい」という需要から生まれています。それだけ、Postgres が広く使われているということです。8. Postgres for generative AIPostgres が Vector Database になる瞬間第 8 章を読んで最初に感じたのは、「Just use Postgres」が生成 AI の時代でも貫かれているということでした。「RAG を実装するなら Pinecone か Weaviate を使おう」——これまでそう考えていました。でも、著者が示すのは違います。Postgres can serve as a powerful vector database for implementing RAG and other gen AI use cases.既に Postgres を使っているなら、まず Postgres で試してみよう。この章はその具体的な実装方法を示しています。pgvector という選択肢pgvector という拡張を有効化するだけで、Postgres が Vector Database になります。CREATE EXTENSION vector;たったこれだけ。新しいデータベースを立てる必要がありません。（「Vector Database 導入提案書」を 3 日かけて書いた過去の自分に教えてあげたい...）vector(1024) という型が使えるようになります。1024 次元のベクトル埋め込みを格納できます。映画の説明文を mxbai-embed-large モデルで変換した埋め込みを、そのまま Postgres のカラムに保存できます。CREATE TABLE omdb.movies (    id BIGINT PRIMARY KEY,    name TEXT NOT NULL,    description TEXT NOT NULL,    movie_embedding VECTOR(1024),    ...);この手軽さ。Docker で pgvector 入りの Postgres を起動するだけで試せます。docker run --name postgres-pgvector \\    -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=password \\    -p 5432:5432 \\    -d pgvector/pgvector:0.8.0-pg17「Vector Database を導入しましょう」という提案をする前に、「Postgres で試した？」と聞き返せるようになりました。Cosine Distance とベクトル類似検索埋め込みを保存するだけじゃありません。類似検索もできます。SELECT id, name, descriptionFROM omdb.moviesORDER BY movie_embedding \u003c=\u003e omdb.get_embedding('May the force be with you')LIMIT 3;\u003c=\u003e は Cosine Distance を計算する演算子です。pgvector が提供しています。この SQL を実行すると、「May the force be with you」というフレーズに最も関連する映画が返ってきます。当然、Star Wars の映画がトップに来ます。埋め込みモデルが学習した「意味の空間」の中で、近い映画を見つけてくれます。でも、最初は全件スキャンになります。4,000 件程度なら許容できますが、規模が大きくなったら？そこでインデックスが必要になります。IVFFlat と HNSW——2 つのインデックス戦略pgvector は 2 種類のインデックスをサポートしています。IVFFlat: クラスタリングベースの高速化CREATE INDEX movie_embeddings_ivfflat_idxON omdb.moviesUSING ivfflat (movie_embedding vector_cosine_ops)WITH (lists = 5);IVFFlat は埋め込みをクラスタ (リスト) に分割します。k-means でセントロイドを計算し、各埋め込みを最も近いセントロイドのリストに配置します。検索時は、クエリの埋め込みに最も近いセントロイドのリストだけをスキャンします。全件スキャンを避けられます。でも、これは近似検索 (ANN: Approximate Nearest Neighbor) です。真の最近傍が他のリストにいたら、見逃す可能性があります。Recall (再現率) が完璧じゃありません。ivfflat.probes パラメータで、スキャンするリスト数を増やせます。Recall は改善しますが、検索速度は落ちます。BEGIN;SET LOCAL ivfflat.probes = 2;SELECT ...COMMIT;トレードオフです。HNSW: 階層グラフによる高精度検索CREATE INDEX movie_embeddings_hnsw_idxON omdb.moviesUSING hnsw (movie_embedding vector_cosine_ops)WITH (m = 8, ef_construction = 16);HNSW は多層グラフを構築します。上位層は疎で、下位層ほど密になります。検索は最上層から始まり、段階的に下層に降りていきます。高速かつ高精度です。著者の実験では、HNSW は IVFFlat より Recall が良いです。データが追加・更新されても Recall が安定しています。インフラエンジニアとして、この安定性は魅力的です。 データが増えても再インデックスが不要です。IVFFlat はセントロイドが固定されるため、データが大きく変化すると Recall が落ちます。映画カタログは継続的に成長します。HNSW を選ぶ理由があります。（夜中の 2 時に「Recall が落ちてます！」というアラートで起こされるのは、もう懲り懲りです）実際に試してみてわかったのは、ベクトルはランダム生成でも類似検索の動作確認は可能だということ。ジャンルごとにパターンを変えれば、「アクション映画同士が近くなる」という挙動を確認できます。本番データがなくても、仕組みの理解には十分です。RAG の実装——Postgres を中心にこの章の核心は、RAG (Retrieval-Augmented Generation) の実装です。RAG の流れです。ユーザーが質問を入力質問を埋め込みに変換 (mxbai-embed-large)Postgres でベクトル類似検索を実行検索結果をコンテキストとして LLM に渡すLLM がコンテキストを考慮して回答を生成著者は Python の Jupyter Notebook で実装を示しています。LLM には TinyLlama (640 MB、1.1 B パラメータ) を使用しています。def retrieve_context_from_postgres(question):    # 埋め込みモデルに接続    embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")    # 質問を埋め込みに変換    embedding = embedding_model.embed_query(question)    # Postgres でベクトル類似検索    query = \"\"\"    SELECT name, vote_average, budget, revenue, release_date    FROM omdb.movies    ORDER BY movie_embedding \u003c=\u003e %s::vector LIMIT 3    \"\"\"    cursor.execute(query, (embedding, ))    # コンテキストを構築    context = \"\"    for row in cursor.fetchall():        context += f\"Movie title: {row[0]}, Vote Average: {row[1]}, ...\"    return contextPostgres から取得した映画情報を LLM に渡します。def answer_question(question, context):    llm = OllamaLLM(model=\"tinyllama\", temperature=0.6)    prompt = f\"\"\"    You're a movie expert and your task is to answer questions about movies    based on the provided context.    This is the user's question: {question}    Consider the following context: {context}    Respond in an engaging style that inspires the user to watch the movies.    \"\"\"    response = llm.invoke(prompt)    return response「海賊映画のおすすめは？」と聞くと、Postgres が Pirates of the Caribbean シリーズを返し、LLM がそれをもとに魅力的な推薦文を生成します。Postgres が RAG のコンテキスト取得レイヤーとして機能しています。LLM は statelessこの章で確認しておきたいのは、「LLM は stateless」という点です。Because LLMs are stateless—meaning they don't retain the history of the interaction—if we want the LLM to consider earlier conversation history, we need to store it separately and pass it to the prompt object.LLM は会話履歴を記憶していません。毎回、コンテキストと履歴を渡す必要があります。この設計は、Postgres のステートレス性とも通じます。Postgres はクライアントのセッション状態を保持しません (connection pooling の文脈で)。毎回のクエリは独立しています。だから、会話履歴も Postgres に保存して、RAG のコンテキストとして渡せばいいのです。全てが Postgres で完結します。確認しておきたい拡張pgai という拡張は、この章で初めて目にしました。Explore the pgai extension if you'd like to implement the RAG workflow purely in SQL and execute it entirely within the database.SQL だけで RAG を実装できます。アプリケーション側に gen AI フレームワークを導入する必要がありません。調べてみたいです。もし実用的なら、Postgres の可能性がさらに広がります。9. Postgres for time seriesTimescaleDB を改めて評価するこの章で取り上げられている TimescaleDB は、名前は知っていましたが、実際に採用を検討したことはありませんでした。時系列データベースと言えば、InfluxDB か Prometheus を中心に検討してきました。「時系列データを扱いたいなら専用のデータベースを追加しましょう」という提案をしてきたこともあります。でも、この章を読んで気づきました。Postgres の拡張機能で時系列データベースができます。「Just use Postgres」の考え方が、ここでも貫かれています。新しいデータベースを追加する前に、まず Postgres で解決できるか確認します。TimescaleDB はその選択肢の 1 つです。運用エンジニアとしては、これは大きいです。新しいデータベースを追加するたびに、バックアップ戦略、モニタリング、アラートルール、障害対応手順が増えます。チームのメンバーも新しい技術を学ばなければいけません。もし Postgres の拡張機能で解決できるなら、運用負荷は格段に減ります。この章を読み終えて、「次に時系列データの相談が来たら、TimescaleDB を試してみよう」と思いました。Postgres のパーティショニングと Hypertable9.1 節では、Postgres のテーブルパーティショニングが紹介されています。CREATE TABLE heart_rate_measurements (  watch_id INT NOT NULL,  recorded_at TIMESTAMPTZ NOT NULL,  heart_rate INT NOT NULL,  activity TEXT NOT NULL CHECK (      activity IN ('walking', 'sleeping', 'resting', 'workout'))) PARTITION BY RANGE (recorded_at);PARTITION BY RANGE (recorded_at) で、recorded_at カラムの値に基づいてテーブルを範囲でパーティション分割する。その後、各パーティションを手動で作成する必要がある。CREATE TABLE measurements_jan2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');CREATE TABLE measurements_feb2025    PARTITION OF heart_rate_measurements    FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');このパーティショニング自体は Postgres の標準機能です。時系列データの場合、直近のデータだけが頻繁にアクセスされて、古いデータは圧縮したり削除したりします。パーティショニングを使えば、それが簡単にできます。でも、パーティションの作成と管理は手動でやる必要があります。著者も書いていますが、pg_partman と pg_cron という拡張機能を使えば自動化できます。そして、9.2 節で登場するのが TimescaleDB です。SELECT create_hypertable(  relation =\u003e 'watch.heart_rate_measurements',  dimension =\u003e by_range('recorded_at', interval '1 month'),  create_default_indexes =\u003e false);この一文で、テーブルが Hypertable に変換されます。Hypertable は Postgres の通常のテーブルですが、TimescaleDB が自動的にパーティション（chunk と呼ばれる）を作成・管理してくれます。新しいデータが挿入されると、TimescaleDB が自動的に新しい chunk を作ります。INSERT INTO watch.heart_rate_measurements VALUES(1,'2025-12-08 00:25:00',57,'sleeping');この INSERT だけで、_timescaledb_internal._hyper_1_13_chunk という新しいパーティションが自動生成されます。手動でパーティションを作る必要がありません。これは大きいです。timescaledb_information.chunks でチャンクのメタデータを確認できます。実際に確認してみると、日付ごとにチャンクが自動生成されていることがわかります。_hyper_1_1_chunk | 2025-01-01 - 2025-01-02_hyper_1_2_chunk | 2025-01-02 - 2025-01-03_hyper_1_3_chunk | 2025-01-03 - 2025-01-04この透過性が TimescaleDB の魅力です。過去のプロジェクトで、パーティショニングを手動で管理していたことがあります。月次バッチで次月のパーティションを作成するスクリプトを cron で回していました。でも、そのスクリプトが失敗したことに気づかず、翌月の INSERT が全部エラーになりました。月初の朝、Slack が火を噴きました。「データが入らない！」というメッセージが次々と流れてくる。あの日の朝のコーヒーは、確実に苦かったです。TimescaleDB を使っていれば、そんなことは起きませんでした。というか、あの朝のコーヒーはもっと美味しかったはずです。データ保持ポリシーの自動化9.4 節では、データ保持ポリシー（retention policy）の話が出てきます。SELECT add_retention_policy(  'watch.heart_rate_measurements', INTERVAL '30 days');これだけで、30 日以上古いデータを自動的に削除するジョブが設定されます。運用の観点から、これは非常にありがたいです。時系列データは増え続けます。ディスク容量は有限です。古いデータを定期的に削除する必要があります。過去のプロジェクトでは、手動で SQL を書いて、古いパーティションを DROP していました。でも、これも失敗することがあります。削除スクリプトのバグで、間違ったパーティションを削除してしまったこともありました。具体的に言うと、measurements_jan2025 を消すはずが measurements_jan2024 を消しました。そう、1 年分のデータが吹っ飛びました。バックアップから復旧しましたが、あの日の胃痛は今でも忘れられません。エンジニアのキャリアにおいて、誰もが一度は通る「DELETE/DROP の洗礼」というやつです。TimescaleDB の retention policy を使えば、そのリスクが減ります。胃痛も減ります。ただし、著者も警告していますが、このコマンドは慎重に使う必要があります。間違った設定をすると、重要なデータを失う可能性があります。time_bucket 関数の威力9.5 節では、TimescaleDB の time_bucket 関数が紹介されています。SELECT  time_bucket('10 minutes', recorded_at) AS period, activity,  AVG(heart_rate)::int AS avg_rate, MAX (heart_rate)::int AS max_rateFROM watch.heart_rate_measurementsWHERE watch_id = 1 AND activity = 'workout'  AND recorded_at \u003e= '2025-04-23' AND recorded_at \u003c '2025-04-24'GROUP BY period, activity ORDER BY period;これで、10 分ごとのバケットに心拍数を集約できます。普通の SQL でやろうとすると、DATE_TRUNC や複雑な計算が必要になります。でも、time_bucket を使えば、読みやすいクエリで簡単に集約できます。さらに、time_bucket はタイムゾーンの指定もできます。SELECT time_bucket('1 week', recorded_at, 'Asia/Tokyo',  '2025-04-01'::timestamptz) AS period, activity,  AVG(heart_rate)::int AS avg_rate,  MAX (heart_rate)::int AS max_rate, MIN (heart_rate)::int AS min_rateFROM watch.heart_rate_measurementsWHERE watch_id = 2 AND recorded_at \u003e= '2025-04-01'AND  recorded_at \u003c '2025-04-15'GROUP BY period, activity ORDER BY period, activity;ユーザーごとに異なるタイムゾーンでデータを集約できます。これはグローバルなサービスでは必須の機能です。そして、time_bucket_gapfill 関数です。SELECT watch_id, time_bucket_gapfill('1 minute', recorded_at) AS minute,  LOCF(AVG(heart_rate)::int) AS avg_rateFROM watch.heart_rate_measurementsWHERE watch_id=1 AND recorded_at BETWEEN '2025-03-02 07:25'  AND '2025-03-02 07:36'GROUP BY watch_id, minute ORDER BY minute;データが欠けている時間帯も含めて、連続した時間バケットを作成してくれます。さらに、LOCF（Last Observation Carried Forward）関数を使えば、欠損値を最後の値で埋めることができます。過去に、時系列データのグラフを作ったことがあります。データに欠損があると、グラフが途切れてしまいます。アプリ側で欠損値を補間する処理を書きましたが、複雑でした。time_bucket_gapfill と LOCF を使えば、データベース側で簡単に処理できます。Continuous Aggregates という機能9.6 節では、Continuous Aggregates（継続的集約）が紹介されています。CREATE MATERIALIZED VIEW watch.low_heart_rate_count_per_5minWITH (timescaledb.continuous) ASSELECT  watch_id,  time_bucket('5 minutes', recorded_at) AS bucket,  MIN(heart_rate) as min_rate,  COUNT(*) FILTER (WHERE heart_rate \u003c 50) AS low_rate_count,  COUNT(*) AS total_measurementsFROM watch.heart_rate_measurementsGROUP BY watch_id, bucket;これは Postgres の Materialized View（マテリアライズドビュー）ですが、TimescaleDB が自動的にリフレッシュしてくれます。リフレッシュポリシーも設定できます。SELECT add_continuous_aggregate_policy  ('watch.low_heart_rate_count_per_5min',  start_offset =\u003e INTERVAL '15 minutes',  end_offset =\u003e INTERVAL '1 minute',  schedule_interval =\u003e INTERVAL '1 minute');これで、1 分ごとに集約結果が更新されます。普通の Materialized View は、手動で REFRESH MATERIALIZED VIEW を実行しないと更新されません。でも、TimescaleDB の Continuous Aggregates は自動的に更新されます。しかも、Hypertable に保存されるので、パーティショニングの恩恵も受けられます。この章の例では、心拍数が 50 BPM 以下の回数をカウントして、徐脈（bradycardia）の兆候を検出しています。リアルタイムで集約結果を更新して、ユーザーにアラートを送ります。これ、単なるデモではありません。実用的です。過去に、IoT デバイスからのデータを集約して、異常を検知するシステムを運用したことがあります。集約処理は別のバッチジョブで定期的に実行していました。でも、リアルタイム性が求められると、バッチでは間に合いません。TimescaleDB の Continuous Aggregates を使えば、リアルタイムに近い形で集約結果を更新できます。B-tree インデックスと BRIN インデックス9.7 節では、時系列データのインデックス戦略が紹介されています。まず、B-tree インデックスです。CREATE INDEX heart_rate_btree_idxON watch.heart_rate_measurements (recorded_at, watch_id);複合インデックスで、recorded_at と watch_id の両方を含めます。これで、時間範囲とデバイス ID の両方で絞り込むクエリが高速化されます。著者の説明によれば、B-tree インデックスは実際のカラム値とテーブル行へのポインタを保存します。だから、特定の行に直接アクセスできます。でも、B-tree インデックスはサイズが大きいです。この章の例では、パーティションごとに数 MB のサイズになっています。そこで登場するのが BRIN（Block Range Index）です。CREATE INDEX heart_rate_brin_idxON watch.heart_rate_measurementsUSING brin (recorded_at);BRIN インデックスは、ページ範囲ごとの最小値と最大値だけを保存します。だから、サイズが非常に小さいです。この章の例では、24 KB しかありません。B-tree の 100 分の 1 です。でも、BRIN はページ全体をスキャンする必要がある場合があります。だから、少量のデータを取得するクエリでは B-tree の方が速いです。著者の説明を読んで、BRIN の仕組みがよくわかりました。時系列データのように、カラム値が物理的な配置と強く相関している場合に BRIN は有効です。心拍数測定データは常に追記されます。新しい測定は常に大きな recorded_at 値を持ちます。だから、ページ内のデータは時系列順に並びます。BRIN はこの特性を活かします。過去に、ログテーブルにインデックスを作ったことがあります。そのテーブルは append-only で、タイムスタンプカラムがありました。B-tree インデックスを作りましたが、サイズが大きくなって困りました。「なんでインデックスがテーブルより大きいんだ？」と首を傾げながら、ディスク容量を確保するために古いインデックスを削除する日々でした。当時は BRIN を検討していませんでした。Postgres のドキュメントで存在は知っていたはずですが、実際に使う場面を意識していませんでした。必要に迫られないと、知識は実践に結びつかないものです。10. Postgres for geospatial data「地理空間データ」の意外な身近さこの章を読んで認識したのは、地理空間データベースの機能が、自分の仕事に意外と近いということです。PostGIS の名前は知っていました。でも、「地理空間データベース」という言葉から受ける印象は、「GIS 専門家のための特殊な技術」でした。Google Maps みたいなサービスを作る時に使うやつ、くらいの認識。要するに、「自分には関係ない」と決めつけていたわけです。実際には、もっと身近なユースケースがあります。著者が冒頭で説明する Geofabrik（OpenStreetMap のデータ抽出サービス）、osm2pgsql（OSM データのインポートツール）、QGIS（データ可視化ツール）。これらのツールと PostGIS の組み合わせで、10 分以内にフロリダ州全体の地理データをローカル環境で扱える状態にできます。この手軽さが、「Just use Postgres」の真髄だと感じました。geometry と geography — 2つのデータ型の意味10.1.2 節で説明される geometry と geography の違いに、初めて向き合いました。geometry 型（Web Mercator projection、SRID 3857）。- 平面（Euclidean plane）として計算- 単位はメートル- 計算が速い- 距離が長いと精度が落ちるgeography 型（WGS 84、SRID 4326）。- 球面（spherical model）として計算- 単位は度（longitude/latitude）だが、計算結果はメートル- 計算が遅い- 地球の曲率を考慮するため正確「なるほど、速度と精度のトレードオフか」と思いました。でも、本当に理解したのは、用途によって使い分ける必要があるということでした。ローカルな範囲（例：Tampa 市内のレストラン検索）なら geometry で十分です。でも、大陸をまたぐような距離の計算なら geography が必要になります。注意点として、ST_Distance に geometry 型を渡すと単位は「度」になります。geography 型を渡すと「メートル」です。最初、この違いを知らずに「距離が 0.003 って何？」と混乱しました。それ、度でした。著者は本章で主に geometry を使っています。理由は明示されていませんが、フロリダ州内のデータを扱っているからでしょう。ST_DWithin と ST_Distance — index の有無で 500 倍の差10.6.2 節の実行計画の比較に目を奪われました。ST_DWithin を使った場合（Listing 10.26）:- 実行時間: 1.125 ms- GiST index を使用（Bitmap Index Scan）- 1,205 件を候補として抽出し、36 件にフィルタリングST_Distance を使った場合（Listing 10.27）:- 実行時間: 488.119 ms- GiST index を使用せず、フルテーブルスキャン（Parallel Seq Scan）- 18,676 件を候補として抽出し、36 件にフィルタリング同じ結果（36 件のレストラン）を得るのに、434 倍の時間がかかっています。なぜこんなに違うのでしょうか。片や 1 ミリ秒でサクッと答え、片や半秒近く考え込んでいます。まるで、道を聞かれて地図アプリを開く人と、記憶を辿って一生懸命思い出そうとする人くらい違います。ST_DWithin is one of the index-aware functions that can use the GiST index by performing an initial fast filtering of the data using the combination of the bounding box operator \u0026\u0026 and the ST_Expand function.著者の説明によると、ST_DWithin は内部で bounding box（境界ボックス）を使った高速フィルタリングをします。GiST index がこの bounding box 検索に対応しています。一方、ST_Distance は常に正確な距離を計算します。bounding box を使わないから、index を利用できません。この違いを知らなかったら、「ST_Distance(point1, point2) \u003c= 500 で 500m 以内を検索」と書いてしまっていたでしょう。数百万件のデータに対してフルスキャンが走ります。「index-aware functions」という概念を、初めて意識しました。GiST の構造 — R-tree で理解できた10.6.1 節の GiST index の説明は、初めて「わかった」感覚がありました。以前、B-tree index については理解していました。でも、GiST（Generalized Search Tree）は「汎用的な index」という説明しか見たことがなく、具体的なイメージが湧きませんでした。著者の図解（Figure 10.6, 10.7, 10.8）がわかりやすかったです。フロリダ州全体を 5 つの大きな矩形（R1〜R5）に分割それぞれの矩形をさらに小さな矩形に分割（R6〜R25）最小の矩形が、実際のテーブル行（points）を指す検索の流れ。1. Downtown Miami の座標が、どの大きな矩形に含まれるかをチェック → R52. R5 の中で、どの小さな矩形に含まれるかをチェック → R243. R24 の中の全 points をスキャン → 該当するものだけ返すR-tree（Rectangular tree）という名前の由来も理解できました。矩形（Rectangle）で空間を階層的に分割していく木構造です。実際に座標変換も試してみました。Walt Disney World の座標を WGS 84 から Web Mercator へ変換すると、経度 -81.5639 が X -9079651.82 に変わります。緯度 28.3852 は Y 3297626.07 になります。単位がメートルに変わるのがわかります。この構造、実は Chapter 6 の全文検索で出てきた GiST index と同じ基盤です。あの時は tsvector 型の lexemes を indexing していました。今回は geometry 型の bounding boxes を indexing しています。GiST は、データ型ごとに異なる index 構造を実装できる汎用フレームワークなんだと、やっと腹落ちしました。QGIS で可視化 — 「見える」ことの重要性10.4 節の QGIS による可視化は、実際に手を動かしました。SELECT name, ST_AsText(way) AS coordinatesFROM florida.planet_osm_pointWHERE name = 'Tampa' and place = 'city';このクエリで得た Tampa の座標を、QGIS で表示した時、「あ、本当に Tampa の中心だ」と思いました。データベースに入っている座標が、実際の地図上の位置と一致します。当たり前のことですが、自分の目で確認するまで信じられませんでした。planet_osm_polygon テーブルの 6.8 100 万の polygons を QGIS で読み込むと、フロリダ州の地図が少しずつレンダリングされていきます。湖、道路、建物、公園。すべてが Postgres のテーブルに格納されています。「データが見える」ことの重要性を、改めて実感しました。osm2pgsql — データインポートの簡単さ10.3 節で紹介されている osm2pgsql ツールは実用的です。docker run --name osm2pgsql --network=\"host\" \\  -e PGPASSWORD=password \\  -v osm2pgsql-volume:/data \\  iboates/osm2pgsql:2.1.1 \\  -H 127.0.0.1 -P 5432 -d postgres -U postgres --schema florida \\  http://d3e4uq6jj8ld3m.cloudfront.net/florida-250501.osm.pbfこのコマンド 1 つで、フロリダ州全体の OSM データ（2025 年 5 月 1 日時点）を Postgres にインポートできます。所要時間は約 10 分。自分の環境（M1 Mac）では 7 分ほどでした。インポート後、以下のテーブルが自動生成されます。planet_osm_point — 単一座標で表現できるもの（レストラン、ホテルなど）planet_osm_line — 線分（道路、川など）planet_osm_polygon — 閉じた領域（建物、公園、湖など）planet_osm_roads — planet_osm_line のサブセット（ズームレベルが低い時のレンダリング用）それぞれのテーブルに、既に GiST index が作成されています（planet_osm_point_way_idx など）。この「すぐに使える」感覚が、PostGIS の魅力だと感じました。ST_Within と ST_Intersects — 空間関係の判定10.5.2 節と 10.5.3 節で紹介される ST_Within と ST_Intersects の違いが、最初は曖昧でした。ST_Within(A, B)。- A が B の中で完全に含まれている場合は true- A の全ての点が、B の内部にある- 例：あるアトラクションが、Disney's Hollywood Studios の中にあるかST_Intersects(A, B)。- A と B が少なくとも 1 点を共有する場合は true- 完全に含まれていなくてもいい、交差していれば OK- 例：ある道路が、Miami の境界を横切っているかListing 10.22 のクエリで理解できました。SELECT l.name, l.highway, ST_Length(l.way) AS len_metersFROM florida.planet_osm_line lJOIN miami m ON ST_Intersects(l.way, m.boundaries)WHERE l.highway IN ('primary', 'secondary')このクエリは、Miami の境界内にある道路だけでなく、境界を横切る道路も取得します。ST_Within を使っていたら、境界を横切る道路は取得できません。この違いを知らないと、「なぜこの道路が結果に含まれるのか」と混乱したでしょう。「Just use Postgres」の再確認この章を読んで、改めて「Just use Postgres」の意味を理解しました。次に「位置情報を扱うから、MongoDB（GeoJSON 対応）を追加しよう」と言われた時、私は聞き返せます。「Postgres で試した？PostGIS なら、既存のインフラでできるかもしれない」新しいデータベースを追加する前に、まず既存の Postgres で何ができるかを確認します。これがこの本の一貫したメッセージです。そして、大抵の場合、Postgres でできてしまいます。追加のインフラを管理する手間（と、深夜の障害対応）が減るのは、エンジニアとしても組織やチームとしてもありがたいです。地理データだって、Postgres でできます。それも、思ったより簡単に。11. Postgres as a message queueメッセージキューとして Postgres を使う、という選択この章で参考になったのは、「Postgres をメッセージキューとして使う判断基準」が明確に示されていた点です。正直に言うと、読む前は「Postgres でメッセージキュー？　無理がある」と思っていました。10 年近くソフトウェアエンジニアをやっている中で、メッセージキューといえば RabbitMQ、Kafka、AWS SQS が標準でした。Postgres はあくまでリレーショナルデータベース。「餅は餅屋」という言葉が頭に浮かびました。というか、新しいツールを導入する言い訳が欲しかっただけかもしれません（インフラエンジニアの悪い癖です）。でも、この章を読み終えて気づきました。「Just use Postgres」の本質は、万能性じゃなくて、既存資産の最大活用でした。Postgres をメッセージキューとして使う 3 つの基準11.1 節で、著者は 3 つの基準を挙げています。1. トランザクショナルな一貫性が必要な場合DMV（運転免許センター）の例が分かりやすかったです。来訪者がチェックインする（ビジネスロジック）と同時に、待機キューにメッセージを追加する（イベント記録）。この 2 つの操作がアトミックに実行される必要があります。もし別々のシステム（Postgres + 専用メッセージキュー）だったら、チェックインは成功したのにメッセージ送信が失敗する可能性があります。その時、アプリケーション側で整合性を保証しなければなりません。If you want the check-in operation and the message added to the visitors queue to be executed atomically (as a single transaction), then use Postgres.この一文は重いです。私が関わったプロジェクトで、「決済処理」と「メール送信キュー」が別々のシステムだったせいで、決済完了したのに確認メールが届かないトラブルがありました。結局、リトライ機構を複雑に実装して解決しましたが、あれは Postgres で統一できていれば避けられたかもしれません。深夜 3 時に「メールキューが詰まった」アラートで起こされることもなかったでしょう（遠い目）。2. メッセージ量が Postgres で処理可能な場合著者は正直です。If the effort is too high or the configuration becomes overly complex, consider using a specialized message queue instead.Postgres の書き込みスケールには限界があります。シングルプライマリインスタンスだから、書き込み負荷が高すぎる場合はシャーディングや分散 Postgres（CitusData、YugabyteDB）が必要になります。でも、DMV の例では「メッセージ量は比較的低い」と明言しています。この「正直さ」がいいです。Postgres は万能じゃない、でも適切なユースケースならシンプルで強力です。3. 既に Postgres を使っている場合If your application already uses Postgres and now needs to support a message queue use case, consider using Postgres first before bringing in a specialized solution.これが「Just use Postgres」の核心です。新しいシステムを追加するコストは、技術的負債だけじゃありません。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を運用しているなら、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。カスタムメッセージキューの実装11.2 節と 11.3 節では、カスタムメッセージキューを実装しています。シンプルな設計CREATE TABLE mq.queue (    id BIGSERIAL PRIMARY KEY,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new');この設計、シンプルだけど実用的です。id: 自動採番（BIGSERIAL）で一意性を保証message: JSON 型でペイロードを格納（柔軟性重視）created_at: FIFO 順序の保証status: メッセージのライフサイクル管理（new → processing → completed）著者が JSON 型を選んだ理由が面白いです。The JSONB type would preprocess messages before storing them, which might slow down ingestion and alter the original structure—for example, by reordering object keys.JSONB はクエリ効率のために前処理を行いますが、メッセージキューでは「プロデューサーからコンシューマーへそのまま渡す」だけだから、JSON 型で十分です。この「ユースケースに応じた選択」が、エンジニアリングの本質だと感じました。FOR UPDATE SKIP LOCKED の威力mq.dequeue 関数の実装で、FOR UPDATE SKIP LOCKED が使われています。SELECT id FROM mq.queueWHERE status = 'new' ORDER BY created_atFOR UPDATE SKIP LOCKEDLIMIT messages_cntこの構文は、改めて確認すると有用です。FOR UPDATE は行レベルロックをかけます。通常なら、他のトランザクションがロックされた行にアクセスしようとするとブロックされて待機します。でも SKIP LOCKED を加えると、ロックされている行をスキップして、次の利用可能な行を取得します。複数のコンシューマーが並行してメッセージを取得しても、お互いをブロックせずに並列処理できます。This allows consumers to process new messages in parallel without blocking each other, improving overall throughput.これは Postgres のメッセージキュー実装におけるキラー機能です。実際に 2 つのワーカーを同時に動かして確認しました。Worker 1 がメッセージ 1, 2 を取得している間、Worker 2 はブロックされずにメッセージ 3, 4 を取得できます。お互いが異なるメッセージを処理する。これが SKIP LOCKED の威力です。以前、複数ワーカーでジョブキューを処理する実装を Rust で書いた時、排他制御で悩んだことがあります。あの時、FOR UPDATE SKIP LOCKED を知っていれば、もっとシンプルに実装できたかもしれません。LISTEN と NOTIFY11.4 節の LISTEN / NOTIFY は、Postgres の隠れた名機能だと感じました。DMV のシナリオでは、来訪者がチェックインすると、待機中の職員にリアルタイムで通知が届きます。-- 職員側（リスナー）LISTEN queue_new_message;-- ターミナル側（ノティファイア）SELECT mq.enqueue('{\"service\": \"car_registration\", \"visitor\": \"Marta Jones\"}');-- → pg_notify('queue_new_message', 'new_message')これで、ポーリング不要の非同期通知が実現できます。ただし、2 つの制限があります。過去の通知は受け取れない: 接続後に発行された通知のみ受信可能レプリカでは使えない: プライマリノードへの接続が必要特に 2 つ目は運用上重要です。読み取り負荷をレプリカに逃がしている構成でも、LISTEN/NOTIFY 専用にプライマリへの接続を維持する必要があります。でも、この制限を理解した上で使えば、非常に強力な機能です。あと、pg_notify はトランザクション終了時に送信されます。途中でロールバックすると通知も送られません。これは整合性の観点から正しい動作ですが、最初は「なぜ通知が来ない？」と悩みました。実装上の考慮事項11.5 節では、いくつかの重要な考慮事項が述べられています。インデックス戦略mq.dequeue 関数は、デフォルトではフルテーブルスキャンを行います。created_at と status にインデックスがないからです。著者は 2 つのオプションを提示しています。オプション 1: created_at のみのインデックス。CREATE INDEX mq_created_at_index_btree ON mq.queue (created_at);オプション 2: パーシャルインデックス（推奨）CREATE INDEX mq_partial_index_btreeON mq.queue (created_at, status)WHERE status = 'new';パーシャルインデックスは、status = 'new' の行だけをインデックスに含めます。これで、インデックスサイズが小さくなり、new メッセージへのアクセスがさらに高速化されます。この「状況に応じた最適化」の姿勢が参考になります。DMV のユースケースでは不要かもしれませんが、高頻度メッセージングなら必須です。パーティショニング11.5.3 節のパーティショニングの話は、時系列データの章（第 9 章）とつながりました。メッセージキューも時系列データの一種です。created_at でレンジパーティショニングすれば、古いメッセージを効率的にアーカイブ・削除できます。CREATE TABLE mq.queue (    id BIGSERIAL,    message JSON NOT NULL,    created_at TIMESTAMPTZ DEFAULT NOW(),    status mq.status NOT NULL DEFAULT 'new',    PRIMARY KEY (id, created_at)) PARTITION BY RANGE (created_at);パーティションごとにメッセージを管理できるから、古いパーティションを削除（DROP TABLE）するだけで大量の古いメッセージを一瞬で消せます。VACUUM の負荷も軽減されます。なぜなら、新しいパーティションだけが頻繁に更新されるからです。この設計パターンは、ログ管理やイベントストアにも応用できそうです。フェイルオーバー機構11.5.4 節で、メッセージ処理の失敗対策が述べられています。コンシューマーがメッセージを取得（status = 'processing'）した後にクラッシュすると、そのメッセージは processing 状態のまま放置されます。著者の提案は、pg_cron を使った定期的なリセットです。a periodic job in the database to check for messages stuck in the processing state and reset their status to new.これは実用的です。ただし、同じメッセージが複数回処理される可能性があるから、コンシューマー側で冪等性を保証する必要があります。pgmq 拡張11.6 節と 11.7 節では、pgmq 拡張が紹介されています。pgmq は「Postgres Message Queue」の略で、AWS SQS 互換の API を提供します。カスタム実装で学んだ原理を、pgmq が抽象化してくれます。可視性タイムアウトpgmq.read 関数の vt（visibility timeout）が面白いです。SELECT msg_id, message, enqueued_atFROM pgmq.read(  queue_name =\u003e 'visitors_queue',  vt         =\u003e 120,  -- 2分間の可視性タイムアウト  qty        =\u003e 1);メッセージを取得してから 120 秒間、そのメッセージは他のコンシューマーから見えなくなります。でも、120 秒以内に pgmq.archive を呼ばないと、メッセージは再びキューに戻ります。これで、コンシューマー失敗時の自動リトライが実現できます。DMV の例では、職員が来訪者を呼び出してから 2 分以内に現れなければ、別の来訪者を呼び出せる仕組みに使われています。この「タイムアウトベースのフェイルオーバー」は、AWS SQS と同じ設計パターンです。アーカイブテーブルpgmq.archive 関数は、メッセージを pgmq.q_visitors_queue から pgmq.a_visitors_queue に移動します。削除（DELETE）ではなくアーカイブ（移動）だから、処理済みメッセージの監査ログを保持できます。これは本番運用で重要です。「このメッセージ、本当に処理されたのか？」を後から確認できます。本全体を読み終えて第 11 章は、この本の最終章です。第 1 章「Meeting Postgres」から始まり、JSON、地理空間、全文検索、時系列、ベクトル検索、グラフ、そしてメッセージキュー。「Just use Postgres」の本質は、Postgres の万能性を主張することじゃありませんでした。既に Postgres を使っているチームが、新しいユースケースに直面した時、別のデータベースを追加する前に、まず Postgres で解決できるか試してみよう、というメッセージです。それは、アーキテクチャをシンプルに保つための選択であり、運用コストを抑えるための選択であり、チームの認知負荷を減らすための選択です。10 年近くソフトウェアエンジニアをやってきて、システムが複雑化する様子を何度も見てきました。「全文検索だから Elasticsearch」「時系列データだから InfluxDB」「メッセージキューだから RabbitMQ」確かに、それぞれの専用ソリューションは強力です。でも、それぞれが運用コストを生みます。バックアップ、モニタリング、アラート、障害対応、バージョンアップ。全てがチームの負担になります。そして、構成図に新しいアイコンが増えるたびに、誰かが「これ誰がメンテするんですか？」と聞く声が聞こえます。「Just use Postgres」は、その複雑化への抵抗です。もちろん、これは「新しい技術を学ぶな」という意味ではありません。新しいツールやサービスが出てきたとき、まず「運用時にどうなるか」を考える。それがベテランエンジニアに求められる姿勢だと思います。機能の魅力だけでなく、3 年後にメンテナンスできる人がいるか、障害時に対応できるか、既存システムとの整合性はどうか。これは Postgres の新機能についても同じです。pgvector は便利ですが、まだ運用実績が浅い。TimescaleDB も Postgres の拡張とはいえ、独自のアップグレードパスがあります。「Postgres だから安心」ではなく、その機能の成熟度を見極める必要があります。結局のところ、謙虚に学び続けるしかありません。新しい技術も、既存の技術も。私が最近考えている技術選定の基準があります。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、40 年の実績がある標準的な選択肢を使い、その可能性を最大限に活かす。それが、この本から学んだことです。もちろん、Postgres で解決できないユースケースもあります。著者は正直にそれを認めています。でも、試す前から諦めるのではなく、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。この本を読み終えて、次に「〇〇が必要だから△△を導入しましょう」と言われた時、私は自信を持って聞き返せるようになりました。「Postgres で試しましたか？」おわりに読むことと、手を動かすこと11 章を読み終えて、私は 1 つの疑問を持っていました。「本当に、Postgres でこれだけのことができるのか？」本に書いてあることを読んで「なるほど」と思うのと、実際に動かして確認するのは、全く別の体験です。少なくとも、私にとっては。だから、手を動かすことにしました。Docker で Postgres を立てて、Rust でコードを書いて、各章の内容を 1 つずつ検証しました。generate_series から始まって、CTE、Window Functions、Recursive Query と進みました。JSONB、全文検索、pgcrypto、pgvector、TimescaleDB、PostGIS、そしてメッセージキュー。全 11 章です。その過程で、いくつかのことに気づきました。手を動かして初めてわかったこと本を読んでいるときは「ふーん」と思っていたことが、実際に動かすと「あ、そういうことか」に変わる瞬間があります。例えば、FOR UPDATE SKIP LOCKED。本には「複数のコンシューマーが並行してメッセージを取得できる」と書いてありました。でも、実際に 2 つのワーカーを同時に動かして、それぞれが異なるメッセージを取得するのを見たとき、初めて腑に落ちました。Worker 1 がメッセージ 1 を取得: {\"service\":\"registration\",\"visitor\":\"Alice\"}Worker 2 がメッセージ 3 を取得: {\"service\":\"registration\",\"visitor\":\"Charlie\"}この出力を見て、「ああ、本当にブロックせずにスキップしてるんだ」と思いました。言葉で理解することと、目で見て理解することは、違うものです。他にも気づきはありました。pg_typeof() の結果を Rust で取得しようとしたらエラーになって、::TEXT でキャストする必要があることを知りました。PL/pgSQL の変数名がテーブルのカラム名と衝突してエラーになることも知りました。TEMP TABLE の名前が別のデモと衝突して、「なんでエラーになるんだ？」と 30 分悩んだこともあります。これらは本には書いてありません。当たり前です。本は概念を説明するものであって、私が遭遇するエラーを予測するものではないから。でも、そういうエラーと向き合う時間こそが、理解を深める時間だったと思います。判断基準が見えてきた11 章を読み終えて、そして検証を終えて、私の中に 1 つの判断基準ができました。「いつ Postgres で十分で、いつ専用ツールを検討すべきか」全文検索なら、数百万件以下のシンプルな検索であれば Postgres で十分です。ただし数億件規模や日本語の形態素解析、複雑なファセット検索が必要なら、Elasticsearch を検討すべきです。ベクトル検索なら、pgvector で数百万ベクトルまでは対応できます。でも、数億ベクトル規模やリアルタイム更新が必要なら、Pinecone や Milvus の出番です。メッセージキューなら、秒間数百メッセージ程度なら Postgres で十分です。でも、秒間数万メッセージや複雑なルーティングが必要なら、RabbitMQ や Kafka を使うべきです。この判断基準は、本を読んだだけでは身につかなかったと思います。実際に動かして、限界を感じて、初めてわかることがありました。「Postgres で試した？」この本を読み始める前、私はこの言葉を言えませんでした。「全文検索が必要です」と言われたら、「Elasticsearch ですね」と即答していました。「時系列データを扱いたい」と言われたら、「InfluxDB か TimescaleDB ですね」と答えていました。TimescaleDB が Postgres の拡張であることすら、あまり意識していませんでした。今は違います。「全文検索が必要です」と言われたら、「どのくらいのデータ量ですか？　検索の要件は？　まず Postgres の tsvector で試してみませんか？」と聞き返せます。「ベクトル検索がしたい」と言われたら、「pgvector で試してみましょうか。数百万ベクトルくらいなら対応できますよ」と提案できます。それが良いことなのかどうか、正直わかりません。もしかしたら、早めに専用ツールを導入した方が、長期的には幸せだったかもしれません。Postgres で頑張った結果、パフォーマンスの壁にぶつかって、結局移行することになるかもしれません。でも、少なくとも「試した上で判断する」ことはできるようになりました。「Postgres で試した？」その一言を、自信を持って言えるようになりました。そして、自分自身にも問いかけるようになりました。新しいデータベースを追加する前に、まず Postgres で試してみる。それで十分なら、アーキテクチャはシンプルなままです。運用負荷も増えません。深夜 3 時のアラート対応の可能性も、1 つ減ります。それだけで、この本を読んだ価値はあったと思います。最後に11 章分の感想を書いて、検証コードを書いて、そしてこの「おわりに」を書いています。読み始めたときは、「Postgres の可能性を広げる本」だと思っていました。読み終えた今は、「技術選定の視点を変える本」だったと思っています。「最適なツールを選ぶ」という言葉は、聞こえが良いです。でも、その「最適」は何を基準にしているのでしょうか。機能の豊富さ？　パフォーマンス？　それとも、運用の複雑さ？この本は、「十数年単位の運用の複雑さ」という視点を私に与えてくれました。新しいデータベースを追加することは、コストです。学習コスト、運用コスト、監視・バックアップ・障害対応の複雑化。全てがチームの負担になります。既に Postgres を使っているなら、まず Postgres で試してみる。それで十分なら、そのコストを払わなくて済みます。それが「Just use Postgres」の本当の意味だと、今は思っています。「できる」と「やるべき」の違いこの本を読んで、1 つ注意しなければならないことがあります。「Postgres でできる」と「Postgres でやるべき」は、違います。本書は Postgres の可能性を示してくれますが、すべてのユースケースで Postgres を選ぶべきだとは言っていません。著者自身も、専用ツールが必要な場面があることを認めています。大事なのは、選択肢を知った上で判断することです。「Postgres でもできるけど、このユースケースでは Kafka の方が適している」と判断するのと、「Postgres でできることを知らずに Kafka を選ぶ」のでは、意味が違います。前者は informed decision、後者は思い込みです。この本は、その informed decision をするための知識を与えてくれました。チームと知識の継承もう 1 つ、この本を読んで考えたことがあります。技術選定は、個人の問題ではありません。チームの問題です。新しいデータベースを導入するということは、チームメンバー全員がそれを学ぶ必要があるということです。障害対応できる人が増えなければ、特定の人に負荷が集中します。その人が退職したら、知識が失われます。Postgres を選ぶということは、チームの認知負荷を抑えるという選択でもあります。多くのエンジニアが Postgres の基本を知っています。採用市場でも、Postgres 経験者を見つけるのは比較的容易です。ドキュメントも豊富で、コミュニティも活発です。「技術的に最適」と「チームにとって最適」は、必ずしも一致しません。十数年単位で考えたとき、チームの持続可能性も重要な判断基準です。謙虚に学び続けることこの本を読んで、もう 1 つ気づいたことがあります。10 年近くこの仕事をしていても、知らないことはたくさんあります。Recursive CTE の活用パターン、BRIN インデックスの使い所、FOR UPDATE SKIP LOCKED の仕組み。どれも Postgres に昔からある機能ですが、実務で使う機会がなければ、深く理解することはありませんでした。新しい技術が出てきたとき、いきなり飛びつくのは危険です。でも、既存の技術の可能性を見落としているのも、同じくらい問題です。これは Postgres の新機能についても同じです。pgvector や TimescaleDB は便利ですが、Postgres 本体と比べれば運用実績は浅い。「Postgres を使う」という判断と、「Postgres の新機能を本番投入する」という判断は、別々に評価する必要があります。結局のところ、謙虚に学び続けるしかありません。はじめにでも書きましたが、私は技術選定についてこう考えています。替えの利く技術は、流行に従う替えの利きづらい基盤は、標準に従う競争優位の核は、自ら設計するPostgres は、競争優位の核になる場合もありますが、基本的には「替えの利きづらい基盤」であることが多いです。だからこそ、流行りの新しいデータベースに飛びつく前に、まず Postgres で何ができるかを確認する。それが、この本から学んだ姿勢です。もちろん、Postgres で全てが解決できるわけではありません。本当に専用ツールが必要な場面もあります。大事なのは、「試した上で判断する」ことです。最適解を求めて複雑さを増やすより、十分解でシンプルさを保つ方が、長期的には幸せなことが多いです。少なくとも、深夜 3 時のアラート対応は減ります。それは、間違いありません。参考書籍失敗から学ぶRDBの正しい歩き方 Software Design plus作者:曽根 壮大技術評論社AmazonSQLアンチパターン 第2版 ―データベースプログラミングで陥りがちな失敗とその対策作者:Bill Karwinオーム社Amazonセンスの良いSQLを書く技術　達人エンジニアが実践している３５の原則作者:ミックKADOKAWAAmazon","isoDate":"2025-11-25T04:52:20.000Z","dateMiliSeconds":1764046340000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、本を読め","link":"https://syu-m-5151.hatenablog.com/entry/2025/11/24/043314","contentSnippet":"はじめに私は本を読むのが好きです。朝、コーヒーを淹れて、ソファに座って、ページを開く。その時間が好きです。物語の中に入り込んで、登場人物の人生を追いかけ、著者の思考を辿り、知らない世界を覗き見る。ただ、それが楽しいんです。でも、誰かに「最近、何か読んだ？」と聞かれて、タイトルを答えると、必ず次の質問が来ます。「へえ、面白かった？ 何か学びはあった？」あるいは、こんな質問が来ます。「その本、どういうジャンル？ 自己啓発系？ノンフィクション？」違和感があります。映画を見たあと、「何か学びはあった？」なんて聞かれません。音楽を聴いたあと、「それ、自己啓発系？」なんて聞かれません。ゲームをクリアしたあと、「成長できた？」なんて聞かれません。でも、本だけは違います。読書には、常に「目的」が求められます。「成長のため」「知識を得るため」「キャリアアップのため」。ただ楽しいから読む、では許されない空気があります。SNSを開けば、「読書のすすめ」が溢れています。「本を読まない人は生き残れない」「年間百冊読めば人生が変わる」「ビジネスパーソン必読書」。どれも善意です。本当に、善意なんです。でも、その善意が、読書を窮屈にしています。私が小説を読んでいると言うと、「へえ、小説なんだ」と言われます。その「なんだ」という響きに、少しだけトゲがあります。まるで、「ビジネス書じゃないんだ」「役に立つ本じゃないんだ」と言われているような。あるいは、ミステリを読んでいると言うと、「息抜きにはいいよね」と言われます。その「息抜き」という言葉に、少しだけ違和感があります。まるで、本来読むべきは「ちゃんとした本」で、娯楽はその合間に挟むもの、と言われているような。おかしくないですか？ 映画は娯楽として認められています。音楽は娯楽として認められています。ゲームは娯楽として認められています（最近は、ですけど）。でも、読書だけは、娯楽であることを許されていません。「ただ楽しいから読む」では、ダメなんでしょうか。物語に没入して、現実を忘れる。登場人物に共感して、泣いたり笑ったりする。推理小説でハラハラして、犯人を当てようとする。SF小説で想像力を膨らませて、知らない世界に思いを馳せる。それだけじゃ、ダメなんでしょうか。この文章を書いている今も、矛盾しています。私は「読書について考えている私」を演出しているのだろう。この文章を投稿したら、何人かが「わかる」って言ってくれるだろう。その承認が欲しいのだろう。でも、それでも書きたいんです。なぜ読書だけが、娯楽であることを奪われるのか。なぜ読書だけが、「成長」や「学び」と結びつけられるのか。そして、その結びつきが、どれだけ読書を窮屈にしているのか。この文章は、その違和感から始まります。答えを出すつもりはありません。ただ、この違和感を言葉にしてみたいんです。もしかしたら、あなたも同じ違和感を抱えているだろう。「ただ楽しいから読む」という、当たり前のことが、当たり前じゃなくなっている世界。その世界を、少しだけ問い直してみませんか。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。では、はじめていきます。加速文化という病私たちは走り続けています私たちは、「加速文化」の中を生きています。現代社会では変化のスピードが絶え間なく加速し、個人も常に成長し続けることを要求されます。「走り続けること」そのものが目的化し、どこに向かっているのか、なぜ走っているのかという本質的な問いは置き去りにされます。「もっと成功しろ」「もっと幸せになれ」「スキルを身につけろ」「成長し続けろ」。現代社会は、こうした強烈なプレッシャーを発し続けます。「もっと」という言葉は、終わりのない要求を意味します。どれだけ達成しても、常に「もっと」が待っています。誰かと比べずにはいられませんSNSを開けば、誰かが何かを達成しています。誰かが本を出版しています。誰かが転職に成功しています。誰かが新しいスキルを身につけています。私たちは比較せずにいられません。そして、比較するたびに自分を劣っていると感じます。「自分は何もしていない」「自分は成長していない」「自分は遅れている」。SNSは、他者の「成功」を可視化し、価値を数値化します。しかし、SNSに現れるのは、他者の「ハイライト」だけです。私たちは自分の未編集の人生と、他者の編集済みの人生を比較してしまいます。より問題なのは、この比較が内面化されることです。自分の中に「比較する目」が住み着きます。常に自分を評価する目。「これは成長でしょうか」「これは生産的でしょうか」。この内なる審判者は、決して満足しません。その基準は加速文化から与えられ、常に「もっと」を要求するからです。だから、走らなければなりません。これは、まるでトレッドミルで走っているようなものです。動いているという実感だけがあって、前進しているという実感はありません。『鏡の国のアリス』の赤の女王が言ったように、「同じ場所にとどまるためには、全力で走り続けなければならない」のです。安定したいから成長したい、というおかしさここに、現代の最も奇妙な矛盾があります。「安定したいから成長したい」。「安定」と「成長」は、本来相反する概念です。安定とは、変化しないこと。成長とは、変化し続けること。なのに、私は「安定したいから成長したい」と言っています。なぜこの矛盾が成立するのでしょうか。「変化する環境の中で生き残るためには、変化し続けなければならない」という論理があるからです。つまり、「安定」は、もはや「変化しないこと」では達成できません。「変化し続けること」によってのみ達成できるのです。しかし、この論理は実は安定を永遠に延期しています。「いつか安定する」という約束のもとで、今は変化し続けます。でも、その「いつか」は決して来ません。終わりのない変化を、「安定」という言葉で正当化しています。不安が売られていますこのロジックは、巧妙なマッチポンプを生み出します。「成長しなければ生き残れない」という不安を煽り、成長のための商品やサービスを売ります。読書、セミナー、資格、転職支援、コーチング。このシステムの巧妙さは、被害者が加害者になることです。私は不安を抱え、本を買い、その経験を「成功体験」として語ります。この語りは、他者に同じ不安を伝染させます。そして、その最も基本的で、最も無害に見えて、最も広く受け入れられているのが「読書」なのです。読書は知的で文化的です。読書を批判することは、知性を否定することのように聞こえます。だから、「読書のすすめ」は抵抗なく受け入れられます。でも、それが実は加速文化の最前線にあるのです。やりたいことがわかりませんここには、もう1つの構造的な問題があります。私たちには、やりたいことがわかりません。「やりたいことを見つけろ」と言われ続けます。就活でも、転職でも、キャリア面談でも。自己分析をしろ。強みを見つけろ。情熱を持て。でも、そんなもの、簡単に見つかるわけがありません。むしろ、「やりたいことを見つけろ」というプレッシャーそのものが、私たちを追い詰めます。「やりたいことがない自分はダメだ」「情熱がない自分は劣っている」。「やりたいこと」は、発見するものではなく、構築するものです。様々な経験の中で、試行錯誤の中で、少しずつ形成されていくものです。にもかかわらず、現代社会は「今すぐ見つけろ」と命令します。ここに、加速文化の最も陰湿な側面があります。加速文化は、「やりたいことを見つけろ」と言いながら、実は「やりたいことを見つける時間」を奪っています。常に何かに追われています。常に次のタスクがあります。その結果、立ち止まって考える余裕がありません。「やりたいこと」を見つけるためには、時間が必要です。無為な時間、退屈な時間、何もしない時間。でも、加速文化は、その時間を「無駄」と見なします。「生産的」ではないから。その結果、私たちは「やりたいこと」を見つけられないまま、「やりたいことを見つけなければ」という焦燥だけを抱え続けます。潰しがきく選択肢という罠その結果、私は「とりあえず潰しがきく選択肢」に逃げ込みます。やりたいことはわかりません。でも、「汎用性の高いスキル」を身につけておけば、将来の選択肢が増えます。どこでも通用します。だから、とりあえずそれを目指そう。これは、一見合理的に見えます。でも、ここに罠があります。「汎用性の高いスキル」は、AIが最も得意とすることなのです。ロジカルシンキング。データ分析。プログラミング。外国語。これは確かに重要です。でも、これはすべて、AIに置き換えられつつあります。人間がAIに勝てるのは、「汎用性」ではありません。「固有性」です。その人だけが持つ価値観。その人だけが面白いと思うこと。その人だけが執着すること。それこそが、AIに代替されない価値です。でも、私は「やりたいことがわからない」まま、「汎用的なスキル」だけを積み上げています。その1つが「読書」です。何を読めばいいかわからないから、「必読書リスト」に従います。リストに従っていれば、「成長している」気分になれます。でも、それは本当に自分が読みたい本なのでしょうか。その結果、私は「やりたいこと」が空っぽなまま、知識だけを積み上げています。人格や欲望にもとづく価値基準が不在のまま、汎用的な情報を消費し続けています。そして、何も変わりません。やりたいことがわからないのに、知識だけ増えていきます。ここでも、本は読んだが問いは増えていません。実は読んでいませんここで、より深刻な問題に気づきます。私たちは、読書すらしていません。本を買っているだけ、リストを眺めているだけ、動画を見ているだけなのです。これらの行為は、「成長の記号」を消費しています。記号を消費しても、実体は得られません。記号としての「読書」を消費しても、読書の実体である「思考の格闘」は得られません。記号としての「知識」を消費しても、知識の実体である「理解」は得られません。記号としての「成長」を消費しても、成長の実体である「変容」は得られません。本を買います。Amazonでポチります。書店でレジに持っていきます。その瞬間、「私は成長しようとしている」という感覚が得られます。購入という行為が、「成長への意志」を示す儀式として機能しています。金を払います。その対価として、「私は成長しようとしている」という自己イメージを得ます。実際に本を読むよりも、はるかに安い買い物です。でも、そのあと自分の中に新しい疑問は生まれたでしょうか。必読書リストを眺めることもあります。「ビジネスパーソン必読書50選」「今年読むべき本ベスト10」。知っている本が何冊かあります。「ああ、これは読んだ」。そして、知らない本をメモします。「いつか読もう」。道筋が見えているだけで、目的地に近づいた気がします。実際には一歩も進んでいないのに。でも、そこに疑問はあるでしょうか。違和感は。より手軽なのが書籍まとめ動画です。10分の動画で得られるのは、本の「結論」だけです。しかし、本の価値は、結論だけにあるのではありません。むしろ、結論に至るまでの過程にこそ、価値があります。著者がどう考え、どう格闘したか。その過程を経験することで、読者の思考が鍛えられ、価値観が揺さぶられ、問いが生まれます。でも、動画は結論だけを与えます。そして、結論だけを知っても、自分は変わりません。疑問も、違和感も、何も残っていません。でも、記号の消費は、心地よいのです。なぜなら、実体を得るよりも、はるかに簡単だから。本を読むには、時間がかかります。理解するには、努力がかかります。変わるには、苦痛が伴います。でも、本を買うのは一瞬です。リストを眺めるのは数分です。動画を見るのは10分です。そして、それでも「成長した気」になれるなら、なぜ本を読む必要があるのでしょうか。こうして、私たちは読書すらしなくなります。アルゴリズムと必読書リスト仮に本を読むとしても、そこには2つの問題があります。1つ目は、アルゴリズムによる自己隷属です。「読書は自由だ」とよく言われます。でも、私は「自由に本を選んでいる」と思いながら、実際には既存の自分の枠内でしか選んでいません。「読みやすい本」「共感できる本」。自分を変えない本ばかりを選び、それを「自由」と呼んでいます。そもそも、「自分の好み」が変わっていかないなら、読書なんてなんのためなのでしょうか。読書の本質的な目的は、自分を変えることです。でも、私の「好きな本を読む」という自由は、実は「今の自分を肯定する本を読む」という自己隷属になっています。ネット書店のおすすめ。SNSのタイムライン。「あなたにおすすめの本」。これはすべて、「あなたの好みに合った本」を提示します。しかし、よく考えてみてください。アルゴリズムは、何を最適化しているのでしょうか。私の成長ではありません。私の満足度です。アルゴリズムの目的は、私に本を買わせること、私を長くサイトに留めることです。だから、アルゴリズムは、私が「気に入りそうな」本を推薦します。でも、私が「気に入る」本は、私を変えません。アルゴリズムは、私の周りに見えない壁を作ります。その壁の内側には、私にとって快適な情報だけがあります。そして、私はその快適さを「自由」と呼びます。でも、それはおそらく本当の自由ではありません。2つ目は、必読書リストという新たな隷属です。アルゴリズムに違和感を覚えた私は、「必読書リスト」に向かいます。「アルゴリズムに選ばされるのはイヤだ」「自分の好みだけで選ぶのは狭い」。だから、他者が選んだ、推奨された、「読むべき」とされる本のリストに従います。確かに、自分では選ばない本を読むことは重要です。でも、決定的な違いがあります。本来のリスト読書は、問いを獲得するための冒険です。でも、現代の「必読書リスト」は、答えを得るための効率化になっています。ここで、二種類のリストを区別してみたいと思います。第一のリストは、古典のリストです。プラトン、カント、ニーチェ、ドストエフスキー。これらの古典を読むことは、苦痛を伴います。理解できません。でも、その理解できなさの中で、自分の価値観が揺さぶられます。「正義とは何か」「自由とは何か」。根源的な問いに直面します。そして、その問いと格闘することで、自分が変わります。第二のリストは、必読書のリストです。「ビジネスパーソン必読書50選」。これらのリストは、「今求められている知識」を効率的に獲得することを目的とします。読みやすく、すぐに役立ち、何より安心できます。「このリストに従っていれば、遅れない」と。でも、それは幻想でしょう。なぜなら、リストを消化しても、問いを獲得していません。必読書リストは、私たちの問いを奪っているかもしれません。「何を読むべきか」「何が重要か」「何のために読むか」。これらを全て他者が決めます。結果として、自分で問いを立てる力が育ちません。自分の価値基準が形成されません。「やりたいこと」が空っぽなままです。でも、リストを消化することで達成感を得られます。だから、また次のリストを探します。アルゴリズムもリストも、「何を読むか」は教えてくれます。でも「なぜ読むのか」「読んだあと、どんな問いと一緒に生きていくのか」は教えてくれません。なぜ私たちはリストに従うのでしょうか。選択の責任からの逃避と、不安の一時的な解消のためです。リストがあれば、「何を読めばいいかわからない」という不安は解消されます。これは、不安の麻酔のようなものです。根本的な解決ではありませんが、痛みを一時的に和らげます。なぜ「もっと読まなきゃ」が終わらないのか読書体験が「数字」に変わるとき本を読むとき、何が起きているでしょうか。物語に没入します。考えが揺さぶられます。知らない世界を覗き見ます。その時間が、楽しい。それが、読書体験です。でも、いつの間にか、別のものを数え始めます。「今月、何冊読んだか」「必読書リストを、どこまで消化したか」「読書時間は、何時間か」。読書体験そのものではなく、読書したという事実が大事になっています。体験は、数字に変換されます。数字は、比較できます。競争できます。SNSに投稿できます。でも、体験そのものは、比較できません。見せられません。だから、数字のほうが「価値がある」ように見えてしまいます。こうして、読書から、読書体験が抜け落ちます。残るのは、数字だけです。満たされない構造ここに、厄介な問題があります。数字は、決して満たされません。50冊読みました。でも、100冊読んでいる人がいます。必読書を読みました。でも、原書で読んでいる人がいます。どれだけ達成しても、「もっと」が待っています。これは、あなたの問題ではありません。構造の問題です。数字による評価は、比較によって成り立っています。他者より多く。他者より速く。他者より難しく。差があるから、価値がある。でも、差は常に脅かされています。だから、新しい差を作らなければなりません。終わりがないのは、そういう仕組みだからです。満たされないのは、あなたが足りないからではありません。満たされないように、できているのです。「楽しむ」が難しい理由「だったら、数字なんか気にせず、楽しめばいい」。その通りです。でも、それが難しい。なぜか。評価される側として生きてきたからです。学校では成績。会社では業績。SNSではいいねの数。私たちは、常に評価されてきました。だから、何かをするとき、無意識に「これは評価されるだろうか」と考えます。本を読むときも、「これは意味があるだろうか」と考えます。評価の目が、内面化されています。自分の中に、審判者が住んでいます。だからこそ、意識的に選ぶ必要があります。数字を追いかけない。比較しない。評価されなくても、読み続ける。これは、単なる心がけではありません。評価の構造からの、意識的な離脱です。完全に離脱する必要はありません。評価を気にする気持ちは、消えません。それは自然なことです。でも、評価を唯一の基準にしないこと。これは可能です。読書が楽しければ、それでいい。年間10冊でも、それでいい。リストを無視しても、それでいい。評価されなくても、読み続けられる。その回路を持つこと。それが、終わりのないループから抜け出す方法です。永遠に満たされない不安のループ数字を追いかける構造と、評価の内面化。この2つが組み合わさると、恐ろしいループが生まれます。「生き残らなきゃ」という不安から始まり、「成長しなきゃ」という焦燥、「読書しなきゃ」という義務感へと続きます。必読書リストを探し、リストを見る、本を買う、動画を見ます。そして「成長した気分」を得ます。しかし問いを獲得していないので、自分は変わっていません。「まだ足りない」と感じます。より多くのリスト、より多くの本、より多くの動画を求めます。そして最初に戻ります。不安は解消されていません。これが、「読書のすすめ」が永遠にバズり続ける理由でしょう。このループは自己強化的です。ループを回るほど、「成長した気分」と「実際の成長」の乖離が大きくなります。私たちは本を買い、動画を見、リストを消化しています。でも、何も変わっていません。その乖離に薄々気づきながらも、認めたくありません。だから、もっと本を買います。そう信じて、ループを回し続けます。なぜ「読書のすすめ」がバズるのでしょうか。『本を読めば変われる』という物語は、不安を和らげるのではなく、不安を生産しています。この物語を読むたびに、「自分は十分に本を読んでいない」でしょう。そして、その不安が、また「読書のすすめ」を求めさせます。巧妙なマッチポンプです。このループから抜け出せないのは、問いが不在だからでしょう。「なぜ読むのか」「何のために読むのか」。この問いがないまま、ただリストを消化します。だから、終わりがありません。本は読みました。けれど、問いは増えていません。だからまた不安になり、次の「読書のすすめ」を探します。私にとって読書とは何かここまで、「成長のための読書」という物語を批判してきました。「本を読まなきゃ」というプレッシャー。「年間100冊」という数値目標。「必読書リスト」という他律的な選択。そして、読書体験を数字に変換し、評価を内面化する構造。これは確かに、読書を窮屈にしています。でも、だからといって、成長すること自体を否定したいわけではありません。私にとって、読書とは、問いを獲得するための冒険です。答えを得るために本を読むのではなく、問いを見つけるために読みます。既存の自分を確認するのではなく、自分を変えるために読みます。安心するために読むのではなく、不安になるために読みます。読書を通じて、自分が変わります。価値観が揺さぶられます。新しい視点を得ます。世界の見え方が変わります。それは、成長です。しかし、それは「成長しなければならない」という義務から生まれる成長ではありません。「年間100冊読めば人生が変わる」という約束に従う成長でもありません。「必読書リスト」を消化することで得られる成長でもありません。それは、読書そのものを楽しむ中で、結果として起こる成長です。物語に没入して、登場人物の選択に心を揺さぶられます。その結果、自分の価値観が変わります。哲学書を読んで、理解できない文章に格闘します。その結果、新しい問いが生まれます。小説を読んで、知らない世界を覗き見ます。その結果、自分の世界が広がります。これは全て、「成長しよう」と思って起こることではありません。ただ楽しんでいたら、結果として起こる変化です。そして、その変化が周りの環境に合っていたら、「成長」と呼ばれます。合わなかったら、ただの変化です。でも、どちらでもいいんです。変化そのものに価値があります。それが「成長」という名前で呼ばれるかどうかは、環境次第です。社会の基準次第です。時代次第です。読書を通じて、自分が変わります。その変化が、たまたま今の環境で「成長」と評価されるだろう。評価されないだろう。でも、それは二の次です。重要なのは、自分が変わったということ。新しい視点を得たということ。世界の見え方が変わったということ。それだけです。だから、こう言いたいのです。読書は、楽しんでいいんです。「何か学びはあったか」なんて気にしなくていいです。「問いは増えたか」なんて確認しなくていいです。「成長できたか」なんて測定しなくていいです。ただ、その時間が楽しければいいです。物語に没入して、現実を忘れる。それだけで十分です。登場人物に共感して、泣いたり笑ったりする。それだけで十分です。推理小説でハラハラして、犯人を当てようとする。それだけで十分です。そして、もし読み終わったあとに、何かが変わっていたら。新しい問いが生まれていたら。それは、ボーナスです。でも、それは目的ではありません。結果です。楽しむことが目的で、成長は結果です。この順序を、逆にしてはいけません。「成長するために読む」ではなく、「楽しんで読んでいたら、結果として成長していた」。これが、私にとっての読書です。読書そのものは、必ずしも人格を育てるわけではありません。むしろ劇薬と言えます。興味の赴くままただ読むのは、時に有害でさえあります。歴史を振り返れば、独裁者も大量虐殺者も、大読書家でした。彼らは膨大な本を読みました。それが彼らを善き人間にしたわけではありません。読書は道具です。道具は、使い方次第で、善にも悪にもなります。では、どう読めばいいのでしょうか。鍵になるのは自発性です。本とテレビ・YouTube・Podcastの決定的な違いは、本が「自発」を要求することです。本は、私が選ばなければ私の手の中にやってきません。本は、私が目を動かさなければ、語り始めてくれません。本は、私が理解しようとしなければ、ただの記号の羅列です。つまり、本を読むためには、能動的かつ自発的に読者が働きかけなければなりません。一方、テレビやYouTube、Podcastは、一方的に情報を流し込んできます。受動的に消費できます。画面を見ていれば、音声を聞いていれば、情報は入ってきます。思考は不要です。この違いこそが決定的です。自発性こそが、思考を生みます。受動的に与えられた情報は、思考を生みません。ただ受け取り、ただ流れるだけです。しかし自発的に獲得した情報は、思考を生みます。なぜなら、獲得するプロセスですでに思考しているからです。だからこそ、本を読むときは「どんな問いを持ってページを開くか」が決定的になります。読書によって得られるものは、考えること。疑問をもつこと。異議を申し立てることです。読書の真の効用は、ここにあります。世の中の常識とされていること、あたりまえと受け入れられている前提を、疑ってかかります。「本当にそうなのか」「なぜそうなのか」「他の可能性はないのか」。こういう問いを持つことが、読書の本質です。この問いを持つ人間は、システムにとって邪魔な存在です。システムが必要としているのは、考えない労働者、考えない消費者です。言われたことを黙って実行する人間。与えられた情報を疑わずに受け入れる人間。しかし読書する人間は、疑います。問います。異議を唱えます。だから、システムは読書を骨抜きにしようとします。「読書のすすめ」を発信します。「必読書リスト」を作ります。「要約動画」を提供します。そうすれば、私たちは本を読みます。けれども考えません。疑いません。問いません。ただ、与えられた情報を消費するだけです。これは読書ではありません。読書の形をした、情報消費です。ここで、現代の読書が抱える問題に気づきます。思考の型を学ぶことが、思考停止を生んでいます。「MECE」「ロジックツリー」「仮説思考」。これらは有用な道具です。しかし「型」を覚えることが目的になると、「型」に縛られ、「型」の外側を見なくなります。世界は、「型」に当てはまらないもので満ちています。むしろ、「型」に当てはまらないものこそが、面白く、新しく、価値があります。もう1つの問題は、作業をすることが、目的化してしまうことです。本を読む、ページをめくる、線を引く、メモを取ります。これらの「作業」をすることで、「自分は頑張っている」という実感を得ます。しかし、読書は本来「作業」ではありません。読書は、思考です。格闘です。問いとの対話です。ページ数をカウントし、読書時間を記録し、読了数を競います。読書を「作業」として扱った瞬間、読書は死にます。読書とアイデンティティの罠ここまでは、読書の「方法」について語ってきました。読書にはもう1つ、深刻な問題があります。読書が、アイデンティティの道具になる時です。「積読」という現象があります。買ったけど読んでいない本が積まれている状態。多くの読書家が、この積読に悩んでいます。「読まなきゃ」「もったいない」「時間がない」。しかし別の角度から見ることもできます。積読は、ファッションです。本棚は、なりたい自分の姿、未来の自分への約束です。読める読めないは別として、難しい本を買ってしまいます。哲学書を買います。古典を買います。専門書を買います。それらを本棚に並べます。本棚の「面構え」が変わります。そして、その本棚を見るたびに、「私はこういう人間でありたい」でしょう。これは、服を買うのと同じです。服を買う時、私たちは「今の自分」に合う服だけを買うわけではありません。「なりたい自分」をイメージして、その自分に相応しい服を買います。そして、その服を着ることで、少しずつ、その自分に近づいていきます。本も同じです。「こういう本を読む人間でありたい」「こういう思考ができる人間になりたい」。そのイメージが、本棚を作ります。そして、その本棚に引っ張られて、自分がそれに相応しい人間になろうとします。ここまでは問題ありません。むしろ、これは積極的に肯定すべきことです。積読は、未来の自分への投資です。今は読めなくても、いつか読めるようになります。今は理解できなくても、いつか理解できるようになります。そう信じて、本を買います。それは、自己形成の1つのプロセスです。問題は、このアイデンティティが、他者との差異化の道具になる時です。ここで、「文化資本」という考え方が参考になります。経済的な資本（お金や資産）とは別に、教養や知識、趣味といった文化的な要素も、社会的な価値を持ちます。高い教育を受けた人、芸術に詳しい人、本をたくさん読む人。こうした人々は、その知識や教養によって、社会的な地位や信頼を獲得します。つまり、文化もまた、資本のように蓄積され、交換され、価値を生み出すのです。読書も、この構造の中にあります。「私は本を読む」という行為は、「私は教養がある」というシグナルを発します。そして、そのシグナルは、「本を読まない人」との境界線を引きます。この境界線は、善意によって引かれます。「もっと本を読んでほしい」という言葉の裏には、「本を読まないあなたは、何かを失っている」という暗黙のメッセージがあります。そして、そのメッセージを受け取った側は、「本を読まなきゃダメなんだ」と感じるか、「所詮マウンティングだ」と反発します。どちらにせよ、分断が生まれます。ここで、恐ろしい矛盾に気づきます。読書によって自分のアイデンティティを保とうとすればするほど、そのアイデンティティは脆くなります。なぜなら、「読書する私」というアイデンティティは、「読書しない他者」の存在によって初めて成り立つからです。他者との差異によって、自分の価値が定義されます。だから、心のどこかで、私は「みんなが本を読む」ことを望んでいません。口では「もっと本を読んで」と言いながら、本音では、他者が本を読まないことを願っています。これは、恐ろしい自己矛盾です。実際、この矛盾は現実のものになりつつあります。「読書」という言葉が氾濫し、「読書している私」という特別さが希薄化していきます。だから、人々はより高い壁を作ろうとします。「全部読む」「原書で読む」「年間100冊読む」。新たな境界線を引きます。でも、それは本質的な解決にはなりません。どんな境界線を引いても、それは結局、他者との差異に依存しています。そして、他者との差異に依存している限り、アイデンティティは脆いのです。本棚で他者と差をつけようとすればするほど、私の本棚からは問いが減っていきます。残るのは「どう見られたいか」という問いだけです。「生き残る」という言葉の暴力性ここで、もう一度、根本的な問いに戻りましょう。「本を読まない人は生き残れない」。この言葉を目にするたびに、私は違和感を覚えます。「生き残る」という言葉は、暴力的です。「生き残る」という言葉を使うとき、私たちは何を前提としているのでしょうか。生き残る人がいます。そして、生き残れない人がいます。「生き残れなかった」人とは、誰のことを指すのでしょうか。過労死した人。病で倒れた人。若くして亡くなった才能ある人々。彼らは、「本を読まなかったから」生き残れなかったのでしょうか。違います。「生き残る/生き残れない」という二分法そのものが、暴力的です。この二分法は、人生を競争に還元しています。しかし人生は競争ではありません。人生は、複雑で、出鱈目で、混沌としていて、多面的なものです。そして、死は、敗北ではありません。同じように、「本を読め」という命令も、暴力的です。「本を読まないあなたは、遅れている」「生き残れない」。このメッセージは、受け手を追い詰めます。しかし、本を読むことは、1つの選択肢に過ぎません。価値ある選択肢ですが、唯一の選択肢ではありません。本を読まなくても、学べることはあります。成長できることはあります。だから、言葉を言い換える必要があります。「生き残る」ではなく、「価値を示し続ける」。「本を読め」ではなく、「本を読む」。この言い換えは、単なる言葉遊びではありません。根本的な視点の転換です。「生き残る」は、生と死の二分法です。ゼロサム・ゲームです。誰かが生き残るためには、誰かが生き残れません。でも、「価値を示す」は、程度の問題です。グラデーションです。みんなが価値を示せます。同じように、「本を読め」は、命令です。義務です。他律です。でも、「本を読む」は、選択です。欲求です。自律です。そして、この転換こそが、読書を解放する鍵です。重要なのは、生き残るために本を読むことではなく、「どう生きたいのか」という問いに少しずつ形を与えていくことです。ここで、改めて考えてみます。成長とは何でしょうか。加速文化の中では、成長は「より多く」「より速く」「より効率的に」として定義されます。より多くの本を読みます。より速く読みます。より効率的に知識を得ます。しかし、それは本当に成長なのでしょうか。成長とは、自分が変わることです。好みが変わります。価値観が変わります。問いが変わります。見える世界が変わります。そして、その変容こそが、「変化する環境の中で価値を示し続ける」ための基盤になります。なぜなら、自分が変われる人は、環境の変化に適応できるからです。自分が変われない人は、環境が変化したとき、取り残されます。「より多く」「より速く」「より効率的に」知識を得ることは、自分を変えません。むしろ、既存の自分を強化します。既存の自分を肥大化させます。そして、環境が変化したとき、その肥大化した自分が、足かせになります。もう1つ、考えてみます。価値とは何でしょうか。これは、一言でいえるような簡単なものではありません。しかし少なくとも、そのガイドラインになるものは、自分軸で持っておいたほうがいいでしょう。この「自分軸」こそが、読書によって獲得すべきものです。自分軸とは、問いです。「何が面白いのか」「何が重要なのか」「何のために働くのか」「何のために生きるのか」。これらの問いに対する自分なりの答え、あるいは答えを探し続ける姿勢。それこそが「自分軸」であり、「やりたいこと」であり、AIに代替されない価値の源泉です。しかし「必読書リスト」は、その問いを奪います。加速を拒否しますここまで、加速文化と読書の問題を語ってきました。では、どうすればいいのでしょうか。加速を拒否します。立ち止まります。これは、単なる怠惰ではありません。積極的な抵抗です。読書を取り戻すために、3つの根本的な問いと向き合う必要があります。これらの問いは、読書という行為の本質に関わるものです。答えを急ぐ必要はありません。問い続けることそのものが、読書を解放する鍵になります。第一の問い：誰のために読むのでしょうか「本を読まなきゃ」と思うとき、私たちは誰の声を聞いているのでしょうか。SNSのタイムラインに流れてくる「読書のすすめ」。「必読書リスト」。「新人が読むべき本」。これは全て、他者の期待です。他者が定めた基準です。でも、その本は、本当に自分が読みたい本なのでしょうか。現代の自己啓発は、「自分らしさを見つけろ」「本当の自分を知れ」と言います。でも、これは罠です。「自分らしさ」を追求することが、かえって自分を見失わせます。なぜなら、「自分らしさ」とは、他者との差異によって定義されるからです。「他の人とは違う、特別な私」。でも、その「特別さ」は、脆いのです。常に他者との比較によってしか成り立ちません。向き合うべきは、自分が関わる人々に対する義務です。家族に対する義務。友人に対する義務。社会に対する義務。そして、読書についても同じです。古典を読む義務。先人たちが残した思想と格闘する義務。この「義務」は、他者から課されるものではありません。自分が自分に課すものです。同時に、断る勇気も必要です。「必読書リスト」を無視していいのです。途中で「この本は自分に合わない」と思ったら、読むのをやめていいのです。誰のために読むのか。この問いに向き合うことは、他者の期待ではなく、自分が向き合いたい問いは何かという方向へ進むことです。読書を義務から解放し、選択として取り戻すことです。第二の問い：何を求めているのでしょうか「この本を読めば成長できる」「年間100冊読めば人生が変わる」「要約を見れば効率的に知識が得られる」。読書は、常に何かの「手段」として語られます。成長のため。キャリアアップのため。生き残るため。でも、本当にそれを求めているのでしょうか。ポジティブ思考が溢れています。「できる」「やればできる」「可能性は無限」。でも、これは現実を単純化します。人生は、複雑で出鱈目で混沌としていて多面的なものです。すべてをコントロールできるわけではありません。失敗もします。うまくいかないこともあります。理不尽なこともあります。読書も同じです。「この本を読めば成長できる」というポジティブな約束に騙されません。むしろ、ネガティブな可能性を受け入れます。「この本は理解できないだろう」「この本を読んでも何も変わらないだろう」「途中で飽きて読み終えられないだろう」。その上で、それでも読みます。不確実性を受け入れながら、それでも本を開きます。そして、感情とも距離を置きます。「読まなきゃ」という焦燥。これらの感情は、読書を苦痛にします。今日は読む気分じゃありません。それなら、読みません。それでいいのです。より、「もっと速く」という呪縛からも自由になります。ゆっくり読んでいいです。同じページを何度も読み返していいです。一冊の本に一年かけてもいいです。速さではなく、深さ。何を求めているのか。この問いに向き合うことは、成果主義・完璧主義から解放されることです。答えを求めるのではなく、問いを見つけます。この本からどんな問いを持ち帰りたいのか。読書を手段から目的へと転換することです。第三の問い：どう読むのでしょうか自己啓発書を読みます。ビジネス書を読みます。要約動画を見ます。こうしたものは、すべて単純化します。「こうすれば成功する」「これをやれば幸せになれる」「この思考法を使えば問題が解決する」。人生を、因果関係の単純な連鎖に還元します。でも、人生は、そんなに単純なものでしょうか。小説を読めば、もっと複雑な世界観が提示されます。登場人物たちは、矛盾しています。善人でも悪人でもありません。理性的でもなければ、ただ感情的なだけでもありません。予測不可能な行動をします。そして、物語には、明確な答えがありません。むしろ、問いが生まれます。「この登場人物の選択は正しかったのか」「自分だったらどうしただろう」「人間とは何なのか」。小説を読めば、破天荒なキャラクターたちの人生を追体験することで、人生をコントロールできないことが学べます。加速文化は、「人生をコントロールできる」という幻想を植え付けます。でも、これは幻想です。人生は、コントロールできません。予測できません。理不尽です。そして、その理不尽さを受け入れることこそが、真の成熟です。小説は、その成熟を促します。同時に、未来だけでなく、過去とも対話します。現代社会は、常に「未来志向」を要求します。「過去にこだわるな」「前を向け」。でも、過去にこだわります。過去に読んだ本を、もう一度読みます。若い頃に読んで理解できなかった本を、今読み直します。そこに、新しい発見があります。昔は好きだった本を、今読み返します。自分がどう変わったかがわかります。過去の自分が選んだ本を尊重します。「あの頃の自分は何を考えていたのか」。その問いが、自分を理解する手がかりになります。過去の自分が選んだ本を「恥ずかしい」と思いません。それもまた、自分の一部です。同じ本を読み返したとき、昔の自分と今の自分で、立ち上がる問いが変わっているか。それが、自分が変わったかどうかの指標になります。どう読むのか。この問いに向き合うことは、単純化から複雑性へ、未来志向から過去との対話へと視点を転換することです。自己啓発書ばかりではなく小説を。新しい本ばかりではなく過去に読んだ本も。それは、読書を知識の獲得から思考の深化へと変えることです。本を読んだあと、問いが増えていないなら、それは「読んだ」とは言えないでしょう。読書の多様性を認めますここで、1つの矛盾に気づくでしょう。「小説を読め」と言いながら、「正しさを押し付けるな」とも言っています。これは矛盾ではないのでしょうか。いや、違います。重要なのは、「正しさ」を一つに固定しないことです。全部読む人もいれば、要約で済ませる人もいます。じっくり読む人もいれば、流し読みする人もいます。ビジネス書を読む人もいれば、小説を読む人もいます。マンガを読む人もいれば、読まない人もいます。そして、どれも「正しい」のです。私が提案しているのは、「小説を読め」ではなく、「自己啓発書『ばかり』を読むな」です。ビジネス書ばかり。要約ばかり。リストばかり。そうやって、1つの形式に固定されることが危険です。だから、多様性を持ちます。複数の形式で読みます。複数の視点を持ちます。読書に「正しさ」を求める必要はありません。「こうあるべき」という規範を押し付ける必要もありません。それぞれの読み方を、それぞれの価値として認めます。本を読むことは、「深い洞察を得る」ためだけではありません。「面白い話をする」ためでもあります。社交のツールとしての読書。これも、1つの正しい読み方です。本の内容を、自分なりに加工して、他者に提供します。それは、相手を見下すためではなく、一緒に楽しむためです。読書から特権性を剥ぎ取ったとき、読書は軽やかになります。堅苦しさがなくなります。誰にでも開かれたものになります。読書の新しい意味読書から「特権性」を剥ぎ取り、「加速」を拒否したとき、何が残るでしょうか。それは、ただ楽しいから読む、という当たり前のことです。本を読みたいから、読みます。面白いから、読みます。その時間が好きだから、読みます。他者との差異を作るためでもなく、自分のアイデンティティを保つためでもなく、「成長しなきゃ」という焦燥からでもなく。そして、「問いを得るため」でもなく、「学びを得るため」でもなく、「効率的に知識を吸収するため」でもありません。ただ読みたいから読みます。これが、本来の読書の形です。「速読」も「効率的な読書術」も「アウトプット前提のインプット」も、全部いりません。ゆっくり読んでもいいです。飛ばし読みしてもいいです。同じページを何度も読み返してもいいです。途中で飽きたら、やめてもいいです。最後まで読まなくてもいいです。読み終わったあと、何もアウトプットしなくてもいいです。SNSに投稿しなくてもいいです。読書記録をつけなくてもいいです。ただ、その時間が楽しかったなら、それで十分です。積読の山を見て、焦る必要はありません。全部読もうとしなくていいのです。今読みたい一冊を、読みます。それだけでいいのです。「もっと読まなきゃ」「遅れている」「追いつかなきゃ」。そんな焦りは、読書を義務にします。楽しむべき読書が、苦痛になります。一冊ずつ読めばいいのです。今読みたい本を、今読みます。それで十分です。そして、読み終えたら、次の一冊。その繰り返しが、気づけば大きな蓄積になります。読書は、競争ではありません。誰かより多く読む必要はありません。誰かより速く読む必要もありません。自分のペースで、自分の読みたい本を、一冊ずつ読みます。それが、読書の本来の形です。読書は、頭の中の掃除です。頭の中を整理します。雑多な思考を整えます。新しい視点を取り入れます。古い固定観念を捨てます。でも、掃除と同じように、読書も「完璧」を求める必要はありません。毎日少しずつでいいのです。一日一ページでもいいのです。完璧に読まなくてもいいのです。流し読みでもいいのです。途中で飽きたら、別の本に移ってもいいのです。読書を、義務にしません。プレッシャーにしません。自分を追い込みません。ただ、自分を大切にする1つの手段として、読書があります。それだけでいいのです。本を読んだら、感想を書かなきゃ。書評を書かなきゃ。SNSに投稿しなきゃ。そんな義務感が、読書を窮屈にします。でも、言語化しなくてもいいのです。ただ読みます。心の中に留めます。それだけでいいのです。本を読んで、何も言葉になりません。でも、何かが変わった気がします。それで十分です。言語化できない読書の体験。それこそが、最も豊かな読書なのでしょう。おわりにこの文章を書き終えて、スマホを見ます。何も変わっていません。タイムラインには相変わらず「読書のすすめ」が流れています。「本を読まない人は生き残れない」というツイートがバズっています。誰かが「必読書リスト」を作っています。たぶん、これからも変わりません。「読書は成長のため」という物語は、これからも繰り返されます。「ビジネスパーソンは本を読め」というメッセージは、これからも発信されます。それは、悪意じゃありません。本当に、善意なんです。だから、厄介なんです。でも、私は諦めません。本を読むのは、楽しいからです。物語に没入するのが、楽しいからです。知らない世界を覗き見するのが、楽しいからです。それだけです。映画を見るのと同じです。音楽を聴くのと同じです。ゲームをプレイするのと同じです。ただ、楽しいから。それ以上でも、それ以下でもありません。私は、誰も説得しようとは思いません。ただ、もしあなたも「ただ楽しいから読む」では、ダメなのかな、と思っているなら。「成長」とか「学び」とか、そういう目的がないと、読書しちゃいけないのかな、と思っているなら。伝えたいんです。大丈夫です。ただ楽しいから読む、それでいいんです。物語に夢中になって、現実を忘れる。それでいいんです。何も学ばなくていいんです。何も成長しなくていいんです。ただ、楽しければいいんです。読書は、競争じゃありません。義務でもありません。成長の手段でもありません。ただ、楽しいから読む。それだけです。ただ、楽しんでください。そして、もし誰かに「何のために読むの？」と聞かれたら、こう答えてください。「楽しいから」。それだけで、十分です。本棚を見ます。また明日、読みます。何を読むかは、まだ決めていません。でも、楽しみです。どんな物語に出会えるか。どんな世界を覗けるか。それが、楽しみです。スマホを置きます。窓を開けます。外を見ます。明日も、本を読もう。ただ、楽しいから。それだけです。参考図書加速する社会 近代における時間構造の変容作者:ハルトムート ローザ福村出版Amazon地に足をつけて生きろ！ 加速文化の重圧に対抗する7つの方法作者:スヴェン・ブリンクマンEvolvingAmazon世界のエリートが学んでいる 教養書必読１００冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazon世界のエリートが学んでいるＭＢＡマーケティング必読書５０冊を１冊にまとめてみた作者:永井孝尚KADOKAWAAmazonさみしい夜のページをめくれ作者:古賀史健ポプラ社Amazon本を読む人はうまくいく作者:長倉 顕太すばる舎Amazon強いビジネスパーソンを目指して鬱になった僕の 弱さ考作者:井上 慎平ダイヤモンド社Amazon読んでいない本について堂々と語る方法 (ちくま学芸文庫)作者:ピエール・バイヤール,大浦康介筑摩書房Amazonビジネス書ベストセラーを１００冊読んで分かった成功の黄金律作者:堀元見徳間書店Amazon自己啓発の教科書　禁欲主義からアドラー、引き寄せの法則まで作者:アナ・カタリーナ・シャフナー日経ナショナル ジオグラフィックAmazon中年の本棚作者:荻原魚雷紀伊國屋書店Amazon","isoDate":"2025-11-23T19:33:14.000Z","dateMiliSeconds":1763926394000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Fish Shell の abbr で使う。キミが好きだよ、エイリアス","link":"https://syu-m-5151.hatenablog.com/entry/2025/11/22/123028","contentSnippet":"はじめにターミナルで作業をしていると、同じコマンドを何度も入力することがありますよね。git checkout -b feature/new-branch や kubectl get pods --all-namespaces のような長いコマンドを毎回タイプするのは面倒です。多くのシェルでは「エイリアス」を使ってこの問題を解決しますが、Fish Shell にはとても優れた機能があります。それが abbreviation（略して abbr） です。生成AIやエージェントがコマンドライン操作を支援するようになった今、履歴(history)の可読性はこれまで以上に重要です。この記事では、なぜエイリアスと別れたのか、そして abbr が現代のターミナルワークに必須のツールである理由を詳しく解説します。fishshell.comabbr とはabbr は、入力した短い文字列を長いコマンドに展開する機能です。たとえば gco と入力してスペースやエンターを押すと、自動的に git checkout に展開されます。最大の特徴は、展開がリアルタイムで可視化されることです。エイリアスと違い、実際に実行されるコマンドを目で確認してから実行できます。abbr の圧倒的なアドバンテージこれが最も重要なポイントです。2024年以降、共同作業や自動化ツールに加え、生成AIがターミナルワークを下支えするようになりました。そして、abbr はこの新しいワークスタイルに完璧にフィットします。履歴から作業内容が誰にでも伝わる例えば、昨日の作業をチームに共有するときです。エイリアスの場合:$ history | tail -20gco feature-branchgaagcm \"Add new feature\"gp origin feature-branch履歴を受け取った人には何が起こったか全く分かりません。gco や gaa が何を意味するのかも相手には伝わりません。個人のローカル設定は共有されていないからです。abbr の場合:$ history | tail -20git checkout feature-branchgit add --allgit commit -m \"Add new feature\"git push origin feature-branch誰が見ても即座に理解できます。ブランチを切り替え、全ての変更をステージングし、コミットしてプッシュしたのだとすぐ分かります。実際の活用例例1: デバッグ支援# あなたのコマンド履歴（abbr を使用）$ kubectl get pods --namespace production$ kubectl logs pod-abc123 --namespace production$ kubectl describe pod pod-abc123 --namespace production$ kubectl get events --namespace production --sort-by='.lastTimestamp'# 共有したい質問: \"このエラーの原因を知りたい\"履歴を見た人はコンテキスト全体を理解して、適切な解決策を提示できます。エイリアス（例：k、kgp、kl）だと、何が起きているか推測すらできません。例2: ワークフローの自動化# 毎日のデプロイ作業（abbr で記録された履歴）$ docker compose build$ docker compose down$ docker compose up -d$ docker compose logs --tail=100 web$ curl https://example.com/health# 「この手順をスクリプト化して」と頼むだけ誰でも履歴を見て、ほぼそのまま自動化スクリプトを組み立てられます。例3: チームメンバーへの説明# Slack や Issue に貼り付けるだけで伝わる昨日のデプロイ手順:$ git pull origin main$ npm install$ npm run build$ docker compose build$ docker compose up -dエイリアスだと毎回「それは何のコマンドか」と説明が必要になりますが、abbr なら誰でも理解できます。コマンド履歴が機械可読になる現代の開発環境では、GitHub Copilot CLI、Claude Code、Cursor、Aider、Warp、Fig といった生成AIベースの支援ツールがあなたのコマンド履歴を解析します。これらのツールは、実際のコマンド履歴を分析して次に実行するべきコマンドを提案します。またエラーの原因を特定して修正方法を示し、作業パターンを学習して効率化を促し、プロジェクトのワークフロー理解にもつながります。エイリアスを使っていると、こうしたAIや人が作業内容を理解するのは困難です。abbr を使えば、履歴に記録されるのは実際のコマンドなので、コンテキストを正確に共有できます。チーム開発での透明性リモートペアプログラミングやスクリーンシェアで作業を共有する際です。# あなた: この手順でデプロイします$ dk build$ dk up -d$ dk logs -fチームメイト: 「...何をしているのか分かりません」abbr なら次のようになります。$ docker compose build$ docker compose up -d$ docker compose logs -fチームメイト: 「完璧に理解しました」ドキュメントとしての履歴あなたのコマンド履歴は、最高のドキュメントになります。例えば、kubectl でのデプロイ作業を考えてみましょう。エイリアスの場合、履歴には k apply -f deployment.yaml、k get pods、k logs -f pod-name のように記録され、意味が分かりません。abbr の場合は kubectl apply -f deployment.yaml、kubectl get pods、kubectl logs -f pod-name と記録されます。これなら、Wiki にコピペできますし、Issue にそのまま貼れます。解析ツールに渡して内容を振り返ってもらうこともでき、新しいチームメンバーの教材にもなります。エイリアスの時代は終わったはっきり言います。エイリアスは過去の遺物です。エイリアスが作られた時代には、コマンド履歴を第三者が読むこともあまり想定されていませんでした。スクリーンシェアで作業を共有する機会も多くありませんでした。しかし、2024年以降の開発環境は根本的に変わりました。コマンド履歴を解析する生成AIやエージェントが普及し、チームメンバーがリアルタイムであなたの画面を見ながら作業することも珍しくありません。履歴が検索可能なナレッジベースとして扱われるのが普通になりつつあります。この新しい現実において、abbr は必須です。エイリアスを使い続けることは、こうしたメリットを自ら放棄しているのと同じです。エイリアスとの決定的な違いエイリアス（Alias）の場合alias gco=\"git checkout\"コマンド履歴には gco と記録される実際に何が実行されたか後から分からない他人と共有する際に説明が必要abbr の場合abbr --add gco \"git checkout\"スペースキーを押すと git checkout が即座に展開されるコマンド履歴には展開後の git checkout が記録される履歴を検索する際に、エイリアスの短縮形ではなく実際のコマンドで検索できるスクリーンショットやドキュメントにそのままコピペできるabbr を使うべき理由abbr の主なメリットは、コマンド履歴が検索しやすくなること、他者とコマンドを共有しやすくなること、そして実際に何が実行されるかが常に可視化されることです。展開されたコマンドを毎回見るため、オプションを自然に覚えられる学習効果があります。history コマンドで過去のコマンドを見たとき、実際に何をしたかが一目瞭然です。同僚に「このコマンドを実行して」と伝える際、abbr で展開されたコマンドをそのまま共有できます。展開後に追加の引数を加えたり、一部を修正したりするのも簡単です。そして、abbr はインタラクティブシェルでのみ展開され、スクリプト内では展開されません。基本的な使い方abbr を追加するabbr --add gst \"git status\"abbr --add gaa \"git add --all\"abbr --add gcm \"git commit -m\"または、短縮形で表現できます。abbr -a gst \"git status\"abbr -a gaa \"git add --all\"abbr -a gcm \"git commit -m\"登録されている abbr を確認するabbr --list# またはabbr -labbr を削除するabbr --erase gst# またはabbr -e gstすべての abbr を表示するabbr --show# またはabbr -s実践的な abbr 設定例私の実際の config.fish から、カテゴリ別に便利な abbr を紹介します。ナビゲーション系# ディレクトリ移動を快適にabbr --add --global -- - 'cd -'           # 直前のディレクトリに戻るabbr --add --global .. 'cd ..'            # 一つ上の階層へabbr --add --global ... 'cd ../..'        # 二つ上の階層へabbr --add --global .... 'cd ../../..'    # 三つ上の階層へGit 系（最も使用頻度が高い）abbr --add --global g gitabbr --add --global ga 'git add'abbr --add --global gaa 'git add --all'abbr --add --global gc 'git commit -v'abbr --add --global gcm 'git commit -m'abbr --add --global gco 'git checkout'abbr --add --global gcb 'git checkout -b'abbr --add --global gp 'git push'abbr --add --global gpl 'git pull'abbr --add --global gst 'git status'abbr --add --global gd 'git diff'abbr --add --global gl 'git log'abbr --add --global gf 'git commit --amend --no-edit'  # 直前のコミットを修正Docker 系abbr --add --global d dockerabbr --add --global dc 'docker compose'abbr --add --global dcu 'docker compose up'abbr --add --global dcd 'docker compose down'abbr --add --global dps 'docker ps'Kubernetes 系abbr --add --global k kubectlabbr --add --global kgp 'kubectl get pods'abbr --add --global kgs 'kubectl get svc'abbr --add --global kgd 'kubectl get deploy'エディタ系abbr --add --global v nvimabbr --add --global vim nvim高度な abbr の使い方1. --global オプションabbr --add --global gst \"git status\"--global スコープで定義すると、universal スコープ（デフォルト）よりもわずかに高速です。config.fish で定義する場合は --global を使用するのがベストプラクティスです。2. --position anywhere - どこでも展開デフォルトでは、abbr はコマンドの位置（行頭）でのみ展開されますが、--position anywhere を使うとパイプの後などでも展開できます。abbr -a L --position anywhere --set-cursor \"% | less\"3. --set-cursor - カーソル位置の指定展開後のカーソル位置を指定できます。% がカーソル位置のマーカーです。abbr --add grepf --set-cursor 'grep -r \"%\" . | fzf'grepf とタイプしてスペースを押すと grep -r \"\" . | fzf に展開され、カーソルが \"\" の中に配置されます。4. --regex - 正規表現によるマッチングパターンを正規表現で指定できます。たとえば、.txt で終わるファイル名を vim で開けます。function vim_edit    echo vim $argvendabbr -a vim_edit_texts --position command --regex \".+\\.txt\" --function vim_edit5. --function - 関数による動的展開関数を使って動的にコマンドを生成できます。bash の !! に相当する機能です。function last_history_item    echo $history[1]endabbr -a !! --position anywhere --function last_history_item6. --command - 特定コマンドでのみ展開（Fish 4.0+）Fish 4.0 以降では、特定のコマンドに対してのみ展開される abbreviation を作成できます。abbr --add --command git co checkoutこの場合、git co は git checkout に展開されますが、co 単独では展開されません。config.fish への設定方法abbr は一度設定すれば記憶されますが、dotfiles として管理する場合は config.fish に記述する必要があります。推奨される設定方法if status is-interactive    # 既存の abbr をクリーンアップ（エラーを無視）    abbr --erase gst 2\u003e/dev/null    abbr --erase gaa 2\u003e/dev/null        # 新しく abbr を追加    abbr --add --global gst 'git status'    abbr --add --global gaa 'git add --all'    # ... 他の abbrendif status is-interactive で囲むことで、インタラクティブシェルでのみ abbr が定義されます。--erase してから --add する理由config.fish は新しいシェルを起動するたびに実行されます。既存の abbr を消してから追加することで、変更が確実に反映されます。私の設定では、すべての abbr をまとめて消去してから再定義しています。if status is-interactive    # 既存のabbreviationをクリーンアップ（エラーを無視）    abbr --erase -- - 2\u003e/dev/null    abbr --erase .. 2\u003e/dev/null    abbr --erase ... 2\u003e/dev/null    # ... すべての abbr を列挙        # ナビゲーション    abbr --add --global -- - 'cd -'    abbr --add --global .. 'cd ..'    # ... 新しく定義endよくある質問abbr とエイリアスはどちらを使うべきかA: abbr を使ってください。議論の余地はありません。エイリアスは裏で展開されるため、実際に何が実行されているかが分かりにくくなります。特に生成AIや外部の支援ツールやチームメンバーと履歴を共有するのが当たり前になった今では、abbr は必須です。ただし、複雑な処理（条件分岐やパイプの組み合わせなど）が必要な場合は、関数を使いましょう。abbr はスクリプトで使えるかA: いいえ。abbr はインタラクティブシェルでのみ展開され、スクリプト内では展開されません。スクリプトでは関数やエイリアスを使ってください。スペースキーを押さずに abbr を展開したくない場合A: Ctrl+Space を押すと、abbr を展開せずにスペースを入力できます。abbr を一時的に無効にしたいときA: abbr は一度定義されると永続化されるので、完全に削除するか、新しいセッションでは config.fish の該当行をコメントアウトしてください。Fish 以外のシェルでも abbr を使う方法「Fish に興味はあるけど、Zsh や Bash から移行するのは大変...」と感じる人も多いでしょう。朗報です。abbr の恩恵は他のシェルでも受けられます。Zsh で abbr を使うZsh には zsh-abbr という優れたプラグインがあります。Fish の abbr に完全にインスパイアされており、ほぼ同じ機能を提供します。他にも追加でいくつか類似ソフトウェアがあるので自分にあうものを選んでほしいです。インストール方法Homebrew を使う場合:brew install olets/tap/zsh-abbr手動インストールの場合:git clone https://github.com/olets/zsh-abbr.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-abbr.zshrc に次の設定を追加します。# プラグインとして読み込むplugins=(... zsh-abbr)使い方# abbr を追加abbr gco=\"git checkout\"abbr gst=\"git status\"# グローバル abbr（コマンド位置以外でも展開）abbr -g L=\"| less\"# 一覧表示abbr list# 削除abbr erase gcoFish とほぼ同じシンタックスで使えます。github.com手動で実装する方法（軽量版）プラグインを使いたくない場合は、以下のコードを .zshrc に追加するだけで基本的な abbr 機能が使えます。# 展開可能なエイリアスのリストtypeset -a ealiasesealiases=()# abbr 風のエイリアス作成関数function abbrev-alias() {    alias $1    ealiases+=(${1%%\\=*})}# スペースキーでエイリアスを展開function expand-ealias() {    if [[ $LBUFFER =~ \"\\\u003c(${(j:|:)ealiases})\\$\" ]]; then        zle _expand_alias        zle expand-word    fi    zle magic-space}zle -N expand-ealias# スペースキーをバインドbindkey ' ' expand-ealiasbindkey '^ ' magic-space  # Ctrl+Space で展開をスキップ# Enter キーでも展開expand-alias-and-accept-line() {    expand-ealias    zle .backward-delete-char    zle .accept-line}zle -N accept-line expand-alias-and-accept-line# abbr を定義abbrev-alias gco=\"git checkout\"abbrev-alias gst=\"git status\"abbrev-alias gcm=\"git commit -m\"dev.toBash で abbr 風の機能を実装するBash には組み込みの abbr 機能はありませんが、bind コマンドを使って似たような動作を実現できます。# スペースキーで展開される「abbr」を実装bind '\"\\e[0n\": \" \"'# abbr のような関数function abbr-expand() {    local cmd=\"${READLINE_LINE%% *}\"    case \"$cmd\" in        gco) READLINE_LINE=\"git checkout${READLINE_LINE#gco}\" ;;        gst) READLINE_LINE=\"git status${READLINE_LINE#gst}\" ;;        gcm) READLINE_LINE=\"git commit -m${READLINE_LINE#gcm}\" ;;    esac}# Space キーにバインドbind -x '\"\\e[0n\": abbr-expand'ただし、Bash での実装は Zsh や Fish ほど洗練されていないため、本格的に abbr を使いたい場合は Zsh + zsh-abbr または Fish への移行 をお勧めします。どのシェルを選ぶべきかabbr を最大限活用したいなら、Fish がネイティブサポートで最高の体験を提供します。Zsh + zsh-abbr は Fish とほぼ同等で、POSIX 互換性も維持できます。Bash は限定的なサポートなので、他の選択肢がない場合のみお勧めします。特に、共同作業や自動化が標準になった今では、abbr のようなトランスペアレントな機能が必須です。どのシェルを使うにしても、エイリアスから abbr への移行を強くお勧めします。よくある質問（追加）今使っている Zsh から Fish に移行すべきかA: abbr だけが目的なら、zsh-abbr プラグインで十分です。ただし、Fish は他にも多くの優れた機能（シンタックスハイライト、自動補完など）を持っているので、試してみる価値はあります。既存のエイリアスを abbr に移行するのは大変かA: 非常に簡単です。alias を abbr に置き換えるだけです。Fish の場合は abbr --add、Zsh の zsh-abbr の場合も abbr コマンドがそのまま使えます。まとめFish Shell の abbr は、単なるショートカット以上の価値があります。変化の激しい開発環境において、abbr は必須のツールです。abbr が提供する価値abbr は誰にとっても読みやすい履歴を残し、あとから状況を把握する人があなたの作業を完璧に理解できるようにします。実際のコマンドが常に見えることで可視性が確保され、コマンドを自然に覚えられる学習効果があります。チームメンバーとのコミュニケーションが円滑になり、意味のある機械可読な履歴が残ります。生成AIエージェントに渡したときにも正確なコンテキストが伝わり、展開後の編集も柔軟で、コマンド履歴がそのまま最高のドキュメントになります。エイリアスから abbr への移行今すぐ始めるべき理由は明確です。Claude Code、Codex、Copilot CLI、Cursor などの支援ツールがあなたの作業を理解できるようになります。チーム開発の透明性が高まり、リモートワークやペアプログラミングでの生産性も劇的に向上します。加えて、コマンド履歴が検索可能で再利用できるナレッジとして蓄積されます。移行は非常に簡単です。Fish を使っているなら abbr --add で定義するだけ。Zsh なら zsh-abbr プラグインをインストールするだけです。エイリアスの時代はおそらく終わる気がしています。それでもエイリアスのおかげで多くの時間を節約できたのは事実ですし、長く愛用してきた相棒でもあります。ありがとう、エイリアス。参考リンクgithub.comgithub.comdev.toddbeck.comwww.youtube.com","isoDate":"2025-11-22T03:30:28.000Z","dateMiliSeconds":1763782228000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"たぶん、読んでない","link":"https://syu-m-5151.hatenablog.com/entry/2025/11/21/100503","contentSnippet":"１　「ねえ、今年、何冊読んだ？」　秋だか冬だかよくわからない曖昧な気温の日の帰り道、奈々子が突然そう聞いてきた。駅までの道はいつも通り混んでなくて、コンビニの前にだけ人が固まって、誰もがスマホを見ていた。私もその一人だった。「え？」「本。本。読書」「……えーと」　指の動きが止まる。さっきまでタイムラインで「＃今月の読了本」とか「＃社会人の学び直し」とかを眺めていたのを、慌ててLINEに切り替える。画面を隠すみたいにスマホを持ち替えてから、私はうーんと声だけ伸ばした。「十冊くらい？」　とりあえず無難そうな数字を出してみる。百と言うほどの勇気も、ゼロと言うほどの正直さもない。「うわ、すご。ちゃんとしてるじゃん」　奈々子は素直に感心して、コンビニのビニール袋をぶらぶら揺らした。中身はたぶん夕ご飯兼夜食。糖質と油でできた「がんばる社会人の味方」みたいなやつ。「直近で読んだ本、なに？」　その追撃は予想してなかった。「え、直近？」「うん。最後に読み終わったやつ」　最後に読み終わった本。　――最後に読み終わった本。　頭の中の本棚を、一応探す。けどそこでまずひっかかるのは「読み終わった」という条件だった。読みかけのまま机に積んだ本なら、タイトルは山のように浮かぶ。「入門なんとか」「ゼロから学ぶなんとか」「要点でわかるなんとか」。でも「最後まで読んだ」と言い切れるやつは、思い出そうとした瞬間、全部グレーアウトする。「……あれ」「なにその“あれ”？　バグ起きてる？」「最後に読み終わった本、いつだっけって……」「こわ。バックアップとってないの？」　奈々子のそういう言い方はいつも冗談っぽくて、でもちょっとだけ刺さる。私は笑ったふりをして、苦い唾を飲み込む。「てかさ」　奈々子が続ける。「うちの会社、来月からなんか“リスキリング読書チャレンジ”っての始まるんだよね。部署ごとに月一冊ビジネス書読んで、感想共有しましょう、みたいな。で、個人でも“今月の一冊”みたいなのやるらしくてさ」「へえ」「でね。せっかくだから、私たちもやろうかなって思って」「私たち？」「ほら、サークルメンバー。社会人になってから、全然会わなくなっちゃったじゃん？　“オンライン読書会”とかやればさ、月一くらいで話すきっかけにもなるかなって」　ああ、そういう流れか、と理解する前に、奈々子はもうスマホを取り出していた。私の視界に、彼女の親指がスタンプを送るような速さでグループ名を打ち込んでいるのが見える。「グループ名なにがいいかな。“読書会”だとダサいよね。“本を読む会”はもっとダサいし」「その辺の差は誤差じゃない？」「“＃積読解消戦線”とか？」「長い」「じゃあ“積読クラブ”とか」　その単語に、私は少しだけ心臓を掴まれた気がした。「よくない？　積読って正直だし。でも“クラブ”ってつけると急に救われる感じしない？　あ、今の、私けっこう名言じゃない？」「自画自賛するな」　笑いながら、私は「積読クラブ」の五文字を頭の中で反芻した。　積読クラブ。　積んだまま、読まない本たち。　その周りに集まる、積んだまま、読まない人たち。　グループは五分後にはできていた。元サークル仲間の名前が次々と追加されていく。就職して地方に散った人たち。営業職。エンジニア。保育士。フリーター。誰もが、プロフィール欄にそれぞれの「がんばってる私」っぽい一言を添えていた。「＃新卒一年目」「＃営業修行中」「コーヒーと本があれば生きていけます」みたいなやつ。　私は自分のプロフィール欄を見て、ため息をついた。「読書と映画と猫が好きです。」　猫は好きだ。映画も好きだ。読書は――好きになりたい、の方が正確かもしれない。２　グループのルールは意外とちゃんとしていた。　一、月に一冊は「自腹で」本を買うこと　二、「今月の一冊」を写真付きでグループに投稿すること　三、月末にはオンラインで一時間だけ感想を話すこと　四、ネタバレは基本あり。ただし他人の「まだ途中」を尊重すること　奈々子が会社の企画を真似して、少し柔らかくしたらしい。ルールが投稿された数秒後には、「いいね」「賛成」「最高」みたいなスタンプが飛び交った。画面の中の絵文字たちは、私よりずっと素直で健康そうだった。　問題は、一だ。　「自腹で」本を買うこと。　それなら余裕だ。　問題は、二と三だ。　「今月の一冊」を写真付きで投稿し、「感想を話す」こと。　私はすでに、自腹で買って読んでない本を、床の上に二十冊くらい積んでいる。　最初の週末、私はその本の塔の前に正座していた。積まれた背表紙たちが、集合写真みたいにこちらを見ている。「入門○○」「ゼロからわかる××」「二十代で身につけたいなんとか」。購入当時の私は、どれも必要だと思っていた。今の私は、どれから逃げるかを考えている。「買わないでも、これのどれかに“今月の一冊”ってタグつければよくない？」　自分に言ってみる。でもそれは、何かを誤魔化すための言い訳にしか聞こえない。そもそもこの「積読クラブ」、積読を増やすために始めたんじゃない。減らすために、だったはずだ。　とりあえず、私は一番上の一冊を手に取る。帯には「今、若手社会人が読むべき一冊！」と書いてあった。誰が決めたのか知らない「今」と「べき」。表紙には、スーツ姿の誰かが笑っているイラスト。笑顔がまぶしい。帯を外し、ページをぱらぱらとめくる。文字が詰まっている。「はじめに」の最初の段落を読む前に、私はスマホを取り出した。「○○　要約」と打ちながら、ため息をつく。　検索結果には、ブログ記事や要約動画がずらっと並んでいた。「３分でわかる」「１０分で理解」「この本のポイントは３つだけ」。そこに並ぶタイトルたちを見ていると、「本を読む」という行為そのものが、もう既に誰かの仕事によって要約済みなんじゃないかって気がしてくる。　私は一番上のブログを開いた。　「この本で著者が伝えたいことは、大きく分けて３つあります。」　その一文を見た瞬間、私はすでに、達成感の影を感じていた。　――あ、わかった気がする。　ブログを最後まで流し読みし、そのまとめを自分の頭の中でさらにまとめてみる。「変化の時代には主体的なキャリア形成が」「行動こそ最大の学び」「失敗を恐れず挑戦を」。どこかで聞いたことのある言葉たちが、どこかからコピペされたみたいに整列していく。私は本を開きもせずに、その本について「語れる気」になり始めていた。　カメラアプリを起動する。表紙をきれいに撮るため、部屋の中で一番明るい場所を探して本を持ち歩く。窓際、机の上、ベッドの上。一番映えそうな角度を探して、何枚か撮る。フィルターをかける。明るさを調整する。「それっぽい」写真ができたところで、私はグループに投稿した。「今月の一冊はこれにしました！」　文末に本の簡単な紹介と、「今の自分にはこれが必要だと思ったので」という一文を添えて送信する。送信ボタンを押した瞬間、スマホが震えたような気がした。実際には震えてない。ただ私の心臓が、勝手に震えただけだ。　数秒後、「おおー！」「それ気になってた！」「感想聞きたい！」とスタンプが返ってくる。画面には、色とりどりの絵文字と既読マーク。「ちゃんとしてる私」を、数秒で証明できてしまったみたいで、少し怖かった。　机の上では、さっき撮影に使った本が、まだ「はじめに」のページすら開かれていないまま、静かに置かれていた。３　月末のオンライン読書会は、想像以上にカオスだった。「じゃあ、今月の一冊、順番に話してこっかー」　奈々子が司会を買って出て、画面に並んだ六つの顔を順番に指名していく。ZOOMの小さな四角の中で、それぞれの生活感が垣間見える。洗濯物が干されたままの部屋。オフィスっぽい背景。カーテンだけが映っている画面。バーチャル背景で海になっているやつ。「じゃあまずは、りおから」　名前を呼ばれて、一瞬だけ返事を忘れる。慌ててマイクをオンにした。「あ、はい」「何読んだんだっけ？」「えっと……これ」　先週、タイムラインに流れてきた感想ツイートを３つくらいスクショして、ノートアプリに箇条書きしたやつが、スマホの裏側で控えている。本体より、そっちの方を信頼している自分が情けない。「えーとね、“自分のキャリアは自分で選べ”みたいな話で……」　自分で選べ。　自分で選べ。　自分で選んだ結果、私は今、要約ブログだけを読んでしゃべっている。　私が拙い言葉で本の内容をなぞる間、画面の中の友人たちは、うんうんと頷いたり、「わかるー」と相づちを打ったりしてくれる。その優しさが、逆に拷問みたいに感じる。「で、特に印象に残ったのが、“行動しないと何も変わらない”っていうところで……」　自分で言って、自分で刺さる。　行動しないと、何も変わらない。　でも私は、本すら開いていない。「りお、なんか変わった？　これ読んで」　奈々子がライトに聞いてくる。彼女に悪気がないのはわかってる。でも、だからこそ、逃げ場がない。「えっと……」　一瞬、本気でZOOMを落としてやろうかと思った。回線不良になったふりをして、消えてしまう。けど、それをやったら、たぶんこのグループからも、本当に消えてしまう気がした。「“変わった”っていうか……なんか、今のままだとやばいかもって思った、かな」　それは嘘ではなかった。ブログを読んで、本を読んだ気になって、それでもどこかで罪悪感を抱えている自分を「やばい」と思っているのは、本当だ。「おー、いいじゃん。危機感、大事」　奈々子が笑って、他のメンバーもうんうん頷く。「てかさ、正直言うと……」　別の四角から、慎ましい声がした。「ちゃんと最後まで読めたの、今月、一冊もないんだよね」　話しているのは、健太だった。サークル時代、いつも端っこで本を読んでた、よく意味のわからない人。卒業してからも、読書メーターみたいなアプリのスクショをよくタイムラインに上げていたから、「あいつはずっと本を読んでる人」だと、勝手に思い込んでいた。「え、そうなの？」「うん。三冊買ったんだけど、どれも途中で飽きてさ。仕事忙しいのもあるけど、なんか……集中できないんだよね。本開いても、三ページくらいでスマホ見ちゃう」　その言葉に、私は勝手にドキッとした。　三ページでスマホ。それは、まさに私のことだった。「でもさ、健太、読書メーターめちゃ更新してるじゃん」　誰かがツッコむ。健太は「あー」と曖昧に笑った。「あれも、正直、ちょっと“盛ってる”」「盛ってる？」「途中までしか読んでない本も、“読了”にしちゃってる。なんかさ、“途中まで読んで放置した本”って、アプリ上でも現実でも、すごい罪悪感あるじゃん。だから、読み切ってなくても、“だいたいわかったからいいや”みたいな感じで、読了にしちゃう。自己満だけど」　画面越しに沈黙が落ちた。その沈黙には、「わかる」と「怖い」と「笑える」と「笑えない」が全部混ざっていた。「ていうかさ」　奈々子が笑いながら言う。「こういう場で“わかる”って言える時点で、もうけっこう重症だよね、私たち」　笑いが広がる。私も笑う。けどその笑いの中で、心のどこかが冷えていく。　――ああ、みんなも同じなんだ。　そう思うと、安心するはずなのに。「じゃあさ」　奈々子が、急に真面目な声になった。「この中に、“ちゃんと読んだ人”、いる？」　一瞬、画面が固まったように見えた。誰も喋らない。誰も名乗り出ない。　そのとき、画面の隅っこで、誰かのアイコンが小さく光った。ミュート解除のマークがつく。私、そこに誰がいたか、正直すぐには思い出せなかった。グループに追加されてたのは知ってたけど、この一ヶ月、ほとんど発言してない人だ。「……一冊だけ、読んだ」　画面の中央に、その人の顔が映し出される。メガネ。無造作な前髪。背景には、本棚。背表紙の色合いからして、ビジネス書じゃなさそうだった。「あれ、みゆき？」　奈々子が目を丸くする。「え、みゆき、いたの？」「いたよ。最初から」　みゆき。大学時代、同じサークルにいたけど、ほとんど話したことがない。いつもイベントの受付をしていて、写真に写るときは端っこにいた。存在感が薄い、というより、空気と同じくらい自然にそこにいる人。「なに読んだの？」　奈々子が聞く。みゆきはちょっと迷ってから、画面から消えた。数秒後、本を一冊持って戻ってくる。「これ」　画面いっぱいに映し出されたのは、聞いたことのない小説のタイトルだった。帯には、どこかの文学賞のロゴ。売り場で平積みになっているイメージが、あまり湧かない。「なんか、意外」「うん。仕事で疲れるからさ、ビジネス書とか、読む気にならなくて。とりあえず、“今読みたいもの読もう”って思って」　その「今読みたいもの」という言葉が、やけにまっすぐに聞こえた。「どうだった？」「うーん……」　みゆきは少し考えてから、言葉を探すみたいに話し始めた。「最初、全然、意味わかんなかった。登場人物が何考えてるのかもよくわかんないし、文章もなんかへんな感じで。でも、読み進めてるうちに、“意味わからないけど、なんかこの感じ、わかるかも”ってところが増えてきて……なんていうか、“答えがないまま終わる話”なんだけど、その“答えのなさ”が、読んでてすごい落ち着いた」　画面の誰かが、「へえ」と感心する。私は、自分の指先が汗ばんでいることに気づいた。「なんかさ」　みゆきは、言葉を足す。「仕事で、毎日、“正解に近づく”ことばっかりやってる気がして。“より正しい資料”“よりわかりやすい説明”“より納得してもらえる提案”。そういうのを目指すのは嫌いじゃないんだけど……なんか、“どこにも着地しない話”を読んでると、“着地しなくてもいい時間”がちゃんとあるの、ありがたいなって思った」　誰かが「わかるかも」とぼそっと呟く。「でね」　みゆきは、小さな声で続ける。「この本読んだこと、別に誰にも言うつもりなかったんだよね。本棚にしまって、終わりでいいかなって。でもこのグループあるから、“一応、報告しとこっか”って思って」　奈々子が笑う。「うちらの積読クラブ、効いてるじゃん」「でもさ」　みゆきは、少しだけ目線を落とした。「なんか、“本を読んだことを報告するために読む”ようになったら嫌だなって、ちょっと思った」　その言葉が、静かに画面全体に降りた。「だから今月は、この一冊だけでいいやって思った。『今月の一冊』っていうより、『今のわたしの一冊』の方が、しっくりくるから」　誰もすぐには何も言わなかった。奈々子ですら、一瞬、言葉を失っているように見えた。　グループの名前は「積読クラブ」だけど、今この瞬間、私たちの前にそびえ立っているのは、本の山じゃなくて、会話の沈黙だった。その沈黙の高さを測りながら、私は、自分の机の上の、開かれていないビジネス書のことを思い出していた。４　読書会が終わってから、しばらくの間、グループチャットは静かだった。いつもなら「おつかれー」「今日も楽しかった！」みたいな軽いメッセージが飛び交うのに、その日はスタンプ一個だけで終わった。　私はノートパソコンを閉じ、部屋の電気を消した。暗くなってから、机の上の本を手探りで見つける。さっきまで「読んだふり」をするための道具だったそれが、急に、すごく重く感じた。　窓の外には、向かいのマンションの灯りがぽつぽつとついていた。どの部屋にも、それぞれの生活があって、それぞれの「今月の一冊」だか「今週のタスク」だか「今日の後悔」だかがあるんだろう。　私はベッドに腰掛け、本を膝の上に置いた。　――今のわたしの一冊。　みゆきの言葉が、何度もリピートされる。　今のわたし。　今のわたし、ってなんだろう。　タイムラインを開けば、誰かの「今」は洪水みたいに流れてくる。今読んだ本。今感じたこと。今考えていること。今、頑張っている自分。今、落ち込んでいる自分。今、立ち直っている自分。誰もが「今」を差し出し合い、その中に正解を探している。　でも、私の「今」は、うまく言語化できない。　ただ、疲れていて、焦っていて、何かにならなきゃいけない気がして、何にもなれていない。　私は本を開いた。　「はじめに」の一行目を読む。　さっき、要約ブログで読んだ内容と、ほとんど同じことが書いてある。でも、紙に印刷されている文字と、スマホのスクロールで流れていく文字は、同じ意味のはずなのに、体感が違う。　二行目を読む。　三行目を読む。　十行目あたりで、スマホに手が伸びそうになる。　「本　要約」「この本　感想」「この本　評判」。　そのどれかを検索すれば、「自分の考え」の代わりになる言葉が、いくらでも手に入る。　でも、今日はとりあえず、それをしないことにしてみる。　ページをめくる。　文字を追う。　頭に入っているのかどうか、自分でもよくわからない。著者の例え話が、いちいち大げさで、鼻につく。自分とは違う世界の人が、違う世界の成功体験を、私に一方的に教えようとしてくる。　途中で、「なんでこれ選んだんだっけ」と思う。　「今の自分にはこれが必要だと思ったので」。　投稿文に書いたあの一文が、じわじわと恥ずかしくなってくる。　そのとき、ふと気づく。　――誰にも見せない読書って、めちゃくちゃ、不安定だ。　カメラロールには、読書中の私の写真はない。　タイムラインにも、読書ログは流れない。　アプリの「読了数」も、増えない。　私がこの本を読んだことを知っているのは、たぶん、今日の私だけだ。明日の私はもう忘れているかもしれないし、来週の私は別のことに追われているかもしれない。この時間は、どこにも記録されないまま、沈んでいく。　その感じが、なぜか、少しだけ、心地よかった。　ページをめくる速度は、遅い。　時々、同じ行を二度読む。　わからない言葉は、そのまま放置する。　大事そうなところに、線を引こうとして、ペンを取りに立ち上がるのがめんどうで、やめる。　「読書」と呼ぶには、あまりにもだらしない。　「学び」と呼ぶには、あまりにも生産性が低い。　それでも、本は、そこにある。　ここで、私とだけ、つながっている。5　その夜、私は本棚の前に立った。　積まれた背表紙の山。　その中から、一冊を適当に引き抜く。　ビジネス書でも、自己啓発書でも、小説でもない、よくわからないエッセイ集だった。たぶん、前にどこかの書店で、「人気芸人が勧める３冊」みたいなポップを見て、勢いで買ったやつだ。　表紙のデザインも、著者の名前も、今まで何度も目にしているはずなのに、「初めてちゃんと見る」感じがした。　ページを開く。　一行目を読む。　面白いかどうかは、まだわからない。　ためになるかどうかも、まったく不明。　この本を読み終わったところで、給料が上がるわけでもないし、フォロワーが増えるわけでもない。　でも、今、ここにある「わからなさ」は、たぶん、誰かのブログでは代替できない。　誰かの要約では、コピーできない。　部屋の中は静かだった。　スマホは、ベッドの向こう側で、画面を下にして置いてある。通知が鳴っても、すぐには気づかない場所。　一ページ。　二ページ。　三ページ。　ちょっとだけ、スマホのことを思い出す。　でも今日は、とりあえず、手を伸ばさない。　四ページ。　五ページ。　どこまで読んだら「読書」と呼べるのかなんて、誰も決めていない。　どこまで理解したら「学び」になるのかなんて、誰も教えてくれない。　きっと私は、これからも、本を読んでるふりをする。　読んでないのに「読んだ」と言いたくなる夜も、またあるだろう。　積読の山は、これからも増えるかもしれない。　それでも、ときどき、こうして誰にも見せない読書をする。　誰にも報告しないまま、本を開いて、閉じる。　「今のわたしの一冊」は、きっと、そのたびに変わる。　変わらないまま終わる夜もある。　それでいいのかどうかなんて、まだわからない。　でも、わからないままページをめくることくらいは、今の私にもできる。　ページの端をつまんで、ゆっくりとめくる。　新しい行が現れる。　そこには、誰かの言葉が並んでいた。　それを「理解」できたかどうかよりも先に、私はただ、その黒いインクの並びを、目で追い続けた。　たぶん、それも、読書のうちに入れていい。　そう勝手に決めて、私はもう一ページ、めくった。","isoDate":"2025-11-21T01:05:03.000Z","dateMiliSeconds":1763687103000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"おい、対話しろ","link":"https://syu-m-5151.hatenablog.com/entry/2025/11/19/194809","contentSnippet":"はじめに会議室の空気が、徐々に重くなっていくのを感じました。「この設計、拡張性に問題があります」。若手エンジニアのAが言いました。声には確信がありました。「いや、今の要件を考えれば、これが最適だよ」。ベテランのBが即座に返します。口調は穏やかですが、譲る気配はありません。「でも、将来的に機能追加があったときに...」「将来のことばかり考えていたら、今のリリースが間に合わない」二人の言葉は交差しますが、交わりません。言葉のキャッチボールに見えて、実は2つのボールが空中でぶつかり合っているだけです。ボールは地面に落ち、誰も拾いません。私は黙って二人を見ていました。どちらの言い分も分かります。Aは技術的負債を恐れています。Bは納期のプレッシャーを感じています。どちらも正しく、どちらも間違っていません。何かが決定的に欠けています。対話が、ありません。二人は話しています。言葉を交わしています。しかし、対話していません。Aは自分の主張を繰り返し、Bも自分の主張を繰り返します。互いに相手の言葉を聞いているようで、実は聞いていません。正確に言えば、相手の言葉を「自分の理解の枠」に無理やり押し込んで解釈しています。会議は平行線のまま終わりました。結論は「後で話し合いましょう」。何も決まりませんでした。廊下を歩きながら、私は考えていました。なぜ私たちは、こんなにも対話ができないのか。技術の話をしているはずなのに、なぜ感情的な対立になるのか。対話は、なぜこんなにも難しいのか。この問いについて、私は何年も考え続けてきました。ある結論に到達しました。私たちは「対話」を誤解しています。対話とは何か、対話を阻むものは何か、対話を可能にするものは何か。これらをまったく理解していません。だから、対話という言葉を知っていても、対話ができません。対話の欠如は、組織を蝕みます。意思決定が遅れます。同じ議論を繰り返します。優秀な人材が疲弊して去っていきます。イノベーションが生まれません。答えはシンプルです。対話していないからです。このブログで、私は対話を語ります。しかし、「傾聴しましょう」「共感しましょう」という話ではありません。対話を阻む認識の構造を語ります。人間がどのように世界を見ているか、なぜ理解し合えないのか、どうすれば対話が可能になるのか。これらを、できる限り深く考えます。対話の前提を理解しなければ、対話は始まりません。まず、私たちは対話を阻んでいるものの正体を知らなければならないからです。このブログが良ければ読者になったり、nwiizoのXやGithubをフォローしてくれると嬉しいです。対話という幻想を解体する「対話が大切です」。誰もが知っています。でも、「対話とは何か」をちゃんと説明できる人は、ほとんどいません。多くの人は、対話を情報のやり取りだと思っています。私が言葉を発します。相手が受け取ります。相手が言葉を返します。私が受け取ります。このキャッチボールが、対話だと。しかし、それはおそらく誤解です。対話は、データ転送ではありません。対話とは相互の世界観を認識して、理解を深めるプロセスです。その過程で認識の変容が起きることもあれば、相違を明確に理解した上で立場を維持することもあります。どちらも対話の成果です。あなたがコードレビューで「この実装は複雑すぎます」と言います。相手は「いや、これは必要な複雑性です」と返します。ここで何が起きているでしょうか。表面的には、意見の交換に見えます。でも、実際に起きているのは、もっと深い層での衝突です。あなたが「複雑」と言うとき、あなたは過去に経験した「複雑なコードがメンテナンス不能になった」記憶を参照しています。あなたにとって「複雑さ」とは、未来の技術的負債の予兆です。一方、相手が「必要な複雑性」と言うとき、相手は「ビジネス要件の複雑さを適切にモデル化した結果」を見ています。相手にとって「複雑さ」とは、現実を正確に反映している証拠です。同じ「複雑」という言葉を使っていますが、指し示すものがまったく違います。この認識の差異に、どちらも気づいていません。対話が成立するためには、この差異に気づかなければならない。「ああ、私たちは同じ言葉を使っているが、違うものを見ているのです」と。この気づきがないまま言葉を交わし続けても、それは対話ではありません。ただの言葉の衝突です。そして、多くの人は対話の目的を誤解しています。対話の目的は、合意することだと思っています。それは副産物に過ぎません。対話の本質は、相互の世界観を認識することです。あなたの目に世界がどう映っているか。相手の目に世界がどう映っているか。この2つの視点を、互いに理解し合うこと。それが対話です。合意なき理解。これは矛盾しているように聞こえます。しかし考えてほしい。あなたは親友と、政治的な意見の相違があります。それでも友人関係は続きます。なぜか。互いの違いを理解しているからです。「あの人はこういう経験をしてきたから、こう考えるのです」。この理解があれば、意見の相違は関係を壊しません。むしろ、理解の深さが関係を強くします。対話をスキルだと考える人も多いです。傾聴のテクニック。共感の言葉。これらを学べば、対話ができると。しかし、テクニックだけではどうしても不十分です。対話は、技術である前に、存在の様式です。「どう在るか」の問題です。防御的な存在様式のまま、いくら傾聴のテクニックを使っても、それは対話になりません。対話とは、自分の腹の中を晒す行為です。「私は確信を持っていない」と認めることです。「私の見方は、1つの解釈に過ぎない」と受け入れることです。「相手の言葉によって、私の認識が変わるだろう」と覚悟することです。この覚悟なしに、対話は始まりません。だから、対話は難しいのです。自分が正しいと思いたい。自分の世界観を守りたい。私を含めて人は変化することを恐れます。これらの防衛本能が、対話を阻みます。対話するためには、まず自分の防衛を解除しなければならない。しかし、防衛を解除することは、無防備になることではありません。対話における強さとは、自分の視点や視座や観点を絶対化しないことです。複数の解釈を許容することです。不確実性の中でも思考し続けることです。対話を不可能にする構造対話が難しいのは、個人の能力の問題ではありません。人間の認識そのものに、対話を阻む構造が組み込まれているからだ。認識の構造的宿命あなたは今、この文章を読んでいますよね。でも、実は「読んでいる」のではありません。脳は、膨大な情報の中から一部を選択し、それを「意味」として構成しています。人間の認識は、選択的です。世界をあるがままに受け取ることはできません。必ず、フィルターを通す。このフィルターを、認知科学では「スキーマ」と呼ぶ。スキーマとは、過去の経験から構築された認識の枠組みです。スキーマは、生存に必要です。毎回ゼロから世界を理解していたら、判断が遅すぎて生き残れません。パターン認識によって、瞬時に判断します。これは、進化が私たちに与えた能力です。しかし、この能力には代償があります。私たちは、世界をあるがままに見ることができません。常に、認識というレンズを通して見ます。対話において、これは致命的な問題を引き起こすことがあります。相手の言葉を聞くとき、私たちは相手が言った言葉を聞いているのではありません。自分のスキーマによって解釈された言葉を聞いています。「この実装、ちょっと心配だな」。相手がこう言ったとき、あなたは何を聞くでしょうか。もしあなたが過去にこの相手から批判された記憶があるなら、「また批判されています」と聞きます。しかし、相手は単に「一緒に確認したい」と言っているだけだろう。バイアスを取り除く。これは、よく言われるアドバイスです。しかし、バイアスを完全に取り除くことは不可能です。 バイアスは、認識の副作用ではありません。認識そのものです。スキーマなしに世界を見ることはできません。できるのは、「自分がバイアスを通して見ています」という自覚だけです。この自覚があるとき、対話の質が変わります。相手の言葉を聞いて、即座に「これはこういう意味です」と決めつけません。「私はこう解釈したが、相手の本当の意図は違うだろう」と留保します。「どういう意味ですか」と確認します。この一手間が、誤解を防ぐことがあります。認識の構造的宿命を受け入れること。これは対話の第一歩です。「私は世界をありのままに見ていない」と認めること。「私の解釈は、1つの可能性に過ぎない」と理解すること。この謙虚さが、対話の基盤です。権力勾配という非対称性「最近、調子はどう」。上司がこう聞きます。あなたは「はい、順調です」と答えます。同じ質問を同僚が聞きます。あなたは「ちょっと行き詰まっている」と本音を言います。同じ言葉なのに、発する人が変わると、意味が変わります。これは、社交辞令の問題ではありません。権力の非対称性が、言語の意味を書き換えています。上司の「最近どう」は、音声学的には同僚の「最近どう」と同じです。しかし、意味論的にはまったく別の文です。上司の言葉には、評価の含意があります。あなたの答えは、業績の報告として受け取られる可能性があります。だから、あなたは防御的になります。これは、悪意の問題ではありません。上司が部下を評価しようとしているわけではありません。だが、位置関係が、言葉に意味を付与します。 発話者の意図とは無関係に。権力の非対称性は、対話を歪めます。上下関係のある場で「自由に意見を言ってください」と言われても、部下は自由に意見を言えません。なぜならその意見が評価に影響する可能性を意識するからだ。たとえ上司が「評価には関係ない」と保証しても、その保証自体が権力の行使です。この問題に対して、「フラットな関係を目指しましょう」というアプローチがあります。しかし、これは幻想です。形式を変えても、構造は変わりません。 給与を決める権限、人事評価をする権限、プロジェクトのアサインを決める権限。これらの権力は、言葉遣いを変えても消えません。むしろ、権力の存在を否認することで、問題は見えにくくなります。現実的なアプローチは、権力の非対称性を前提とすることです。「私とあなたには、権力の差がある」と認めること。その上で、「この制約の中で、どこまで対話を開くことができるか」と問うこと。1つの方法は、構造を明示することです。「私は上司として聞いているのではなく、エンジニアとして意見を聞きたい」と宣言します。「今日の議論は、人事評価には一切関係ない」と約束します。その約束を守る。一貫性のある行動によって、徐々に信頼が生まれます。もう1つは、リスクを先に取ることです。権力を持つ側が、先に自分の弱さを晒す。「私もこの技術については自信がない」と認めます。「あなたの方が詳しいので、教えてほしい」と頼る。権力者が弱さを見せることで、非対称性が少し和らぐ。さらに、組織への信頼が一定以上ある場合、匿名フィードバックの併用も有効です。たとえば、月次で匿名のエンゲージメントサーベイを実施します。そこで出た意見を全体会議で議論します。これにより、権力関係の影響を受けずに本音の課題が可視化され、対話の素材となります。ただし、組織自体への信頼がなければ、良質なフィードバックは集まりません。匿名フィードバックは、信頼の上に成り立つ仕組みです。しかし、これはすべて部分的な緩和に過ぎません。権力の非対称性は根深く、簡単には変わらない。それでも、それを自覚して、丁寧に扱うことはできます。対話は完璧にはなりません。権力勾配がある限り、完全に対等な対話は困難です。でも、不完全な対話でも、無対話よりはるかにましです。時間的ズレという錯誤「あなたはいつもそうです」。この言葉を聞いたことがあるでしょう。しかし、よく考えてほしい。「あなたはいつもそうです」と言うとき、あなたは何を見ているのか。目の前の現在の相手を見ているのでしょうか。それとも、記憶の中の過去の相手を見ているのか。後者です。私たちは、相手の過去の行動パターンを記憶しています。そのパターンを、今の相手に投影しています。「あなたはいつも約束を守らない」と言うとき、私たちは過去の2回、3回の出来事を思い出しています。それを「いつも」に拡大しています。しかし、今この瞬間の相手は、過去の相手ではありません。人は変わります。状況は変わります。昨日の相手と今日の相手は、厳密には別の存在です。でも、私たちは記憶の中の相手と対話しています。現在の相手の言葉を、過去のパターンに当てはめて解釈しています。これは、認識の効率化です。毎回相手を新しく理解するのは、コストが高い。だから、脳は過去の経験からパターンを作り、それを使って瞬時に判断します。ほとんどの場合、これは有効です。相手の性格や行動パターンは、そう簡単には変わりません。しかし、対話においては、この効率化が仇となります。相手が変化しようとしているとき、成長しようとしているとき、私たちは過去のレッテルを貼り続けます。「あの人はこういう人です」という決めつけが、相手の変化を見えなくします。「以前のあなたなら、こう言っただろうけど」という前置きは、実は相手を過去に縛りつけています。多くの場合、予言の自己成就が起きます。「どうせ変わっていないと思われているなら、変わる必要はない」と相手は感じます。時間的ズレを意識するとは、「相手は過去の相手ではありません」と認めることです。「今のあなたは、どう考えていますか」と問うことです。過去のパターンは参考にしつつ、決定的な判断材料にしないことです。これは、自分自身に対しても同じです。「私はこういう人間です」という自己認識は、実は過去の自分のパターンです。対話するとは、この時間的ズレを認識することです。相手と自分、両者とも常に変化しています。過去に縛られず、現在に向き合う。しかし、ここで残酷な真実に向き合わなければならない。人は何にでもなれるから、何にもなれません。 無限の可能性があるように見えて、実は時間は有限です。「いつかやろう」は、気づいた時には「もうできません」に変わっています。時間は不可逆です。我々は有限の存在です。だからこそ、今この瞬間の対話が重要になります。先延ばしにした対話は、永遠に失われる可能性があります。ここまで見てきた3つの制約――認識の不可避性、関係性の非対称性、時間性の逆説――は、いわば人間という存在の「ハードウェアの制約」です。私たちは世界をそのまま受け取れないし、権力関係から自由でもないし、時間の外にも出られない。では、この制約の上で、私たちはどうやって対話を可能にしていけばいいのでしょうか。制約を完全に消すことはできません。しかし、制約を認識し、それを前提としながら、対話を開く力を育てることはできます。だからこそ、「制約付きの人間」がどんな力を鍛えれば対話が成り立つのかを、具体的に見ていく必要があります。ここからは、対話を可能にする3つの具体的な力を見ていきます。対話を可能にする力ここまで、対話を阻む構造的な問題を見てきました。認識の限界、権力の非対称性、時間の錯誤。これは、私たちの認識そのものに組み込まれた、避けがたい制約です。しかし、これらの制約を認識することが、対話への第一歩です。制約を知ることで、私たちは対話を可能にする力を育てることができます。対話は、単なる技術ではありません。同時に、訓練可能な能力でもあります。以下の3つの力は、対話を可能にする中核的な能力です。中断する力朝、目覚ましが鳴る。あなたは無意識にスマホを取ります。これらの行動は、意識的な判断を経ていない。自動的に起きています。人間の判断の大部分は、自動的に処理されています。私はこれを、2つのシステムとして理解しています。システム1は、速く、自動的で、直感的。システム2は、遅く、意識的で、論理的。システム1は、エネルギー効率が良い。過去の経験からパターンを学習し、瞬時に判断します。もしすべての判断をシステム2で処理したら、私たちは何もできません。しかし、システム1には限界があります。新しい状況に対応できません。複雑な判断ができません。バイアスに支配されます。対話は、システム1では処理できません。誰かがあなたを批判します。システム1は、瞬時に「攻撃です」と判断します。防御反応が起動します。反論します。言い訳します。相手を攻撃し返します。これはすべて、自動的に起きます。意識する前に、もう言葉が口から出ています。この自動反応が、対話を壊します。中断する力とは、この自動反応を一時停止する力です。システム1からシステム2へ、意識的に切り替える力です。具体的には、どうするでしょうか。まず、自分の反応に気づく。「今、私は防御的になっています」と認識します。この気づきが、自動反応を中断します。次に、一呼吸置く。文字通り、深呼吸します。これは単なる気休めではありません。呼吸は、自律神経系に直接作用します。深く息を吐くことで、交感神経の興奮が抑えられます。そして、問いかけます。「相手は本当に攻撃しているのか」「他の解釈はないか」「今、反応する必要があるのか」。中断する力は、訓練で育つ。最初、反応した後で気づく。「ああ、また自動的に反応してしまいました」。でも、気づくことが第一歩です。繰り返すうちに、反応している最中で気づくようになります。やがて反応する前に気づけます。これは、メタ認知的な筋肉です。使えば使うほど、強くなります。この力があると、対話の質が変わります。相手の言葉を、条件反射的に解釈しない。一度受け止めて、考えます。「この言葉は、どういう意味だろう」「相手は、何を伝えようとしているのだろう」。この思考の間が、誤解を防ぐ。理解する力人間の認識は、一人ひとり異なります。私たちは皆、異なる「認識の枠組み」を持っています。同じ入力に対して、異なる処理をします。異なる出力を生み出す。相手の言葉を理解するとは、その言葉の表面的な意味を把握することではありません。相手がどんな認識の枠組みでその言葉を生成したかを推測することです。「この実装は複雑すぎます」。この言葉を聞いたとき、あなたは何を理解すべきでしょうか。言葉の辞書的な意味ではありません。相手の認識の枠組みを理解すべきです。相手の認識の枠組みは、何で構成されているでしょうか。まず、価値観。相手は何を大切にしているのでしょうか。品質か、速度か、保守性か、パフォーマンスでしょうか。次に、経験。相手はどんな経験をしてきたのでしょうか。どんな失敗から学んだのでしょうか。「この実装は複雑すぎます」と言う人の認識の枠組みを推測しよう。もしかしたら、この人は過去に複雑なコードでデバッグに苦労した経験があるだろう。だから、「複雑さ」は「将来の苦痛」を意味しています。あるいは、この人はシンプルさを美徳とする価値観を持っているだろう。この認識の枠組みを理解せずに、言葉だけに反応してはいけません。理解するための問いには、段階があります。第一層の問い：事実の確認。「この部分が複雑だと感じるのは、どの部分ですか」。具体的にどこを指しているかを特定します。第二層の問い：解釈の探索。「その部分は、どういう問題を引き起こしますか」。相手がどう解釈しているかを明らかにします。第三層の問い：背景の理解。「過去に似た経験がありますか」「なぜそう考えるようになったのですか」。価値観や経験の背景を探ります。相手の認識の枠組みの根源に迫ります。この段階的な問いかけによって、相手の認識の枠組みが少しずつ解像度を上げて見えてきます。「ああ、以前このパターンでバグが多発したんです」「デバッグに一週間かかったことがあって」。この情報が、相手の認識の枠組みを明らかにします。そして、あなたは理解します。「なるほど、この人は『複雑さ』を『デバッグの困難さ』と結びつけて考えているのです」。この理解があれば、応答が変わります。「確かに、この部分は複雑に見えますね。でも、テストを充実させることで、デバッグの困難さは抑えられます」。これは、相手の認識の枠組みを尊重した応答です。「複雑じゃない」と否定するのではなく、「複雑だが、あなたの懸念には対処できます」と提案します。これなら、対話が続きます。理解する力とは、共感することではありません。認識論的な探索です。 相手がどんなプログラムを実行しているかを、逆アセンブルする作業です。表面の出力から、内部のロジックを推測します。この探索は、時間がかかる。でも、この時間を省略してはいけません。理解せずに議論しても、平行線になるだけです。変容する力「あのときの自分なら、絶対にこうは言わなかったな」と感じる瞬間があります。価値観が変わった、というほどドラマチックではない。でも、世界の見え方が微妙にズレている。この「見え方のズレ」こそが、認識の枠組みの変容です。「意見を変えます」と「認識の枠組みを変えます」は、まったく違います。新しい設計思想を学ぶとき、最初は既存の知識を使って理解しようとします。しかし、本当にその考え方を習得するには、認識の枠組みそのものを変える必要があります。「拡張性」という概念を理解するには、単に技術パターンを学ぶだけでなく、ソフトウェアの時間軸についての認識を根本から変える必要があります。「今のコードの美しさ」から「将来の変更の容易さ」へ。視点を変える必要があります。対話においても、同じことが必要になります。相手の言葉を聞いて、「ああ、そういう見方もあるのか」と新しい視点を知ります。それだけでは変容ではありません。その視点を、自分の認識の枠組みに統合します。自分の認識の枠組みを、少し変えます。これは変容です。「以前、複雑さは常に避けるべきだと思っていました。しかし今、必要な複雑さと不要な複雑さを区別すべきです」。この変化が、認識の枠組みの変容です。なぜ認識の枠組みの変容が重要なのでしょうか。それは、表面的な変化は持続しないからだ。誰かに説得されて意見を変えます。その場では納得します。でも、一週間後、元の意見に戻っています。なぜか。認識の枠組みが変わっていないからだ。一方、認識の枠組みが変わると、変化は持続します。いや、「持続する」という表現が正確ではありません。もはや、元に戻るという選択肢がない。 新しい世界の見方を獲得した後、古い見方には戻れません。これは、成長の本質です。10年前の自分と今の自分を比べてみてほしい。意見が変わっただけではないはずです。世界の見方が変わっています。判断の基準が変わっています。これが認識の枠組みの変容です。もし認識の枠組みが変わっていないなら、それは10年間成長していないのです。対話は、この認識の枠組みの変容を可能にします。相手の異なる世界観に触れることで、自分の認識の枠組みを疑う機会が生まれます。「自分の見方は、絶対ではないだろう」と気づく。しかし、認識の枠組みの変容は、容易ではありません。なぜなら自己同一性の問題があるからだ。「私」という感覚は、認識の枠組みによって支えられています。 だから、認識の枠組みを変えることは、ある意味で古い自分を手放すことです。慣れ親しんだ自己イメージから離れ、新しい自己へと移行します。これは、怖い。でも、この手放しこそが、成長です。 古い自分に固執することは、成長を拒否することです。ある意味で、古い自分は死に、新しい自分が生まれる。この変容を恐れない勇気が、対話には必要です。「相手の言葉によって、私は変わるだろう」と覚悟すること。「私の世界観は、絶対ではありません」と認めること。この勇気があって初めて、本当の対話が可能になります。中断する力、理解する力、変容する力。これは対話を可能にする基礎的な能力です。しかし、これらの力を阻む、より深い障害があります。それは、私たちが日々生きている「物語」です。自分について、組織について、世界についての物語。この物語は、私たちを定義すると同時に、私たちを縛り付けます。ナラティヴという牢獄私たちは、物語の中に生きています。朝起きて、鏡を見ます。「私は〇〇です」と言える。この「〇〇」は、物語です。「私は内向的な人間です」「私は論理的に考える人間です」。これはすべて、自分について語る物語です。MBTIなんかはまさしくそうです。四文字のラベルで自分を定義し、そのラベルに沿って行動します。物語が、自己を作り出す。職場で、あるプロジェクトが失敗します。あなたは理由を考えます。「計画が甘かったからだ」「コミュニケーション不足だったからだ」。これも、物語です。起きた出来事を、因果関係で結びつけた説明です。これらの物語を、ナラティヴと呼ぶ。 ナラティヴは、現実そのものではありません。現実の解釈です。でも、私たちはナラティヴを通してしか現実を認識できません。ナラティヴには、3つの層があります。第一の層は、解釈のフレームです。「何を見るか」を決める枠組み。同じコードを見ても、ある人は「保守性」を見ます。別の人は「パフォーマンス」を見ます。第二の層は、正当化の物語です。「なぜそう見るのか」を説明する因果の鎖。「過去にレガシーコードで苦しんだから、保守性を重視する」。経験が、価値観を生み、価値観が、見方を決めます。第三の層は、アイデンティティの核です。「私は誰か」を定義する自己物語。「私は品質にこだわるエンジニアです」。この自己定義が、すべての判断の基盤になります。ナラティヴは、必要です。ナラティヴなしに、私たちは行動できません。何が重要かを決められません。優先順位をつけられません。選択ができません。ナラティヴは、複雑な現実を理解可能なパターンに圧縮します。しかし、ナラティヴは、牢獄にもなります。ナラティヴが固定化すると、新しい情報を受け入れられなくなります。すべてを既存のナラティヴで解釈しようとします。「やっぱりそうでした」ばかりで、「意外でした」がない。これは、学習の停止です。より問題なのは、ナラティヴの防衛化です。ナラティヴを修正しようとする試みを、自己への攻撃と感じます。「あなたの見方は違うだろう」と言われて、「私の経験を否定するのか」と反応します。これは、ナラティヴと自己が同一化しているからだ。対話において、ナラティヴの衝突は避けられません。二人の人間が会えば、2つのナラティヴがぶつかります。問題は、ナラティヴがあることではありません。ナラティヴを絶対化することです。「私の見方が正しい」と考えるとき、あなたはナラティヴを絶対化しています。この態度では、対話は不可能です。対話するとは、ナラティヴの相対性を認めることです。「私の見方は、1つの可能性に過ぎない」と理解すること。「相手の見方も、1つの可能性です」と受け入れること。「もしかしたら、第三の見方があるだろう」と探索すること。ナラティヴの保持的懐疑。 これが、対話の核心です。自分のナラティヴを持ちつつ、それが絶対でないという意識を保つ。相手のナラティヴを尊重し、新しいナラティヴを共創する可能性に開かれている。しかし簡単ではありません。ナラティヴを懐疑することが、自己の確実性を手放すことだからだ。この不確実性に耐える力が、対話には必要です。この態度を保つとき、新しい地平が開けます。ナラティヴを持ちながらも、それに縛られない。1つの見方を持ちながらも、他の見方を排除しない。この柔軟性が、見えなかったものを見えるようにします。対話とは、この新しい地平を開くための冒険です。しかし、ナラティヴの牢獄に閉じ込められたとき、何が起きるでしょうか。個人レベルでは、学習が停止します。成長が止まります。より深刻なのは、組織レベルでの影響です。組織のメンバー一人ひとりが、自分のナラティヴに固執します。「私の見方が正しい」と確信します。他者の見方を受け入れません。この状態では、対話は成立しない。対話なき組織は、どうなるのでしょうか。答えは明確です。緩やかな、だが確実な衰退です。ナラティヴは個人の中だけに存在しているわけではありません。「私はこういう人間だ」という物語と同じように、組織もまた「私たちはこういう会社だ」「うちの部署はこういう役割だ」という物語を持っています。個人のナラティヴが集まり、絡み合い、共有されることで、組織レベルのナラティヴが立ち上がります。そして厄介なことに、この組織ナラティヴもまた、私たちを守りながら、同時に縛ります。個人の対話不全は、組織の対話不全として増幅される。ここからは、視点を個人から組織へと一段スライドさせて、対話の欠如が組織に何をもたらすのかを見ていきます。組織に広がる牢獄視点を個人から組織へと広げよう。なぜなら私たちの多くは、単独で働いているのではなく、組織という集合体の中で対話しているからだ。組織とは何か。表面的には、人々の集まりに見えます。実際には、個々の人間と、その人間同士の相互作用の両方から成り立っています。何か問題が起きたとき、人は、よくわからない抽象的なものに原因を押しつけて思考停止してしまうことがあります。「政府の政策が悪い」「社会の仕組みが悪い」。よくわからないものよりは、具体的な何か—たとえば自分自身の行動—に原因を求めた方が、問題の解決につながる。たとえば、ある施策が推進されようとしているが、その施策について疑問があるから議論したい。誰が推進しているのか教えてほしい。そう尋ねたら、「誰というわけではなくて、組織として進めています」という返答があったとします。しかし、具体的な生身の人間を通さない意思決定など存在しない。「組織として進めています」というのは事実だろうが、そう言うと霧の中を彷徨うような感覚になります。解像度を上げてみれば、誰かが意見を持っていて、誰かが同調して進めているのです。だから、知りたければ、課題を解決したければ、まずは生身の人に働きかけることです。組織の緩やかな衰退対話の欠如は、組織を内側から蝕みます。しかし、組織が成熟するにつれて、ある種の宿命的な問題が生じます。これを構造的無能化と呼びます。構造的無能化とは、組織が思考力と実行力を段階的に喪失し、環境変化に適応できなくなる現象です。これは急激な破綻ではありません。ゆっくりと、気づかれないうちに進行する慢性的な機能不全です。構造的無能化の根本には、ナラティヴの固定化と対話の欠如があります。組織の各メンバーが自分のナラティヴに閉じこもります。部門ごとに異なるナラティヴを持ちます。「営業は数字しか見ていない」「開発は現実を知らない」「経営は現場を理解していない」。これらのナラティヴは、互いを排除し合います。対話は起きません。そして、組織は徐々に機能を失っていきます。なぜ成功が失敗の種となるのか皮肉なことに、成功した組織ほど、この罠にはまりやすい。企業が成功すると、その成功をもたらした方法を固定化しようとします。「この方法でうまくいった」という経験が、標準化とルーティン化を促します。効率を最大化するために、分業を進めます。これは合理的です。しかし、この成功体験は、組織のナラティヴを固定化します。「私たちはこうやって成功した」という物語が、組織のアイデンティティになります。この物語は、誇りの源泉です。同時に、変化への抵抗の源泉でもあります。「なぜ変える必要があるのか。これでうまくいっています」。成功のナラティヴは、新しい情報を拒絶します。異なる意見を排除します。対話を閉ざします。問題は、この効率化が前提としている「環境の安定性」です。市場が変わらず、顧客ニーズが変わらず、技術が変わらなければ、標準化とルーティン化は機能し続けます。しかし、環境は変わります。しかも、成功した企業ほど、その変化に気づきにくい。なぜなら、既存のやり方で「まだ」利益が出ているからです。固定化されたナラティヴは、変化のシグナルを見えなくします。全体を見失う組織効率化の代償として、最初に現れるのが断片化です。断片化とは、組織の各部分が自律的に機能する一方で、全体としての統合性を失う状態です。営業部門は「売上」だけを見ます。開発部門は「機能」だけを見ます。カスタマーサポートは「問い合わせ対応」だけを見ます。誰も「顧客の体験全体」を見ていません。この断片化は、部門ごとのナラティヴの固定化から生まれます。営業は「数字こそ正義です」というナラティヴを持ちます。開発は「技術的品質が最重要です」というナラティヴを持ちます。それぞれのナラティヴは、部門内では共有されています。しかし、部門を超えた対話はありません。異なるナラティヴを持つ者同士が話すとき、それは対話ではなく、対立になります。「私の仕事はここまで」「それはあなたの部署の仕事」。明確な役割分担は、一見すると効率的です。しかし、組織を横断する課題—たとえば「なぜ顧客満足度が下がっているのか」—に対して、誰も答えを持っていない状況が生まれます。断片化した組織では、問題が「部門間の隙間」に落ちます。誰の責任でもない問題は、誰も解決しません。対話がないからです。新しいものを生み出せない組織断片化が進むと、次に訪れるのが不全化です。不全化とは、組織が新しい課題を認識し、新しい解決策を生み出す能力を失うことです。視野が狭くなり、思考が硬直化します。外部の変化—新しい競合の登場、技術革新、顧客ニーズの変化—を捉えられなくなります。なぜこうなるのか。断片化した組織では、各部門が自部門の指標だけを追求します。営業は売上目標、開発は納期、サポートは対応時間。これらの指標を達成することが「仕事」になります。全体最適ではなく、部分最適の連鎖です。新しい事業を生み出すには、部門を横断した協力が必要です。しかし、断片化した組織では、その協力を生み出す仕組みがありません。部門を超えた対話がないからです。各部門が自分たちのナラティヴに閉じこもり、他部門のナラティヴを理解しようとしません。本質を掴めない組織そして最終段階が表層化です。表層化とは、問題認識が表面的になり、根本原因に到達できない状態です。収益が悪化します。離職率が上がります。顧客満足度が下がります。これらの「症状」は見えます。しかし、「なぜそうなっているのか」という本質的な問いに答えられません。なぜ根本原因に到達できないのでしょうか。深い対話がないからです。表層化した組織では、各自が固定化されたナラティヴで問題を解釈します。「これは営業の問題です」「これは開発の問題です」。しかし、誰も「私たちの組織のあり方の問題ではないか」とは問いません。なぜなら、そう問うことは、組織全体のナラティヴ—「私たちはこういう会社です」という自己定義—を疑うことになるからだ。表層化した組織では、対症療法が繰り返されます。「売上が下がった→営業人員を増やそう」「離職率が高い→給与を上げよう」。これらの施策は、表面的な症状には対処しますが、根本原因—組織文化の問題、マネジメントの問題、ビジョンの喪失—には触れません。根本原因に触れるには、深い対話が必要です。しかし、ナラティヴの牢獄に閉じ込められた組織には、その対話ができません。個人の能力ではなく、構造の問題重要なのは、これは個人の能力の問題ではないということです。組織の一人ひとりは、多くの場合、有能です。変革したいという意志もあります。しかし、構造的無能化に巻き込まれることで、個々の能力が発揮できなくなります。個人を責めても、問題は解決しません。構造を変えなければなりません。しかし、構造は人が作り、人が維持していることも事実です。構造を変える責任は、その構造内の人々全員にあります。そして、構造を変えるには、対話が必要です。部門を超えた対話。階層を超えた対話。過去の成功を疑う対話。企業変革という長い道のりどうすればこの悪循環から抜け出せるのでしょうか。企業変革には、4つのプロセスが必要だと考えられます。第一に、全社戦略を考えられるようになること。 断片化した視点から脱却し、全体を見渡す力を取り戻す。第二に、全社戦略へのコンセンサスを形成すること。 組織全体で方向性を共有します。第三に、部門内での変革を推進すること。 各部門で具体的なアクションを起こす。第四に、全社戦略・変革施策をアップデートすること。 実行の中で学び、修正し続けます。このプロセスを阻む困難があります。3つの困難です。「多義性」の困難。 ある状況について複数の解釈が存在していても、その状態を捉えられなくなります。「複雑性」の困難。 ある事象の背後で複数の要因が絡み合い、状況が明確に認識されず、解決策もわかりにくくなります。「自発性」の困難。 変革の方向性を打ち出しても、現場で積極的に実行されなくなります。これらの困難を乗り越える鍵は何か。それは対話だ。企業変革と適応課題：人が変わるということここまでの議論で、「構造的無能化」や「企業変革の4つのプロセス」という、組織レベルの枠組みを見てきました。しかし、どれだけ立派なプロセスを設計しても、それだけで変革が進むわけではありません。なぜなら、変わるのは「組織」そのものではなく、組織の中にいる人間だからです。ここからは視点をもう一段インナーレイヤーに寄せます。企業変革の根っこには、必ず 「適応課題」＝人々の認識の枠組みの変容 が横たわっています。そして、この認識の変容には、「5つの重力」のような困難がまとわりついている。それが何なのかを、1つずつほどいていきます。なぜプロジェクトは失敗するのでしょうか。多くの人は、技術的な問題だと捉えます。設計が悪かった。実装に問題があった。技術的な解決策を探す。しかし、これらの解決策を導入しても、同じ問題が繰り返されます。なぜか。問題が技術的側面だけでなく、適応的側面を持つからだ。適応的側面とは何でしょうか。それは、人々の認識の枠組み、つまりナラティヴの問題です。組織のメンバーが固定化されたナラティヴに閉じこもっています。「品質より速度が重要です」「速度より品質が重要です」。このナラティヴの対立が、技術的な解決策を無効化します。どんなに優れたツールを導入しても、どんなに合理的なプロセスを設計しても、ナラティヴが変わらなければ、問題は解決しません。そして、ナラティヴを変えるには、対話が必要です。技術的問題と適応課題のスペクトラム私は、問題を2つの軸で捉えるようになった。技術的側面と適応的側面です。技術的側面とは、既存の知識と技術で解決できる部分。適応的側面とは、認識の枠組みの変容が必要な部分。重要なのは、これは二者択一ではなく、連続的なスペクトル上に存在するということです。純粋に技術的な問題の例。サーバーのレスポンスが遅い。データベースのクエリを最適化します。キャッシュを導入します。これで解決します。問題は外部にあります。解決策も外部にあります。純粋に適応的な課題の例。チームのコミュニケーションがうまくいかない。誰もが「相手が理解してくれません」と感じています。この問題を「コミュニケーションツールの問題」だと定義すれば、Slackを導入すれば解決するはずです。しかし、実際には解決しない。なぜなら問題の本質はツールではなく、互いの認識の違いにあるからだ。ただし、現実の問題の多くは、両方の側面を持っています。たとえば「技術的負債が増え続けています」という問題。一見技術的に見えますが、「品質と速度のどちらを優先するか」という価値観の問題、「リファクタリングに時間を使うことを許容するか」という組織文化の問題といった適応的側面も含みます。問題を見誤る典型的なパターンは、適応的側面を持つ問題に技術的解決策だけを当てはめることです。「ツールを導入したのに、なぜうまくいかないんだろう」。ツールだけが問題なのではなく、認識や関係性も問題なのだと気づかない。多くの組織の問題は、適応課題です。「イノベーションが生まれません」。これは、予算の問題でも、人材の問題でもない。リスクを取ることを恐れる文化の問題です。失敗を許容しない価値観の問題です。これを変えるには、組織の認識を変える必要があります。適応課題における変容の困難さ適応課題に直面したとき、人間は変化に時間を要します。なぜなら変化することは、一部の自分を失うことだからだ。 長年培ってきた考え方。慣れ親しんだ行動パターン。自分を定義してきた価値観。これらを手放すことは、怖い。言い換えれば、自分のナラティヴを手放すことです。「私はこういう人間です」という自己物語。「私たちはこういう組織です」という集団物語。これらのナラティヴは、アイデンティティの核です。だから、適応課題は、感情的な反応を伴う。論理的に説明しても、すぐには納得しない。データを示しても、即座には受け入れません。これは、頑固なのではありません。恐怖なのだ。この恐怖を乗り越えるには、何が必要でしょうか。対話です。一方的な説得ではありません。命令でもありません。対話を通じて、自分のナラティヴを相対化します。「私の見方は、絶対ではないだろう」と気づきます。他者のナラティヴに触れます。「そういう見方もあるのか」と理解します。そして、徐々に、自分のナラティヴを更新していきます。この変容は、対話なしには起きません。適応課題における5つの変容の困難良いアイデアを提示すれば、人は変わるだ。しかし、現実には変わりません。なぜか。変化には、5つの困難があるからです。この困難は、高くそびえ立つ障害物ではありません。むしろ、重力のように働きます。目には見えませんが、常に働いています。私たちを、元の場所に引き戻そうとします。どんなに優れたアイデアでも、この5つの困難を越えられなければ、人は変わりません。この5つの困難は、独立して存在するのではありません。互いに影響し合い、変化を阻む仕組みを形成しています。1つの困難を越えても、次の困難が待っています。5つすべてを理解しなければ、変化は起きません。第一の困難：頭の作り変え毎朝、同じ道を通って会社に行きます。信号の位置を覚えています。どこで曲がるか、体が覚えています。考えなくても、着きます。これが、慣れです。仕事も同じです。20年、30年かけて、物事の見方を学んできました。「こういう問題には、こう対処する」。瞬時に判断できます。考えなくても、答えが出ます。この慣れが、あなたの強みです。経験と呼ばれるものです。新しい考え方を受け入れるとは、この慣れた道を捨てることです。新しい道を覚え直すことです。でも、新しい道では迷います。間違えます。時間がかかります。だから、脳は嫌がります。「複雑すぎる」「よく分からない」。怠惰ではありません。効率を求める本能です。この困難を越えるには、いきなり全部の道を変えようとしてはいけません。「いつもの道の、この角を少し変えてみよう」。一部だけ変えます。慣れたら、また一部変えます。「あなたがやってきたことは、間違いではありません。ちょっと拡張するだけです」。こう言われると、安心します。第二の困難：暗闇への恐怖夜、真っ暗な部屋を歩くとき、あなたは慎重になります。手を前に伸ばします。障害物を探ります。何かにぶつからないか、不安です。明かりをつければ、普通に歩けます。でも、暗闇では怖い。新しい方針、新しいやり方。これは、暗闇を歩くようなものです。「うまくいくのか」「失敗したらどうなるのか」。答えが見えません。過去の経験も役に立ちません。予測ができません。だから、体が固くなります。心臓がドキドキします。頭が真っ白になります。だから、ここでは「全部を明るくしよう」としないことが大事になります。小さな懐中電灯で、一歩先だけ照らす。「まず、この小さな範囲で試そう」。失敗しても、被害は小さい。成功すれば、次の一歩が見えます。その繰り返し以外に、暗闇を抜ける方法はありません。第三の困難：自分の定義を変える痛み10年間、営業として働いてきました。顧客と話すのが好きです。契約が取れたときの達成感。売上目標を達成したときの誇り。これらが、あなたです。名刺には「営業部」と書いてあります。自己紹介するとき、「営業をやっています」と言います。あなたは、営業です。「これからはマネジメント職に」。この言葉を聞いたとき、何を感じるでしょうか。「私は営業じゃなくなるのか」。不安です。10年間、営業として生きてきました。営業の自分しか、知りません。営業じゃない自分は、誰なのでしょうか。自分が分からなくなります。この困難を越えるには、「営業を辞める」ではなく、「営業の経験を活かす」だ。「営業の経験は無くなりません。それは基盤です。その上に、新しいスキルを積み上げます」。こう言われると、自分は消えないと分かります。過去は捨てません。未来につながります。第四の困難：体に染みついた癖毎朝、目覚ましが鳴ります。あなたは無意識にスマホを取ります。メールをチェックします。考えていません。体が勝手に動きます。これが、習慣です。仕事でも同じです。資料を作るとき、いつものテンプレートを使います。会議の進め方も、いつも同じです。使い慣れたツール。決まった手順。考えなくても、できます。楽です。新しいやり方は、違います。毎回考えなければなりません。どうするんだっけ、と迷います。間違えます。遅くなります。疲れます。だから、体は元のやり方に戻ろうとします。「やっぱり、いつものやり方の方が早い」。そう感じます。この困難を越えるには、最初の遅さを許します。「新しいやり方は、最初は遅いです。でも、一ヶ月後には速くなります」。この移行期間を、我慢します。組織として、支援します。第五の困難：自分で決めたい気持ち子供の頃、親に「これを食べなさい」と言われました。嫌でした。でも、「何が食べたい」と聞かれて、同じものを選んだとき、喜んで食べました。人間は、自分で決めたいのです。職場でも同じです。上司が「この方法でやりなさい」と命令します。あなたは、反発します。たとえそれが良い方法でも、押し付けられると嫌です。なぜか。自分で決めていないからです。この困難を越えるには、命令ではなく、提案します。「こういう選択肢があります。どう考えますか」。相手を、意思決定に参加させます。「一緒に考えましょう」。相手が自分で気づき、自分で選びます。そのとき、反発は消えます。同じ結論でも、自分で選んだら、納得します。この5つの困難は、別々に立っているのではありません。連動しています。第一の困難を越えて、新しい考え方を理解しても、第二の困難の不安が残ります。第三の困難の「自分が分からなくなる」恐怖も待っています。第四の困難の習慣の引力が、あなたを元に戻そうとします。そして、第五の困難。自分で決めていないと感じれば、すべてが無駄になります。だから、変革を推進する者は、5つすべてを理解しなければなりません。1つだけ対処しても、他の困難が残ります。人は変わりません。5つすべてに、丁寧に向き合う必要があります。これが、変容のメカニズムです。ここまで見てきた「5つの困難」は、私たちが変わろうとするときに働く重力でした。頭の作り変えへの抵抗、暗闇への恐怖、自己定義の揺らぎ、体に染みついた癖、そして自分で決めたいという欲求。では、この重力に抗いながら、どうやって認識の枠組みを変えていけばいいのか。そこで必要となる具体的なプロセスこそが、対話です。対話は、単なる話し合いの技術ではなく、認識の変容を起こすための手順そのものです。ここからは、対話がどのような段階を経て認識を変えていくのかを、「4つの段階」として見ていきます。対話による変容の促進対話が必要なのは、まさにこの適応課題においてです。技術的問題なら、専門家が答えを出せばいい。でも、適応課題は、当事者全員が変わらなければ解決しない。変わるためには、まず現在の認識、つまり固定化されたナラティヴを可視化しなければならない。ここで大事なのは、変化を強制できないと理解しておくことです。 説得しようとすればするほど、心理的反発が強まる。ナラティヴを否定されることは、自己を否定されることだからだ。だから、対話が必要になります。対話とは、相手を説得する行為ではありません。相手が自分自身を説得できるよう手助けする行為です。「私たちは、なぜこのパターンを繰り返しているのか」。「私たちは、どんな前提で動いているのか」。これらの問いに向き合うこと。これは、組織のナラティヴを問い直す作業です。対話を通じて、集団のナラティヴが可視化されます。「ああ、私たちはリスクを避けることを最優先にしてきたのです」。この気づきが、変化の第一歩になります。そして、対話を通じて、新しいナラティヴが創発します。「リスクを取らないことも、リスクではないか」。互いの視点を統合することで、誰も一人では到達できなかった地平が開けます。これが、ナラティヴの牢獄から抜け出す唯一の道です。適応課題は、対話なしには解決しない。 命令では解決しない。説得では解決しない。強制では解決しない。なぜなら解決には、当事者全員の認識の変容が必要だからだ。認識の変容は、対話を通じてのみ起きます。対話のプロセス：認識の変容の四段階対話は、どのように起きるのでしょうか。どのようなプロセスを経て、認識は変容するのでしょうか。多くの対話のフレームワークは、行動のステップを示す。傾聴します。質問します。要約します。しかし、これは表面的です。本当の対話は、もっと深い層で起きています。認識の変容の層で。対話のプロセスを4つの段階として捉え直してみたい。第一段階：自己の相対化対話が始まる前、私たちは自分の認識を絶対視しています。「世界はこうです」と思っています。正確には、「私が見ている世界」と「世界そのもの」を区別していない。第一段階は、この区別に気づくことです。「私が見ているのは、世界の一つの側面に過ぎない」と認識すること。 これを、自己の相対化と呼ぶ。どうやって相対化が起きるのでしょうか。最も効果的なのは、自分とまったく違う視点に出会うことです。同じ状況を見ているのに、相手はまったく違う解釈をしています。この衝突が、相対化のきっかけになります。「この設計は複雑すぎます」と思っていました。しかし、相手は「この設計は適切な抽象化です」と言います。最初は「相手が間違っています」と感じます。ところが、相手の説明を聞いているうちに、何かがひっかかる。「もしかして、私が見ていないものを、この人は見ているのだろう」。この瞬間、相対化が始まります。「私はこう見ています。でも、世界はもっと複雑だろう」。この距離感が、対話の始まりになります。自己の相対化は、謙虚さを生む。「私は確信していない」と認めることができるようになります。「私の見方は、1つの可能性に過ぎない」と受け入れることができるようになります。この謙虚さがなければ、対話は始まらない。第二段階：他者の世界への接近自己を相対化したとき、他者の世界が見えてきます。相手もまた、1つの認識の体系を持っています。相手の言葉は、その体系から生成されています。第二段階は、この相手の認識の体系に近づくことです。相手の世界を、内側から理解しようと試みること。 これを、他者の世界への接近と呼ぶ。接近するとは、相手の前提を探ることです。「なぜそう考えるのですか」と問う。「どういう経験から、その結論に至ったのですか」と尋ねます。相手の認識の枠組みを、少しずつ解読していく。「この設計は適切な抽象化です」と言う相手。なぜそう考えるのでしょうか。相手に聞いてみます。すると、相手は過去のプロジェクトの話をします。要件が頻繁に変わるプロジェクトでした。柔軟な設計にしていたおかげで、変更に対応できました。その経験から、「抽象化は投資です」という信念が生まれた。この話を聞いて、あなたは理解します。「ああ、この人は『抽象化』を『変更への備え』として見ているのです」。一方、あなたは「抽象化」を「複雑さの源」として見ていました。同じ言葉、違う意味。この差異が、可視化されます。接近は、共感とは違います。共感は、感情的な同調です。しかし、接近は、認識論的な理解です。「あなたの認識の構造が分かる」。感情は一致しなくても、認識は理解できます。接近することで、相手の言葉の真意が分かります。対立が和らぐ。「この人は私を攻撃しているわけではない。ただ、違う視点から見ているだけです」。第三段階：差異の構造化自分の世界と相手の世界を理解したとき、次の段階が来る。2つの世界の違いを、明確に構造化することです。第三段階は、差異を整理し、パターンを見出すこと。 これを、差異の構造化と呼ぶ。構造化とは、「何が違うのか」を言語化することです。漠然と「意見が違う」ではなく、「どこが、なぜ、違うのか」を明確にします。あなたと相手の対立を、構造化しよう。まず、事実の層では一致しています。「このコードは複数の抽象レイヤーを持っています」。これは、どちらも認めます。次に、解釈の層で分かれます。あなたは「複数の抽象レイヤーは、理解を困難にする」と解釈します。相手は「複数の抽象レイヤーは、変更を容易にする」と解釈します。そして、価値観の層でも分かれます。あなたは「即座の理解可能性」を重視します。相手は「長期的な柔軟性」を重視します。さらに、経験の層でも違います。あなたは過去に複雑なコードで苦労しました。相手は過去に硬直的な設計で苦労しました。この構造化によって、対立の本質が見えます。これは、技術的な議論ではなかった。価値観の対立でした。 どちらの価値観も正しい。でも、優先順位が違います。その優先順位の違いは、異なる経験から生まれています。構造化すると、対立が外在化されます。「AとBの対立」ではなく、「即座の理解可能性 vs 長期的な柔軟性」という構造の問題になります。人格の対立から、構造の対立へ。これは対話を生産的にします。第四段階：統合への創発そして最後の段階。2つの世界観を統合する、新しい視点が創発します。第四段階は、どちらの視点も含みつつ、どちらでもない第三の地平を見出すこと。 これを、統合への創発と呼ぶ。統合は、妥協ではありません。妥協とは、両者が譲り合って中間点を取ることです。これは取引です。統合とは、より高次の視点を見出すことです。AかBかではなく、AとBを包含するCを創造することです。あなたと相手の対立に戻ろう。即座の理解可能性 vs 長期的な柔軟性。どちらも大切です。では、どうするでしょうか。問いを変えます。「どちらを選ぶか」ではなく、「どちらも実現する方法はないか」と。この問いが、創発を促す。議論を続けるうちに、アイデアが生まれます。「コア部分は抽象化します。でも、抽象化のレイヤーは最小限にします。各レイヤーの責務を明確にドキュメント化します。さらに、具体的な使用例をテストコードで示す」。この解決策は、あなたの懸念に応えています。ドキュメントとテストによって、理解可能性が保たれます。同時に、相手の懸念にも応えています。抽象化によって、柔軟性が保たれます。これが統合です。 どちらの視点も否定せず、両方を満たす新しい解を見出す。この解は、対話の前には存在しなかった。あなた一人では到達できなかった。相手一人でも到達できなかった。2つの視点が出会い、対話を通じて、創発しました。統合への創発は、対話の究極の目標です。しかし、必ずしも達成されるとは限らない。時には、差異の構造化で終わることもあります。それでもいい。統合できなくても、理解は深まっています。論破という暴力対話の対極にあるものについて語ろう。論破です。論破とは、相手を言い負かすことです。相手の主張の矛盾を指摘します。相手の論理の欠陥を突く。相手を沈黙させます。「勝ちました」と感じます。なぜ人は論破したがるのでしょうか。それは、即座の快楽があるからだ。相手を打ち負かす瞬間、ドーパミンが放出されます。優越感を感じます。自己肯定感が高まる。この快楽が、論破を強化します。論破は、対話を殺す。 いや、対話を殺すだけではありません。関係を壊します。信頼を失います。学習機会を逃す。最終的には、自分自身を孤立させます。論破された相手は、何を感じるでしょうか。屈辱です。「自分は間違っていました」という敗北感。「この人とは、もう話したくない」という拒絶。論破によって、あなたは1つの議論には勝っただろう。しかし、相手との対話の可能性を永久に失いました。より悪いことに、論破は自分自身の成長も止めます。なぜなら論破する人は、相手から学ぶ機会を放棄しているからだ。相手の視点を理解しようとしない。相手の経験から学ぼうとしない。ただ、相手の間違いを見つけることに集中します。論破に依存すると、世界が狭くなります。対話可能な相手が減っていく。人々は、あなたを避けるようになります。あなたは孤立します。対話と論破の違いは何か。目的が違います。論破の目的は、勝利です。相手を打ち負かすこと。一方、対話の目的は、相互理解です。共に学ぶこと。姿勢が違います。論破する人は、相手を敵と見ます。対話する人は、相手をパートナーと見ます。結果が違います。論破の後には、勝者と敗者が残ります。対話の後には、両者の成長が残ります。もしあなたが「正しさ」を証明したいなら、論破すればいい。しかし、もしあなたが「真実」に近づきたいなら、対話しなければならない。なぜなら真実は一人の人間の視点に収まらないからだ。真実は複数の視点の交差点にあります。論破から対話へのシフトは、パラダイムの転換です。ゼロサムゲームから、ポジティブサムゲームへ。思考の終わりから、思考の始まりへ。自己の強化から、自己の拡張へ。このシフトには、勇気が要る。「勝つ」という快楽を手放す勇気。「正しい」という確信を疑う勇気。「変わる」という可能性を受け入れる勇気。この勇気こそが、成長の源です。AI時代における対話の価値生成AIが登場して、私たちの仕事は変わりました。コードを書く速度が上がりました。ドキュメントを作成する時間が減りました。質問に対する答えが、即座に返ってくるようになった。人間同士の対話は、不要になったのでしょうか。AIに質問すれば答えが返ってきます。AIと議論すれば、論理的な反論が返ってきます。人間と対話する必要が、あるのでしょうか。あります。 それも、これまで以上に。なぜならAIとの対話と人間との対話は、現時点では本質的に異なる性質を持つからだ。AIとの対話と人間との対話の違いは、以下の軸で捉えられます。経験の固有性。AI：訓練データのパターンから応答を生成します。人間：固有の人生経験から応答が生まれます。この違いは、応答の予測可能性に影響します。AIの応答は洗練されていますが、パターンの組み合わせです。人間の応答は、データのパターンでは予測できない個別性を持ちます。相互的変容の有無。AI：対話によって自身の認識の枠組みは変わりません(現時点)。人間：対話によって互いの認識が変容しうる。AIに話を聞いてもらっても、「理解された」という実感は限定的です。なぜならAIには「あなたの話が私の認識を変えた」という相互的な影響がないからだ。一方、人間同士では「あなたの話を聞いて、私は何かを感じました」という実存的な承認が生まれます。関係性の蓄積。AI：各セッションは独立しています。人間：対話の履歴が信頼や理解の基盤となります。2つの異なる人生経験が衝突し、融合し、まったく新しい視点が創発します。この過程は、関係性の深まりを前提とします。実存的リスク。AI：どんな意見を言っても関係性にリスクはありません。人間：意見の衝突が関係性を損なう可能性があります。否定されると、傷つく。この摩擦が、人間との対話を難しくします。しかし、この摩擦の中にこそ、成長があります。重要なのは、AIと人間のどちらが優れているかではありません。それぞれの特性を理解し、目的に応じて使い分けることです。情報の整理、アイデアの初期生成、論理のチェックなどはAIが効率的です。一方、認識の変容、実存的な対話、信頼関係の構築などは、人間同士の対話が適しています。しかし、ここにリスクもあります。AIとの対話は、楽です。予測可能です。抵抗がない。反論されても、傷つかない。一方、人間との対話は、難しい。予測不可能です。摩擦があります。否定されると、傷つく。だから、私たちはAIとの対話に逃げる危険があります。人間との対話を避け、AIとだけ話すようになります。これは、対話筋力の退化です。AI時代だからこそ、意識的に人間と対話しなければならない。 不快でも、難しくても、予測不可能でも。なぜなら、その摩擦の中にこそ、成長があります。創造があります。人間性があります。おわりに会議室の二人は、まだ平行線でした。Aは「拡張性」を主張し続けます。Bは「納期」を主張し続けます。どちらも譲らない。どちらも、相手を理解しようとしない。私は、口を開きました。「すみません、確認したいのですが。Aさんが『拡張性』と言うとき、具体的にどんなリスクを心配しているんですか」Aは少し驚いた顔で答えた。「前のプロジェクト、機能追加のたびに大規模な修正を要し、半年間リリース停止になったんです。だから...」「なるほど。では、Bさんが『納期』を強調するのは、どういう背景があるのですか」Bも答えた。「顧客との契約で、この機能のリリース日が明示されていて。遅れると、ペナルティが発生するんです」沈黙が流れた。Aが言いました。「契約の話、知りませんでした。それなら、確かに納期は守らないといけないですね」Bも言いました。「前のプロジェクトでそんなことがあったんですね。それは大変でしたね。じゃあ、最小限の拡張性を確保する方法、一緒に考えてほしいです」会議室の空気が、少し変わりました。対立から、対話へ。対話は、魔法ではありません。 すべての問題を解決するわけではありません。意見の対立が消えるわけでもない。でも、対話があれば、前に進めます。互いを理解しながら、解を探せます。私たちの多くは、対話の仕方を教わっていない。学校でも、職場でも。だから、本能的に反応します。防御します。攻撃します。関係が壊れていく。でも、対話は学べます。 訓練できます。一歩ずつ、積み重ねられます。完璧である必要はない。不完全な対話でも、無対話よりはるかにましです。まず、自分の自動反応を中断すること。「今、私は防御的になっています」と気づくこと。一呼吸置くこと。次に、相手の世界を理解しようとすること。「なぜそう考えるのか」と問うこと。相手の背景、経験、価値観を探ること。そして、自分のナラティヴから降りること。「私の見方は、絶対ではありません」と認めること。新しい視点に開かれていること。対話は、時間がかかる。効率的ではありません。しかし持続可能です。 対話を通じて築かれた理解は、表面的な合意よりもはるかに強い。対話を通じて生まれた解は、押し付けられた解よりもはるかに実行可能です。そして、対話は、私たち自身を変えます。相手の視点に触れることで、自分の認識が広がる。自分の限界に気づく。新しい可能性が見えます。対話は、自己を拡張する行為です。技術だけでは、組織は動かない。プロセスだけでは、イノベーションは生まれません。ツールだけでは、問題は解決しない。必要なのは、人と人との対話です。異なる世界観が出会い、衝突し、融合する場です。エンジニアとして、私たちは論理を重視します。データを重視します。効率を重視します。これは大切です。しかし、それだけでは足りない。人間の認識の複雑さ、関係性の重要性、対話の力。 これらを理解しなければ、どんなに優れた技術も、組織の中で機能しない。対話は完成しない。永遠に未完成です。でも、試み続けることができます。 その試みの一歩一歩が、こじれた現場に、小さな橋を架けていく。あなたの次の一歩は、何か。今日、誰と対話するでしょうか。その対話の中で、あなたはどう変わるでしょうか。答えは、対話の中にあります。参考文献対話の実践力: ケアを極める聞き方・話し方作者:小瀬古伸幸中央法規出版Amazon学びをつくる問いと対話のデザイン: 探究・研修・大人の学び作者:福島 創太学文社Amazon優れたリーダーはなぜ、対話力を磨くのか？作者:堀井悠,松本悠幹クロスメディア・パブリッシング(インプレス)Amazonダイアローグ――対立から共生へ、議論から対話へ作者:デヴィッド・ボーム英治出版Amazon問いの編集力 思考の「はじまり」を探究する作者:安藤昭子ディスカヴァー・トゥエンティワンAmazon私たちはどう学んでいるのか　――創発から見る認知の変化 (ちくまプリマー新書)作者:鈴木宏昭筑摩書房Amazon創造と創発の心理学〈下〉: 越境がもたらす癒しと変容作者:吉野 大輔学文社Amazon創造と創発の心理学〈上〉: つながりがもたらす新たな秩序作者:吉野 大輔学文社Amazon「良い質問」を40年磨き続けた対話のプロがたどり着いた 「なぜ」と聞かない質問術作者:中田 豊一ダイヤモンド社Amazon「変化を嫌う人」を動かす:魅力的な提案が受け入れられない4つの理由作者:ロレン・ノードグレン,デイヴィッド・ションタル,船木 謙一(監修)草思社Amazon他者と働く──「わかりあえなさ」から始める組織論 (NewsPicksパブリッシング)作者:宇田川元一ニューズピックスAmazon企業変革のジレンマ 「構造的無能化」はなぜ起きるのか作者:宇田川元一日経BPAmazon私文ホワイトカラーが AI・コンサルに仕事を奪われない働き方戦略作者:株式会社板橋　東京中央支店かんき出版Amazonだから僕たちは、組織を変えていける ――やる気に満ちた「やさしいチーム」のつくりかた【ビジネス書グランプリ2023「マネジメント部門賞」受賞！】作者:斉藤徹クロスメディア・パブリッシング（インプレス）AmazonDD(どっちもどっち)論 「解決できない問題」には理由がある (WPB eBooks)作者:橘玲集英社AmazonHigh Conflict よい対立 悪い対立 世界を二極化させないために作者:アマンダ・リプリーディスカヴァー・トゥエンティワンAmazon「わかりあえない」を越える――目の前のつながりから、共に未来をつくるコミュニケーション・NVC作者:マーシャル・B・ローゼンバーグ海士の風Amazon有と無: 見え方の違いで対立する二つの世界観作者:細谷功株式会社dZEROAmazon「無理」の構造　この世の理不尽さを可視化する作者:細谷功株式会社dZEROAmazonはじめての人類学: 講談社現代新書作者:奥野 克巳AudibleAmazon文化人類学入門（増補改訂版） (中公新書)作者:祖父江孝男中央公論新社Amazonアイデア資本主義 文化人類学者が読み解く資本主義のフロンティア作者:大川内 直子AudibleAmazon","isoDate":"2025-11-19T10:48:09.000Z","dateMiliSeconds":1763549289000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"バイブコーディングと継続的デプロイメント","link":"https://speakerdeck.com/nwiizo/baibukodeingutoji-sok-de-depuroimento","contentSnippet":"2025年9月30日（火）、「バイブコーディングもくもく会 #03」というイベントで登壇することになった。\rhttps://aimokumoku.connpass.com/event/368935/\r\r正直に言うと、このイベントがどんな空気感なのか、まだ全然掴めていない。ゆるい感じなのか、ガチな感じなのか。笑いを取りに行くべきなのか、真面目にやるべきなのか。そういう「場の空気」みたいなものが事前に分からないのは、けっこう怖い。だから、とりあえず色々なパターンを想定して準備している。要するに、どんな状況になっても対応できるように、という保険をかけまくっているのだ。我ながら、慎重すぎるかもしれない。\r\rブログとGithubはこちら。\rhttps://syu-m-5151.hatenablog.com/\rhttps://github.com/nwiizo\r\r一応、置いておく。見られるのは恥ずかしいけど、見られないのも寂しい。そういう矛盾した感情を抱えながら、当日を迎えることになりそうだ。Marp の資料はこちらです。\rhttps://github.com/nwiizo/3shake-marp-templates/blob/main/slides/2025/vibe-coding-continuous-deployment.md","isoDate":"2025-09-30T04:00:00.000Z","dateMiliSeconds":1759204800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Webアプリケーションにオブザーバビリティを実装するRust入門ガイド","link":"https://speakerdeck.com/nwiizo/webapurikesiyonniobuzababiriteiwoshi-zhuang-sururustru-men-gaido","contentSnippet":"2025年9月10日（水）、「Rustの現場に学ぶ〜Webアプリの裏側からOS、人工衛星まで〜」というイベントで登壇させていただきます。\r\rhttps://findy.connpass.com/event/359456/\r\r他の登壇者の話が聞きたすぎるけど調整能力の圧倒的な不足で登壇したらすぐに帰らなければなりません。\r\r今回の発表内容のベースとなったのはこちらのブログです。\r- 「RustのWebアプリケーションにオブザーバビリティを実装するインフラエンジニアのための入門ガイド」","isoDate":"2025-09-10T04:00:00.000Z","dateMiliSeconds":1757476800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2025年夏 コーディングエージェントを統べる者","link":"https://speakerdeck.com/nwiizo/2025nian-xia-kodeinguezientowotong-beruzhe","contentSnippet":"2025年9月5日（金）、台風接近という悪天候の中でしたが、「CNCJ: コーディングエージェント × セキュリティ ミートアップ」に登壇させていただきました。\r\r天候の影響で現地参加が難しい方も多い中、オンラインでの参加や配信により、多くの方にお聞きいただくことができました。\r\r### 📍 イベント情報\r- 開催日: 2025年9月5日（金）\r- イベント詳細: CNCFコミュニティページ\r\r### 📹 録画・資料公開予定\r- 録画: CNCJのYouTubeチャンネルにて後日公開予定\r- 発表資料: Connpassページに掲載予定\r\r### 📝 関連ブログ\r今回の発表内容のベースとなった考え方については、こちらのブログ記事でも詳しく解説しています：\r- 「2025年夏 AIエージェントシステムに対する考え方」\r\r台風の中、ご参加・ご視聴いただいた皆様、ありがとうございました。","isoDate":"2025-09-05T04:00:00.000Z","dateMiliSeconds":1757044800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"転職したらAWS MCPサーバーだった件","link":"https://speakerdeck.com/nwiizo/zhuan-zhi-sitaraaws-mcpsabadatutajian","contentSnippet":"「 転職したらMCPサーバーだった件」というタイトルで登壇したことがある。本日は「JAWS-UG SRE支部 #13 つよつよSREの秘伝のタレ」というなんとなく強そうなイベントで登壇しました。\r\r🔍 イベント詳細:\r- イベント名: JAWS-UG SRE支部 #13 つよつよSREの秘伝のタレ\r- 公式URL: https://jawsug-sre.connpass.com/event/358781/\r- ハッシュタグ: https://x.com/search?q=%23jawsug_sre\u0026f=live\r- 参考資料①: https://speakerdeck.com/nwiizo/zhuan-zhi-sitaramcpsabadatutajian","isoDate":"2025-07-23T04:00:00.000Z","dateMiliSeconds":1753243200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"生成AIで小説を書くためにプロンプトの制約や原則について学ぶ / prompt-engineering-for-ai-fiction ","link":"https://speakerdeck.com/nwiizo/prompt-engineering-for-ai-fiction","contentSnippet":"諸君、聞かれよ。本日、私は「女オタ生成AIハッカソン2025夏東京」なる前代未聞の催しにて、生まれて初めて登壇することと相成った。かつての私は純朴なプログラマーであり、「変数名を30分悩んだ挙句、結局tmpにする」という、実に平凡な悩みを抱える程度の技術者であったのだ。\r\r歳月は容赦なく流れ、今や私はプロンプトエンジニアリングという名の魔境に足を踏み入れた哀れな求道者となり果てた。昨夜も丑三つ時まで、私は薄暗い書斎でディスプレイの冷たき光に照らされながら、「なぜ生成AIは『簡潔に』と百回唱えても、源氏物語の長文を生成するのか」という哲学的難題と格闘していたのである。\r\r30分という持ち時間に対し50枚のスライドを用意するという、まるで賽の河原で石を積む如き徒労に及んでいる。そのうち半分は「プロンプトという名の現代呪術における失敗例集」と題した、私の苦悩の結晶である。ああ、AIとの対話とは、かくも人間の正気を奪うものなのか。\r\r---\r\rブログも書いた。\r生成AIで物語を書くためにプロンプトの制約や原則について学ぶ、という話をしてきました #女オタ生成AI部\rhttps://syu-m-5151.hatenablog.com/entry/2025/06/30/171149","isoDate":"2025-06-29T04:00:00.000Z","dateMiliSeconds":1751169600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Claude Code どこまでも/ Claude Code Everywhere","link":"https://speakerdeck.com/nwiizo/claude-everywhere","contentSnippet":"僕がClaude Codeに初めて触れたのは、2025年の春だった。生成AIにはすでに慣れ親しんでいた。流行に乗り遅れてはいけないと必死に勉強し、エディターの補完機能やコード生成ツールとして日常的に活用していた。ただ、当時の僕にとってそれはまだ「CLIで動く便利なコーディング支援ツール」程度の認識でしかなかった。「AIが90%のコードを自動生成」という謳い文句を見ても、半信半疑でターミナルを開いたのを覚えている。\r\rイベント名:【オフライン開催】KAGのLT会 #6 〜御社のエンジニア育成どうしてる!? スペシャル〜\r公式URL: https://kddi-agile.connpass.com/event/357862/\r\r「実装」から「設計」へのパラダイムシフト というより無限に体力が必要という話をした \rhttps://syu-m-5151.hatenablog.com/entry/2025/06/19/102529\r\r【参考文献】\r  - 公式ドキュメント\r    - Claude Code 公式サイト https://www.anthropic.com/claude-code\r    - Claude Code ドキュメント https://docs.anthropic.com/en/docs/claude-code/overview\r    - Claude Code Best Practices https://www.anthropic.com/engineering/claude-code-best-practices\r    - 抽象化をするということ - 具体と抽象の往復を身につける https://speakerdeck.com/soudai/abstraction-and-concretization\r    - How I Use Claude Code https://spiess.dev/blog/how-i-use-claude-code\r    - LLMの制約を味方にする開発術 https://zenn.dev/hidenorigoto/articles/38b22a2ccbeac6\r    - Claude Code版Orchestratorで複雑なタスクをステップ実行する https://zenn.dev/mizchi/articles/claude-code-orchestrator\r    - Agentic Coding Recommendations https://lucumr.pocoo.org/2025/6/12/agentic-coding/\r    - Claude Codeに保守しやすいコードを書いてもらうための事前準備 https://www.memory-lovers.blog/entry/2025/06/12/074355\r    - Claude Codeによる技術的特異点を見届けろ https://zenn.dev/mizchi/articles/claude-code-singularity-point","isoDate":"2025-06-18T04:00:00.000Z","dateMiliSeconds":1750219200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"転職したらMCPサーバーだった件","link":"https://speakerdeck.com/nwiizo/zhuan-zhi-sitaramcpsabadatutajian","contentSnippet":"本日、Forkwell さんに悪ふざけに付き合ってもらってイベントやりました。ありがとうございます。「転職したらMCPサーバーだった件」 🎵🧭 というタイトルで登壇しました！\r\r🔍 イベント詳細:\r- イベント名: 転職したらMCPサーバーだった件\r- 公式URL: https://forkwell.connpass.com/event/354289/\r- ハッシュタグ: https://x.com/search?q=%23Forkwell_MCP\u0026f=live\r- 参考資料①: https://speakerdeck.com/nwiizo/kokohamcpnoye-ming-kemae\r- 参考資料②: https://syu-m-5151.hatenablog.com/entry/2025/03/09/020057\r- 参考資料③: https://speakerdeck.com/superbrothers/that-time-i-changed-jobs-as-a-kubernetes","isoDate":"2025-05-15T04:00:00.000Z","dateMiliSeconds":1747281600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"ここはMCPの夜明けまえ","link":"https://speakerdeck.com/nwiizo/kokohamcpnoye-ming-kemae","contentSnippet":"本日、「AI駆動開発実践の手引き -これが僕/私のAI（アイ）棒」というイベントで「ここはMCPの夜明けまえ」 🎵🧭 というタイトルで登壇しました！\r\r🔍 イベント詳細:\r- イベント名: 【ハイブリッド開催】AI駆動開発実践の手引き -これが僕/私のAI（アイ）棒-\r- 公式URL: https://hack-at-delta.connpass.com/event/350588/\r\r📝 登壇ブログ\r- 2025年4月、AIとクラウドネイティブの交差点で語った2日間の記録 #CNDS2025 #hack_at_delta\r- https://syu-m-5151.hatenablog.com/entry/2025/04/24/113500","isoDate":"2025-04-23T04:00:00.000Z","dateMiliSeconds":1745380800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"生成AIによるCloud Native基盤構築の可能性と実践的ガードレールの敷設について","link":"https://speakerdeck.com/nwiizo/sheng-cheng-ainiyorucloud-native-ji-pan-gou-zhu-noke-neng-xing-toshi-jian-de-gadorerunofu-she-nituite","contentSnippet":"こんにちは皆さん！本日はCloud Native Daysのプレイベントで登壇させていただきます。2019年以来の登壇となりますが、当時はまだ肩こりなんて無縁だったんですよね…。\r\r時の流れは容赦ないもので、最近の肩こりが辛くて昨日も整骨院に通ってきました。30分の持ち時間に対してスライドが80枚以上という暴挙にも出ています。\r\r---\r\r本日、「CloudNative Days Summer 2025 プレイベント」というイベントで「生成AIによるCloud Native 基盤構築の可能性と実践的ガードレールの敷設について」 🎵🧭 というタイトルで登壇しました！\r\r\r🔍 イベント詳細:\r- イベント名: CloudNative Days Summer 2025 プレイベント\r- 公式URL:https://cloudnativedays.connpass.com/event/351211/ \r- イベントのURL: https://event.cloudnativedays.jp/cnds2025\r\r📝 登壇ブログ\r- 2025年4月、AIとクラウドネイティブの交差点で語った2日間の記録 #CNDS2025 #hack_at_delta\r- https://syu-m-5151.hatenablog.com/entry/2025/04/24/113500","isoDate":"2025-04-22T04:00:00.000Z","dateMiliSeconds":1745294400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubernetesで実現できるPlatform Engineering の現在地","link":"https://speakerdeck.com/nwiizo/kubernetesdeshi-xian-dekiruplatform-engineering-noxian-zai-di","contentSnippet":"本日、「Kubernetesで実践する Platform Engineering - FL#88」というイベントで「Kubernetesで実現できるPlatform Engineering の現在地」🎵🧭 というタイトルで登壇しました！\r\r🔍 イベント詳細:\r- イベント名: Kubernetesで実践する Platform Engineering - FL#88\r- 公式URL: https://forkwell.connpass.com/event/348104/\r\r🗣️ 関連スライド\r- インフラをつくるとはどういうことなのか、 あるいはPlatform Engineeringについて\r- https://speakerdeck.com/nwiizo/inhurawotukurutohadouiukotonanoka-aruihaplatform-engineeringnituite\r- Platform Engineeringは自由のめまい\r- https://speakerdeck.com/nwiizo/platform-engineeringhazi-you-nomemai","isoDate":"2025-03-25T04:00:00.000Z","dateMiliSeconds":1742875200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SLI/SLO・ラプソディあるいは組織への適用の旅","link":"https://speakerdeck.com/nwiizo/slorapusodeiaruihazu-zhi-henoshi-yong-nolu","contentSnippet":"こんにちは、花粉症が辛いです。登壇する時にくしゃみしないために朝から外出を自粛してます。15分なのにスライドが40枚あります。\r\r\r本日、「信頼性向上の第一歩！～SLI/SLO策定までの取り組みと運用事例～」というイベントで「SLI/SLO・ラプソディあるいは組織への適用の旅」🎵🧭 というタイトルで登壇しました！\r\r🔍 イベント詳細:\r- イベント名: 信頼性向上の第一歩！～SLI/SLO策定までの取り組みと運用事例～\r- 公式URL: https://findy.connpass.com/event/345990/\r\r📚 さらに！4日後の3月25日には翻訳した書籍に関する登壇する別イベントもあります！😲\r「Kubernetesで実践する Platform Engineering - FL#88」🐳⚙️\r興味がある方はぜひ参加してください！👨‍💻👩‍💻\r👉 https://forkwell.connpass.com/event/348104/\r\rお見逃しなく！🗓️✨","isoDate":"2025-03-20T04:00:00.000Z","dateMiliSeconds":1742443200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"インフラをつくるとはどういうことなのか、 あるいはPlatform Engineeringについて","link":"https://speakerdeck.com/nwiizo/inhurawotukurutohadouiukotonanoka-aruihaplatform-engineeringnituite","contentSnippet":"2025年02月13日 Developers Summit 2025 13-E-4 にて「インフラをつくるとはどういうことなのか、 あるいはPlatform Engineeringについて - Platform Engineeringの効果的な基盤構築のアプローチ」というタイトルで登壇します。同日にPFEM特別回 でも登壇するのですが資料頑張って作ったのでそっちも読んでください。完全版は機会があればお話するので依頼してください。\r\rイベント名:  Developers Summit 2025\r\r公式URL: https://event.shoeisha.jp/devsumi/20250213\r\rセッションURL: https://event.shoeisha.jp/devsumi/20250213/session/5546\r\r登壇ブログ: https://syu-m-5151.hatenablog.com/entry/2025/02/14/071127","isoDate":"2025-02-13T05:00:00.000Z","dateMiliSeconds":1739422800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Platform Engineeringは自由のめまい ","link":"https://speakerdeck.com/nwiizo/platform-engineeringhazi-you-nomemai","contentSnippet":"2025年02月13日 Kubernetesで実践するPlatform Engineering発売記念！ PFEM特別回にて「Platform Engineeringは自由のめまい - 技術の選択における不確実性と向き合う」というタイトルで登壇します。同日にDevelopers Summit 2025 でも登壇したのですが資料頑張って作ったのでそっちも読んでください。\r\rイベント名: Kubernetesで実践するPlatform Engineering発売記念！ PFEM特別回\r\r公式URL: https://platformengineering.connpass.com/event/342670/\r\r登壇ブログ: https://syu-m-5151.hatenablog.com/entry/2025/02/14/071127","isoDate":"2025-02-12T05:00:00.000Z","dateMiliSeconds":1739336400000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Site Reliability Engineering on Kubernetes","link":"https://speakerdeck.com/nwiizo/site-reliability-engineering-on-kubernetes","contentSnippet":"2025年01月26日 10:35-11:05（ルーム A）にて「Site Reliability Engineering on Kubernetes」というタイトルで登壇します。\r\rイベント名: SRE Kaigi 2025\r\r公式URL: https://2025.srekaigi.net/\r\rセッションURL: https://fortee.jp/sre-kaigi-2025/proposal/a75769d1-7835-4762-a1f6-508e714c8c8e\r\r登壇ブログ: https://syu-m-5151.hatenablog.com/entry/2025/01/26/005033","isoDate":"2025-01-26T05:00:00.000Z","dateMiliSeconds":1737867600000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"メインテーマはKubernetes","link":"https://speakerdeck.com/nwiizo/meintemahakubernetes","contentSnippet":"2024年16:20-17:00（Track A）にて「メインテーマはKubernetes」というタイトルで登壇します。\r\rイベント名: Cloud Native Days Winter 2024\r\r公式URL:https://event.cloudnativedays.jp/cndw2024/\r\rセッションURL:https://event.cloudnativedays.jp/cndw2024/talks/2373","isoDate":"2024-11-28T05:00:00.000Z","dateMiliSeconds":1732770000000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"SREの前に","link":"https://speakerdeck.com/nwiizo/srenoqian-ni","contentSnippet":"2024年11月06日(水) 18:00～19:00の予定に遅刻してしまい、大変申し訳ございませんでした。お詫びとして、当初非公開予定であった資料を公開させていただきます。元々、公開する予定ではなかったので補足が足りない部分などあると思いますのでご容赦下さい。\r\rブログなどで補足情報出すかもなので気になればフォローしてください\r- https://syu-m-5151.hatenablog.com/\r- https://x.com/nwiizo\r\r\rSREの前に - 運用の原理と方法論\r公式URL: https://talent.supporterz.jp/events/2ed2656a-13ab-409c-a1d9-df8383be25fd/","isoDate":"2024-11-06T05:00:00.000Z","dateMiliSeconds":1730869200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2024年版 運用者たちのLLM","link":"https://speakerdeck.com/nwiizo/2024nian-ban-yun-yong-zhe-tatinollm","contentSnippet":"Cloud Operator Days 2024 クロージングイベント\rhttps://cloudopsdays.com/closing/\r\rとても、端的に言うと「プロンプトエンジニアリングをしよう」って話。\rこの発表資料は、LLM（大規模言語モデル）によるIT運用の可能性と課題を探っています。AIOpsの概念を基に、LLMがインシデント対応、ドキュメンテーション、コード分析などの運用タスクをどのように改善できるかを説明しています。同時に、LLMの「幻覚」や不完全性といった課題も指摘し、適切な利用方法やプロンプトエンジニアリングの重要性を強調しています。\r\r登壇時ブログ\rhttps://syu-m-5151.hatenablog.com/entry/2024/09/06/154607","isoDate":"2024-09-06T04:00:00.000Z","dateMiliSeconds":1725595200000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Observability Conference 2022 に登壇しました","link":"https://zenn.dev/nwiizo/articles/d837b78914de23","contentSnippet":"「Dapr の概念と実装から学ぶ Observability への招待」 というタイトルで登壇します。https://event.cloudnativedays.jp/o11y2022/talks/1382:embed:cite セッション概要Dapr は CloudNative な技術を背景に持つ分散アプリケーションランタイムです。本セッションでは Dapr の Observability に関する各種機能と、その実装について解説していきます。さらにスリーシェイクの Dapr と Observability への取り組みに関してもご紹介します。Dapr の機能でカバーできる点...","isoDate":"2022-03-11T04:02:18.000Z","dateMiliSeconds":1646971338000,"authorName":"nwiizo","authorId":"nwiizo"}]},"__N_SSG":true},"page":"/members/[id]","query":{"id":"nwiizo"},"buildId":"UdZ-gNg4n6NQDyq0LRXx0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>