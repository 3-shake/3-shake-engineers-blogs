<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon shortcut" type="image/png" href="https://blog.3-shake.com//logo.png"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&amp;display=swap"/><title>tozastation | 3-shake Engineers&#x27; Blogs</title><meta property="og:title" content="tozastation"/><meta property="og:url" content="https://blog.3-shake.com//members/tozastation"/><meta name="twitter:card" content="summary_large_image"/><meta property="og:site" content="3-shake Engineers&#x27; Blogs"/><meta property="og:image" content="https://blog.3-shake.com//og.png"/><link rel="canonical" href="https://blog.3-shake.com//members/tozastation"/><link rel="preload" href="/_next/static/css/9786ca6d05ae5921dd67.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9786ca6d05ae5921dd67.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.9e003f150a446b53bdd9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-cce22bd67a8d071bbba9.js" as="script"/><link rel="preload" href="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.85a3e1e6e7317f7794be.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/members/%5Bid%5D-82bdc617ad4a0fa06a9e.js" as="script"/></head><body><div id="__next"><header class="site-header"><div class="content-wrapper"><div class="site-header__inner"><a class="site-header__logo-link" href="/"><img src="/logo.svg" alt="3-shake Engineers&#x27; Blogs" class="site-header__logo-img"/><span class="site-header__logo-text">3-shake<br/>Engineers&#x27; Blogs</span></a><div class="site-header__links"><a href="https://3-shake.com/category/recruit/" class="site-header__link">Recruit</a><a href="https://3-shake.com/" class="site-header__link">Company</a></div></div></div></header><section class="member"><div class="content-wrapper"><header class="member-header"><div class="member-header__avatar"><img src="/avatars/tozastation.jpg" alt="tozastation" width="100" height="100" class="member-header__avatar-img"/></div><h1 class="member-header__name">tozastation</h1><p class="member-header__bio">tarako_chan</p><div class="member-header__links"><a href="https://twitter.com/tozastation" class="member-header__link"><img src="/icons/twitter.svg" alt="Twitterのユーザー@tozastation" width="22" height="22"/></a><a href="https://github.com/tozastation" class="member-header__link"><img src="/icons/github.svg" alt="GitHubのユーザー@tozastation" width="22" height="22"/></a><a href="https://github.com/tozastation" class="member-header__link"><img src="/icons/link.svg" alt="ウェブサイトのリンク" width="22" height="22"/></a></div></header><div class="member-posts-container"><div class="post-list"><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2020-09-07T12:53:18.000Z" class="post-link__date">2 years ago</time></div></a><a href="https://qiita.com/tozastation/items/804949c69df5d53643c6" class="post-link__main-link"><h2 class="post-link__title">Kubernetes (k8s) 管理者用GUI Lens</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-12-30T14:30:36.000Z" class="post-link__date">2 years ago</time></div></a><a href="https://qiita.com/tozastation/items/eafde1a75c35bb9d1a68" class="post-link__main-link"><h2 class="post-link__title">テストで使いたくて，DinD (Docker in Docker) でk8sの環境を整えた</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-14T05:18:21.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/1f3c3f213b42e1689406" class="post-link__main-link"><h2 class="post-link__title">Istioが作るサービスメッシュ~サンプルアプリのデプロイ~</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-14T04:08:22.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/e2f3029a8d7636da1fba" class="post-link__main-link"><h2 class="post-link__title">Istioが作るサービスメッシュ~環境構築~</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-11T02:43:55.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/e4f77a96707b4e1fdf2d" class="post-link__main-link"><h2 class="post-link__title">k8sにAPIサーバをデプロイ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-09T02:10:52.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/ca19034433f30ea5baee" class="post-link__main-link"><h2 class="post-link__title">Dockerfileの書き方 ~Multi Stage Build~</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-07T07:23:16.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/1ea437db7cfa2eea0965" class="post-link__main-link"><h2 class="post-link__title">kubectl コマンド集</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-07T04:55:32.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/a69a102fdc3f62d566b4" class="post-link__main-link"><h2 class="post-link__title">Goでレイヤードアーキテクチャ</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-07T03:05:34.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/e259c6d94a32e5c721e4" class="post-link__main-link"><h2 class="post-link__title">[入門] GoでAPIを書いて，k8sにデプロイするまで [随時更新]</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-03-07T03:02:58.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/029440f6471d3ab97b8c" class="post-link__main-link"><h2 class="post-link__title">[入門] Goとは [随時更新予定]</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-02-06T06:58:15.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/37c593b91c788e1f306d" class="post-link__main-link"><h2 class="post-link__title">k8sで、HTTP/2(gRPCサーバ)を負荷分散したい</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2019-02-05T06:03:12.000Z" class="post-link__date">3 years ago</time></div></a><a href="https://qiita.com/tozastation/items/ec97be03ae61ab01de26" class="post-link__main-link"><h2 class="post-link__title">C#でgRPCサーバを実装してみた</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/tozastation"><img src="/avatars/tozastation.jpg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">tozastation</div><time dateTime="2018-06-17T07:54:26.000Z" class="post-link__date">4 years ago</time></div></a><a href="https://qiita.com/tozastation/items/b017db842bf89dc8480b" class="post-link__main-link"><h2 class="post-link__title">Nimファイル操作</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article></div></div></div></section><footer class="site-footer"><div class="content-wrapper"><p>© <!-- -->3-shake Inc.</p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"member":{"id":"tozastation","name":"tozastation","role":"SRE","bio":"tarako_chan","avatarSrc":"/avatars/tozastation.jpg","sources":["https://qiita.com/tozastation/feed"],"includeUrlRegex":"","twitterUsername":"tozastation","githubUsername":"tozastation","websiteUrl":"https://github.com/tozastation"},"postItems":[{"title":"Kubernetes (k8s) 管理者用GUI Lens","contentSnippet":"Lensとはlensapp/lensk8sで動作する全てのリソースをモニタリングしてくれるGUIアプリLinux/Mac/Windowsで動作するこんな感じ（kindで作ったクラスタ見てます）助かりポイントリソース（Pod/Namespace/CRD/etc...）がなんかおかしいkubectl describeでイベントを確認する -\u003e コマンド長い -\u003e Lens使う全ての名前空間のアプリを一覧できる機能がありがたい -\u003e （k get po -A -o wide以上の情報✨）Podのログまで見たい時は，k9sを使ってますクラスタをいくつも管理してる場合（Dev/Stg/(Prd) + 別PJ ...）に，コンテキスト切り替え等管理が煩雑に -\u003e Lens使うSlackのワークスペースに参加するかのように，左タブにクラスタを追加しすぐに切り替え確認できる✨以上です．快適なk8sライフをーーー！","link":"https://qiita.com/tozastation/items/804949c69df5d53643c6","isoDate":"2020-09-07T12:53:18.000Z","dateMiliSeconds":1599483198000,"authorName":"tozastation","authorId":"tozastation"},{"title":"テストで使いたくて，DinD (Docker in Docker) でk8sの環境を整えた","contentSnippet":"TL;DRこちらのDockerfileを見納めくださいkindとアプリケーションのコンテナを分けても良かったのですが，kubeconfigの受け渡しが面倒だったので妥協しましたhttps://github.com/tozastation/kw/blob/master/Dockerfile.test事の経緯Kubernetesを制御するツールを書きたい．テストのために，KubernetesクラスタをホストPCに建てるの面倒だし，環境を汚したくない．．．kind (kubernetes in docker) なるものを発見しかし，アプリの開発をDocker内で行っていると，kindとアプリのコンテナは同一ネットワークに参加していないため，ネットワーク的に参照できないという問題がクラスタを作るときにネットワークを指定する系のissueを見つけたそこで，DinD (Docker in Docker)クラスタの作成をホスト側でなく，コンテナ内のDockerで行うことによってネットワーク的に参照可能にする．  kind in DinDdocker:dindというベースイメージがAlpineなので追加パッケージがある場合は，apkを使います．使い方はこちらの記事が参考になりますFROM docker:dindRUN apk --no-cache add curlRUN curl -Lo /usr/local/bin/kind https://github.com/kubernetes-sigs/kind/releases/download/v0.6.1/kind-$(uname)-amd64 \u0026\u0026 chmod +x /usr/local/bin/kindkindでクラスタ作成 (Docker Deamonの起動を待つ必要あり)以下をコンテナ内で実行すれば環境構築完了です！# クラスタの作成 kind create cluster# KUBECONFIGに，作成したクラスタのkubeconfigパスを登録 export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"kind\")\"dindコンテナを立ち上げるときに詰まった点問題docker run 時に，コマンドを渡すとdocker deamonが起動しなく，kindを実行できなかった解決策docker run 実行後，execでコマンドを渡すようにするまとめkubernetesの環境構築は，minikubeなりkindが，Dockerで構築するオプションを提供してくれていて非常に便利だなと思いました．お試しで気軽に建てるときには自分も重宝しようと思います．DinDは初めて使いましたが，プロジェクトごとにDockerが作れるためホストから見たときのコンテナが散らからなくて管理しやすそうという印象です．Kubernetesアプリとかプロバイダー作っている世の方々はテスト用のクラスタ構築とかどうされているのかなと気になりました．","link":"https://qiita.com/tozastation/items/eafde1a75c35bb9d1a68","isoDate":"2019-12-30T14:30:36.000Z","dateMiliSeconds":1577716236000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Istioが作るサービスメッシュ~サンプルアプリのデプロイ~","contentSnippet":"サンプルアプリ題材: BookInfo アプリケーション※ 事前にIstioをKubernetesにデプロイしておいてください．構成サンプルアプリのデプロイistio-1.0.6 directorykubectl apply -f samples/bookinfo/platform/kube/bookinfo.yamlbookinfo.yaml取り敢えず，一部分を抽出してみました．普段ならtargetPortを指定していますが，サイドカーインジェクションのため，指定しなくても大丈夫と言うことなのでしょうか．それ以外は差し当たっていつも通りのyamlかなと思いました．それでは次に行きましょう！################################################################################################### Details service##################################################################################################apiVersion: v1kind: Servicemetadata:  name: details  labels:    app: detailsspec:  ports:  - port: 9080    name: http  selector:    app: details---apiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: details-v1spec:  replicas: 1  template:    metadata:      labels:        app: details        version: v1    spec:      containers:      - name: details        image: istio/examples-bookinfo-details-v1:1.8.0        imagePullPolicy: IfNotPresent        ports:        - containerPort: 9080Ingress Envoy(Gatewayのデプロイ)# ingressのデプロイkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yamlbookinfo-gateway.yamlいきなりapiVersionに注目ですね．networking.istio.io/v1alpha3が使われています．Istioによる拡張APIでしょうか．GatewayとVirtual Serviceどちらも初めて聞きました．筆者はGatewayに対するDeploymentとConfigMapがこの二つかなと解釈しました．apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata:  name: bookinfo-gatewayspec:  selector:    istio: ingressgateway # use istio default controller  servers:  - port:      number: 80      name: http      protocol: HTTP    hosts:    - \"*\"---apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata:  name: bookinfospec:  hosts:  - \"*\"  gateways:  - bookinfo-gateway  http: # /api/v1/products/ + /productpage or /login or /logout だとリクエストを，productpageに流すみたいですね．  - match:    - uri:        exact: /productpage    - uri:        exact: /login    - uri:        exact: /logout    - uri:        prefix: /api/v1/products    route:    - destination:        host: productpage        port:          number: 9080まとめistioに利用するにあたって増えたAPIを理解する必要がありそうです．次は，カナリアリリースのドキュメントでも見ていきましょう．","link":"https://qiita.com/tozastation/items/1f3c3f213b42e1689406","isoDate":"2019-03-14T05:18:21.000Z","dateMiliSeconds":1552540701000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Istioが作るサービスメッシュ~環境構築~","contentSnippet":"サービスメッシュとはサービスメッシュという用語は，マイクロサービスのネットワークが構築するアプリケーションの相互作用を表します． ※　複数のPodが相互接続しているイメージだと解釈しています．サービスメッシュが抱える問題サービスメッシュの規模が大きくなるにつれて，管理が難しくなる．それは，サービスディスカバリ[1]，負荷分散，障害回復，ログ収集などが挙げられます．[1]: どのサーバ上でどのようなサービス（アプリケーション）が動作しているかに加え、そのポート番号、IPアドレス、ホスト名などを見つけることその他にもA/Bテスト[1]，カナリアリリース[2]，レート制限[3]など、より複雑な運用要件も挙げられる．[1] A/Bテストホームページの一部のデザインやページそのものを複数パターン用意し，それらを不規則に表示させ，どちらがより多くクリックされたか，より多く申し込みがあったかなどを検証する行為のこと[2] カナリアリリース新規プロダクトやサービスを本番環境の一部分にデプロイし，先に一部のユーザに提供することで新機能のフィードバッグをいち早く得ることで機能としての必要性を受け取ったうえで，全てのデプロイを行うという方法複数のノードで構成された運用サーバの一部分のノードにのみリリースを行い、デプロイされた一部のノードにてテストを実施してから、全てのノードへのデプロイを行うといった方法[3] レート制限ここはむしろ自分が知りたい内容でもあります．APIのリクエスト数を制限する方法Istioとはこれらの要件に対し，サービスメッシュ全体に関する運用管理を提供してくれます．Istioのメリット既存のコードを変更することなく，サービスメッシュの管理をすることができる．※ Podの先頭にEnvoyというプロキシをサイドカーとして配置することでメッシュサービスを管理します．※ Podからなるサービス間通信もセキュアに行ってくれるIstioお試しIstioのリリース版をダウンロードcurl -L https://git.io/getLatestIstio | sh -※ この後にパスを通しますが自分の環境だとshellがぶっ壊れました．curl後に推奨されるパスを通すと上手くいきました．minikubeの起動minikube start --memory=8192 --cpus=4 --kubernetes-version=v1.10.0 \\    --extra-config=controller-manager.cluster-signing-cert-file=\"/var/lib/localkube/certs/ca.crt\" \\    --extra-config=controller-manager.cluster-signing-key-file=\"/var/lib/localkube/certs/ca.key\" \\    --vm-driver=`your_vm_driver_choice`※ your_vm_driver_choice = virtualboxカスタムリソースのデプロイ# in istio-1.0.6 directorykubectl apply -f install/kubernetes/helm/istio/templates/crds.yamlIstioのデプロイ# k8s定義ファイルの出力helm template install/kubernetes/helm/istio --name istio --namespace istio-system \u003e istio.yaml# Namespaceの作成kubectl create namespace istio-system# デプロイkubectl apply -f istio.yamlインジェクションの確認# サンプルアプリのデプロイkubectl apply -f samples/sleep/sleep.yaml# Podの確認kubectl get pods# まだ，　Pod内のコンテナは一つ・・・NAME                     READY   STATUS    RESTARTS   AGEsleep-79cc87b6b9-cjgbk   1/1     Running   0          4m※ ここで何故かminikubeだと，Podが生成されなかったため，Docker for Desctopにて，kubernetesを起動しました．サイドカーインジェクションを名前空間単位で有効にする# ラベリングkubectl label namespace default istio-injection=enabled# 確認kubectl get namespace -L istio-injectionインジェクションが有効になっているか確認# Podを一度消去kubectl delete pod ポッド名# 無事にできていそう！NAME                     READY   STATUS    RESTARTS   AGEsleep-79cc87b6b9-gwxz9   2/2     Running   0          38sまとめ今回は，サービスメッシュが抱える問題とlstioのアプローチそして，lstioの環境構築を行いました．次は，公式の例を元にlstioをガシガシ使ってみたいと思います！参照IstioA/Bテストカナリアリリースレート制限","link":"https://qiita.com/tozastation/items/e2f3029a8d7636da1fba","isoDate":"2019-03-14T04:08:22.000Z","dateMiliSeconds":1552536502000,"authorName":"tozastation","authorId":"tozastation"},{"title":"k8sにAPIサーバをデプロイ","contentSnippet":"では早速Dockerを使っていた方は入りやすいと思います．Kubernetesは複数コンテナからなる一つのサービスをPodという単位で管理します．Docker-Compose ≒ Podと解釈して頂いていいと思います．題材https://github.com/tozastation/gRPC-Training-Golangを使って説明していきます．Docker-ComposeDBとAPIサーバのサービスを立ち上げるとする．いつもならこんな感じだと思います．version: '3'services:    db:      build: ./mssql_container      ports:        - 1433:1433      environment:        - ACCEPT_EULA=Y        - SA_PASSWORD=Test@1234        - MSSQL_PID=Express        - MSSQL_LCID=1041        - MSSQL_COLLATION=Japanese_CI_AS      volumes:        - ./mssql_container/sql:/init-data        - ./mssql_container/start-up.sh:/start-up.sh      command: \"sh -c '/start-up.sh \u0026 /opt/mssql/bin/sqlservr;'\"    api:      build: .      stdin_open: true      tty: true      ports:        - 3001:3001Pod(Kubernetes)一方Kubernetesですとこう書きます．Deployment(deployment.yaml)Deploymentの責務は，Podを維持すること及びデプロイの管理です．apiVersion: extensions/v1beta1kind: Deployment # KubernetesのどのAPIを使用するか指定metadata:   name: tozastation-grpc  namespace: tozastation # tozastationという名前空間にデプロイを指定(詳細はあとに記載)spec:　 # ReplicaSetという，設定したPod数を維持する(負荷分散)  replicas: 1　  template:    metadata:      labels: # 検索が容易になるようにラベル付け        app: tozastation-grpc　# kubectl get pods -l tozastation-grpc みたいに使う    # Podの中身を設定していく    spec:      containers:        - name: grpc-server　# Podの名前          image: tozastation/grpc_training_golang:latest　# Podのイメージパス          ports: # Exposeするポート            - containerPort: 3001        - name: mssql          image: microsoft/mssql-server-linux:latest          ports:            - containerPort: 1433          env: # 環境変数            - name: ACCEPT_EULA              value: Y            - name: SA_PASSWORD              value: Test@1234            - name: MSSQL_PID              value: Express            - name: MSSQL_LCID              value: 1041            - name: MSSQL_COLLATION              value: Japanese_CI_AS         command: [\"sh\"]　# コンテナ起動時に呼び出されるShellコマンド         args: [\"-c\", \"'/start-up.sh \u0026 /opt/mssql/bin/sqlservr;'\"]Service(service.yaml)Serviceの責務は，Deployment(Pod)をサービスとして公開することです．apiVersion: v1kind: Servicemetadata:  labels:     name: tozastation-grpc  name: tozastation-servicespec:  clusterIP: None # Serviceの種類を指定します．(詳細はあとに記載)  ports:    - name: tozastation-grpc # Portの名前      port: 3001 # 外に見せる場合のPort番号      protocol: TCP # プロトコル      targetPort: 3001 # バインドするPodのポート番号  selector:    app: tozastation-grpc # バインドするPodの名前Namespace(名前空間について)Kubernetesは，クラスタ内で隔離された環境を再現するために，名前空間というAPIを持っています．これができると，このサービスのStaging環境はあそこにデプロイという感じで定義できます．apiVersion: v1kind: Namespacemetadata:  name: tozastation-stagServiceの種類ClusterIP内部のクラスタ向けに公開します．clusterIP: 10.0.100.80HeadLess Service通常，Podをデプロイした際の負荷分散は，VIPが割り振られ行われます．しかし，gRPC通信などは，これだとうまく分散されないなどの現状があります．PodのIPは全てこちらで知っておきたいなどの場合にこれを用います．clusterIP: NoneNodePort全てのKubernetes NodeのIP:Portで受けたトラフィックをコンテナに転送する形で、外部疎通性を確立しますKubernetesのDiscovery＆LBリソース（その1）type: NodePortports:  - name: \"http-port\"    protocol: \"TCP\"    port: 8080    targetPort: 80    nodePort: 30080LoadBalancerグローバルIPを発行してくれる(クラウドベンダーで主に利用可能)type: LoadBalancer最後に作成したファイルを以下で実行すればおしまいです！kubectl apply -f [設定ファイルのパス]まとめちょっとしたAPIサーバをKubernetesでデプロイする手順を書かせていただいきました．少しでも興味を持ってもらい知識共有ができたらなと思います！","link":"https://qiita.com/tozastation/items/e4f77a96707b4e1fdf2d","isoDate":"2019-03-11T02:43:55.000Z","dateMiliSeconds":1552272235000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Dockerfileの書き方 ~Multi Stage Build~","contentSnippet":"DockerfileとはDockerイメージの中身を定義する設定ファイルになります．Vagrantを利用した方がある方は，Vagrantfileを思い出して頂くとイメージしやすいかも知れません．題材https://github.com/tozastation/gRPC-Training-Golangを使って説明していきます．APIサーバのイメージ作りです．Dockerfileに記載する流れベースとなるイメージの指定プログラムを動作するのに必要なパッケージの導入ポート開放プログラム起動コマンドDockerfile(Multi Stage Build)以下Dockerfileの例になります．FROMが2箇所あると思います．イメージを２個取得していることになりますが，これには理由があります．golang:latestというイメージの中にはGoの実行環境が入ってる訳だが，これが772MBというサイズを要する．今後，コンテナをポコポコ立てて行くとなると，なるべく起動を軽量化したいとなると思います．そこで，Multi Stage Buildです．# Build ContainerFROM golang:latest as builderWORKDIR /go/src/github.com/tozastation/gRPC-Training-GolangCOPY . .# Set Environment VariableENV CGO_ENABLED=0ENV GOOS=linuxENV GOARCH=amd64# BuildRUN go build -o app main.go# Runtime ContainerFROM alpineRUN apk add --no-cache ca-certificatesCOPY --from=builder /go/src/github.com/tozastation/gRPC-Training-Golang/app /appENV CONNECTION_STRING=Data Source=127.0.0.1:1433;Initial Catalog=Weather;User ID=SA;Password=Test@1234ENV VENDER=mssqlENV OPENWEATHER_URL=http://api.openweathermap.org/data/2.5/weatherENV OPENWEATHER_CREDENTIAL=\u0026APPID=1e16e8941ce99bdd844d129d5179d98aEXPOSE 3001ENTRYPOINT [\"/app\"]Multi Stage Buildこれは，ビルド環境から成果物を生成し，実行に必要な成果物のみを超軽量のベースイメージに移すことで，Dockerイメージの容量を最小限にすることができます．先ほどのgolang:latestは772MBでしたが，今回作成したイメージは21.2MBになります．圧倒的ですね．Go言語との相性Goのコンパイルは，シングルバイナリになります．排出される1ファイルのみを運べば，実行可能な環境が整います．Runtime用の特殊なパッケージを用意する必要もなく，alpineという軽量なLinuxイメージのみで動きます．Dockerとの相性もバッチリ書き方(やっと)基本的なDockerfileの構文は，以下のキーワードのみです．Linux環境をShellコマンドで整えたことがある方はすんなりいけるかもしれません．使いそうな順に書いています．大体これで作れるので，他にも知りたい方は公式サイトを参照ください！FROM ベースイメージの取得# FROM golang:latest WORKDIR 作業ディレクトリの指定(存在しない場合は自動で作ります．)# WORKDIR /go/src/github.com/tozastation/gRPC-Training-GolangADD ローカルファイルをDockerイメージ内にコピーする(圧縮ファイルが自動解凍される)# ADD: . .COPY 同上(圧縮ファイルが自動解凍されない)# COPY: . .RUN Shellコマンドの実行# RUN apt-get install -y vimENV 環境変数を定義# ENV PORT=3306EXPOSE 開放するポート番号# EXPOSE 3306ENTRYPOINT 実行するコマンドを指定# ENTRYPOINT [\"/app\"]※ Multi Stage Buildで必要な項目は，FROMをビルド用と実行用で２つ用いる事，実行用イメージに成果物をCOPYで移すことくらいと手軽にできますので説明は割愛します．","link":"https://qiita.com/tozastation/items/ca19034433f30ea5baee","isoDate":"2019-03-09T02:10:52.000Z","dateMiliSeconds":1552097452000,"authorName":"tozastation","authorId":"tozastation"},{"title":"kubectl コマンド集","contentSnippet":"良く使うコマンドたちリスト表示Podkubectl get podsServicekubectl get svcポートフォワードkubectl port-forward svc/サービス名 ポート番号:ポート番号作成Podkubectl apply -f ファイルパスNamespacekubectl create namespace 名前空間名消去Podkubectl delete pod Pod名Pod完全消去kubectl delete pods Pod名 --grace-period=0 --force","link":"https://qiita.com/tozastation/items/1ea437db7cfa2eea0965","isoDate":"2019-03-07T07:23:16.000Z","dateMiliSeconds":1551943396000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Goでレイヤードアーキテクチャ","contentSnippet":"レイヤードアーキテクチャとはAPIサーバを実装するときはだいたい以下の流れだと思います．データを受け取って -\u003e 処理して -\u003e データベースに保存レイヤードアーキテクチャは簡単に言うと，この流れを責務に乗っ取り分割することで，依存関係を少なくし，メンテナンスのしやすい構成にしようと言う考え方になります．構成UI(Presentation)層: クライアントのことを指します． APIサーバでは考えません．Application層: Domain層を用いて，クライアントが欲しいデータを返すのが責務です．Domain層: クライアントからのデータを処理するのが責務です．Infrastructure層: DBとの通信が責務です．題材https://github.com/tozastation/gRPC-Training-Golangを使って説明していきます構成├── domain: ドメイン層│   ├── repository: 依存性逆転の原則│   └── service: ロジック├── idl: Protocol Bufferの定義ファイル├── implements: アプリケーション層├── infrastructure: インフラ層│   └── persistence│       ├── model: DBモデル│       └── mssql: mssqlサーバ用Repository├── interfaces: その他│   ├── auth: 認証│   ├── di: 依存性の注入│   ├── handler: ハンドラー│   └── rpc: gRPC生成ファイル├── main.go├── protoc.sh└── vendor: Goのパッケージドメイン層repositoryrepositoryでは，インフラ層におけるrepositoryのインターフェースを作成します．依存性逆転の原則(抽象に依存せよ)先ほど説明したレイヤードアーキテクチャだけでは困ることがあります．それは，上位レイヤが下位レイヤの実体を持っている必要があることです．下位レイヤの実装が変更されると，上位レイヤは影響を受けることや，上位レイヤは下位レイヤの実装後でないと参照できないなどの不便さが残ります．そのため，interfaceを用いて，抽象に依存します．これにより，上位レイヤは下位レイヤの詳細は知らずとも良くなります．実装package repository// IUserRepository is ...type IUserRepository interface {    FindUserByUserToken(ctx context.Context, token string) (*db.User, error)    CreateUser(user *db.User) (string, error)    Login(uID string, password []byte) (string, error)}service実装アプリケーション層がサービス層を参照するためにインターフェースを定義しています．インフラ層のインターフェースに依存した構造体を定義しています．構造体を生成するメソッドを定義しています(のちに解説しますがこれがDIと呼ばれるものです．)定義したインターフェースに合わせてメソッドを実装します．package service// IUserService ...type IUserService interface {    GetMe(ctx context.Context, token string) (*rpc_user.GetUser, error)    SignIn(ctx context.Context, uID, password string) (string, error)    SignUp(ctx context.Context, user *rpc_user.PostUser) (string, error)}type userService struct {    irepo.IUserRepository}// NewUserService is ...func NewUserService(repo irepo.IUserRepository) IUserService {    return \u0026userService{repo}}func (srv *userService) GetMe(ctx context.Context, token string) (*rpc_user.GetUser, error) {    user, err := srv.IUserRepository.FindUserByUserToken(ctx, token)    if err != nil {        return nil, err    }    return dbToPostUser(user), nil}func dbToPostUser(user *db.User) *rpc_user.GetUser {    return \u0026rpc_user.GetUser{        Name:     user.Name,        CityName: user.CityName,    }}アプリケーション層implement実装Application層が依存するもの無くない？と思われると思います．これはGoの特徴なのですが，DIを行うメソッドを定義した際に，インターフェースを返り値として実装しています．このようにすることで，定義したインターフェースの内容を満たさないと関数が実装されたとコンパイラが認めないため，この構造体は定義したメソッドを全て含んでいるということを担保することができます．package implements// IUserImplement is ...type IUserImplement interface {    Get(ctx context.Context, p *rpc_user.GetRequest) (*rpc_user.GetResponse, error)    Login(ctx context.Context, p *rpc_user.LoginRequest) (*rpc_user.LoginResponse, error)    Post(ctx context.Context, p *rpc_user.PostRequest) (*rpc_user.PostResponse, error)}type userImplement struct {    isrv.IUserService    *logrus.Logger}// NewUserImplement is ...func NewUserImplement(s isrv.IUserService, l *logrus.Logger) IUserImplement {    return \u0026userImplement{s, l}}func (imp *userImplement) Get(ctx context.Context, p *rpc_user.GetRequest) (*rpc_user.GetResponse, error) {    imp.Logger.Infoln(\"[START] GetMyBoughtVegetablesRPC is Called from Client\")    token := p.GetToken()    user, err := imp.IUserService.GetMe(ctx, token)    if err != nil {        return nil, err    }    res := rpc_user.GetResponse{        User: user,    }    imp.Logger.Infoln(\"[END] GetMyBoughtVegetablesRPC is Called from Client\")    return \u0026res, nil}インフラ層persistence責務はデータ永続化ですmodel実装DBに受け渡すデータを構造体として定義しますpackage db// User is ...type User struct {    BaseModel    Name        string    CityName    string    Password    []byte    AccessToken string}mssql実装特徴としては，database/sqlを用いて，SQL文を直書きしています．理由としては，ormは独自の構文を使用するため他ライブラリの以降にコストがかかりますが，SQL文だと汎用性があるためです．package mssql// UserRepository istype UserRepository struct {    *sql.DB}// NewUserRepository is ...func NewUserRepository(Conn *sql.DB) irepo.IUserRepository {    return \u0026UserRepository{Conn}}// FindUserByUserToken is ...func (repo *UserRepository) FindUserByUserToken(ctx context.Context, token string) (*db.User, error) {    dbUser := db.User{}    if err := repo.DB.QueryRow(\"SELECT CityName FROM [Weather].[dbo].[Users] WHERE AccessToken = \" + token).Scan(\u0026dbUser.CityName); err != nil {        return nil, err    }    return \u0026dbUser, nil}その他authJWT(Json Web Token)という技術を用いています．興味ある方はこちらを参照di依存性注入(Dependency Injection)先ほどからDIDIと言ってきましたが，DIとは外部からクラスを射し込むことを言います．実装テストを書くときを考えてみてください．サービス層のテストをするとなると，DBの情報が欲しいとなります．となるとリポジトリが必要そうです．しかしこれでは，DBに依存してしまい切り分けた意味がなくなってしまいます．DIをすることにより，決まった値を返すモックのリポジトリを注入することができ，晴れて依存性を解消できるという訳です．package divar logger = logrus.New()// InitializeUser is ...func InitializeUser() implements.IUserImplement {    repo := mssql.NewUserRepository(handler.OpenDBConnection())    srv := service.NewUserService(repo)    imp := implements.NewUserImplement(srv, logger)    return imp}handlerDBなどのコネクションを生成するメソッドを集めています．rpcgRPCの説明は後に回します","link":"https://qiita.com/tozastation/items/a69a102fdc3f62d566b4","isoDate":"2019-03-07T04:55:32.000Z","dateMiliSeconds":1551934532000,"authorName":"tozastation","authorId":"tozastation"},{"title":"[入門] GoでAPIを書いて，k8sにデプロイするまで [随時更新]","contentSnippet":"概要新しい技術を使ってみたいけど， 何がいいの？どう使うの？時間ないよ と躊躇してしまうこともあると思います．本記事では，自分が躓いたポイントも踏まえつつ題材を元にガシガシ書いていこうかなと思います．自分自身も最近学び始めた技術ですので，入門として書かせていただきます！構成Go編Goとは Goで，レイヤードアーキテクチャを書いてみるGoでgRPCを使ってみる(お待ち下さい😓)Goのテストを書こう(お待ち下さい😓)Docker編Dockerとは(お待ち下さい😓)Dockerfileの書き方Dockerの使い方(お待ち下さい😓)Kubernetes(k8s)編Kubernetes(k8s)とは (お待ち下さい😓)k8sにAPIサーバをデプロイkubectlコマンドログの収集 (お待ち下さい😓)Istio on KubernetesIstioが作るサービスメッシュ~環境構築~Istioが作るサービスメッシュ~サンプルアプリのデプロイ~","link":"https://qiita.com/tozastation/items/e259c6d94a32e5c721e4","isoDate":"2019-03-07T03:05:34.000Z","dateMiliSeconds":1551927934000,"authorName":"tozastation","authorId":"tozastation"},{"title":"[入門] Goとは [随時更新予定]","contentSnippet":"Goについてプログラミング言語Goの情報サイトであるgolang.jpさんに書いています．Googleさんが発表したオープンソースのプログラミング言語になります．自分が特に抜粋するならば，以下の二点になります．シンプルさ大規模なシステムにおいて，属人性（個の技術スキル）による複雑な実装は，運用やメンテナンスを難しくさせます．Goには，継承・Generics・例外が実装されていません(参考記事)．また，ループ処理を例にあげると，他の言語だとfor・ while・do/whileなど選択肢がありますが，Goでは，forしか実装されていません．このように，プログラマーによる表現の違いを抑制することができます．このことから，初学者でも取っつきやすいと思っています．自分を例に出すと，APIサーバを作ろうとなった時に，Java, Scala-\u003ePythonなどを学んできましたが， Goの学習コストの方が低いですし，モチベーションが上がりました．システムを開発している方達でも，多人数で精度やばらつきを維持するプログラムを生産していくのは難しいと思います（ペアプロやモブプロもありますが）．高速さ開発において，速度は重要です．ここでいう開発速度は，プログラミング言語のパフォーマンスではなく，チームとしての開発速度になります．特にプログラムのビルド時間は特に重要であると考えています．実装したシステムを開発環境やステージング環境で動かしたいことを考えてみて下さい．経験した方もいると思いますが，規模の大きなシステムになる程ビルド時間はものすごい量になります．下記は，C#とGoで同様の処理のプログラムを書き，ビルドを行なった結果です．C#ももちろん好きです結果C#の2分の1ほどの速度で実現できています．- C#: 約447秒- Go: 約200秒まとめプログラミングを初めて3年ほど経ち，チーム開発を何度か経験してみてGoはいいなと思ったので本記事を書かせていただきました．興味のある方はぜひ試してみて下さい！","link":"https://qiita.com/tozastation/items/029440f6471d3ab97b8c","isoDate":"2019-03-07T03:02:58.000Z","dateMiliSeconds":1551927778000,"authorName":"tozastation","authorId":"tozastation"},{"title":"k8sで、HTTP/2(gRPCサーバ)を負荷分散したい","contentSnippet":"kubernetesで、HTTP/2を負荷分散してみたはじめに背景現在学生プロジェクトでインフラとして、GKE(Google Kubernetes Engine)を利用させて頂いています。REST APIサーバを、gRPCに換装してみようとした時にある問題にぶち当たりました。前提知識としてKubernetesのロードバランサServices are a “layer 4” (TCP/UDP over IP) construct, the proxy was purely in userspace現行のKubernetesのIngressは、OSI参照モデルのトランスポート層(L4)のロードバランサを提供しています。(beta版はその限りではないです！)gRPCの通信の仕方gRPCでの通信は、 一つのTCPコネクションを使い回します。つまり、コネクションが確立している間はサーバとクライアントは繋がり続けます。問題とはgRPC is a modern RPC protocol implemented on top of HTTP/2. HTTP/2 is a Layer 7 (Application layer) protocol (参照: 公式サイトより)というように、gRPCは、アプリケーション層(L7)で動作するのです。つまり、L4のロードバランサでは、 適切に負荷分散できない場合 があるのです！Ingrees使えないじゃん！Envoyに出会いましたL7 LoadBalancerと調べていたら記事が見つかって来ました。ref:  kubernetesでgRPCするときにenvoy挟んでみたよEnvoyとは、クラウドネイティブ向けつまりマイクロサービス向けのProxyとして設計されたオープンソースのソフトです。L7プロキシももちろん対応です！セットアップでは早速導入していきましょう！流れはこんな感じです。1. gRPCサーバのServiceをLoad BalancerからHeadless Serviceにする2. Envoyをクラスタにデプロイし外部公開gRPCサーバのServiceをLoad BalancerからHeadless ServiceにするEnvoyのコンフィグファイルはこんな感じになっています。apiVersion: v1kind: ConfigMapmetadata:  name: \"envoy-config\"data:  envoy.json: |    {      \"listeners\": [        {          \"address\": \"tcp://0.0.0.0:15001\",          \"filters\": [            {              \"type\": \"read\",              \"name\": \"http_connection_manager\",              \"config\": {                \"codec_type\": \"auto\",                \"stat_prefix\": \"ingress_http\",                \"route_config\": {                  \"virtual_hosts\": [                    {                      \"name\": \"service\",                      \"domains\": [\"*\"],                      \"routes\": [                        {                          \"timeout_ms\": 0,                          \"prefix\": \"/\",                          \"cluster\": \"\"                        }                      ]                    }                  ]                },                \"filters\": [                  {                    \"type\": \"decoder\",                    \"name\": \"router\",                    \"config\": {}                  }                ]              }            }          ]        }      ],      \"admin\": {       \"access_log_path\": \"/dev/stdout\",       \"address\": \"tcp://127.0.0.1:8001\"      },      \"cluster_manager\": {        \"clusters\": [          {            \"name\": \"rakusale-grpc\",            \"features\": \"http2\",            \"connect_timeout_ms\": 250,            \"type\": \"strict_dns\",            \"lb_type\": \"round_robin\",            \"hosts\": [{\"url\": \"tcp://Host名:Port番号\"}]         }        ]      }    }注目して欲しいところはここです！\"cluster_manager\": {        \"clusters\": [          {            \"name\": \"rakusale-grpc\",            \"features\": \"http2\",            \"connect_timeout_ms\": 250,            \"type\": \"strict_dns\",            \"lb_type\": \"round_robin\",            \"hosts\": [{\"url\": \"tcp://Host名:Port番号\"}]         }        ]      }clusters[\"hosts\"]のところに、サーバ側のIPとポートを指定します。つまり、Envoy側が負荷分散するためにPodのIPを知っている必要があります。ここで、Headless ServiceHeadless Serviceは、Serviceに対してDNSリクエストを送るとPodのIPを返してくれるものです。動かしたいサーバのサービスはこのように設定しています。apiVersion: v1kind: Servicemetadata:  labels:     name: \"\"  name: \"\"spec:  clusterIP: None  ports:    - name: \"\"      port: 3001      protocol: TCP      targetPort: 3001  selector:    app: \"\"ClusterIP: Noneとするだけで、Headless Serviceになります！Envoyをクラスタにデプロイし外部公開必要なものは3つです。- envoy_configmap.yaml : さっき紹介しています！- envoy_deployment.yaml- envoy_service.yamlenvoy_deployment.yamlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: \"envoy\"spec:  replicas: 2  template:    metadata:      labels:        app: \"envoy\"    spec:      volumes:        - name: envoy          configMap:            name: \"envoy-config\"        # - name: tls        #   secret:        #     secretName: tlssecret      containers:        - name: envoy          image: envoyproxy/envoy:6e3633496f5a9412abdca8bac7db6b701ae8ce14          command:            - \"/usr/local/bin/envoy\"          args:            - \"--config-path /etc/envoy/envoy.json\"          resources:            limits:              memory: 512Mi          ports:            - containerPort: 15001              name: app            - containerPort: 8001              name: envoy-admin          volumeMounts:            - name: envoy              mountPath: /etc/envoy            # - name: tls            #   mountPath: /etc/tlssecret            #   readOnly: trueenvoy_service.yamlapiVersion: v1kind: Servicemetadata:  labels:    name: \"envoy\"  name: \"envoy-service\"spec:  type: LoadBalancer  selector:    app: \"envoy\"  ports:    - name: tcp      port: 15001      targetPort: 15001以上を用意して kubectl apply してGIPに接続すれば、負荷分散が完成しているはずです！終わりにKubernetesクラスタ内のパケットとかネットワーク監視するツールで良いのがあったら教えて欲しいです！あと、gRPCサーバ書く時にログをきちんと書く実装にしないと心底不便だなと思いました、、、いつもはフレームワークがやってくれていたので、、、今後も精進していきます","link":"https://qiita.com/tozastation/items/37c593b91c788e1f306d","isoDate":"2019-02-06T06:58:15.000Z","dateMiliSeconds":1549436295000,"authorName":"tozastation","authorId":"tozastation"},{"title":"C#でgRPCサーバを実装してみた","contentSnippet":"C#でgRPCサーバを実装してみたはじめに背景学生のプロジェクトで、 ApiaryやSwaggerなどのAPIドキュメントツールを用いていたが、規模が大きくなるに連れて編集が面倒だな〜と感じていた。↓こんな感じのyamlを書き続ける そう思ってた時に出会ったのがgRPCである。gRPCとはgRPCは、RPC (Remote Procedure Call) を実現するためにGoogleが開発したプロトコルの1つです。Protocol Buffers を使ってデータをシリアライズし、高速な通信を実現できる点が特長です。ref: gRPCって何？でも本当にいいなって思ったのはここじゃない！gRPCでは、IDL（インターフェース定義言語）を使ってあらかじめAPI仕様を .proto ファイルとして定義し、そこからサーバー側＆クライアント側に必要なソースコードのひな形を生成します。つまりこんなprotoファイルを定義してあげると、APIドキュメントの生成、Client\u0026Server用のコードを自動生成してくれる。すごい、、！データのやり取りのロジック実装に時間を取られることがないというのが圧倒的メリットだと思ってる！weather.protosyntax = \"proto3\";package proto.weather;service Weathers{    rpc Get (GetRequest) returns (GetResponse) {}}message Weather {    double ID = 1;    string CityName = 2;    double TempMax = 3;    double TempMin = 4;    double Wind = 5;    string Type = 6;    string Description = 7;}message GetRequest {    string CityName = 1;}message GetResponse {    Weather Weather = 1;}開発環境ツールなどOS: MacOSVisual Studio: IDEDotnet SDK(2.2.103): コンテナイメージ bloomrpc : Postmanのように、GUIでAPIを叩ける有難いツールDocker for MacgRPC-Trainning : 今回利用するリポジトリC#のライブラリ\u003cPackageReference Include=\"Dapper\" Version=\"1.50.5\" /\u003e\u003cPackageReference Include=\"Google.Cloud.Language.V1\" Version=\"1.1.0\" /\u003e\u003cPackageReference Include=\"Google.Protobuf\" Version=\"3.6.1\" /\u003e\u003cPackageReference Include=\"Grpc\" Version=\"1.18.0\" /\u003e\u003cPackageReference Include=\"Grpc.Core\" Version=\"1.18.0\" /\u003e\u003cPackageReference Include=\"Grpc.Tools\" Version=\"1.18.0\"\u003e具体的な実装事前準備MacOSに、protoファイルを別言語へコンパイルするためのツールとプラグインを入れておくこと↓とっても分かりやすいですref: C#でgRPC環境を作成する　ディレクトリ構成DDD風にしてみました。Application/Implements主にここにRPCの実装を行う(ASP.NET CoreのControllerだと思ってもらって大丈夫です！)流れとしては、自動生成されたファイルを基底クラスとて継承します。あとはメソッドをオーバーライドして、ロジックを書いていく感じてす。Implements/WeatherImpl.csusing Proto.Weather;using System.Threading.Tasks;using Grpc.Core;using WeatherApi.Application.Domain.Service.Interface;using System;namespace WeatherApi.Application.Implements{    public class WeatherImpl: Weathers.WeathersBase    {        IWeatherService service;        public WeatherImpl(IWeatherService _service) : base()        {            if (_service == null)            {                throw new ArgumentNullException(nameof(IWeatherService));            }            this.service = _service;        }        public override Task\u003cGetResponse\u003e Get(GetRequest request, ServerCallContext context)        {            string cityName = request.CityName;            var weathers = this.service.FindCurrentWeatherByCityName(cityName);            var response = new GetResponse();            response.Weather = weathers[0];            return Task.FromResult(response);        }    }}Application/Program.csImplementsで作成したRPCをServerにBindします。残りの実装は、既存のWebフレームワークと変わりません。using System;using Microsoft.Extensions.Configuration;using WeatherApi.Application.Infrastructure;using Grpc.Core;using Proto.Weather;using Proto.User;using WeatherApi.Application.Implements;using WeatherApi.Application.Domain.Service;using Proto.Ping;namespace Application{    public class Program    {        public static IConfigurationRoot configuration;        public static SqlHandler sqlHandler;        public static void Main(string[] args)        {            const int Port = 5000;            var weatherImpl = new WeatherImpl(new WeatherService());            var userImpl = new UserImpl(new UserService());            var pingImpl = new PingImpl();            Server server = new Server            {                Services = {                     Weathers.BindService(weatherImpl),                    Check.BindService(pingImpl),                    Users.BindService(userImpl)                },                Ports = { new ServerPort(\"0.0.0.0\", Port, ServerCredentials.Insecure) }            };            server.Start();            Console.WriteLine(\"Tozawa server listening on port \" + Port);            Console.WriteLine(\"Press any key to stop the server...\");            Console.Read();            server.ShutdownAsync().Wait();        }    }}セットアッププロジェクトファイルのクローンgit clone https://github.com/tozastation/gRPC-Training.gitgRPCサーバの起動docker-compose build; docker-compose up grpc使い方bloomrpcの起動　\u0026 プラスボタンから作成した proto ファイルを追加しますあとは、jsonを渡すかのように書き送信するだけです終わりにC#のgRPC実装があまりないので今回の記事で、C#でのgRPCサーバ実装およびDebug方法の共有ができれば何よりです！今後はInterceptor(Middleware)の実装もしていきたい切実にドキュメントが増えてほしい、、、","link":"https://qiita.com/tozastation/items/ec97be03ae61ab01de26","isoDate":"2019-02-05T06:03:12.000Z","dateMiliSeconds":1549346592000,"authorName":"tozastation","authorId":"tozastation"},{"title":"Nimファイル操作","contentSnippet":"概要SlackのAPIを叩きチャンネルのログをファイルとして吐き出すプログラムを書いている途中、ファイル出力で躓いたため自分用のドキュメントを残そうと思った。Filemode5つのモードがあるが、Qiitaで見るのは主にfmWriteなので、全て記述してみる。modecontentfmRead読み込み専用fmWrite書き込み専用fmReadWrite読み書き可能(ファイルが存在しない場合、生成)fmReadWriteExisting読み書き可能(ファイルが存在しない場合、何もしない)fmAppend末尾に書き込み※公式ドキュメントExampleFileOpenNimドキュメントproc open(f: var File; filehandle: FileHandle; mode: FileMode = fmRead): bool {..}コードvar file : File = open(\"hoge.txt\", fmRead)var [変数名] : [型名] = open([ファイル名], [モード])課題jsonファイルを吐かせたかったが、現状ではtxtの方法しか分からなかったため、jsonデータをjsonファイルとして書き出せる手段を探していきたい。参考にさせていただいた記事タイトル : nimファイル操作ユーザ名 : @6in さん","link":"https://qiita.com/tozastation/items/b017db842bf89dc8480b","isoDate":"2018-06-17T07:54:26.000Z","dateMiliSeconds":1529222066000,"authorName":"tozastation","authorId":"tozastation"}]},"__N_SSG":true},"page":"/members/[id]","query":{"id":"tozastation"},"buildId":"BXAj4GsVQK9FFxHSXHQVr","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon shortcut","type":"image/png","href":"https://blog.3-shake.com//logo.png"}],["link",{"rel":"stylesheet","href":"https://fonts.googleapis.com/css2?family=Inter:wght@400;700\u0026display=swap"}],["title",{"children":"tozastation | 3-shake Engineers' Blogs"}],["meta",{"property":"og:title","content":"tozastation"}],["meta",{"property":"og:url","content":"https://blog.3-shake.com//members/tozastation"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"property":"og:site","content":"3-shake Engineers' Blogs"}],["meta",{"property":"og:image","content":"https://blog.3-shake.com//og.png"}],["link",{"rel":"canonical","href":"https://blog.3-shake.com//members/tozastation"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-4beebf4ac9054f0bf4e6.js"></script><script src="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" async=""></script><script src="/_next/static/chunks/commons.9e003f150a446b53bdd9.js" async=""></script><script src="/_next/static/chunks/pages/_app-cce22bd67a8d071bbba9.js" async=""></script><script src="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.85a3e1e6e7317f7794be.js" async=""></script><script src="/_next/static/chunks/pages/members/%5Bid%5D-82bdc617ad4a0fa06a9e.js" async=""></script><script src="/_next/static/BXAj4GsVQK9FFxHSXHQVr/_buildManifest.js" async=""></script><script src="/_next/static/BXAj4GsVQK9FFxHSXHQVr/_ssgManifest.js" async=""></script></body></html>