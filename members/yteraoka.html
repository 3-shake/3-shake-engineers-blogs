<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="icon shortcut" type="image/png" href="https://blog.3-shake.com//logo.png"/><link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;700&amp;family=Roboto:wght@300;400;500;700&amp;display=swap" rel="stylesheet"/><title>yteraoka | 3-shake Engineers&#x27; Blogs</title><meta property="og:title" content="yteraoka"/><meta property="og:url" content="https://blog.3-shake.com//members/yteraoka"/><meta name="twitter:card" content="summary_large_image"/><meta property="og:site" content="3-shake Engineers&#x27; Blogs"/><meta property="og:image" content="https://blog.3-shake.com//og.png"/><link rel="canonical" href="https://blog.3-shake.com//members/yteraoka"/><link rel="preload" href="/_next/static/css/337c552ef6d4082ede0d.css" as="style"/><link rel="stylesheet" href="/_next/static/css/337c552ef6d4082ede0d.css" data-n-g=""/><noscript data-n-css="true"></noscript><link rel="preload" href="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.9e003f150a446b53bdd9.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-c57e3b9671dd7cf89bcb.js" as="script"/><link rel="preload" href="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.8252249df9bf225c6a6c.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/members/%5Bid%5D-82bdc617ad4a0fa06a9e.js" as="script"/></head><body><div id="__next"><header class="site-header"><div class="content-wrapper"><div class="site-header__inner"><a class="site-header__logo-link" href="/"><img src="/logo.svg" alt="3-shake Engineers&#x27; Blogs" class="site-header__logo-img"/><span class="site-header__logo-text">3-shake<br/>Engineers&#x27; Blogs</span></a><div class="site-header__links"><a href="https://3-shake.com/category/recruit/" class="site-header__link">Recruit</a><a href="https://3-shake.com/" class="site-header__link">Company</a></div></div></div></header><section class="member"><div class="content-wrapper"><header class="member-header"><div class="member-header__avatar"><img src="/avatars/yteraoka.jpeg" alt="yteraoka" width="100" height="100" class="member-header__avatar-img"/></div><h1 class="member-header__name">yteraoka</h1><p class="member-header__bio">ojisan</p><div class="member-header__links"><a href="https://twitter.com/yteraoka" class="member-header__link"><img src="/icons/twitter.svg" alt="Twitterのユーザー@yteraoka" width="22" height="22"/></a><a href="https://github.com/yteraoka" class="member-header__link"><img src="/icons/github.svg" alt="GitHubのユーザー@yteraoka" width="22" height="22"/></a><a href="https://blog.1q77.com/" class="member-header__link"><img src="/icons/link.svg" alt="ウェブサイトのリンク" width="22" height="22"/></a></div></header><div class="member-posts-container"><div class="post-list"><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2022-02-26T15:52:27.000Z" class="post-link__date">12 days ago</time></div></a><a href="https://blog.1q77.com/2022/02/istio-exit-on-zero-active-connections/" class="post-link__main-link"><h2 class="post-link__title">istio sidecar の停止を connection がなくなるまで遅らせる</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2022-01-08T05:27:39.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://blog.1q77.com/2022/01/telepresence-part-2/" class="post-link__main-link"><h2 class="post-link__title">telepresence 入門 (2)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2022-01-04T16:23:46.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://blog.1q77.com/2022/01/docker-on-lima/" class="post-link__main-link"><h2 class="post-link__title">Docker on Lima</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-12-31T14:54:43.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://blog.1q77.com/2021/12/telepresence-part-1/" class="post-link__main-link"><h2 class="post-link__title">telepresence 入門 (1)</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-12-31T07:28:54.000Z" class="post-link__date">2 months ago</time></div></a><a href="https://blog.1q77.com/2021/12/cloudfront-security-headers/" class="post-link__main-link"><h2 class="post-link__title">CloudFront のレスポンスに Security Headers を追加する</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-09-23T14:20:51.000Z" class="post-link__date">6 months ago</time></div></a><a href="https://blog.1q77.com/2021/09/podman-on-mac/" class="post-link__main-link"><h2 class="post-link__title">mac で podman</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-09-19T16:58:10.000Z" class="post-link__date">6 months ago</time></div></a><a href="https://blog.1q77.com/2021/09/lima/" class="post-link__main-link"><h2 class="post-link__title">Lima で nerdctl</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-09-18T16:07:27.000Z" class="post-link__date">6 months ago</time></div></a><a href="https://blog.1q77.com/2021/09/replace-docker-desktop-with-minikube/" class="post-link__main-link"><h2 class="post-link__title">Docker Desktop の代わりに Minikube を使ってみる</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-04-10T14:44:13.000Z" class="post-link__date">a year ago</time></div></a><a href="https://blog.1q77.com/2021/04/step-ca-with-google-oidc/" class="post-link__main-link"><h2 class="post-link__title">Google 認証でクライアント証明書発行のセルフサービス化</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-04-04T04:03:47.000Z" class="post-link__date">a year ago</time></div></a><a href="https://blog.1q77.com/2021/04/terraform-dev_overrides/" class="post-link__main-link"><h2 class="post-link__title">Terraform でカスタム provider を使うための dev_overrides 設定</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=blog.1q77.com" width="14" height="14" class="post-link__site-favicon"/>blog.1q77.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2021-01-13T14:04:22.000Z" class="post-link__date">a year ago</time></div></a><a href="https://qiita.com/yteraoka/items/e74e8bf24f72f7ed5f15" class="post-link__main-link"><h2 class="post-link__title">glibc, musl libc, go の resolver の違い</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-12-22T03:02:36.000Z" class="post-link__date">4 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/41faf5d183b997b66625" class="post-link__main-link"><h2 class="post-link__title">Bitwardenで自分専用パスワードマネージャーサーバー構築</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-12-11T05:03:26.000Z" class="post-link__date">4 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/77b329606f44b4e19b92" class="post-link__main-link"><h2 class="post-link__title">STNSの独自サーバーを書いてみた</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-12-07T14:39:03.000Z" class="post-link__date">4 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/cfa185f8846850648ab4" class="post-link__main-link"><h2 class="post-link__title">CentOS7でPostgreSQL 10のPacemakerクラスタ作成</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-08-20T13:00:07.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/b1dbb687eb08a4313688" class="post-link__main-link"><h2 class="post-link__title">KVM ホストを Prometheus と Grafana で可視化</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-08-19T08:20:25.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/5a4a891790595acf47e8" class="post-link__main-link"><h2 class="post-link__title">/proc の environ を見やすくする</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-05-04T13:31:34.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/601e273066df262dc1ff" class="post-link__main-link"><h2 class="post-link__title">GSuite 管理下アカウントの接続アプリ情報を確認する方法</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-03-16T14:58:18.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/c161e6b22422bb0eb7df" class="post-link__main-link"><h2 class="post-link__title">GitLab Runner を AWS Spotfleet で節約運用</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2017-02-10T12:55:51.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/1f5d828545bf8a798289" class="post-link__main-link"><h2 class="post-link__title">GitLab の Artifact 掃除ツール</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2016-12-05T10:49:32.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/e82e4d28f6a23915d190" class="post-link__main-link"><h2 class="post-link__title">pgrepup で論理レプリケーションによる PostgreSQL の Upgrade</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2016-11-30T15:08:15.000Z" class="post-link__date">5 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/380ded6b68b630bb9388" class="post-link__main-link"><h2 class="post-link__title">mod_proxy_hcheck で BalancerMember の healthcheck</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-12-25T14:32:15.000Z" class="post-link__date">6 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/e4593ed59fd3f923275f" class="post-link__main-link"><h2 class="post-link__title">えっ？アドベントカレンダーって25日目があるの？からの curl で HTTP2</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-12-14T15:02:25.000Z" class="post-link__date">6 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/60e31477c62b7c94d32e" class="post-link__main-link"><h2 class="post-link__title">XFS の speculative preallocation について調べてみた</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-12-02T11:29:30.000Z" class="post-link__date">6 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/7119d4e1e2f8faddfb64" class="post-link__main-link"><h2 class="post-link__title">Ansible の Template 機能の紹介</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-11-30T15:53:59.000Z" class="post-link__date">6 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/d526e4c4c869b8592667" class="post-link__main-link"><h2 class="post-link__title">PostgreSQL の ALTER DEFAULT PRIVILEGES</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-07-02T01:43:45.000Z" class="post-link__date">7 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/5a4027df290665110751" class="post-link__main-link"><h2 class="post-link__title">OpenLDAPをマルチドメインでレプリケーション</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2015-05-12T11:28:02.000Z" class="post-link__date">7 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/de9da64ca2d9261b0292" class="post-link__main-link"><h2 class="post-link__title">Ansible Vault を賢く使う</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2014-09-20T04:04:32.000Z" class="post-link__date">7 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/06fc91f37913c938d936" class="post-link__main-link"><h2 class="post-link__title">modern.IE を Vagrant で使う</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2014-09-19T12:37:24.000Z" class="post-link__date">7 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/dc4eae7fd657af4ab4a2" class="post-link__main-link"><h2 class="post-link__title">LDAPサーバーに REST API アクセス</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article><article class="post-link"><a class="post-link__author" href="/members/yteraoka"><img src="/avatars/yteraoka.jpeg" class="post-link__author-img" width="35" height="35"/><div class="post-link__author-name"><div class="post-link__author-name">yteraoka</div><time dateTime="2014-08-05T05:54:25.000Z" class="post-link__date">8 years ago</time></div></a><a href="https://qiita.com/yteraoka/items/e661c2a8c6e7617e64f9" class="post-link__main-link"><h2 class="post-link__title">CentOS 7 の nmcli で bonding</h2><div class="post-link__site"><img src="https://www.google.com/s2/favicons?domain=qiita.com" width="14" height="14" class="post-link__site-favicon"/>qiita.com</div></a></article></div></div></div></section><footer class="site-footer"><div class="content-wrapper"><p>© <!-- -->3-shake Inc.</p></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"member":{"id":"yteraoka","name":"yteraoka","role":"SRE","bio":"ojisan","avatarSrc":"/avatars/yteraoka.jpeg","sources":["https://blog.1q77.com/feed/","https://qiita.com/yteraoka/feed"],"includeUrlRegex":"","twitterUsername":"yteraoka","githubUsername":"yteraoka","websiteUrl":"https://blog.1q77.com/"},"postItems":[{"title":"istio sidecar の停止を connection がなくなるまで遅らせる","contentSnippet":"新機能 EXIT_ON_ZERO_ACTIVE_CONNECTIONS 以前、「Istio 導入への道 – sidecar の調整編」という記事で、Istio の sidecar (istio-proxy) が、アプリの終了を待たずに停止してしまってアプリ側が通信できなくなるという問題に対して preStop hook に netstat などを使った wait 処理を入れるというのを紹介しましたが、あれは listen しているプロセスがいなくなるまで待つというもので、nginx などのように処理中のリクエストは完了を待つが、listen している socket は signal を受けるとすぐに close するというサーバーの場合には有効に働きませんでした。これに対して 2021年11月にリリースされた Istio 1.12 では drain モードに変更した後、アクティブなコネクションがなくなるまで待つという設定ができるようになっていました。(drain モードについては後述) Istio 1.12 Change Notes に次のように書かれています。 Added support for envoy to track active connections during drain and quit if active connections […]istio sidecar の停止を connection がなくなるまで遅らせる first appeared on 1Q77.","link":"https://blog.1q77.com/2022/02/istio-exit-on-zero-active-connections/","isoDate":"2022-02-26T15:52:27.000Z","dateMiliSeconds":1645890747000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"telepresence 入門 (2)","contentSnippet":"前回の telepresence 入門 (1) の続きです。今回は Kubernetes クラスタの Service へのアクセスをインターセプトして手元の環境に転送することを試します。Kubernetes 側の volume も手元で mount させるし、環境変数も引っ張ってきます。 バージョンなど環境については前回と同じ。 インターセプト設定 Kubernetes の Service 宛の通信を手元に転送することを　intercept と呼ぶようです。 インターセプト前の状態 前回使った hello という Service, Deployment を使います。 $ kubectl create deploy hello --image=k8s.gcr.io/echoserver:1.4 $ kubectl expose deploy hello --port 80 --target-port 8080 Service は port 80 をコンテナの port 8080 に転送するようになっています。 $ kubectl get svc […]telepresence 入門 (2) first appeared on 1Q77.","link":"https://blog.1q77.com/2022/01/telepresence-part-2/","isoDate":"2022-01-08T05:27:39.000Z","dateMiliSeconds":1641619659000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Docker on Lima","contentSnippet":"以前、「Lima で nerdctl」という記事を書きました。その後、lima の VM 上で docker daemon を実行し、ホスト側から docker コマンドでアクセスするという方法があることを知りました。たまたま、brew upgrade を実行していたところ lima が 0.8.0 に更新されたのを見て Github の releases ページを見、試してみようかなと思ったのでメモです。 ちなみに、前回試した時のバージョンは 0.6.4 でした。 Docker 入りの VM を起動させる 私は brew のインストール先を $HOME にしていますが、brew で lima をインストールすると ~/.homebrew/Cellar/lima/0.8.0/share/doc/lima/examples/docker.yaml に docker 入りの VM を作成するための設定ファイルも一緒にインストールされています。これを使うことで簡単に VM が作成できます。次のように limactl start に続けてファイルの path を指定するだけです。 $ limactl start ~/.homebrew/Cellar/lima/0.8.0/share/doc/lima/examples/docker.yaml ファイルは URL […]Docker on Lima first appeared on 1Q77.","link":"https://blog.1q77.com/2022/01/docker-on-lima/","isoDate":"2022-01-04T16:23:46.000Z","dateMiliSeconds":1641313426000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"telepresence 入門 (1)","contentSnippet":"telepresence というツールがあります。手元の端末が Kubernetes クラスタ内にいるかのような通信を可能にし、Kubernetes の Pod の Container への通信をインターセプトして手元の端末に流すことができます。これの仕組みを調べてみます。(以前は Python で書かれていたようですが、v2 は Go で書き直されたみたいです) 確認に使用した telepresence と mac の version $ telepresence version Client: v2.4.9 (api v3) $ sw_vers ProductName: macOS ProductVersion: 12.1 BuildVersion: 21C52 Kubernetes クラスタは GKE の v1.21.5-gke.1302 クラスタへの terffic-manager のデプロイ helm を使ってインストールすることができます。デフォルトではクラスタワイドな設定になりますが、特定の namespace にのみインストールしたり、namespace 毎に権限を分けてインストールすることも可能です。 $ helm repo add datawire https://app.getambassador.io $ […]telepresence 入門 (1) first appeared on 1Q77.","link":"https://blog.1q77.com/2021/12/telepresence-part-1/","isoDate":"2021-12-31T14:54:43.000Z","dateMiliSeconds":1640962483000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"CloudFront のレスポンスに Security Headers を追加する","contentSnippet":"「Amazon CloudFront が設定可能な CORS、セキュリティ、およびカスタム HTTP レスポンスヘッダーをサポート」で Lambda@Edge なしで Response にカスタムヘッダーを追加することが可能になりました。 これを使って、このサイトにも Security Headers を追加してみます。 CloudFront のコンソールで、Policies → Response headers にアクセスすると Managed policies があり、次のポリシーが存在します。 CORS-and-SecurityHeadersPolicy CORS-With-Preflight CORS-with-preflight-and-SecurityHeadersPolicy SecurityHeadersPolicy SimpleCORS SecurityHeaders と CORS の組み合わせパターンですね。 SecurityHeadersPolicy ではレスポンスに次の Header がセットされます。 Strict-Transport-Security: max-age=31536000 X-Content-Type-Options: nosniff X-Frame-Options: SAMEORIGIN X-XSS-Protection: 1; mode=block Referrer-Policy: strict-origin-when-cross-origin これを Distributions → Behaviors の Response headers policy […]CloudFront のレスポンスに Security Headers を追加する first appeared on 1Q77.","link":"https://blog.1q77.com/2021/12/cloudfront-security-headers/","isoDate":"2021-12-31T07:28:54.000Z","dateMiliSeconds":1640935734000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"mac で podman","contentSnippet":"Docker Desktop 代替シリーズ第三部、Podman です。(第一部 minkikube 編、第二部 Lima + nerdctl 編) Podman については Red Hat さんのブログが大変参考になります。 今回の Docker 社によるライセンス周りの変更が発表されるよりだいぶ前に How to replace Docker with Podman on a Mac という記事もありました。Podman は Linux 上で稼働する daemon とそのクライアントという構成となっており、Windows と Mac にはクライアントしかありません。この時の記事では Vagrant を使って Linux 仮想サーバーを起動させるという処理を macOS app にしようという内容でした。現在は podman machine というサブコマンドで仮想サーバーを起動させることができるようになっているようです。それでは試してみましょう。 podman のインストール $ brew install podman これにより podman, podman-remote, […]mac で podman first appeared on 1Q77.","link":"https://blog.1q77.com/2021/09/podman-on-mac/","isoDate":"2021-09-23T14:20:51.000Z","dateMiliSeconds":1632406851000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Lima で nerdctl","contentSnippet":"Docker Desktop の代わりに docker cli + Minikube ってのを試しただけど、Kubernetes は docker を非推奨にしてるし、kubernetes は不要な場合は無駄が多いしなあ… ってことで lima も試してみる。 (2021/01/05 追記: Docker on Lima も見てね) Lima は自動のファイル共有、ポートフォワード、containerd をサポートした仮想マシンを提供してくれるツール。Windows subsystem for Linux の mac 版とも言えるとドキュメントに書かれている。 今回は Intel Mac 環境で試しています。M1 Mac の場合は qemu に patch が必要みたいです。 Lima のインストール Homebrew でインストール $ brew install lima Lima Virtual Machine の起動 limactl コマンドを使う $ […]Lima で nerdctl first appeared on 1Q77.","link":"https://blog.1q77.com/2021/09/lima/","isoDate":"2021-09-19T16:58:10.000Z","dateMiliSeconds":1632070690000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Docker Desktop の代わりに Minikube を使ってみる","contentSnippet":"Docker のおかげで今の便利なコンテナがあります、ありがとうございます。でもどうなるのかやってみたかったんです。 Goodbye Docker Desktop, Hello Minikube! を参考に試してみます。 環境は Intel Mac の Big Sur です。Mac で docker を使うには docker daemon を稼働させるための Linux 仮想マシンが必要で、Docker Desktop はそこをうまいことやってくれています。その docker daemon の稼働する仮想マシンとして minikube 用のものを活用しようという話です。 Docker Desktop の Uninstall アプリとしては消したけどでっかい VM のイメージは残ってるから、もう再度インストールすることもないという状況になったら削除しよう。 ~/Library/Containers/com.docker.docker/Data/vms/0/data/Docker.raw Docker CLI の Install $ brew install docker 必要なら docker-compose も。ただし、今時は docker のサブコマンドになっているので docker-compose コマンドをインストールする必要はなさそう。 $ brew […]Docker Desktop の代わりに Minikube を使ってみる first appeared on 1Q77.","link":"https://blog.1q77.com/2021/09/replace-docker-desktop-with-minikube/","isoDate":"2021-09-18T16:07:27.000Z","dateMiliSeconds":1631981247000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Google 認証でクライアント証明書発行のセルフサービス化","contentSnippet":"以前、caddy について調べてて発見した smallstep でクライアント証明書発行を便利にできないかなということで調査です。(Hashicorp Vault でもできるっぽいけど用途的にわざわざクラスタ組むの面倒だなあって) Connect your identity provider and issue X.509 certificates for user authentication to services というドキュメントを参考に進めます。前提に Getting Started でセットアップした step-ca が起動していることとあるので、まずはこちらから。 Getting Started まずは、step と step-ca コマンドをインストール。Homebrew でもインストールできます。 CA の初期化 ファイルの保存場所をカレントディレクトリ配下にしておきます。デフォルトでは $HOME/.step となっています。 export STEPPATH=./step step ca init で root と intermediate 証明書を作成します。$STEPPATH/config/defaults.json と $STEPPATH/config/ca.json というファイルも生成されます。defaults.json の方は step コマンド用で、ca.json は CA […]Google 認証でクライアント証明書発行のセルフサービス化 first appeared on 1Q77.","link":"https://blog.1q77.com/2021/04/step-ca-with-google-oidc/","isoDate":"2021-04-10T14:44:13.000Z","dateMiliSeconds":1618065853000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Terraform でカスタム provider を使うための dev_overrides 設定","contentSnippet":"healthchecks.io が大変便利なので Self-hosted なサーバーを用意して、設定を terraform で管理したいなあということがありまして、terraform-provider-healthchecksio の接続先サーバーを指定可能にしようと思いました。provider のコードを編集して build するところまではすぐに出来たのですが、このバイナリを terraform からどうやって使うのかな？でハマったのでメモです。 Terraform 0.14 から ~/.terraformrc の provider_installation 内に dev_overrides という設定を書くことができるようになっていました。(Development Overrides for Provider Developers) ~/.terraformrc に次の設定を入れておけば、指定したディレクトリ直下にあるバイナリを使ってくれます。場所はどこでも良いですが、ここで go build すればバイナリが生成されるのでそのまま使えるようにここを指定してみました。 provider_installation { dev_overrides { \"kristofferahl/healthchecksio\" = \"/Users/teraoka/ghq/github.com/yteraoka/terraform-provider-healthchecksio\" } direct {} } terraform init では次のような Warning が表示されます。 % terraform init Initializing the backend... Initializing provider plugins... […]Terraform でカスタム provider を使うための dev_overrides 設定 first appeared on 1Q77.","link":"https://blog.1q77.com/2021/04/terraform-dev_overrides/","isoDate":"2021-04-04T04:03:47.000Z","dateMiliSeconds":1617509027000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"glibc, musl libc, go の resolver の違い","contentSnippet":"先日、resolv.conf で timeout を調整したいなと思うことがありました、しかし、Docker だの Kubernetes だのといった時代です。Linux しか使っていなかったとしても resolver が glibc のそれだとは限らないわけですね。そこで glibc, musl libc (alpine のやつです), go の resolver の違いを調べてみました。環境それぞれ docker container を使い、問い合わせの状況は tcpdump で確認しました。macOS Catalina (10.15.7)docker desktop 3.0.3 (51017)container image はalpine:3.12.3 / musl-1.1.24-r10debian:buster-20201209 (10.7) / glibc 2.28-10Go は mac 上で 1.15.6 を使い、クロスコンパイルして上記の alpine 上で実行しました。クロスコンパイルなのでデフォルトで CGO_ENABLED=0 のはずですが、明示してビルドしました。resolv.conf の options などの対応状況resolv.confの例nameserver 8.8.8.8nameserver 8.8.4.4search default.svc.cluster.local svc.cluster.local cluster.local asia-northeast1-b.c.project-id.internal c.project-id.internal google.internaloptions ndots:5 timeout:5 attempts:2resolv.conf の options にはいろいろありますが、今回の 3 つの resolver 全てで対応されているのはndotstimeoutattemptsの3つでした。 nameserver は勿論ですが search にも対応しています。昔の musl libc は search (domain 補完) には対応していませんでしたが 1.1.13 から対応していました。nameserver は複数回指定可能ですが、使われるのは3つまでです。4つ以上指定しても3つまでしか使われません。それでは機能の違いを見ていきましょうndotsまずは ndots です、Kubernetes を使うようになってはじめて知りました。指定するのは問い合わせるドメインに含まれる dot (.) の数です、Kubernetes のデフォルト設定では 5 (ndots:5) となっています。例えば ping www.google.com とした場合、dot の数は 2 です。dot の数が ndots 以上の場合dot の数が ndots で指定した以上であればそれは FQDN であろうと判断し、search で指定したドメインを補完せずに DNS サーバーに問い合わせます。ここで、NXDOMAIN (そんなドメイン存在しないよ) が返ってきたら glibc と go は search で指定したドメインを補完して問い合わせます。しかし、musl libc は補完した問い合わせを行わないでそんなドメインは存在しないよということで終了してしまいます。その昔、musl はそもそもドメイン補完に対応していなかったので当時から使っている人は常に FQDN で指定しているでしょうから問題ないでしょうが、知らないで ndots を小さくすると解決できるはずのドメインが解決できないかもしれません。dot の数が ndots 未満の場合dot の数が ndots 未満であった場合はまず search で指定した複数のドメインを順番に補完して問い合わせます、見つかるまで繰り返すため（これの上限を確認するの忘れた）、順序は重要ですかね。ただ、順序以上に ndots で指定する数が重要です、5 というのは結構多いです。Kubernetes でデフォルト設定だと service.namespace.svc.cluster.local という FQDN を指定したとしても 4 なわけです。search に並んでいる全てのドメインの補完を試した後に service.namespace.svc.cluster.local をそのまま問い合わせてやっと欲しい結果が得られるわけです、DNS サーバー側にネガティブキャッシュがあるとはいえ、DNS サーバーとの通信はその回数行われるわけですから、かなりの無駄です。GKE のデフォルト設定では6つもドメインが列挙されています。EKS は4つ。ndots を小さくして名前解決ができないドメインが発生してしまうよりも、無駄な問い合わせが増えたとしても名前解決できる値がデフォルトなっているわけですね。ndots はチューニングの余地がありそうです。ちなみに、末尾に . をつけた場合はそれは FQDN だぞということを意味するので ping www.google.com. などとした場合はどの resolver でもドメインの補完は行いません。nameserver複数サーバーを指定した場合の扱いが glibc と musl で大きく異なるため、まず説明しておきます。glibc と Go は一つ目のサーバーに問い合わせて、timeout 秒待っても応答がない場合に、次のサーバーに問い合わせます。musl libc は複数の nameserver に同時にリクエストを送って最初に返ってきた応答を使います。3つのネームサーバーが指定されていたら3倍の問い合わせが発生してしまうわけですね。timeout と attemptstimeout と attempts は関連しているため一緒に扱います。また、nameserver の数にもよるためそれごとにまとめます。nameserver が1つの場合glibc と Go は問い合わせを投げては timeout 秒待ち、応答がなければ再度問い合わせて、合計 attempts 回問い合わせます。musl libc は timeout 秒を attempts で割った秒数をそれぞれの問い合わせで待ちます。つまり、リトライを含めた合計で最大 timeout 秒待つということです。 timeout:5 attempts:2 というデフォルト設定では 2.5 秒ずつ待ちます。nameserver が2つの場合glibc と Go は1つ目の nameserver に問い合わせて timeout 秒待ち、次は2つ目の nameserver に問い合わせます。これが attempts の1回分です。timeout:5 attempts:2 というデフォルト設定ではそれぞれの nameserver に対して2回ずつ、合計4回問い合わせます。その都度5秒待ちます。musl libc は nameserver の項で書いた通り、複数の nameserver に対して同時に問い合わせるため、nameserver が1つの場合と同じです。nameserver が3つの場合nameserver が3つになると glibc の timeout の振る舞いが少し変わります。1つ目の nameserver に対しては timeout で指定した秒数待ちますが、2つ目は timeout で指定したのよりも短くなり、3つ目は timeout で指定したものより長くなります。timeout:5 の場合は5秒、3秒、6秒となります。このセットを attempts 回繰り返します。Go は常に timeout 秒待ちます。musl libc はこれまでと同じです。timeout 時のドメイン補完何度問い合わせても timeout するような場合はもういろいろダメなのであまり重要ではないですが、動作に違いがあったので書いておきます。timeout が続く状態で、2つ目以降の search ドメインを補完するのかどうか、glibc は2つ目以降のドメイン補完は行いませんが、補完なしの問い合わせを行います。Go は律儀に全てのドメイン補完を試しますし、補完なしでの問い合わせもします。musl libc は1つ目の補完の問い合わせは行いますが、それで終わりです。補完なしの問い合わせもありません。Go の動作確認で使ったコードpackage mainimport (    \"context\"    \"net\"    \"os\"    \"log\")func main() {    ctx := context.Background()    resolver := \u0026net.Resolver{}    for _, v := range os.Args[1:] {        log.Printf(\"Resolving %s\\n\", v)        names, err := resolver.LookupHost(ctx, v)        if err != nil {            log.Fatal(err)        }        for _, name := range names {            log.Printf(\"%s\\n\", name)        }    }}ndots:5 の凶悪さを可視化する文章でつらつら書いても分かりづらいので、 GKE 環境で storage.googleapis.com にアクセスしようとした場合にどんな問い合わせが発生するかを見てみましょう。default namespace に立てた Pod で ping storage.googleapis.com を実行した際の tcpdump の出力を加工しました。(横幅を抑えるために)Cloud Storage の API にアクセスしようとする度にこの数の問い合わせが発生すると思うとゾッとしますね。マイクロサービスで Keep-Alive などをしていない場合は大量の内部通信でもこれが発生しているかもしれません。JVM などのように DNS のキャッシュをしてくれれば良いですけどね。ndots:2 として常に FQDN で指定するということが徹底できると良いのかな。14:53:34.706909 ▶︎ A? storage.googleapis.com.default.svc.cluster.local. (66)14:53:34.707130 ▶︎ AAAA? storage.googleapis.com.default.svc.cluster.local. (66)14:53:34.708881 ◁ NXDomain 0/1/0 (159)14:53:34.708940 ◁ NXDomain 0/1/0 (159)14:53:34.709051 ▶︎ A? storage.googleapis.com.svc.cluster.local. (58)14:53:34.709133 ▶︎ AAAA? storage.googleapis.com.svc.cluster.local. (58)14:53:34.709615 ◁ NXDomain 0/1/0 (151)14:53:34.709689 ◁ NXDomain 0/1/0 (151)14:53:34.709771 ▶︎ A? storage.googleapis.com.cluster.local. (54)14:53:34.709816 ▶︎ AAAA? storage.googleapis.com.cluster.local. (54)14:53:34.712211 ◁ NXDomain 0/1/0 (147)14:53:34.712280 ◁ NXDomain 0/1/0 (147)14:53:34.712387 ▶︎ A? storage.googleapis.com.asia-northeast1-b.c.my-project-id.internal. (83)14:53:34.712479 ▶︎ AAAA? storage.googleapis.com.asia-northeast1-b.c.my-project-id.internal. (83)14:53:34.716561 ◁ NXDomain 0/1/0 (189)14:53:34.716623 ◁ NXDomain 0/1/0 (189)14:53:34.716718 ▶︎ A? storage.googleapis.com.c.my-project-id.internal. (65)14:53:34.716760 ▶︎ AAAA? storage.googleapis.com.c.my-project-id.internal. (65)14:53:34.719891 ◁ NXDomain 0/1/0 (162)14:53:34.720191 ◁ NXDomain 0/1/0 (162)14:53:34.720304 ▶︎ A? storage.googleapis.com.google.internal. (56)14:53:34.720390 ▶︎ AAAA? storage.googleapis.com.google.internal. (56)14:53:34.724145 ◁ NXDomain 0/1/0 (145)14:53:34.724352 ◁ NXDomain 0/1/0 (145)14:53:34.724458 ▶︎ A? storage.googleapis.com. (40)14:53:34.724500 ▶︎ AAAA? storage.googleapis.com. (40)14:53:34.726930 ◁ 4/0/0 AAAA 2404:6800:4004:813::2010, AAAA 2404:6800:4004:81c::2010, AAAA 2404:6800:4004:81d::2010, AAAA 2404:6800:4004:81e::2010 (152)14:53:34.726957 ◁ 16/0/0 A 172.217.161.80, A 172.217.175.16, A 172.217.175.48, A 172.217.175.80, A 172.217.175.112, A 216.58.197.144, A 172.217.25.208, A 172.217.25.240, A 172.217.26.48, A 172.217.31.176, A 172.217.161.48, A 172.217.174.112, A 172.217.175.240, A 216.58.220.112, A 216.58.197.208, A 216.58.197.240 (296)","link":"https://qiita.com/yteraoka/items/e74e8bf24f72f7ed5f15","isoDate":"2021-01-13T14:04:22.000Z","dateMiliSeconds":1610546662000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Bitwardenで自分専用パスワードマネージャーサーバー構築","contentSnippet":"Qiita では初の Bitwarden 記事っぽい パスワードマネージャーサービスは有償、無償でいくつかありますが 1password.com がメジャーどころでしょうか。家族プランがあって良さげです。エンタープライズでは evidian とかありますね。パスワードの使い回しで多くの人が被害にあっている昨今ではこういったツールは欠かせません。私は今のところ keepass + dropbox でデバイスまたぎで使ってます。1password の家族プランで家族全体を守りたいなと思ってるところ。さて、この記事では有償だろうと知らない人にパスワードは預けられないぜという方のために Bitwarden を使って自分専用パスワードマネージャーサーバーを構築してみたいと思います。でもそんな場合はソースコードもくまなくチェックですよね...Bitwarden も無料のサービスとして提供されてるので普通の人はそれを。Since all of your data is fully encrypted before it ever leaves your device, only you have access to it. Not even the team at bitwarden can read your data, even if we wanted to. Your data is sealed with end-to-end AES-256 bit encryption, salted hashing, and PBKDF2 SHA-256試す前は GCP の永久無料サーバー f1-micro で使えたりするんだろうか？と淡い期待を抱いていたのですが最低2GBメモリが必要だよって書いてありました...さらに、なんと .NET と SQLServer という組み合わせ... 触ったことないぞ .NET も SQLServer もまったくわかりませんが https://help.bitwarden.com/article/install-on-premise/ に書いてあるとおりにすればきっとできるはず。幸いにも時代は Docker です。docker さえ使えれば大丈夫。.NET も SQL Server も Linux で動く あ、Let's Encrypt の証明書で https 対応するのでドメインが必要ですSYSTEM REQUIREMENTSCPU: Dual core @ 2GHz or fasterMemory: 2GB of RAM or moreStorage: 10GB or moreDocker: Engine 1.8+ and Compose 1.17.1+Docker の使えるサーバーの準備DigitalOcean で 4GB メモリのインスタンスを使いました（doctl 便利）。doctl compute droplet create testsv01 \\  --image docker-16-04 \\  --region sgp1 \\  --size 4gb \\  --ssh-keys 12345678DigitalOcean は最初は root でログインすることになるので以降の手順では sudo とか出てきませんDNS 設定はそれぞれの DNS サーバーの手順で。セットアップまずはセットアップ用スクリプトのダウンロードcurl -s -o bitwarden.sh \\    https://raw.githubusercontent.com/bitwarden/core/master/scripts/bitwarden.sh \\    \u0026\u0026 chmod u+x bitwarden.shインストーラ実行./bitwarden.sh install後は聞かれたことに答えていけば大丈夫。スクリプトの置かれたディレクトリに bwdata というサブディレクトリが作成され、これからの設定情報は全部その配下に保存されます証明書取得まずはこの質問、ドメインと Let's Encrypt 用のメールアドレスさえ指定すれば証明書取得は自動でやってくれます。AMCE の http-01 なのでもちろん Let'e Encrypt サーバー側からアクセスできるとことに無いとダメです。EV 証明書持ってるからそれを使いたいんだよという場合は n を選択してファイルを指定の場所に配置します(!) Enter the domain name for your bitwarden instance (ex. bitwarden.company.com):(!) Do you want to use Let's Encrypt to generate a free SSL certificate? (y/n):(!) Enter your email address (Let's Encrypt will send you certificate expiration reminders):certbot/certbot docker image を使って http-01 で Let's Encrypt の証明書が取得され bwdata/letsencrypt 配下にもろもろ保存されますinstallation id 設定(!) Enter your installation id (get it at https://bitwarden.com/host): (!) Enter your installation key: https://bitwarden.com/host で installtion id, installlation key を取得する必要がありますYou should use a unique id and key for each bitwarden installationと書かれています。メールアドレス入力して Submit したら id と key が表示されますライセンスの必要な機能があるからかなNetwork と Push 通知の設定Network 設定は環境に合わせて選んでください。Push 通知を使えばスマホアプリなどへ通知してすぐに同期されるようにできるみたいです。サーバーから https://push.bitwarden.com へアクセス出来る必要があります。(!) Do you want to use the default ports for HTTP (80) and HTTPS (443)? (y/n): (!) Is your installation behind a reverse proxy? (y/n): (!) Do you want to use push notifications? (y/n): これに答えたらBuilding nginx config.Building docker environment files.Building docker environment override files.Building app settings.Building FIDO U2F app id.Building docker-compose.yml.Setup completeSetup complete!! あっという間です環境変数設定終わりかと思ったのですがもうちょっと設定がありましたbwdata/env/global.override.env の環境変数を環境に合わせて書き換えます書き換えるのはこのあたりglobalSettings__yubico__clientId=REPLACEglobalSettings__yubico__key=REPLACEglobalSettings__mail__replyToEmail=no-reply@example.comglobalSettings__mail__smtp__host=REPLACEglobalSettings__mail__smtp__username=REPLACEglobalSettings__mail__smtp__password=REPLACEglobalSettings__mail__smtp__ssl=trueglobalSettings__mail__smtp__port=587globalSettings__mail__smtp__useDefaultCredentials=falseglobalSettings__disableUserRegistration=false同じディレクトリに bwdata/env/mssql.override.env というファイルもあり、これには SQL Server のパスワードなどが書かれています起動設定は完了したので後は起動するだけ（のはず）./bitwarden.sh start(省略)Creating network \"docker_default\" with the default driverCreating nginx ...Creating web ...Creating attachments ...Creating api ...Creating icons ...Creating nginxCreating attachmentsCreating identity ...Creating apiCreating webCreating mssql ...Creating iconsCreating mssqlCreating mssql ... doneTotal reclaimed space: 0B1.15.1: Pulling from bitwarden/setupDigest: sha256:9d4fd9ea670b2ee84eeb19278d6a80bee10f5a50796b3a5f11af1dbdd4b052cfStatus: Image is up to date for bitwarden/setup:1.15.1bitwarden is up and running!===================================================visit https://warden.example.comto update, run './bitwarden.sh updateself' and then './bitwarden.sh update'更新も簡単にできそうな感じdocker ps すると次のようになってますnginx --\u003e web --\u003e api --\u003e mssql っぽいかな# docker psCONTAINER ID        IMAGE                          COMMAND             CREATED             STATUS              PORTS                                      NAMESd53c1771b26d        bitwarden/mssql:1.15.1         \"/entrypoint.sh\"    2 hours ago         Up 2 hours          1433/tcp                                   mssqlaced30eab718        bitwarden/web:1.21.0           \"/entrypoint.sh\"    2 hours ago         Up 2 hours          80/tcp                                     web4b1cb2705ed3        bitwarden/icons:1.15.1         \"/entrypoint.sh\"    2 hours ago         Up 2 hours          80/tcp                                     iconsfd6f0a7ceffe        bitwarden/api:1.15.1           \"/entrypoint.sh\"    2 hours ago         Up 2 hours          80/tcp                                     apida0f0d6bd5d9        bitwarden/identity:1.15.1      \"/entrypoint.sh\"    2 hours ago         Up 2 hours          80/tcp                                     identity335623ec380f        bitwarden/attachments:1.15.1   \"/entrypoint.sh\"    2 hours ago         Up 2 hours          80/tcp                                     attachmentsbfda1f95a168        bitwarden/nginx:1.15.1         \"/entrypoint.sh\"    2 hours ago         Up 2 hours          0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp   nginxブラウザでアクセスしてみる動いてる！！まずは「Create a new account」からアカウントを作成しますが、ここでエラーが発生。作成に失敗しました。docker logs api とか docker logs mssql とかしてログをあさってみると、vault という名前のデータベースに接続できないらしいことがわかりましたSQL Server に接続してみると...# docker exec -it mssql /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P パスワード1\u003e select name from sys.databases;2\u003e goname--------------------------------------------------------------------------------------------------------------------------------mastertempdbmodelmsdb(4 rows affected)vault なんてデータベースは存在しない...（パスワードは先に書いたように bwdata/env/mssql.override.env にあります）bitwarden.sh や bwdata/scripts/run.sh を眺めてみると updatedb ってのを実行すればなんかできそうな気がする# ./bitwarden.sh updatedb _     _ _                         _| |__ (_) |___      ____ _ _ __ __| | ___ _ __| '_ \\| | __\\ \\ /\\ / / _` | '__/ _` |/ _ \\ '_ \\| |_) | | |_ \\ V  V / (_| | | | (_| |  __/ | | ||_.__/|_|\\__| \\_/\\_/ \\__,_|_|  \\__,_|\\___|_| |_|Open source password management solutionsCopyright 2015-2017, 8bit Solutions LLChttps://bitwarden.com, https://github.com/bitwarden===================================================Docker version 17.09.0-ce, build afdb6d4docker-compose version 1.13.0, build 1719ceb1.15.1: Pulling from bitwarden/setupDigest: sha256:9d4fd9ea670b2ee84eeb19278d6a80bee10f5a50796b3a5f11af1dbdd4b052cfStatus: Image is up to date for bitwarden/setup:1.15.1Migrating database.Beginning transactionBeginning database upgradeFetching list of already executed scripts.The [dbo].[Migration] table could not be found. The database is assumed to be at version 0.Executing SQL Server script 'Bit.Setup.DbScripts.2017-08-19_00_InitialSetup.sql'Creating the [dbo].[Migration] tableThe [dbo].[Migration] table has been createdExecuting SQL Server script 'Bit.Setup.DbScripts.2017-08-22_00_LicenseCheckScripts.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-08-30_00_CollectionWriteOnly.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-09-06_00_CipherDetails.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-09-08_00_OrgUserCounts.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-10-25_00_OrgUserUpdates.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-11-06_00_FamilyPlanAdjustments.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-11-13_00_IndexTuning.sql'Executing SQL Server script 'Bit.Setup.DbScripts.2017-11-24_00_UpdateProcs.sql'Upgrade successfulMigration successful.Database update completeそれらしいことをしてそうな出力再度データベースを確認してみるroot@testsv01:~# docker exec -it mssql /opt/mssql-tools/bin/sqlcmd -S localhost -U SA -P パスワード1\u003e select name from sys.databases;2\u003e goname--------------------------------------------------------------------------------------------------------------------------------mastertempdbmodelmsdbvault(5 rows affected)1\u003e use vault2\u003e ;3\u003e goChanged database context to 'vault'.1\u003e select name from sysobjects where xtype = 'U';2\u003e goname--------------------------------------------------------------------------------------------------------------------------------CipherCollectionCollectionCipherCollectionGroupCollectionUserDeviceFolderGrantGroupGroupUserInstallationOrganizationOrganizationUserU2fUserMigration(16 rows affected)1\u003evault データベースできてる！！アカウント作成できました 後から気付いたのですが、ちゃんと次の手順として ./bitwarden.sh updatedb が書かれてました。単に私が早まっただけでした ちゃんとドキュメント読めと...ログインしてみるこれがアカウント作成直後にログインした画面ですSMTP 設定が正しく行われていればメールを送ってメールアドレスの確認ができます。SMTP 設定に間違いがあった場合は bwdata/env/mssql.override.env を書き換えて ./bitwarden.sh restart で変更を反映できます。 docker inspect で環境変数が反映されてることを確認できます。Types登録する情報のタイプとしてLoginCardIdentitySecure Noteの4種類があります。それぞれの入力項目は次のようになっています。これらを任意のフォルダに分けて保存できたり、お気に入りに登録したりできるようです。LoginCardIdentitySecure NoteAppsブラウザ用アドオン、スマホ用アプリ、デスクトップ用アプリと各プラットフォームで使えるようになっています。その他Organization 設定をすれば組織内でパスワードを共有するとかいう使い方もできるっぽいですでも有償かな？ Licensing for paid featuresLDAP や ActiveDirectory との連携もできるっぽいです https://help.bitwarden.com/article/ldap-directory/細かいところまで全然試せていませんが一般的なパスワードマネージャーの機能は備えているようですHelp Center参考資料SQL Server まったくわからなかったので コマンドラインでSQL Serverを使う最低限のメモ に助けてもらいました","link":"https://qiita.com/yteraoka/items/41faf5d183b997b66625","isoDate":"2017-12-22T03:02:36.000Z","dateMiliSeconds":1513911756000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"STNSの独自サーバーを書いてみた","contentSnippet":"オンプレサーバーに加え、複数のクラウドサービスや VPS を使うようになってくると Linux サーバーのアカウント管理が非常に面倒になってきました。（全てがコンテナと SaaS になるのはまだ先）そこで以前から気になっていた STNS を試してみることにしました。すべてを VPN で接続しているわけでもなく、IPアドレス制限で頑張るのもクラウドらしくなく LDAP 公開は出来ない。STNS はクライアント証明書も使えるし、ユーザーのパスワードを登録しなければ最悪取られてもアカウント名、uid, gid, 公開鍵程度なので公開しても大丈夫そう。STNSサーバー が公開されていますが、サーバーグループごとにログインできる人をコントロールしったかったため独自のサーバーを書くことにしました。STNS は HTTP で https://stns.jp/en/interface に沿った JSON を返せば良いだけなのでサーバーを書くのも難しくありませんでした。作ったプロトタイプサーバー管理 UI としてDjango の Adminapp が便利そうだったので Django に挑戦しましたユーザーは複数の公開鍵を持つユーザーは複数の unix group に所属できるユーザーには有効期限を設定可能ユーザーは有効・無効フラグをもつサーバーの識別は Basic 認証のIDまたはクライアント証明書のCommonNameを使うサーバーとユーザーがそれぞれ1つ以上のロールを持ち、これでどのユーザーがどのサーバーにログインできるのかを制御するサーバーの起動CentOS 7 でとりあえず起動sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpmsudo yum -y install git gcc python36u python36u-devel python36u-pipgit clone https://github.com/yteraoka/morion.gitcd moriongit checkout developpython3.6 -m venv venv. venv/bin/activatepip install -r requirements.txtcd morionpython manage.py migratepython manage.py createsuperuserpython manage.py runserver 0.0.0.0:8000http://server:8000/admin/ にアクセスして createsuperuser で作成したアカウントでログインして各種リソースを作成する。クライアント証明書によるサーバー識別は未実装なので Basic 認証を使う、サーバーリソースにパスワードを設定する。CentOS 7 でのクライアントセットアップpackage のインストールcurl -fsSL https://repo.stns.jp/scripts/yum-repo.sh | shsudo yum -y install stns libnss-stns libpam-stns nscdlibnss_stns.conf の編集sudoedit /etc/stns/libnss_stns.conf次の内容で package に含まれており、環境に合わせて編集するapi_end_point = [\"http://localhost:1104/v3\"]# user = \"basic_user\"# password = \"basic_password\"# wrapper_path = \"/usr/local/bin/stns-query-wrapper\"# chain_ssh_wrapper = \"/usr/libexec/openssh/ssh-ldap-wrapper\"# ssl_verify = true# request_timeout = 3# http_proxy = \"http://example.com:8080\"# [request_header]# x-api-key = \"token\"ログイン時に Home directory を作成するための設定echo 'session required pam_mkhomedir.so skel=/etc/skel/ umask=0022' \\   | sudo bash -c \"cat \u003e\u003e /etc/pam.d/sshd\"OpenSSH Server が Public key を STNS サーバーから取得できるようにするsudo sed -i -r \\  -e 's@^#?(AuthorizedKeysCommand) .*@\\1 /usr/lib/stns/stns-key-wrapper@' \\  -e 's@^#?(AuthorizedKeysCommandUser) .*@\\1 root@' \\   /etc/ssh/sshd_configsudo systemctl restart sshdnsswitch.conf 設定 (passwd, shadow, group に stns を追加)sudoedit /etc/nsswitch.confpasswd:     files sss stnsshadow:     files sss stnsgroup:      files sss stnsnscd.conf 設定 (毎回外部へ問い合わせていては遅いのでキャッシュさせる)sudoedit /etc/nscd.confsudo systemctl enable nscdsudo systemctl start nscd(TTL の動作が期待と違ってよくわからん・・・)SELinux 対応SELinux が enforcing だとうまく動かないため、関係するドメインを permissive にするsudo yum -y install policycoreutils-pythonsudo semanage permissive -a sshd_tsudo semanage permissive -a chkpwd_tsudo semanage permissive -a nscd_t試してみて便利、使えそう履歴などを残そうとすると Web UI を持たせるよりも TOML や YAML で Git 管理の方が良かったオリジナルの STNS サーバーはコレで TOML ファイル管理更新頻度的にも Web UI は必要じゃなかったサーバー書き直して実用したいSlidehttps://www.slideshare.net/yteraoka1/stns追記ユーザー（グループ）単位でのアクセス制限については sshd_config の AllowGroups を使うことで解決できるため、これを理由に独自サーバーを実装する必要はなかったと判明しました。","link":"https://qiita.com/yteraoka/items/77b329606f44b4e19b92","isoDate":"2017-12-11T05:03:26.000Z","dateMiliSeconds":1512968606000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"CentOS7でPostgreSQL 10のPacemakerクラスタ作成","contentSnippet":"クラウドのマネージドサービスが楽でいいよなぁと思いながら CentOS 7 + PostgreSQL 10 のサーバー3台で pacemaker を使ったクラスタを作成してみますTL;DRVagrant で試すためのファイルが https://github.com/yteraoka/postgresql10-pacemaker にあります改善点など issue や PR をお願いします$ git clone https://github.com/yteraoka/postgresql10-pacemaker.git$ cd postgresql10-pacemaker$ ssh-keygen -t rsa -b 2048 -P \"\" id_rsa$ vagrant up3台起動したら db1 にログインします$ vagrant ssh db1init.sh を実行するとクラスタが作成されます$ bash /vagrant/init.shGoCardless の Postmortem今日たまたま Nuzzel (最近のお気に入りアプリです) で Incident review: API and Dashboard outage on 10 October 2017 という記事に出会いましたGoCardless というサイトで今年10月10日に発生した PostgreSQL クラスタのインシデントに対する postmortem ですこの記事で作成するものとほぼ同じ構成のクラスタで発生したインシデントで大変興味深いMaster のディスクアレイで3本のディスク同時故障ってホントかよって思わなくもないが、私もファームウエアの問題でバタバタHDDが止まってRAID6が停止した経験があるからな・・・Master のファイルシステムにアクセスできなくなった影響で Sync Standby のプロセスが一部クラッシュし、再起動してしまった影響で Master への昇格がうまくいかなかったようだ。それでも Master をシャットダウンすれば残りの2台でなんとかクラスタ組んだまま復旧できそうなものだがうまくいかなかったらしいそして、今は以前から検討していた Zero-downtime upgrade が可能な構成になっているそうだ。Youtube (Zero-downtime Postgres Upgrades) にプレゼンが公開されている手前に pgBouncer を入れて、切替時に一瞬 pgBouncer で待たせるっぽい最近 pgBouncer の事例をちょいちょい耳にしますね、PostgreSQL への接続数を減らす効果もある一方、server side prepared statement が使えなくなるデメリットも有るGitLab EE の例 ではどこで使われるかわからない、クラウドかもしれないしということでか repmgrd + Consul + pgBouncer でしたZero-downtime つながりでこれもリンクをはっておこう Near-Zero Downtime Automated Upgrades of PostgreSQL Clusters in Cloud構成説明やっとこの記事の本題へ3台の CentOS 7 サーバーPGDG の YUM Repository から PostgreSQL 10.x をインストールResource Agent は PostgreSQL 10 対応のために GitHub から最新のものを取得3台がそれぞれ Master、Sync Standby、ASync Standby となるMaster が Master 用 VIP をもつSync Standby が Replica 用 VIP をもつ（ReadOnly アクセス用）Standby が2台とも Down している場合は Master がこれも持つReplication Slot は使用しないfailover 時に新 Master に slot が存在せず（replicateされない）、Slave が接続できないSlave が突然死ぬと Master に WAL が残り続けて困る？ Archive Log は rsync over SSH で自分以外の2台へ送るQuorum、Stonith は無効replication traffic 用ネットワークを別途作らない（作っても良いけど）図解初期状態確認[vagrant@db1 ~]$ sudo pcs status --fullCluster name: pg10Stack: corosyncCurrent DC: db2 (2) (version 1.1.16-12.el7_4.5-94ff4df) - partition with quorumLast updated: Wed Dec  6 16:41:13 2017Last change: Wed Dec  6 16:41:02 2017 by root via crm_attribute on db13 nodes configured8 resources configuredOnline: [ db1 (1) db2 (2) db3 (3) ]Full list of resources: master-vip     (ocf::heartbeat:IPaddr2):       Started db1 replica-vip    (ocf::heartbeat:IPaddr2):       Started db2 Clone Set: ping-clone [ping]     ping       (ocf::pacemaker:ping):  Started db1     ping       (ocf::pacemaker:ping):  Started db3     ping       (ocf::pacemaker:ping):  Started db2     Started: [ db1 db2 db3 ] Master/Slave Set: pgsql-master [pgsql]     pgsql      (ocf::heartbeat:pgsql10):       Master db1     pgsql      (ocf::heartbeat:pgsql10):       Slave db3     pgsql      (ocf::heartbeat:pgsql10):       Slave db2     Masters: [ db1 ]     Slaves: [ db2 db3 ]Node Attributes:* Node db1 (1):    + master-pgsql                      : 1000    + pgsql-data-status                 : LATEST    + pgsql-master-baseline             : 0000000005000098    + pgsql-receiver-status             : normal (master)    + pgsql-status                      : PRI    + pingd                             : 100* Node db2 (2):    + master-pgsql                      : 100    + pgsql-data-status                 : STREAMING|SYNC    + pgsql-receiver-status             : normal    + pgsql-status                      : HS:sync    + pingd                             : 100* Node db3 (3):    + master-pgsql                      : -INFINITY    + pgsql-data-status                 : STREAMING|ASYNC    + pgsql-receiver-status             : normal    + pgsql-status                      : HS:async    + pingd                             : 100Migration Summary:* Node db1 (1):* Node db3 (3):* Node db2 (2):PCSD Status:  db2: Online  db3: Online  db1: OnlineDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[vagrant@db1 ~]$db1 を止めてみるVirtualBox マネージャから電源オフ[vagrant@db2 ~]$ sudo pcs status --fullCluster name: pg10Stack: corosyncCurrent DC: db2 (2) (version 1.1.16-12.el7_4.5-94ff4df) - partition with quorumLast updated: Wed Dec  6 16:45:30 2017Last change: Wed Dec  6 16:45:15 2017 by root via crm_attribute on db23 nodes configured8 resources configuredOnline: [ db2 (2) db3 (3) ]OFFLINE: [ db1 (1) ]Full list of resources: master-vip     (ocf::heartbeat:IPaddr2):       Started db2 replica-vip    (ocf::heartbeat:IPaddr2):       Started db3 Clone Set: ping-clone [ping]     ping       (ocf::pacemaker:ping):  Started db3     ping       (ocf::pacemaker:ping):  Started db2     ping       (ocf::pacemaker:ping):  Stopped     Started: [ db2 db3 ]     Stopped: [ db1 ] Master/Slave Set: pgsql-master [pgsql]     pgsql      (ocf::heartbeat:pgsql10):       Slave db3     pgsql      (ocf::heartbeat:pgsql10):       Master db2     pgsql      (ocf::heartbeat:pgsql10):       Stopped     Masters: [ db2 ]     Slaves: [ db3 ]     Stopped: [ db1 ]Node Attributes:* Node db2 (2):    + master-pgsql                      : 1000    + pgsql-data-status                 : LATEST    + pgsql-master-baseline             : 00000000062ACA00    + pgsql-receiver-status             : normal (master)    + pgsql-status                      : PRI    + pingd                             : 100* Node db3 (3):    + master-pgsql                      : 100    + pgsql-data-status                 : STREAMING|SYNC    + pgsql-receiver-status             : normal    + pgsql-status                      : HS:sync    + pingd                             : 100Migration Summary:* Node db3 (3):* Node db2 (2):PCSD Status:  db2: Online  db3: Online  db1: OfflineDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[vagrant@db2 ~]$db1 復帰起動させる$ vagrant up db1ホントは pg_basebackup からやりたいところだが、データが壊れていないこと前提でそのまま再度参加させてみる異常終了後は起動しないように lock ファイルが残っているのでこれを削除$ sudo rm /var/lib/pgsql/10/tmpdir/PGSQL.lock起動させる[vagrant@db2 ~]$ sudo pcs cluster start db1db1: Starting Cluster...[vagrant@db2 ~]$ sudo pcs status --fullCluster name: pg10Stack: corosyncCurrent DC: db2 (2) (version 1.1.16-12.el7_4.5-94ff4df) - partition with quorumLast updated: Wed Dec  6 16:51:17 2017Last change: Wed Dec  6 16:50:28 2017 by root via crm_attribute on db23 nodes configured8 resources configuredOnline: [ db1 (1) db2 (2) db3 (3) ]Full list of resources: master-vip     (ocf::heartbeat:IPaddr2):       Started db2 replica-vip    (ocf::heartbeat:IPaddr2):       Started db3 Clone Set: ping-clone [ping]     ping       (ocf::pacemaker:ping):  Started db1     ping       (ocf::pacemaker:ping):  Started db3     ping       (ocf::pacemaker:ping):  Started db2     Started: [ db1 db2 db3 ] Master/Slave Set: pgsql-master [pgsql]     pgsql      (ocf::heartbeat:pgsql10):       Slave db1     pgsql      (ocf::heartbeat:pgsql10):       Slave db3     pgsql      (ocf::heartbeat:pgsql10):       Master db2     Masters: [ db2 ]     Slaves: [ db1 db3 ]Node Attributes:* Node db1 (1):    + master-pgsql                      : -INFINITY    + pgsql-data-status                 : STREAMING|ASYNC    + pgsql-receiver-status             : normal    + pgsql-status                      : HS:async    + pingd                             : 100* Node db2 (2):    + master-pgsql                      : 1000    + pgsql-data-status                 : LATEST    + pgsql-master-baseline             : 00000000062ACA00    + pgsql-receiver-status             : normal (master)    + pgsql-status                      : PRI    + pingd                             : 100* Node db3 (3):    + master-pgsql                      : 100    + pgsql-data-status                 : STREAMING|SYNC    + pgsql-receiver-status             : normal    + pgsql-status                      : HS:sync    + pingd                             : 100Migration Summary:* Node db1 (1):* Node db3 (3):* Node db2 (2):PCSD Status:  db1: Online  db3: Online  db2: OnlineDaemon Status:  corosync: active/disabled  pacemaker: active/disabled  pcsd: active/enabled[vagrant@db2 ~]$復帰した12月7日がもうすぐ終わってしまうのでここまで。そのうち続きを書くということで・・・","link":"https://qiita.com/yteraoka/items/cfa185f8846850648ab4","isoDate":"2017-12-07T14:39:03.000Z","dateMiliSeconds":1512657543000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"KVM ホストを Prometheus と Grafana で可視化","contentSnippet":"Prometheus は最近 Docker / Kubernetes 関連でよく目にしますが、さまざまな exporter があるため Docker に限らず使えます。サーバーのメトリクスを収集するにはオフィシャル exporter である node_exporter が使えます。これを Grafana で可視化するには「Node Exporter Fullby idealista」がなかなか良いです。Grafana の Dashboard は「Grafana Dashboards - discover and share dashboards for Grafana. | Grafana Labs」で公開されているものから検索できます。これを自分の環境に合わせて書き換えてみることから始めるのが入りやすいです。node_exporter ではブロックデバイスごと、ネットワークデバイスごとの IO の情報が取得できるため、KVM ゲスト単位の情報もデバイス名からたどればわかるのですがかなり面倒です。そこで、3rd party 製ですが libvirt_exporter を使うことでドメイン（ゲスト）名でそれぞれのメトリクスを取得、表示することができます。（バイナリは配布されていないので go での build が必要です）負荷が上がってるのはどのゲストの影響かな？ていうのがわかりやすくなります。この Dashboard は kvm-host-dashboard.json で試せます。関連ドキュメントPrometheus + Grafana + cAdvisor で Docker container のリソースモニタリングPrometheus の Service Discovery","link":"https://qiita.com/yteraoka/items/b1dbb687eb08a4313688","isoDate":"2017-08-20T13:00:07.000Z","dateMiliSeconds":1503234007000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"/proc の environ を見やすくする","contentSnippet":"Linux であるプロセスに「どんな環境変数が設定されてるのかな？」とか「ちゃんと設定した環境変数が反映されてるかな？」という場合に /proc/{PID}/environ を確認することがあります。が、このファイル、null (\\0) （less で見ると ^@ になります）区切りの NAME=VALUE となっており、grep は --text をつけないと Binary ファイルだからとマッチしたかどうかしか表示されない、--text をつけても結局1行なので全部が表示されるだけなので --color をつけてマッチした箇所に色をつけたりしてました。もっと良い方法があるだろうと思いつつも頻繁に実行するわけでもないのでまあいいやって。ある時ふと sed で 's/\\x00/\\n/g' したらあっさり改行区切りにできました。sed 's/\\x00/\\n/g' /proc/$$/environ以上、小ネタでした。","link":"https://qiita.com/yteraoka/items/5a4a891790595acf47e8","isoDate":"2017-08-19T08:20:25.000Z","dateMiliSeconds":1503130825000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"GSuite 管理下アカウントの接続アプリ情報を確認する方法","contentSnippet":"急速に広まったGmail/Google Docsのフィッシング詐欺に対してGoogleが公式声明を発表  |  TechCrunch Japanこんな記事がありまして、不用意に Gmail へのアクセスを許可してしまうと大変なことになるというのが周知されればそれはそれで今後のためにはなるかなという感じですが、良くわかんないけど便利そうだからと許可しちゃうってのはありそうです。GSuite において Google Drive へのアクセスはドメイン（組織）内からのみに限定可能ですが Gmail ではそれができません。自分自身のアカウントについてはアカウントに接続されているアプリで確認できますが、あなたが GSuite の管理者で、誰かが不用意にアクセスを許可していないか調べなければならないとしたらどうすれば良いでしょうか。GSuite 管理者は誰がどんなアプリ、サイトにアクセスを許可しているかを Google Apps Admin SDK Directory API の Tokens で確認することができます。list で次のようなレスポンスが得られます。get では clientId を指定することで当該 clientId に限定した情報が得られますが、list より得られる情報が増えたりはしません。利用者に確認するまでもなく削除が必要な場合は delete で当該アクセス権を削除できます。{    \"etag\": \"\\\"eqO3c9wtJJ4wVWz2xe0E9HiU_D0/2MNtXFuDDgEQvKBqfbFtTDmReGI\\\"\",    \"items\": [        {            \"anonymous\": false,            \"clientId\": \"292824132082.apps.googleusercontent.com\",            \"displayText\": \"Google APIs Explorer\",            \"etag\": \"\\\"eqO3c9wtJJ4wVWz2xe0E9HiU_D0/ZWXyg2w4w1l-_X9W1wX_vSO76fs\\\"\",            \"kind\": \"admin#directory#token\",            \"nativeApp\": false,            \"scopes\": [                \"https://www.googleapis.com/auth/apps.licensing\"            ],            \"userKey\": \"123456789012345678901\"        },        {            \"anonymous\": false,            \"clientId\": \"123456123456-abcdefghijklmnopqrstuvwxyz123456.apps.googleusercontent.com\",            \"displayText\": \"Zendesk\",            \"etag\": \"\\\"eqO3c9wtJJ4wVWz2xe0E9HiU_D0/QXKjHjHAD9x50uNksx_wmy4kVLQ\\\"\",            \"kind\": \"admin#directory#token\",            \"nativeApp\": false,            \"scopes\": [                \"https://www.googleapis.com/auth/userinfo.email\",                \"https://www.googleapis.com/auth/plus.me\"            ],            \"userKey\": \"123456789012345678901\"        },        {            \"anonymous\": false,            \"clientId\": \"Google Chrome\",            \"displayText\": \"Google Chrome\",            \"etag\": \"\\\"eqO3c9wtJJ4wVWz2xe0E9HiU_D0/sIMeSUCOeG2Jl1L5wIOh2MTQpgQ\\\"\",            \"kind\": \"admin#directory#token\",            \"nativeApp\": true,            \"scopes\": [                \"https://www.google.com/accounts/OAuthLogin\"            ],            \"userKey\": \"123456789012345678901\"        },        {            \"anonymous\": false,            \"clientId\": \"123123123123-123456abcdefghijklmnopqrstuvwxyz.apps.googleusercontent.com\",            \"displayText\": \"Grafana\",            \"etag\": \"\\\"eqO3c9wtJJ4wVWz2xe0E9HiU_D0/K6LTcWi4y797iMujMH9jeHFXQ-U\\\"\",            \"kind\": \"admin#directory#token\",            \"nativeApp\": false,            \"scopes\": [                \"https://www.googleapis.com/auth/userinfo.profile\",                \"https://www.googleapis.com/auth/userinfo.email\"            ],            \"userKey\": \"123456789012345678901\"        }    ],    \"kind\": \"admin#directory#tokenList\"}それぞれに、どの権限を許可しているのかは scopes に入っています。https://developers.google.com/identity/protocols/googlescopes#scriptv1 などにあります。scope の一例scopehttps://mail.google.com/Read, send, delete, and manage your emailhttps://www.googleapis.com/auth/userinfo.profileView your basic profile infohttps://www.googleapis.com/auth/userinfo.emailView your email addresshttps://www.googleapis.com/auth/plus.meKnow who you are on Googlehttps://www.googleapis.com/auth/driveView and manage the files in your Google Drivehttps://www.googleapis.com/auth/drive.readonlyView the files in your Google Drivehttps://www.googleapis.com/auth/spreadsheetsView and manage your spreadsheets in Google Drivehttps://www.googleapis.com/auth/spreadsheets.readonlyView your Google Spreadsheetshttps://www.google.com/m8/feedsManage your contactshttps://www.google.com/calendar/feedsManage your calendarsなどなど沢山ありますhttps://mail.google.com/ とか https://www.googleapis.com/auth/drive なんて怖すぎてとてもじゃないけど許可したくないですよね？でも意外と簡単に許可しちゃうんですよ...OAuth 2.0 Playground を見ると Gmail には個別の権限もあるっぽいhttps://www.googleapis.com/auth/gmail.composehttps://www.googleapis.com/auth/gmail.inserthttps://www.googleapis.com/auth/gmail.labelshttps://www.googleapis.com/auth/gmail.metadatahttps://www.googleapis.com/auth/gmail.modifyhttps://www.googleapis.com/auth/gmail.readonlyhttps://www.googleapis.com/auth/gmail.send一括で調査するhttps://github.com/yteraoka/googleapps-directory-tools に GSuite のアカウントを操作するスクリプト群があります。tokens.py で Tokens API にアクセスできます。$ ./tokens.pyusage: tokens.py [-h] [--auth_host_name AUTH_HOST_NAME]                 [--noauth_local_webserver]                 [--auth_host_port [AUTH_HOST_PORT [AUTH_HOST_PORT ...]]]                 [--logging_level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]                 {list,get,delete} ...tokens.py: error: too few arguments$ ./tokens.py listusage: tokens.py list [-h] [--scopes SCOPES] [--whitelist WHITELIST] [--json]                      [--jsonPretty]                      userKeytokens.py list: error: too few argumentsuser.py でアカウント一覧が取得できるので$ user.py list -d example.com \\    | awk '{print $1}' \\    | xargs -n 1 tokens.py listで example.com の全アカウントの接続アプリ情報が取得できます。が、ノイズが多くても困るので --scopes で重要な scope だけ指定すればそれが含まれるアプリだけが表示されます。$ tokens.py list \\  --scopes https://mail.google.com/ \\  --scopes https://www.googleapis.com/auth/drive \\  user@example.comこれでも全員が OAuth でログインするようなアプリがあったりすると邪魔なのでホワイトリストファイルとして clientId リストのファイルを作成し --whitelist でファイルを指定すると表示から除外できます。$ ./user.py list -d example.com \\  | awk '{print $1}' \\  | xargs -n 1 ./tokens.py list \\      --scopes https://mail.google.com/ \\      --scopes https://www.googleapis.com/auth/drive \\      --whitelist whitelist.txtこんな感じで怪しげなアプリに許可してないかチェックできますね。","link":"https://qiita.com/yteraoka/items/601e273066df262dc1ff","isoDate":"2017-05-04T13:31:34.000Z","dateMiliSeconds":1493904694000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"GitLab Runner を AWS Spotfleet で節約運用","contentSnippet":"GitLab には GitLab CI が統合されて gitlab.yml というファイルを入れておくだけで push 時に任意の処理を実行させることができます。その実行を GitLab Runner をセットアップしたサーバーで行います。沢山プロジェクトで使い出すと Runner 待ちが発生してしまいます。そこでこの Runner を必要な時にだけ必要な数を用意したい。安く。ということで AWS Spotfleet (スポットフリートの仕組み) で平日の朝 9:00 から 20:00 までだけ起動させておくという運用を考えます。Runner 起動時に GitLab サーバーへ登録し、shutdown 時には登録を解除させます。実はこの環境を作った後に気づいたのですが GitLab Runner には queue の状況によって Docker Machine を使って runner をオートスケールしてくれる機能がありました。Spot Instance を使うこともできるようです GitLab Runner 1.1 with AutoscalingSpotfleet での EC2 Instance 起動AWS CLI の request-spot-fleet を使います。Spotfleet のリクエストは次のようなコマンドでできます。$ aws ec2 request-spot-fleet \\    --spot-fleet-request-config file://config.jsonリクエストの詳細は ユーザーガイド にあります。config.json は スポットフリート設定の例 もありますが Web Console からポチポチと作って実際に動いている設定を describe-spot-fleet-requests で確認するのが近道です。$ aws ec2 describe-spot-fleet-requests次のように RequestId を指定すれば見たいものだけに絞れます$ aws ec2 describe-spot-fleet-request \\    --spot-fleet-request-ids sfr-73fbd2ce-aa30-494c-8788-1cee4EXAMPLE今回使うのはこんな JSON ファイルです。config.json.tmpl{  \"SpotPrice\": \"{{SpotPrice}}\",  \"TargetCapacity\": {{TargetCapacity}},  \"ValidFrom\": \"{{ValidFrom}}\",  \"ValidUntil\": \"{{ValidUntil}}\",  \"TerminateInstancesWithExpiration\": true,  \"IamFleetRole\": \"arn:aws:iam::{{AccountId}}:role/aws-ec2-spot-fleet-role\",  \"AllocationStrategy\": \"lowestPrice\",  \"ExcessCapacityTerminationPolicy\": \"Default\",  \"LaunchSpecifications\": [    {      \"ImageId\": \"{{ImageId}}\",      \"KeyName\": \"{{KeyName}}\",      \"EbsOptimized\": false,      \"BlockDeviceMappings\": [        {          \"DeviceName\": \"/dev/sda1\",          \"Ebs\": {            \"DeleteOnTermination\": true,            \"VolumeSize\": {{VolumeSize}},            \"VolumeType\": \"gp2\",            \"Encrypted\": false          }        }      ],      \"SecurityGroups\": [        {          \"GroupId\": \"{{GroupId}}\"        }      ],      \"SubnetId\": \"{{SubnetId}}\",      \"InstanceType\": \"{{InstanceType}}\",      \"UserData\": \"{{UserData}}\"    }  ]}{{ }} 部分は次項で紹介する実行用 shell script の中で sed で置換します。Python やら Ruby で JSON を生成すればもうちょっと柔軟にできますが今回は Shell Script で。UserData には base64 で encode した文字列を指定する必要があるため次のようにします。中身は後で。UserData=$(cat userdata.txt | base64 -w 0)Spotfleet のリクエストスクリプト先の JSON の {{ }} 部分を置換してリクエストを送るスクリプトです。Ubuntu 16.04 を指定しています。CentOS でも構いません。spotfeet.sh#!/bin/bashtmpfile=$(mktemp)# ValidFrom, ValidUntil は UTC で指定するためexport TZ=UTC# 何時間起動させるか (9:00 - 20:00 なら11時間)hours=11# 起動させるインスタンス数TargetCapacity=2# インスタンスタイプInstanceType=m4.large# ストレージボリュームサイズVolumeSize=50# 入札価格の上限SpotPrice=0.08ValidFrom=$(date +%Y-%m-%dT%H:%M:%S)ValidUntil=$(date -d \"$hours hours\" +%Y-%m-%dT%H:%M:%S)# AWS Account IDAccountId=123456789012# Ubuntu 16.04ImageId=ami-c68fc7a1# Security GroupGroupId=sg-12345678# どの Subnet で Instance を起動させるか(複数指定可能)SubnetId=\"subnet-11111111, subnet-22222222\"# SSH 用 key nameKeyName=your-key-nameUserData=$(cat userdata.txt | base64 -w 0)sed \\  -e \"s/{{AccountId}}/${AccountId}/\" \\  -e \"s/{{KeyName}}/${KeyName}/\" \\  -e \"s/{{ValidFrom}}/${ValidFrom}/\" \\  -e \"s/{{ValidUntil}}/${ValidUntil}/\" \\  -e \"s/{{ImageId}}/${ImageId}/\" \\  -e \"s/{{VolumeSize}}/${VolumeSize}/\" \\  -e \"s/{{InstanceType}}/${InstanceType}/\" \\  -e \"s/{{SpotPrice}}/${SpotPrice}/\" \\  -e \"s/{{UserData}}/${UserData}/\" \\  -e \"s/{{TargetCapacity}}/${TargetCapacity}/\" \\  -e \"s/{{GroupId}}/${GroupId}/\" \\  -e \"s/{{SubnetId}}/${SubnetId}/\" \\  config.json.tmpl \u003e $tmpfile# response 保存用ファイルfleet_response_file=$(mktemp)aws ec2 request-spot-fleet \\  --spot-fleet-request-config file://$tmpfile \u003e $fleet_response_file# response から request id を取り出すrequest_id=$(cat $fleet_response_file | jq -r .SpotFleetRequestId)# status が fulfilled になるまで待つwhile :do    sleep 10    status=$(aws ec2 describe-spot-fleet-requests \\                 --spot-fleet-request-id $request_id \\             | jq -r .SpotFleetRequestConfigs[].ActivityStatus)    if [ \"$status\" = \"fulfilled\" ] ; then        break    fidone# 起動された EC2 Instance の id を取得するinstance_ids=$(aws ec2 describe-spot-fleet-instances \\                   --spot-fleet-request-id $request_id \\                   | jq -r .ActiveInstances[].InstanceId)# EC2 instance に Tag をセットするaws ec2 create-tags \\  --resources $instance_ids \\  --tags \"Key=Name,Value=GitLab Runner\" \"Key=xxx,Value=yyy\"# 一時ファイルの削除rm $tmpfilerm $fleet_response_fileGitLab Runner のインストール、登録、解除UserData で渡すインスタンス作成時に実行するスクリプトでは次のようなことを行いますDocker のインストールGitLab Runner のインストールInstall GitLab Runner using the official GitLab repositoriesdocker hub へ push したり private registory から pull するためにログインのための情報をセット同時実行数を CPU の数に合わせるRunner の GitLab サーバーへの登録Docker in Docker で docker build したりするので docker.sock をマウントさせるキャッシュをサーバー間や日をまたいだ別のインスタンスでも共有できるように s3 に保存するShutdown 時に GitLab サーバーから削除されるように設定systemd の ExecStopPost を override.conf で設定Docker Machine でオートスケールさせるよりもいろいろいじる余地があります。userdata.txt#!/bin/bash# Docker のインストールcurl -sSL https://get.docker.com/ | sh# GitLab Runner repository のインストールcurl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.deb.sh | sudo bash# Debian Stretch からは同名のパッケージが Debian 側で提供されるので# gitlab 側の repository からインストールされるようにするcat \u003e /etc/apt/preferences.d/pin-gitlab-runner.pref \u003c\u003cEOFExplanation: Prefer GitLab provided packages over the Debian native onesPackage: gitlab-ci-multi-runnerPin: origin packages.gitlab.comPin-Priority: 1001EOF# Install GitLab CI Runnerapt-get updateapt-get install gitlab-ci-multi-runner# docker loginmkdir /root/.dockerecho '{\"auths\":{\"https://index.docker.io/v1/\":{\"auth\":\"dXNlcjpob2dlaG9nZWZ1Z2FmdWdh\"}}}' \u003e /root/.docker/config.jsonchmod 0600 /root/.docker/config.json# 同時実行上限変更(CPUのスレッド数に合わせる)sed -i \"s/^concurrent.*/concurrent = $(grep -c processor /proc/cpuinfo)/\" /etc/gitlab-runner/config.toml# GitLab Server への登録/usr/bin/gitlab-ci-multi-runner register \\ -u https://gitlab.example.com/ci \\ -n -r {RegisterToken} \\ --docker-image docker:latest \\ --tag-list docker \\ --run-untagged \\ --name \"$(curl -s http://169.254.169.254/latest/meta-data/instance-id)-$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\" \\ --executor docker --docker-privileged \\ --docker-volumes \"/root/.docker:/root/.docker:ro\" \\ --docker-volumes \"/var/run/docker.sock:/var/run/docker.sock\" \\ --cache-type s3 \\ --cache-s3-server-address s3.amazonaws.com \\ --cache-s3-access-key AWS_ACCESS_KEY \\ --cache-s3-secret-key AWS_ACCESS_SECRET \\ --cache-s3-bucket-name s3-bucket-name \\ --cache-s3-bucket-location ap-northeast-1 \\ --cache-s3-cache-path cache \\ --cache-cache-shared true# 停止後に GitLab サーバーから削除する設定mkdir /etc/systemd/system/gitlab-runner.service.dcat \u003e /etc/systemd/system/gitlab-runner.service.d/override.conf \u003c\u003cEOF[Service]ExecStopPost=/usr/bin/gitlab-ci-multi-runner unregister -n $(curl -s http://169.254.169.254/latest/meta-data/instance-id)-$(curl -s http://169.254.169.254/latest/meta-data/instance-type)EOFsystemctl daemon-reload","link":"https://qiita.com/yteraoka/items/c161e6b22422bb0eb7df","isoDate":"2017-03-16T14:58:18.000Z","dateMiliSeconds":1489676298000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"GitLab の Artifact 掃除ツール","contentSnippet":"GitLab では CI で build した成果物を artifact として保存することができます。なんかいつのまにか GitLab サーバーのディスクの使用量が増えちゃってるなあと思ったらこの artifact が膨れ上がっていました。gitlab-ci.yml の expire_in で保存する期間を制限しましょう。また、GitLab は次のようにしてバックアップを取得することができます。保存する期間を指定しておけば、バックアップ時にそれを過ぎたものは自動で削除してくれます。gitlab-rake gitlab:backup:createこのバックアップファイルには次のファイルが含まれます。repositories/dbuploads.tar.gzbuilds.tar.gzartifacts.tar.gzlfs.tar.gzはい、 artifacts.tar.gz が含まれてます。artifact が増えるとバックアップファイルも大きくなるのです。GitLab の画面から artifact を削除するには pipeline の build 履歴から各 build を開いて Erase ボタンをポチポチする必要があります。数が少なければポチポチやっても良いのですが大量にあると大変です。なんかツールがないかなと思ったらやっぱりありました。gitlab-artifact-cleanup の使い方docker で試しますdocker run -it --rm python:2.7 bashprivate_token は GitLab の User Settings -\u003e Account で確認できますpip install python-gitlab python-dateutil pytzcat \u003c\u003c_EOD_ \u003e /root/.python-gitlab.cfg[global]default = gitlab1ssl_verify = truetimeout = 5[gitlab1]url = https://gitlab.example.comprivate_token = ****************_EOD_curl -O https://gitlab.com/JonathonReinhart/gitlab-artifact-cleanup/raw/master/gitlab-artifact-cleanuppython gitlab-artifact-cleanup --all-projects -m \"5 days\"これで 5 日を過ぎた tag 付きでない artifact が削除されます。--project {namespace}{project_namem} で特定のプロジェクトだけを対象にすることもできます。-n もしくは --dry-run で削除される build を確認することができます。python-gitlab のマズイ実装gitlab-artifact-cleanup で使われている python-gitlab は paging されたデータをなぜか再帰呼出しして取得しようとするのでページ数が多いとgitlab.exceptions.GitlabConnectionError: Can't connect to GitLab server (maximum recursion depth exceeded in cmp)というエラーでコケてしまいます。sys.setrecursionlimit(2000)などとして回避することも可能ですが、時間ができたら直してもらうようにしよう。gitlab-artifact-cleanup script unable to delete some artifacts #217えー...","link":"https://qiita.com/yteraoka/items/1f5d828545bf8a798289","isoDate":"2017-02-10T12:55:51.000Z","dateMiliSeconds":1486731351000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"pgrepup で論理レプリケーションによる PostgreSQL の Upgrade","contentSnippet":"きっかけ先日、pgrepup – upgrade PostgreSQL using logical replication という記事で pgrepup というツールの存在を知りました。これを試してみます。環境は次の通り。CentOS 7.2.1511PostgreSQL 9.4.10PostgreSQL 9.6.1pgrepup 0.3.7Logical DecodingPostgreSQL では 9.4 から Logical Decoding という機能が追加されています。第46章ロジカルデコーディング (Chapter 46. Logical Decoding)Qiita にも記事があります PostgreSQLのLogical Decoding機能についての紹介 。また、PostgreSQL Advent Calendar 2016 の1日目でも紹介されています「Logical Decodingを使ったCDC（Change Data Capture）の実現方法を考えてみる」pgrepup はこの Logical Decoding で論理レプリケーションを行いメジャーバージョンアップを簡単に行うためのツールです。Github repository の説明には次のように書かれていますPgrepup is a tool for upgrading through PostgreSQL major versions using logical replication and pglogical extensionPostgreSQL での replication といえば WAL (Write Ahead Logging) ファイルの転送に始まり、Streaming Replication がこれまでの標準構成でこれからもそうだと思いますが、これでは異なるバージョン間でのレプリケーションが行えないために MySQL のようなバージョンアップ方法が使えませんでした。(Slony-I や Pgpool-II を使うという方法はある)（ちょっと脱線）これは Oracle Database も似た感じで redo log と WAL が同じような役割で archive log 転送で replica を作成するというのがお金をかけない replication で SharePlex や GoldenGate が Logical Decoding っぽい有償 replication ツールです。異なるバージョン間でも使えるためバージョンアップの停止時間短縮に使えます。LogMinor という redo log の decode ツールもあります、これは無償。pglogicalpgrepup では論理レプリケーションに pglogical (pglogical Docs) を使います。pglogical は Logical Decoding 機能を使うため PostgreSQL 9.4 以上が必須で、origin filtering と conflict detection のためには 9.5 以上が必要となります。試してみる構成PostgreSQL 9.4 から 9.6 への更新を模してみます。pgrepup は PostgreSQL サーバーのホストにインストールする必要はなく、ネットワーク越しにレプリケーション元、先の2台の PostgreSQL にアクセスできれば良いので3台のサーバーを用意します。Hostname用途PostgreSQL VersionIP Addressdb1Source DB9.4.1010.130.5.106/16db2Destination DB9.6.110.130.11.130/16repuppgrepup(9.6)10.130.21.132/16DigitalOcean の仮想サーバーを使いました。（DigitalOcean は初期状態では root でログインすることになるのでそのまま root で作業していますが権限として root が必要な作業ばかりではありません）pgrepup は Python 2.7 以上が必要ですが、CentOS 7 の Python は 2.7.5 なのでこれをそのまま使います。準備db1 (Source)レプリケーション元の DB サーバーを PostgreSQL 9.4 でセットアップします。テスト用のユーザー (scott) と DB (testdb) を作成し　animal と fish という table を作っておきます。後で理由がわかりますが animal は PRIMARY KEY 有り、fish は無しです。pgrepup から postgres ユーザーでアクセスするためパスワードを設定します。pgrepup がどんな Query を発行しているのか確認できるように log_statement = 'all' としています。/var/lib/pgsql/9.4/data/pg_log/ にログ出力されます。yum -y install https://download.postgresql.org/pub/repos/yum/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-3.noarch.rpmyum -y install postgresql94-server postgresql94-contribyum -y install http://packages.2ndquadrant.com/pglogical/yum-repo-rpms/pglogical-rhel-1.0-2.noarch.rpmyum -y install postgresql94-pglogical/usr/pgsql-9.4/bin/postgresql94-setup initdbsed -i -e \"s/^#\\?listen_addresses =.*/listen_addresses = '*'/\" \\       -e \"s/^#\\?wal_level =.*/wal_level = logical/\" \\       -e \"s/^#\\?max_worker_processes =.*/max_worker_processes = 10/\" \\       -e \"s/^#\\?max_replication_slots =.*/max_replication_slots = 10/\" \\       -e \"s/^#\\?max_wal_senders =.*/max_wal_senders = 10/\" \\       -e \"s/^#\\?shared_preload_libraries =.*/shared_preload_libraries = 'pglogical'/\" \\       -e \"s/^#\\?log_line_prefix =.*/log_line_prefix = '%t user=%u, db=%d, remote=%r, pid=%p, xid=%x '/\" \\       -e \"s/^#\\?log_statement =.*/log_statement = 'all'\" \\       /var/lib/pgsql/9.4/data/postgresql.confecho \"host all all 10.130.0.0/16 md5\" \u003e\u003e /var/lib/pgsql/9.4/data/pg_hba.confsystemctl start postgresql-9.4su -l postgres -c psqlpostgres=# ALTER USER postgres PASSWORD 'secret';postgres=# CREATE USER scott PASSWORD 'tiger';postgres=# CREATE DATABASE testdb OWNER scott ENCODING 'UTF-8';postgres=# \\c testdbtestdb=# set role scott;testdb=\u003e create table animal (id serial primary key, name text, created_at timestamp default current_timestamp);testdb=\u003e create table fish (id serial, name text, created_at timestamp default current_timestamp);testdb=\u003e INSERT INTO animal (name) VALUES ('cat');testdb=\u003e INSERT INTO animal (name) VALUES ('dog');testdb=\u003e INSERT INTO fish (name) VALUES ('saba');testdb=\u003e INSERT INTO fish (name) VALUES ('maguro');db2 (Destination)レプリケーション先の DB を PostgreSQL 9.6 でセットアップします。pgrepup から postgres ユーザーでアクセスするためパスワードを設定します。こちらには DB は追加しません。yum -y install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpmyum -y install postgresql96-server postgresql96-contribyum -y install http://packages.2ndquadrant.com/pglogical/yum-repo-rpms/pglogical-rhel-1.0-2.noarch.rpmyum -y install postgresql96-pglogical/usr/pgsql-9.6/bin/postgresql96-setup initdbsed -i -e \"s/^#\\?listen_addresses =.*/listen_addresses = '*'/\" \\       -e \"s/^#\\?wal_level =.*/wal_level = logical/\" \\       -e \"s/^#\\?max_worker_processes =.*/max_worker_processes = 10/\" \\       -e \"s/^#\\?max_replication_slots =.*/max_replication_slots = 10/\" \\       -e \"s/^#\\?max_wal_senders =.*/max_wal_senders = 10/\" \\       -e \"s/^#\\?shared_preload_libraries =.*/shared_preload_libraries = 'pglogical'/\" \\       -e \"s/^#\\?log_line_prefix =.*/log_line_prefix = '%t user=%u, db=%d, remote=%r, pid=%p, xid=%x '/\" \\       -e \"s/^#\\?log_statement =.*/log_statement = 'all'\" \\       /var/lib/pgsql/9.6/data/postgresql.confecho \"host all all 10.130.0.0/16 md5\" \u003e\u003e /var/lib/pgsql/9.6/data/pg_hba.confsystemctl start postgresql-9.6su -l postgres -c psqlpostgres=# ALTER USER postgres PASSWORD 'secret';repup (pgrepup)yum -y install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpmyum -y install gcc libffi-devel python-devel openssl-devel postgresql96-develyum -y install epel-releaseyum -y install python-pipPATH=/usr/pgsql-9.6/bin:$PATH pip install pgrepup[root@repup ~]# pgrepup --helpPgrepup 0.3.7Pgrepup - PostGreSQL REplicate and UPgradeA tool for upgrading a PostgreSQL cluster to a new major version using logical replication.Usage:  pgrepup [-c config] config  pgrepup [-c config] check [source|destination|all]  pgrepup [-c config] fix  pgrepup [-c config] setup  pgrepup [-c config] start  pgrepup [-c config] status  pgrepup [-c config] stop  pgrepup [-c config] uninstall  pgrepup -h | --help  pgrepup --versionOptions:  -c config     Optional config file. [default: ~/.pgrepup]  -h --help     Show this screen  --version     Show versionQuick start:    1) Configure pgrepup using the config command    pgrepup config    2) Check source and destination clusters with the check command    pgrepup check    3) Apply all the hints/fixes suggested by the check command    4) Prepare both clusters for replication using pglogical    pgrepup setup    5) Launch the replication process using the start command    pgrepup startconfig -\u003e check -\u003e setup -\u003e start というステップを踏むようです。pgrepup の設定[root@repup ~]# pgrepup configPgrepup 0.3.7Create a new pgrepup configConfiguration filename [~/.pgrepup]SecurityDo you want to encrypt database credentials using a password? [Y/n] YPassword:You'll be prompted for password every time pgrepup needs to connect to databaseFolder where pgrepup store temporary dumps and pgpass file [/tmp]Source Database configurationIp address or Dns name: 10.130.5.106Port: 5432Connect Database: [template1] postgresUsername: postgresPassword:Destination Database configurationIp address or Dns name: 10.130.11.130Port: 5432Connect Database: [template1] postgresUsername: postgresPassword:Configuration saved to /root/.pgrepup.You can now use the check command to verify setup of source and destination databasespgrepup check でレプリケーションできるかどうかの確認pgrepup check で pgrepup の config が正しいかどうか、PostgreSQL の設定がレプリケーションを行うための条件を満たしているかをチェックします。[root@repup ~]# pgrepup checkPgrepup 0.3.7Password:Global checkings... \u003e  Folder /tmp exists and is writable ...........................................OKChecking Source... \u003e  Connection PostgreSQL connection to 10.130.5.106:5432 with user postgres .....OK \u003e  pglogical installation Traceback (most recent call last):  File \"/usr/bin/pgrepup\", line 25, in \u003cmodule\u003e    main()  File \"/usr/lib/python2.7/site-packages/pgrepup/cli.py\", line 65, in main    dispatch(__doc__)  File \"/usr/lib/python2.7/site-packages/pgrepup/helpers/docopt_dispatch.py\", line 39, in __call__    function(**self._kwargify(arguments))  File \"/usr/lib/python2.7/site-packages/pgrepup/commands/check.py\", line 62, in check    c = checks(t, 'pglogical_installed', db_conn=conn)  File \"/usr/lib/python2.7/site-packages/pgrepup/commands/check.py\", line 168, in checks    checks_result[c] = create_extension(db_conn, 'pglogical', test=True)  File \"/usr/lib/python2.7/site-packages/pgrepup/helpers/database.py\", line 99, in create_extension    cur.execute(\"CREATE EXTENSION IF NOT EXISTS %s\" % extension_name)psycopg2.ProgrammingError: required extension \"pglogical_origin\" is not installedエラーですね、PostgreSQL 9.4 で pglogical を使うためには pglogical_origin extension が必要だったようです。すべての DB に対してCREATE EXTENSION IF NOT EXISTS pglogical_origin;を実行します。[root@db1 ~]# su -l postgres -c psqlpsql (9.4.10)Type \"help\" for help.postgres=# CREATE EXTENSION IF NOT EXISTS pglogical_origin;CREATE EXTENSIONpostgres=# \\c testdbYou are now connected to database \"testdb\" as user \"postgres\".testdb=# CREATE EXTENSION IF NOT EXISTS pglogical_origin;CREATE EXTENSIONtestdb=# \\c template1You are now connected to database \"template1\" as user \"postgres\".template1=# CREATE EXTENSION IF NOT EXISTS pglogical_origin;CREATE EXTENSIONtemplate1=# \\q[root@db1 ~]#再度 pgrepup check を実行してみます。[root@repup ~]# pgrepup checkPgrepup 0.3.7Password:Global checkings... \u003e  Folder /tmp exists and is writable ...........................................OKChecking Source... \u003e  Connection PostgreSQL connection to 10.130.5.106:5432 with user postgres .....OK \u003e  pglogical installation .......................................................OK \u003e  Needed wal_level setting .....................................................OK \u003e  Needed max_worker_processes setting ..........................................OK \u003e  Needed max_replication_slots setting .........................................OK \u003e  Needed max_wal_senders setting ...............................................OK \u003e  pg_hba.conf settings .........................................................KO    Hint: Add the following lines to /var/lib/pgsql/9.4/data/pg_hba.conf:        host replication pgrepup_replication 10.130.11.130/32 md5        host all pgrepup_replication 10.130.11.130/32 md5    After adding the lines, remember to reload postgreSQL \u003e  Local pg_dumpall version .....................................................OK \u003e  Source cluster tables without primary keys \u003e      template1 \u003e          pglogical_origin.replication_origin ..................................KO    Hint: Add a primary key or unique index or use the pgrepup fix command \u003e      testdb \u003e          public.animal ........................................................OK \u003e          public.fish ..........................................................KO    Hint: Add a primary key or unique index or use the pgrepup fix command \u003e          pglogical_origin.replication_origin ..................................KO    Hint: Add a primary key or unique index or use the pgrepup fix command \u003e      postgres \u003e          pglogical_origin.replication_origin ..................................KO    Hint: Add a primary key or unique index or use the pgrepup fix commandChecking Destination... \u003e  Connection PostgreSQL connection to 10.130.11.130:5432 with user postgres ....OK \u003e  pglogical installation .......................................................OK \u003e  Needed wal_level setting .....................................................OK \u003e  Needed max_worker_processes setting ..........................................OK \u003e  Needed max_replication_slots setting .........................................OK \u003e  Needed max_wal_senders setting ...............................................OK \u003e  pg_hba.conf settings .........................................................KO    Hint: Add the following lines to /var/lib/pgsql/9.6/data/pg_hba.conf:        host replication pgrepup_replication 10.130.11.130/32 md5        host all pgrepup_replication 10.130.11.130/32 md5    After adding the lines, remember to reload postgreSQL \u003e  Local pg_dumpall version .....................................................OK実際には色付きで表示されるのでもっと見やすいですがprimary key が全ての table に必要pg_hba.conf にレプリケーション先からの接続許可設定の追加が必要ということのようです。primary key については pgrepup fix コマンドを実行すれば pgreup がやってくれます。pg_hba.conf への追記db1echo \"host replication pgrepup_replication 10.130.11.130/32 md5host all pgrepup_replication 10.130.11.130/32 md5\" \u003e\u003e /var/lib/pgsql/9.4/data/pg_hba.confsystemctl reload postgresql-9.4db2echo \"host replication pgrepup_replication 10.130.11.130/32 md5host all pgrepup_replication 10.130.11.130/32 md5\" \u003e\u003e /var/lib/pgsql/9.6/data/pg_hba.confsystemctl reload postgresql-9.6ところで、pgrepup は PostgreSQL への接続しか許可されていないのにどうして pg_hba.conf の中身が確認できたのでしょう？ソースコードを確認したら temporary table に COPY で読み込んでました。確かにこうすれば出来ますね。pgrepup fix[root@repup ~]# pgrepup fixPgrepup 0.3.7Password:Find Source cluster's databases with tables without primary key/unique index... \u003e  template1 \u003e      Found pglogical_origin.replication_origin without primary key Added __pgrepup_id field \u003e  postgres \u003e      Found pglogical_origin.replication_origin without primary key Added __pgrepup_id field \u003e  testdb \u003e      Found public.fish without primary key ..............Added __pgrepup_id field \u003e      Found pglogical_origin.replication_origin without primary key Added __pgrepup_id fieldそれぞれの table に __pgrepup_id 列が追加されました。次のようなクエリが実行されます。c.execute(\"ALTER TABLE %s.%s ADD COLUMN %s BIGSERIAL NOT NULL PRIMARY KEY\" % (schema, table, get_unique_field_name()))全行更新なので行数の多い table が対象の場合は結構辛そうです。unique な column がある場合は手動で primary key 設定を行う方が良さそうです。再度 pgrepup check問題点を修正したので再度 pgrepup check を実行します。[root@repup ~]# pgrepup checkPgrepup 0.3.7Password:Global checkings... \u003e  Folder /tmp exists and is writable ...........................................OKChecking Source... \u003e  Connection PostgreSQL connection to 10.130.5.106:5432 with user postgres .....OK \u003e  pglogical installation .......................................................OK \u003e  Needed wal_level setting .....................................................OK \u003e  Needed max_worker_processes setting ..........................................OK \u003e  Needed max_replication_slots setting .........................................OK \u003e  Needed max_wal_senders setting ...............................................OK \u003e  pg_hba.conf settings .........................................................OK \u003e  Local pg_dumpall version .....................................................OK \u003e  Source cluster tables without primary keys \u003e      template1 \u003e          pglogical_origin.replication_origin ..................................OK \u003e      testdb \u003e          public.animal ........................................................OK \u003e          public.fish ..........................................................OK \u003e          pglogical_origin.replication_origin ..................................OK \u003e      postgres \u003e          pglogical_origin.replication_origin ..................................OKChecking Destination... \u003e  Connection PostgreSQL connection to 10.130.11.130:5432 with user postgres ....OK \u003e  pglogical installation .......................................................OK \u003e  Needed wal_level setting .....................................................OK \u003e  Needed max_worker_processes setting ..........................................OK \u003e  Needed max_replication_slots setting .........................................OK \u003e  Needed max_wal_senders setting ...............................................OK \u003e  pg_hba.conf settings .........................................................OK \u003e  Local pg_dumpall version .....................................................OK全部 OK となりました。pgrepup setup続いて pgrepup setup コマンドです。[root@repup ~]# pgrepup setupPgrepup 0.3.7Password:Check if there are active subscriptions in Destination nodes .....................OKGlobal tasks \u003e  Remove nodes from Destination cluster \u003e      postgres .................................................................OK \u003e      template1 ................................................................OK \u003e  Create temp pgpass file ......................................................OK \u003e  Drop pg_logical extension in all databases of Source cluster \u003e      template1 ................................................................OK \u003e      postgres .................................................................OK \u003e      testdb ...................................................................OK \u003e  Drop pg_logical extension in all databases of Destination cluster \u003e      postgres .................................................................OK \u003e      template1 ................................................................OKSetup Source \u003e  Create user for replication ..................................................OK \u003e  Dump globals and schema of all databases .....................................OK \u003e  Setup pglogical replication sets on Source node name \u003e      template1 ................................................................OK \u003e      postgres .................................................................OK \u003e      testdb ...................................................................OKSetup Destination \u003e  Create and import source globals and schema ..................................OK \u003e  Setup pglogical Destination node name \u003e      postgres .................................................................OK \u003e      template1 ................................................................OK \u003e      testdb ...................................................................OKCleaning up \u003e  Remove temporary pgpass file .................................................OK \u003e  Remove other temporary files .................................................OKpg_dumpall -s --if-exists -c で Source DB から schema を dump して Destination DB へ適用されます。その後Source DB (db1) では次のような処理が        c = db_conn.cursor()        c.execute(\"CREATE EXTENSION pglogical\")        c.execute(\"SELECT pglogical.drop_node(node_name := %s, ifexists := false)\", ['Source'])        c.execute(\"SELECT pglogical.create_node(node_name := %s, dsn := %s );\",                  ['Source', get_dsn_for_pglogical('Source', db_name=db)])        c.execute(\"SELECT pglogical.replication_set_add_all_tables('default', '{%s}'::text[]);\" % ','.join(db_schemas))        c.execute(\"SELECT pglogical.replication_set_add_all_sequences( set_name := 'default', schema_names := %s)\",                  [db_schemas])        db_conn.commit()Destination DB (db2) では次のような処理が行われます        c = db_conn.cursor()        drop_extension(db_conn, \"pglogical\")        c.execute(\"DROP SCHEMA IF EXISTS pglogical CASCADE\")        c.execute(\"CREATE EXTENSION pglogical\")        c.execute(\"SELECT pglogical.drop_node(node_name := %s, ifexists := false)\", ['Destination'])        c.execute(\"SELECT pglogical.create_node( node_name := %s, dsn := %s );\", [            'Destination', get_dsn_for_pglogical('Destination', db)        ])        db_conn.commit()pgrepup start でレプリケーション開始setup で準備が完了したので start でレプリケーションを開始します。[root@repup ~]# pgrepup startPgrepup 0.3.7Password:Start replication and upgrade \u003e  postgres .................................................................OK \u003e  template1 ................................................................OK \u003e  testdb ...................................................................OKsource2016-12-05 11:28:46 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(53378), pid=30759, xid=0 LOG:  statement: BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ, READ ONLY;        SET DATESTYLE = ISO;        SET INTERVALSTYLE = POSTGRES;        SET extra_float_digits TO 3;        SET statement_timeout = 0;        SET lock_timeout = 0;        SET TRANSACTION SNAPSHOT '00000785-1';2016-12-05 11:28:46 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(53378), pid=30759, xid=0 LOG:  statement: SELECT nspname, relname FROM pglogical.tables WHERE set_name = ANY(ARRAY['default'])2016-12-05 11:28:46 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(53378), pid=30759, xid=0 LOG:  statement: COPY \"public\".\"animal\" TO stdout2016-12-05 11:28:46 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(53378), pid=30759, xid=0 LOG:  statement: COPY \"public\".\"fish\" TO stdout2016-12-05 11:28:46 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(53378), pid=30759, xid=0 LOG:  statement: ROLLBACKdestination2016-12-05 11:28:45 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(52284), pid=30330, xid=0 LOG:  statement: BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;        SET session_replication_role = 'replica';        SET DATESTYLE = ISO;        SET INTERVALSTYLE = POSTGRES;        SET extra_float_digits TO 3;        SET statement_timeout = 0;        SET lock_timeout = 0;2016-12-05 11:28:45 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(52284), pid=30330, xid=0 LOG:  statement: COPY \"public\".\"animal\" FROM stdin2016-12-05 11:28:45 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(52284), pid=30330, xid=1843 LOG:  statement: COPY \"public\".\"fish\" FROM stdin2016-12-05 11:28:45 JST user=pgrepup_replication, db=testdb, remote=10.130.11.130(52284), pid=30330, xid=1843 LOG:  statement: COMMITここで COPY コマンドでデータがコピーされるようです。はて？これは INDEX やら制約やらがついた状態で行われるのだろうか？FK とかついたテーブルも用意するべきだったか。pgrepup status で状況確認[root@repup ~]# pgrepup statusPgrepup 0.3.7Password:Configuration \u003e  Source database cluster ......................................................OK \u003e  Destination database cluster .................................................OKPglogical setup \u003e  Source database cluster \u003e      template1 ................................................................OK \u003e      postgres .................................................................OK \u003e      testdb ...................................................................OK \u003e  Destination database cluster \u003e      postgres .................................................................OK \u003e      template1 ................................................................OK \u003e      testdb ...................................................................OKReplication status \u003e  Database postgres \u003e      Replication status ..............................................replicating \u003e  Database template1 \u003e      Replication status ..............................................replicating \u003e  Database testdb \u003e      Replication status ..............................................replicating \u003e  Xlog difference (bytes) ..................................................107080Xlog difference (bytes) で遅延が確認できるんだとか。レプリケーション中（アップグレード中）にレコードを追加してみるsourcetestdb=# select * from animal; id | name |         created_at----+------+----------------------------  1 | cat  | 2016-12-05 09:59:40.456885  2 | dog  | 2016-12-05 09:59:43.048854(2 rows)testdb=# insert into animal (name) values ('mouse');INSERT 0 1testdb=# select * from animal; id | name  |         created_at----+-------+----------------------------  1 | cat   | 2016-12-05 09:59:40.456885  2 | dog   | 2016-12-05 09:59:43.048854  3 | mouse | 2016-12-05 11:45:32.51085(3 rows)testdb=#destinationtestdb=# select * from animal; id | name  |         created_at----+-------+----------------------------  1 | cat   | 2016-12-05 09:59:40.456885  2 | dog   | 2016-12-05 09:59:43.048854  3 | mouse | 2016-12-05 11:45:32.51085(3 rows)testdb=#Destination DB にレコード追加が反映されました。Sequenceレプリケーション中に Sequence は更新されませんが、pgrepup stop を実行すると、そこで反映されます。ですからアップグレードの完了には stop 処理が必要です。https://github.com/rtshome/pgrepup/#sequencessourcetestdb=# select * from animal_id_seq;-[ RECORD 1 ]-+--------------------sequence_name | animal_id_seqlast_value    | 3start_value   | 1increment_by  | 1max_value     | 9223372036854775807min_value     | 1cache_value   | 1log_cnt       | 32is_cycled     | fis_called     | tdestination 側は1のまま。destinationtestdb=# select * from animal_id_seq;-[ RECORD 1 ]-+--------------------sequence_name | animal_id_seqlast_value    | 1start_value   | 1increment_by  | 1max_value     | 9223372036854775807min_value     | 1cache_value   | 1log_cnt       | 0is_cycled     | fis_called     | fDDLDDL もレプリケートされませんhttps://github.com/rtshome/pgrepup/#ddl-commandsstop, setup, start でやり直せとあります長期間使うためのツールではないので、これはこれで問題なさそうです。コードはコメントアウトされているhttps://github.com/rtshome/pgrepup/blob/141cca9e03a2d595e90f8282b7808cf5fabb1bf5/pgrepup/commands/setup.py#L138pgrepup stop[root@repup ~]# pgrepup stopPgrepup 0.3.7Password:Check active subscriptions in Destination nodes \u003e  template1 ................................................................Active \u003e      Launch stop command ..................................................OK \u003e  testdb ...................................................................Active \u003e      Launch stop command ..................................................OK \u003e  postgres .................................................................Active \u003e      Launch stop command ..................................................OKsequence が更新されたか確認してみますsourcetestdb=# select * from animal_id_seq;-[ RECORD 1 ]-+--------------------sequence_name | animal_id_seqlast_value    | 4start_value   | 1increment_by  | 1max_value     | 9223372036854775807min_value     | 1cache_value   | 1log_cnt       | 32is_cycled     | fis_called     | tdestinationtestdb=# select * from animal_id_seq;-[ RECORD 1 ]-+--------------------sequence_name | animal_id_seqlast_value    | 1004start_value   | 1increment_by  | 1max_value     | 9223372036854775807min_value     | 1cache_value   | 1log_cnt       | 0is_cycled     | fis_called     | tSELECT pglogical.synchronize_sequence( seqoid ) FROM pglogical.sequence_stateで Sequence の同期が行われるわけですが、+1000 されるようです。pgrepup uninstall で掃除[root@repup ~]# pgrepup uninstallPgrepup 0.3.7Password:Check active subscriptions in Destination nodes \u003e  template1 ...............................................................Stopped \u003e  testdb ..................................................................Stopped \u003e  postgres ................................................................StoppedUninstall operations \u003e  Remove nodes from Destination cluster \u003e      postgres .................................................................OK \u003e      template1 ................................................................OK \u003e      testdb ...................................................................OK \u003e  Drop pg_logical extension in all databases \u003e      Source \u003e          template1 ............................................................OK \u003e          postgres .............................................................OK \u003e          testdb ...............................................................OK \u003e      Destination \u003e          postgres .............................................................OK \u003e          template1 ............................................................OK \u003e          testdb ...............................................................OK \u003e  Drop user for replication ....................................................OK \u003e  Drop unique fields added by fix command \u003e          template1 \u003e              pglogical_origin.replication_origin ..............................KO \u003e          postgres \u003e              pglogical_origin.replication_origin ..............................KO \u003e          testdb \u003e              public.fish ......................................................OK \u003e              pglogical_origin.replication_origin ..............................KO \u003e              public.animal ....................................................OKpglogical extension の削除と pgrepup fix で追加された __pgrepup_id column の削除が行われました。pglogical_origin の KO は無視して良さそうです（pglogical schema は除外リストに入ってるけど _origin の方は入ってないんですよね）。まとめPRIMARY KEY がすべてのテーブルに必須ということで、もともと PRIMARY KEY 相当の列がすべてのテーブルに存在すれば良いですが、そうではない大きなテーブルがあるといきなりこのツールでやってしまうのは怖い感じです。3TB超のCacooのPostgreSQL 9.3を9.5にアップグレードした話 という例を見ると pg_upgrade で良いのかなという気もする。これをきっかけに Logical Decoding を調査して分析用のDBへ必要なテーブルだけ同期させるとか、複数のアプリのテーブルを1つのDBに集めるとかができると便利かなという感じです。FDW と Materialized View の組み合わせの方が良かったりもしそうだけど。おまけAlibaba 製の同期ツールでも Logical Decoding が使われているようです。https://github.com/aliyun/rds_dbsyncPostgreSQL -\u003e PostgreSQLPostgreSQL -\u003e GreenplumMySQL -\u003e PostgreSQLMySQL -\u003e Greenplumっていう同期（コピー？）が出来るみたいPGConf. Asia 2016 行けば良かったhttp://www.pgconf.asia/JP/material/","link":"https://qiita.com/yteraoka/items/e82e4d28f6a23915d190","isoDate":"2016-12-05T10:49:32.000Z","dateMiliSeconds":1480934972000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"mod_proxy_hcheck で BalancerMember の healthcheck","contentSnippet":"最近は nginx が優勢で新たなプロジェクトで Apache httpd を選択することは少なくなっているかもしれませんが、昔から稼働してるものはまだ Apache httpd が多いのではないでしょうか。Apache ではさばききれないっていうサービスばかりではないですし、mod_rewrite による超絶技巧が仕込んであったりして移行が厳しいとか。さて、そんな Apache httpd は mod_proxy_balancer を使うことで HTTP や AJP のロードバランサーとして機能させることができ、balancer manager からオンラインでメンバーサーバーの状態を変更することができます。が、ヘルスチェック機能がなく、突然不調になったサーバーを自動で無効にしたいという場合に不便でした。エラーとなった場合に一定時間アクセスを振り分けないという設定はできるのですが、きちんと復旧するまでアクセスを振り分けないということができませんでした。（他の監視システムとか Consul と consul-template とかで外から操作することは可能だけど面倒くさい） そんなこんなで、やっぱり HAProxy とかがいいかなあなんて思っていたところ Apache 2.4.21 で mod_proxy_hcheck が追加されました。設定例ドキュメントにある例は次のようになっていますProxyHCExpr ok234 {%{REQUEST_STATUS} =~ /^[234]/}ProxyHCExpr gdown {%{REQUEST_STATUS} =~ /^[5]/}ProxyHCExpr in_maint {hc('body') !~ /Under maintenance/}\u003cProxy balancer://foo\u003e  BalancerMember http://www.example.com/  hcmethod=GET hcexpr=in_maint hcuri=/status.php  BalancerMember http://www2.example.com/  hcmethod=HEAD hcexpr=ok234 hcinterval=10  BalancerMember http://www3.example.com/ hcmethod=TCP hcinterval=5 hcpasses=2 hcfails=3  BalancerMember http://www4.example.com/\u003c/Proxy\u003eProxyPass \"/\" \"balancer://foo\"ProxyPassReverse \"/\" \"balancer://foo\"ProxyHCExpr で worker が有効かどうかを判別する条件を指定します。HTTP Response の status code や body に含まれる文字列を判断材料にできます。これに名前をつけて BalancerMember 指定の中で使うことができます。hcmethod はヘルスチェックアクセスの HTTP メソッド、hcinterval はチェックの間隔（秒）。hcpasses で何回成功したサーバを正常とするか、hcfails で何回失敗したらダウンと判断するかを指定します。後で説明しますが、正常な状態では失敗だけがカウントされ、ダウン状態では成功だけがカウントされます。設定項目ParameterDefaultDescriptionhcmethodNoneOPTIONS, HEAD, GET から選択hcpasses1何回成功したら正常とみなすかhcfails1何回失敗したらダウンとみなすかhcinterval30ヘルスチェックの間隔hcuri監視する URL の pathhctemplateworker ごとに同じ設定を何度も書かなくて済むようにテンプレート登録するhcexpr成功とみなす条件、未指定の場合は HTTP Code の 2xx, 3xx が正常とみなされる監視リクエスト監視リクエストは次のように組み立てられますwctx-\u003ereq = apr_psprintf(ctx-\u003ep,                   \"%s %s%s%s HTTP/1.0\\r\\nHost: %s:%d\\r\\n\\r\\n\",                   method,                   (wctx-\u003epath ? wctx-\u003epath : \"\"),                   (wctx-\u003epath \u0026\u0026 *hc-\u003es-\u003ehcuri ? \"/\" : \"\" ),                   (*hc-\u003es-\u003ehcuri ? hc-\u003es-\u003ehcuri : \"\"),                   hc-\u003es-\u003ehostname, (int)hc-\u003es-\u003eport);次のような設定の場合\u003cProxy balancer://test-lb\u003e  BalancerMember http://backend1:8080 method=GET hcuri=/healthcheck  BalancerMember ...\u003c/Proxy\u003eそれぞれの変数はこのようになり変数名値wctx-\u003epath(null)hc-\u003es-\u003ehcuri/healthcheckhc-\u003es-\u003ehostnamebackend1hc-\u003es-\u003eport8080リクエストはこうなりますGET /healthcheck HTTP/1.0Host: backend1あまりなさそうなケースですが、Proxy 先がホストやポート毎に Path の異なる次のような場合は\u003cProxy balancer://test-lb\u003e  BalancerMember http://backend1:8080/abc method=GET hcuri=/healthcheck  BalancerMember http://backend2:8080/xyz method=GET hcuri=/healthcheck  ...\u003c/Proxy\u003ewctx-path に /abc や /xyz が入るため、リクエストは GET /abc//healthcheck となります。/ が重なって気持ち悪いですね。どうしてこのようなコードになっているのか Subversion のログを見てもわからなかったのですが issue を上げたほうが良いのかな。Host ヘッダーは BalancerMember で指定する URL のホスト部分です。Proxy 先が NameBased VirtualHost で ProxyPreserveHost On でないと機能しないような場合は困りますね（これもあまりなさそうではありますが）。また、HAProxy の様に任意のヘッダーを追加することもできません。hc-\u003es-\u003ehcuri がどうやっても空っぽでおかしいなと思ったら bug でしたhttps://bz.apache.org/bugzilla/show_bug.cgi?id=60038これは監視用の URL が常に空っぽ GET  HTTP/1.0 となるということで致命的なわけですが、この状態でリリースされてしまうというのがこのモジュールの現在の扱いというわけです。もう続きを読むのをやめましょうか？カウンタの仕様hcpasses, hcfails で helthcheck に何回成功すれば有効にし、何回失敗すれば無効にするかを指定できますが、成功したり失敗したりする状態ではどうなるかDown 状態では pass だけがカウントされるUp 状態では fail だけがカウントされるUp / Down の状態が変わるときだけカウンタがリセットされるという仕様のようです。ということで一度増えた fail カウントは Down になるまでリセットされることなく増え続けます。一時的なタイムアウトが少しづつ溜まった場合にも Down となります。逆に Down 状態で時々成功するといった場合にも復活してしまうことになります。hcpasses=5, hcfails=5 の場合の動作f f f f f (Down) f f f f s s s s f f f f f f f f f f f f s (Up)1 2 3 4 5        - - - - 1 2 3 4 - - - - - - - - - - - - 5f s s f s f s s s f s s s s s s s s s s s s s s s s f (Down)1 - - 2 - 3 - - - 4 - - - - - - - - - - - - - - - - 5状態の保存BalancerPersist がデフォルトの Off では Restart (SIGHUP) / Graceful Restart で状態はクリアされてしまいますが、On にしておくと Graceful Restart はもちろん、Stop / Start, Restart (SIGHUP) でも状態が保存されたままとなります。BalancerPersist を On にすると *.persist というファイルに hcpasses, hcfails の設定値もカウンターも hcuri なども保存されています。そして、この状態では設定ファイルを変更して restart しても balancer の設定は *.persist ファイルから読み込まれるため反映されません。stop して *.persist ファイルを削除してから start させるか、balancer manager インターフェースから変更する必要があります。ヘルスチェックのタイムアウトmod_proxy_hcheck の設定にはタイムアウトに関するものがありません。これは困ります。調べてみたところ ProxyTimeout の値が適用されるようです。未設定であれば core の Timeout の値となります。いずれも秒での指定です。ここにもちょっとした罠（？）があります。mod_proxy_hcheck は mod_watchdog を使って ProxyHCTPsize で設定した Thread 数（デフォルト16）を使ってヘルスチェックを行います。watchdog のイベントは2秒おきに発生しますが、ヘルスチェックのレスポンスが遅いと2秒おきにドンドン Thread を埋め尽くし、queue に溜まって行きます。そして、他の正常な worker の監視に影響を与えてしまう可能性があるのです。ProxyTimeout は全体に影響してしまうので、どうしても遅い処理があったりすると困りますが、十分に短い秒数をセットするのが良さそうです。ちなみにヘルスチェックでない通常のアクセス時に ProxyTimeout にひっかかると 502 エラーを返します。他の worker への retry はされません。接続できなかった場合は retry されます。また、その worker の状態が正常である限りアクセスは割り振られます。これを回避するためには failontimeout=On を設定することで1度タイムアウトが発生したらエラー状態となり、一定時間（retry設定）アクセスを割り振らなくなります。その間にヘルスチェックでダウン状態と判断されるか復旧すると被害を抑えられそうです。ヘルスチェックのインターバルhcinterval 設定でヘルスチェックの監視間隔を秒単位で指定できますが、2秒未満にすることはできません。mod_proxy.h/* The watchdog runs every 2 seconds, which is also the minimal check */#define HCHECK_WATHCHDOG_INTERVAL (2)まとめえっ！っていう致命的なバグがあったりしてまだ不安ですが、興味を持った方のフィードバックによって改善されて行くのではないでしょうか。","link":"https://qiita.com/yteraoka/items/380ded6b68b630bb9388","isoDate":"2016-11-30T15:08:15.000Z","dateMiliSeconds":1480518495000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"えっ？アドベントカレンダーって25日目があるの？からの curl で HTTP2","contentSnippet":"うーん、ネタが思い浮かばない・・・が、しかし、アドベントカレンダーって一般的には24日間らしいぞhttps://en.wikipedia.org/wiki/Advent_calendarそういうことなので皆様良いお年を！でも、中には25日のもあるっぽいし、小心者だから小ネタを少々。curl ってcurl -O http://www.example.com/[001-025].htmlって実行すると 001.html から 025.html までゲットできるんですよ。[1-25].html ならゼロパディングされません。え？ man pages の 1 ページ目に書いてある？ {a,b,c}.html も使えるって？知らなかった...では次はこれcurl -Lso /dev/null -w 'http_code: %{http_code}time_connect: %{time_connect}time_namelookup: %{time_namelookup}time_starttransfer: %{time_starttransfer}time_redirect: %{time_redirect}time_total: %{time_total}' http://www.google.com/って実行するとhttp_code: 200time_connect: 0.042time_namelookup: 0.029time_starttransfer: 0.188time_redirect: 0.056time_total: 0.327っていう出力が得られます。どこに時間がかかってるのかわかって便利ですね。あぁ...しょぼい...最後につい先日公開された 7.46.0 でついに HTTP2 に対応した (間違いでしたもっと前から対応してました、お使いのバイナリが対応しているかどうかは別として) ということなので早速これを試してみますhttp://curl.haxx.se/changes.html#7_46_0環境は Ubuntu 15.10 です$ cat /etc/os-release NAME=\"Ubuntu\"VERSION=\"15.10 (Wily Werewolf)\"ID=ubuntuID_LIKE=debianPRETTY_NAME=\"Ubuntu 15.10\"VERSION_ID=\"15.10\"HOME_URL=\"http://www.ubuntu.com/\"SUPPORT_URL=\"http://help.ubuntu.com/\"BUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"まだパッケージでの提供はないので nghttp2 と curl の最新ソースコードをダウンロードしてビルドしますconfigure  curl version:     7.46.0  Host setup:       x86_64-pc-linux-gnu  Install prefix:   /home/ytera  Compiler:         gcc  SSL support:      enabled (OpenSSL)  SSH support:      no      (--with-libssh2)  zlib support:     enabled  GSS-API support:  no      (--with-gssapi)  TLS-SRP support:  enabled  resolver:         default (--enable-ares / --enable-threaded-resolver)  IPv6 support:     enabled  Unix sockets support: enabled  IDN support:      no      (--with-{libidn,winidn})  Build libcurl:    Shared=yes, Static=yes  Built-in manual:  enabled  --libcurl option: enabled (--disable-libcurl-option)  Verbose errors:   enabled (--disable-verbose)  SSPI support:     no      (--enable-sspi)  ca cert bundle:   /etc/ssl/certs/ca-certificates.crt  ca cert path:     no  LDAP support:     no      (--enable-ldap / --with-ldap-lib / --with-lber-lib)  LDAPS support:    no      (--enable-ldaps)  RTSP support:     enabled  RTMP support:     no      (--with-librtmp)  metalink support: no      (--with-libmetalink)  PSL support:      no      (libpsl not found)  HTTP2 support:    enabled (nghttp2)  Protocols:        DICT FILE FTP FTPS GOPHER HTTP HTTPS IMAP IMAPS POP3 POP3S RTSP SMB SMBS SMTP SMTPS TELNET TFTP$ ~/bin/curl --versioncurl 7.46.0 (x86_64-pc-linux-gnu) libcurl/7.46.0 OpenSSL/1.0.2d zlib/1.2.8 nghttp2/1.6.1-DEVProtocols: dict file ftp ftps gopher http https imap imaps pop3 pop3s rtsp smb smbs smtp smtps telnet tftp Features: IPv6 Largefile NTLM NTLM_WB SSL libz TLS-SRP HTTP2 UnixSocketsHTTP/2 対応できてるっぽいですね。       --http2              (HTTP)  Tells  curl  to  issue  its  requests using HTTP 2. This              requires that the underlying libcurl was built  to  support  it.              (Added in 7.33.0)え？ 7.33.0 で http2 使えたの？$ /usr/bin/curl --versioncurl 7.43.0 (x86_64-pc-linux-gnu) libcurl/7.43.0 GnuTLS/3.3.15 zlib/1.2.8 libidn/1.28 librtmp/2.3Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets 7.43.0 だから --http2 はあるぞ。でも Features に HTTP2 がない、libnghttp2 へのリンクもない。ubuntu のパッケージで入る奴は HTTP2 対応してないようだ$ /usr/bin/curl --http2 -vo /dev/null https://www.google.co.jp/curl: (1) Unsupported protocolやっぱり。それでは気を取り直して$ ~/bin/curl --http2 -svo /dev/null https://www.google.co.jp/*   Trying 2404:6800:4004:814::2003...* Connected to www.google.co.jp (2404:6800:4004:814::2003) port 443 (#0)* ALPN, offering h2* ALPN, offering http/1.1* Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH* successfully set certificate verify locations:*   CAfile: /etc/ssl/certs/ca-certificates.crt  CApath: none* TLSv1.2 (OUT), TLS header, Certificate Status (22):} [5 bytes data]* TLSv1.2 (OUT), TLS handshake, Client hello (1):} [512 bytes data]* TLSv1.2 (IN), TLS handshake, Server hello (2):{ [100 bytes data]* TLSv1.2 (IN), TLS handshake, Certificate (11):{ [3700 bytes data]* TLSv1.2 (IN), TLS handshake, Server key exchange (12):{ [148 bytes data]* TLSv1.2 (IN), TLS handshake, Server finished (14):{ [4 bytes data]* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):} [70 bytes data]* TLSv1.2 (OUT), TLS change cipher, Client hello (1):} [1 bytes data]* TLSv1.2 (OUT), TLS handshake, Finished (20):} [16 bytes data]* TLSv1.2 (IN), TLS change cipher, Client hello (1):{ [1 bytes data]* TLSv1.2 (IN), TLS handshake, Finished (20):{ [16 bytes data]* SSL connection using TLSv1.2 / ECDHE-ECDSA-AES128-GCM-SHA256* ALPN, server accepted to use h2* Server certificate:*    subject: C=US; ST=California; L=Mountain View; O=Google Inc; CN=*.google.com*    start date: Dec 10 17:52:51 2015 GMT*    expire date: Mar  9 00:00:00 2016 GMT*    subjectAltName: www.google.co.jp matched*    issuer: C=US; O=Google Inc; CN=Google Internet Authority G2*    SSL certificate verify ok.* Using HTTP2, server supports multi-use* Connection state changed (HTTP/2 confirmed)* TCP_NODELAY set* Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0} [5 bytes data]* Using Stream ID: 1 (easy handle 0x23a7dd0)} [5 bytes data]\u003e GET / HTTP/1.1\u003e Host: www.google.co.jp\u003e User-Agent: curl/7.46.0\u003e Accept: */*\u003e { [5 bytes data]\u003c HTTP/2.0 200\u003c date:Fri, 25 Dec 2015 14:17:38 GMT\u003c expires:-1\u003c cache-control:private, max-age=0\u003c content-type:text/html; charset=Shift_JIS\u003c p3p:CP=\"This is not a P3P policy! See https://www.google.com/support/accounts/answer/151657?hl=en for more info.\"\u003c server:gws\u003c x-xss-protection:1; mode=block\u003c x-frame-options:SAMEORIGIN\u003c set-cookie:NID=74=Mn2oEfF0V7xa41Ehje7AW2rDFU3Gf0HqF8WbSZmf01NfiMCOcef0Z8DJQUeQ0QOf-LfNplRMrvYRi76IKRam-EWPxLF-pN1l6i-JpK-nzCGKKP3AH_hb40G-HBymT0im; expires=Sat, 25-Jun-2016 14:17:38 GMT; path=/; domain=.google.co.jp; HttpOnly\u003c alternate-protocol:443:quic,p=1\u003c alt-svc:quic=\"www.google.com:443\"; ma=600; v=\"30,29,28,27,26,25\",quic=\":443\"; ma=600; v=\"30,29,28,27,26,25\"\u003c accept-ranges:none\u003c vary:Accept-Encoding\u003c { [5 bytes data]* Connection #0 to host www.google.co.jp left intactキター！それでは良いお年を。","link":"https://qiita.com/yteraoka/items/e4593ed59fd3f923275f","isoDate":"2015-12-25T14:32:15.000Z","dateMiliSeconds":1451053935000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"XFS の speculative preallocation について調べてみた","contentSnippet":"この記事は「エムスリー Advent Calendar 2015」の 15 日目の記事です。きっかけはこれFilesystem            Size  Used Avail Use% Mounted on/dev/mapper/vg01-lv_apps                       18G   18G  184M  99% /appsは！？毎分監視してるのにいきなり90%や95%をすっ飛ばして99%のアラートが！# du -sh .17G     .# ls -lhtotal 17G-rw-r--r-- 1 root root 8.4G Sep 29 23:41 20150929-www-access.ltsv-rw-r--r-- 1 root root  93K Sep 29 23:40 20150929-www-error.logどいうこと？ 8.4GB のファイルがあるだけなのになんで 17GB 使ってることになってんの？ls や find で見つからないのにディスクが溢れそうっていうのは通常削除済みのファイルを開いているプロセスが存在してるパターンなのだが今回はそれではない。なにかのバグでファイルがおかしくなっているのだろうか？# ls -lhtotal 17G-rw-r--r-- 1 root root 8.5G Sep 29 23:59 20150929-www-access.ltsv-rw-r--r-- 1 root root  95K Sep 29 23:56 20150929-www-error.log-rw-r--r-- 1 root root  21M Sep 30 00:02 20150930-www-access.ltsv# du -sh *17G     20150929-www-access.ltsv96K     20150929-www-error.log31M     20150930-www-access.ltsvしばらくすると戻った。なんじゃこりゃ？（日付が変わってファイルが close されたのも関係ある？）# ls -lhtotal 8.6G-rw-r--r-- 1 root root 8.5G Sep 29 23:59 20150929-www-access.ltsv-rw-r--r-- 1 root root  95K Sep 29 23:56 20150929-www-error.log-rw-r--r-- 1 root root  55M Sep 30 00:07 20150930-www-access.ltsv-rw-r--r-- 1 root root  158 Sep 30 00:04 20150930-www-error.log# du -sh *8.5G    20150929-www-access.ltsv96K     20150929-www-error.log63M     20150930-www-access.ltsv4.0K    20150930-www-error.logRHEL も 7 から xfs がデフォルトになったし  RDBMS in the Cloud: PostgreSQL on AWS (PDF) にもデータ領域には xfs 使えって書いてあるし時代は xfs だろうということで選択していた xfs が怪しいんだろうなと思ったもののよくわからない、この後も再発したため、とりあえず　ext4 で作りなおすことで回避していたものの数カ月後、別のサーバーでも発生したのでこれは xfs に仕掛けがあるんだろうと調べてみた。ググってみると「なんじゃこりゃ？」と思ったのは私だけではなかったっぽい。 xfs には speculative preallocation というファイル拡張時に大きめの連続した領域を確保することでフラグメントを抑えてパフォーマンスの低下を防ごうという機能が存在するようです。http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=055388a3188f56676c21e92962fc366ac8b5cb72この commit 後にも修正されているのでこれは最新のコードではありません。 kernel-2.6.32-573.8.1.el6.src.rpm を見てみました。では、この preallocation で同様の状況が再現可能か試してみます。次の簡単なスクリプトを用意します。xfs-preallocation-test.pl#!/usr/bin/perluse strict;use warnings;sub bytes {    my ($size) = @_;    if ($size =~ s/g\\z//i) {        $size *= 1024 * 1024 * 1024;    }    elsif ($size =~ s/m\\z//i) {        $size *= 1024 * 1024;    }    elsif ($size =~ s/k\\z//i) {        $size *= 1024;    }    return $size;}if (scalar(@ARGV) != 4) {    print \"Usage: $0 /path/to/src_file /path/to/out_file bs size\\n\";    exit(1);}my ($src_file, $out_file, $bs, $size) = @ARGV;my $buf;my $wrote = 0;$bs = bytes($bs);$size = bytes($size);my $w;open($w, \"\u003e\u003e\", $out_file)  or die \"cannot open '$out_file': $!\";my $r;open($r, \"\u003c\", $src_file)  or die \"cannot open $src_file: $!\";sleep 3;while ($wrote \u003c $size) {    sysread($r, $buf, $bs);    syswrite($w, $buf, length($buf));    $wrote += length($buf);}close($r);close($w);DigitalOcean の CentOS 6.7 x86_64 サーバーで 30GB の xfs パーティションを作ってテストしてみます。truncate -s 30G /xfs-diskmkfs -t xfs /xfs-diskmkdir /xfsmount -o loop -t xfs /xfs-disk /xfsperl xfs-preallocation-test.pl /dev/zero /xfs/test 8k 8.5g \u0026watch -n 1 'ls -l /xfs/test; ls -lh /xfs/test; du -sh /xfs/test; xfs_bmap -v /xfs/test'この処理の出力を asciinema にアップしました。ファイルサイズ拡張時に一気に 2GB や 4GB 増える様子が確認できます。https://asciinema.org/a/e8mbpx26cxkqvr1zkad30i9ht(ttyrec より共有が楽ちんですね、手元にファイルで保存しておくこともできるし便利)/dev/zero から 8.5G を書き込んだファイルのフラグメント状態を確認すると今回は次のように 5.8GB, 277MB, 1.7GB, 876MB に分断されていました。[root@xfstest ~]# xfs_bmap -v /xfs/test/xfs/test: EXT: FILE-OFFSET           BLOCK-RANGE        AG AG-OFFSET              TOTAL   0: [0..11936783]:        96..11936879        0 (96..11936879)      11936784   1: [11936784..12505039]: 15728704..16296959  1 (64..568319)          568256   2: [12505040..16031583]: 24117296..27643839  1 (8388656..11915199)  3526544   3: [16031584..17825791]: 19923088..21717295  1 (4194448..5988655)   1794208このような細切れのファイルでは 8.5GB のファイルなのに 17GB も割り当てられるような状況にならないので状況再現のために xfs_fsr でデフラグします。[root@xfstest ~]# xfs_fsr /xfs/test[root@xfstest ~]# xfs_bmap -v /xfs/test/xfs/test: EXT: FILE-OFFSET          BLOCK-RANGE        AG AG-OFFSET               TOTAL   0: [0..3791751]:        11936880..15728631  0 (11936880..15728631)  3791752   1: [3791752..17825791]: 31488064..45522103  2 (30784..14064823)    14034040[root@xfstest ~]#1.8GB と 6.8GB にまとまりました。ここで先ほどと同じスクリプトでファイルに 3GB ほど追記してみます。(3GB も要らなかった)perl xfs-preallocation-test.pl /dev/zero /xfs/test 8k 3g \u0026watch -n 1 'ls -l /xfs/test; ls -lh /xfs/test; du -sh /xfs/test; xfs_bmap -v /xfs/test'8.5GB にちょいと追記しただけで今度は 17GB まで膨れ上がりました、そして、書き込み終了後もしばらく 17GB のままです（なんだか見覚えのある感じです）。[root@xfstest ~]# date; ls -lh --full-time /xfs/test; du -h /xfs/testSun Dec 13 21:51:09 EST 2015-rw-r--r-- 1 root root 12G 2015-12-13 21:49:56.831999975 -0500 /xfs/test17G     /xfs/test[root@xfstest ~]# date; ls -lh --full-time /xfs/test; du -h /xfs/testSun Dec 13 21:54:30 EST 2015-rw-r--r-- 1 root root 12G 2015-12-13 21:49:56.831999975 -0500 /xfs/test17G     /xfs/test[root@xfstest ~]# date; ls -lh --full-time /xfs/test; du -h /xfs/testSun Dec 13 21:54:34 EST 2015-rw-r--r-- 1 root root 12G 2015-12-13 21:49:56.831999975 -0500 /xfs/test12G     /xfs/test5分近く 17GB をキープしていました。これは fs.xfs.speculative_prealloc_lifetime の値が効いていそうです。[root@xfstest ~]# sysctl fs.xfs.speculative_prealloc_lifetimefs.xfs.speculative_prealloc_lifetime = 300[root@xfstest ~]# cat /proc/sys/fs/xfs/speculative_prealloc_lifetime300おそらくこれれすね、あの 99% になった現象は。毎日発生するわけではなかったのでたまたまうまい具合に連続したエクステントを確保できてしまった場合に一気に大きく割り当てられてしまったのでしょう。一度の拡張は最大で8GB （デフォルトの bsize=4096 の場合）。ボリュームの空き容量やクオータ制限での残り書き込み可能サイズでこの値は小さくなります。freespace       max prealloc size  \u003e5%             full extent (8GB)  4-5%             2GB (8GB \u003e\u003e 2)  3-4%             1GB (8GB \u003e\u003e 3)  2-3%           512MB (8GB \u003e\u003e 4)  1-2%           256MB (8GB \u003e\u003e 5)  \u003c1%            128MB (8GB \u003e\u003e 6)こんなのやだよ、無効にしたいよという場合にはどうするかというと、マウントオプションで allocsize を明示的に指定すれば良いようです。パフォーマンスを気にする必要の無いログサーバーなどであれば無駄に確保されるより良さそうです。  allocsize=size    Sets the buffered I/O end-of-file preallocation size when    doing delayed allocation writeout (default size is 64KiB).    Valid values for this option are page size (typically 4KiB)    through to 1GiB, inclusive, in power-of-2 increments.    The default behaviour is for dynamic end-of-file    preallocation size, which uses a set of heuristics to    optimise the preallocation size based on the current    allocation patterns within the file and the access patterns    to the file. Specifying a fixed allocsize value turns off    the dynamic behaviour.これで少し安心して xfs が使えそうです。","link":"https://qiita.com/yteraoka/items/60e31477c62b7c94d32e","isoDate":"2015-12-14T15:02:25.000Z","dateMiliSeconds":1450105345000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Ansible の Template 機能の紹介","contentSnippet":"__________________\u003c template について書くよ \u003e ------------------        \\   ^__^         \\  (oo)\\_______            (__)\\       )\\/\\                ||----w |                ||     ||Ansible というかほぼ Jinja2 なのだけれども、ちょいちょいググってしまうものをまとめてみます（これをまとめるまで知らなかったものもあります、たまにはドキュメントを見直すのが良いです）。template module だけでなく task 定義で使う YAML 内でも使われています。(Qiita の syntax highlight は Jinja にも対応しているのですね)Ansible Managedまずはこれ、つい使いたくなるけど罠があるので私は使わない。(Subversion の $Id$ は便利なのに)http://docs.ansible.com/ansible/intro_configuration.html#ansible-managedテンプレートファイルに{{ ansible_managed }}と書いておけば実行時に ansible.cfg などで設定したフォーマットにしたがって置換してくれます。いつ誰が更新したかとかを入れられます。ansible_managed = Ansible managed: {file} modified on %Y-%m-%d %H:%M:%S by {uid} on {host}が、この機能は曲者で、ファイルに変更があったかどうかを比べるのにこの行まで含んでしまうから timestamp なんかを入れてしまった日には本来何も変更してないはずなのにファイルが入れ替えられてしまい。毎回 notify handler が発動してしまうという悲劇を産みます。気をつけましょう。defaultansible のいつしかの version からデフォルトでは未定義の変数を参照するとエラーで終了してしまいますが、指定しない場合のデフォルト値を指定しておけば大丈夫ということで、特に何も指定がなければこの値ですよというのを指定します。role の default/main.yml を使うという手もありますが、参照すべきファイルが増えるのは嬉しくないので単純な値ならこれで。{{ some_variable | default('foo bar') }}task の中で便利に使えるのが 1.8 で登場した default(ommit) ですね（と言ってもこれ書いてて知った）。http://docs.ansible.com/ansible/playbooks_filters.html#omitting-undefined-variables-and-parameters に例がありますが、ファイルのパーミッションやオーナーなど、with_items でループ処理したい時、殆どは指定が不要なのにひとつだけ指定したいといった場合、それまでは全部の item に値をもたせるか、default でデフォルト値を設定する必要がありましたが、ommit を使えば未定義の場合はその項目の指定自体がないことにできます。便利（実は今知った...）リスト操作min, maxhttp://docs.ansible.com/ansible/playbooks_filters.html#list-filtersリストの中の最小値、最大値を取り出してくれますunique, union,...http://docs.ansible.com/ansible/playbooks_filters.html#set-theory-filters2つのリストの和とか差とかrandomhttp://docs.ansible.com/ansible/playbooks_filters.html#random-number-filterリストの中からランダムに選択されます。 数字内のランダムな値、ステップ数を指定したランダムな値（100のうち10刻みでのランダムな値）とかを取り出せます。cron モジュールとセットで、複数台のサーバーで同時に実行されるのを避ける場合に便利そうです。ドキュメントにも cron の例があります。{{ 60 | random(step=5) }} * * * * /some/commandshufflehttp://docs.ansible.com/ansible/playbooks_filters.html#shuffle-filterシャッフル動作確認test.yml- hosts: all  gather_facts: no  vars:    aaa:      - A      - B      - C    bbb:      - A      - B      - C      - X      - Y      - Z    ccc: [1,2,3,4,5]    ddd: [1,1,2,2,3,3,4,4,5,5]    eee: [1,2,3]    fff: [3,4,5]  tasks:    - debug: msg=\"[aaa] {{ aaa }}\"    - debug: msg=\"[bbb] {{ bbb }}\"    - debug: msg=\"[ccc] {{ ccc }}\"    - debug: msg=\"[ddd] {{ ddd }}\"    - debug: msg=\"[random] {{ aaa | random }}\"    - debug: msg=\"[unique] {{ ddd | unique }}\"    - debug: msg=\"[union1] {{ aaa | union(bbb) }}\"    - debug: msg=\"[union2] {{ aaa | union(ccc) }}\"    - debug: msg=\"[intersect] {{ bbb | intersect(aaa) }}\"    - debug: msg=\"[difference] {{ bbb | difference(aaa) }}\"    - debug: msg=\"[symmetric_difference] {{ eee | symmetric_difference(fff) }}\"    - debug: msg=\"[max] {{ ccc | max}}\"    - debug: msg=\"[min] {{ ccc | min}}\"    - debug: msg=\"[shuffle] {{ bbb | shuffle}}\"を用意してansible-playbook -i localhost, test.ymlとすれば動作確認ができます。計算http://docs.ansible.com/ansible/playbooks_filters.html#mathべき乗や平方根、対数が使えるようですその他のフィルターIPアドレス関連やコメント用、ハッシュ処理などあるので確認してみてください。http://docs.ansible.com/ansible/playbooks_filters.htmljoin は使う場面ありますよね{{ list | join(\" \") }}この join するリストがシンプルなリストならこれで良いのですがハッシュ/ディクショナリのリストだった場合にどうするかというとaaa:  - en: apple    ja: ringo  - en: orange    ja: mikan  - en: grape    ja: budouという変数があって、en だけや ja だけを join したい場合は次のようにします2つ繋げることもできるんです{{ aaa | map(attribute='en') | join(',') }}{{ aaa | map(attribute='ja') | join(',') }}結果apple,orange,graperingo,mikan,budoumap() については http://jinja.pocoo.org/docs/dev/templates/#map を。Loop 処理最後にループ処理ですtask での Loop はこちらにいろいろ載ってます、ヘぇ、そんなのもあったんだぁという感じなので読んでみましょう。task の with_xxx には沢山の種類があって、when は with_xxx のそれぞれの値に対しても使えます。http://docs.ansible.com/ansible/playbooks_loops.htmlテンプレートファイル内での Loop 処理についてはこちらhttp://jinja.pocoo.org/docs/dev/templates/{% for item in items %}{% if item.key1 == 'AAA' %}  {{ item.key1 }} {{ item.key2 }}{% elif item.key1 == 'BBB' %}  ...{% else %}  ...{% endif %}{% endfor %}が基本でしょうか。if, elif, else, endif です。（いろんな言語使ってるといつも悩みますね、elif のところ。あれどれだっけ？と）Loop のアイテムごとに番号を振りたいと思ったら loop.index (1から始まる), loop.index0 (0から始まる) という変数が使えます最初と最後を示す loop.first, loop.last という Boolean 変数もあります{% for item in items %}{% if loop.first %}Begin{% endif %}{{ loop.index }}: {{ item }}{% if loop.last %}End{% endif %}{% endfor %}詳細はこちらhttp://jinja.pocoo.org/docs/dev/templates/#list-of-control-structuresエスケープtemplate ファイル内に {{ とか {% が含まれる場合は raw を使います{% raw %}...{% endraw %}まとめとりとめもなく書いてしまいましたが、また思いついたら書くことにします。まだカレンダーに空きがあるようなので。","link":"https://qiita.com/yteraoka/items/7119d4e1e2f8faddfb64","isoDate":"2015-12-02T11:29:30.000Z","dateMiliSeconds":1449055770000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"PostgreSQL の ALTER DEFAULT PRIVILEGES","contentSnippet":"PostgreSQL には ALTER DEFAULT PRIVILEGES といいう新規作成した TABLE などにデフォルトで設定される権限を変更する機能があります（標準SQLにはありません）。http://www.postgresql.org/docs/9.4/static/sql-alterdefaultprivileges.htmlhttp://www.postgresql.jp/document/9.4/html/sql-alterdefaultprivileges.html (日本語)（上記ドキュメントは 9.4 ですがこの機能は 9.0 から存在します）ここではこれの使い方を見てみたいと思います。シンタックスはドキュメントにある通りですが、次のようになっておりALTER DEFAULT PRIVILEGES    [ FOR { ROLE | USER } target_role [, ...] ]    [ IN SCHEMA schema_name [, ...] ]    abbreviated_grant_or_revokeFOR で誰が作成した場合に有効かを指定（デフォルトはこのコマンドの実行ユーザー）スキーマを限定する場合は IN SCHEMA で指定abbreviated_grant_or_revoke で誰にどんな権限を与える（もしくは剥奪する）かどのユーザーでログインしているかわかりやすいように psql のプロンプトをカスタマイズしておきます$ cat \u003e ~/.psqlrc \u003c\u003c_EOD_\\set PROMPT1 '%n@%/%R%#%x '_EOD_アプリから接続するための appuser と、レポーティング用の report ユーザーが存在する環境を作りますCREATE USER appuser;CREATE USER report;CREATE DATABASE testdb01 OWNER appuser ENCODING 'utf8';この状態で appuser が普通にテーブルを作成するとappuser@testdb01=\u003e CREATE TABLE table01 (id int, name text);CREATE TABLEappuser@testdb01=\u003e \\d         List of relations Schema |  Name   | Type  |  Owner  --------+---------+-------+--------- public | table01 | table | appuser(1 row)appuser@testdb01=\u003e \\z                            Access privileges Schema |  Name   | Type  | Access privileges | Column access privileges --------+---------+-------+-------------------+-------------------------- public | table01 | table |                   | (1 row)appuser@testdb01=\u003e このように appuser がオーナーで他にアクセス権限はついていません。もしも、毎度毎度 TABLE 作成ごとに report ユーザーに SELECT 権限をつける必要があるとしたらここで ALTER DEFAULT PRIVILEGES が使えます。ALTER DEFAULT PRIVILEGES GRANT SELECT ON TABLES TO report;appuser@testdb01=\u003e ALTER DEFAULT PRIVILEGES GRANT SELECT ON TABLES TO report;ALTER DEFAULT PRIVILEGESappuser@testdb01=\u003e CREATE TABLE table02 (id int, name text);CREATE TABLEappuser@testdb01=\u003e \\d         List of relations Schema |  Name   | Type  |  Owner  --------+---------+-------+--------- public | table01 | table | appuser public | table02 | table | appuser(2 rows)appuser@testdb01=\u003e \\z                               Access privileges Schema |  Name   | Type  |    Access privileges    | Column access privileges --------+---------+-------+-------------------------+-------------------------- public | table01 | table |                         |  public | table02 | table | appuser=arwdDxt/appuser+|         |         |       | report=r/appuser        | (2 rows)appuser@testdb01=\u003e 自動的に report ユーザーに SELECT 権限が付与されました。ここでは report というユーザーに付与されるように設定しましたが、例えば nantoka_ro という Read-Only 用 role を作成し、これに SELECT 権限がつくようにしておいて、必要なユーザーにその role を付与する方が使い勝手は良いかもしれません。（PostgreSQL の user はログイン権限のついた role だからほぼ同じですけれども）TABLE オーナー (appuser) と同じ権限で良いのであれば DEFAULT PRIVILEGES を使うまでもなくGRANT appuser TO report;で appuser の権限がまるっと report ユーザーに付与されます。複数ユーザーなら role でまとめられます。ログインIDは人ごとに変えたいけれども DB やテーブルは共有するという場合は全員に共通の role を割り当て、各自はログイン後に set role ロール名 すると TABLE を作ったりした場合の OWNER がその role になります。グループで共有するならグループ所有にするという考え方ですね。ALTER DEFAULT PRIVILEGES の FOR で role を指定するとその role を付与されたユーザーなら誰が作っても GRANT されるのかと思ったらこれは効きませんでした。残念。組織 Advent Calendar 初挑戦ですが、Qiita に残るとなると組織の宣伝っぽいことは書きづらいですね。","link":"https://qiita.com/yteraoka/items/d526e4c4c869b8592667","isoDate":"2015-11-30T15:53:59.000Z","dateMiliSeconds":1448898839000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"OpenLDAPをマルチドメインでレプリケーション","contentSnippet":"CentOS 7 で OpenLDAP サーバー2台をマルチマスタ構成でセットアップする方法ですexample.com と example.net のマルチドメインで構成しますSSL / TLS 対応もさせますインストールsudo yum -y install openldap-servers openldap-clients初期化やり直すときはここからsudo systemctl stop slapdsudo rm -fr /etc/openldap/slapd.d/cn=config/olcDatabase={2}hdb{,.ldif}sudo rm -fr /etc/openldap/slapd.d/cn=config/olcDatabase={3}hdb{,.ldif}sudo rm -fr /var/lib/ldapsudo install -d -o ldap -g ldap -m 0700 /var/lib/ldapsudo install -d -o ldap -g ldap -m 0700 /var/lib/ldap/example.comsudo install -d -o ldap -g ldap -m 0700 /var/lib/ldap/example.netsudo systemctl start slapdデフォルトの URI を設定今後の作業を楽にするため、デフォルトの URI を設定します/etc/openldap/ldap.conf にURL ldapi://を設定します。この URI でアクセスすれば root ならなんでもできます。LogLevel 設定sudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifyreplace: olcLogLevelolcLogLevel: config stats sync conns__EOD__systemd でのログ確認は journalctl -aru slapd | less あたりで。my-domain.com の修正yum でインストールした場合のサンプル設定が my-domain.com なので example.com に変更sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={1}monitor,cn=configchangetype: modifyreplace: olcAccessolcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external ,cn=auth\" read by dn.base=\"cn=Manager,dc=example,dc=com\" read by * none__EOD__確認sudo ldapsearch -b cn=config olcDatabase=monitorスキーマのインポートsudo ldapadd -f /etc/openldap/schema/cosine.ldifsudo ldapadd -f /etc/openldap/schema/inetorgperson.ldifposix アカウントなど必要であればそれ (nis.ldif ?) などもこれは /etc/openldap/slapd.d/cn=config/cn=schema/ に保存されるので /var/lib/ldap/* の削除では消えませんexample.com, example.net 用のデータベース作成sudo ldapadd \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configobjectClass: olcDatabaseConfigobjectClass: olcHdbConfigolcDatabase: {2}hdbolcDbDirectory: /var/lib/ldap/example.comolcSuffix: dc=example,dc=comolcRootDN: cn=Manager,dc=example,dc=comolcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external ,cn=auth\" manage by * noneolcAccess: to dn.subtree=\"\" by * read__EOD__sudo ldapadd \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configobjectClass: olcDatabaseConfigobjectClass: olcHdbConfigolcDatabase: {3}hdbolcDbDirectory: /var/lib/ldap/example.netolcSuffix: dc=example,dc=netolcRootDN: cn=Manager,dc=example,dc=netolcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external ,cn=auth\" manage by * noneolcAccess: to dn.subtree=\"\" by * read__EOD__olcDatabase={2}hdb, olcDatabase={3}hdb と数字部分を明示するところとドメインごとに別のディレクトリ (olcDbDirectory) を指定するところがポイントデータベースのチューニング値はなんとなくですsudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcDbConfigolcDbConfig: set_cachesize 0 67108864 1olcDbConfig: set_lg_dir .olcDbConfig: set_lg_bsize 33554432olcDbConfig: set_lk_max_objects 3000olcDbConfig: set_lk_max_locks 3000olcDbConfig: set_lk_max_lockers 3000olcDbConfig: set_flags DB_LOG_AUTOREMOVE__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configchangetype: modifyadd: olcDbConfigolcDbConfig: set_cachesize 0 67108864 1olcDbConfig: set_lg_dir .olcDbConfig: set_lg_bsize 33554432olcDbConfig: set_lk_max_objects 3000olcDbConfig: set_lk_max_locks 3000olcDbConfig: set_lk_max_lockers 3000olcDbConfig: set_flags DB_LOG_AUTOREMOVE__EOD__ドメイン作成と RootDN アカウント作成この例のパスワードは 2h6R\u00269QE-6sudo ldapadd \u003c\u003c__EOD__dn: dc=example,dc=comdc: exampleo: \"Example, Inc.\"objectClass: dcObjectobjectClass: organizationdn: cn=Manager,dc=example,dc=comobjectClass: organizationalRoleobjectClass: simpleSecurityObjectcn: ManageruserPassword: {SSHA}T6fIJME1FHwKEuS9MG7BBhYU6TYLGRnX__EOD__sudo ldapadd \u003c\u003c__EOD__dn: dc=example,dc=netdc: exampleo: \"Example, ltd.\"objectClass: dcObjectobjectClass: organizationdn: cn=Manager,dc=example,dc=netobjectClass: organizationalRoleobjectClass: simpleSecurityObjectcn: ManageruserPassword: {SSHA}T6fIJME1FHwKEuS9MG7BBhYU6TYLGRnX__EOD__{SSHA} で始まるパスワードのハッシュ値は slappasswd コマンドで生成できます$ slappasswdNew password: パスワードRe-enter new password: パスワード{SSHA}T6fIJME1FHwKEuS9MG7BBhYU6TYLGRnXOU 作成とりあえず People と Group を作成します。(People に対応するのは Groups じゃないのか？と思いつつ)sudo ldapadd \u003c\u003c__EOD__dn: ou=People,dc=example,dc=comou: PeopleobjectClass: organizationalUnitdn: ou=Group,dc=example,dc=comou: GroupobjectClass: organizationalUnitdn: ou=People,dc=example,dc=netou: PeopleobjectClass: organizationalUnitdn: ou=Group,dc=example,dc=netou: GroupobjectClass: organizationalUnit__EOD__ACL 設定sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configreplace: olcAccessolcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external ,cn=auth\" manage by * breakolcAccess: to attrs=userPassword           by anonymous auth           by self write           by dn.exact=\"cn=Replication,dc=example,dc=com\" read           by dn.regex=\"uid=(user001|user002|user003),ou=People,dc=example,dc=com\" write           by group/groupOfUniqueNames/uniqueMember.exact=\"cn=Administrators,ou=Groups,dc=example,dc=com\" write           by * noneolcAccess: to *           by dn.regex=\"uid=(user001|user002|user003),ou=People,dc=example,dc=com\" write           by group/groupOfUniqueNames/uniqueMember.exact=\"cn=Administrators,ou=Groups,dc=example,dc=com\" write           by * read__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configreplace: olcAccessolcAccess: to * by dn.base=\"gidNumber=0+uidNumber=0,cn=peercred,cn=external ,cn=auth\" manage by * breakolcAccess: to attrs=userPassword           by anonymous auth           by self write           by dn.exact=\"cn=Replication,dc=example,dc=net\" read           by dn.regex=\"uid=(user001|user002|user003),ou=People,dc=example,dc=net\" write           by group/groupOfUniqueNames/uniqueMember.exact=\"cn=Administrators,ou=Groups,dc=example,dc=net\" write           by * noneolcAccess: to *           by dn.regex=\"uid=(user001|user002|user003),ou=People,dc=example,dc=net\" write           by group/groupOfUniqueNames/uniqueMember.exact=\"cn=Administrators,ou=Groups,dc=example,dc=net\" write           by * read__EOD__user001, user002, user003 もしくは Administrators グループに所属するメンバーは各エントリの書き換えや新規エントリの追加が可能パスワード以外の参照であれば誰でも可能パスワードは認証での利用か、本人が認証後に変更することは可能Replication (後で設定する) 用ユーザーはパスワードを参照可能認証テストここらで認証のテストをしてみますldapsearch -x -D \"cn=Manager,dc=example,dc=com\" -H ldap://localhost/ -Wldapsearch -x -D \"cn=Manager,dc=example,dc=net\" -H ldap://localhost/ -Wどちらでも認証できたでしょうか一意性制約メールドレスや uid の重複登録を許容しないように Unique 制約をつけますsudo ldapadd \u003c\u003c__EOD__dn: cn=module,cn=configcn: moduleobjectClass: olcModuleListolcModulePath: /usr/lib64/openldap/olcModuleLoad: unique.la__EOD__モジュールの定義は /etc/openldap/slapd.d/cn=config/cn=module{N}.ldif に書かれるので /var/lib/ldap/* を消しても残りますsudo ldapadd \u003c\u003c__EOD__dn: olcOverlay=unique,olcDatabase={2}hdb,cn=configobjectClass: olcOverlayConfigobjectClass: olcUniqueConfigolcOverlay: uniqueolcUniqueUri: ldap:///ou=People,dc=example,dc=com?uid?subolcUniqueUri: ldap:///ou=People,dc=example,dc=com?mail?sub__EOD__sudo ldapadd \u003c\u003c__EOD__dn: olcOverlay=unique,olcDatabase={3}hdb,cn=configobjectClass: olcOverlayConfigobjectClass: olcUniqueConfigolcOverlay: uniqueolcUniqueUri: ldap:///ou=People,dc=example,dc=net?uid?subolcUniqueUri: ldap:///ou=People,dc=example,dc=net?mail?sub__EOD__インデックス作成性能向上のためインデックスを作成します。レプリケーション時にも使われます。sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcDbIndexolcDbIndex: uid pres,eq,sub-add: olcDbIndexolcDbIndex: ou pres,eq,sub-add: olcDbIndexolcDbIndex: mail pres,eq,sub-add: olcDbIndexolcDbIndex: objectClass eq-add: olcDbIndexolcDbIndex: entryCSN eq-add: olcDbIndexolcDbIndex: entryUUID eq__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configchangetype: modifyadd: olcDbIndexolcDbIndex: uid pres,eq,sub-add: olcDbIndexolcDbIndex: ou pres,eq,sub-add: olcDbIndexolcDbIndex: mail pres,eq,sub-add: olcDbIndexolcDbIndex: objectClass eq-add: olcDbIndexolcDbIndex: entryCSN eq-add: olcDbIndexolcDbIndex: entryUUID eq__EOD__SSL / TLS 設定証明書設定Mozilla の Network Security Services (NSS) ツールを使う例があったりするのですが、手持ちの PEM を使う方法がよくわからなかったため、PEM ファイルをそのまま使います。certutil で作られているファイルを削除sudo rm /etc/openldap/certs/*手持ちの証明書がなければ自己署名の証明書を作成sudo openssl req -x509 -days 3650 -out /etc/openldap/certs/server.crt \\   -newkey rsa:2048 -keyout /etc/openldap/certs/server.key -nodes \\   -subj \"/C=JP/ST=Tokyo/L=Minato-ku/O=Example, Inc./CN=ldap.example.com/emailAddress=test@example.com\"/etc/openldap/certs/ca.crt に中間証明書を /etc/openldap/certs/server.crt にサーバー証明書を /etc/openldap/certs/server.key に秘密鍵を置きます。秘密鍵は ldap ユーザーだけが読めれば良いのでsudo chown ldap:ldap /etc/openldap/certs/server.keysudo chmod 400 /etc/openldap/certs/server.keysudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifydelete: olcTLSCACertificatePath-replace: olcTLSCertificateFileolcTLSCertificateFile: /etc/openldap/certs/server.crt-replace: olcTLSCertificateKeyFileolcTLSCertificateKeyFile: /etc/openldap/certs/server.key-replace: olcTLSProtocolMinolcTLSProtocolMin: 3.1__EOD__中間証明書が必要な場合はこちらもsudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifyreplace: olcTLSCACertificateFileolcTLSCACertificateFile: /etc/openldap/certs/ca.crt__EOD__確認sudo ldapsearch -b cn=config objectClass=olcGlobalPEM ファイルを更新したら slapd の再起動が必要ですが、その前に ldaps (636) ポートを Listen するように /etc/sysconfig/slapd を書き換えます(SLAP_URLS に ldaps:/// を追加)sudo sed -i -e 's#SLAPD_URLS=\"ldapi:/// ldap:///\"#SLAPD_URLS=\"ldapi:/// ldap:/// ldaps:///\"#' /etc/sysconfig/slapdslapd の再起動sudo systemctl restart slapdクライアント側の設定となりますが /etc/openldap/ldap.conf で TLS_REQCERT allow と指定しない場合、ldapsearch コマンドなどでいわゆるオレオレ証明書（自己署名）サイトなどにアクセスできなくなります。TLS_CACERT /etc/pki/tls/certs/ca-bundle.crt としておくと一般的な証明機関で発行されたサーバーへの接続が可能となります。テストldapsearch -x -H ldaps://localhost/ -W -D cn=Manager,dc=example,dc=com -b dc=example,dc=com cn=Manageropenssl s_client -connect localhost:636openssl コマンドで接続できるのに ldapsearch で接続できない場合は ldap.conf の TLS_REQCERT allow を確認レプリケーション設定ServerID 設定ldap1 と ldap2 というサーバーでレプリケーションするとしますolcServerID をサーバーごとに一意に設定しますldap1 でsudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifyreplace: olcServerIDolcServerID: 1__EOD__ldap2 でsudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifyreplace: olcServerIDolcServerID: 2__EOD__を実行しますレプリケーション用ユーザー作成ACL 設定で既に出ていますが Replication というユーザー(?)を作成しますレプリケーションは参照さえできれば良いため RootDN (Manager) とは別にユーザーを作成しますこの例のパスワードは YB)gNF!Q6)sudo ldapadd \u003c\u003c__EOD__dn: cn=Replication,dc=example,dc=comobjectClass: organizationalRoleobjectClass: simpleSecurityObjectcn: ReplicationuserPassword: {SSHA}bSU9ueMZ93IIlXB7KdiiLby9S0bF0C+J__EOD__sudo ldapadd \u003c\u003c__EOD__dn: cn=Replication,dc=example,dc=netobjectClass: organizationalRoleobjectClass: simpleSecurityObjectcn: ReplicationuserPassword: {SSHA}bSU9ueMZ93IIlXB7KdiiLby9S0bF0C+J__EOD__パスワード変更は次のようにして行えます (-S はパスワード入力プロンプト表示の指定)sudo ldappasswd -S cn=Replication,dc=example,dc=comレプリケーションモジュール登録sudo ldapadd \u003c\u003c__EOD__dn: cn=module,cn=configobjectClass: olcModuleListcn: moduleolcmodulePath: /usr/lib64/openldapolcModuleLoad: syncprov.la__EOD__一意性(Unique)モジュールと同様に /etc/openldap/slapd.d/cn=config/cn=module{N}.ldif に書かれますプロバイダ側設定sudo ldapadd \u003c\u003c__EOD__dn: olcOverlay=syncprov,olcDatabase={2}hdb,cn=configobjectClass: olcOverlayConfigobjectClass: olcSyncProvConfigolcOverlay: syncprovolcSpSessionLog: 1000__EOD__sudo ldapadd \u003c\u003c__EOD__dn: olcOverlay=syncprov,olcDatabase={3}hdb,cn=configobjectClass: olcOverlayConfigobjectClass: olcSyncProvConfigolcOverlay: syncprovolcSpSessionLog: 1000__EOD__コンシューマ側設定双方のサーバーでもう一方のサーバーと同期するように設定する双方向のため(?) oldMirrorMode を有効にしますsudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcSyncReplolcSyncRepl: rid=001  provider=ldap://{相方サーバー}/  bindmethod=simple  binddn=\"cn=Replication,dc=example,dc=com\"  credentials=\"レプリケーションユーザーのパスワード\"  type=refreshAndPersist  interval=00:00:05:00  searchbase=\"dc=example,dc=com\"  scope=sub  retry=\"5 10 30 +__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configchangetype: modifyadd: olcSyncReplolcSyncRepl: rid=002  provider=ldap://{相方サーバー}/  bindmethod=simple  binddn=\"cn=Replication,dc=example,dc=net\"  credentials=\"レプリケーションユーザーのパスワード\"  type=refreshAndPersist  interval=00:00:05:00  searchbase=\"dc=example,dc=net\"  scope=sub  retry=\"5 10 30 +__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={2}hdb,cn=configchangetype: modifyadd: olcMirrorModeolcMirrorMode: TRUE__EOD__sudo ldapmodify \u003c\u003c__EOD__dn: olcDatabase={3}hdb,cn=configchangetype: modifyadd: olcMirrorModeolcMirrorMode: TRUE__EOD__ユーザー、グループを登録してみるsudo ldapadd \u003c\u003c__EOD__dn: uid=user01,ou=People,dc=example,dc=comobjectClass: personobjectClass: inetorgpersonobjectClass: organizationalpersonobjectClass: topuserPassword: {SSHA}OLoVlITmijyBebCFt8f9UfC+w3ikhDTKmail: user01@example.comgivenName: Tarouid: user01cn: Taro Yamadasn: Yamadadn: cn=group01,ou=Group,dc=example,dc=comobjectClass: groupOfUniqueNamesobjectClass: topdescription:: dGVzdCBncm91cAo=cn: group01uniqueMember: uid=user01,ou=People,dc=example,dc=com__EOD__2台のサーバーで検索してレプリケーションがうまく動作しているかを確認するldapsearch -b dc=example,dc=com uid=user01ldapsearch -b dc=example,dc=com cn=group01レプリケーションがうまく動作していなかったら firewalld の設定を確認してみましょうsudo firewall-cmd --list-allservices に ldap, ldaps が入っていなかったら追加しますsudo firewall-cmd --add-service=ldapsudo firewall-cmd --add-service=ldap --permanentsudo firewall-cmd --add-service=ldapssudo firewall-cmd --add-service=ldaps --permanentログも確認sudo journalctl -aru slapd | lessサイズ制限変える必要はないかもしれないが、検索で全件表示させようと思っても制限にかかって一部しか出力されないので制限を緩和するsudo ldapmodify \u003c\u003c__EOD__dn: cn=configchangetype: modifyreplace: olcSizeLimitolcSizeLimit: 3000__EOD__phpLDAPadmin の設定phpLDAPadmin は EPEL リポジトリからインストールできますsudo yum -y install epel-releasesudo yum -y install phpldapadmin設定ファイルが /etc/phpldapadmin/config.php にありますbase 指定マルチドメイン環境で利用する場合それぞれの base を指定する必要があります$servers-\u003esetValue('server','base',array('dc=example,dc=com', 'dc=example,dc=net'));不要な警告の抑制期待するスキーマが無いと警告が表示されてちょっとうるさいので表示しないようにします$config-\u003ecustom-\u003eappearance['hide_template_warning'] = true;ログインID設定uid だけでログインするには次のようにしますが、両方のドメインに存在する uid の場合にどちらかのドメインでしかログインできません$servers-\u003esetValue('login','attr','uid');$servers-\u003esetValue('login','base',array('ou=people,dc=example,dc=com', 'ou=people,dc=example,dc=net'));そのため dn で ID 指定する必要がありますID に uid=user01,ou=people,dc=example,dc=com と入力します$servers-\u003esetValue('login','attr','dn');一意性制約OpenLDAP とは別に Unique 制約の設定があります。デフォルトで mail, uid, uidNumber に制約がかかっていますが、ドメインをまたいでの制約となっており、uid=user01,ou=people,dc=example,dc=com が存在すると uid=user01,ou=people,dc=example,dc=net が登録できません。そこでこの設定を mail だけに変更します。$servers-\u003esetValue('unique','attrs',array('mail'));アクセス制限yum で phpldapadmin をインストールした場合、デフォルトでは localhost からしかアクセスできないようになっているため /etc/httpd/conf.d/phpldapadmin.conf を編集する必要がありますおまけOpenDJ という LDAP サーバーを非特権ユーザーで使っていたため、既存サーバーは 1389/tcp と 1636/tcp を Listen しています。これをスムーズに移行させるため iptables (firewalld) に redirect の設定をしてみます。/etc/firewalld/direct.xml\u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e\u003cdirect\u003e    \u003crule ipv=\"ipv4\" table=\"nat\" chain=\"PREROUTING\" priority=\"0\"\u003e-p tcp --dport 1389 -j REDIRECT --to-port 389\u003c/rule\u003e    \u003crule ipv=\"ipv4\" table=\"nat\" chain=\"PREROUTING\" priority=\"0\"\u003e-p tcp --dport 1636 -j REDIRECT --to-port 636\u003c/rule\u003e\u003c/direct\u003elocalhost からのアクセスでは OUTPUT に書かないとダメらしいですが、外部からのアクセスしかないのでこれだけで良いでしょう。LDAPサーバーOpenDJを使おう(1)LDAPサーバーOpenDJを使おう(2)","link":"https://qiita.com/yteraoka/items/5a4027df290665110751","isoDate":"2015-07-02T01:43:45.000Z","dateMiliSeconds":1435801425000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"Ansible Vault を賢く使う","contentSnippet":"Ansible でパスワードやAPIキーなどの機密情報を扱う場合には ansible-vault を使うことで暗号化されたファイルとして変数を保存できます。簡単な使い方は以前書きましたAnsible Vault を試すしかしながら、ansible-vault はファイル単位での暗号化であるため、暗号化の不要な項目までまるっと暗号化され、どんな変数が定義されているのかすらわからなくなってしまうという問題がありました。これは結構不便です。そんなことをずっと思っていたのですが、ふとドキュメントを読んでいたらこの問題の解決策が書いてありました。Splitting Out Host and Group Specific Dataなんと、バージョン 1.4 という ansible-vault 登場(1.5)以前からある機能でした！！が、As an advanced use-case, you can create directories named after your groups or hosts, and Ansible will read all the files in these directories. An example with the ‘raleigh’ group:からはじまる Ansible Vault を使う場合はサブディレクトリを入れると良いよという説明が加えられたのは 1.8 の時でした。どういう機能かというと、たとえば appservers というグループに属するホストで DB のパスワードを扱いたい場合、group_vars/appservers というファイルにパスワード以外の秘密にしなくても良い変数があったとしても vault で暗号化してしまうと上で書いたような使い辛さがあります。でも実は group_vars/appservers/public.yml, group_vars/appservers/db_password.valut という具合にグループ名をディレクトリとし、その配下に任意の名前でファイルを置いておけば全部読み込んでくれるのです。こういうことであれば、暗号化するファイルは変数名をファイル名にするというルールにすることでどこでどんな変数が定義されているかが復号のためのパスワードを知らない人でもわかります。また、テキストファイルであれば .yml サフィックスをつけておけば editor へのヒントになるし、vault で暗号化しているなら .vault というサフィックスにすることで名前を見ただけで暗号化されていることが分かりますね。2017-09-28 追記Ansible 2.2.1 から変数ファイルは .yml, .yaml, .json という拡張子のファイルもしくは拡張子のない（ドットを含まない）ファイルしか読み込まないようになっていました。（https://github.com/ansible/ansible/issues/18223 あたりから辿れる）Best Practices の Variables and Vaults 項 ではgroup_vars/{group_name}/vars, group_vars/{group_name}/vault というファイルを作り、vars ファイルにpassword: \"{{ vault_password }}\"some_secret: \"{{ some_secret }}\"vault ファイルにvault_password: \"XXXXXXX\"some_secret: \"YYYYYYYY\"と書いて暗号化するべしと書いてあります。もうひとつおまけAnsible Vault を使った playbook ではそのルートフォルダに ansible.cfg を置いてansible.cfg[defaults]ask_vault_pass = Trueと書いておけば --ask-vault-pass オプションをつけ忘れてERROR: A vault password must be specified to decrypt ...と怒られることが無くなります。ansible.cfg[defaults]vault_password_file=/some/where/password.txtとすることもできます。グローバルな設定に書いちゃうとパスワードが違うかもしれないので注意。","link":"https://qiita.com/yteraoka/items/de9da64ca2d9261b0292","isoDate":"2015-05-12T11:28:02.000Z","dateMiliSeconds":1431430082000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"modern.IE を Vagrant で使う","contentSnippet":"Mitchell さんの Tweet で modern.IE の Vagrant 用イメージが公開されていることを知ったので試してみました。Modern.ie is now beta testing Vagrant images as a choice for running IE test environments! http://t.co/c8j8G8V9X5— Mitchell Hashimoto (@mitchellh) 2014, 9月 19Modern.IE については「 無償で Web ページの検証ができる modern.IE. とは? - monoe's blog - Site Home - MSDN Blogs 」で詳しく説明されていました。Vagrant 用で対応している仮想環境は今のところ VirtualBox だけの様ですです。Vagrant 用でなければもっと幅広く対応されています (Hyper-V, VMware, Virtual PC, Parallels など)。$ vagrant init xp-ie6 http://aka.ms/vagrant-xp-ie6$ vagrant upダウンロードに時間がかかるのでしばらく待ち時間がありますが、すでに VirtualBox で Vagrant を使っていればたったこれだけで IE の実行環境が立ち上げられます。便利ですね。イメージファイルは次のものがあるようです。XP with IE6XP with IE8Vista with IE7Windows 7 with IE8Windows 7 with IE9Windows 7 with IE10Windows 7 with IE11Windows 8 with IE10Windows 8 with IE11 (リンク先が Win7 IE11 になってる)ライセンスへの同意があるのでリンクはこちらにしておきます。http://blog.syntaxc4.net/post/2014/09/03/windows-boxes-for-vagrant-courtesy-of-modern-ie.aspx~/.vagrant.d/boxes/ 内のサイズは XP-IE6 が 701M で Win7-IE11 が 3.5GB でした。SSH 接続はできないので Vagrantfile に config.vm.boot_timeout を小さい値で設定しましょう。そして headless モードでは不便なので、コメントアウトされている gui 設定アンコメントしましょう。config.vm.provider \"virtualbox\" do |vb|  vb.gui = trueendぶっちゃけ、ゲストが Windows の場合 vagrant ではインポートだけやって VirtualBox マネージャで起動や停止したほうが良いかもしれず。Vagrant 用の box ファイルじゃない VirtualBox 用の仮想イメージも提供されてます。https://modern.ie/ja-jp/virtualization-tools#downloads","link":"https://qiita.com/yteraoka/items/06fc91f37913c938d936","isoDate":"2014-09-20T04:04:32.000Z","dateMiliSeconds":1411185872000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"LDAPサーバーに REST API アクセス","contentSnippet":"別名: LDAPサーバーOpenDJを使おう(3)LDAPサーバーOpenDJを使おう(1) の続きの LDAPサーバーOpenDJを使おう(2) の続きです。いよいよ REST API の登場です。LDAP で問い合わせるの面倒だから HTTP の REST API あったら便利だよねということです。OpenDJ にはそれ自身に組み込まれた HTTP サーバーと、任意の LDAP サーバーをバックエンドとして利用できる REST API Gateway がありますが、今回の紹介は前者です。権限の問題が解決できずにずっと放置していましたが、もう OpenDJ を使うことはなさそうなので中途半端ですが公開します。なぜ使わないかというと各メジャーバージョンの最初のリリースしか公開されなくなり、Subscription を買わないと更新できなくなったからです。プロダクション環境で使うためには Subscription が必要です。前提次のコマンドでセットアップされた状態で始めます$ setup \\   --cli \\   --baseDN dc=example,dc=com \\   --sampleData 5 \\   --ldapPort 1389 \\   --adminConnectorPort 4444 \\   --rootUserDN cn=Directory\\ Manager \\   --rootUserPassword password \\   --enableStartTLS \\   --ldapsPort 1636 \\   --generateSelfSignedCertificate \\   --hostName localhost \\   --no-prompt \\   --noPropertiesFile \\   --acceptLicenseHTTP サーバーを有効にするstatus コマンド出力の Connection Handlers 部分。HTTP は無効になってます。          --- Connection Handlers ---Address:Port : Protocol               : State-------------:------------------------:-----------           : LDIF                   : Disabled0.0.0.0:161  : SNMP                   : Disabled0.0.0.0:1389 : LDAP (allows StartTLS) : Enabled0.0.0.0:1636 : LDAPS                  : Enabled0.0.0.0:1689 : JMX                    : Disabled0.0.0.0:8080 : HTTP                   : Disabled有効にします。$ bin/dsconfig \\   set-connection-handler-prop \\   --hostname localhost \\   --port 4444 \\   --bindDN \"cn=Directory Manager\" \\   --bindPassword password \\   --handler-name \"HTTP Connection Handler\" \\   --set enabled:true \\   --no-prompt \\   --trustAll          --- Connection Handlers ---Address:Port : Protocol               : State-------------:------------------------:-----------           : LDIF                   : Disabled0.0.0.0:161  : SNMP                   : Disabled0.0.0.0:1389 : LDAP (allows StartTLS) : Enabled0.0.0.0:1636 : LDAPS                  : Enabled0.0.0.0:1689 : JMX                    : Disabled0.0.0.0:8080 : HTTP                   : Enabled有効になりました。$ curl -v http://127.0.0.1:8080/?_prettyPrint=true* About to connect() to 127.0.0.1 port 8080 (#0)*   Trying 127.0.0.1... connected* Connected to 127.0.0.1 (127.0.0.1) port 8080 (#0)\u003e GET /?_prettyPrint=true HTTP/1.1\u003e User-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.14.0.0 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\u003e Host: 127.0.0.1:8080\u003e Accept: */*\u003e \u003c HTTP/1.1 401 Unauthorized\u003c Server: grizzly/2.3.3\u003c WWW-Authenticate: Basic realm=\"org.forgerock.opendj\"\u003c Content-Type: application/json\u003c Date: Fri, 23 May 2014 15:24:20 GMT\u003c Content-Length: 89\u003c {    \"code\": 401,    \"message\": \"Invalid Credentials\",    \"reason\": \"Unauthorized\"}* Connection #0 to host 127.0.0.1 left intact* Closing connection #0動いてるっぽい。けれども 401 が返って来てるので認証が必要みたいです。(認証不要にもできますけどやらないですよね)HTTP でアクセスしてみるsetup コマンドで作成されたサンプルデータ内のユーザー(user.0 .. user.4)のパスワードは password でしたので curl の --user オプションで渡してます。$ curl --user user.0:password 'http://localhost:8080/users/user.0?_prettyPrint=true'{  \"_rev\" : \"000000005049b1ed\",  \"schemas\" : [ \"urn:scim:schemas:core:1.0\" ],  \"contactInformation\" : {    \"telephoneNumber\" : \"+1 685 622 6202\",    \"emailAddress\" : \"user.0@maildomain.net\"  },  \"_id\" : \"user.0\",  \"name\" : {    \"familyName\" : \"Amar\",    \"givenName\" : \"Aaccf\"  },  \"userName\" : \"user.0@maildomain.net\",  \"displayName\" : \"Aaccf Amar\"}ユーザー情報が取得できました。uid が _id として返ってきてます。userName は mail で displayName は cn ですね。これらのマッピングの定義は config/http-config.json に書かれています。$ bin/ldapsearch \\    --hostname localhost \\    --port 1389 \\    --baseDN \"dc=example,dc=com\" \\    \"uid=user.0\"dn: uid=user.0,ou=People,dc=example,dc=compostalAddress: Aaccf Amar$01251 Chestnut Street$Panama City, DE  50369postalCode: 50369description: This is the description for Aaccf Amar.uid: user.0employeeNumber: 0initials: ASAgivenName: AaccfobjectClass: personobjectClass: inetorgpersonobjectClass: organizationalpersonobjectClass: toppager: +1 779 041 6341mobile: +1 010 154 3228cn: Aaccf Amarsn: AmartelephoneNumber: +1 685 622 6202street: 01251 Chestnut StreethomePhone: +1 225 216 5900l: Panama Citymail: user.0@maildomain.netst: DE別ユーザーの情報も取得できます。$ curl --user user.0:password 'http://localhost:8080/users/ser.1?_prettyPrint=true'{  \"_rev\" : \"00000000950cacbf\",  \"schemas\" : [ \"urn:scim:schemas:core:1.0\" ],  \"contactInformation\" : {    \"telephoneNumber\" : \"+1 390 103 6917\",    \"emailAddress\" : \"user.1@maildomain.net\"  },  \"_id\" : \"user.1\",  \"name\" : {    \"familyName\" : \"Atp\",    \"givenName\" : \"Aaren\"  },  \"userName\" : \"user.1@maildomain.net\",  \"displayName\" : \"Aaren Atp\"}データを変更してみるそれではユーザーの追加を試してみましょう。まず、追加リクエスト用の JSON ファイルを作ります。useradd.json{  \"_id\": \"user.5\",  \"contactInformation\": {    \"emailAddress\": \"user.5@example.com\"  },  \"name\": {    \"familyName\": \"Abe\",    \"givenName\": \"Shinzo\"  },  \"displayName\": \"Shinzo Abe\"}$ curl --user user.0:password \\       -X POST \\       -H \"Content-Type: application/json\" \\       --data @useradd.json \\       \"http://localhost:8080/users?_action=create\u0026_prettyPrint=true\"{  \"code\" : 403,  \"reason\" : \"Forbidden\",  \"message\" : \"Insufficient Access Rights: アクセス権限が不十分なため、エントリ uid=user.5,ou=people,dc=example,dc=com を追加できません\"}権限が足りないようです。一般ユーザーなんだから当然ですよね。では user.0 さんに力を授けましょう。grant-all.ldifdn: uid=user.0,ou=People,dc=example,dc=comchangetype: modifyadd: aciaci: (version 3.0;acl \"Admin\"; allow(all)(userdn = \"ldap:///all\");)$ bin/ldapmodify \\    --hostname localhost \\    --port 1389 \\    --bindDN cn=Directory\\ Manager \\    --bindPassword password \\    --defaultAdd \\    --filename grant-all.ldifuid=user.0,ou=People,dc=example,dc=com の MODIFY 要求を処理していますDN uid=user.0,ou=People,dc=example,dc=com に対して MODIFY の操作が成功しましたこれで user.0 さんはほとんどの操作が可能になったはずです。ユーザー追加に再チャレンジ。あれ？ダメですね。どうやれば良いのでしょう？HTTPS への切り替えや SSL 証明書を入れ替える方法とかを書こうと思っていましたがここでおしまい。ApacheDS 試してみようかな。","link":"https://qiita.com/yteraoka/items/dc4eae7fd657af4ab4a2","isoDate":"2014-09-19T12:37:24.000Z","dateMiliSeconds":1411130244000,"authorName":"yteraoka","authorId":"yteraoka"},{"title":"CentOS 7 の nmcli で bonding","contentSnippet":"CentOS 7 (RHEL7) からネットワーク周りの設定は nmcli (NetworkManager) を使えということらしいので、bonding 設定を nmcli で試してみた。前提ネットワークのインターフェースが2つある両方とも switch に接続されている片方だけIPアドレスが設定してあり有効になっているオンラインで切り替える↓こんな環境$ nmcli cNAME      UUID                                  TYPE            DEVICEenp8s0f0  86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  enp8s0f0enp8s0f1  5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --$ ip addr1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00    inet 127.0.0.1/8 scope host lo       valid_lft forever preferred_lft forever    inet6 ::1/128 scope host       valid_lft forever preferred_lft forever2: enp8s0f0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP qlen 1000    link/ether a8:0c:0d:xx:xx:xxe brd ff:ff:ff:ff:ff:ff    inet 192.168.0.100/24 brd 192.168.0.255 scope global enp8s0f0       valid_lft forever preferred_lft forever    inet6 fe80::aa0c:dff:xxxx:xxxx/64 scope link       valid_lft forever preferred_lft forever3: enp8s0f1: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP qlen 1000    link/ether a8:0c:0d:xx:xx:xx brd ff:ff:ff:ff:ff:ffbonding 設定bonding デバイスを追加する※デバイス名はふざけてます、良い子は真似しないように (bond0 とかで良いのではないかと)$ sudo nmcli connection add type bond autoconnect no con-name jamesbond ifname jamesbond mode active-backupConnection 'jamesbond' (3f972a86-feaf-4fd3-89c0-0b781d76aa18) successfully added.$ nmcli cNAME       UUID                                  TYPE            DEVICEenp8s0f0   86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  enp8s0f0jamesbond  3f972a86-feaf-4fd3-89c0-0b781d76aa18  bond            --enp8s0f1   5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --slave を追加するひとまず使ってないインターフェースを slave として追加$ sudo nmcli connection add type bond-slave autoconnect no ifname enp8s0f1 master jamesbondConnection 'bond-slave-enp8s0f1' (30170a37-4157-44be-99dd-9e99442839ff) successfully added.$ nmcli cNAME                 UUID                                  TYPE            DEVICEjamesbond            3f972a86-feaf-4fd3-89c0-0b781d76aa18  bond            --   enp8s0f0             86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  enp8s0f0bond-slave-enp8s0f1  30170a37-4157-44be-99dd-9e99442839ff  802-3-ethernet  --   enp8s0f1             5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --connection.* を確認すると jamesbond の slave だということがわかります$ nmcli -f connection c s bond-slave-enp8s0f1connection.id:                          bond-slave-enp8s0f1connection.uuid:                        30170a37-4157-44be-99dd-9e99442839ffconnection.interface-name:              enp8s0f1connection.type:                        802-3-ethernetconnection.autoconnect:                 noconnection.timestamp:                   0connection.read-only:                   noconnection.permissions:connection.zone:                        --connection.master:                      jamesbondconnection.slave-type:                  bondconnection.secondaries:connection.gateway-ping-timeout:        0@autoconnect` が有効で DHCP サーバーがいたりするとこの時点でIPアドレスが振られちゃってますbonding インターフェースにIPアドレスなどを設定する設定前$ nmcli -f ipv4 c s jamesbondipv4.method:                            autoipv4.dns:ipv4.dns-search:ipv4.addresses:ipv4.routes:ipv4.ignore-auto-routes:                noipv4.ignore-auto-dns:                   noipv4.dhcp-client-id:                    --ipv4.dhcp-send-hostname:                yesipv4.dhcp-hostname:                     --ipv4.never-default:                     noipv4.may-fail:                          yes設定$ sudo nmcli c e jamesbondnmcli\u003e set ipv4.method manualnmcli\u003e set ipv4.dns 192.168.0.5,192.168.0.6nmcli\u003e set ipv4.dns-search example.comnmcli\u003e set ipv4.addresses 192.168.0.100/24 192.168.0.1nmcli\u003e p ipv4['ipv4' setting values]ipv4.method:                            manualipv4.dns:                               192.168.0.5, 192.168.0.6ipv4.dns-search:                        example.comipv4.addresses:                         { ip = 192.168.0.100/24, gw = 192.168.0.1 }ipv4.routes:ipv4.ignore-auto-routes:                noipv4.ignore-auto-dns:                   noipv4.dhcp-client-id:                    --ipv4.dhcp-send-hostname:                yesipv4.dhcp-hostname:                     --ipv4.never-default:                     noipv4.may-fail:                          yesnmcli\u003e verifyVerify connection: OKnmcli\u003e saveConnection 'jamesbond' (3f972a86-feaf-4fd3-89c0-0b781d76aa18) successfully updated.nmcli\u003e quitrestart で入れ替わるように autoconnect を調整するsudo sed -i -e 's/ONBOOT=no/ONBOOT=yes/' /etc/sysconfig/network-scripts/ifcfg-bond-slave-enp8s0f1sudo sed -i -e 's/ONBOOT=no/ONBOOT=yes/' /etc/sysconfig/network-scripts/ifcfg-jamesbondsudo nmcli c m enp8s0f0 connection.autoconnect nosed でファイルを直接書き換えているのには意味があって、no にする場合はすぐに down したりしない(autoconnect だから down とは関係ない)が、nmcli コマンドで yes に変更するとすぐにインターフェースが有効になってしまうため。restart$ sudo service network restart; sudo nmcli c up bond-slave-enp8s0f11行で入力しないと slave が up してくれなくてネットワークアクセスできなくなっちゃいます。もちろん、コンソールからアクセスしてれば分けてももーまんたい。OS 再起動すれば up するんですけど、なにが足りないんだろうか。誰か教えてください。もう一方のインターフェースも bonding に追加$ sudo nmcli connection add type bond-slave ifname enp8s0f0 master jamesbond今回は autoconnect の指定はなし (default yes) で ok$ nmcli cNAME                 UUID                                  TYPE            DEVICEjamesbond            0f731c67-075e-447a-8ddb-c7a4b9dd0d81  bond            jamesbondbond-slave-enp8s0f1  9bb37dd8-24ed-4335-9e5e-b3a1da8fa51c  802-3-ethernet  enp8s0f1enp8s0f1             5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --   bond-slave-enp8s0f0  bdb5239b-a868-40e2-8ff7-81be4313efac  802-3-ethernet  enp8s0f0enp8s0f0             86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  --$ cat /proc/net/bonding/jamesbondEthernet Channel Bonding Driver: v3.7.1 (April 27, 2011)Bonding Mode: fault-tolerance (active-backup)Primary Slave: NoneCurrently Active Slave: enp8s0f1MII Status: upMII Polling Interval (ms): 100Up Delay (ms): 0Down Delay (ms): 0Slave Interface: enp8s0f1MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: a8:0c:0d:xx:xx:xxSlave queue ID: 0Slave Interface: enp8s0f0MII Status: upSpeed: 1000 MbpsDuplex: fullLink Failure Count: 0Permanent HW addr: a8:0c:0d:xx:xx:xxSlave queue ID: 0ついでなので slave を切り替えてみる$ sudo ifenslave -c jamesbond enp8s0f0$ grep 'Active Slave:' /proc/net/bonding/jamesbondCurrently Active Slave: enp8s0f0$ sudo ifenslave -c jamesbond enp8s0f1$ grep 'Active Slave:' /proc/net/bonding/jamesbondCurrently Active Slave: enp8s0f1bonding を解除する片方のインターフェースを開放enp8s0f1 が Active Slave なので enp8s0f0 を外して独立させる$ nmcli cNAME                 UUID                                  TYPE            DEVICEjamesbond            0f731c67-075e-447a-8ddb-c7a4b9dd0d81  bond            jamesbondbond-slave-enp8s0f1  9bb37dd8-24ed-4335-9e5e-b3a1da8fa51c  802-3-ethernet  enp8s0f1enp8s0f1             5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --   bond-slave-enp8s0f0  bdb5239b-a868-40e2-8ff7-81be4313efac  802-3-ethernet  enp8s0f0enp8s0f0             86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  --$ sudo nmcli c del bond-slave-enp8s0f0$ nmcli cNAME                 UUID                                  TYPE            DEVICEjamesbond            0f731c67-075e-447a-8ddb-c7a4b9dd0d81  bond            jamesbondbond-slave-enp8s0f1  9bb37dd8-24ed-4335-9e5e-b3a1da8fa51c  802-3-ethernet  enp8s0f1enp8s0f1             5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --   enp8s0f0             86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  --独立インターフェースを再設定bond-slave-enp8s0f0 が消えたので enp8s0f0 を再設定だけど、前のがの残ってるから今回は autoconnect をいじるだけ$ nmcli -f connection,ipv4 c s enp8s0f0connection.id:                          enp8s0f0connection.uuid:                        86b340dd-08f8-4067-95da-64e5d065c9e2connection.interface-name:              --connection.type:                        802-3-ethernetconnection.autoconnect:                 noconnection.timestamp:                   1407189160connection.read-only:                   noconnection.permissions:connection.zone:                        --connection.master:                      --connection.slave-type:                  --connection.secondaries:connection.gateway-ping-timeout:        0ipv4.method:                            manualipv4.dns:                               192.168.0.5, 192.168.0.6ipv4.dns-search:                        example.comipv4.addresses:                         { ip = 192.168.0.100/24, gw = 192.168.0.1 }ipv4.routes:ipv4.ignore-auto-routes:                noipv4.ignore-auto-dns:                   noipv4.dhcp-client-id:                    --ipv4.dhcp-send-hostname:                yesipv4.dhcp-hostname:                     --ipv4.never-default:                     noipv4.may-fail:                          yesnmcli connection edit enp8s0f0 で set connection.autoconnect yes して save すると、すぐ有効になっちゃうから modify コマンドを使うこと。bonding インターフェースが自動起動しないようにして、enp8s0f0 が起動するようにする$ sudo nmcli c m jamesbond connection.autoconnect no$ sudo nmcli c m bond-slave-enp8s0f1 connection.autoconnect no$ sudo nmcli c m enp8s0f0 connection.autoconnect yesrestart network service$ sudo service network restart今回は enp8s0f0 がちゃんと Up したぞ。bonding の設定を掃除$ sudo nmcli c del bond-slave-enp8s0f1$ sudo nmcli c del jamesbond$ nmcli cNAME      UUID                                  TYPE            DEVICEenp8s0f1  5ddd53ce-04fc-4def-8c28-142c5c1e3f4a  802-3-ethernet  --enp8s0f0  86b340dd-08f8-4067-95da-64e5d065c9e2  802-3-ethernet  enp8s0f0元に戻った。めでたしめでたし。2015/12/15 追記実は sed とか不要だった？環境はCentOS Linux release 7.1.1503 (Core)まだケーブルは1本しかささってないけど、今後のために bonding にしておこうと試した。p2p1 だけが接続されている状態から p2p2 との bonding に切り替える。# bonding interface 作成nmcli connection add type bond autoconnect no con-name bond0 ifname bond0 mode active-backup# bonding の slave を作成nmcli connection add type bond-slave autoconnect no ifname p2p2 master bond0nmcli connection add type bond-slave autoconnect no ifname p2p1 master bond0# bonding interface に ip address などを設定するnmcli c e bond0\u003e set ipv4.method manual\u003e set ipv4.dns xxx.xxx.xxx.xxx,xxx.xxx.xxx.xxx\u003e set ipv4.dns-search example.com\u003e set ipv4.addresses xxx.xxx.xxx.xxx/xx\u003e set ipv4.gateway xxx.xxx.xxx.xxx\u003e set ipv4.routes xxx.xxx.xxx.xxx/xx xxx.xxx.xxx.xxx, xxx.xxx.xxx.xxx/xx xxx.xxx.xxx.xxx\u003e p ipv4\u003e save\u003e quit# 物理 interface の自動起動を無効にするnmcli c m p2p1 connection.autoconnect nonmcli c m p2p2 connection.autoconnect no# bonding interface の自動起動を有効にするnmcli c m bond-slave-p2p1 connection.autoconnect yesnmcli c m bond-slave-p2p2 connection.autoconnect yesnmcli c m bond0 connection.autoconnect yesこれで再起動すれば okOS の再起動を避けたい場合は以前と同じようにsudo service network restart; sudo nmcli c up bond-slave-p2p1","link":"https://qiita.com/yteraoka/items/e661c2a8c6e7617e64f9","isoDate":"2014-08-05T05:54:25.000Z","dateMiliSeconds":1407218065000,"authorName":"yteraoka","authorId":"yteraoka"}]},"__N_SSG":true},"page":"/members/[id]","query":{"id":"yteraoka"},"buildId":"LURuPhi8jmEcg1vWXkQ5K","nextExport":false,"isFallback":false,"gsp":true,"head":[["meta",{"name":"viewport","content":"width=device-width"}],["meta",{"charSet":"utf-8"}],["link",{"rel":"icon shortcut","type":"image/png","href":"https://blog.3-shake.com//logo.png"}],["link",{"href":"https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;500;700\u0026family=Roboto:wght@300;400;500;700\u0026display=swap","rel":"stylesheet"}],["title",{"children":"yteraoka | 3-shake Engineers' Blogs"}],["meta",{"property":"og:title","content":"yteraoka"}],["meta",{"property":"og:url","content":"https://blog.3-shake.com//members/yteraoka"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"property":"og:site","content":"3-shake Engineers' Blogs"}],["meta",{"property":"og:image","content":"https://blog.3-shake.com//og.png"}],["link",{"rel":"canonical","href":"https://blog.3-shake.com//members/yteraoka"}]]}</script><script nomodule="" src="/_next/static/chunks/polyfills-4beebf4ac9054f0bf4e6.js"></script><script src="/_next/static/chunks/main-8a83f0fd99327c4684a8.js" async=""></script><script src="/_next/static/chunks/webpack-e067438c4cf4ef2ef178.js" async=""></script><script src="/_next/static/chunks/framework.1daf1ec1ecf144ee9147.js" async=""></script><script src="/_next/static/chunks/commons.9e003f150a446b53bdd9.js" async=""></script><script src="/_next/static/chunks/pages/_app-c57e3b9671dd7cf89bcb.js" async=""></script><script src="/_next/static/chunks/81b50c7ab23905e464b4340eb234bd6ea389d26b.8252249df9bf225c6a6c.js" async=""></script><script src="/_next/static/chunks/pages/members/%5Bid%5D-82bdc617ad4a0fa06a9e.js" async=""></script><script src="/_next/static/LURuPhi8jmEcg1vWXkQ5K/_buildManifest.js" async=""></script><script src="/_next/static/LURuPhi8jmEcg1vWXkQ5K/_ssgManifest.js" async=""></script></body></html>